<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date></start_date>
		<end_date></end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[]]></city>
		<state></state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>218380</proc_id>
	<acronym>SIGGRAPH '95</acronym>
	<proc_desc>Proceedings of the 22nd annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-701-4</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1995</copyright_year>
	<publication_date>09-15-1995</publication_date>
	<pages>520</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM member price $48</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>A.0</cat_node>
			<descriptor>Conference proceedings</descriptor>
			<type>S</type>
		</primary_category>
		<other_category>
			<cat_node>B.4.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>C.3</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>G.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>H.5.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.2.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.3.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
		<other_category>
			<cat_node>I.4.0</cat_node>
			<descriptor></descriptor>
			<type></type>
		</other_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010178</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010178.10010224</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371.10010382</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010583.10010588</concept_id>
			<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010178.10010216</concept_id>
			<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10003120</concept_id>
			<concept_desc>CCS->Human-centered computing</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10002950</concept_id>
			<concept_desc>CCS->Mathematics of computing</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010520.10010553</concept_id>
			<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010520.10010570</concept_id>
			<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10002944.10011122.10002947</concept_id>
			<concept_desc>CCS->General and reference->Document types->General conference proceedings</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Algorithms</gt>
		<gt>Design</gt>
		<gt>Experimentation</gt>
		<gt>Human Factors</gt>
		<gt>Languages</gt>
		<gt>Management</gt>
		<gt>Measurement</gt>
		<gt>Performance</gt>
		<gt>Reliability</gt>
		<gt>Theory</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>P272117</person_id>
			<author_profile_id><![CDATA[81100086760]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Susan]]></first_name>
			<middle_name><![CDATA[G.]]></middle_name>
			<last_name><![CDATA[Mair]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Univ. of British Columbia, Vancouver, Canada]]></affiliation>
			<role><![CDATA[Editor]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
		<ch_ed>
			<person_id>PP14049320</person_id>
			<author_profile_id><![CDATA[81339494791]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>2</seq_no>
			<first_name><![CDATA[Robert]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Cook]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Editor]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1995</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>218386</article_id>
		<sort_key>10</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Computer graphics achievement award]]></title>
		<page_from>10</page_from>
		<doi_number>10.1145/218380.218386</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218386</url>
		<categories>
			<primary_category>
				<cat_node>A.0</cat_node>
				<descriptor>Conference proceedings</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.2</cat_node>
				<descriptor>People</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003521.10003522</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->History of computing->Historical people</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122.10002947</concept_id>
				<concept_desc>CCS->General and reference->Document types->General conference proceedings</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39073283</person_id>
				<author_profile_id><![CDATA[81100563035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akeley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378516</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kurt Akclcy and Tom Jermoluk. "High-Performance Polygon Kcndcring." SIGGRAPlI '88 Confercncc Proceedings, pp. 239- 246.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617502</ref_obj_id>
				<ref_obj_pid>616006</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kurt Akclcy. "The Silicon Graphics 4D/24OGTX Superworkstation." IEEE Computer Graphics and Applications. July 19X9. pp. 71-83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97913</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Paul IIachcrli and Kurt Akcley. "The Accumulation Buffer: Hardware Support for High-Quality Kcndering," SIGGKAPH '90 Confercncc Proceedings. pp. 309-3 1 X.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Mark Scfal and Kurt Akcley, "The OpenGL Graphics System: A Specification." Silicon Graphics. Inc., 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Kurt Akclcy. "ReaiityEnEine Graphics." SIGGRAPH '93 Conference Proceedings. pp. 109-116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 95. Los Angeles, California, August 6-l 1, 1995 1995 ACM SIGGRAPH Awards Computer Graphics 
Achievement Award  Kurt Akeley The SIGGRAPH achicvcmcnt award is presented to Kurt Akeley in recognition 
of his contributions Lo the architccturc, design, and realization of high performance 3D graphics hardware 
systems. These architectures define not only his company, but provide the high-performance 3D graphics 
facilities that cnablc scientists, cnginccrs. animators. and other users to visualize. crcatc, and dream 
in new ways. Kurt Akclcy received a B.E.E. Prom the University of Dclawarc in 19X0. and an M .S.E.E. 
from Stanford in 19X2. That year hc was a mcmhcr. with previous Achicvcmcnt Awn&#38;c Jim Clark. of the 
lounding team ol Silicon Graphics. lncorporatcd. Ilc has hecn a key technical contributor since then 
wtth primary dcsipn rcsponsi­ billty for most of the high-end graphics architccturcs in SGI s product 
history. Kurt dcvclopcd the rramc burfcrs and proccssol subsystems lor the early INS series products. 
and many of the CAD tools used to design thcsc and other products. Ilc was instrumental in dcvcloplng 
the graphics systems for the Power Series and Onyx systems. including the GTX. the VGX. and the Rcalltyl~n~inc. 
Ilc also led the dcsipn and documentation of the OpcnGI. graphics software specification. which is now 
supported by Silicon Graphics and many other workstation and personal computer vendors. Clearly he is 
a hardwarc wizard. hut one who also has a deep understanding of software and cspccially the critical 
interaction bctwecn hardware and softwarc. Currently. as Vice President and Chief Ilngincer at %;I. hc 
is rcsponsiblc for the specification of cxtcnsions to the OpcnGL graphics software interface. llc has 
rccclved clfhr patents and has others pending. In uddltion to his design contributions. Kurt has published 
papers ahout those architccturcs. and ahout graphics architectures in gcncral These publications arc 
seminal contributions [or the graphics community around the world It is practically impossihlc f or a 
paper descrihlng IICW ideas m graphics architecture to omit. as background. cssent~al rcl crcnccs to 
Kurt Akclcy s contributiom III addition. al the mn~al SIGGRAI II coni crcnccs Kurt s courses on graphics 
architccturcs and graphics programming have cducatcd Icf~ons 01 prol cssionalh in our Industry. llc alxo 
contributes on the panclh UKI program committees. In 19X4 his collcagucs at Silicon Graphics rcco~nizctl 
his contributions bv sclcct~ilg him a5 the rlrst overall Splril 0r %;I award winner. I le rccclvcd the 
IO01 I)lstlnguishcd Alumnus award from the I)cpartmcnl of l:lcctrlcal I:n~lncerin~ 01. lhc Ilnlvcrslt)~ 
01 l>cluwurc. and \vil.\ g~vcrl a I JnivcrxItv 01. ljclawarc I rcsidcnlial ( Ilallon lor Outs13nd1n; 
Achlcvcmcl;l 111 I ) ).?  In short. through his tangible achievements Kurt AkeIey has enabled a unique 
industry, and has enriched the lives of many thousands of users who are now able to break new ground 
in their own fields because of the power his inventions provide.?herefore. SIGGRAPH is pleased to present 
the SIGGRAPH Achievement Award to Kurt Akeley. l ublications  Kurt Akclcy and Tom Jermoluk. High-Performance 
Polygon Kcndcring. SIGGRAPlI 88 Confercncc Proceedings, pp. 23% 246. Kurt Akclcy. The Silicon Graphics 
4D/24OGTX Superworkstation. IEEE Computer Graphics and Applications. July 19X9. pp. 71-83. Paul IIachcrli 
and Kurt Akcley. The Accumulation Buffer: Hardware Support for High-Quality Kcndering, SIGGKAPH 90 Confercncc 
Proceedings. pp. 309-3 1 X. Mark Scfal and Kurt Akcley, The OpenGL Graphics System: A Specification. 
Silicon Graphics. Inc., 1992. Kurt Akclcy. ReaiityEnEine Graphics. SIGGRAPH 93 Confer­ cncc l rocccdings. 
pp. 109-l 16. Previous award winners I994 Kcnncth E. I orrance 1991 Pat Ilanrahan 1992 Ilcnry Fuchs 
IO ) I James T. Kajiya 1090 Richard Shoup and Alvy Ray Smith 1089 John Warnock IOXX Ala11 11. Barr 10X7 
Kohcrt Cook 10X6 Turner Whitted 10x5 I.orcn ( arpentcr 10X4 James II. ( lark 19x3 James I: Bllnn IO 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218388</article_id>
		<sort_key>11</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Steven A. Coons award for outstanding creative contributions to computer graphics]]></title>
		<page_from>11</page_from>
		<page_to>12</page_to>
		<doi_number>10.1145/218380.218388</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218388</url>
		<categories>
			<primary_category>
				<cat_node>A.0</cat_node>
				<descriptor>Biographies/autobiographies</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.2</cat_node>
				<descriptor>People</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003457.10003521.10003522</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->History of computing->Historical people</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122.10002948</concept_id>
				<concept_desc>CCS->General and reference->Document types->Biographies</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P148498</person_id>
				<author_profile_id><![CDATA[81100063773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jose]]></first_name>
				<middle_name><![CDATA[Luis]]></middle_name>
				<last_name><![CDATA[Encarna&#231;&#227;o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>95335</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Encarnacao, J. and E.G. Schlechtendahl. "Computer Aided Design: fundamentals and system architectures." Springer Verlag, 1983. A second edition with R. Lindner was published in 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807497</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Encarnacao, J., G. Enderle, K. Kansy, G. Nees, E.G. Schlechtendahl, J. Weiss and P. Wisskirchen. "The workstation concept of GKS and the resulting conceptual differences to the GSPC core system." Computer Graphics 14(3), 226-230 (1980).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Encarnacao, J. "Computer Graphics, Eine Einfuhrung in die Programmierung und Anwendung von graphischcn Systemen." Oldcnbourg, Munich, 1975. Two revised editions with W. StraBer will be followed by a fourth edition in 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Encarnacao, J., W. Giloi, J. Saniter, W. StraBer and K Waldschmidt. Programmicrungs- und gcraetctechnische Rcalisicrung cincr 4x4 Matrix fucr Koordinatentransformationcn auf Computer-Bildschirmgcracten; Elcktron. Rechenanlagen 15, 5. 1972.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>101234</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Encarnacao, J., P Bono. M. Encarna@io and W. Hcrzner. "PC Graphics with GKS." Hanser Verlag (1987) and Prentice Hall (1990)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The 1995 Steven A. Coons Award is presented to Dr. Jose Luis Encarnacao for leudershtp in applied research 
using computer graphics for a broad range of industrial and medical applicntmns. in international graphics 
standards and in computer graphtcs education. His work in computer graphics began at the Tcchmcal Umvcr­ 
stty of Berlin tn 1967 when hc was also associated with the Hcinrich Hertz Institute. In the period 1972-75 
hc wxs ;lt the University of Saarbrtickcn. In 1975 hc became Professor at the Technical University of 
Darmstadt. an appointment he holds today. His earliest work involved dcvelopmcnt of basic algorithms 
such as hidden-line elimination and hardware for matrtx manipula­tions (4 by 4). The Encarna$io Scan-Grid 
Hidden Surface algorithm was particularly appropriate for surface representations used in CAD applications 
for car body design. It was during this period he became concerned with graphics systems applied to computer 
aided design-early evidence of his long-term commit­ment to technology transfer between academia and 
industry. a hallmark of the German and European technical communities. His efforts in standards are consistent 
with that commitment. Starting in 1970, Encarnacao lead successful efforts at Standard­ization in Computer 
Graphics in Germany. He convened experts from all over Germany and involved many from the group of researchers 
he had assembled in Darmstadt. He has been espe­cially active in DIN and IS0 standards, and was a primary 
contributor to the GKS graphics standard. He participated in the celebrated meetings at the Chateau Seillac 
in 1976 that led to the Proposed Core Standard put forth by SIGCRAPH. Recognizing at that time the need 
for a practical 2D system, he spearheaded the effort to produce the Graphical Kernel System, GKS, an 
extensive elaboration of an early version of the 3D Core proposals. Subse­quently, under Encarna@o s 
direction, Giinter Pfaff et al devel­oped a GKS implementation at Darmstadt that became the leading commercial 
product of its kind and remains in use in German CAD and other graphics application systems. As a professor 
specializing in computer graphics for more than twenty years, Dr. Encarnaqlo has been the advisor for 
more than forty Ph.D. students, many of whom have themselves attained professorial positions. To ensure 
the stability of Computer Graphics in academic computer science and in industry he initially established 
the Interactive Graphics Research Group (THD-GRIS) in 1975 at the Technical University of Darmstadt. 
In 1984 he also founded, and became chairman of the board, of the Computer Graphics Center (ZGDV). In 
addition to the main laboratory in Darmstadt, it now has associated laboratories in Restock and in 1995 
ACM SIGGRAPH Awards Steven A. Coons Award for Outstanding Creative Contributions to Computer Graphics 
 Jose Luis Encarna@o Portugal. In I987 he became the director of the Fraunhofcr Institute for Computer 
Graphtcs in Darmstadt. At the request of the government, hc a~clccl in the m-unification task by establishing 
;I Frnunhofcr Graphics Laboratory III Restock. These institutes are staffed hy computer graphtcs professtonals, 
more than a hundred in Darmstadt alone and provide a home for many more students pursuing study and rcscarch 
In computer graphtcs and Its applica­ lions. An early cxamptc of successful technology transfer accom­ 
plishcd by thcsc Important and valued institutes is the GRADAS system dcvclopcd In 1975 for AEG. The 
GRADAS systems was used by the Deutschc Bundcspost for more than I5 years for interactive archiving, 
handling and editing of communications (telephone) drawings. A mom recent example is the TRITON system 
dcvclopcd for the Deutsche Wetterdicnst (weather bureau) for vtsualizing and presenting weather forecast 
on TV and now in use by six European TV-channels. The latest contribution is an ISDN-based medical teleconsultation 
system, KAMEDIN, now being very successfully deployed by the German Telekom. In 1980 Encarnacao was the 
dnving force behind founding EUROGRAPHICS, the eminent forum for graphics researchers and practitioners 
in the greater European community. Not only did he spearhead its formation, but he devoted considerable 
energy as its first chairman to sustaining it through its early and formative years. He is the author 
of more than one hundred journal articles as well as eight text books. In particular, his book on computer 
aided design was one of the earliest to apply a systems approach for describing CAD. It was translated 
into four languages. Revised and extended in a second edition, it remains one of the few books that presents 
a coherent and unified approach to a theoretical underpinning for computer aided design. EncarnaQo serves 
on several editorial boards, including Computer Graphics and Applications, The Visual Computer, and Computer-Aided 
Design. and is Editor-in-Chief of Computers &#38; Graphics. There are many ways Dr. Encarna@.o has inspired 
and influ­enced both the German and broader European computer graphics communities. He has worked effectively 
to infuse Computer graphics into the international community, forging ties with universities, institutes 
and industry around the world. including Brazil, China, Japan, Mexico, Portugal, Spain, and the USA. 
He has received prizes and been awarded honorary degrees in recognition of these achievements. SIGGRAPH 
95, Los Angeles, California, August 6-11. 1995 Dr. Encarna9b has been a tireless worker and supporter 
of computer graphics both as a key enabling technology and as a critical academic discipline within the 
field of computer science. His vision, insight, and energies have successfully sustained his devotion 
to academic excellence while providing critical liaison with industrial and standardization efforts. 
In recognition of these accomplishments and contributions to Computer Graphics, SIGGRAPH is pleased to 
present Dr. Jose Luis EncamacZo the Steven Anson Coons Award. References Encarnaclo, J. and E.G. Schlechtendahl. 
Computer Aided Design: fundamentals and system architectures. Springer Verlag, 1983. A second edition 
with R. Lindner was published in 1990. Encarnacao, J., G. Enderle, K. Kansy, G. Nees, E.G. Schlechtendahl, 
J. Weiss and P. Wisskirchen. The workstation concept of GKS and the resulting conceptual differences 
to the GSPC core system. Computer Graphics 14(3), 226-230 (1980). EncarnacBo, J. Computer Graphics, Eine 
Einfuhrung in die Programmierung und Anwendung von graphischcn Systemen. Oldcnbourg, Munich, 1975. Two 
revised editions with W. StraBer will be followed by a fourth edition in 1995. EncarnacBo. J., W. Giloi, 
J. Saniter, W. StraBer and K Waldschmidt. Programmicrungs- und gcraetctechnische Rcalisicrung cincr 4x4 
Matrix fucr Koordinatentransformationcn auf Computer-Bildschirmgcracten; Elcktron. Rechenanlagen 15, 
5. 1972. Ilncarnacao. J., I? Bono. M. Encarna@io and W. Hcrzner. PC Graphics with GKS. llanscr Vcrlag 
(1987) and Prcnticc llall ( 1000) I rcvious award winners 1903 1:dwin E. Catmull 1991 Andrics van Dam 
19x9 David C. Lvans 1987 Donald I? Grccnbcrg IOXS Picrrc BCzicr 19X3 Ivan E. Sutherland  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218391</article_id>
		<sort_key>13</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Geometry compression]]></title>
		<page_from>13</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/218380.218391</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218391</url>
		<keywords>
			<kw><![CDATA[3D graphics hardware]]></kw>
			<kw><![CDATA[compression]]></kw>
			<kw><![CDATA[geometry compression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P196764</person_id>
				<author_profile_id><![CDATA[81100240083]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deering]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sun Microsystems, 2550 Garcia Avenue, UMPK14-202, Mountain View, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cook, Robert, L. Carpenter, and E. Catmull. The Reyes Image Rendering Architecture. Proceedings of SIGGRAPH '87 (Anaheim, CA, July 27-31, 1987). In Computer Graphics 21, 4 (july 1987), 95-102.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>221271</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Danskin, John. Compressing the X Graphics Protocol, Ph.D. Thesis, Princeton University, 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Deering, Michael, S. Winner, B. Schediwy, C. Duffy and N. Hunt. The Triangle Processor and Normal Vector Shader: A VLSI system for High Performance Graphics. Proceedings of SIGGRAPH '88 (Atlanta, GA, Aug 1-5, 1988). In Computer Graphics 22, 4 (July 1988), 21-30.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166130</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Deering, Michael, and S. Nelson. Leo: A System for Cost Effective Shaded 3D Graphics. Proceedings of SIGGRAPH '93 (Anaheim, California, August 1-6, 1993). In Computer Graphics (August 1993), 101-108.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951141</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Durkin, James, and J. Hughes. Nonpolygonal Isosulface Rendering for Large Volume Datasets. Proceedings of Visualization '94, IEEE, 293-300.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Foley, James, A. van Dam, S. Feiner and J Hughes. Computer Graphics: Principles and Practice, 2nd ed., Addison-Wesley, 1990.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>573326</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Pennebaker, William, and J. Mitchell. JPEG Still Image Compression Standard, Van Nostrand Reinhold, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Geometry Compression Michael Deering Sun Microsystems ABSTRACT This paper introduces the concept of 
Geometry Compression, al­lowing 3D triangle data to be represented with a factor of 6 to 10 times fewer 
bits than conventional techniques, with only slight loss­es in object quality. The technique is amenable 
to rapid decompres­sion in both software and hardware implementations; if 3D render­ing hardware contains 
a geometry decompression unit, application geometry can be stored in memory in compressed format. Geome­try 
is .rst represented as a generalized triangle mesh, a data struc­ture that allows each instance of a 
vertex in a linear stream to spec­ify an average of two triangles. Then a variable length compression 
is applied to individual positions, colors, and normals. Delta com­pression followed by a modi.ed Huffman 
compression is used for positions and colors; a novel table-based approach is used for nor­mals. The 
table allows any useful normal to be represented by an 18-bit index, many normals can be represented 
with index deltas of 8 bits or less. Geometry compression is a general space-time trade­off, and offers 
advantages at every level of the memory/intercon­nect hierarchy: less storage space is needed on disk, 
less transmis­sion time is needed on networks. CR Categories and Subject Descriptors: I.3.1 [Computer 
Graph­ics]: Hardware Architecture; I.3.3 [Computer Graphics]: Picture/ Image Generation Display algorithms; 
I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism. Additional Keywords and Phrases: 3D 
graphics hardware, com­pression, geometry compression. 1 INTRODUCTION Modern 3D computer graphics makes 
extensive use of geometry to describe 3D objects. Many graphics techniques are available for such use. 
Complex smooth surfaces can be succinctly represented by high level abstractions such as trimmed NURBS. 
Detailed surface geom­etry can many times be rendered by use of texture maps. But as real­ism is added, 
more and more raw geometry is required, usually in the form of triangles. Position, color, and normal 
components of these 2550 Garcia Avenue, UMPK14-202 Mountain View, CA 94043-1100 michael.deering@Eng.Sun.COM 
(415) 786-6325 Permission to make digital/hard copy of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage, the copyright notice, the title of the publication and its date appear, and notice is given 
that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to 
redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 triangles are typically represented as .oating point numbers; an iso­lated triangle can take on 
the order of 100 bytes or more of storage to describe. To maximize detail while minimizing the number 
of trian­gles, triangle re-tessellation techniques can be employed. The tech­niques described in the 
current paper are complementary: for a .xed number of triangles, minimize the total bit-size of the representation, 
subject to quality (and implementation) trade-offs. While many techniques exist for (lossy and lossless) 
compression of 2D pixel images, and at least one exists for 2D geometry [2], no cor­responding techniques 
have previously been available for compres­sion of 3D triangles. This paper describes a viable algorithm 
for Ge­ometry Compression, which furthermore is suitable for implementa­tion in real-time hardware. The 
availability of a decompression unit within rendering hardware means that geometry can be stored and 
transmitted entirely in compressed format. This addresses one of the main bottlenecks in current graphics 
accelerators: input bandwidth. It also greatly increases the amount of geometry that can be cached in 
main memory. In distributed networked applications, compression can help make shared VR display environments 
feasible, by greatly reducing transmission time. Even low-end video games are going true 3D with a vengeance, 
but without compression even CD-ROMs are limited to a few tens of millions of triangles total storage. 
The technique described here can achieve (lossy) compression ra­tios of between 6 and 10 to 1, depending 
on the original representa­tion format and the .nal quality level desired. The compression proceeds in 
four stages. The .rst is the conversion of triangle data into a generalized triangle mesh form. The second 
is the quantiza­tion of individual positions, colors, and normals. Quantization of normals includes a 
novel translation to non-rectilinear representa­tion. In the third stage the quantized values are delta 
encoded be­tween neighbors. The .nal stage performs a Huffman tag-based variable-length encoding of these 
deltas. Decompression is the re­verse of this process; the decompressed stream of triangle data is then 
passed to a traditional rendering pipeline, where it is processed in full .oating point accuracy. 2REPRESENTATION 
OF GEOMETRY Today, most major MCAD and many animation modeling packages allow the use of CSG (constructive 
solid geometry) and free-form NURBS in the construction and representation of geometry. The re­sulting 
trimmed polynomial surfaces are a high-level representation of regions of smooth surfaces. However for 
hardware rendering, these surfaces are typically pre-tessellated in software into triangles prior to 
transmission to the rendering hardware, even on hardware that supports some form of hardware NURBS rendering. 
Further­more, much of the advantage of the NURBS representation of ge­ometry is for tasks other than 
real-time rendering. These non-ren­dering tasks include representation for machining, physical analysis 
(for example, simulation of turbulence .ow), and interchange. Al­so, accurately representing the trimming 
curves for NURBS is quite data intensive; as a compression technique, trimmed NURBS can be not much more 
compact than pre-tessellated triangles, at least at typical rendering tessellation densities. Finally, 
not all objects are compactly represented by NURBS; outside the mechanical engi­neering world of automobile 
hoods and jet turbine blades, the enter­tainment world of tiger s teeth and tennis shoes do not have 
large, smooth areas where NURB representations would have any advan­tage. Thus while NURBS will continue 
to be appropriate in many cases in the modeling world, compressed triangles will be far more compact 
for many classes of application objects. For many years photorealistic batch rendering has made extensive 
use of texture map techniques (color texture maps, normal bump maps, displacement maps) to compactly 
represent .ne geometric detail. With texture mapping support starting to appear in rendering hardware, 
real-time renders can also apply these techniques. Tex­ture mapping works quite well for large objects 
in the far back­ground: clouds in the sky, buildings in the distance. At closer dis­tances, textures 
work best for 3D objects that are mostly .at: bill­boards, paintings, carpets, marble walls, etc. But 
for nearby objects that are not .at, there is a noticeable loss of quality. One technique is the signboard 
, where the textured polygon always swivels to face the observer. But this technique falls short: when 
viewed in ste­reo, especially head-tracked virtual reality stereo, nearby textures are plainly perceived 
as .at. Here even a lower detail but fully three dimensional polygonal representation of a nearby object 
is much more realistic. Thus geometry compression and texture mapping are complementary techniques; each 
is more appropriate for a dif­ferent portion of a scene. What is important to note is that geometry compression 
achieves the same or better representation density as texture mapping. In the limit they are the same 
thing; in the Reyes rendering architecture [1] deformation mapped texels are converted into micro-polygons 
before being rendered. Since the very early days of 3D raster computer graphics, polyhe­dral representation 
of geometry has been supported. Speci.ed typ­ically as a list of vertices, edges, and faces, arbitrary 
geometry can be expressed. These representations, such as winged-edge data structures (cf. [6]), were 
as much designed to support editing of the geometry as display. Nowadays vestiges of these representations 
live on as interchange formats (for example, Wavefront OBJ). While theoretically compact, some of the 
compaction is given up for readability by use of ASCII representation of the data in inter­change .les. 
Also, few of these formats are set up to be directly passed to rendering hardware as drawing instructions. 
Another his­torical vestige is the support of n-sided polygons in such formats. While early rendering 
hardware could accept such general primi­tives, nearly all of today s (very much faster) hardware mandates 
that all polygon geometry be reduced to triangles before being sub­mitted to hardware. Polygons with 
more than three sides cannot in general be guaranteed to be either planar or convex. If quadrilater­als 
are accepted as rendering primitives, the .ne print somewhere indicates that they will be (arbitrarily) 
split into a pair of triangles before rendering. In keeping with this modern reality, we restrict geometry 
to be compressed to triangles. Modern graphics languages specify binary formats for the represen­tation 
of collections of 3D triangles, usually as arrays of vertex data structures. PHIGS PLUS, PEX, XGL, and 
proposed extensions to OpenGL are of this form. These formats de.ne the storage space taken by executable 
geometry today. Triangles can be isolated or chained in zig-zag or star strips. Iris-GL, XGL, and PEX 
5.2 de.ne a form of generalized triangle strip that can switch from a zig-zag to star-like vertex chaining 
on a vertex by vertex basis (at the expense of an extra header word per vertex in XGL and PEX). In addition, 
a restart code allows multiple disconnected strips of triangles to be speci.ed within one array of vertices. 
In these languages, all vertex components (positions, colors, normals) may be speci.ed by 32-bit single 
precession IEEE .oating point numbers, or 64-bit double precision numbers. XGL, IrisGL, and OpenGL also 
have some 32-bit integer support. IrisGL and OpenGL support input of vertex position components as 16-bit 
integers; normals and colors can be any of these as well as 8-bit components. As will be seen, positions, 
colors, and normals can be quantized to sig­ni.cantly fewer than 32 bits (single precision IEEE .oating 
point), with little loss in visual quality. Indeed, such bit-shaving can be uti­lized in commercial 3D 
graphics hardware, so long as supported by appropriate numerical analysis (cf. [3][4]). 3 GENERALIZED 
TRIANGLE MESH The .rst stage of geometry compression is to convert triangle data into an ef.cient linear 
strip form: the generalized triangle mesh. This is a near-optimal representation of triangle data, given 
.xed storage. The existing concept of a generalized triangle strip structure allows for compact representation 
of geometry while maintaining a linear data structure. That is, the geometry can be extracted by a single 
monotonic scan over the vertex array data structure. This is very important for pipe­lined hardware implementations, 
a data format that requires random ac­cess back to main memory during processing is very problematic. 
However, by con.ning itself to linear strips, the generalized trian­gle strip format leaves a potential 
factor of two (in space) on the ta­ble. Consider the geometry in .gure 1. While it can be represented 
by one triangle strip, many of the interior vertices appear twice in the strip. This is inherent in any 
approach wishing to avoid refer­ences to old data. Some systems have tried using a simple regular mesh 
buffer to support re-use of old vertices, but there is a problem with this in practice: in general, geometry 
does not come in a per­fectly regular rectangular mesh structure. 1 45 Start 6 12 24 30 2526 29 Generalized 
Triangle Strip: R6, O1, O7, O2, O3, M4, M8, O5, O9, O10, M11, M17, M16, M9, O15, O8, O7, M14, O13, M6, 
O12, M18, M19, M20, M14, O21, O15, O22, O16, O23, O17, O24, M30, M29, M28, M22, O21, M20, M27, O26, M19, 
O25, O18 Generalized Triangle Mesh: R6p, O1, O7p, O2, O3, M4, M8p, O5, O9p, O10, M11, M17p, M16p, M-3, 
O15p, O-5, O6, M14p, O13p, M-9, O12, M18p, M19p, M20p, M-5, O21p, O-7, O22p, O-9, O23, O-10, O-7, M30, 
M29, M28, M-1, O-2, M-3, M27, O26, M-4, O25, O-5 Legend: First letter: R = Restart, O = Replace Oldest, 
M = Replace Mi Trailing p = push into mesh buffer Number is vertex number, -number is mesh buffer reference 
where -1 is most recent pushed vertex. Figure 1. Generalized Triangle Mesh The generalized technique 
employed by geometry compression ad­dresses this problem. Old vertices are explicitly pushed into a queue, 
and then explicitly referenced in the future when the old ver­tex is desired again. This .ne control 
supports irregular meshes of nearly any shape. Any viable technique must recognize that storage is .nite; 
thus the maximum queue length is .xed at 16, requiring a 4-bit index. We refer to this queue as the mesh 
buffer. The combi­nation of generalized triangle strips and mesh buffer references is referred to as 
a generalized triangle mesh. The .xed mesh buffer size requires all tessellators/re-strippers for compressed 
geometry to break up any runs longer than 16 unique ref­erences. Since geometry compression is not meant 
to be programmed directly at the user level, but rather by sophisticated tessellators/re-for­matters, 
this is not too onerous a restriction. Sixteen old vertices al­lows up to 94% of the redundant geometry 
to avoid being re-speci.ed. Figure 1 also contains an example of a general mesh buffer repre­sentation 
of the surface geometry. The language of geometry compression supports the four vertex re­placement codes 
of generalized triangle strips (replace oldest, re­place middle, restart clockwise, and restart counterclockwise), 
and adds another bit in each vertex header to indicate if this vertex should be pushed into the mesh 
buffer or not. The mesh buffer ref­erence command has a 4-bit .eld to indicate which old vertex should 
be re-referenced, along with the 2-bit vertex replacement code. Mesh buffer reference commands do not 
contain a mesh buff­er push bit; old vertices can only be recycled once. Geometry rarely is comprised 
purely of positional data; generally a normal and/or color are also speci.ed per vertex. Therefore, mesh 
buffer entries contain storage for all associated per-vertex informa­tion (speci.cally including normal 
and color). For maximum space ef.ciency, when a vertex is speci.ed in the data stream, (per vertex) normal 
and/or color information should be directly bundled with the position information. This bundling is controlled 
by two state bits: bundle normals with vertices (bnv), and bundle colors with vertices (bcv). When a 
vertex is pushed into the mesh buffer, these bits control if its bundled normal and/or color are pushed 
as well. During a mesh buffer reference command, this process is reversed; the two bits spec­ify if a 
normal and/or color should be inherited from the mesh buffer storage, or inherited from the current normal 
or current color. There are explicit commands for setting these two current values. An im­portant exception 
to this rule occurs when an explicit set current nor­mal command is followed by a mesh buffer reference, 
with the bnv state bit active. In this case, the former overrides the mesh buffer nor­mal. This allows 
compact representation of hard edges in surface ge­ometry. The analogous semantics are also de.ned for 
colors, allow­ing compact representation of hard edges in textures. Two additional state bits control 
the interpretation of normals and col­ors when the stream of vertices is turned into triangles. The replicate 
normals over triangle (rnt) bit indicates that the normal in the .nal ver­tex that completes a triangle 
should be replicated over the entire trian­gle. The replicate colors over triangle (rct) bit is de.ned 
analogously. 4 COMPRESSION OF XYZ POSITIONS The 8-bit exponent of 32-bit IEEE .oating-point numbers 
allows po­sitions literally to span the known universe: from a scale of 15 billion light years, down 
to the radius of sub-atomic particles. However for any given tessellated object, the exponent is really 
speci.ed just once by the current modeling matrix; within a given modeling space, the object geometry 
is effectively described with only the 24-bit .xed­point mantissa. Visually, in many cases far fewer 
bits are needed; thus the language of geometry compression supports variable quan­tization of position 
data down to as little a one bit. The question then is what is the maximum number of bits that should 
be supported? Based on empirical visual tests we have done, as well as silicon im­plementation considerations, 
we decided to limit our implementation to support of at most 16 bits of precision per component of position. 
We still assume that the position and scale of the local modeling spac­es are speci.ed by full 32-bit 
or 64-bit .oating-point coordinates. If suf.cient numerical care is taken, multiple such modeling spaces 
can be stitched together without cracks, forming seamless geometry co­ordinate systems with much greater 
than 16-bit positional precision. Most geometry is local, so within the 16-bit (or less) modeling space 
(of each object), the delta difference between one vertex and the next in the generalized mesh buffer 
stream is very likely to b less than 16 bits in signi.cance. Indeed one can histogram the bit length 
of neighboring position deltas in a batch of geometry, and based upon this histogram assign a variable 
length code to compact­ly represent the vertices. The typical coding used in many other similar situations 
is customized Huffman code; this is the case for geometry compression. The details of the coding of position 
deltas are postponed until section 7, where they can be discussed in the context of color and normal 
delta coding as well. 5 COMPRESSION OF RGB COLORS We treat colors similar to positions, but with a smaller 
maximum ac­curacy. Thus rgba color data is .rst quantized to 12-bit unsigned frac­tion components. These 
are absolute linear re.ectivity values, with 1.0 representing 100% re.ectivity. An additional parameter 
allows color data effectively to be quantized to any amount less than 12 bits, i.e. the colors can all 
be within a 5-5-5 rgb color space. (The a .eld is optional, controlled by the color alpha present (cap) 
state bit.) Note that this decision does not necessarily cause mock banding on the .­nal rendered image; 
individual pixel colors are still interpolated be­tween these quantized vertex colors, and also are subject 
to lighting. After considerable debate, it was decided to use the same delta cod­ing for color components 
as is used for positions. Compression of color data is where geometry compression and traditional image 
compression face the most similar problem. However, many of the more advanced techniques for image compression 
were rejected for geometry color compression because of the difference in focus. Image compression (for 
example, JPEG [7]) makes several assump­tions about the viewing of the decompressed data that cannot 
be made for geometry compression. In image compression, it is known a priori that the pixels appear in 
a perfectly rectangular array, and that when viewed, each pixel subtends a narrow range of visual angles. 
In geometry compression, one has almost no idea what the relationship between the viewer and the rasterized 
geometry will be. In image compression, it is known that the spatial frequency on the viewer s eyes of 
the displayed pixels is likely higher than the hu­man visual system s color acuity. This is why colors 
are usually converted to yuv space (cf. [6]), so that the uv color components can be represented at a 
lower spatial frequency than the y (intensity) component. Usually the digital bits representing the sub-sampled 
uv components are split up among two or more pixels. Geometry compression can t take advantage of this 
because the display scale of the geometry relative to the viewer s eye is not .xed. Also, given that 
compressed triangle vertices are connected to 4 - 8 or more oth­er vertices in the generalized triangle 
mesh, there is no consistent way of sharing half the color information across vertices. Similar arguments 
apply for the more sophisticated transforms used in traditional image compression, such as the discrete 
cosine transform. These transforms assume a regular (rectangular) sampling of pixel val­ues, and require 
a large amount of random access during decompression. Another traditional approach avoided was pseudo-color 
look-up ta­bles. Any such look-up table would have to have a (.xed) maxi­mum size, and yet still is a 
very expensive resource for real-time processing. While pseudo-color indices would result in a slightly 
higher compression ratio for certain scenes, it was felt that the rgb model is more general and considerably 
less expensive. Finally, the rgb values are represented as linear re.ectance values. In theory, if all 
the effects of lighting are known ahead of time, a bit or two could have been shaved off the representation 
if the rgb com­ponents had been represented in a nonlinear, or perceptually linear (sometime referred 
to as gamma corrected) space. However, in gen­eral, the effects of lighting are not predictable, and 
considerable hardware resources would have had to be expended to convert from nonlinear to linear light 
on the .y. 6 COMPRESSION OF NORMALS Probably the most innovative concept in geometry compression is 
the method of compressing surface normals. Traditionally 96-bit normals (three 32-bit IEEE .oating-point 
numbers) are used in cal­culations to determine 8-bit color intensities. 96 bits of information theoretically 
could be used to represent 296 different normals, spread evenly over the surface of a unit sphere. This 
is a normal ev­ery 2-46 radians in any direction. Such angles are so exact that spreading out angles 
evenly in every direction from earth you could point out any rock on Mars with sub-centimeter accuracy. 
But for normalized normals, the exponent bits are effectively un­used. Given the constraint N = 1, at 
least one ofN ,N , or N , xy z must be in the range of 0.5 to 1.0. During rendering, this normal will 
be transformed by a composite modeling orientation matrix T: N' = N · T. Assuming the typical implementation 
in which lighting is per­formed in world coordinates, the view transform is not involved in the processing 
of normals. If the normals have been pre-normal­ized, then to avoid redundant re-normalization of the 
normals, the composite modeling transformation matrix T is typically pre-nor­malized to divide out any 
scale changes, and thus: 222 + + = 1, etc. T00, T10T20 ,, During the normal transformation, .oating-point 
arithmetic hard­ware effectively truncates all additive arguments to the accuracy of the largest component. 
The result is that for a normalized normal, be­ing transformed by a scale preserving modeling orientation 
matrix, in all but a few special cases, the numerical accuracy of the transformed normal value is reduced 
to no more than 24-bit .xed-point accuracy. Even 24-bit normal components are still much higher in angular 
ac­curacy than the (repaired) Hubble space telescope. Indeed, in some systems, 16-bit normal components 
are used. In [3] 9-bit normal components were successfully used. After empirical tests, it was determined 
that an angular density of 0.01 radians between normals gave results that were not visually distinguishable 
from .ner repre­sentations. This works out to about 100,000 normals distributed over the unit sphere. 
In rectilinear space, these normals still require high accuracy of representation; we chose to use 16-bit 
components including one sign and one guard bit. This still requires 48 bits to represent a normal. But 
since we are only interested in 100,000 speci.c normals, in theory a single 17­bit index could denote 
any of these normals. The next section shows how it is possible to take advantage of this observation. 
 Normal as Indices The most obvious hardware implementation to convert an index of a normal on the unit 
sphere back into aN N N value, is by table xyz look-up. The problem is the size of the table. Fortunately, 
there are several symmetry tricks that can be applied to vastly reduce the size of the table (by a factor 
of 48). (In [5], effectively the same symme­tries are applied to compress processed voxel data.) First, 
the unit sphere is symmetrical in the eight quadrants by sign bits. In other words, if we let three of 
the normal representation bits be the three sign bits of the xyz components of the normal, then we only 
need to .nd a way to represent one eighth of the unit sphere. Second, each octant of the unit sphere 
can be split up into six identi­cal pieces, by folding about the planesx = y,x = z, and y = z. (See Figure 
2.) The six possible sextants are encoded with another three bits. Now only 1/48 of the sphere remains 
to be represented. This reduces the 100,000 entry look-up table in size by a factor of 48, requiring 
only about 2,000 entries, small enough to .t into an on-chip ROM look-up table. This table needs 11 address 
bits to in­dex into it, so including our previous two 3-bit .elds, the result is a grand total of 17 
bits for all three normal components. Representing a .nite set of unit normals is equivalent to positioning 
points on the surface of the unit sphere. While no perfectly equal angular density distribution exists 
for large numbers of points, many near-optimal distributions exist. Thus in theory one of these with 
the same sort of 48-way symmetry described above could be used for the decompression look-up table. However, 
several addi­tional constraints mandate a different choice of encoding: 1) We desire a scalable density 
distribution. This is one in which zero­ing more and more of the low order address bits to the table 
still results in fairly even density of normals on the unit sphere. Otherwise a dif­ferent look-up table 
for every encoding density would be required. 2) We desire a delta-encodable distribution. Statistically, 
adjacent vertices in geometry will have normals that are nearby on the sur­face of the unit sphere. Nearby 
locations on the 2D space of the unit-sphere surface are most succinctly encoded by a 2D offset. We desire 
a distribution where such a metric exists. 3) Finally, while the computational cost of the normal encoding 
process is not too important, in general, distributions with lower en­coding costs are preferred. For 
all these reasons, we decided to utilize a regular grid in the an­gular space within one sextant as our 
distribution. Thus rather than a monolithic 11-bit index, all normals within a sextant are much more 
conveniently represented as two 6-bit orthogonal angular ad­dresses, revising our grand total to 18-bits. 
Just as for positions and colors, if more quantization of normals is acceptable, then these 6­bit indices 
can be reduced to fewer bits, and thus absolute normals can be represented using anywhere from 18 to 
as few as 6 bits. But as will be seen, we can delta encode this space, further reducing the number of 
bits required for high quality representation of normals. Normal Encoding Parameterization Points on 
a unit radius sphere are parameterized by two angles, . and f , using spherical coordinates.. is the 
angle about the y axis; x > y Figure 2. Encoding of the six sextants of each octant of a sphere. f is 
the longitudinal angle from the y=0 plane. The mapping be­tween rectangular and spherical coordinates 
is: x = cos.·cosf y = sinf z = sin.·cosf (1) Points on the sphere are folded .rst by octant, and then 
by sort order of xyz into one of six sextants. All the table encoding takes place in the positive octant, 
in the region bounded by the half spaces: xz= zyy0 == This triangular-shaped patch runs from 0 to p/4 
radians in ., and from 0 to as much as 0.615479709 radians in ff:. max Quantized angles are represented 
by two n-bit integers. and f , nn where n is in the range of 0 to 6. For a given n, the relationship 
be­tween these indices. andf is . n. .. n= asin tan.f max ·(n . n /. () ) 2 (2) n () ff = f ·f /2 nn 
max These two equations show how values of. andf can be con­ nn verted to spherical coordinates. and 
f, which in turn can be con­verted to rectilinear normal coordinate components via equation 1. To reverse 
the process, e.g. to encode a given normal N into. and n f , one cannot just invert equation 2. Instead, 
the N must be .rst n folded into the canonical octant and sextant, resulting in N . Then N must be dotted 
with all quantized normals in the sextant. For a .xed n, the values of. andf that result in the largest 
(nearest nn unity) dot product de.ne the proper encoding of N. Now the complete bit format of absolute 
normals can be given. The uppermost three bits specify the octant, the next three bits the sex­ tant, 
and .nally two n-bit .elds specify. and f . The 3-bit sex­ nn tant .eld takes on one of six values, the 
binary codes for which are shown in .gure 2. This discussion has ignored some details. In particular, 
the three nor­mals at the corners of the canonical patch are multiply represented (6, 8, and 12 times). 
By employing the two unused values of the sextant .eld, these normals can be uniquely encoded as 26 special 
normals. This representation of normals is amenable to delta encoding, at least within a sextant. (With 
some additional work, this can be extended to sextants that share a common edge.) The delta code between 
two normals is simply the difference in. and f:.. and .f. nnn n 7 COMPRESSION TAGS There are many techniques 
known for minimally representing vari­able-length bit .elds (cf. [7]). For geometry compression, we have 
chosen a variation of the conventional Huffman technique. The Huffman compression algorithm takes in 
a set of symbols to be represented, along with frequency of occurrence statistics (histo­grams) of those 
symbols. From this, variable length, uniquely iden­ti.able bit patterns are generated that allow these 
symbols to be rep­resented with a near-minimum total number of bits, assuming that symbols do occur at 
the frequencies speci.ed. Many compression techniques, including JPEG [7], create unique symbols as tags 
to indicate the length of a variable-length data-.eld that follows. This data .eld is typically a speci.c-length 
delta val­ue. Thus the .nal binary stream consists of (self-describing length) variable length tag symbols, 
each immediately followed by a data .eld whose length is associated with that unique tag symbol. The 
binary format for geometry compression uses this technique to represent position, normal, and color data 
.elds. For geometry compression, these <tag, data> .elds are immediately preceded by (a more conventional 
computer instruction set) op-code .eld. These .elds, plus potential additional operand bits, are referred 
to as ge­ometry instructions (see .gure 3). Traditionally, each value to be compressed is assigned its 
own associated label, e.g. an xyz delta position would be represented by three tag-value pairs. However, 
the delta xyz values arenot uncorrelated, and we can get both a denser and simpler representation by 
taking advantage of this fact. In general, the xyz deltas statistically point equally in all directions 
in space. This means that if the number of bits to represent the largest of these deltas is n, then statistically 
the other two delta values require an av­erage of n-1.4 bits for their representation. Thus we made the 
decision to use a single .eld-length tag to indicate the bit length of .x, .y, and .z. This also means 
that we cannot take advantage of another Huffman tech­nique that saves somewhat less than one more bit 
per component, but our bit savings by not having to specify two additional tag .elds (for.y and .z) outweigh 
this. A single tag .eld also means that a hardware decom­pression engine can decompress all three .elds 
in parallel, if desired. Similar arguments hold for deltas of rgbavalues, and so here also a single .eld-length 
tag indicates the bit-length of the r, g, b, and a (if present) .elds. Both absolute and delta normals 
are also parameterized by a single value (n), which can be speci.ed by a single tag. We chose to limit 
the length of the Huffman tag .eld to the relative­ly small value of six bits. This was done to facilitate 
high-speed low-cost hardware implementations. A 64-entry tag look-up table allows decoding of tags in 
one clock cycle. Three such tables exist: one each for positions, normals, and colors. The tables contain 
the length of the tag .eld, the length of the data .eld(s), a data normal­ization coef.cient, and an 
absolute/relative bit. One additional complication was required to enable reasonable hardware implementations. 
As will be seen in the next section, all instruction are broken up into an eight-bit header, and a variable 
length body. Suf.cient information is present in the header to deter­mine the length of the body. But 
in order to give the hardware time to process the header information, the header of one instruction must 
be placed in the stream before the body of the previous instruc­tion. Thus the sequence ... B0 H1B1 H2B2 
H3 ... has to be encoded: ... H1 B0 H2 B1 H3 B2 ... . 8 GEOMETRY COMPRESSION INSTRUCTIONS All of the 
pieces come together in the geometry compression in­struction set, seen in .gure 3. The Vertex command 
specifies a Huffman compressed delta en­coded position, as well as possibly a normal and/or color, depend­ing 
on bundling bits (bnv and bcv). Two additional bits specify a vertex replacement code (rep); another 
bit controls mesh buffer pushing of this vertex (mbp). The Normal command speci.es a new current normal; 
the Color command a new current color. Both also use Huffman encoding of delta values. The Set State 
instruction updates the .ve state bits: rnt, rct, bnv, bcv, and cap. The Mesh Buffer Reference command 
allows any of the sixteen most recently pushed vertices (and associated normals and/or col­ors) to be 
referenced as the next vertex. A 2-bit vertex replacement code is also speci.ed. The Set Table command 
sets entries in one of the three Huffman de­coding tables (Position, Normal, or Color) to the entry value 
speci.ed. Color bits Normal  Norm bits 6-n Color Color bits 6-n Mesh Buffer Reference r e p Set 
State Set Table Pass Through Address Data  VNOP Bit Count 0 s  Position: tag .x .y .z  Normal: 
tag .r .g .b  .a Color: Figure 3. Geometry Compression Instruction Set The Pass Through command allows 
additional graphics state not controlled directly by geometry compression to be updated in-line. The 
VNOP (Variable length no-op) command allows .elds within the bit stream to be aligned to 32-bit word 
boundaries, so that aligned .elds can be patched at run-time.  9 RESULTS The results are presented in 
.gure 5 a-l and table 1. Figures 5 a-g are of the same base object (a triceratops), but with different 
quantization thresholds on positions and normals. Figure 5a is the original full .oat­ing-point representation: 
96-bit positions, and 96-bit normals, which we denote by P96/N96. Figure 5b and 5c show the effects of 
purely po­sitional quantization: P36/N96 and P24/N96, respectively. Figures 5d and 5e show only normal 
quantization: P96/N18 and P96/N12. Fig­ures 5f and 5g show combined quantization: P48/N18 and P30/N36. 
Figures 5 h-l show only the quantized results: for a galleon (P30/ N12), a Dodge Viper (P36/N14), two 
views of a 57 Chevy (P33/ N13), and an insect (P39/N15). Without zooming into the object, positional 
quantization much above 24-bits has virtually no signi.cant visible effect. As the nor­mal quantization 
is reduced, the positions of specular highlights on the surfaces are offset slightly, but it is not visually 
apparent that these changes are reductions in quality, at least above 12 bits per normal. Note that the 
quantization parameters were photographed with the objects: otherwise even the author was not able to 
distin­guish between the original and most compressed versions. Compression (and other) statistics on 
these objects are summarized in table 1. The .nal column shows the compression ratios achieved over existing 
executable geometry formats. While the total byte count of the compressed geometry is an unambiguous 
number (and shown in the penultimate column), to state a compression ratio, some assumptions must be 
made about the object s uncompressed executable representa­tion. We assumed optimized generalized triangle 
strips, with both po­sitions and normals represented by .oating-point values. This is how the original 
size column was calculated. To see the effect of pure 16­bit .xed point simple strip representation, 
we also show the byte count for this mode of OpenGL (the average strip length went way down, in the range 
of 2-3). Because few if any commercial products take advan­tage of generalized triangle strips, the potential 
memory space savings are considerably understated by the numbers in the table. The earlier columns in 
the table break down the bit usage by com­ponent: just position tag/data, just normal tag/data, and everything 
else (overhead). The quant columns show the quantization thresholds. All results in table 1 are (measured) 
actual compression, with one exception. Because our software compressor does not yet implement a full 
meshifying algorithm, we present estimated mesh buffer results in parentheses (always next to actual 
results). This es­timate assumes a 42% hit ratio in the mesh buffer. While certainly there is statistical 
variation between objects (with respect to compression ratios), we have noted some general trends. When 
compressing using the highest quality setting of the quanti­zation knobs (P48/N18), the compression ratios 
are typically about 6. When most objects start showing visible quantization artifacts, the ratios are 
nearly 10. 10 GEOMETRY COMPRESSION SOFTWARE So far the focus has been on the justi.cation and description 
of the geometry compression format. This section addresses some of the issues that arise when actually 
performing the compression; the next section addresses issues related to hardware and software im­plementation 
of decompression. An important measure for any form of compression is the ratio of the time required 
for compression relative to decompression. Several oth­erwise promising techniques for image compression 
have failed in the marketplace because they require several thousand times more time to compress than 
to decompress. It is acceptable for off-line image compression to take 60X more time than decompression, 
but not too much more; for real-time video conferencing the ratio should be 1. Geometry compression does 
not have this real-time requirement. Even if geometry is being constructed on the .y, most techniques 
for creating geometry (such as CSG) take orders of magnitude more time than displaying geometry. Also, 
unlike the continuous images found in movies, in most applications of geometry compression a com­pressed 
3D object will be displayed for many sequential frames before being discarded. If the 3D object needs 
to be animated, this is typically done with modeling matrices. Indeed for a CD-based game, it is quite 
likely that an object will be decompressed billions of times by custom­ers, while compressed only once 
by the authoring company. Like some other compression systems, geometry compression algo­rithms can have 
a compression-time vs. compression-ratio knob. Thus for a given target level of quality, the more time 
allowed for compression, the better the compression ratio that can be achieved by a geometry compression 
system. There is a corresponding knob for quality of the resulting compressed 3D object. The lower the 
quality knob, the better the compression ratio achieved. We have found an esthetic judgment involved 
in geometry compres­sion, based upon our experiences with the system so far. Some 3D objects start to 
look bad when the target quantization of normals and/ or positions is reduced even a little, others are 
visually unchanged even with a large amount of quantization. Sometimes the compres­sion does cause visible 
artifacts, but may only make the object look different, not necessarily lower in quality. Indeed in one 
case an ele­phant started looking better (more wrinkled skin) the more we quan­tized his normals! The 
point is that there is also a subjective compo­nent to geometry compression. In any highly compressed 
case, the original artist or modeling person that created the 3D object should also pass (interactive) 
judgment on the visual result of the compres­sion. He or She alone can really say if the compressed object 
has cap­tured the spirit of the original intent in creating the model.  But once a model has been created 
and compressed, it can be put into a library, to be used as 3D clip-art at the system level. Below is 
an outline of the geometry compression algorithm: 1. Input explicit bag of triangles to be compressed, 
along with quantization thresholds for positions, normals, and colors. 2. Topologically analyze connectivity, 
mark hard edges in nor­mals and/or color. 3. Create vertex traversal order &#38; mesh buffer references. 
 4. Histogram position, normal, and color deltas. 5. Assign variable length Huffman tag codes for deltas, 
based on histograms, separately for positions, normals, colors. 6. Generate binary output stream by 
first outputting Huffman table initializations, then traversing the vertices in order, out­putting appropriate 
tag and delta for all values.  Implementation status: a compressor of Wavefront OBJ format has been 
implemented. It supports compression of positions and nor­mals, and creates full generalized triangle 
strips, but does not yet implement a full meshifying algorithm. The geometry compression format supports 
many more sophisticated compression opportuni­ties than our existing compressor utilizes. We hope in 
the future to explore variable precision geometry, and .ne structured updates of the compression tables. 
Eventually modelers should generate com­pressed geometry directly; our current compressor spends a lot 
of time .guring out geometric details that the tessellator already knew. The current (un-optimized) software 
can compress ~3K tris/sec. 11 GEOMETRY DECOMPRESSION HARDWARE While many of the techniques employed by 
geometry compression are universal, some of the details were speci.cally designed to al­low-cost, high-speed 
hardware implementations. A geometry com­pression format designed purely for software decompression would, 
of course, be a little different. The features that make the geometry compression instruction set amenable 
to hardware implementation include: one pass sequential processing, limited local storage requirements, 
tag look-up rather than usual Hamming bit-sequential processing, and most arithmetic is comprised of 
shifts, adds, and look-ups. Below is an outline of the geometry decompression algorithm: 1. Fetch the 
rest of the next instruction, and the first 8 bits of the instruction after that. 2. Using the tag table, 
expand any compressed value fields to full precision.  3a.If values are relative, add to current value; 
otherwise replace. 3b.If mesh buffer reference, access old values. 3c.If other command, do housekeeping. 
4. If normal, pass index through ROM table to obtain fullN NN values. xyz 5. Output values in generalized 
triangle strip form to next stage. Implementation status: a software decompressor has been imple­mented, 
and successfully decompresses compressed geometry, at a rate of ~10K triangles/second. Hardware designs 
are in progress, a simplified block diagram can be seen in figure 4. data in  Figure 4. Decompression 
hardware block diagram (simpli.ed). 12 CONCLUSIONS A new technique for (lossy) compression of 3D geometric 
data has been presented. Compression ratios of 6 to 10 to one are achievable with little loss in displayed 
object quality. The technique has been de­signed for the constraints of low cost inclusion into real-time 
3D ren­dering hardware, but is also of use in pure software implementations.   ACKNOWLEDGEMENTS The 
author would like to thank Aaron Wynn for his work on the hard­ware and the meshi.er, Michael Cox for 
help with the writing, Scott Nelson for comments on the paper and help with .gures 1 &#38; 2, and Viewpoint 
DataLabs for the 3D objects used in .gure 5.  REFERENCES 1. Cook, Robert, L. Carpenter, and E. Catmull. 
The Reyes Image Rendering Architecture. Proceedings of SIGGRAPH 87 (Anaheim, CA, July 27-31, 1987). In 
Computer Graphics 21, 4 (july 1987), 95-102. 2. Danskin, John. Compressing the X Graphics Protocol, 
Ph.D. Thesis, Princeton University, 1994. 3. Deering, Michael, S. Winner, B. Schediwy, C. Duffy and 
N. Hunt. The Triangle Processor and Normal Vector Shader: A VLSI system for High Performance Graphics. 
Proceedings of SIGGRAPH '88 (Atlanta, GA, Aug 1-5, 1988). In Computer Graphics 22, 4 (July 1988), 21-30. 
 4. Deering, Michael, and S. Nelson. Leo: A System for Cost Ef­fective Shaded 3D Graphics. Proceedings 
of SIGGRAPH 93 (Anaheim, California, August 1-6, 1993). In Computer Graphics (August 1993), 101-108. 
 5. Durkin, James, and J. Hughes. Nonpolygonal Isosurface Ren­dering for Large Volume Datasets. Proceedings 
of Visualiza­tion 94, IEEE, 293-300. 6. Foley, James, A. van Dam, S. Feiner and J Hughes. Computer Graphics: 
Principles and Practice, 2nd ed., Addison-Wesley, 1990. 7. Pennebaker, William, and J. Mitchell. JPEG 
Still Image Com­pression Standard, Van Nostrand Reinhold, 1993.   5a5b 5c 5d5e 5f 5g5h 5i 5j5k 5l 
Table 1: object name #. s .strip length overhead/ vertex xyz quant bits/ xyz norm quant bits/ norm bits/tri 
original size (bytes) OpenGL 16-bit comp compressed size (bytes) compression ratio triceratops 6,039 
15.9 8.2 48 35.8 18 16.9 61.3 (41.2) 179,704 151,032 46,237 (31,038) 3.9X (5.8X) triceratops 6,039 15.9 
8.2 30 17.8 12 11.0 36.0 (26.5) 179,704 151,032 27,141 (19,959 6.7X (9.1X) galleon 5,118 12.2 8.2 30 
22.0 12 11.0 41.3 (29.7) 155,064 105,936 26,358 (18,954) 5.9X (8.2X) viper 58,203 23.8 8.2 36 20.1 14 
10.9 37.5 (27.2) 1,698,116 1,248,492 272,130 (197,525) 6.3X (8.6X) 57chevy 31,762 12.9 8.2 33 17.3 13 
10.9 35.8 (26.4) 958,160 565,152 141,830 (104,691) 6.8X (9.2X) insect 229,313 3.3 8.7 39 26.3 15 12.8 
58.7 (40.9) 8,383,788 5,463,444 1,680,421 (1,170,237) 5.0X (7.2X)  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218392</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Polygon-assisted JPEG and MPEG compression of synthetic images]]></title>
		<page_from>21</page_from>
		<page_to>28</page_to>
		<doi_number>10.1145/218380.218392</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218392</url>
		<keywords>
			<kw><![CDATA[JPEG]]></kw>
			<kw><![CDATA[MPEG]]></kw>
			<kw><![CDATA[client-server graphics]]></kw>
			<kw><![CDATA[polygon-assisted compression]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.2</cat_node>
				<descriptor>Approximate methods</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor>Distributed/network graphics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.3</cat_node>
				<descriptor>Real-time and embedded systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010570</concept_id>
				<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Integrated Systems, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Agrawala, M., Beers, A.C., Chaddha, N., "Model-based Motion Estimation for Synthetic Animations." Submitted for publication.]]></ref_text>
				<ref_id>Agrawala95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chen, S.E., Williams, L., "View Interpolation for Image Synthesis," Proc. SIGGRAPH '93 (Anaheim, California, August 1-6, 1993). In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 279-288.]]></ref_text>
				<ref_id>Chen93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H., "Hierarchical Geometric Models for Visible Surface Algorithms," CACM, Vol. 19, No. 10, October, 1976, pp. 547-554.]]></ref_text>
				<ref_id>Clark76</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cook, R., Carpenter, L, Catmull, E., "The REYES Image Rendering Architecture," Computer Graphics (Proc. Siggraph), Vol. 21, No. 4, July, 1987, pp. 95-102.]]></ref_text>
				<ref_id>Cook87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192629</ref_obj_id>
				<ref_obj_pid>192593</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Danskin, J., "Higher Bandwidth X," Proc. Multimedia '94 (San Francisco, October 15-20, 1994), ACM, pp. 89-96.]]></ref_text>
				<ref_id>Danskin94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218391</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Deering, M., "Geometry Compression," Proc. SIG- GRAPH '95 (Los Angeles, California, August 7-11, 1995), In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH.]]></ref_text>
				<ref_id>Deering95</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197961</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fowler, J.E., Yagel, R., "Lossless Compression of Volume Data," Proc. 1994 Symposium on Volume Visualization, A. Kaufman and W. Krueger eds., ACM, pp. 43-50.]]></ref_text>
				<ref_id>Fowler94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192171</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gershbein, R., Schroeder, P., Hanrahan, P., "Textures and Radiosity: Controlling Emission and Reflection with Texture Maps," Proc. SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIG- GRAPH, pp. 51-58.]]></ref_text>
				<ref_id>Gershbein94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166155</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Guenter, B.K., Yun, H.C., Mersereau, R.M., "Motion Compensated Compression of Computer Animation Frames," Proc. SIGGRAPH '93 (Anaheim, California, August 1-6, 1993). In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 297-304.]]></ref_text>
				<ref_id>Guenter93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, P., Lawson, J., "A Language for Shading and Lighting Calculations," Computer Graphics (Proc. Siggraph), Vol. 24, No. 4, August, 1990, pp. 289-298.]]></ref_text>
				<ref_id>Hanrahan90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P., Garland, M., "Multiresolution Modeling for Fast Rendering," Proc. Graphics Intelface '94 (May 18-20, 1994, Banff, Alberta), Canadian Information Processing Society, pp. 43-50.]]></ref_text>
				<ref_id>Heckbert94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hoppe, H. DeRose, T., Duchamp, T., McDonald, J., Stuetzle, W., "Mesh Optimization," Proc. SIGGRAPH '93 (Anaheim, California, August 1-6, 1993). In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 19-26.]]></ref_text>
				<ref_id>Hoppe93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Lane, T., Independent JPEG Group Software Codec, Version 4, Internet distribution, URL ftp://ftp.uu.net/graphics/jpeg.]]></ref_text>
				<ref_id>Lane</ref_id>
			</ref>
			<ref>
				<ref_obj_id>103090</ref_obj_id>
				<ref_obj_pid>103085</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Le Gall, D., "MPEG: A Video Compression Standard for Multimedia Applications," CACM, Vol. 34, No. 4, April, 1991, pp. 46-58.]]></ref_text>
				<ref_id>Le Gall91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Luo, J., et al., "A New Method for Block Effect Removal in Low Bit-Rate Image Compression," Proc. ICASSP '94, pp. V-341-344.]]></ref_text>
				<ref_id>Luo94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>96083</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Luther, A.C., Digital Video in the PC Environment, 2nd edition, McGraw-Hill Book Company, New York, 1991.]]></ref_text>
				<ref_id>Luther91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197959</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Muraki, S., "Multiscale 3D Edge Representation of Volume Data by a DOG Wavelet," Proc. 1994 Symposium on Volume Visualization, A. Kaufman and W. Krueger eds., ACM, pp. 35-42.]]></ref_text>
				<ref_id>Muraki94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949853</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ning, P., Hesselink, L., "Fast Volume Rendering of Compressed Data," Proc. Visualization '93, G. Nielson and D. Bergeron ed., IEEE, October, 1993, pp. 11-18.]]></ref_text>
				<ref_id>Ning93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Rowe, L.A., Gong, K., Patel, K., Wallach, D., MPEG-1 Video Software Encoder, Version 1.3, Internet distribution, URL ftp://mm-ftp.cs.berkeley.edu/pub/multimedia/mpeg.]]></ref_text>
				<ref_id>Rowe</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192628</ref_obj_id>
				<ref_obj_pid>192593</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Smith, B.C., "Fast Software Processing of Motion JPEG Video," Proc. Multimedia '94 (San Francisco, October 15-20, 1994), ACM, pp. 77-88.]]></ref_text>
				<ref_id>Smith94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>103089</ref_obj_id>
				<ref_obj_pid>103085</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Wallace, G., "The JPEG Still Picture Compression Standard," CACM, Vol. 34, No. 4, April, 1991, pp. 30-44.]]></ref_text>
				<ref_id>Wallace91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192198</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Wallach, D.S., Kunapalli, S., Cohen, M.F., "Accelerated MPEG Compression of Dynamic Polygonal Scenes," Proc. SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 193-197.]]></ref_text>
				<ref_id>Wallach94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617911</ref_obj_id>
				<ref_obj_pid>616032</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Wang, S.W., Kaufman, A.E., "Volume Sampled Voxelization of Geometric Primitives," IEEE Computer Graphics and Applications, Vol. 14, No. 5, September, 1994, pp. 26-32.]]></ref_text>
				<ref_id>Wang94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197963</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Westermann, R., "A Multiresolution Framework for Volume Rendering," Proc. 1994 Symposium on Volume Visualization, A. Kaufman and W. Krueger eds., ACM, pp. 51-58.]]></ref_text>
				<ref_id>Westermann94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197956</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Wilhelms, J., Van Gelder, A., "Multi-Dimensional Trees for Controlled Volume Rendering and Compression," Proc. 1994 Symposium on Volume Visualization, A. Kaufman and W. Krueger eds., ACM, pp. 27-34.]]></ref_text>
				<ref_id>Wilhelms94</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Polygon-Assisted JPEG and MPEG Compression of Synthetic Images Marc Levoy Computer Science Department 
Stanford University Abstract Recent advances in realtime image compression and decompression hardware 
make it possible for a high-performance graphics engine to operate as a rendering server in a networked 
environment. If the client is a low-end workstation or set-top box, then the rendering task can be split 
across the two devices. In this paper, we explore one strategy for doing this. For each frame, the server 
generates a high-quality rendering and a low-quality rendering, subtracts the two, and sends the difference 
in compressed form. The client generates a matching low quality rendering, adds the decompressed difference 
image, and displays the composite. Within this paradigm, there is wide latitude to choose what constitutes 
a high-quality versus low-quality render­ing. We have experimented with textured versus untextured sur­faces, 
.ne versus coarse tessellation of curved surfaces, Phong versus Gouraud interpolated shading, and antialiased 
versus nonantialiased edges. In all cases, our polygon-assisted compres­sion looks subjectively better 
for a .xed network bandwidth than compressing and sending the high-quality rendering. We describe a software 
simulation that uses JPEG and MPEG-1 compression, and we show results for a variety of scenes. CR Categories: 
I.4.2 [Computer Graphics]: Compression Approximate methods I.3.2 [Computer Graphics]: Graphics Sys­tems 
 Distributed/network graphics Additional keywords: client-server graphics, JPEG, MPEG, polygon-assisted 
compression 1. Introduction In this era of open systems, it is common for multiple graphics engines that 
are software compatible but have greatly differing performance to reside on the same network. A research 
group might have a dozen low-end workstations on desktops and one high-performance workstation in a centralized 
laboratory. Future multi-user video games may have hundreds of set-top boxes connected by cable or phone 
lines to a centralized game server. Recent advances in realtime image compression and decompression hardware 
make it possible for the high­performance machine to operate as a rendering server for the low-end machines. 
This can be accomplished straightforwardly by rendering on the server, then compressing and transmitting 
an image stream to the client. The client decompresses and displays the image stream in a window distinct 
from its own frame buffer. Unfortunately, the standards for compressing images and video -mainly JPEG 
[Wallace91] and MPEG [Le Gall91] -were developed for use on natural scenes, and they are not well suited 
Address: Center for Integrated Systems Email: levoy@cs.stanford.edu Stanford University Web: http://www-graphics.stanford.edu 
Stanford, CA 94305-4070 Permission to make digital/hard copy of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage, the copyright notice, the title of the publication and its date appear, and notice 
is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 for compressing synthetic images. In particular, they perform poorly at the edges of objects and 
in smoothly shaded areas. In this paper, we consider an alternative solution that parti­tions the rendering 
task between client and server. We use the server to render those features that cannot be rendered in 
real time on the client -typically textures and complex shading. These are compressed using JPEG or MPEG 
and sent to the client. We use the client to render those features that compress poorly using JPEG or 
MPEG -typically edges and smooth shading. The two renderings are combined in the client for display on 
its screen. The resulting image is subjectively better for the same bandwidth than can be obtained using 
JPEG or MPEG alone. Alternatively, we can produce an image of comparable quality using less bandwidth. 
The remainder of the paper is organized as follows. In sec­tion 2, we give an overview of our solution, 
and we suggest typi­cal hardware realizations. In section 3, we describe a software simulator we have 
built to test our idea, and we discuss several implementation issues. In section 4, we explore ways to 
partition the rendering task between client and server. Some partitionings work well, and some do not, 
as we shall see. In sections 5 and 6, we discuss related work, limitations, and extensions.  2. Client-server 
relationship Figure 1 shows the .ow of data in our proposed client­server system. The hardware consists 
of a high-performance workstation (henceforth called the server), a low-performance workstation (henceforth 
called the client), and a network. To pro­duce each frame of synthetic imagery, these two machines per­form 
the following three steps: (1) On the server, compute a high-quality and low-quality rendering of the 
scene using one of the partitioning stra­tegies described in section 4. (2) Subtract the two renderings, 
apply lossy compression to the difference image, and send it to the client. (3) On the client, decompress 
the difference image, compute a low-quality rendering that matches the low-quality render­ing computed 
on the server, add the two images, and display the resulting composite image.  Depending on the partitioning 
strategy, there may be two geometric models describing the scene or one model with two rendering options. 
The low-quality model may reside on both machines, or it may be transmitted from server to client (or 
client to server) for each frame. If the model resides on both machines, this can be implemented using 
display lists or two cooperating copies of the application program. The latter solution is com­monly 
used in networked visual simulation applications. To provide interactive performance, the server in such 
a system would normally be a graphics workstation with hardware­accelerated rendering. The client might 
be a lower-end hardware-accelerated workstation, or it might be a PC performing rendering in software, 
or it might be a set-top box utilizing a  Figure 1: Flow of data in proposed client-server rendering 
system. High and low-quality renderings may differ in shading, geometric detail, or other aspects. Transmission 
of the low-quality geometric model is optional, so it is drawn dashed in the .gure. combination of software 
and hardware. Differencing and compression on the server, and decompression and addition on the client, 
would most likely be performed in hardware, although real-time software implementations are also beginning 
to appear. One important caveat regarding the selection of client and server is that there are often 
slight differences in pixel values between equivalent-quality renderings computed by high­performance 
and low-performance machines, even if manufac­tured by the same vendor. If both renderings are antialiased, 
these differences are likely to be small. Nevertheless, they may adversely affect the reconstruction 
in step three.  3. Software simulation Since no commercially available workstation yet offers both high-performance 
rendering and real-time compression/decompression, we have built a software simulation. Rendering is 
performed using the REYES rendering system [Cook87] or the SoftImage Creative Environment, and compres­sion 
is performed using the Independent JPEG Group s codec [Lane] or the Berkeley MPEG-1 codec [Rowe]. Images 
in our simulations are represented as 24-bit RGB pixels with two exceptions. First, the codecs performs 
compres­sion in YCrCb using 4:1:1 subsampling -4 pixels of Y to 1 each of Cr and Cb. Second, pixels in 
the difference image D are com­puted from pixels in the high and low quality images H and L using the 
formula D = 127+(H -L)/2. This formula maps zero­difference pixels to gray and maps positive and negative 
differ­ences to lighter and darker colors, respectively (see .gure 2c). Following this division by 2, 
features in the difference image are represented by pixel excursions (from the mean) half as large as 
corresponding features in the high-quality rendering. To prevent these features from being quantized 
twice as severely during compression, we adjust the quantization tables to compensate. The effect is 
to match the feature degradation (and code size) that would have resulted had we not divided the difference 
pixels by 2. Our experiments using the JPEG codec are presented in .gures 2 through 5. Selected statistics 
for .gures 2 and 3 are summarized in table I. The table gives statistics for image-based compression, 
for polygon-assisted compression using resident geometric models, and for polygon-assisted compression 
using a transmitted low-quality model. Whenever a model is transmitted, it should be compressed. We have 
not implemented compression of geometric models, and little research has been done on the subject, but 
we can estimate the performance of a simple lossless scheme as follows. We assume a polygon mesh, which 
contains on average one vertex per polygon. In our application, vertices can be transformed from object 
space to screen space prior to transmission, after which a two-byte .xed point representation suf.ces 
for each coordinate. Thus, we need 10 bytes per vertex (6 bytes for XYZ + 4 bytes for RGBA). One can 
then difference the coordinates and colors of successive vertices and encode the differences using Huffman 
or arithmetic coding. If successive vertices are spatially adjacent as they would be in a mesh, this 
technique should perform well. Danskin has used a similar method to compress sequences of mouse events 
in X, obtaining 3:1 compression [Danskin94]. We thus estimate that each polygon in a compressed model 
requires 3.3 bytes. To help us understand the performance of polygon-assisted compression, we have computed 
the entropies of the high-quality renderings and difference images. As expected, the latter con­sistently 
have less entropy than the former. We have also com­puted the root mean square errors in the polygon-assisted 
compressions and image-based compressions, relative in each case to the high-quality renderings. Unfortunately, 
root mean square error is a poor measure of subjective image quality. In our opinion, the only meaningful 
way to evaluate the performance of our method is to look at the images. We have also computed several 
animations using motion JPEG and MPEG-1. Our conclusions match those for still images: polygon-assisted 
compression yields subjectively better imagery than image-based compression for the same bandwidth. MPEG-1 
looks better than motion JPEG even at higher compres­sion rates due to its use of motion compensation, 
but neither looks as good as polygon-assisted compression. Unfortunately, still images or analog videotape 
recordings do not capture the full quality difference, which is only evident by looking at a worksta­tion 
screen.  4. Partitioning strategies Image-based compression (e.g. JPEG) of synthetic images fails most 
severely at the silhouette edges of objects and in smoothly shaded areas. What these two features have 
in common is spatial coherence. In other words, they both exhibit relatively large-scale structure. In 
choosing how to partition a synthetic image into a polygon rendering and a compressed difference image, 
we should therefore strive to incorporate into the rendering as much of the coherent structure of the 
synthetic image as possi­ble. In the following paragraphs, we describe three partitioning strategies 
-two that work well and one that does not. 4.1. Textured versus untextured surfaces High-end graphics 
workstations (such as the Silicon Graphics RealityEngine) are typically optimized for the display of 
textured surfaces, while low-end workstations (such as the Silicon Graphics Indy) are typically optimized 
for the display of untex­tured surfaces. Given these capabilities, the most obvious way to partition 
rendering between a high-end server and a low-end client is to omit surface texture on the client. To 
demonstrate this, we consider a room composed of .at surfaces that exhibit smooth shading and texture 
(see .gure 2). The model contains 1131 polygons with a .xed color at each ver­tex. This color was calculated 
using a hierarchical radiosity algo­rithm that approximates the diffuse interre.ection among textured 
surfaces [Gershbein94]. The high-quality rendering (.gure 2a) employs antialiasing, Gouraud-interpolated 
shading, and textur­ing. The low-quality rendering (.gure 2b) employs antialiasing and Gouraud-interpolated 
shading but no texturing. The differ­ ence between the two renderings is shown in .gure 2c. Figures 2d 
through 2g show image-based compression of the high-quality rendering using varying JPEG quality factors. 
Figures 2h through 2k show polygon-assisted compression using quality factors selected to match as closely 
as possible the code sizes in .gures 2d through 2g, assuming that the geometric model resides on both 
machines. The quality factors, code sizes, and compression rates are given below each image. Figures 
2l and 2m (on the next page of .gures) give one more pair, enlarged so that details may be seen. The 
statistics for these two .gures also appear in table I. In every case, polygon-assisted compression is 
superior to image-based compression. There are two distinct reasons for this: . The polygon-assisted 
rendering contains undegraded edges and smoothly shaded areas -precisely those features that fare poorly 
in JPEG compression. . The difference image contains less information than the high­quality rendering, 
so it can be compressed using a higher JPEG quality factor without increasing code size -even higher 
than is required to compensate for the division by 2 in the differ­ence image representation. Thus, texture 
features, which are present only in the difference image, fare better using our method.  As an alternative 
to comparing images at matching code sizes, we can compare the code sizes of images of equal quality. 
Unfortunately, such comparisons are dif.cult because the degra­dations of the two methods are different 
-polygon-assisted compression always produces perfect edges and smooth shading, while JPEG never does. 
If one allows that .gure 2j generated using polygon-assisted compression is comparable in quality to 
.gure 2d generated using image-based compression, then our method gives an additional 3x compression 
for this scene. Table I also estimates the number of bytes required to gen­erate .gure 2m if the model 
is transmitted from server to client using the lossless compression method proposed in section 3. This 
size (13529 bytes) lies between the code sizes of .gures 2e and 2f. Even in this case, polygon-assisted 
compression is supe­rior in image quality to image-based compression, both in terms of its edges and 
smooth shading and in terms of the JPEG quality factor used to transmit the texture information. If rendered 
on a Silicon Graphics RealityEngine, both renderings can be performed -and the difference image computed 
-in a single pass through the data by remicrocoding the fragment generators. 4.2. Fine versus coarse 
tessellation of curved surfaces Many algorithms for displaying curved surfaces operate by subdividing 
(tessellating) each surface into a mesh of small polygons. The shading applied at each polygon vertex 
is typically expensive -possibly including a sophisticated re.ection model and texture, but the shading 
across a polygon is typically simple ­constant or linearly interpolated. This suggests a partitioning 
stra­tegy in which the client renders a surface using a coarse tessella­tion, and the server renders 
the surface twice -once using a coarse tessellation and once using a .ne tessellation. To demonstrate 
this, we consider a bowling pin modeled as a bicubic patch mesh (see .gure 3). The geometry and surface 
properties are modeled in the RenderMan scene description language [Hanrahan90]. Using the REYES rendering 
system [Cook87], we tessellate the patch mesh twice, generating two micropolygon models with associated 
vertex colors. Figure 3d shows image-based compression of the high­quality rendering using a JPEG quality 
factor of 15. Figure 3e shows polygon-based compression using a quality factor selected to match the 
code size in .gure 3d, assuming that the low-quality model resides on both machines. Again, the superiority 
of polygon-assisted compression over image-based compression is evident, particularly along edges and 
in smoothly shaded areas. 4.3. Antialiased Phong-shaded versus   nonantialiased Gouraud-shaded We 
now demonstrate a partitioning strategy that does not work well. Our model is an extruded letter de.ned 
using a few large polygons (see .gure 4) and rendered using SoftImage. The high-quality rendering uses 
texturing, antialiasing, Phong lighting, and Phong interpolated shading. (Although Phong shading is not 
supported on current high-performance workstations, it can be approximated using an environmental re.ectance 
map.) The low-quality rendering uses Phong lighting and Gouraud interpo­lated shading, but no texturing 
or antialiasing. Although polygon-assisted compression is still superior to image-based compression, 
the difference is less pronounced than in the previous demonstrations. The most obvious artifact in .gure 
4j is that the edges are not well antialiased. Subtracting a jagged edge (.gure 4g) from an antialiased 
edge (.gure 4f) yields an edge-like structure in the difference image (.gure 4h). This structure fares 
poorly during JPEG compression, leaving the edge jagged in the reconstruction. This degradation will 
also occur to edges in textures in section 4.1, but in most applications important edges are modeled 
as geometry, not as texture. The other disturbing artifact in .gure 4j is a blockiness on the face of 
the letter. The Phong-shaded (.gure 4f) and Gouraud­shaded (.gure 4g) pixels on the face are similar 
in color, but they do not match exactly. These small differences are lost during compression, leading 
to the appearance of 8x8 block artifacts. These artifacts can be reduced somewhat through the application 
of post-processing techniques [Luo94]. For comparison, .gure 5 shows a more successful parti­tioning 
of the same scene. In this case, the high and low-quality renderings differ only by the omission of texture. 
This partition­ing is less practical, however, because it requires antialiasing on the client. An alternative 
partitioning would omit antialiasing on both server and client. In this case, edges would have jaggies, 
but these jaggies would not be compounded by JPEG artifacts. Room Pin A. High-quality rendering .g2a 
.g3a number of polygons 1131 75,467 number of pixels 512 x 512 320 x 800 entropy (bits/pixel) 5.8 
3.0 B. Low-quality rendering .g 2b .g 3b number of polygons 1131 3153  C. Difference image .g 2c .g 
3c entropy (bits/pixel) 3.0 1.3  D. Image-based compression .g 2l .g 3d JPEG quality factor 15 15 
JPEG code size (bytes/frame) 9836 6030 compression ratio 80:1 127:1 error versus uncompressed (rms) 6.1 
4.1  E. Polygon-assisted compression (resident model) .g 2m .g 3e JPEG quality factor 41 55 JPEG 
code size (bytes/frame) 9759 5955 compression ratio 81:1 129:1 error versus uncompressed (rms) 7.5 2.9 
 F. Polygon-assisted compression (transmitted model) JPEG code size (from above) 9759 5955 Model size 
(est. bytes/frame) 3770 10510 Total size 13529 16465 compression ratio      58:1 47:1 Table I: 
Image-based compression versus polygon-assisted compression, compared for the scenes pictured in .gures 
2 and 3. In D, we select a quality factor that gives 20 frames per second while requiring 2 Mbs or less 
of network bandwidth. In E, we select a quality factor that matches as closely as possible the code size 
obtained in D, assuming that the low-quality geometric model resides on both client and server. In F, 
we estimate the number of bytes required to generate D assuming that a losslessly compressed low-quality 
model is transmitted from server to client.  5. Related work The problem presented in this paper is 
a special case of two general problems: compression of geometric models and compression of synthetic 
images. Although these two problems have been recognized for a long time, interest in them has risen 
recently due to the growing synergy between digital video, com­puter graphics, and networking technologies. 
Schemes for compressing geometric models can be categorized as lossy or lossless. Lossy schemes can be 
further subdivided into methods that simplify the geometry and methods that represent the full geometry 
using a quantized representation. Geometric simpli.cation methods include hand-generation of hierarchical 
models [Clark76], automatic decimation of polygon meshes [Hoppe93], and 3D scan conversion of geometry 
into mul­tiresolution voxel arrays [Wang94]. A good survey of these methods is given in [Heckbert94]. 
Quantized representations is largely an unexplored area; a notable exception is [Deering95] in these 
proceedings. Lossless compression of geometric models is also largely unexplored. The prospect of a consumer 
market for downloadable video games, in which the model is a signi.cant fraction of the total game size, 
makes this an attractive area for future research. Schemes for compressing synthetic images can be categor­ized 
according to the role played by the underlying geometric model. In the present paper, the model is used 
to partition the synthetic image into a set of polygons and a difference image. Alternatively, the model 
could be used to locally adapt the quanti­zation table in a block-based compression scheme to match the 
characteristics of commonly occurring blocks. Adaptation could be based on block content hints from the 
model or importance hints from the application program. Although there is a large literature on adaptive 
quantization, we know of no results that incorporate information from a 3D graphics pipeline. This seems 
like a fruitful area for future research. Another possibility is to augment a transform coding scheme 
by adding basis functions that directly represent edges and bilinearly interpolated shading. The screen 
axis aligned rectangles of Intel s DVI PLV standard offer some of this [Luther91]. The present paper 
can be viewed as a generalization of this scheme to unconstrained overlapping tri­angles. For animation 
sequences, a geometric model can be used to derive optical .ow -the interframe movement of each pixel 
in an image. Optical .ow can in turn be used to compute block motion vectors [Wallach94] or block image 
warps [Agrawala95]. Flow can also be used to implement non-blocked predictive coding of closely spaced 
views [Guenter93] or to derive an image morph that interpolates between widely spaced views [Chen93]. 
Among these, only [Guenter93] is lossless. Textures and volumes provide another opportunity for combining 
compression and image synthesis. A lossless compres­sion scheme for volumes based on DPCM and Huffman 
coding is described in [Fowler94]. Lossy schemes include vector quantiza­tion [Ning93], multidimensional 
trees [Wilhelms94], differences of Gaussians wavelets [Muraki94] and Haar and Daubechies wavelets [Westermann94]. 
Also related are algorithms for apply­ing texture manipulation operators (such as magnifying or minify­ing) 
directly to JPEG representations of textures [Smith94]. 6. Conclusions We have described a method for 
using a high-performance graphics workstation as a rendering server for a low-performance workstation, 
and we have explored several strategies for partition­ing the rendering task between client and server. 
Our method improves image quality over compressing and transmitting a sin­gle rendering because it removes 
from the transmitted image those features that compress poorly -mainly edges and smooth shading. These 
are instead rendered locally on the client. Our method has several limitations. First, it will require 
a careful implementation to avoid excessive latency. Second, it requires either storing the low-quality 
geometric model on both machines or transmitting it for each frame. The former solution requires some 
memory in the client. The later solution depends on our ability to compress the model. Some geometric 
models will not compress well, and complex models will always be too large to transmit. Third, our method 
is most useful on scenes of moderate image complexity. For synthetic scenes whose com­plexity approximates 
that of natural scenes, our method will con­vey little or no advantage. Finally, our method is most useful 
at high compression rates (more than 25:1). If the network can sup­port transmission of the high-quality 
rendering at a low compres­sion rate, both MPEG and motion JPEG perform well enough for most interactive 
tasks. Extensions to our work include investigating the compres­sion of geometric models, employing an 
alternative coding tech­nique such as wavelet-based compression, and exploring the use of polygon-assisted 
compression as a .le format for archiving, (non-realtime) image transmission, and printing. In addition, 
a quantitative model of compression error that reliably captures sub­jective image quality is sorely 
needed. Regarding the longevity of our method, while it is true that low-end machines are getting more 
powerful each year, there will always be a high-end machine that costs more money and delivers more performance. 
Thus, although the partitionings described in this paper may become obsolete, there will probably always 
be partitionings for which our method provides an advantage. In a similar vein, an assumption underlying 
our method is that compression, transmission, and decompression taken together are less expensive than 
rendering the original model locally on the client. Although the computational expense of compressing 
a pixel using transform-based coding is largely independent of image content and will probably remain 
constant for the forsee­able future, the cost of rendering that pixel will rise as image syn­thesis methods 
become more sophisticated. This points toward a continuing niche for our method. 7. Acknowledgements 
Discussions with Anoop Gupta, Pat Hanrahan, David Heeger, and Navin Chaddha were useful during this project. 
The suggestions of one reviewer were particularly insightful, and I have attempted to incorporate them 
into my revised manuscript. The possibility of remicrocoding the RealityEngine was suggested to me by 
Brian Cabral of Silicon Graphics. The radiosity model was provided by Reid Gershbein, and the the bowling 
pin model was taken from Steve Upstill s RenderMan Companion. This research was supported by the NSF 
under contract CCR-9157767.  8. References [Agrawala95] Agrawala, M., Beers, A.C., Chaddha, N., Model-based 
Motion Estimation for Synthetic Anima­tions. Submitted for publication. [Chen93] Chen, S.E., Williams, 
L., View Interpolation for Image Synthesis, Proc. SIGGRAPH 93 (Anaheim, Cali­fornia, August 1-6, 1993). 
In Computer Graphics Proceed­ings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 279-288. [Clark76] 
Clark, J.H., Hierarchical Geometric Models for Visi­ble Surface Algorithms, CACM, Vol. 19, No. 10, 
October, 1976, pp. 547-554. [Cook87] Cook, R., Carpenter, L, Catmull, E., The REYES Image Rendering 
Architecture, Computer Graphics (Proc. Siggraph), Vol. 21, No. 4, July, 1987, pp. 95-102. [Danskin94] 
Danskin, J., Higher Bandwidth X, Proc. Mul­timedia 94 (San Francisco, October 15-20, 1994), ACM, pp. 
89-96. [Deering95] Deering, M., Geometry Compression, Proc. SIG-GRAPH 95 (Los Angeles, California, 
August 7-11, 1995), In Computer Graphics Proceedings, Annual Conference Series, 1995, ACM SIGGRAPH. [Fowler94] 
Fowler, J.E., Yagel, R., Lossless Compression of Volume Data, Proc. 1994 Symposium on Volume Visuali­zation, 
A. Kaufman and W. Krueger eds., ACM, pp. 43-50. [Gershbein94] Gershbein, R., Schroeder, P., Hanrahan, 
P., Tex­tures and Radiosity: Controlling Emission and Re.ection with Texture Maps, Proc. SIGGRAPH 94 
(Orlando, Florida, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, 
ACM SIG- GRAPH, pp. 51-58. [Guenter93] Guenter, B.K., Yun, H.C., Mersereau, R.M., Motion Compensated 
Compression of Computer Anima­tion Frames, Proc. SIGGRAPH 93 (Anaheim, California, August 1-6, 1993). 
In Computer Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 297-304. [Hanrahan90] 
Hanrahan, P., Lawson, J., A Language for Shad­ing and Lighting Calculations, Computer Graphics (Proc. 
Siggraph), Vol. 24, No. 4, August, 1990, pp. 289-298. [Heckbert94] Heckbert, P., Garland, M., Multiresolution 
Modeling for Fast Rendering, Proc. Graphics Interface 94 (May 18-20, 1994, Banff, Alberta), Canadian 
Informa­tion Processing Society, pp. 43-50. [Hoppe93] Hoppe, H. DeRose, T., Duchamp, T., McDonald, J., 
Stuetzle, W., Mesh Optimization, Proc. SIGGRAPH 93 (Anaheim, California, August 1-6, 1993). In Computer 
Graphics Proceedings, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 19-26. [Lane] Lane, T., Independent 
JPEG Group Software Codec, Version 4, Internet distribution, URL ftp://ftp.uu.net/graphics/jpeg. [Le 
Gall91] Le Gall, D., MPEG: A Video Compression Stan­dard for Multimedia Applications, CACM, Vol. 34, 
No. 4, April, 1991, pp. 46-58. [Luo94] Luo, J., et al., A New Method for Block Effect Remo­val in Low 
Bit-Rate Image Compression, Proc. ICASSP 94, pp. V-341-344. [Luther91] Luther, A.C., Digital Video in 
the PC Environment, 2nd edition, McGraw-Hill Book Company, New York, 1991. [Muraki94] Muraki, S., Multiscale 
3D Edge Representation of Volume Data by a DOG Wavelet, Proc. 1994 Symposium on Volume Visualization, 
A. Kaufman and W. Krueger eds., ACM, pp. 35-42. [Ning93] Ning, P., Hesselink, L., Fast Volume Rendering 
of Compressed Data, Proc. Visualization 93, G. Nielson and D. Bergeron ed., IEEE, October, 1993, pp. 
11-18. [Rowe] Rowe, L.A., Gong, K., Patel, K., Wallach, D., MPEG-1 Video Software Encoder, Version 1.3, 
Internet distribution, URL ftp://mm-ftp.cs.berkeley.edu/pub/multimedia/mpeg. [Smith94] Smith, B.C., 
Fast Software Processing of Motion JPEG Video, Proc. Multimedia 94 (San Francisco, October 15-20, 1994), 
ACM, pp. 77-88. [Wallace91] Wallace, G., The JPEG Still Picture Compression Standard, CACM, Vol. 34, 
No. 4, April, 1991, pp. 30-44. [Wallach94] Wallach, D.S., Kunapalli, S., Cohen, M.F., Accelerated MPEG 
Compression of Dynamic Polygonal Scenes, Proc. SIGGRAPH 94 (Orlando, Florida, July 24-29, 1994), In 
Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 193-197. [Wang94] Wang, 
S.W., Kaufman, A.E., Volume Sampled Vox­elization of Geometric Primitives, IEEE Computer Graphics and 
Applications, Vol. 14, No. 5, September, 1994, pp. 26-32. [Westermann94] Westermann, R., A Multiresolution 
Frame­work for Volume Rendering, Proc. 1994 Symposium on Volume Visualization, A. Kaufman and W. Krueger 
eds., ACM, pp. 51-58. [Wilhelms94] Wilhelms, J., Van Gelder, A., Multi-Dimensional Trees for Controlled 
Volume Rendering and Compres­sion, Proc. 1994 Symposium on Volume Visualization,A. Kaufman and W. Krueger 
eds., ACM, pp. 27-34.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218395</article_id>
		<sort_key>29</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[QuickTime VR]]></title>
		<subtitle><![CDATA[an image-based approach to virtual environment navigation]]></subtitle>
		<page_from>29</page_from>
		<page_to>38</page_to>
		<doi_number>10.1145/218380.218395</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218395</url>
		<keywords>
			<kw><![CDATA[environment maps]]></kw>
			<kw><![CDATA[image registration]]></kw>
			<kw><![CDATA[image warping]]></kw>
			<kw><![CDATA[panoramic images]]></kw>
			<kw><![CDATA[real-time display]]></kw>
			<kw><![CDATA[view interpolation]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Geometric correction</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Registration</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.3</cat_node>
				<descriptor>Real-time and embedded systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010570</concept_id>
				<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14048866</person_id>
				<author_profile_id><![CDATA[81451598765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shenchang]]></first_name>
				<middle_name><![CDATA[Eric]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apple Computer, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807465</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lippman, A. Movie Maps: An Application ofthe Optical Videodisc to Computer Graphics. Computer Graphics(Proc. SIGGRAPH'80), 32-43.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>65448</ref_obj_id>
				<ref_obj_pid>65445</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ripley, D. G. DVI-a Digital Multimedia Technology. Communications ofthe ACM. 32(7):811-822. 1989.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Miller, G., E Hoffert, S. E Chen, E Patterson, D. Blackketter, S. Rubin, S. A. Applin, D. Yim, J. Hanan. qqae Virtual Museum: Interactive 3 D Navigation of a Multimedia Database. qqae Journal ofVisualization andComputer Animation, (3): 183-197, 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Mohl, R. CognitiveSp ace in the Interactive Movie Map: an Investigation of Spatial Learning in the Virtual Environments. MIT Doctoral Thesis, 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Apple Computer, Inc. QuickTime, Version 1.5 for Developers CD. 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. andM. E. Newell. Texture and Reflection in Computer Generated Images. Communications of the ACM, 19(10):542-547. October 1976.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hall, R. Hybrid Techniques for RapidImage Synthesis. in Whitted, T. and R. Cook, eds. Image Rendering Tricks, Course Notes 16 for SIGGRAPH'86. August 1986.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Greene, N. Environment Mapping andOther Applications of World Projections. Computer Graphics and Applications, 6(11):21-29. November 1986.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Yelick, S. Anamorphic Image Processing. B.S. Thesis. Department of Electrical Engineering and Computer Science. May, 1980.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>562794</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hodges, M and R. Sasnett. Multimedia Computing- Case Studies from MIT Project Athena. 89-102. Addison-Wesley. 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Miller, G. andS. E. Chen. Real-Time Display of Surroundings using Environment Maps. Technical Report No. 44, 1993, Apple Computer, Inc.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Greene, N andM. Kass. Approximating Visibility with Environment Maps. Technical Report No. 41. Apple Computer, Inc.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192192</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Regan, M. and R. Pose. Priority Rendering with a Virtual Reality Address Recalculation Pipeline. Computer Graphics (Proc. SIGGRAPH'94), 155-162.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6023</ref_obj_id>
				<ref_obj_pid>6020</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Greene, N. Creating Raster Ominmax Images from Multiple Perspective Views using the Elliptical Weighted Average Filter. IEEE Computer Graphics and Applications. 6(6):21-27, June, 1986.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108696</ref_obj_id>
				<ref_obj_pid>108693</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Irani, M. and S. Peleg. Improving Resolution by Image Registration. Graphical Models and Image Processing. (3), May, 1991.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R. Image Mosaicing for Tele-Reality Applications. DEC Cambridge Research Lab Technical Report, CRL 94/2. May, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Mann, S. and R. W. Picard. Virtual Bellows: Constructing High Quality Stills from Video. Proceedings of ICIP-94. 363- 367. November, 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. and L. Williams. View Interpolation for Image Synthesis. Computer Graphics(Proc. SIGGRAPH'93), 279-288.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Cheng, N. L. View Reconstruction form Uncalibrated Cameras for Three-Dimensional Scenes. Master's Thesis, Department of Electrical Engineering and Computer Sciences, U. C. Berkeley. 1995.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Laveau, S. and O. Faugeras. 3-D Scene Representation as a Collection of Images and Fundamental Matrices. INRIA, Technical Report No. 2205, February, 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Williams, L. Pyramidal Parametrics. Computer Graphics(Proc. SIGGRAPH'83), 1-11.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Berman, D. R., J. T. Bartell and D. H. Salesin. Multiresolution Painting and Compositing. Computer Graphics (Proc. SIGGRAPH'94), 85-90.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166125</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. andD. Fox. Pad: An Alternative Approach to the Computer Interface. Computer Graphics (Proc. SIGGRAPH'93), 57-72.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>138597</ref_obj_id>
				<ref_obj_pid>138590</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Hoffert, E., L. Mighdoll, M. Kreuger, M. Mills, J. Cohen, et al. QuickTime: an Extensible Standard for Digital Multimedia. Proceedings of the IEEE Computer Conference (CompCon'92), February 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>562706</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Apple Computer, Inc. Inside Macintosh: QuickTime. Addison-Wesley. 1993.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. and G. S. P. Miller. Cylindrical to planar image mapping using scanline coherence. United States Patent number 5,396,583. Mar. 7, 1995.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378497</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Chen, M. A Study in Interactive 3-D Rotation Using 2-D Control Devices. Computer Graphics (Proc. SIGGRAPH'88), 121-130.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357335</ref_obj_id>
				<ref_obj_pid>357332</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Weghorst, H., G. Hooper andD. Greenberg. Improved Computational Methods for Ray Tracing. ACM Transactions on Graphics. 3(1):52-69. 1986.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA['Electronic Panning' Device Opens Viewing Range. Digital Media: A Seybold Report. 2(3): 13-14. August, 1992.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Clark, J. H. Hierarchical Geometric Models for Visible Surface Algorithms. Communications of the ACM, (19)10:547- 554. October, 1976]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166149</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Funkhouser, T. A. and C. H. Sdquin. Adaptive Display Algorithm for Interactive Frame Rates During Visualization of Complex Virtual Environments. Computer Graphics(Proc. SIGGRAPH'93), 247-254.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 QuickTime® VR An Image-Based Approach to Virtual Environment Navigation Shenchang Eric Chen Apple 
Computer, Inc. ABSTRACT Traditionally, virtual reality systems use 3D computer graphics to model and 
render virtual environments in real-time. This approach usually requires laborious modeling and expensive 
special purpose rendering hardware. The rendering quality and scene complexity are often limited because 
of the real-time constraint. This paper presents a new approach which uses 360-degree cylindrical panoramic 
images to compose a virtual environment. The panoramic image is digitally warped on-the-fly to simulate 
camera panning and zooming. The panoramic images can be created with computer rendering, specialized 
panoramic cameras or by "stitching" together overlapping photographs taken with a regular camera. Walking 
in a space is currently accomplished by "hopping" to different panoramic points. The image-based approach 
has been used in the commercial product QuickTime VR, a virtual reality extension to Apple Computer's 
QuickTime digital multimedia framework. The paper describes the architecture, the file format, the authoring 
process and the interactive players of the VR system. In addition to panoramic viewing, the system includes 
viewing of an object from different directions and hit-testing through orientation-independent hot spots. 
CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation Viewing algorithms; 
I.4.3 [ Image Processing]: Enhancement Geometric correction, Registration. Additional Keywords: image 
warping, image registration, virtual reality, real-time display, view interpolation, environment maps, 
panoramic images.  INTRODUCTION A key component in most virtual reality systems is the ability to perform 
a walkthrough of a virtual environment from different viewing positions and orientations. The walkthrough 
requires the synthesis of the virtual environment and the simulation of a virtual camera moving in the 
environment with up to six degrees of freedom. The synthesis and navigation are usually accomplished 
with one of the following two methods. 1.1 3D Modeling and Rendering Traditionally, a virtual environment 
is synthesized as a collection of 3D geometrical entities. The geometrical entities are rendered in real-time, 
often with the help of special purpose 3D rendering engines, to provide an interactive walkthrough experience. 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
The 3D modeling and rendering approach has three main problems. First, creating the geometrical entities 
is a laborious manual process. Second, because the walkthrough needs to be performed in real-time, the 
rendering engine usually places a limit on scene complexity and rendering quality. Third, the need for 
a special purpose rendering engine has limited the availability of virtual reality for most people since 
the necessary hardware is not widely available. Despite the rapid advance of computer graphics software 
and hardware in the past, most virtual reality systems still face the above problems. The 3D modeling 
process will continue to be a very human-intensive operation in the near future. The real­time rendering 
problem will remain since there is really no upper bound on rendering quality or scene complexity. Special­purpose 
3D rendering accelerators are still not ubiquitous and are by no means standard equipment among personal 
computer users. 1.2 Branching Movies Another approach to synthesize and navigate in virtual environments, 
which has been used extensively in the video game industry, is branching movies. Multiple movie segments 
depicting spatial navigation paths are connected together at selected branch points. The user is allowed 
to move on to a different path only at these branching points. This approach usually uses photography 
or computer rendering to create the movies. A computer-driven analog or digital video player is used 
for interactive playback. An early example of this approach is the movie-map [1], in which the streets 
of the city of Aspen were filmed at 10-foot intervals. At playback time, two videodisc players were used 
to retrieve corresponding views to simulate the effects of walking on the streets. The use of digital 
videos for exploration was introduced with the Digital Video Interactive technology [2]. The DVI demonstration 
allowed a user to wander around the Mayan ruins of Palenque using digital video playback from an optical 
disk. A "Virtual Museum" based on computer rendered images and CD-ROM was described in [3]. In this example, 
at selected points in the museum, a 360-degree panning movie was rendered to let the user look around. 
Walking from one of the points to another was simulated with a bi-directional transition movie, which 
contained a frame for each step in both directions along the path connecting the two points. An obvious 
problem with the branching movie approach is its limited navigability and interaction. It also requires 
a large amount of storage space for all the possible movies. However, this method solves the problems 
mentioned in the 3D approach. The movie approach does not require 3D modeling and rendering for existing 
scenes; it can use photographs or movies instead. Even for computer synthesized scenes, the movie-based 
approach decouples rendering from interactive playback. The movie-based approach allows rendering to 
be performed at the highest quality with the greatest complexity without affecting the playback performance. 
It can also use inexpensive and common video devices for playback. 1.3 Objectives Because of the inadequacy 
of the existing methods, we decided to explore a new approach for the creation and navigation of virtual 
environments. Specifically, we wanted to develop a new system which met the following objectives: First, 
the system should playback at interactive speed on most personal computers available today without hardware 
acceleration. We did not want the system to rely on special input or output devices, such as data gloves 
or head-mount displays, although we did not preclude their use. Second, the system should accommodate 
both real and synthetic scenes. Real-world scenes contain enormously rich details often difficult to 
model and render with a computer. We wanted the system to be able to use real-world scenery directly 
without going through computer modeling and rendering. Third, the system should be able to display high 
quality images independent of scene complexity. Many virtual reality systems often compromise by displaying 
low quality images and/or simplified environments in order to meet the real-time display constraint. 
We wanted our system's display speed to be independent of the rendering quality and scene complexity. 
 1.4 Overview This paper presents an image-based system for virtual environment navigation based on the 
above objectives. The system uses real-time image processing to generate 3D perspective viewing effects. 
The approach presented is similar to the movie-based approach and shares the same advantages. It differs 
in that the movies are replaced with orientation­independent images and the movie player is replaced 
with a real-time image processor. The images that we currently use are cylindrical panoramas. The panoramas 
are orientation­independent because each of the images contains all the information needed to look around 
in 360 degrees. A number of these images can be connected to form a walkthrough sequence. The use of 
orientation-independent images allows a greater degree of freedom in interactive viewing and navigation. 
These images are also more concise and easier to create than movies. We discuss work related to our approach 
in Section 2. Section 3 presents the simulation of camera motions with the image-based approach. In Section 
4, we describe QuickTime VR, the first commercial product using the image-based method. Section 5 briefly 
outlines some applications of the image-based approach and is followed by conclusions and future directions. 
2. RELATED WORK The movie-based approach requires every displayable view to be created and stored in 
the authoring stage. In the movie­map [1] [4], four cameras are used to shoot the views at every point, 
thereby, giving the user the ability to pan to the left and right at every point. The Virtual Museum 
stores 45 views for each 360-degree pan movie [3]. This results in smooth panning motion but at the cost 
of more storage space and frame creation time. The navigable movie [5] is another example of the movie­based 
approach. Unlike the movie-map or the Virtual Museum, which only have the panning motion in one direction, 
the navigable movie offers two-dimensional rotation. An object is photographed with a camera pointing 
at the object's center and orbiting in both the longitude and the latitude directions at roughly 10-degree 
increments. This process results in hundreds of frames corresponding to all the available viewing directions. 
The frames are stored in a two-dimensional array which are indexed by two rotational parameters in interactive 
playback. When displaying the object against a static background, the effect is the same as rotating 
the object. Panning to look at a scene is accomplished in the same way. The frames in this case represent 
views of the scene in different view orientations. If only the view direction is changing and the viewpoint 
is stationary, as in the case of pivoting a camera about its nodal point (i.e. the optical center of 
projection), all the frames from the pan motion can be mapped to a canonical projection. This projection 
is termed an environment map, which can be regarded as an orientation-independent view of the scene. 
Once an environment map is generated, any arbitrary view of the scene, as long as the viewpoint does 
not move, can be computed by a reprojection of the environment map to the new view plane. The environment 
map was initially used in computer graphics to simplify the computations of specular reflections on a 
shiny object from a distant scene [6], [7], [8]. The scene is first projected onto an environment map 
centered at the object. The map is indexed by the specular reflection directions to compute the reflection 
on the object. Since the scene is far away, the location difference between the object center and the 
surface reflection point can be ignored. Various types of environment maps have been used for interactive 
visualization of virtual environments. In the movie­map, anamorphic images were optically or electronically 
processed to obtain 360-degree viewing [1], [9]. A project called "Navigation" used a grid of panoramas 
for sailing simulation [10]. Real-time reprojection of environment maps was used to visualize surrounding 
scenes and to create interactive walkthrough [11], [12]. A hardware method for environment map look-up 
was implemented for a virtual reality system [13]. While rendering an environment map is trivial with 
a computer, creating it from photographic images requires extra work. Greene and Heckbert described a 
technique of compositing multiple image streams with known camera positions into a fish-eye view [14]. 
Automatic registration can be used to composite multiple source images into an image with enhanced field 
of view [15], [16], [17]. When the viewpoint starts moving and some objects are nearby, as in the case 
of orbiting a camera around an object, the frames can no longer be mapped to a canonical projection. 
The movement of the viewpoint causes "disparity" between different views of the same object. The disparity 
is a result of depth change in the image space when the viewpoint moves (pivoting a camera about its 
nodal point does not cause depth change). Because of the disparity, a single environment map is insufficient 
to accommodate all the views. The movie-based approach simply stores all the frames. The view interpolation 
method presented by Chen and Williams [18] stores only a few key frames and synthesizes the missing frames 
on-the-fly by interpolation. However, this method requires additional information, such as a depth buffer 
and camera parameters, for each of the key frames. Automatic or semi-automatic methods have been developed 
for registering and interpolating images with unknown depth and camera information [16], [19], [20]. 
 3. IMAGE-BASED RENDERING The image-based approach presented in this paper addresses the simulation of 
a virtual camera's motions in photographic or computer synthesized spaces. The camera's motions have 
six degrees of freedom. The degrees of freedom are grouped in three classes. First, the three rotational 
degrees of freedom, termed "camera rotation", refer to rotating the camera's view direction while keeping 
the viewpoint stationary. This class of motions can be accomplished with the reprojection of an environment 
map and image rotation. Second, orbiting a camera about an object while keeping the view direction centered 
at the object is termed "object rotation" because it is equivalent to rotating the object. This type 
of motion requires the movement of the viewpoint and can not be achieved with an environment map. Third, 
free motion of the camera in a space, termed "camera movement", requires the change of both the viewpoint 
and the viewing direction and has all six degrees of freedom. In addition to the above motions, changing 
the camera's field-of-view, termed "camera zooming", can be accomplished through multiple resolution 
image zooming. Without loss of generality, the environment is assumed to be static in the following discussions. 
However, one can generalize this method to include motions via the use of time­varying environment maps, 
such as environment map movies or 360-degree movies. 3.1 Camera Rotation A camera has three rotational 
degrees of freedom: pitch (pivoting about a horizontal axis), yaw (pivoting about a vertical axis) and 
roll (rotating about an axis normal to the view plane). Camera rolling can be achieved trivially with 
an image rotation. Pitch and yaw can be accomplished by the reprojection of an environment map. An environment 
map is a projection of a scene onto a simple shape. Typically, this shape is a cube [8] or a sphere [6], 
[7]. Reprojecting an environment map to create a novel view is dependent on the type of the environment 
map. For a cubic environment map, the reprojection is merely displaying the visible regions of six texture 
mapped squares in the view plane. For a spherical environment map, non-linear image warping needs to 
be performed. Figure 1 shows the reprojection of the two environment maps. If a complete 360 degree panning 
is not required, other types of environment maps such as cylindrical, fish-eye or wide-angled planar 
maps can be used. A cylindrical map allows 360-degree panning horizontally and less than 180-degree panning 
vertically. A fish-eye or hemi-spherical map allows 180-degree panning in both directions. A planar map 
allows less than 180-degree panning in both directions. few key views of an object. The new views are 
interpolated on­the-fly from the key views, which also means the rotation angle can be arbitrary. 3.3 
Camera Movement A camera moving freely in a scene involves the change of viewpoint and view direction. 
The view direction change can be accomplished with the use of an environment map. The viewpoint change 
is more difficult to achieve. A simple solution to viewpoint change is to constrain the camera's movement 
to only particular locations where environment maps are available. For a linear camera movement, such 
as walking down a hallway, environment maps can be created for points along the path at some small intervals. 
The cost of storing the environment maps is roughly six times the cost of storing a normal walkthrough 
movie if a cubic map is used. The resulting effects are like looking out of a window from a moving train. 
The movement path is fixed but the passenger is free to look around. Environment map movies are similar 
to some special format movies such as Omnimax® (180 degree fish-eye) or CircleVision (360-degree cylindrical) 
movies, in which a wider than normal field-of-view is recorded. The observer can control the viewing 
direction during the playback time. For traversing in a 2D or 3D space, environment maps can be arranged 
to form a 2D or 3D lattice. Viewpoints in space are simply quantized to the nearest grid point to approximate 
the motion (figure 2). However, this approach requires a larger number of environment maps to be stored 
in order to obtain smooth motion. A more desirable approach may be the view interpolation method [18] 
or the approximate visibility method [12], which generates new views from a coarse grid of environment 
maps. Instead of constraining the movement to the grid points, the nearby environment maps are interpolated 
to generate a smooth path. Figure 2. An unconstrained camera path and an Figure 1. Reprojecting a cubic 
and a spherical environment map. 3.2 Object Rotation As mentioned earlier, orbiting the camera around 
an object, equivalent to rotating the object about its center, can not be accomplished simply with an 
environment map. One way of solving this problem is the navigable movie approach. The movie contains 
frames which correspond to all the allowable orientations of an object. For an object with full 360-degree 
rotation in one direction and 140 degrees in another direction at 10 degree increments, the movie requires 
504 frames. If we store the frames at 256 by 256 pixel resolution, each frame is around 10K bytes after 
compression. The entire movie consumes roughly 5 MB of storage space. This amount of space is large but 
not impractical given the current capacity of approximately 650 MB per CD-ROM. The view interpolation 
approach [18] needs to store only a approximated path along the grid lines. 3.4 Camera Zooming Changing 
the camera's field of view is equivalent to zooming in and out in the image space. However, using image 
magnification to zoom in does not provide more detail. Zooming out through image reduction may create 
aliasing artifacts as the sampling rate falls below the Nyquist limit. One solution is multiple resolution 
image zooming. A pyramidal or quadtree-like structure is created for each image to provide different 
levels of resolution. The proper level of resolution is selected on-the-fly based on the zooming factor. 
To achieve the best quality in continuous zooming, the two levels which bound the current zooming factor 
can be interpolated, similar to the use of mip-maps for anti-aliasing in texture mapping [21]. In order 
to avoid loading the entire high resolution image in memory while zooming in, the image can be segmented 
so that the memory requirement is independent of the zoom factor. As the zoom factor increases, a smaller 
percentage of a larger image is visible. Conversely, a larger percentage of a lower resolution image 
needs to be displayed. Therefore, the number of pixels required of the source image is roughly constant 
and is only related to the number of pixels displayed. One way of segmenting the image is dividing the 
multiple levels of image into tiles of the same size. The higher resolution images yield more tiles and 
vice versa. In this way, when the zooming factor changes, only a fixed number of tiles need to be visited. 
The different levels of resolution do not need to come from the same image. The detailed image could 
be from a different image to achieve effects like the "infinite zoom" [22], [23].  4. QUICKTIME VR The 
image-based approach has been implemented in a commercial product called QuickTime VR, built on top of 
Apple Computer's QuickTime digital multimedia framework. The current implementation includes continuous 
camera panning and zooming, jumping to selected points and object rotation using frame indexing. Currently, 
QuickTime VR uses cylindrical environment maps or panoramic images to accomplish camera rotation. The 
choice of a cylindrical map over other types is based on a number of factors. It is easier to capture 
a cylindrical panorama than other types of environment maps. One can use commercially available panoramic 
cameras which have a rotating vertical slit. We have also developed a tool which automatically stitches 
together a set of overlapping photographs (see 4.3.1.2) to create a seamless panorama. The cylindrical 
map only curves in one direction, which makes it efficient to perform image warping. QuickTime VR includes 
an interactive environment which uses a software-based real-time image processing engine for navigating 
in space and an authoring environment for creating VR movies. The interactive environment is implemented 
as an operating system component that can be accessed by any QuickTime 2.0 compliant application program. 
The interactive environment comprises two types of players. The panoramic movie player allows the user 
to pan, zoom and navigate in a scene. It also includes a hot spot picking capability. Hot spots are regions 
in an image that allow for user interaction. The object movie player allows the user to rotate an object 
or view the object from different viewing directions. The players run on most Macintosh® and Windows 
platforms. The panoramic authoring environment consists of a suite of tools to perform panoramic image 
stitching, hot spot marking, linking, dicing and compression. The object movies are created with a motion-controllable 
camera. The following sections briefly describe the movie format, the players and the process of making 
the movies. 4.1 The Movie Format QuickTime VR currently includes two different types of movies: panoramic 
and object. 4.1.1 The Panoramic Movie Conventional QuickTime movies are one-dimensional compressed sequences 
indexed by time. Each QuickTime movie may have multiple tracks. Each track can store a type of linear 
media, such as audio, video, text, etc. Each track type may have its own player to decode the information 
in the track. The tracks, which usually run parallel in time, are played synchronously with a common 
time scale. QuickTime allows new types of tracks and players to be added to extend its capabilities. 
Refer to [24] and [25] for a detailed description of the QuickTime architecture. Panoramic movies are 
multi-dimensional event-driven spatially-oriented movies. A panoramic movie permits a user to pan, zoom 
and move in a space interactively. In order to retrofit panoramic movies into the existing linear movie 
framework, a new panoramic track type was added. The panoramic track stores all the linking and additional 
information associated with a panoramic movie. The actual panoramic images are stored in a regular QuickTime 
video track to take advantage of the existing video processing capabilities. An example of a panoramic 
movie file is shown in figure 3. The panoramic track is divided into three nodes. Each node corresponds 
to a point in a space. A node contains information about itself and links to other nodes. The linking 
of the nodes form a directed graph, as shown in the figure. In this example, Node 2 is connected to Node 
1 and Node 3, which has a link to an external event. The external event allows custom actions to be attached 
to a node. External event Node Graph Node 1 External event Node 2 Node 3 Figure 3. A panoramic movie 
layout and its corresponding node graph. The nodes are stored in three tracks: one panoramic track and 
two video tracks. The panoramic track holds the graph information and pointers to the other two tracks. 
The first video track holds the panoramic images for the nodes. The second video track holds the hot 
spot images and is optional. The hot spots are used to identify regions of the panoramic image for activating 
appropriate links. All three tracks have the same length and the same time scale. The player uses the 
starting time value of each node to find the node's corresponding panoramic and hot spot images in the 
other two tracks. The hot spot track is similar to the hit test track in the Virtual Museum [3]. The 
hot spots are used to activate events or navigation. The hot spots image encodes the hot spot id numbers 
as colors. However, unlike the Virtual Museum where a hot spot needs to exist for every view of the same 
object, the hot spot image is stored in panoramic form and is thereby orientation-independent. The hot 
spot image goes through the same image warping process as the panoramic image. Therefore, the hot spots 
will stay with the objects they attach to no matter how the camera pans or zooms. The panoramic and the 
hot spot images are typically diced into smaller frames when stored in the video tracks for more efficient 
memory usage (see 4.2.1 for detail). The frames are usually compressed without inter-frame compression 
(e.g., frame differencing). Unlike linear video, the panoramic movie does not have an a priori order 
for accessing the frames. The image and hot spot video tracks are disabled so that a regular QuickTime 
movie would not attempt to display them as linear videos. Because the panoramic track is the only one 
enabled, the panoramic player is called upon to traverse the contents of the movie at playback time. 
 The track layout does not need to be the same as the physical layout of the data on a storage medium. 
Typically, the tracks should be interleaved when written to a slow medium, such as a CD-ROM, to minimize 
the seek time. 4.1.2 The Object Movie An object movie typically contains a two-dimensional array of 
frames. Each frame corresponds to a viewing direction. The movie has more than two dimensions if multiple 
frames are stored for each direction. The additional frames allow the object to have time-varying behavior 
(see 4.2.2). Currently, each direction is assumed to have the same number of frames. The object frames 
are stored in a regular video track. Additional information, such as the number of frames per direction 
and the numbers of rows and columns, is stored with the movie header. The frames are organized to minimize 
the seek time when rotating the object horizontally. As in the panoramic movies, there is no inter-frame 
compression for the frames since the order of rotation is not known in advance. However, inter-frame 
compression may be used for the multiple frames within each viewing direction.  4.2 The Interactive 
Environment The interactive environment currently consists of two types of players: the panoramic player 
and the object player. 4.2.1 The Panoramic Player The panoramic player allows the user to perform continuous 
panning in the vertical and the horizontal directions. Because the panoramic image has less than 180 
degrees vertical field-of­view, the player does not permit looking all the way up or down. Rotating about 
the viewing direction is not currently supported. The player performs continuous zooming through image 
magnification and reduction as mentioned previously. If multiple levels of resolution are available, 
the player may choose the right level based on the current memory usage, CPU performance, disk speed 
and other factors. Multiple level zooming is not currently implemented in QuickTime VR. Compressed Tiles 
 Figure 4. Panoramic display process. The panoramic player allows the user to control the view orientation 
and displays a perspectively correct view by warping a panoramic image. Figure 4 shows the panoramic 
display process. The panoramic images are usually compressed and stored on a hard disk or a CD-ROM. The 
compressed image needs to be decompressed to an offscreen buffer first. The offscreen buffer is generally 
smaller than the full panorama because only a fraction of the panorama is visible at any time. As mentioned 
previously, the panoramic image is diced into tiles. Only the tiles overlapping the current view orientation 
are decompressed to the offscreen buffer. The visible region on the offscreen buffer is then warped to 
display a correct perspective view. As long as the region moves inside the offscreen buffer, no additional 
decompression is necessary. To minimize the disk access, the most recent tiles may be cached in the main 
memory once they are read. The player also performs pre-paging to read in adjacent tiles while it is 
idle to minimize the delay in interactive panning. The image warp, which reprojects sections of the cylindrical 
image onto a planar view, is computed in real-time using a software-based two-pass algorithm [26]. An 
example of the warp is shown in figure 5, where the region enclosed by the yellow box in the panoramic 
image is warped to create a perspective view below. The performance of the player varies depending on 
many factors such as the platform, the color mode, the panning mode and the window sizes. The player 
is currently optimized for display in 16-bit color mode. Some performance figures for different processors 
are given below. These figures indicate the number of updates per second in a 640x400-pixel window in 
16-bit color mode. Because the warping is performed with a two-pass algorithm, panning in 1D is faster 
than full 2D panning. Note that the Windows version has a different implementation for writing to display 
which may affect the performance. Processor 1D Panning 2D Panning PowerPC601/80 29.5 11.6 MC68040/40 
12.3 5.4 Pentium/90 11.4 7.5 486/66 5.9 3.6 The player can perform image warping at different levels 
of quality. The lower quality settings perform less filtering and the images are more jagged but are 
faster. To achieve the best balance between quality and performance, the player automatically adjusts 
the quality level to maintain a constant update rate. When the user is panning, the player switches to 
lower quality to keep up with the user. When the user stops, the player updates the image in higher quality. 
Moving in space is currently accomplished by jumping to points where panoramic images are attached. In 
order to preserve continuity of motion, the view direction needs to be maintained when jumping to an 
adjacent location. The panoramas are linked together by matching their orientation manually in the authoring 
stage (see 4.3.1.4). Figure 6 shows a sequence of images generated from panoramas spaced 5 feet apart. 
The default user interface for navigation uses a combination of a 2D mouse and a keyboard. When the cursor 
moves over a window, its shape changes to reflect the permissible action at the current cursor location. 
The permissible actions include: continuous panning in 2D; continuous zooming in and out (controlled 
by a keyboard); moving to a different node; and activating a hot spot. Clicking on the mouse initiates 
the corresponding actions. Holding down and dragging the mouse performs continuous panning. The panning 
speed is controlled by the distance relative to the mouse click position. In addition to interactive 
control, navigation can be placed under the control of a script. A HyperCard® external command and a 
Windows DLL have been written to drive the player. Any application compatible with the external command 
or DLL can control the playback with a script. A C run-time library interface will be available for direct 
control from a program. 4.2.2 The Object Player While the panoramic player is designed to look around 
a space from the inside, the object player is used to view an object from the outside. The object player 
is based on the navigable movie approach. It uses a two-dimensional array of frames to accommodate object 
rotation. The object frames are created with a constant color background to facilitate compositing onto 
other backgrounds. The object player allows the user to grab the object using a mouse and rotate it with 
a virtual sphere-like interface [27]. The object can be rotated in two directions corresponding to orbiting 
the camera in the longitude and the latitude directions. If there is more than one frame stored for each 
direction, the multiple frames are looped continuously while the object is being rotated. The looping 
enables the object to have cyclic time varying behavior (e.g. a flickering candle or streaming waterfall). 
 4.3 The Authoring Environment The authoring environment includes tools to make panoramic movies and 
object movies. QuickTime VR Movies Figure 7. The panoramic movie authoring process. 4.3.1 Panoramic 
Movie Making A panoramic movie is created in five steps. First, nodes are selected in a space to generate 
panoramas. Second, the panoramas are created with computer rendering, panoramic photography or stitching 
a mosaic of overlapping photographs. Third, if there are any hot spots on the panorama, a hot spot image 
is constructed by marking regions of the panorama with pseudo colors corresponding to the hot spot identifiers. 
Alternatively, the hot spots can be generated with computer rendering [28], [3]. Fourth, if more than 
one panoramic node is needed, the panoramas are linked together by manually registering their viewing 
directions. Finally, the panoramic images and the hot spot images are diced and compressed to create 
a panoramic movie. The authoring process is illustrated in figure 7. 4.3.1.1 Node Selection The nodes 
should be selected to maintain visual consistency when moving from one to another. The distance between 
two adjacent nodes is related to the size of the virtual environment and the distance to the nearby objects. 
Empirically we have found that a 5-10 foot spacing to be adequate with most interior spaces. The spacing 
can be significantly increased with exterior scenes. 4.3.1.2 Stitching The purpose of stitching is to 
create a seamless panoramic image from a set of overlapping pictures. The pictures are taken with a camera 
as it rotates about its vertical axis in one direction only. The camera pans at roughly equal, but not 
exact, increments. The camera is mounted on a tripod and centered at its nodal point with minimal tilting 
and rolling. The camera is usually mounted sideways to obtain the maximum vertical field­of-view. The 
setup of the camera is illustrated in figure 8. The scene is assumed to be static although some distant 
object motion may be acceptable. Nodal point Camera mountedRotation sideways Leveling Figure 8. Camera 
setup for taking overlapping pictures. The stitcher uses a correlation-based image registration algorithm 
to match and blend adjacent pictures. The adjacent pictures need to have some overlap for the stitcher 
to work properly. The amount of overlap may vary depending on the image features in the overlapping regions. 
In practice, a 50% overlap seems to work best because the adjacent pictures may have very different brightness 
levels. Having a large overlap allows the stitcher to more easily smooth out the intensity variation. 
The success rate of the automatic stitching depends on the input pictures. For a typical stitching session, 
about 8 out of 10 panoramas can be stitched automatically, assuming each panorama is made from 12 pictures. 
The remaining 2 panoramas requires some manual intervention. The factors which contribute to automatic 
stitching failure include, but are not limited to, missing pictures, extreme intensity change, insufficient 
image features, improper camera mounting, significant object motion and film scanning errors. In addition 
to being able to use a regular 35 mm camera, the ability to use multiple pictures, and hence different 
exposure settings, to compose a panorama has another advantage. It enables one to capture a scene with 
a very wide intensity range, such as during a sunset. A normal panoramic camera captures the entire 360 
degrees with a constant exposure setting. Since film usually has a narrower dynamic range than the real 
world does, the resultant panorama may have areas under or over exposed. The stitcher allows the exposure 
setting to be specifically tailored for each direction. Therefore, it may create a more balanced panorama 
in extreme lighting conditions. Although one can use other devices, such as video or digital cameras 
for capturing, using still film results in high resolution images even when displayed at full screen 
on a monitor. The film can be digitized and stored on Kodak's PhotoCD. Each PhotoCD contains around 100 
pictures with 5 resolutions each. A typical panorama is stitched with the middle resolution pictures 
(i.e., 768 x 512 pixels) and the resulting panorama is around 2500 x 768 pixels for pictures taken with 
a 15 mm lens. This resolution is enough for a full screen display with a moderate zoom angle. The stitcher 
takes around 5 minutes to automatically stitch a 12-picture panorama on a PowerPC 601/80 MHz processor, 
including reading the pictures from the PhotoCD and some post processing. An example of a panoramic image 
stitched automatically is shown in figure 9. 4.3.1.3 Hot Spot Marking Hot spots identify regions of 
a panoramic image for interactions, such as navigation or activating actions. Currently, the hot spots 
are stored in 8-bit images, which limit the number of unique hot spots to 256 per image. One way of creating 
a hot spot image is by painting pseudo colors over the top of a panoramic image. Computer renderers may 
generate the hot spot image directly. The hot spot image does not need to have the same resolution as 
the panoramic image. The resolution of the hot spot image is related to the precision of picking. A very 
low resolution hot spot image may be used if high accuracy of picking is not required. 4.3.1.4 Linking 
The linking process connects and registers view orientation between adjacent panoramic nodes. The links 
are directional and each node may have any number of links. Each link may be attached to a hot spot so 
that the user may activate the link by clicking on the hot spot. Currently, the linking is performed 
by manually registering the source and destination view orientations using a graphical linker. The main 
goal of the registration is to maintain visual consistency when moving from one node to another. 4.3.1.5 
Dicing and Compression The panoramic and hot spot images are diced before being compressed and stored 
in a movie. The tile size should be optimized for both data loading and offscreen buffer size. A large 
number of tiles increases the overhead associated with loading and decompressing the tiles. A small number 
of tiles requires a large offscreen buffer and reduces title paging efficiency. We have found that dicing 
a panoramic image of 2500x768 pixels into 24 vertical stripes provides an optimal balance between data 
loading and tile paging. Dicing the panorama into vertical stripes also minimizes the seek time involved 
when loading the tiles from a CD-ROM during panning. A panorama of the above resolution can be compressed 
to around 500 KB with a modest 10 to 1 compression ratio using the Cinepak compressor, which is based 
on vector quantization and provides a good quality vs. speed balance. Other compressors may be used as 
well for different quality and speed tradeoffs. The small disk footprint for each panorama means that 
a CD-ROM with over 600 MB capacity can hold more than 1,000 panoramas. The capacity will only increase 
as higher density CD-ROMs and better compression methods become available. The hot spot image is compressed 
with a lossless 8-bit compressor. The lossless compression is necessary to ensure the correctness of 
the hot spot id numbers. Since the hot spots usually occupy large contiguous regions, the compressed 
size is typically only a few kilo-bytes per image.  4.3.2 Object Movie Making Making an object movie 
requires photographing the object from different viewing directions. To provide a smooth object rotation, 
the camera needs to point at the object's center while orbiting around it at constant increments. While 
this requirement can be easily met in computer generated objects, photographing a physical object in 
this way is very challenging unless a special device is built. Currently, we use a device, called the 
"object maker," to accomplish this task. The object maker uses a computer to control two stepper motors. 
The computer-controlled motors orbit a video camera in two directions by fixing its view direction at 
the center of the object. The video camera is connected to a frame digitizer inside the computer, which 
synchronizes frame grabbing with camera rotation. The object is supported by a nearly invisible base 
and surrounded by a black curtain to provide a uniform background. The camera can rotate close to 180 
degrees vertically and 360 degrees horizontally. The camera typically moves at 10-degree increments in 
each direction. The entire process may run automatically and takes around 1 hour to capture an object 
completely. If multiple frames are needed for each direction, the object may be captured in several passes, 
with each pass capturing a full rotation of the object in a fixed state. The multi-pass capture requires 
that the camera rotation be repeatable and the object motion be controllable. In the case of candle light 
flickering, the multiple frames may need to be captured successively before the camera moves on to the 
next direction.  5. APPLICATIONS The panoramic viewing technology can be applied to applications which 
require the exploration of real or imaginary scenes. Some example applications include virtual travel, 
real estate property inspection, architecture visualizations, virtual museums, virtual shopping and virtual 
reality games. An example of panoramic movie application is the commercial CD-ROM title: Star Trek/The 
Next Generation® Interactive Technical Manual. This title lets the user navigate in the Starship Enterprise 
using panoramic movies. Several thousand still photographs were shot to create more than two hundred 
panoramic images, which cover most areas in the starship. In addition, many object movies were created 
from the props in the set. The object movie can be applied to visualize a scientific or engineering simulation. 
Most simulations require lengthy computations on sophisticated computers. The simulation results can 
be computed for all the possible view orientations and stored as an object movie which can be inspected 
by anyone with a personal computer. Time-varying environment maps may be used to include motions in a 
scene. An example of time-varying environment maps has been generated using time-lapse photography. A 
camera was fixed at the same location and took a panoramic picture every 30 minutes during a whole day. 
The resulting movie shows the time passage while the user is freely looking around. Another use of the 
orientation-independent movie is in interactive TV. A movie can be broadcast in a 360-degree format, 
perhaps using multiple channels. Each TV viewer can freely control the camera angle locally while watching 
the movie. A similar idea called electronic panning camera has been demonstrated for video conferencing 
applications [29]. Although most applications generated with the image-based approach are likely to be 
CD-ROM based in the near future because of CD-ROM's large storage capacity, the variable­resolution files 
make the approach practical for network transmission. A low-resolution panoramic movie takes up less 
than 100 KB per node and provides 360-degree panning in a 320x240-pixel window with reasonable quality. 
As network speeds improve and better compression technologies become available, on-line navigation of 
panoramic spaces may become more common in the near future. One can use the same spatial navigation metaphor 
to browse an informational space. The ability to attach information to some spatial representations may 
make it easier to become familiar with an intricate information space. 6. CONCLUSIONS AND FUTURE DIRECTIONS 
The image-based method makes use of environment maps, in particular cylindrical panoramic images, to 
compose a scene. The environment maps are orientation-independent images, which allow the user to look 
around in arbitrary view directions through the use of real-time image processing. Multiple environment 
maps can be linked together to define a scene. The user may move in the scene by jumping through the 
maps. The method may be extended to include motions with time-varying environment maps. In addition, 
the method makes use of a two-dimensional array of frames to view an object from different directions. 
The image-based method also provides a solution to the levels of detail problem in most 3D virtual reality 
display systems. Ideally, an object should be displayed in less detail when it is farther away and in 
more detail when it is close to the observer. However, automatically changing the level of detail is 
very difficult for most polygon based objects. In practice, the same object is usually modeled at different 
detail levels and the appropriate one is chosen for display based on some viewing criteria and system 
performance [30], [31]. This approach is costly as multiple versions of the objects need to be created 
and stored. Since one can not predict how an object will be displayed in advance, it is difficult to 
store enough levels to include all possible viewing conditions. The image-based method automatically 
provides the appropriate level of detail. The images are views of a scene from a range of locations. 
As the viewpoint moves from one location to another within the range, the image associated with the new 
location is retrieved. In this way, the scene is always displayed at the appropriate level of detail. 
This method is the underlying technology for QuickTime VR, a system for creating and interacting with 
virtual environments. The system meets most of the objectives that we described in the introduction. 
The playback environment supports most computers and does not require special hardware. It uses images 
as a common representation and can therefore accommodate both real and imaginary scenes. The display 
speed is independent of scene complexity and rendering quality. The making of the Star Trek title in 
a rather short time frame (less than 2 months for generating all the panoramic movies of the Enterprise) 
has demonstrated the system's relative ease in creating a complex environment. The method s chief limitations 
are the requirements that the scene be static and the movement be confined to particular points. The 
first limitation may be eased somewhat with the use of time-varying environment maps. The environment 
maps may have motions in some local regions, such as opening a door. The motion may be triggered by an 
event or continuously looping. Because the motions are mostly confined to some local regions, the motion 
frames can be compressed efficiently with inter-frame compression. Another solution to the static environment 
constraint is the combination of image warping and 3D rendering. Since most backgrounds are static, they 
can be generated efficiently from environment maps. The objects which are time-varying or event driven 
can be rendered on-the-fly using 3D rendering. The rendered objects are composited onto the map-generated 
background in real-time using layering, alpha masking or z­buffering. Usually, the number of interactive 
objects which need to be rendered in real-time is small, therefore, even a software based 3D renderer 
may be enough for the task. Being able to move freely in a photographic scene is more difficult. For 
computer rendered scenes, the view interpolation method may be a solution. The method requires depth 
and camera information for automatic image registration. This information is not easily obtainable from 
photographic scenes. Another constraint with the current panoramic player is its limitation in looking 
straight up or down due to the use of cylindrical panoramic images. This limitation can be removed if 
other types of environment maps, such as cubic or spherical maps, are used. However, capturing a cubic 
or a spherical map photographically may be more difficult than a cylindrical one. The current player 
does not require any additional input and output devices other than those commonly available on personal 
computers. However, input devices with more than two degrees of freedom may be useful since the navigation 
is more than two-dimensional. Similarly, immersive stereo displays combined with 3D sounds may enhance 
the experience of navigation. One of the ultimate goals of virtual reality will be achieved when one 
can not discern what is real from what is virtual. With the ability to use photographs of real scenes 
for virtual navigation, we may be one step closer. 7. ACKNOWLEDGMENTS The author is grateful to the 
entire QuickTime VR team for their tremendous efforts on which this paper is based. Specifically, the 
author would like to acknowledge the following individuals: Eric Zarakov, for his managerial support 
and making QuickTime VR a reality; Ian Small, for his contributions to the engineering of the QuickTime 
VR product; Ken Doyle, for his QuickTime integration work; Michael Chen, for his work on user interface, 
the object maker and the object player; Ken Turkowski, for code optimization and PowerPC porting help; 
Richard Mander, for user interface design and study; and Ted Casey, for content and production support. 
The assistance from the QuickTime team, especially Jim Nitchal s help on code optimization, is also appreciated. 
Dan O'Sullivan and Mitch Yawitz's early work on navigable movies contributed to the development of the 
object movie. Most of the work reported on in this paper began in the Computer Graphics program of the 
Advanced Technology Group at Apple Computer, Inc. The panoramic player was inspired by work from Gavin 
Miller. Ned Greene and Lance Williams contributed ideas related to environment mapping and view interpolation. 
Frank Crow's encouragement and support throughout were critical in keeping the research going. The author's 
interns, Lili Cheng, Chase Garfinkle and Patrick Teo, helped in shaping up the project into its current 
state. The images in figure 6 are extracted from the "Apple Company Store in QuickTime VR" CD. The Great 
Wall photographs in figure 9 were taken with the assistance of Helen Tahn, Zen Jing and Professor En-Hua 
Wu. Thanks go to Vicki de Mey for proofreading the paper.   REFERENCES [1] Lippman, A. Movie Maps: 
An Application of the Optical Videodisc to Computer Graphics. Computer Graphics(Proc. SIGGRAPH 80), 32-43. 
[2] Ripley, D. G. DVI a Digital Multimedia Technology. Communications of the ACM. 32(7):811-822. 1989. 
[3] Miller, G., E. Hoffert, S. E. Chen, E. Patterson, D. Blackketter, S. Rubin, S. A. Applin, D. Yim, 
J. Hanan. The Virtual Museum: Interactive 3D Navigation of a Multimedia Database. The Journal of Visualization 
and Computer Animation, (3): 183-197, 1992. [4] Mohl, R. Cognitive Sp ace in the Interactive Movie Map: 
an Investigation of Spatial Learning in the Virtual Environments. MIT Doctoral Thesis, 1981. [5] Apple 
Computer, Inc. QuickTime, Version 1.5 for Developers CD. 1992. [6] Blinn, J. F. and M. E. Newell. Texture 
and Reflection in Computer Generated Images. Communications of the ACM, 19(10):542-547. October 1976. 
[7] Hall, R. Hybrid Techniques for Rapid Image Synthesis. in Whitted, T. and R. Cook, eds. Image Rendering 
Tricks, Course Notes 16 for SIGGRAPH 86. August 1986. [8] Greene, N. Environment Mapping and Other Applications 
of World Projections. Computer Graphics and Applications, 6(11):21-29. November 1986. [9] Yelick, S. 
Anamorphic Image Processing. B.S. Thesis. Department of Electrical Engineering and Computer Science. 
May, 1980. [10] Hodges, M and R. Sasnett. Multimedia Computing Case Studies from MIT Project Athena. 
89-102. Addison-Wesley. 1993. [11] Miller, G. and S. E. Chen. Real-Time Display of Surroundings using 
Environment Maps. Technical Report No. 44, 1993, Apple Computer, Inc. [12] Greene, N and M. Kass. Approximating 
Visibility with Environment Maps. Technical Report No. 41. Apple Computer, Inc. [13] Regan, M. and R. 
Pose. Priority Rendering with a Virtual Reality Address Recalculation Pipeline. Computer Graphics (Proc. 
SIGGRAPH 94), 155-162. [14] Greene, N. Creating Raster Ominmax Images from Multiple Perspective Views 
using the Elliptical Weighted Average Filter. IEEE Computer Graphics and Applications. 6(6):21-27, June, 
1986. [15] Irani, M. and S. Peleg. Improving Resolution by Image Registration. Graphical Models and Image 
Processing. (3), May, 1991. [16] Szeliski, R. Image Mosaicing for Tele-Reality Applications. DEC Cambridge 
Research Lab Technical Report, CRL 94/2. May, 1994. [17] Mann, S. and R. W. Picard. Virtual Bellows: 
Constructing High Quality Stills from Video. Proceedings of ICIP-94. 363­ 367. November, 1994. [18] Chen, 
S. E. and L. Williams. View Interpolation for Image Synthesis. Computer Graphics(Proc. SIGGRAPH 93), 
279-288. [19] Cheng, N. L. View Reconstruction form Uncalibrated Cameras for Three-Dimensional Scenes. 
Master's Thesis, Department of Electrical Engineering and Computer Sciences, U. C. Berkeley. 1995. 
 [20] Laveau, S. and O. Faugeras. 3-D Scene Representation as a Collection of Images and Fundamental 
Matrices. INRIA, Technical Report No. 2205, February, 1994. [21] Williams, L. Pyramidal Parametrics. 
Computer Graphics(Proc. SIGGRAPH 83), 1-11. [22] Berman, D. R., J. T. Bartell and D. H. Salesin. Multiresolution 
Painting and Compositing. Computer Graphics (Proc. SIGGRAPH 94), 85-90. [23] Perlin, K. and D. Fox. Pad: 
An Alternative Approach to the Computer Interface. Computer Graphics (Proc. SIGGRAPH 93), 57-72. [24] 
Hoffert, E., L. Mighdoll, M. Kreuger, M. Mills, J. Cohen, et al. QuickTime: an Extensible Standard for 
Digital Multimedia. Proceedings of the IEEE Computer Conference (CompCon 92), February 1992. [25] Apple 
Computer, Inc. Inside Macintosh: QuickTime. Addison-Wesley. 1993. [26] Chen, S. E. and G. S. P. Miller. 
Cylindrical to planar image mapping using scanline coherence. United States Patent number 5,396,583. 
Mar. 7, 1995. [27] Chen, M. A Study in Interactive 3-D Rotation Using 2-D Control Devices. Computer Graphics 
(Proc. SIGGRAPH 88), 121-130. [28] Weghorst, H., G. Hooper and D. Greenberg. Improved Computational Methods 
for Ray Tracing. ACM Transactions on Graphics. 3(1):52-69. 1986. [29] 'Electronic Panning' Device Opens 
Viewing Range. Digital Media: A Seybold Report. 2(3):13-14. August, 1992. [30] Clark, J. H. Hierarchical 
Geometric Models for Visible Surface Algorithms. Communications of the ACM, (19)10:547­ 554. October, 
1976 [31] Funkhouser, T. A. and C. H. Séquin. Adaptive Display Algorithm for Interactive Frame Rates 
During Visualization of Complex Virtual Environments. Computer Graphics(Proc. SIGGRAPH 93), 247-254. 
  Figure 5. A perspective view created from warping a region enclosed by the yellow box in the panoramic 
image. Figure 9. A stitched panoramic image and some of the photographs the image stitched from.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218398</article_id>
		<sort_key>39</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Plenoptic modeling]]></title>
		<subtitle><![CDATA[an image-based rendering system]]></subtitle>
		<page_from>39</page_from>
		<page_to>46</page_to>
		<doi_number>10.1145/218380.218398</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218398</url>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Registration</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Projections</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Hidden line/surface removal</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010068</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Random projections and metric embeddings</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14058906</person_id>
				<author_profile_id><![CDATA[81100137780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leonard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McMillan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB 3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14081142</person_id>
				<author_profile_id><![CDATA[81100206034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bishop]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill, CB 3175 Sitterson Hall, Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adelson, E. H., and J. R. Bergen, "The Plenoptic Function and the Elements of Early Vision," Computational Models of Visual Proeessing, Chapter 1, Edited by Michael Landy and J. Anthony Movshon. The MIT Press, Cambridge, Mass. 1991.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357313</ref_obj_id>
				<ref_obj_pid>357311</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Anderson, D., "Hidden Line Elimination in Projected Grid Surfaces," ACM Transactions on Graphics, October 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Barnard, S.T. "A Stochastic Approach to Stereo Vision," SRI International, Technical Note 373, April 4, 1986.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Beier, T. and S. Neely, "Feature-Based Image Metamorphosis," Computer Graphics (SIGGRAPH'92 Proceedings), Vol. 26, No. 2, pp. 35-42, July 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. and M. E. Newell, "Texture and Reflection in Computer Generatedlmages," Communications oftheACM, vol. 19, no. 10, pp. 542-547, October 1976.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bolles, R. C., H. H. Baker, and D. H. Marimont, "Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion," International Journal of Computer Vision, Vol. 1, 1987.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., "A Subdivision Algorithm for Computer Display of Curved Surfaces" (Ph. D. Thesis), Department of Computer Science, University of Utah, Tech. Report UTEC-CSc-74-133, December 1974.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E. and L. Williams. "View Interpolation for Image Synthesis," Computer Graphics (SIGGRAPH'93 Proceedings), pp. 279-288, July 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171658</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Faugeras, O., Three-dimensional Computer Vision: A Geometric Viewpoint, The MIT Press, Cambridge, Massachusetts, 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13023</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Greene, N., "Environment Mapping and Other Applications of World Projections," IEEE Computer Graphics and Applications, November 1986.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>189583</ref_obj_id>
				<ref_obj_pid>189359</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Hartley, R.I., "Self-Calibration from Multiple Views with a Rotating Camera," Proceedings of the European Conference on Computer Vision, May 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>893978</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. S., "Fundamentals of Texture Mapping and Image Warping," Masters Thesis, Dept. of EECS, UCB, Technical Report No. UCB/CSD 89/516, June 1989.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Horn, B., and B.G. Schunck, "Determining Optical Flow," Artificial Intelligence, Vol. 17, 1981.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>51482</ref_obj_id>
				<ref_obj_pid>51481</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Kanatani, K., "Transformation of Optical Flow by Camera Rotation," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 10, No. 2, March 1988.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Laveau, S. and O. Faugeras, "3-D Scene Representation as a Collection of Images and Fundamental Matrices," INRIA, Technical Report No. 2205, February, 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lenz, R. K. and R. Y. Tsai, "Techniques for Calibration ofthe Scale Factor and Image Center for High Accuracy 3D Machine Vision Metrology," Proceedings of IEEE International Conference on Robotics and Automation, March 31 - April 3, 1987.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807465</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Lippman, A., "Movie-Maps: An Application of the Optical Videodisc to Computer Graphics," SIGGRAPH '80 Proceedings, 1980.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Longuet-Higgins, H. C., "A Computer Algorithm for Reconstructing a Scene from Two Projections," Nature, Vol. 293, September 1981.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Longuet-Higgins, H. C., "The Reconstruction of a Scene From Two Projections - Configurations That Defeat the 8-Point Algorithm," Proceedings of the First IEEE Conference on Artificial Intelligence Applications, Dec 1984.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Lucas, B., and T. Kanade, "An Iterative Image Registration Technique with an Application to Stereo Vision," Proceedings of the Seventh International Joint Conference on Artificial Intelligence, Vancouver, 1981.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897810</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[McMillan, Leonard, "A List-Priority Rendering Algorithm for Redisplaying Projected Surfaces," Department of Computer Science, UNC, Technical Report TR95-005, 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Mann, S. and R. W. Picard, "Virtual Bellows: Constructing High Quality Stills from Video," Proceedings of the First IEEE International Conference on Image Processing, November 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Press, W. H., B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes in C, Cambridge University Press, Cambridge, Massachusetts, pp. 309-317, 1988.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192192</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Regan, M., and R. Pose, "Priority Rendering with a Virtual Reality Address Recalculation Pipeline," SIGGRAPH'94 Proceedings, 1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Szeliski, R., "Image Mosaicing for Tele-Reality Applications," DEC and Cambridge Research Lab Technical Report, CRL 94/ 2, May 1994.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>865071</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Tomasi, C., and T. Kanade, "Shape and Motion from Image Streams: a Factorization Method; Full Report on the Orthographic Case," Technical Report, CMU-CS-92-104, Carnegie Mellon University, March 1992.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Tsai, R. Y., "A Versatile Camera Calibration Technique for High- Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses," IEEE Journal of Robotics and Automation, Vol. RA-3, No. 4, August 1987.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97919</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Westover, L. A., "Footprint Evaluation for Volume Rendering," SIGGRAPH'90 Proceedings, August 1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Wolberg, G., Digital Image Warping, IEEE Computer Society Press, Los Alamitos, CA, 1990.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Plenoptic Modeling: An Image-Based Rendering System Leonard McMillan and Gary Bishop Department of 
Computer Science University of North Carolina at Chapel Hill ABSTRACT Image-based rendering is a powerful 
new approach for generating real-time photorealistic computer graphics. It can provide convinc­ing animations 
without an explicit geometric representation. We use the plenoptic function of Adelson and Bergen to 
provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. 
The plenoptic function is a param­eterized function for describing everything that is visible from a 
given point in space. We present an image-based rendering system based on sampling, reconstructing, and 
resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a 
geometric invariant for cylindrical projections that is equiva­lent to the epipolar constraint de.ned 
for planar projections. CR Descriptors: I.3.3 [Computer Graphics]: Picture/Image Gen­eration display 
algorithms, viewing algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism hidden 
line/ surface removal; I.4.3 [Image Processing]: Enhancement regis­tration; I.4.7 [Image Processing]: 
Feature Measurement projections; I.4.8 [Image Processing]: Scene Analysis. 1. INTRODUCTION In recent 
years there has been increased interest, within the computer graphics community, in image-based rendering 
systems. These sys­tems are fundamentally different from traditional geometry-based rendering systems. 
In image-based systems the underlying data rep­resentation (i.e model) is composed of a set of photometric 
observations, whereas geometry-based systems use either mathe­matical descriptions of the boundary regions 
separating scene elements (B-rep) or discretely sampled space functions (volumetric). The evolution of 
image-based rendering systems can be traced through at least three different research .elds. In photogrammetry 
the initial problems of camera calibration, two-dimensional image reg­istration, and photometrics have 
progressed toward the determina­tion of three-dimensional models. Likewise, in computer vision, problems 
such as robot navigation, image discrimination, and image understanding have naturally led in the same 
direction. In computer graphics, the progression toward image-based rendering systems CB 3175 Sitterson 
Hall, Chapel Hill, NC 27599 (919) 962-1797 mcmillan@cs.unc.edu http://www.cs.unc.edu/~mcmillan (919) 
962-1886 gb@cs.unc.edu http://www.cs.unc.edu/~gb Permission to make digital/hard copy of part or all 
of this work for personal or classroom use is granted without fee provided was initially motivated by 
the desire to increase the visual realism of the approximate geometric descriptions by mapping images 
onto their surface (texture mapping) [7], [12]. Next, images were used to approximate global illumination 
effects (environment mapping) [5], and, most recently, we have seen systems where the images them­selves 
constitute the signi.cant aspects of the scene s description [8]. Another reason for considering image-based 
rendering systems in computer graphics is that acquisition of realistic surface models is a dif.cult 
problem. While geometry-based rendering technology has made signi.cant strides towards achieving photorealism, 
creating accurate models is still nearly as dif.cult as it was ten years ago. Tech­nological advances 
in three-dimensional scanning provide some promise in model building. However, they also verify our worst 
sus­picions the geometry of the real-world is exceedingly complex. Ironically, the primary subjective 
measure of image quality used by proponents of geometric rendering systems is the degree with which the 
resulting images are indistinguishable from photographs. One liability of image-based rendering systems 
is the lack of a consistent framework within which to judge the validity of the results. Fundamentally, 
this arises from the absence of a clear prob­lem de.nition. Geometry-based rendering, on the other hand, 
has a solid foundation; it uses analytic and projective geometry to describe the world s shape and physics 
to describe the world s surface prop­erties and the light s interaction with those surfaces. This paper 
presents a consistent framework for the evaluation of image-based rendering systems, and gives a concise 
problem def­inition. We then evaluate previous image-based rendering methods within this new framework. 
Finally, we present our own image-based rendering methodology and results from our prototype implementa­tion. 
 2. THE PLENOPTIC FUNCTION Adelson and Bergen [1] assigned the name plenoptic function (from the latin 
root plenus, meaning complete or full, and optic pertaining to vision) to the pencil of rays visible 
from any point in space, at any time, and over any range of wavelengths. They used this function to develop 
a taxonomy for evaluating models of low-level vision. The plenoptic function describes all of the radiant 
energy that can be per­ceived from the point of view of the observer rather than the point of view of 
the source. They postulate all the basic visual measurements can be considered to characterize local 
change along one or two dimensions of a single function that describes the structure of the information 
in the light impinging on an observer. that copies are not made or distributed for profit or commercial 
advantage, the copyright notice, the title of the publication and its date appear, and notice is given 
that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to 
redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 Adelson and Bergen further formalized this functional description by providing a parameter space 
over which the plenoptic function is valid, as shown in Figure 1. Imagine an idealized eye which we are 
free to place at any point in space(Vx, Vy, Vz). From there we can select any of the viewable rays by 
choosing an azimuth and elevation angle (.,f) as well as a band of wavelengths, ., which we wish to consider. 
 FIGURE 1. The plenoptic function describes all of the image information visible from a particular viewing 
 position. In the case of a dynamic scene, we can additionally choose the time, t, at which we wish to 
evaluate the function. This results in the fol­lowing form for the plenoptic function: p = P .f.V,V,V,t) 
(1) ( ,,, xyz In computer graphics terminology, the plenoptic function describes the set of all possible 
environment maps for a given scene. For the purposes of visualization, one can consider the plenoptic 
function as a scene representation. In order to generate a view from a given point in a particular direction 
we would need to merely plug in appropriate values for (Vx, Vy, Vz) and select from a range of (.,f) 
for some constant t. We de.ne a complete sample of the plenoptic function as a full spherical map for 
a given viewpoint and time value, and an incom­plete sample as some solid angle subset of this spherical 
map. Within this framework we can state the following problem def­inition for image-based rendering. 
Given a set of discrete samples (complete or incomplete) from the plenoptic function, the goal of image-based 
rendering is to generate a continuous representation of that function. This problem statement provides 
for many avenues of exploration, such as how to optimally select sample points and how to best reconstruct 
a continuous function from these samples.  3. PREVIOUS WORK 3.1 Movie-Maps The Movie-Map system by 
Lippman [17] is one of the earliest attempts at constructing an image-based rendering system. In Movie-Maps, 
incomplete plenoptic samples are stored on interactive video laser disks. They are accessed randomly, 
primarily by a change in viewpoint; however, the system can also accommodate panning, tilt­ing, or zooming 
about a .xed viewing position. We can characterize Lippman s plenoptic reconstruction technique as a 
nearest-neighbor interpolation because, when given a set of input parameters (Vx, Vy, Vz, ., f, t), the 
Movie-Map system can select the nearest partial sam­ple. The Movie-Map form of image-based rendering 
can also be interpreted as a table-based evaluation of the plenoptic function. This interpretation re.ects 
the database structure common to most image­based systems. 3.2 Image Morphing Image morphing is a very 
popular image-based rendering technique [4], [28]. Generally, morphing is considered to occur between 
two images. We can think of these images as endpoints along some path through time and/or space. In this 
interpretation, morphing becomes a method for reconstructing partial samples of the continuous ple­noptic 
function along this path. In addition to photometric data, morphing uses additional information describing 
the image .ow .eld. This information is usually hand crafted by an animator. At .rst glance, this type 
of augmentation might seem to place it outside of the plenoptic function s domain. However, several authors 
in the .eld of computer vision have shown that this type of image .ow infor­mation is equivalent to changes 
in the local intensity due to in.nitesimal perturbations of the plenoptic function s independent variables 
[20], [13]. This local derivative behavior can be related to the intensity gradient via applications 
of the chain rule. In fact, mor­phing makes an even stronger assumption that the .ow information is constant 
along the entire path, thus amounting to a locally linear approximation. Also, a blending function is 
often used to combine both reference images after being partially .owed from their initial con.gurations 
to a given point on the path. This blending function is usually some linear combination of the two images 
based on what percentage of the path s length has been traversed. Thus, morphing is a plenoptic reconstruction 
method which interpolates between samples and uses local derivative information to construct approxi­mations. 
 3.3 View Interpolation Chen s and Williams [8] view interpolation employs incomplete plenoptic samples 
and image .ow .elds to reconstruct arbitrary viewpoints with some constraints on gaze angle. The reconstruction 
process uses information about the local neighborhood of a sample. Chen and Williams point out and suggest 
a solution for one of the key problems of image-based rendering determining the visible sur­faces. Chen 
and Williams chose to presort the quadtree compressed .ow-.eld in a back-to-front order according to 
its (geometric) z­value. This approach works well when all of the partial sample images share a common 
gaze direction, and the synthesized view­points are restricted to stay within 90 degrees of this gaze 
angle. An image .ow .eld alone allows for many ambiguous visibility solutions, unless we restrict ourselves 
to .ow .elds that do not fold, such as rubber-sheet local spline warps or thin-plate global spline warps. 
This problem must be considered in any general-purpose image-based rendering system, and ideally, it 
should be done without transporting the image into the geometric-rendering domain. Establishing .ow .elds 
for a view interpolation system can also be problematic. Chen and Williams used pre-rendered synthetic 
images to determine .ow .elds from the z-values. In general, accu­rate .ow .eld information between two 
samples can only be estab­lished for points that are mutually visible to both samples. This points out 
a shortcoming in the use of partial samples, because reference images seldom have a 100% overlap. Like 
morphing, view interpolation uses photometric informa­tion as well as local derivative information in 
its reconstruction pro­cess. This locally linear approximation is nicely exploited to approximate perspective 
depth effects, and Chen and Williams show it to be correct for lateral motions relative to the gaze direction. 
View interpolation, however, adds a nonlinearity by allowing the visibility process to determine the 
blending function between reference frames in a closest-take-all (a.k.a. winner-take-all) fashion.  
3.4 Laveau and Faugeras Laveau and Faugeras [15] have taken advantage of the fact that the epipolar geometries 
between images restrict the image .ow .eld in such a way that it can be parameterized by a single disparity 
value and a fundamental matrix which represents the epipolar relationship. They also provide a two-dimensional 
raytracing-like solution to the visibility problem which does not require an underlying geometric description. 
Their method does, however, require establishing cor­respondences for each image point along the ray 
s path. The Laveau and Faugeras system also uses partial plenoptic samples, and results are shown only 
for overlapping regions between views. Laveau and Faugeras also discuss the combination of informa­tion 
from several views but primarily in terms of resolving visibility. By relating the reference views and 
the desired views by the homog­enous transformations between their projections, Laveau and Faugeras can 
compute exact perspective depth solutions. The recon­struction process again takes advantage of both 
image data and local derivative information to reconstruct the plenoptic function.  3.5 Regan and Pose 
Regan and Pose [23] describe a hybrid system in which plenoptic samples are generated on the .y by a 
geometry-based rendering sys­tem at available rendering rates, while interactive rendering is provided 
by the image-based subsystem. At any instant, a user inter­acts with a single plenoptic sample. This 
allows the user to make unconstrained changes in the gaze angle about the sample point. Regan and Pose 
also discuss local reconstruction approximations due to changes in the viewing position. These approximations 
amount to treating the objects in the scene as being placed at in.nity, resulting in a loss of the kinetic 
depth effect. These partial updates can be com­bined with the approximation values.  4. PLENOPTIC MODELING 
We claim that all image-based rendering approaches can be cast as attempts to reconstruct the plenoptic 
function from a sample set of that function. We believe that there are signi.cant insights to be gleaned 
from this characterization. In this section, we propose our prototype system in light of this plenoptic 
function framework. We call our image-based rendering approach Plenoptic Model­ing. Like other image-based 
rendering systems, the scene description is given by a series of reference images. These reference images 
are subsequently warped and combined to form representations of the scene from arbitrary viewpoints. 
The warping function is de.ned by image .ow .eld information that can either be supplied as an input 
or derived from the reference images. Our discussion of the plenoptic modeling image-based render­ing 
system is broken down into four sections. First, we discuss the representation of the plenoptic samples. 
Next, we discuss their acqui­sition. The third section covers the determination of image .ow .elds, if 
required. And, .nally, we describe how to reconstruct the plenoptic function from these sample images. 
4.1 Plenoptic Sample Representation The most natural surface for projecting a complete plenoptic sample 
is a unit sphere centered about the viewing position. One dif.culty of spherical projections, however, 
is the lack of a representation that is suitable for storage on a computer. This is particularly dif.cult 
if a uniform (i.e. equal area) discrete sampling is required. This dif.­culty is re.ected in the various 
distortions which arise in planar projections of world maps in cartography. Those uniform mappings which 
do exist are generally ill-suited for systematic access as a data structure. Furthermore, those which 
do map to a plane with consistent neighborhood relationships are generally quite distorted and, there­fore, 
non-uniform. A set of six planar projections in the form of a cube has been sug­gested by Greene [10] 
as an ef.cient representation for environment maps. While this representation can be easily stored and 
accessed by a computer, it provides signi.cant problems relating to acquisition, alignment, and registration 
when used with real, non-computer-gen­erated images. The orthogonal orientation of the cube faces requires 
precise camera positioning. The wide, 90 degree .eld-of-view of each face requires expensive lens systems 
to avoid optical distortion. Also, the planar mapping does not represent a uniform sampling, but instead, 
is considerably oversampled in the edges and corners. How­ever, the greatest dif.culty of a cube-oriented 
planar projection set is describing the behavior of the image .ow .elds across the bound­aries between 
faces and at corners. This is not an issue when the six planar projections are used solely as an environment 
map, but it adds a considerable overhead when it is used for image analysis. We have chosen to use a 
cylindrical projection as the plenoptic sample representation. One advantage of a cylinder is that it 
can be easily unrolled into a simple planar map. The surface is without boundaries in the azimuth direction, 
which simpli.es correspon­dence searches required to establish image .ow .elds. One short­coming of a 
projection on a .nite cylindrical surface is the boundary conditions introduced at the top and bottom. 
We have chosen not to employ end caps on our projections, which has the problem of lim­iting the vertical 
.eld of view within the environment.  4.2 Acquiring Cylindrical Projections A signi.cant advantage of 
a cylindrical projection is the simplicity of acquisition. The only acquisition equipment required is 
a video camera and a tripod capable of continuous panning. Ideally, the cam­era s panning motion would 
be around the exact optical center of the camera. In practice, in a scene where all objects are relatively 
far from the tripod s rotational center, a slight misalignment offset can be tolerated. Any two planar 
perspective projections of a scene which share a common viewpoint are related by a two-dimensional homogenous 
transform: u a11 a12 a13 x = va21 a22 a23 y (2)1 wa31 a32 a33 uv x' = ----y' = ---­ ww where x and 
y represent the pixel coordinates of an image I, and x and y are their corresponding coordinates in a 
second image I . This well known result has been reported by several authors [12], [28], [22]. The images 
resulting from typical camera motions, such as pan, tilt, roll, and zoom, can all be related in this 
fashion. When creating a cylindrical projection, we will only need to consider panning cam­era motions. 
For convenience we de.ne the camera s local coordinate system such that the panning takes place entirely 
in the x­z plane. In order to reproject an individual image into a cylindrical pro­jection, we must .rst 
determine a model for the camera s projection or, equivalently, the appropriate homogenous transforms. 
Many dif­ferent techniques have been developed for inferring the homogenous transformation between images 
sharing common centers of projec­tion. The most common technique [12] involves establishing four corresponding 
points across each image pair. The resulting trans­forms provide a mapping of pixels from the planar 
projection of the .rst image to the planar projection of the second. Several images could be composited 
in this fashion by .rst determining the transform which maps the Nth image to image N-1. These transforms 
can be catenated to form a mapping of each image to the plane of the .rst. This approach, in effect, 
avoids direct determination of an entire cam­era model by performing all mappings between different instances 
of the same camera. Other techniques for deriving these homogeneous transformations without speci.c point 
correspondences have also been described [22], [25]. The set of homogenous transforms, Hi, can be decomposed 
into two parts which will allow for arbitrary reprojections in a manner similar to [11]. These two parts 
include an intrinsic transform, S, which is determined entirely by camera properties, and an extrinsic 
transform, Ri, which is determined by the rotation around the cam­era s center of projection: u = Hix 
= S 1RiSx (3) This decomposition decouples the projection and rotational compo­nents of the homogeneous 
transform. By an appropriate choice of coordinate systems and by limiting the camera s motion to panning, 
the extrinsic transform component is constrained to a function of a single parameter rotation matrix 
describing the pan. cos. 0 sin. R = (4)0 10 y sin. 0 cos. Since the intrinsic component s properties 
are invariant over all of the images, the decomposition problem can be broken into two parts: the determination 
of the extrinsic rotation component, Ri, followed by the determination of an intrinsic projection component, 
S. The .rst step in our method determines estimates for the extrinsic panning angle between each image 
pair of the panning sequence. This is accomplished by using a linear approximation to an in.nitesimal 
rotation by the angle .. This linear approximation results from sub­stituting1 + O .2 for the cosine 
terms and.+ O .3 () () for the sine terms of the rotation matrix. This in.nitesimal perturbation has 
been shown by [14] to reduce to the following approximate equations: .(xC )2 x 2 x'= . ---------------------------+ 
() xf-O . f (5).(xCx( y ) yC) 2 y'= y -----------------------------------------------+ () -O . f where 
f is the apparent focal length of the camera measured in pixels, and (Cx, Cy) is the pixel coordinate 
of the intersection of the optical axis with the image plane. (Cx, Cy) is initially estimated to be at 
the center pixel of the image plane. A better estimate for (Cx, Cy) is found during the intrinsic matrix 
solution. These equations show that small panning rotations can be approximated by translations for pixels 
near the image s center. We require that some part of each image in the sequence must be visible in the 
successive image, and that some part of the .nal image must be visible in the .rst image of the sequence. 
The .rst stage of the cylindrical registration process attempts to register the image set by computing 
the optimal translation in x which maximizes the normal­ized correlation within a region about the center 
third of the screen. This is .rst computed at a pixel resolution, then re.ned on a 0.1 sub­pixel grid, 
using a Catmull-Rom interpolation spline to compute sub­pixel intensities. Once these translations, ti, 
are computed, Newton s method is used to convert them to estimates of rotation angles and the focal length, 
using the following equation: N t .. 2p atan --f -i = 0 (6) . .. i=1 where N is the number of images 
comprising the sequence. This usu­ally converges in as few as .ve iterations, depending on the original 
estimate for f. This .rst phase determines an estimate for the relative rotational angles between each 
of the images (our extrinsic param­eters) and the initial focal length estimate measured in pixels (one 
of the intrinsic parameters). The second stage of the registration process determines the S, or structural 
matrix, which describes various camera properties such as the tilt and roll angles which are assumed 
to remain constant over the group of images. The following model is used: S = OOP (7) xz where P is the 
projection matrix: 1 s C x P = (8) 0 . C y 00 f and (Cx, Cy) is the estimated center of the viewplane 
as described pre­viously, sis a skew parameter representing the deviation of the sampling grid from a 
rectilinear grid, .determines the sampling grid s aspect ratio, and f is the focal length in pixels as 
determined from the .rst alignment stage. The remaining terms, Oand O, describe the combined effects 
xzof camera orientation and deviations of the viewplane s orientation from perpendicular to the optical 
axis. Ideally, the viewplane would be normal to the optical axis, but manufacturing tolerances allow 
these numbers to vary slightly [27]. 10 0 O = 0 cos. sin. (9) xxx 0 sin. cos. xx cos. sin. 0 zz O = 
 (10) sin. cos. 0 z zz 0 01 In addition, the . term is indistinguishable from the camera s roll zangle 
and, thus, represents both the image sensor s and the camera s rotation. Likewise, ., is combined with 
an implicit parameter,f, that x represents the relative tilt of the camera s optical axis out of the 
pan­ning plane. If fis zero, the images are all tangent to a cylinder and for a nonzero fthe projections 
are tangent to a cone. This gives six unknown parameters, (Cx, Cy, s, ., .x, .), to be z determined in 
the second stage of the registration process. Notice that, when combined with the .i and f parameters 
determined in the .rst stage, we have a total of eight parameters for each image, which is consistent 
with the number of free parameters in a general homo­geneous matrix. The structural matrix, S, is determined 
by minimizing the fol­lowing error function: n 1 error(Cx,Cys..x,.z)= .1 Correlation(Ii 1,SRyiSIi) (11) 
,,, i=1 where Ii-1 and Ii represent the center third of the pixels from images i-1 and i respectively. 
Using Powell s multivariable minimization method [23] with the following initial values for our six parameters, 
image width image height C= -----------------------------C= ------------------------------­ 22 xy (12) 
s=0 .=1 . =0 . =0 xz the solution typically converges in about six iterations. At this point we will 
have a new estimate for (Cx, Cy) which can be fed back into stage one, and the entire process can be 
repeated. The registration process results in a single camera model, S(Cx, Cy, s, ., .x, .z, f), and 
a set of the relative rotations, .i, between each of the sampled images. Using these parameters, we can 
compose mapping functions from any image in the sequence to any other image as follows: I' i = S 1RRR 
R SIj (13) yyyy i+1 i+2 i+3 j We can also reproject images onto arbitrary surfaces by modifying S. Since 
each image pixel determines the equation of a ray from the center-of-projection, the reprojection process 
merely involves inter­secting these rays with the projection manifold.  4.3 Determining Image Flow Fields 
Given two or more cylindrical projections from different positions within a static scene, we can determine 
the relative positions of cen­ters-of-projection and establish geometric constraints across all potential 
reprojections. These positions can only be computed to a scale factor. An intuitive argument for this 
is that from a set of images alone, one cannot determine if the observer is looking at a model or a full-sized 
scene. This implies that at least one measurement is required to establish a scale factor. The measurement 
may be taken either between features that are mutually visible within images, or the distance between 
the acquired image s camera positions can be used. Both techniques have been used with little difference 
in results. To establish the relative relationships between any pair of cylin­drical projections, the 
user speci.es a set of corresponding points that are visible from both views. These points can be treated 
as rays in space with the following form:  cos (f .) a sin (f .)x (., v)= + tD(., v) D(., v)= (14) 
aa Caaa .. kC v av . a . where C= (A , A , A ) is the unknown position of the cylinder s a xyz center 
of projection, fa is the rotational offset which aligns the angu­lar orientation of the cylinders to 
a common frame, ka is a scale factor which determines the vertical .eld-of-view, andC is the scanline 
v a where the center of projection would project onto the scene (i.e. the line of zero elevation, like 
the equator of a spherical map). A pair of tiepoints, one from each image, establishes a pair of rays 
which ideally intersect at the point in space identi.ed by the tie­point. In general, however, these 
rays are skewed. Therefore, we use the point that is simultaneously closest to both rays as an estimate 
of the point s position, p , as determined by the following derivation. x xb a p(.,,v .b, vb) = ----------------(15) 
aa 2 where(. , v ) and(.b, vb) are the tiepoint coordinates on cylin­ aa ders A and B respectively. The 
two points,x and xb , are given by a x = C+ tD(., v ) a a aaa (16) xb = Cb + sDb(.b, vb) where Det Ca 
 Cb, Db(.b, vb), Da(. a, va) × Db(.b, vb) t = ------------------------------------------------------------------------------------------------------------------------ 
 ­ 2 D(., v ) × ( , ) aaa Db .b vb (17) Det Cb C, D(., v ), D(., v ) × Db(.b, ) a aaa aaa vb s = ------------------------------------------------------------------------------------------------------------------------­ 
2 D(., v ) × Db(.b, vb) aa a This allows us to pose the problem of .nding a cylinder s position as a 
minimization problem. For each pair of cylinders we have two sets of six unknowns, [(Ax,Ay,Az,fa,ka,Cva), 
(Bx,By,Bz,fb,kb, Cvb)]. In general, we have good estimates for the k and Cv terms, since these values 
are found by the registration phase. The position of the cyl­inders is determined by minimizing the distance 
between these skewed rays. We also choose to assign a penalty for shrinking the ver­tical height of the 
cylinder in order to bring points closer together. This penalty could be eliminated by accepting either 
the k or Cv val­ues given by the registration. We have tested this approach using from 12 to 500 tiepoints, 
and have found that it converges to a solution in as few as ten iterations of Powell s method. Since 
no correlation step is required, this process is considerably faster than the minimization step required 
to deter­mine the structural matrix, S. The use of a cylindrical projection introduces signi.cant geo­metric 
constraints on where a point viewed in one projection might appear in a second. We can capitalize on 
these restrictions when we wish to automatically identify corresponding points across cylinders. While 
an initial set of 100 to 500 tiepoints might be established by hand, this process is far too tedious 
to establish a mapping for the entire cylinder. Next, we present a geometric constraint for cylindri­cal 
projections that determines the possible positions of a point given its position in some other cylinder. 
This constraint plays the same role that the epipolar geometries [18], [9], used in the computer vision 
community for depth-from-stereo computations, play for planar pro­jections. First, we will present an 
intuitive argument for the existence of such an invariant. Consider yourself at the center of a cylindrical 
pro­jection. Every point on the cylinder around you corresponds to a ray in space as given by the cylindrical 
epipolar geometry equation. When one of the rays is observed from a second cylinder, its path projects 
to a curve which appears to begin at the point corresponding to the origin of the .rst cylinder, and 
it is constrained to pass through the point s image on the second cylinder. This same argument could 
obviously have been made for a pla­nar projection. And, since two points are identi.ed (the virtual image 
of the camera in the second projection along with the corresponding point) and, because a planar projection 
preserve lines, a unique, so called epipolar line is de.ned. This is the basis for an epipolar geom­etry, 
which identi.es pairs of lines in two planar projections such that if a point falls upon one line in 
the .rst image, it is constrained to fall on the corresponding line in the second image. The existence 
of this invariant reduces the search for corresponding points from an O(N2) problem to O(N). Cylindrical 
projections, however, do not preserve lines. In gen­eral, lines map to quadratic parametric curves on 
the surface of a cyl­inder. Surprisingly, we can completely specify the form of the curve with no more 
information than was needed in the planar case. The paths of these curves are uniquely determined sinusoids. 
This cylindrical epipolar geometry is established by the following equation. N cos (f .) + N sin (f 
 .) xa ya v .= --------------------------------------------------------------------------------+ C (18) 
() - Nk va za where N = (Cb Ca)× D(., v ) (19) aa a This formula gives a concise expression for the 
curve formed by the projection of a ray across the surface of a cylinder, where the ray is speci.ed by 
its position on some other cylinder. This cylindrical epipolar relationship can be used to establish 
image .ow .elds using standard computer vision methods. We have used correlation methods [9], a simulated 
annealing-like relaxation method [3], and the method of differences [20] to compute stereo dis­parities 
between cylinder pairs. Each method has its strengths and weaknesses. We refer the reader to the references 
for further details.  4.4 Plenoptic Function Reconstruction Our image-based rendering system takes as 
input cylindrically pro­jected panoramic reference images along with scalar disparity images relating 
each cylinder pair. This information is used to auto­matically generate image warps that map reference 
images to arbitrary cylindrical or planar views that are capable of describing both occlusion and perspective 
effects. yP FIGURE 2. Diagram showing the transfer of the known disparity values between cylinders A 
and B to a new viewing position V. We begin with a description of cylindrical-to-cylindrical map­pings. 
Each angular disparity value, a, of the disparity images, can be readily converted into an image .ow 
vector .eld,(.a, v(.a)) using the epipolar relation given by Equation 18 ++ for each position on the 
cylinder, (., v). We can transfer disparity val­ues from the known cylindrical pair to a new cylindrical 
projection in an arbitrary position, as in Figure 2, using the following equations. a= (B V)cos (f .)+ 
(B V)sin (f .) xxA yyA b= (B A)cos (f .) (B A)sin (f .) yyA xxA (20)c= (V A)cos (f .) (V A)sin (f .) 
yyA xxA abcot (a.( ,v) + ) cot (ß.( ,v)) = -------------------------------------------­ c By precomputing[cos 
(fi .),sin (f .)] for each column of the cylindrical reference image and storinga in place of the i 
cot () disparity image, this transfer operation can be computed at interac­tive speeds. Typically, once 
the disparity images have been transferred to their target, the cylindrical projection would be reprojected 
as a pla­nar image for viewing. This reprojection can be combined with the disparity transfer to give 
a single image warp that performs both oper­ations. To accomplish this, a new intermediate quantity, 
d, called the generalized angular disparity is de.ned as follows: d= (B A)cos (f .)+ (B A)sin (f .) xxA 
yyA (21) 1 d.( ,v) = -------------------------------------------­ dbcot (a.( ,v) + ) This scalar function 
is the cylindrical equivalent to the classical ste­ reo disparity. Finally, a composite image warp from 
a given reference image to any arbitrary planar projection can be de.ned as rDA(.,v)+ kr d.( ,v) · x(.,v) 
= --------------------------------------------------------­ nDA(.,v)+ kn d.( ,v) · (22) sDA(.,v)+ ks 
d.( ,v) · y(.,v) = --------------------------------------------------------­ nDA(.,v)+ kn d.( ,v) · where 
r= vok= r·(C V) × ra s= × k= s·(C V) (23) ou sa n= uvk = n·(C V) × a n and the vectorspouandv ,, are 
de.ned by the desired view as shown in Figure 3. (0,0) u (1,0) (0,1) p FIGURE 3. The center-of-projection, 
p , a vector to the origin, o , and two spanning vectors (u and v) uniquely determine the planar projection. 
In the case where d.( ,v) = constant , the image warp de.ned by Equation 22, reduces to a simple reprojection 
of the cylindrical image to a desired planar view. The perturbation introduced by allowingd.( ,v) to 
vary over the image allows arbitrary shape and occlusions to be represented. Potentially, both the cylinder 
transfer and image warping approaches are many-to-one mappings. For this reason we must con­sider visibility. 
The following simple algorithm can be used to deter­mine an enumeration of the cylindrical mesh which 
guarantees a proper back-to-front ordering, (See Appendix). We project the desired viewing position onto 
the reference cylinder being warped and partition the cylinder into four toroidal sheets. The sheet bound­aries 
are de.ned by the .and v coordinates of two points, as shown in Figure 4. One point is de.ned by the 
intersection of the cylinder with the vector from the origin through the eye s position. The other is 
the intersection with the vector from the eye through the origin. 3 Projection of Eye Position 4 3 2 
Sheet 1 Sheet 2 3 Sheet 4 Sheet 3 FIGURE 4. A back-to-front ordering of the image .ow .eld can be established 
by projecting the eye s position onto the cylinder s surface and dividing it into four toroidal sheets. 
Next, we enumerate each sheet such that the projected image of the desired viewpoint is the last point 
drawn. This simple partitioning and enumeration provides a back-to-front ordering for use by a paint­er 
s style rendering algorithm. This hidden-surface algorithm is a generalization of Anderson s [2] visible 
line algorithm to arbitrary projected grid surfaces. Additional details can be found in [21]. At this 
point, the plenoptic samples can be warped to their new position according to the image .ow .eld. In 
general, these new pixel positions lie on an irregular grid, thus requiring some sort of recon­struction 
and resampling. We use a forward-mapping [28] recon­struction approach in the spirit of [27] in our prototype. 
This involves computing the projected kernel s size based on the current disparity value and the derivatives 
along the epipolar curves. While the visibility method properly handles mesh folds, we still must consider 
the tears (or excessive stretching) produced by the exposure of previously occluded image regions. In 
view interpola­tion [8] a simple distinguished color heuristic is used based on the screen space projection 
of the neighboring pixel on the same scan­line. This approach approximates stretching for small regions 
of occlusion, where the occluder still abuts the occluded region. And, for large exposed occluded regions, 
it tends to interpolate between the boundaries of the occluded region. These exposure events can be handled 
more robustly by combining, on a pixel-by-pixel basis, the results of multiple image warps according 
to the smallest-sized reconstruction kernel.  5. RESULTS We collected a series of images using a video 
camcorder on a leveled tripod in the front yard of one of the author s home. Accurate leveling is not 
strictly necessary for the method to work. When the data were collected, no attempt was made to pan the 
camera at a uniform angu­lar velocity. The autofocus and autoiris features of the camera were disabled, 
in order to maintain a constant focal length during the col­lection process. The frames were then digitized 
at a rate of approximately 5 frames per second to a resolution of 320 by 240 pix­els. An example of three 
sequential frames are shown below. Immediately after the collection of the .rst data set, the process 
was repeated at a second point approximately 60 inches from the .rst. The two image sequences were then 
separately registered using the methods described in Section 4.2. The images were reprojected onto the 
surface of a cylinder with a resolution of 3600 by 300 pixels. The results are shown in Figures 5a and 
5b. The operating room scene, in Figure 5c, was also constructed using these same methods. Next, the 
epipolar geometry was computed by specifying 12 tie­points on the front of the house. Additional tiepoints 
were gradually added to establish an initial disparity image for use by the simulated  FIGURE 5. Cylindrical 
images a and b are panoramic views separated by approximately 60 inches. Image c is a panoramic view 
of an operating room. In image d, several epipolar curves are superimposed onto cylindrical image a. 
annealing and method of differences stereo-correspondence rou­tines. As these tiepoints were added, we 
also re.ned the epipolar geometry and cylinder position estimates. The change in cylinder position, however, 
was very slight. In Figure 5d, we show a cylin­drical image with several epipolar curves superimposed. 
Notice how the curves all intersect at the alternate camera s virtual image and vanishing point. After 
the disparity images are computed, they can be interac­tively warped to new viewing positions. The following 
four images show various reconstructions. When used interactively, the warped images provide a convincing 
kinetic depth effect.  6. CONCLUSIONS The plenoptic function provides a consistent framework for image­based 
rendering systems. The various image-based methods, such as morphing and view interpolation, are characterized 
by the different ways they implement the three key steps of sampling, reconstructing, and resampling 
the plenoptic function. We have described our approach to each of these steps. Our method for sampling 
the plenoptic function can be done with equip­ment that is commonly available, and it results in cylindrical 
samples about a point. All the necessary parameters are automatically esti­mated from a sequence of images 
resulting from panning a video camera through a full circle. Reconstructing the function from these samples 
requires esti­mating the optic .ow of points when the view point is translated. Though this problem can 
be very dif.cult, as evidenced by thirty years of computer vision and photogrammetry research, it is 
greatly simpli.ed when the samples are relatively close together. This is because there is little change 
in the image between samples (which makes the estimation easier), and because the viewer is never far 
from a sample (which makes accurate estimation less important). Resampling the plenoptic function and 
reconstructing a planar projection are the key steps for display of images from arbitrary view­points. 
Our methods allow ef.cient determination of visibility and real-time display of visually rich environments 
on conventional workstations without special purpose graphics acceleration. The plenoptic approach to 
modeling and display will provide robust and high-.delity models of environments based entirely on a 
set of reference projections. The degree of realism will be determined by the resolution of the reference 
images rather than the number of primitives used in describing the scene. Finally, the dif.culty of pro­ducing 
realistic models of real environments will be greatly reduced by replacing geometry with images.  ACKNOWLEDGMENTS 
We are indebted to the following individuals for their contributions and suggestions on this work: Henry 
Fuchs, Andrei State, Kevin Arthur, Donna McMillan, and all the members of the UNC/UPenn collaborative 
telepresence-research group. This research is supported in part by Advanced Research Projects Agency 
contract no. DABT63-93-C-0048, NSF Coopera­tive Agreement no. ASC-8920219, Advanced Research Projects 
Agency order no. A410, and National Science Foundation grant no. MIP-9306208. Approved by ARPA for public 
release - distribution unlimited.  REFERENCES [1] Adelson, E. H., and J. R. Bergen, The Plenoptic Function 
and the Elements of Early Vision, Computational Models of Visual Pro­cessing, Chapter 1, Edited by Michael 
Landy and J. Anthony Movs­hon. The MIT Press, Cambridge, Mass. 1991. [2] Anderson, D., Hidden Line Elimination 
in Projected Grid Sur­faces, ACM Transactions on Graphics, October 1982. [3] Barnard, S.T. A Stochastic 
Approach to Stereo Vision, SRI Inter­national, Technical Note 373, April 4, 1986. [4] Beier, T. and S. 
Neely, Feature-Based Image Metamorphosis, Computer Graphics (SIGGRAPH 92 Proceedings), Vol. 26, No. 2, 
pp. 35-42, July 1992. [5] Blinn, J. F. and M. E. Newell, Texture and Re.ection in Computer Generated 
Images, Communications of the ACM, vol. 19, no. 10, pp. 542-547, October 1976. [6] Bolles, R. C., H. 
H. Baker, and D. H. Marimont, Epipolar-Plane Image Analysis: An Approach to Determining Structure from 
Motion, International Journal of Computer Vision, Vol. 1, 1987. [7] Catmull, E., A Subdivision Algorithm 
for Computer Display of Curved Surfaces (Ph. D. Thesis), Department of Computer Sci­ence, University 
of Utah, Tech. Report UTEC-CSc-74-133, December 1974. [8] Chen, S. E. and L. Williams. View Interpolation 
for Image Syn­thesis, Computer Graphics (SIGGRAPH 93 Proceedings), pp. 279-288, July 1993. [9] Faugeras, 
O., Three-dimensional Computer Vision: A Geomet­ric Viewpoint, The MIT Press, Cambridge, Massachusetts, 
1993. [10] Greene, N., Environment Mapping and Other Applications of World Projections, IEEE Computer 
Graphics and Applica­tions, November 1986. [11] Hartley, R.I., Self-Calibration from Multiple Views with 
a Rotat­ing Camera, Proceedings of the European Conference on Com­puter Vision, May 1994. [12] Heckbert, 
P. S., Fundamentals of Texture Mapping and Image Warping, Masters Thesis, Dept. of EECS, UCB, Technical 
Report No. UCB/CSD 89/516, June 1989. [13] Horn, B., and B.G. Schunck, Determining Optical Flow, Arti­.cial 
Intelligence, Vol. 17, 1981. [14] Kanatani, K., Transformation of Optical Flow by Camera Rota­tion, IEEE 
Transactions on Pattern Analysis and Machine Intelligence, Vol. 10, No. 2, March 1988. [15] Laveau, S. 
and O. Faugeras, 3-D Scene Representation as a Col­lection of Images and Fundamental Matrices, INRIA, 
Technical Report No. 2205, February, 1994. [16] Lenz, R. K. and R. Y. Tsai, Techniques for Calibration 
of the Scale Factor and Image Center for High Accuracy 3D Machine Vision Metrology, Proceedings of IEEE 
International Conference on Robotics and Automation, March 31 - April 3, 1987. [17] Lippman, A., Movie-Maps: 
An Application of the Optical Video­disc to Computer Graphics, SIGGRAPH 80 Proceedings, 1980. [18] Longuet-Higgins, 
H. C., A Computer Algorithm for Reconstruct­ing a Scene from Two Projections, Nature, Vol. 293, September 
1981. [19] Longuet-Higgins, H. C., The Reconstruction of a Scene From Two Projections - Con.gurations 
That Defeat the 8-Point Algorithm, Proceedings of the First IEEE Conference on Arti.cial Intelli­gence 
Applications, Dec 1984. [20] Lucas, B., and T. Kanade, An Iterative Image Registration Tech­nique with 
an Application to Stereo Vision, Proceedings of the Seventh International Joint Conference on Arti.cial 
Intelli­gence, Vancouver, 1981. [21] McMillan, Leonard, A List-Priority Rendering Algorithm for Redisplaying 
Projected Surfaces, Department of Computer Sci­ence, UNC, Technical Report TR95-005, 1995. [22] Mann, 
S. and R. W. Picard, Virtual Bellows: Constructing High Quality Stills from Video, Proceedings of the 
First IEEE Inter­national Conference on Image Processing, November 1994. [23] Press, W. H., B. P. Flannery, 
S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes in C, Cambridge University Press, Cambridge, 
Massachusetts, pp. 309-317, 1988. [24] Regan, M., and R. Pose, Priority Rendering with a Virtual Reality 
Address Recalculation Pipeline, SIGGRAPH 94 Proceedings, 1994. [25] Szeliski, R., Image Mosaicing for 
Tele-Reality Applications, DEC and Cambridge Research Lab Technical Report, CRL 94/ 2, May 1994. [26] 
Tomasi, C., and T. Kanade, Shape and Motion from Image Streams: a Factorization Method; Full Report on 
the Orthographic Case, Technical Report, CMU-CS-92-104, Carnegie Mellon Uni­versity, March 1992. [27] 
Tsai, R. Y., A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using 
Off-the-Shelf TV Cameras and Lenses, IEEE Journal of Robotics and Automa­tion, Vol. RA-3, No. 4, August 
1987. [28] Westover, L. A., Footprint Evaluation for Volume Rendering, SIGGRAPH 90 Proceedings, August 
1990. [29] Wolberg, G., Digital Image Warping, IEEE Computer Society Press, Los Alamitos, CA, 1990. 
 APPENDIX We will show how occlusion compatible mappings can be deter­mined on local spherical frames 
embedded within a global cartesian frame, W. The projected visibility algorithm for cylindrical surfaces 
given in the paper can be derived by reducing it to this spherical case. First, consider an isolated 
topological multiplicity on the pro­jective mapping from Si to Sj, as shown below z Si Sj p1 p2 x y 
Theorem 1: In the generic case, the points of a topological multi­plicity induced by a mapping from Si 
to Sj, and the two frame origins are coplanar. Proof: The points of the topological multiplicity are 
colinear with the origin of Sj since they share angular coordinates. A second line segment connects the 
local frame origins, Si and Sj. In general, these two lines are distinct and thus they de.ne a plane 
in three space. Thus, a single af.ne transformation, A, of W can accomplish the following results. Translate 
Si to the origin  Rotate Sj to lie on the x-axis  Rotate the line along the multiplicity into the xy-plane 
 Scale the system so that Sj has the coordinate (1,0,0). With this transformation we can consider the 
multiplicity entirely within the xy-plane, as shown in the following .gure. a-.2 b .2 a-.1 a a.1 Theorem 
2: Ifcos.1 > cos and( ,,a).[ .2 .1 .20, p] then a < b. Proof: The length of sides a and b can be computed 
in terms of the angles.1,.2 anda using the law of sines as follows. a 1 b 1 -------------= ------------------------------------------= 
----------------------------­ sin.1 sin (a.1) sin.2 sin (a.2) a sina cot.2 cosa --= -------------------------------------------­ 
b sina cot.1 cosa if cos.1 > cos.2then cot > cot < .1 .2, thus ab Thus, an occlusion compatible mapping, 
can be determined by enumerating the topological mesh de.ned onASiin an order of increasing cos. , while 
allowing later mesh facets to overwrite pre­vious ones. This mapping is occlusion compatible since, by 
Theorem 2, greater range values will always proceed lesser values at all mul­tiplicities. Notice, that 
this mapping procedure only considers the changes in the local frame s world coordinates, and makes no 
use of the range information itself. Si Sj  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218405</article_id>
		<sort_key>47</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Multi-level direction of autonomous creatures for real-time virtual environments]]></title>
		<page_from>47</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/218380.218405</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218405</url>
		<categories>
			<primary_category>
				<cat_node>C.3</cat_node>
				<descriptor>Real-time and embedded systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Channels and controllers</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010590</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Buses and high-speed links</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010570</concept_id>
				<concept_desc>CCS->Computer systems organization->Real-time systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010520.10010553</concept_id>
				<concept_desc>CCS->Computer systems organization->Embedded and cyber-physical systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P33299</person_id>
				<author_profile_id><![CDATA[81100047544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Blumberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, 20 Ames St., Cambridge MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P282976</person_id>
				<author_profile_id><![CDATA[81100291883]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tinsley]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Galyean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, 20 Ames St., Cambridge MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arkin, R.C., Integrating Behavioral, Perceptual, and World Knowledge in Reactive Navigation, in Designing Autonomous Agents, R Maes, Editor. 1990, MIT Press, Cambridge, pp.105-122.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>162261</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Badler, N.I., C. Phillips, and B.L. Webber, Simulating Humans: Computer Graphics, Animation, and Control. 1993, Oxford University Press, New York.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>189843</ref_obj_id>
				<ref_obj_pid>189829</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blumberg, B. Action-Selection in Hamsterdam: Lessons from Ethology. in Third International Conference on the Simulation of Adaptive Behavior. Brighton, England,1994, MIT Press. pp.108- 117.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brooks, R., A Robust Layered Control System for a Mobile Robot. 1986. IEEE Journal of Robotics and Automation RA-2.pp. 14-23.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, Armin and Thomas W. Calvert. Dynamic Animation of Human Walking. Proceedings of SIGGRAPH 89 (Boston, MA, July 31-August 4, 1989). In Computer Graphics 23, 3 (July 1989),233-242.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>922009</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Galyean, T. A. Narrative Guidance of Interactivity, Ph.D. Dissertation, Massachusetts Institute of Technology, 1995.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325244</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Girard, Michael and A. A. Maciejewski. Computational Modeling for the Computer Animation of Legged Figures. Proceedings of SIGGRAPH 85 (San Francisco, CA, July 22-26, 1985). In Computer Graphics 19, 263-270.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171189</ref_obj_id>
				<ref_obj_pid>171174</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Horswill, I. A Simple, Cheap, and Robust Visual Navigation System. in Second International Conference on the Simulation of Adaptive Behavior. Honolulu, HI, 1993. MIT Press, pp.129-137.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Koga, Yoshihito, Koichi Kondo, James Kuffner, and Jean- Claude Latombe. Planning Motion with Intentions. Proceedings of SIGGRAPH 94 (Orlando, FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM, SIGGRAPH, pp. 395-408.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>532147</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Latombe, J. C., Robot Motion Planning. 1991, Kluwer Academic Publishers, Boston.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Ludlow, A., The Evolution and Simulation of a Decision Maker, in Analysis of Motivational Processes, F.T.&amp;.T. Halliday, Editor. 1980, Academic Press, London.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Maes, R, Situated Agents Can Have Goals. Journal for Robotics and Autonomous Systems 6(1&amp;2), 1990, pp. 49-70.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791451</ref_obj_id>
				<ref_obj_pid>791214</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Maes, R, T. Darrell, and B. Blumberg. The ALIVE System: Full-body Interaction with Autonomous Agents. in Proceedings of Computer Animation'95 Conference, Switzerland, April 1995, IEEE Press, pp. 11-18.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97882</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[McKenna, Michael and David Zeltzer. Dynamic Simulation of Autonomous Legged Locomotion. Proceedings of SIGGRAPH 90 (Dallas, TX, August 6-10, 1990). In Computer Graphics 24, 4 (August 1990), pp.29-38.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22939</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Minsky, M., The Society of Mind. 1988, Simon &amp; Schuster, New York.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122755</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Raibert, Marc H. and Jessica K. Hodgins. Animation of Dynamic Legged Locomotion. Proceedings of SIGGRAPH 91 (Las Vegas, NV, July 28-August 2, 1991). In Computer Graphics 25, 4 (July 1991), 349-358]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Renault, O., N. Magnenat-Thalmann, D. Thalmann. A visionbased approach to behavioral animation. The Journal of Visualization and Computer Animation 1(1),1990, pp.18-21.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Reynolds, Craig W. Flocks, Herds, and Schools: A Distributed Behavioral Model. Proceedings of SIGGRAPH 87 (Anaheim, CA, July 27-31, 1987). In Computer Graphics 21, 4 (July 19987), 25-34.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Tinbergen, N., The Study of Instinct. 1950, Clarendon press, Oxford.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Tu, Xiaoyuan and Demetri Terzopoulos. Artificial Fishes: Physics, Locomotion, Perception, Behavior. Proceedings of SIG- GRAPH 94 (Orlando, FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM, SIGGRAPH, pp. 43-50.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>171190</ref_obj_id>
				<ref_obj_pid>171174</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Tyrrell, T. The Use of Hierarchies for Action Selection, in Second International Conference on the Simulation of Adaptive Behavior. 1993. MIT Press, pp.138-147.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617556</ref_obj_id>
				<ref_obj_pid>616011</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Wilhelms J., R. Skinner. A 'Notion' for Interactive Behavioral Animation Control. IEEE Computer Graphics and Applications 10(3) May 1990, pp. 14-22.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>111155</ref_obj_id>
				<ref_obj_pid>111154</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[David Zeltzer, Task Level Graphical Simulation: Abstraction, Representation and Control, in Making Them Move. Ed. Badler, N., Barsky, B., and Zeltzer D. 1991, Morgan Kaufmann Publishers, Inc.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-Level Direction of Autonomous Creatures for Real-Time Virtual Environments Bruce M. Blumberg and 
Tinsley A. Galyean. MIT Media Lab ABSTRACT There have been several recent efforts to build behavior-based 
autonomous creatures. While competent autonomous action is highly desirable, there is an important need 
to integrate autonomy with directability . In this paper we discuss the problem of build­ing autonomous 
animated creatures for interactive virtual environ­ments which are also capable of being directed at 
multiple levels. We present an approach to control which allows an external entity to direct an autonomous 
creature at the motivational level, the task level, and the direct motor level. We also detail a layered 
architecture and a general behavioral model for perception and action-selection which incorporates explicit 
support for multi-level direction. These ideas have been implemented and used to develop several autonomous 
animated creatures. 1. INTRODUCTION Since Reynold's seminal paper in 1987, there have been a num­ber 
of impressive papers on the use of behavioral models to gener­ate computer animation. The motivation 
behind this work is that as the complexity of the creature's interactions with its environment and other 
creatures increases, there is an need to endow the crea­tures with the ability to perform autonomous 
activity. Such crea­tures are, in effect, autonomous agents with their own perceptional, behavioral, 
and motor systems. Typically, authors have focused on behavioral models for a speci.c kind of creature 
in a given environment, and implemented a limited set of behav­iors. There are examples of locomotion 
[2, 5, 7, 14, 16], .ocking [18], grasping [9], and lifting [2]. Tu and Terzopoulus's Fish [20] represent 
one of the most impressive examples of this approach. Advances in behavioral animation are critically 
important to the development of creatures for use in interactive virtual environ­ments. Research in autonomous 
robots [4, 8, 12] supports the need to couple real-time action with dynamic and unpredictable envi­ronments. 
Their insights only serve to strengthen the argument for autonomous animated creatures. Pure autonomy, 
perhaps, should not be the ultimate goal. Imag­ine making an interactive virtual Lassie experience for 
children. Suppose the autonomous animated character playing Lassie did a .ne job as a autonomous dog, 
but for whatever reason was ignor­ing the child. Or suppose, you wanted the child to focus on some . 
MIT Media Lab, 20 Ames St., Cambridge MA, 02139. bruce/tag@media.mit.edu Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 aspect of the environment which was important 
to the story, but Lassie was distracting her. In both cases, you would want to be able to provide external 
control, in real-time, to the autonomous Lassie. For example, by increasing its motivation to play , 
it would be more likely to engage in play. Alternatively, Lassie might be told to go over to that tree 
and lie down so as to be less dis­tracting. Thus, there is a need to understand how to build animated 
char­acters for interactive virtual environments which are not only capable of competent autonomous action 
but also capable of responding to external control. We call this quality directability. This is the fundamental 
problem addressed in this paper. This paper makes 3 primary contributions to the body of litera­ture 
regarding animated autonomous characters. Speci.cally, we describe: An approach to control which allows 
an external entity to direct a virtual character at a number of different levels.  A general behavioral 
model for perception and action­selection in autonomous animated creatures but which also supports external 
control.  A layered architecture which supports extensibility, re­usability and multiple levels of direction. 
 An experimental toolkit which incorporates these ideas has been successfully used to build a number 
of creatures: a virtual dog used in an interactive virtual environment, and several crea­tures used in 
an interactive story telling environment. The remainder of the paper is organized as follows. In section 
2 we present a more detailed problem statement and summarize the key contributions of our approach. In 
section 3 we present an over­view of the general architecture. In sections 4, 5 and 6 we discuss the 
motor, sensory and behavior systems in more depth. In section 7 we discuss how directability is integrated 
into our architecture. Finally, in section 8 we discuss some aspects of our implementa­tion and give 
examples of its use.  2. PROBLEM STATMENT An autonomous agent is a software system with a set of goals 
which it tries to satisfy in a complex and dynamic environment. It is autonomous in the sense that it 
has mechanisms for sensing and interacting with its environment, and for deciding what actions to take 
so as to best achieve its goals[12]. In the case of an autono­mous animated creature, these mechanisms 
correspond to a set of sensors, a motor system and associated geometry, and lastly a behavior system. 
In our terminology, a creature is an animate object capable of goal-directed and time-varying behavior. 
Deciding on the right action or set of actions is complicated by a number of factors. For example, due 
to the problems inherent in sensing and perception, a creature s perception of its world is likely to 
be incomplete at best, and completely erroneous at worst. There may be competing goals which work at 
cross-purposes (e.g. moving toward food may move the creature away from water). This can lead to dithering 
in which the creature oscillates among competing activities. On the other hand, an important goal may 
be un-obtainable, and pursuit of that goal may prevent the satisfaction of lower priority, but attainable 
goals. External opportunities need to be weighed against internal needs in order to provide just the 
right level of opportunistic behavior. Actions may be unavailable or unreliable. To successfully produce 
competent autonomous action over extended periods of time, the Behavior System must provide solutions 
to these problems, as well as others. However, as mentioned earlier, strict autonomy is not the goal. 
We need, in addition, to direct the creature at a number of different levels. Three levels of input, 
(motivational, task, and direct) are outlined in Figure 1. Additionally, commands at the direct level 
need to be able to take three imperative forms: Do it, independent of the Behavior System.  Do it, 
if the Behavior System doesn t object.  Suggest how an action should be performed, should the Behavior 
System wish to perform that action.  Thus, the behavior and motor systems must be designed and implemented 
in such a way that it is possible to support these lev­els and types of direction at run-time. Building 
autonomous animated creatures is inherently an itera­tive process. This is particularly true since we 
are in the early phases of understanding how to build them. Ideally, a common approach should be taken 
for the speci.cation of geometry through to behavior so that a developer need only learn a single framework. 
Lastly, an embedded interpreter is required to facilitate testing, as well as run-time direction. 2.1 
Multiple Levels of Control We provide an approach to control which allows an external entity to direct 
an autonomous animated creature at a number of different levels. These levels are detailed in Figure 
1. By providing Motivational Task Direct Level Level Level just do the do THIS the do what right thing 
right way I tell you "you are"go to that"wag your hungry" tree" tail" Figure 1: Here we articulate three 
levels at which a creature can be directed. At the highest level the creature would be in.uenced by changing 
its current motiva­tion and relying on it to react to this change. If you tell it to be hungry it will 
go off looking for food. At the task level you give it a high level directive and you expect it to carry 
out this command in a reasonable manner (for example walking around a building instead of through it.) 
At the lowest level you want to give a creature a command that directly changes its geometry. the ability 
to direct the creature at multiple levels the animator or developer can choose the appropriate level 
of control for a given situation. Both Badler and Zeltzer have proposed similar decom­position of control 
[2, 23]. 2.2 A General Behavior Model We propose a distributed behavioral model, inspired by work in 
Ethology and autonomous robot research, for perception and action-selection in autonomous animated creatures 
but which also supports external control. The contributions of this model include: A general model of 
action-selection which provides greater control over temporal patterns of behavior than previously described 
approaches have offered.  A natural and general way to model the effect of exter­nal stimuli and internal 
motivation.  An approach in which multiple behaviors may suggest actions to be performed and preferences 
for how the  actions are to be executed, while still maintaining the advantages of a winner-take-all 
architecture. An implementation which supports motivational and task level direction at run-time. We 
also describe a robotics inspired approach to low-level autonomous navigation in which creatures rely 
on a form of syn­thetic vision to perform navigation and obstacle avoidance. 2.3 A Layered Architecture 
A 5-layered architecture for autonomous animated creatures is described. Several important abstraction 
barriers are provided by the architecture: One between the Behavior System and the Motor Skills, which 
allows certain behaviors (e.g. move­toward ) to be independent of the Motor Skills which perform the 
desired action (e.g. drive vs. walk ) in a given creature.  One between the Motor Skills and geometry 
which serves as both an abstraction barrier and a resource manager.  The result is an architecture which 
encourages re-usability and extensibility, while providing the necessary foundation to support autonomous 
action with interactive direction. 3. ARCHITECTURE Figure 2: Block diagram of a creature s architecture. 
The basic structure consists of the three basic parts (Geometry, Motor Skills and Behavior) with two 
layers of abstraction between these parts (Controller, and Degrees of Freedom). Figure 2 shows the basic 
architecture for a creature.The geome­try provides the shapes and transforms that are manipulated over 
time for animation. The Motor Skills provide atomic motion ele­ments which manipulate the geometry in 
order to produce coordi­nated motion. Walking or Wagging the tail are examples of Motor Skills. Motor 
Skills manipulate the geometry with no knowledge of the environment or state of a creature, other than 
that needed to execute the skill. At the top rests the Behavior System of a creature. This element is 
responsible for deciding what to do, given its goals and sensory input and triggering the correct Motor 
Skills to achieve the current task or goal. In addition to these three parts, there are two layers of 
insulation, the controller and the degrees of freedom (DOFs), which are important to making this architecture 
generalizable and extensible. Behaviors implement high level capabilities such as, .nd food and eat , 
or sit down and shake , as well as low level capabilities such as move to or avoid obstacle by issuing 
the appropriate motor commands (i.e forward , left , sit , etc.) to the control­ler. Some behaviors may 
be implemented in a creature-indepen­dent way. For example, the same move to behavior may be applicable 
to any creature with basic locomotive skills (e.g. for­ward, left, right,...) although each may use different 
Motor Skills to perform the required action. It is the controller which provides this common interface 
to the Motor Skills by mapping a generic command ( forward ) into the correct motor skill(s) and parame­ters 
for a given creature. In this way, the same behavior may be used by more than one type of creature. Figure 
3 shows the sources of input to the creature. Sensors are Figure 3: There are two sources of input to 
a creature. First are sensors associated with the creature. These sensors are used by the Behavior System 
to enable both task level and autonomous behavior. The other source of input is from the user (or application 
using the creature.) This input can happen at multiple levels, ranging from simply adjusting a creature's 
current motivational state to directly turning a motor skill on or off. elements of a creature which 
the creature uses to interrogate the environment for relevant information. The creature may also take 
additional input from the user or the application using the creature. These directives can enter the 
creature s computational model at the three different levels.  4. MOTOR SYSTEM We use the term motor 
system to refer to the three layers that lie between the Behavior System and the geometry, Figure 2. 
These parts include the Motor Skills in the center, and the abstrac­tion and interface barriers on either 
side of the Motor Skills. Together these three layers of the architecture provide the mapping from motor 
commands to changes in the geometry over time. The motor system is designed to meet the following 5 important 
criteria: Act as an abstraction barrier between high-level com­mands (e.g. forward ) and the creature 
speci.c implementation (e.g. walking ).  Support multiple imperative forms for commands.  Provide a 
generic set of commands which all creatures can perform.  Minimize the amount of house-keeping required 
of the Behavior System.  Provides resource management so as to support coher­  ent, concurrent motion. 
Within these three layers of the motor system, the controller pro­vides the high level abstraction barrier 
and the support of multiple imperative forms. Motor skills can be inherited allowing basic skills to 
be shared amongst creatures. Also, Motor Skills are designed to minimize the house-keeping that an external 
user or Behavior System must do. It is the degree of freedom abstraction barrier that serves as the resource 
manager. 4.1 Degrees of Freedom (DOFs) Degrees of Freedom (DOFs) are knobs that can be used to modify 
the underlying geometry. They are the mechanism by which creatures are repositioned and reshaped. For 
example, DOFs might be used to wag the tail, move a joint, or reposition an entire leg. DOFs serve 2 
important functions: Resource management. Provides a locking mechanism so that competing Motor Skills 
do not con.ict.  An abstraction barrier. Utilizes interpolators to re-map simple input values (0 to 
1) to more complex motion.  The resource management system is a simple one. Each DOF can be locked by 
a motor skill, restricting it by anyone else until unlocked. This locking provides a mechanism for insuring 
coher­ent, concurrent motion. As long as two or more Motor Skills do not con.ict for DOFs they are free 
to run concurrently. Alternatively, if a motor skill requests DOFs that are already locked it will be 
informed it cannot run currently. When functioning as an abstraction barrier a DOF provides a mechanism 
to map a simple input value (often a number between 0 and 1) to another space via interpolators and inverse 
kinematics such as in, Figure 4. It is this abstraction that allows a motor skill wag tail interpolator 
0 knee joint stepping interpolator 10 Figure 4: DOFs in a creature can provide interfaces to the geometry 
at several dif­ferent levels. For example, joints (and therefore the associated transformations) can 
be directly controlled or indirectly as is the case with this leg. Here inverse kine­matics is used to 
move the foot. An additional level of abstraction can be added by using interpolators. The interpolator 
on the leg provides a one knob interface for the motor skill. By giving a number between 0 and 1 the 
motor skill can set the location of the leg along one stepping cycle. Likewise the tail can be wagged 
with only one number. to position a leg along a stepping cycle via one number. Note that when a high 
level DOF (the stepping DOF in this example) is locked the lower level DOFs it utilizes (the leg joints) 
are in turn locked. 4.2 Motor Skills A motor skill utilizes one or more DOFs to produce coordinated 
movement. Walking, turning, or lower head are all examples of Motor Skills. A motor skill can produce 
complicated motion, and the DOFs locking mechanism insures that competing Motor Skills are not active 
at the same time. In addition, Motor Skills present an extremely simple interface to upper layers of 
the architecture. A motor skill can be requested to turned on, or to turn off. In either case, arguments 
may be passed as part of the request. Motor skills rely heavily on degrees of freedom to do their work. 
Each motor skill declares which DOFs it needs in order to perform its task. It can only become active 
if all of these DOFs are unlocked. Once active, a motor skill adjusts all the necessary DOFs with each 
time-step to produce coordinated animation. Most Motor Skills are spring-loaded. This means that if they 
have not been requested to turn on during an update cycle, they begin to move their DOFs back toward 
some neutral position and turn off within a few time-steps.The advantage of this approach is that a behavior, 
which turns on a skill, need not be concerned with turning it off at the correct time. The skill will 
turn itself off, thereby reducing the amount of bookkeeping. Because Motor Skills are spring-loaded the 
Behavior System is required to spec­ify which skills are to be active with each time-step. This may seem 
like a burden but it is consistent with a reactive behavior sys­tem which re-evaluates what actions it 
should perform during every update cycle. It should also be noted that this spring-loaded feature can 
be turned off, to facilitate sources of direction other than the Behavior System. There are a number 
of basic Motor Skills which all creatures inherit, such as ones for setting the position or heading of 
the crea­ture. 4.3 Controller The controller is a simple but signi.cant layer in the architec­ture which 
serves an important function as an abstraction barrier between the Behavior System and the underlying 
Motor Skills. The primary job of the controller is to map commands such as forward , turn , halt , look 
at etc. into calls to turn on or turn off the appropriate motor skill(s). Thus, forward may result in 
the walk motor skill being turned on in the dog but the move motor skill in the case of the car. This 
is an important function because it allows the Behavior System or application to use one set of commands 
across a potentially wide-range of creatures, and lets the motor system of each creature to interpret 
them differently but appropriately. The controller accepts commands in the form of a data structure called 
a motor command block. This data structure speci.es the command and any arguments. In addition, a motor 
command block can store return arguments, allowing functions that inquire about the state of a creature 
(its position, its velocity) to be treated by the same mechanism as all other commands. A command block 
can be issued to the controller as one of three imperative forms: primary command - to be executed immediately; 
secondary - to be queued at a lower priority; and as a meta command - suggesting how another command 
should be run, Figure 5. These different impera­ Figure 5: An incoming command is represented in a motor 
command block con­sisting of a command id and an optional list of typed arguments. If these argu­ments 
are not supplied then defaults stored in the controller are used. Any given command will turn on or off 
one or more Motor Skills, while also providing any necessary arguments. Commands take two levels of importance. 
Primary com­mands are executed right away, while secondary commands are queued. These queued commands 
are executed at the end of each time cycle, and only if the nec­essary resources (DOFS) are available. 
It is expected that these secondary com­mands will be used for suggested but not imperative actions. 
The last type of input into the controller is in the form of a meta-command. These commands are stored 
as suggestion of how to execute a command. For example, if you are going to walk I suggest that you walk 
slowly. These are only stored in the controller and it is the responsibility of the calling application 
(or user) to use or ignore a sugges­tion. tive forms are used extensively by the Behavior System (see 
sec­ tion 6.6). They allow multiple behaviors to simultaneously express their preferences for motor actions. 
 5. SENSING There are at least three types of sensing available to autono­mous animated creatures: 
Real-world sensing using real-world noisy sensors.  Direct sensing via direct interrogation of other 
vir­tual creatures and objects.  Synthetic Vision in which the creature utilizes vision techniques to 
extract useful information from an image rendered from their viewpoint.  While it is important to support 
all three types of sensing, we have found synthetic vision to be particularly useful for low-level navigation 
and obstacle avoidance. Several researchers, including Renault [17], Reynolds[18], and Latombe[10] have 
suggested sim­ilar approaches. External World  Level of  Inhibition Interest Motor Commands Figure 
6:The purpose of a Behavior is to evaluate the appropriateness of the behav­ior, given external stimulus 
and internal motivations, and if appropriate issue motor commands. Releasing Mechanisms act as .lters 
or detectors which identify signi.­cant objects or events from sensory input, and which output a value 
which corre­sponds to the strength of the sensory input. Motivations or goals are represented via Internal 
Variables which output values which represents the strength of the motiva­tion. A Behavior combines the 
values of the Releasing Mechanisms and Internal Variables on which it depends and that represents the 
value of the Behavior before Level of Interest and Inhibition from other Behaviors. Level of Interest 
is used to model boredom or behavior-speci.c fatigue. Behaviors must compete with other behaviors for 
control of the creature, and do so using Inhibition (see text for details).There are a variety of explicit 
and implicit feedback mechanisms. 5.1 Synthetic Vision For Navigation Horswill [8] points out that while 
vision in general is a very hard problem, there are many tasks for which it is possible to use what he 
calls light-weight vision. That is, by factoring in the characteristics of the robot s interaction with 
the environment and by tailoring the vision task to the speci.c requirements of a given behavioral task, 
one can often simplify the problem. As a result, vision techniques developed for autonomous robots tend 
to be computationally cheap, easy to implement, and reasonably robust. Synthetic vision makes sense for 
a number of reasons. First, it may be the simplest and fastest way to extract useful information from 
the environment (e.g. using vision for low-level obstacle avoidance and navigation versus a purely analytical 
solution). This may be particularly true if one can take advantage of the rendering hardware. Moreover, 
synthetic vision techniques will probably scale better than analytical techniques in complex environments. 
Finally, this approach makes the creature less dependent on the implementation of its environment because 
it does not rely on other creatures and objects to respond to particular queries. Our approach is simple. 
The scene is rendered from the crea­ture s eye view and the resulting image is used to generate a poten­tial 
.eld from the creature s perspective (this is done in an approach similar to that of Horswill). Subsequently, 
a gradient .eld is calculated, and this is used to derive a bearing away from areas of high potential. 
Following Arkin [1], some behaviors within the Behavior System represent their pattern of activity as 
a potential .elds as well (for example, moveto). These potential .elds are combined with the .eld generated 
by the vision sensor to arrive at a compromise trajectory. This sensor is a simple example of using a 
technique borrowed from robotics. It was simple to implement, works well in practice, and is general 
enough to allow our virtual dog to wander around in new environments without modi.cation.  6. BEHAVIOR 
SYSTEM The purpose of the Behavior System is to send the right set of control signals to the motor system 
at every time-step. That is, it must weigh the potentially competing goals of the creature, assess Goals/Motivations 
Internal Variable  Internal Variable  the state of its environment, and choose the set of actions 
which make the most sense at that instant in time. More generally, it provides the creature with a set 
of high-level behaviors of which it is capable of performing autonomously in a potentially unpredict­able 
environment. Indeed, it is this ability to perform competently in the absence of external control which 
makes high level motiva­tional or behavioral control possible. Action-selection has been a topic of some 
interest among Ethol­ogists and Computer Scientists alike, and a number of algorithms have been proposed 
[3, 4, 12, 19-22]. Earlier work [3], presented a computational model of action-selection which draws 
heavily on ideas from Ethology. The algorithm presented below is derived from this work but incorporates 
a number of important new fea­tures. The interested reader may consult [3] for the ethological jus­ti.cation 
for the algorithm. The remainder of this section describes the major components of the Behavior System, 
and how it decides to do the right thing 6.1 Behaviors While we have spoken of a Behavior System as a 
monolithic entity, it is in fact a distributed system composed of a loosely hier­archical network of 
self-interested, goal-directed entities called Behaviors. The granularity of a Behavior s goal may vary 
from very general (e.g. reduce hunger ) to very speci.c (e.g. chew food ). The major components of an 
individual Behavior are shown in Figure 6. This model of a distributed collection of goal­directed entities 
is consistent with ethological models as well as recent theories of the mind [15]. Behaviors compete 
for control of the creature on the basis of a value which is re-calculated on every update cycle for 
each Behav­ior. The value of a Behavior may be high because the Behavior sat­is.es an important need 
of the creature (e.g. its Internal Variables have a high value). Or it may be high because the Behavior 
s goal is easily achievable given the Behavior s perception of its environ­ment (e.g. its Releasing Mechanisms 
have a high value). Behaviors in.uence the system in several ways: by issuing motor commands which change 
the creature s relationship to its environment, by modifying the value of Internal Variables, by inhibiting 
other Behaviors, or by issuing suggestions which in.u­ence the motor commands issued by other Behaviors. 
Behaviors are distinguished from Motor Skills in two ways. First, a Behavior is goal-directed whereas 
a Motor Skill is not. For example, Walking in our model is a Motor Skill. Moving toward an object of 
interest is a Behavior. Second, a Behavior decides when it should become active, whereas a Motor Skill 
runs when something else decides it should be active. Typically, Behav­iors rely on Motor Skills to perform 
the actions necessary to accomplish the Behavior s goals. 6.2 Releasing Mechanisms and Pronomes Behaviors 
rely on objects called Releasing Mechanisms to .l­ter sensory input and identify objects and/or events 
which are rele­vant to the Behavior, either because they are important to achieving the Behavior s goal, 
or because their presence deter­mines the salience of the Behavior given the creature s immediate environment. 
Releasing Mechanisms output a continuous value which typically depends on whether the stimuli was found, 
on its distance and perhaps on some measure of its quality. This is impor­tant because by representing 
the output of a Releasing Mechanism as a continuous quantity, the output may be easily combined with 
the strength of internal motivations which are also represented as continuous values. This in turn allows 
the creature to display the kind of behavior one .nds in nature where a weak stimulus (e.g. day-old pizza) 
but a strong motivation (e.g. very hungry) may result in the same behavior as a strong stimulus (e.g. 
chocolate cake) but weak motivation (e.g. full stomach). Figure 7: Releasing Mechanisms identify signi.cant 
objects or events from sensory input and output a value which represents the strength of the stimulus. 
By varying the allowed maximum for a given Releasing Mechanism, a Behavior can be made more or less sensitive 
to the presence of whatever input causes the Releasing Mechanism to have a non-zero value. A Releasing 
Mechanism has 4 phases (Find, Filter, Weight and Temporal Filtering), as indicated above, each of which 
is implemented by call­backs. Releasing Mechanisms can often share the same generic callback for a given 
phase. Temporal Filtering is provided to deal with potentially noisy data. While Releasing Mechanisms 
may be looking for very different objects and events, they typically have a common structure. This is 
described in more detail in Figure 7. The importance of this is that it is possible to share functionality 
across Releasing Mechanisms. In addition to transducing a value from sensory input, a Releas­ing Mechanism 
also .lls in a data structure available to the Behav­ior called a Pronome [15]. The Pronome acts like 
a pronoun in English: The use of Pronomes makes it possible for the Behavior to be written in terms of 
it , where it is de.ned by the Behav­ior's Pronome. Thus, a stopNearAndDo Behavior can be imple­mented 
without reference to the kind of object it is stopping near. Pronomes may be shared among behaviors, 
thus allowing the con­struction of a generic .nd and do hierarchy. While the motiva­tion for Pronomes 
comes from theories of mind [15] as opposed to ethology, they make sense in an ethological context as 
well. In any event, the use of Pronomes greatly facilitates the integration of external control, and 
simpli.es the construction of behavior net­works by providing a level of abstraction. 6.3 Internal Variables 
Internal Variables are used to model internal state. Like Releas­ing Mechanisms, Internal Variables express 
their value as a contin­uous value. This value can change over time based on autonomous growth and damping 
rates. In addition, Behaviors can potentially modify the value of an Internal Variable as a result of 
their activity. Both Releasing Mechanisms and Internal Variables may be shared by multiple Behaviors. 
 6.4 Behavior Groups Behaviors are organized into groups of mutually inhibiting behaviors called Behavior 
Groups as shown in Figure 8. While we .nd a loose hierarchical structure useful this is not a requirement 
(i.e. all the Behaviors can be in a single Behavior Group). Behav­ior Groups are important because they 
localize the interaction among Behaviors which facilitates adding new Behaviors. ... Figure 8:Behaviors 
are organized into groups of mutually inhibiting Behaviors called Behavior Groups. These Behavior Groups 
are in turn organized in a loose hierarchical fashion. Behavior Groups at the upper levels of the hierarchy 
contain general types of behaviors (e.g. engage-in-feeding ) which are largely driven by motivational 
considerations, whereas lower levels contain more speci.c behaviors (e.g. pounce or chew ) which are 
driven more by immediate sensory input. The arbitration mechanism built into the algorithm insures that 
only one Behavior in a given Behavior Group will have a non-zero value after inhibition. This Behavior 
is then active, and may either issue primary motor commands, or activate the Behavior Group which contains 
its children Behaviors (e.g. search-for-food , sniff , chew might be the children of engage-in-feeding 
). The dark gray behaviors represent the path of active Behaviors on a given tick. Behaviors which lose 
to the primary Behav­ior in a given Behavior Group may nonetheless in.uence the resulting actions of 
the creature by issuing either secondary or meta-commands. 6.5 Inhibition and Level of Interest A creature 
has only limited resources to apply to satisfying its needs (e.g. it can only walk in one direction at 
a time), and thus there needs to be some mechanism to arbitrate among the compet­ing Behaviors. Moreover, 
once a creature is committed to satisfy­ing a goal, it makes sense for it to continue pursuing that goal 
unless something signi.cantly more important comes along. We rely on a phenomena known as the avalanche 
effect [15] to both arbitrate among Behaviors in a Behavior Group and to pro­vide the right amount of 
persistence. This is done via mutual inhi­bition. Speci.cally, a given Behavior A will inhibit a Behavior 
B by a gain IAB times Behavior A s value. By (a) restricting the inhibitory gains to be greater than 
1, (b) by clamping the value of a Behavior to be 0 or greater, and (c) requiring that all Behaviors inhibit 
each other, the avalanche effect insures that once the sys­tem has settled, only one Behavior in a Behavior 
Group will have a non-zero value. This model of inhibition was .rst proposed, in an ethological context 
by Ludlow [11], but see Minsky as well. This model provides a robust mechanism for winner-take-all arbitration. 
It also provides a way of controlling the relative persis­tence of Behaviors via the use of inhibitory 
gains. When the gains are low, the system will tend to dither among different behaviors. When the gains 
are high, the system will show more persistence. The use of high inhibitory gains can, however, result 
in patho­logical behavior in which the creature pursues a single, but unat­tainable goal, to the detriment 
of less important, but achievable ones. Ludlow addressed this problem by suggesting that a level of interest 
be associated with every Behavior. It is allowed to vary between 0 and 1 and it has a multiplicative 
effect on the Behavior s value.When Behavior is active the level of interest decreases which in turn 
reduces the value of the Behavior regardless of its intrinsic value. Eventually, this will allow another 
Behavior to become active (this is known as time-sharing in the ethological lit­erature). When the behavior 
is no longer active, its level of interest rises. Inhibitory gains and level of interest provide the 
designer with a good deal of control over the temporal aspects of behavior. This is an important contribution 
of this algorithm. 6.6 Use of Primary, Secondary and Meta-commands Being active means that a Behavior 
or one if its children has top priority to issue motor commands. However, it is extremely impor­tant 
that less important Behaviors (i.e. those which have lost the competition for control) still be able 
to express their preferences for actions. This is done by allowing Behaviors which lose to issue suggestions 
in the form of secondary and meta-commands as described earlier. These suggestions are posted prior to 
the win­ning Behavior taking its action, so it can utilize the suggestions as it sees .t. For example, 
a dog may have a behavior which alters the dog s characteristics so as to re.ect its emotional state. 
Thus, the behav­ior may issue secondary commands for posture as well as for the desired ear, tail and 
mouth position, and use a meta-command for the desired gait. The use of a meta-command for gait re.ects 
the fact that the behavior may not know whether the dog should go forward or not. However, it does know 
how it wants the dog to move, should another behavior decide that moving makes sense. Despite the use 
of secondary and meta-commands, the winning behavior still has ultimate say over what actions get performed 
while it is active. It can over-rule a secondary command by remov­ing it from the queue or by executing 
a Motor Skill which grabs a DOF needed by a given secondary command. In the case of meta­commands, the 
winning behavior can choose to ignore the meta­command, in which case it has no effect. 6.7 The Algorithm 
The action-selection algorithm is described below. The actual equations are provided in appendix A. On 
each update cycle: (1) All Internal Variables update their value based on their pre­vious value, growth 
and damping rates, and any feedback effects. (2) Starting at the top-level Behavior Group, the Behaviors 
within it compete to become active. This is done as follows: (3) The Releasing Mechanisms of each Behavior 
update their value based on the current sensory input. This value is then summed with that of the Behavior 
s Internal Variables and the result is multiplied by its Level Of Interest. This is repeated for all 
Behaviors in the group. (4) For each Behavior in the group, the inhibition due to other Behaviors in 
the group is calculated and subtracted from the Behavior s pre-inhibition value and clamped to 0 or greater. 
 (5) If after step (4) more than one Behavior has a non-zero value then step (4) is repeated (using the 
post-inhibition values as the basis) until this condition is met. The Behavior with a non-zero value 
is the active Behavior for the group. (6) All Behaviors in the group which are not active are given 
a chance to issue secondary or meta-commands. This is done by exe­cuting a suggestion callback associated 
with the Behavior. (7) If the active Behavior has a Behavior Group as a child (i.e. it is not a Behavior 
at the leaf of the tree), then that Behavior Group is made the current Behavior Group and the process 
is repeated starting at step (3). Otherwise, the Behavior is given a chance to issue primary motor commands 
via the execution of a callback associated with the Behavior.   7. INTEGRATION OF DIRECTABILITY Having 
described the Behavior and Motor Systems we are now in a position to describe how external control is 
integrated into this architecture. This is done in a number of ways. First, motivational control is provided 
via named access to the Internal Variables which represent the motivations or goals of the Behavior System. 
By adjusting the value of a given motivational variable, the creature can be made more or less likely 
to engage in Behaviors which depend on that variable. Second, all the constituent parts of a Behavior 
are also accessi­ble at run-time, and this provides another mechanism for exerting behavioral control. 
For example, by changing the type of object for which the Releasing Mechanism is looking, the target 
of a given Behavior can easily be altered (e.g. .re hydrants versus user s pants leg ). In addition, 
a Behavior may be made more or less opportunistic by adjusting the maximum allowed strength of its Releasing 
Mechanisms. A Behavior can be made in-active by set­ting its level of interest to zero. Third, the Behavior 
System is structured so that action-selec­tion can be initiated at any node in the system. This allows 
an external entity to force execution of a particular part or branch of the Behavior System, regardless 
of motivational and sensory fac­tors which might otherwise favor execution of other parts of it. Since 
branches often correspond to task-level collections of Behaviors, this provides a form of task-level 
control. Forth, it is easy to provide imaginary sensory input which in turn may trigger certain behaviors 
on the part of the creature. For example, objects may be added to the world which are only visible to 
a speci.c creature.The advantage of this technique is that it does not require the external entity to 
know anything about the internal structure of the creature s Behavior System. The mechanisms described 
above for controlling the Behavior System naturally support both prescriptive and proscriptive con­trol. 
For example, by adjusting the level of a motivational variable which drives a given branch of the Behavior 
System, the director is expressing a weighted preference for or against the execution of that behavior 
or group of behaviors. The multiple imperative forms supported by the Motor Control­ler allow the director 
to express weighted preferences directly at the motor level. For example, at one extreme, the director 
may shut off the Behavior System and issue motor commands directly to the creature. Alternatively, the 
Behavior System can be running, and the director may issue persistent secondary or meta­commands which 
have the effect of modifying or augmenting the output of the Behavior System. For example, the external 
entity might issue a secondary command to wag tail . Unless this was explicitly over-ruled by a Behavior 
in the Behavior System, this would result in the Dog wagging its tail. External meta-commands may also 
take the form of spatial potential .eld maps which can be combined with potential .eld maps generated 
from sensory data to effectively attract or repel the creature from parts of its environ­ment. 8. IMPLEMENTATION 
These ideas have been implemented as part of an object-ori­ented architecture and toolkit for building 
and controlling autono­mous animated creatures. This toolkit is based on Open Inventor 2.0. Most of the 
components from which one builds a creature, including all of the components of the Behavior System, 
and the action-selection algorithm itself are derived from Inventor classes.This allows us to de.ne most, 
of a creature and its Behav­ior System via a text .le using the Inventor .le format. This is important 
for rapid prototyping and quick-turnaround, as well as to facilitate the use of models generated via 
industry-standard model­ers. We also make extensive use of Inventor s ability to provide named access 
to .eld variables at run-time. This is important for integration of external control. Callbacks are used 
to implement most of the custom functionality associated with speci.c Releas­ing Mechanisms and Behaviors. 
This coupled with extensive parameterization reduces the need to create new subclasses. Lastly, an embedded 
Tcl interpreter is provided for interactive run-time control. We have developed several creatures using 
this tool kit. For the Alive project [13], we have developed Silas T. Dog an autono­mous animated dog 
which interacts with a user in a 3D virtual world in a believable manner. Silas responds to a dozen or 
so ges­tures and postures of the user, and responds appropriately (e.g. if the user bends over and holds 
out her hand, Silas moves toward the outstretched hand and eventually sits and shakes his paw). The dog 
always looks at its current object of interest (head, hand, etc.), and when it is sad or happy, its tail, 
ears and head move appropriately. Silas has a number of internal motivations which he is constantly trying 
to satisfy. For example, if his desire to fetch is high, and the user has not played ball with him, he 
will .nd a ball and drop it at the person s feet. Similarly, he will periodically take a break to get 
a drink of water, or satisfy other biological functions. Silas represents roughly 3000 lines of C++ code, 
of which, 2000 lines are for implementing his 24 dog speci.c Motor Skills. He responds to 70 motor commands. 
His Behavior System is com­prised of roughly 40 Behaviors, 11 Behavior Groups, 40 Releasing Mechanisms 
and 8 Internal Variables. Silas runs at 15Hz on a Onyx Reality Engine with rendering and sensing time 
(i.e. the sec­ond render for his synthetic vision) comprising most of the update time. The evaluation 
of the Behavior System itself typically takes less than 6-8 milliseconds. We have also developed a number 
of creatures which are used in the context of an interactive story system [6]. This system fea­tures 
a computational director which provides direction to the creatures so as to meet the requirements of 
the story. For example, at the beginning of the story, a dog hops out of a car and wanders around. If 
the user, who is wearing a head-mounted display, does not pay attention to the dog, the director will 
send the dog over to the user. If the user still does not pay attention, the director effec­tively tells 
the dog: the user s leg is a .ne replacement for a hydrant, and you really have to... . The resulting 
behavior on the part of the dog usually captures the user s attention.  9. CONCLUSION Autonomy and directability 
are not mutually exclusive. We have detailed an architecture and a general behavioral model for perception 
and action-selection which can function autonomously while accepting direction at multiple levels. This 
multi-level direc­tion allows a user to direct at whatever level of detail is desirable. In addition, 
this blend of autonomy and direction is demonstrated with several creatures within the context of two 
applications.  Acknowledgments The authors would like to thank Professors Pattie Maes, Alex P. Pentland 
and Glorianna Davenport for their support of our work. Thanks go, as well, to the entire ALIVE team for 
their help. In par­ticular, thanks to Bradley Rhodes for suggesting the use of pro­nomes. We would also 
like to thank Taylor Galyean and Marilyn Feldmeier for their work modeling the setting and Lucky. This 
work was funded in part by Sega North America and the Television of Tomorrow Consortium. Appendix A. 
Behavior Update Equation: = Max .·Combine( , ) ·v.,0 vit .liit .rmki .iv jt .nmi mt.kj m Where at time 
t for Behavior i,vit is its value; liit is the level of interest; rmkt and ivjt are the values of Releasing 
Mechanism k, and Internal Variable j, where k and j range over the Releasing Mecha­nisms and Internal 
Variables relevant to Behavior i; nmi (n>1) is the Inhibitory Gain that Behavior m applies against Behavior 
i; vmt is the value of Behavior m, where m ranges over the other Behav­iors in the current Behavior Group. 
Combine() is the function used to combine the values of the Releasing Mechanisms and Internal Variables 
for Behavior i (i.e addition or multiplication). Internal Variable Update Equations: ivit = (ivit( 1)·dampi)+ 
growthi .effectskit k Where at time t for Internal Variable i,ivit is its value; ivi(t-1) is its value 
on the previous time step; dampi and growthi are damping rates and growth rates associated with Internal 
Variable i; and effectskit are the adjustments to its value due to the activity of Behavior k, where 
k ranges over the Behaviors which directly effect its value when active. effectskit = (modifyGainki ·) 
vkt( 1) Where effectskit is the effect of Behavior k on Internal Variable i at time t; modifyGainki is 
the gain used by Behavior k against Inter­nal Variable i and vk(t-1) is the value of Behavior k in the 
preceding time step. Level of Interest Update Equation: = Clamp( (( )·dampi)+ growthi liit liit( 1 )·bRatei)),01) 
(, vit( 1 Where liit is the Level Of Interest of Behavior i at time t, and bRatei is the boredom rate 
for Behavior i. Clamp(x,y,z) clamps x to be between y and z. Note Level Of Interest is just a special 
case of an Internal Variable. Releasing Mechanism Update Equation: = Clamp TemporalFilter (t , ( rm rmit 
, it( 1) Find s( it,dMini,dMaxi) ·Filter ()· sitWeight (,dOpti)),mini,maxi) sit Where rmit is the value 
of Releasing Mechanism i at time t; sit is the relevant sensory input for i; dMini and dMaxi are minimum 
and maximum distances associated with it; Find() returns 1 or 0 if the object of interest is found within 
sit and within dMini to dMaxi; Filter() returns 1 or 0 if the object of interest matches some addi­tional 
criteria; Weight() weights the strength of the stimulus based on some metric such as optimal distance 
dOpti; TemporalFilter() applies a .ltering function (latch, average, integration, or immedi­ate) over 
some period t; and Clamp() clamps the resulting value to the range mini to maxi. REFERENCES 1. Arkin, 
R.C., Integrating Behavioral, Perceptual, and World Knowledge in Reactive Navigation, in Designing Autonomous 
Agents, P. Maes, Editor. 1990, MIT Press, Cambridge, pp.105-122. 2. Badler, N.I., C. Phillips, and B.L. 
Webber, Simulating Humans: Computer Graphics, Animation, and Control. 1993, Oxford University Press, 
New York. 3. Blumberg, B. Action-Selection in Hamsterdam: Lessons from Ethology. in Third International 
Conference on the Simulation of Adaptive Behavior. Brighton, England,1994, MIT Press. pp.108­  117. 
4. Brooks, R., A Robust Layered Control System for a Mobile Robot. 1986. IEEE Journal of Robotics and 
Automation RA-2.pp. 14-23. 5. Bruderlin, Armin and Thomas W. Calvert. Dynamic Anima­tion of Human Walking. 
Proceedings of SIGGRAPH 89 (Boston, MA, July 31-August 4, 1989). In Computer Graphics 23, 3 (July 1989), 
233-242. 6. Galyean, T. A. Narrative Guidance of Interactivity, Ph.D. Dissertation, Massachusetts Institute 
of Technology, 1995. 7. Girard, Michael and A. A. Maciejewski. Computational Mod­eling for the Computer 
Animation of Legged Figures. Proceedings of SIGGRAPH 85 (San Francisco, CA, July 22-26, 1985). In Com­puter 
Graphics 19, 263-270. 8. Horswill, I. A Simple, Cheap, and Robust Visual Navigation System. in Second 
International Conference on the Simulation of Adaptive Behavior. Honolulu, HI, 1993. MIT Press, pp.129-137. 
 9. Koga, Yoshihito, Koichi Kondo, James Kuffner, and Jean-Claude Latombe. Planning Motion with Intentions. 
Proceedings of SIGGRAPH 94 (Orlando, FL, July 24-29, 1994). In Computer Graphics Proceedings, Annual 
Conference Series, 1994, ACM, SIGGRAPH, pp. 395-408. 10. Latombe, J. C., Robot Motion Planning. 1991, 
Kluwer Aca­demic Publishers, Boston. 11. Ludlow, A., The Evolution and Simulation of a Decision Maker, 
in Analysis of Motivational Processes, F.T.&#38;.T. Halliday, Editor. 1980, Academic Press, London. 
12. Maes, P., Situated Agents Can Have Goals. Journal for Robotics and Autonomous Systems 6(1&#38;2), 
1990, pp. 49-70. 13. Maes, P., T. Darrell, and B. Blumberg. The ALIVE System: Full-body Interaction 
with Autonomous Agents. in Proceedings of Computer Animation 95 Conference, Switzerland, April 1995, 
IEEE Press, pp. 11-18. 14. McKenna, Michael and David Zeltzer. Dynamic Simulation of Autonomous Legged 
Locomotion. Proceedings of SIGGRAPH 90 (Dallas, TX, August 6-10, 1990). In Computer Graphics 24, 4 (August 
1990), pp.29-38. 15. Minsky, M., The Society of Mind. 1988, Simon &#38; Schuster, New York. 16. Raibert, 
Marc H. and Jessica K. Hodgins. Animation of Dynamic Legged Locomotion. Proceedings of SIGGRAPH 91 (Las 
Vegas, NV, July 28-August 2, 1991). In Computer Graphics 25, 4 (July 1991), 349 358 17. Renault, O., 
N. Magnenat-Thalmann, D. Thalmann. A vision­based approach to behavioral animation. The Journal of Visualiza­tion 
and Computer Animation 1(1),1990, pp.18-21. 18. Reynolds, Craig W. Flocks, Herds, and Schools: A Distrib­uted 
Behavioral Model. Proceedings of SIGGRAPH 87 (Anaheim, CA, July 27-31, 1987). In Computer Graphics 21, 
4 (July 19987), 25-34. 19. Tinbergen, N., The Study of Instinct. 1950, Clarendon press, Oxford. 20. 
Tu, Xiaoyuan and Demetri Terzopoulos. Arti.cial Fishes: Physics, Locomotion, Perception, Behavior. Proceedings 
of SIG-GRAPH 94 (Orlando, FL, July 24-29, 1994). In Computer Graph­ics Proceedings, Annual Conference 
Series, 1994, ACM, SIGGRAPH, pp. 43-50. 21. Tyrrell, T. The Use of Hierarchies for Action Selection, 
in  Second International Conference on the Simulation of Adaptive Behavior. 1993. MIT Press, pp.138-147. 
22. Wilhelms J., R. Skinner. A 'Notion' for Interactive Behavioral Animation Control. IEEE Computer Graphics 
and Applications 10(3) May 1990, pp. 14-22. 23. David Zeltzer, Task Level Graphical Simulation: Abstraction, 
Representation and Control, in Making Them Move. Ed. Badler, N., Barsky, B., and Zeltzer D. 1991, Morgan 
Kaufmann Publish­ers, Inc.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218407</article_id>
		<sort_key>55</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Realistic modeling for facial animation]]></title>
		<page_from>55</page_from>
		<page_to>62</page_to>
		<doi_number>10.1145/218380.218407</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218407</url>
		<keywords>
			<kw><![CDATA[RGB/Range scanners]]></kw>
			<kw><![CDATA[discrete deformable models]]></kw>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[feature-based facial adaptation]]></kw>
			<kw><![CDATA[physics-based facial modeling]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P307748</person_id>
				<author_profile_id><![CDATA[81100386220]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuencheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto, Department of Computer Science, 10 King's College Road, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109166</person_id>
				<author_profile_id><![CDATA[81100294834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Demetri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto, Department of Computer Science, 10 King's College Road, Toronto, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31100025</person_id>
				<author_profile_id><![CDATA[81100026581]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Keith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Waters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Equipment Corporation, Cambridge Research Lab., One Kendall Square, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>617851</ref_obj_id>
				<ref_obj_pid>616029</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Akimoto, Y. Suenaga, and R. Wallace. Automatic creation of 3D facial models. IEEE Computer Graphics and Applications, 13(5): 16- 22, September 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[James Doyle and James Philips. Manual on Experimental Stress Analysis. Society for Experimental Mechanics, fifth edition, 1989.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Irfan A. Essa. Visual Interpretation of Facial Expressions using Dynamic Modeling. PhD thesis, MIT, 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Frick and Hans. Human Anatomy, volume 1. Thieme Medical Publishers, Stuttgart, 1991.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Gray. Anatomy of the Human Body. Lea &amp; Febiber, Philadelphia, PA, 29th edition, 1985.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Brian Guenter. A system for simulating human facial expression. In State of the Art in Computer Animation, pages 191-202. Springer- Verlag, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[T. Kurihara and K. Arai. A transformation method for modeling and animation of the human face from photographs. In State of the Art in Computer Animation, pages 45-57. Springer-Verlag, 1991.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Y.C. Lee, D. Terzopoulos, and K. Waters. Constructing physics-based facial models of individuals. In Proceedings of Graphics Interface '93, pages 1-8, Toronto, May 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[N. Magneneat-Thalmann, H. Minh, M. Angelis, and D. Thalmann. Design, transformation and animation of human faces. Visual Computer, 5:32-39,1989.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134452</ref_obj_id>
				<ref_obj_pid>134450</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[D. Metaxas and E. Milios. Reconstruction of a color image from nonuniformly distributed sparse and noisy data. Computer Vision, Graphics, and Image Processing, 54(2):103-111, March 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>113038</ref_obj_id>
				<ref_obj_pid>113034</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Nahas, H. Hutric, M. Rioux, and J. Domey. Facial image synthesis using skin texture recording. Visual Computer, 6(6):337-343,1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37424</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Oka, K. Tsutsui, A. Ohba, Y. Kurauchi, and T. Tago. Real-time manipulation of texture-mapped surfaces. In SIGGRAPH 21, pages 181-188. ACM Computer Graphics, 1987.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>569955</ref_obj_id>
				<ref_obj_pid>800193</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[E Parke. Computer generated animation of faces. In ACM National Conference, pages 451-457. ACM, 1972.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[E Parke. Parameterized models for facial animation. IEEE Computer Graphics and Applications, 2(9):61-68, November 1982.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[E Parke. Parameterized models for facial animation revisited. In SIG- GRAPH Facial Animation Tutorial Notes, pages 43-56. ACM SIG- GRAPH, 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Elizabeth C. Patterson, Peter C. Litwinowicz, and N. Greene. Facial animation by spatial mapping. In State of the Art in Computer Animation, pages 31-44. Springer-Verlag, 1991.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806812</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[S. Platt and N. Badler. Animating facial expression. Computer Graphics, 15(3):245-252, August 1981.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>48905</ref_obj_id>
				<ref_obj_pid>48904</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos. The computation of visible-surface representations. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-10(4):417-438, 1988.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and M. Vasilescu. Sampling and reconstruction with adaptive meshes. In Proceedings of Computer Vision and Pattern Recognition Conference, pages 70-75. IEEE, June 1991.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and K. Waters. Physically-based facial modeling, analysis, and animation. Visualization and ComputerAnimation, 1:73- 80, 1990.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37405</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[K. Waters. A muscle model for animating three-dimensional facial expression. Computer Graphics, 22(4): 17-24,1987.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[K. Waters. A physcial model of facial tissue and muscle articulation derived from computer tomography data. In Visualization in Biomedical Computing, pages 574-583. SPIE, Vol. 1808, 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[K. Waters and D. Terzopoulos. Modeling and animating faces using scanned data. Visualization and Computer Animation, 2:123-128, 1991.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97906</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[L. Williams. Performance-driven facial animation. In SIGGRAPH 24, pages 235-242. ACM Computer Graphics, 1990.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. Yau and N. Duffy. 3-D facial animation using image samples. InNew Trends in Computer Graphics, pages 64-73. Springer-Verlag, 1988.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Realistic Modeling for Facial Animation Yuencheng Lee1, Demetri Terzopoulos1, and Keith Waters2 University 
of Toronto1and Digital Equipment Corporation2 Abstract A major unsolved problem in computer graphics 
is the construc­tion and animation of realistic human facial models. Traditionally, facial models have 
been built painstakingly by manual digitization and animated by ad hoc parametrically controlled facial 
mesh defor­mations or kinematic approximation of muscle actions. Fortunately, animators are now able 
to digitize facial geometries through the use of scanning range sensors and animate them through the 
dynamic simulation of facial tissues and muscles. However, these techniques require considerableuserinputto 
constructfacialmodelsofindivid­uals suitable for animation. In this paper, we present a methodology for 
automating this challenging task. Starting with a structured fa­cial mesh, we develop algorithms that 
automatically construct func­tional models of the heads of human subjects from laser-scanned range and 
re.ectance data. These algorithms automatically insert contractile muscles at anatomically correct positions 
within a dy­namic skin model and root them in an estimated skull structure with a hinged jaw. They also 
synthesize functional eyes, eyelids, teeth, and a neck and .t them to the .nal model. The constructed 
face may be animated via muscle actuations. In this way, we create the most authentic and functional 
facial models of individuals available to date and demonstrate their use in facial animation. CR Categories: 
I.3.5 [Computer Graphics]: Physically based modeling; I.3.7 [Computer Graphics]: Animation. Additional 
Keywords: Physics-based Facial Modeling, Facial Animation, RGB/Range Scanners, Feature-Based Facial Adapta­tion, 
Texture Mapping, Discrete Deformable Models.  Introduction Two decades have passed since Parke s pioneering 
work in ani­mating faces [13]. In the span of time, signi.cant effort has been devoted to the development 
of computational models of the human face for applications in such diverse areas as entertainment, low 
bandwidth teleconferencing, surgical facial planning, and virtual reality. However, the task of accurately 
modeling the expressive human face by computer remains a major challenge. Traditionally,computerfacialanimationfollows 
threebasicpro­cedures: (1) design a 3D facial mesh, (2) digitize the 3D mesh, and (3) animate the 3D 
mesh in a controlled fashion to simulate facial actions. In procedure (1), it is desirable to have a 
re.ned topological mesh that captures the facial geometry. Often this entails digitizing 1Department 
of Computer Science, 10 King s College Road, Toronto, ON, Canada, M5S 1A4. {vlee j dt}@cs.toronto.edu 
2Cambridge Research Lab., One Kendall Square, Cambridge, MA 02139. waters@crl.dec.com Permission to make 
digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage, the copyright notice, the 
title of the publication and its date appear, and notice is given that copying is by permission of ACM, 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 as many nodes as possible. 
Care must be taken not to oversample the surface because there is a trade-off between the number of nodes 
and the computational cost of the model. Consequently, meshes developed to date capture the salient features 
of the face with as few nodes as possible (see [17, 14, 21, 9, 23] for several different mesh designs). 
In procedure (2), a general 3D digitization technique uses pho­togrammetry of several images of the face 
taken from different angles. A common technique is to place markers on the face that can be seen from 
two or more cameras. An alternative technique is to manually digitize a plaster cast of the face using 
manual 3D dig­itization devices such as orthogonal magnetic .elds sound captors [9], or one to two photographs 
[9, 7, 1]. More recently, automated laser range .nders can digitize on the order of 1053D points from 
a solid object such as a person s head and shoulders in just a few seconds [23]. In procedure (3), an 
animator must decide which mesh nodes to articulate and how much they should be displaced in order to 
produce a speci.c facial expression. Various approaches have been proposed for deforming a facial mesh 
to produce facial expres­sions; for example, parameterized models [14, 15], control-point models [12, 
7], kinematic muscle models [21, 9], a texture-map­assembly model [25], a spline model [11], feature-tracking 
mod­els [24, 16], a .nite element model [6], and dynamic muscle mod­els [17, 20, 8, 3]. 1.1 Our Approach 
The goal of our work is to automate the challenging task of cre­ating realistic facial models of individuals 
suitable for animation. We develop an algorithm that begins with cylindrical range and re­.ectance data 
acquired by a Cyberware scanner and automatically constructs an ef.cient and fully functional model of 
the subject s head, as shown in Plate 1. The algorithm is applicable to various individuals (Plate 2 
shows the raw scans of several individuals). It proceeds in two steps: In step 1, the algorithm adapts 
a well-structured face mesh from [21] to the range and re.ectance data acquired by scanning the sub­ject, 
thereby capturing the shape of the subject s face. This approach has signi.cant advantages because it 
avoids repeated manual modi.­cation of control parameters to compensate for geometric variations in the 
facial features from person to person. More speci.cally, it allows the automatic placement of facial 
muscles and enables the use of a single control process across different facial models. The generic face 
mesh is adapted automatically through an im­age analysis technique that searches for salient local minima 
and maxima in the range image of the subject. The search is directed according to the known relative 
positions of the nose, eyes, chin, ears, and other facial features with respect to the generic mesh. 
Facial muscle emergence and attachment points are also known rel­ative to the generic mesh and are adapted 
automatically as the mesh is conformed to the scanned data. In step 2, the algorithm elaborates the geometric 
model con­structed in step 1 into a functional, physics-based model of the subject s face which is capable 
of facial expression, as shown in the lower portion of Plate 1. Wefollowthe physics-basedfacialmodelingapproachproposed 
by Terzopoulos and Waters [20]. Its basic features are that it ani­mates facial expressions by contracting 
synthetic muscles embed­ded in an anatomically motivated model of skin composed of three spring-mass 
layers. The physical simulation propagates the muscle forces through the physics-based synthetic skin 
thereby deforming theskintoproducefacialexpressions. Amongtheadvantagesofthe physics-based approach are 
that it greatly enhances the degree of realism over purely geometric facial modeling approaches, while 
re­ducing the amount of work that must be done by the animator. It can be computationally ef.cient. It 
is also amenable to improvement, with an increase in computational expense, through the use of more sophisticated 
biomechanical models and more accurate numerical simulation methods. We propose a more accurate biomechanical 
model for facial animation compared to previous models. We develop a new biome­chanical facial skin model 
which is simpler and better than the one proposed in [20]. Furthermore, we argue that the skull is an 
impor­tant biomechanical structure with regard to facial expression [22]. To date, the skin-skull interface 
has been underemphasizedin facial animation despite its importance in the vicinity of the articulate 
jaw; therefore we improve upon previous facial models by developing an algorithm to estimate the skull 
structure from the acquired range data, and prevent the synthesized facial skin from penetrating the 
skull. Finally, our algorithm includes an articulated neck and synthe­sizes subsidiary organs, including 
eyes, eyelids, and teeth, which cannot be adequately imaged or resolved in the scanned data, but which 
are nonetheless crucial for realistic facial animation.  2 Generic Face Mesh and Mesh Adaptation The 
.rst step of our approach to constructing functional facial mod­els of individuals is to scan a subject 
using a Cyberware Color DigitizerTM. The scanner rotates 360degrees around the subject, who sits motionless 
on a stool as a laser stripe is projected onto the head and shoulders. Once the scan is complete, the 
device has acquired two registered images of the subject: a range image (Figure 1) a topographic map 
that records the distance from the sensorto points onthefacialsurface,anda re.ectance(RGB) image (Figure 
2) which registers the color of the surface at those points. The images are in cylindrical coordinates, 
with longitude (0 360) degrees along the x axis and vertical height along the y axis. The resolution 
of the images is typically 512.256pixels (cf. Plate 1) The remainder of this section describes an algorithm 
which re­duces the acquired geometric and photometric data to an ef.cient geometric model of the subject 
s head. The algorithm is a two-part process which repairs defects in the acquired images and conforms 
a generic facial mesh to the processed images using a feature-based matching scheme. The resulting mesh 
captures the facial geometry as a polygonal surface that can be texture mapped with the full res­olution 
re.ectance image, thereby maintaining a realistic facsimile of the subject s face. 2.1 Image Processing 
One of the problems of range data digitization is illustrated in Fig­ure 1(a). In the hair area, in the 
chin area, nostril area, and even in the pupils, laser beams tend to disperse and the sensor observes 
no range value for these corresponding 3D surface points. We must correct for missing range and texture 
information. We use a relaxation method to interpolate the range data. In particular, we apply a membrane 
interpolation method described in [18]. The relaxation interpolates values for the missing points so 
as to bring them into successively closer agreement with surrounding points by repeatedly indexing nearest 
neighbor values. Intuitively, it stretches an elastic membrane over the gaps in the surface. The images 
interpolated through relaxation are shown in Figure 1(b) and (a) (b) Figure 1: (a) Range data of Grace 
from a Cyberware scanner. (b) Recovered plain data. illustrate improvements in the hair area and chin 
area. Relaxation works effectively when the range surface is smooth, and particularly in the case of 
human head range data, the smoothness requirement of the solutions is satis.ed quite effectively. Figure 
2(a) shows two 512.256re.ectance (RGB) texture maps as monochrome images. Each re.ectance value represents 
the surface color of the object in cylindrical coordinates with cor­responding longitude (0 360degrees) 
and latitude. Like range images, the acquired re.ectance images are lacking color informa­tion at certain 
points. This situation is especially obvious in the hair area and the shoulder area (see Figure 2(a)). 
We employ the membrane relaxation approach to interpolate the texture image by repeated averaging of 
neighboring known colors. The original tex­ture image in Figure 2(a) can be compared with the interpolated 
texture image in Figure 2(b). (a) (b) Figure 2: (a) Texture data of George with void points displayed 
in white and (b) texture image interpolated using relaxation method. The method is somewhat problematic 
in the hair area where range variations may be large and there is a relatively high percent­age of missing 
surface points. A thin-plate relaxation algorithm [18] may be more effective in these regions because 
it would .ll in the larger gaps with less .attening than a membrane [10]. Although the head structure 
in the cylindrical laser range data is distorted along the longitudinal direction, important features 
such as the slope changes of the nose, forehead, chin, and the contours of the mouth, eyes, and nose 
are still discernible. In order to locate the contours of those facial features for use in adaptation 
(see below), we use a modi.ed Laplacian operator (applied to the discrete image through local pixel differencing) 
to detect edges from the range map shown in Figure 3(a) and produce the .eld function in Fig. 3(b). For 
details about the operator, see [8]. The .eld function highlights important features of interest. For 
example, the local maxima of the modi.ed Laplacian reveals the boundaries of the lips, eyes, and chin. 
 2.2 Generic Face Mesh and Mesh Adaptation The next step is to reduce the large arrays of data acquired 
by the scanner into a parsimonious geometric model of the face that can eventually be animated ef.ciently. 
Motivated by the adaptive mesh­ing techniques [19] that were employed in [23], we signi.cantly (a) (b) 
Figure 3: (a) Original range map. (b) Modi.ed Laplacian .eld function of (a). improved the technique 
by adapting a generic face mesh to the data. Figure 4 shows the planar generic mesh which we obtain through 
a cylindrical projection of the 3D face mesh from [21]. One of the advantages of the generic mesh is 
that it has well-de.ned features which form the basis for accurate feature based adaptation to the scanned 
data and automatic scaling and positioning of facial mus­cles as the mesh is deformed to .t the images. 
Another advantage is that it automatically produces an ef.cient triangulation, with .ner triangles over 
the highly curved and/or highly articulate regions of the face, such as the eyes and mouth, and larger 
triangles elsewhere. Figure 4: Facial portion of generic mesh in 2D cylindrical coordi­nates. Dark lines 
are features for adaptation. We label all facial feature nodes in the generic face prior to the adaptation 
step. The feature nodes include eye contours, nose contours, mouth contours, and chin contours. For any 
speci.c range image and its positive Laplacian .eld function (Figure 3), the generic mesh adaptation 
procedureperforms the following steps to locate feature points in the range data (see [8] for details): 
Mesh Adaptation Procedures 1. Locate nose tip 6. Locate eyes 2. Locate chin tip 7. Activate spring forces 
 3. Locate mouth contour 8. Adapt hair mesh 4. Locate chin contour 9. Adapt body mesh 5. Locate ears 
10. Store texture coordinates  Once the mesh has been .tted by the above feature based match­ing technique 
(see Plate 3), the algorithm samples the range image at the location of the nodes of the face mesh to 
capture the facial geometry, as is illustrated in Figure 5. The node positions also provide texture map 
coordinates that are used to map the full resolution color image onto the triangles (see Plate 3).  
2.3 Estimation of Relaxed Face Model Ideally, the subject s face should be in a neutral, relaxed expression 
when he or she is being scanned. However, the scanned woman in  (a) (b) Figure 5: (a) Generic geometric 
model conformed to Cyberware scan of Heidi . (b) Same as (a). Note that Heidi s mouth is now closed, 
subsequent to estimation of the relaxed face geometry. the Heidi dataset is smiling and her mouth is 
open (see Plate 2). We have made our algorithm tolerant of these situations. To con­struct a functional 
model, it is important to .rst estimate the relaxed geometry. That is, we must infer what the Heidi subject 
would look like had her face been in a relaxed pose while she was be­ing scanned. We therefore estimate 
the range values of the closed mouth contour from the range values of the open mouth contour by the following 
steps: 1. Perform adaptation procedures in Sec. 2.2 without step 3. 2. Store nodal longitude/latitude 
into adapted face model. 3. Perform lip adaptation in step 3 in sec. 2.2 4. Store nodal range values 
into adapted face model.  As a result, the .nal reconstructed face model in Figure 5(b) will have a 
relaxed mouth because the longitude and latitude recorded is the default shape of our closed mouth model 
(see Figure 4). Moreover, the shape of the .nal reconstructed face is still faithful to the head data 
because the range value at each facial nodal point is obtained correctly after the lip adaptation procedure 
has been performed. Relaxing the face shown in Figure 5(a) results in the image in Figure 5(b) (with 
eyelids inserted see below).  3 The Dynamic Skin and Muscle Model This section describes how our system 
proceeds with the construc­tion of a fully functional model of the subject s face from the facial mesh 
produced by the adaptation algorithm described in the previ­ous section. To this end, we automatically 
create a dynamic model of facial tissue, estimate a skull surface,and insert the major muscles of facial 
expression into the model. The following sections describe each of these components. We also describe 
our high-performance parallel, numerical simulation of the dynamic facial tissue model. 3.1 Layered Synthetic 
Tissue Model The skull is covered by deformable tissue which has .ve distinct layers [4]. Four layers 
epidermis, dermis, sub-cutaneous connec­tive tissue, and fascia comprise the skin, and the .fth consists 
of the muscles of facial expression. Following [20], and in accordance with the structure of real skin 
[5], we have designed a new, synthetic tissue model (Figure 6(a)). The tissue model is composed of triangular 
prism elements (see Figure 6(a)) which match the triangles in the adapted facial mesh. The epidermal 
surface is de.ned by nodes 1, 2, and 3, which are connected by epidermal springs. The epidermis nodes 
are also connected by dermal-fatty layer springs to nodes 4, 5, and 6, which de.ne the fascia surface. 
Fascia nodes are interconnected by fascia (a) (b) Figure 6: (a) Triangular skin tissue prism element. 
(b) Close-up view of right side of an individual with conformed elements. springs. They are also connected 
by muscle layer springs to skull surface nodes 7, 8, 9. Figure 9(b) shows 684such skin elements assembled 
into an extended skin patch. Several synthetic muscles are embedded into the muscle layer of the skin 
patch and the .gure shows the skin deformation due to muscle contraction. Muscles are .xed in an estimated 
bony subsurface at their point of emergence and are at­tached to fascia nodes as they run through several 
tissue elements. Figure 6(b) shows a close-up view of the right half of the facial tissue model adapted 
to an individual s face which consists of 432 elements.  3.2 Discrete Deformable Models (DDMs) A discrete 
deformable model has a node-spring-node structure, which is a uniaxial .nite element. The data structure 
for the node consists of the nodal mass mi, position xi(t)= xi(t)yi(t)zi(t)]0 , velocity vi=dxi/dt, acceleration 
ai=d2 xi/dt2, and net nodal n forces fi(t). The data structure for the spring in this DDM consists of 
pointers to the head node iand the tail node jwhich the spring interconnects, the natural or rest length 
lkof the spring, and the spring stiffness ck.  3.3 Tissue Model Spring Forces By assembling the discrete 
deformable model according to histolog­ical knowledge of skin (see Figure 6(a)), we are able to construct 
an anatomically consistent, albeit simpli.ed, tissue model. Figure 6(b) shows a close-up view of the 
tissue model around its eye and nose parts of a face which is automatically assembled by following the 
above approach. The force spring jexerts on node iis 28 of the primary facial muscles, including the 
zygomatic major and minor, frontalis, nasii, corrugator, mentalis, buccinator, and angulii depressor 
groups. Plate 4 illustrates the effects of automatic scaling and positioning of facial muscle vectors 
as the generic mesh adapts to different faces. To better emulate the facial muscle attachments to the 
fascia layer in our model, a group of fascia nodes situated along the muscle path i.e., within a predetermined 
distance from a central muscle vector, in accordance with the muscle width experience forces from the 
contraction of the muscle. The face construction algorithm determines the nodes affected by each muscle 
in a precomputation step. To apply muscle forces to the fascia nodes, we calculate a force for each node 
by multiplying the muscle vector with a force length scaling factor and a force width scaling factor 
(see Figure 7(a)). Function 01(Figure 8(a)) scales the muscle force according to the length ratio Ej,i, 
while 02(Figure 8(b)) scales it according to the width Wj,iat node iof muscle j: F AF jj Ej,i=((m-xi)·mj)/(km-mjk) 
Wj,i=kpi-(pi·nj)njk The force muscle jexerts on node iis j fi=01(Ej,i)02(Wj,i)mj 01scales the force 
according to the distance ratio Ej,i, where Ej,i=pj,i/dj, with djthe muscle jlength.  02scales the force 
according to the width ratio Wj,i/wj, with wjthe muscle jwidth.  mjis the normalized muscle vector for 
muscle j  Note that the muscle force is scaled to zero at the root of the muscle .ber in the bone and 
reaches its full strength near the end of the muscle .ber. Figure 9(b) shows an example of the effect 
of muscle forces applied to a synthetic skin patch. x i . x i j,i n j p i m jF A m j m j . j,i linear 
muscle fiber j  segment l of piecewise linear muscle fiber j (a) (b) Figure 7: (a) Linear muscle .ber. 
(b) Piecewise linear muscle .ber. gj=cj(lj-lr)sj j each layer has its own stress-strain relationship 
cjand the dermal-fatty layer uses biphasic springs (non-constant cj) [20]  ljrand lj=jjxj-xijjare the 
rest and current lengths for spring j  sj=(xj-xi)/ljis the spring direction vector for spring j   
3.4 Linear Muscle Forces The muscles of facial expression, or the muscular plate, spreads out below the 
facial tissue. The facial musculature is attached to the skin tissue by short elastic tendons at many 
places in the fascia, but is .xed to the facial skeleton only at a few points. Contractions of the facial 
muscles cause movement of the facial tissue. We model 1.10 1.00 1.00 0.90 0.90 0.80 0.80 0.70 0.70 0.60 
0.60 0.50 0.50 0.40 0.40 0.30 0.30 0.20 0.20 0.10 0.10 0.00 0.00 0.00 0.20 0.40 0.60 0.80 1.00 0.00 
0.20 0.40 0.60 0.80 1.00 (a) (b) Figure 8: (a) Muscle force scaling function 01wrt Ej,i,(b) Muscle force 
scaling function 02wrt Wj,i/wj 3.5 Piecewise Linear Muscle Forces In addition to using linear muscle 
.bers in section 3.4 to simulate sheet facial muscles like the frontalis and the zygomatics, we also 
model sphincter muscles, such as the orbicularis oris circling the mouth, by generalizing the linear 
muscle .bers to be piecewise linear and allowing them to attach to fascia at each end of the segments. 
Figure 7(b) illustrates two segments of an N-segment ll+1 piecewise linear muscle jshowing three nodes 
mj, mj,and l+2 mj. The unit vectors mj,l, mj,l+1and nj,l, nj,l+1are parallel and normal to the segments, 
respectively. The .gure indicates fascia node iat xi, as well as the distance pj,i=a+b, the width Wj,i, 
and the perpendicular vector pifrom fascia node ito the nearest segment of the muscle. The length ratio 
Ej,ifor fascia node iin muscle .ber jis P l+1 Nk+1 k (m-xi)·+ km-mk j mj,lk l+1 jj Ej,i= P N k+1 k km-mk 
k 1 jj The width Wj,icalculation is the same as for linear muscles. The remaining muscle force computations 
are the same as in sec­tion 3.4. Plate 4 shows all the linear muscles and the piecewise linear sphincter 
muscles around the mouth.  3.6 Volume Preservation Forces In order to faithfully exhibit the incompressibility 
[2] of real human skin in our model, a volume constraint force based on the change of volume (see Figure 
9(a)) and displacements of nodes is calculated and applied to nodes. In Figure 9(b) the expected effect 
of volume preservation is demonstrated. For example, near the origin of the muscle .ber, the epidermal 
skin is bulging out, and near the end of the muscle .ber, the epidermal skin is depressed. The volume 
preservation force element eexerts on nodes iin element eis e eee ee h qi =k1(V-V)ni+k2(pi -phi) h Veand 
Veare the rest and current volumes for e  n eiis the epidermal normal for epidermal node i  pheiand 
p eiare the rest and current nodal coordinates for node iwith respect to the center of mass of e  k1k2are 
force scaling constants  1, 2, 3, 4, 5, 6 Skull Penetration Penalized Nodes 4, 5, 6 Fascia Nodes 4, 
5, 6 (a) (b) Figure 9: (a) Volume preservation and skull nonpenetration ele­ ment. (b) Assembled layered 
tissue elements under multiple muscle forces. 3.7 Skull Penetration Constraint Forces Because of the 
underlying impenetrable skull of a human head, the facial tissue during a facial expression will slide 
over the underlying bony structure. With this in mind, for each individual s face model reconstructed 
from the laser range data,we estimate the skull surface normals to be the surface normals in the range 
data image. The skull is then computed as an offset surface. To prevent nodes from penetrating the estimated 
skull (see Figure 9(a)), we apply a skull non-penetration constraint to cancel out the force component 
on the fascia node which points into the skull; therefore, the resulting force will make the nodes slide 
over the skull. The force to penalize fascia node iduring motion is: { -(fin ·ni)niwhen fin ·ni<0 si 
= 0 otherwise fn iis the net force on fascia node i  niis the nodal normal of node i   3.8 Equations 
of Motion for Tissue Model Newton s law of motion governs the response of the tissue model to forces. 
This leads to a system of coupled second order ODEs that relate the node positions, velocities, and accelerations 
to the nodal forces. The equation for node iis d2 xi dxi h mi +Ii +ghi+qhi+hsi+h i =fi dt2dt miis the 
nodal mass,  Iiis the damping coef.cient,  ghiis the total spring force at node i,  qhiis the total 
volume preservation force at node i,  hsiis the total skull penetration force at node i, h iis the 
total nodal restoration force at node i,  h fiis the total applied muscle force at node i,  3.9 Numerical 
Simulation The solution to the above system of ODEs is approximated by using the well-known, explicit 
Euler method. At each iteration, the nodal acceleration at time tis computed by dividing the net force 
by nodal mass. The nodal velocity is then calculated by integrating once, and another integration is 
done to compute the nodal positions at the next time step t+t, as follows: t 1tttttt ai =(hfi -Iivi -ghi 
-qhi -shi -hhi) mi t+ ttt v=v+ta iii t+ ttt+ t x =x+tv iii  3.10 Default Parameters The default parameters 
for the physical/numerical simulation and the spring stiffness values of different layers are as follows: 
Mass (m) Time step ( t) Damping (I) 0.5 0.01 30 Epid Derm-fat 1 Derm-fat 2 Fascia Muscle c 60 30 70 
80 10  3.11 Parallel Processing for Facial Animation The explicit Euler method allows us to easily 
carry out the numerical simulation of the dynamic skin/muscle model in parallel. This is becauseat eachtime 
stepall the calculationsare basedontheresults from the previous time step. Therefore, parallelization 
is achieved by evenly distributing calculations at each time step to all available processors. This parallel 
approach increases the animation speed to allow us to simulate facial expressions at interactive rates 
on our Silicon Graphics multiprocessor workstation.  4 Geometry Models for Other Head Components To 
complete our physics-based face model, additional geometric models are combined along with the skin/muscle/skull 
models de­veloped in the previous section. These include the eyes, eyelids, teeth, neck, hair, and bust 
(Figure 10). See Plate 5 for an example of a complete model. (a) (b) (c)   4.1 Eyes Eyes are constructed 
from spheres with adjustable irises and ad­justable pupils (Figure 10(a)). The eyes are automatically 
scaled to .t the facial model and are positioned into it. The eyes rotate kinematically in a coordinated 
fashion so that they will always con­verge on a speci.ed .xation point in three-dimensional space that 
de.nes the .eld of view. Through a simple illumination computa­tion, the eyes can automatically dilate 
and contract the pupil size in accordance with the amount of light entering the eye. 4.2 Eyelids The 
eyelids are polygonal models which can blink kinematically during animation (see Figure 10(a)). Note 
that the eyelids are open in Figure 10(a). If the subject is scanned with open eyes, the sensor will 
not observe the eyelid texture. An eyelid texture is synthesized by a relaxation based interpolation 
algorithm similar to the one described in section 2.1. The relaxation algorithm interpolates a suitable 
eyelid texture from the immediately surrounding texture map. Figure 11 shows the results of the eyelid 
texture interpolation. (a) (b) Figure 11: (a) Face texture image with adapted mesh before eyelid texture 
synthesis (b) after eyelid texture synthesis. 4.3 Teeth We have constructed a full set of generic teeth 
based on dental images. Each tooth is a NURBS surfaces of degree 2. Three different teeth shapes, the 
incisor, canine, and molar, are modeled (Figure 10(b)). We use different orientations and scalings of 
these basic shapes to model the full set of upper and lower teeth shown in Figure 10(a). The dentures 
are automatically scaled to .t in length, curvature, etc., and are positioned behind the mouth of the 
facial model.  4.4 Hair, Neck, and Bust Geometry The hair and bust are both rigid polygonal models (see 
Figure 10(c)). They are modeled from the range data directly, by extending the facial mesh in a predetermined 
fashion to the boundaries of the range and re.ectance data, and sampling the images as before. The neck 
can be twisted, bent and rotated with three degrees of freedom. See Figure 12 for illustrations of the 
possible neck articulations.  5 Animation Examples Plate 1 illustrates several examples of animating 
the physics-based face model after conformation to the Heidi scanned data (see Plate 2). The surprise 
expression results from contraction of the outer frontalis, major frontalis, inner frontalis, zygomatics 
major, zygomatics minor, depressor labii, and mentalis, and rotation of the jaw.  The anger expression 
results from contraction of the corruga­tor, lateral corrugator, levator labii, levator labii nasi, anguli 
depressor, depressor labii, and mentalis.  The quizzical look results from an asymmetric contraction 
of the major frontalis, outer frontalis, corrugator, lateral corru­gator, levator labii, and buccinator. 
 The sadnessexpression results from a contraction of the inner frontalis, corrugator, lateral corrugator, 
anguli depressor, and depressor labii.  Plate 6 demonstrates the performance of our face model con­struction 
algorithm on two male individuals ( Giovanni and Mick ). Note that the algorithm is tolerant of some 
amount of facial hair. Plate 7 shows a third individual George. Note the image at the lower left, which 
shows two additional expression effects cheek puf.ng, and lip puckering that combine to simulate the 
vigorous blowing of air through the lips. The cheek puf.ng was created by applying outwardly directed 
radial forces to in.ate the deformable cheeks. The puckered lips were created by applying radial pursing 
forces and forward protruding forces to simulate the action of the orbicularis oris sphincter muscle 
which circles the mouth. Finally, Plate 8 shows several frames from a two-minute ani­mation Bureaucrat 
Too (a second-generation version of the 1990 Bureaucrat which was animated using the generic facial model 
in [20]). Here George tries to read landmark papers on facial mod­eling and deformable models in the 
SIGGRAPH 87 proceedings, only to realize that he doesn t yet have a brain! 6 Conclusion and Future Work 
The human face consists of a biological tissue layer with nonlin­ear deformation properties, a muscle 
layer knit together under the skin, and an impenetrable skull structure beneath the muscle layer. We 
have presented a physics-based model of the face which takes all of these structures into account. Furthermore, 
we have demon­strated a new technique for automatically constructing face models of this sort and conforming 
them to individuals by exploiting high­resolution laser scanner data. The conformation process is carried 
out by a feature matching algorithm based on a reusable generic mesh. The conformation process, ef.ciently 
captures facial geom­etry and photometry, positions and scales facial muscles, and also estimates the 
skull structure over which the new synthetic facial tissue model can slide. Our facial modeling approach 
achieves an unprecedented level of realism and .delity to any speci.c individ­ual. It also achieves a 
good compromise between the complete emulation of the complex biomechanical structures and function­ality 
of the human face and real-time simulation performance on state-of-the-art computer graphics and animation 
hardware. Although we formulate the synthetic facial skin as a layered tis­sue model, our work does not 
yet exploit knowledge of the variable thickness of the layers in different areas of the face. This issue 
will in all likelihood be addressed in the future by incorporating additional input data about the subject 
acquired using noninvasive medical scanners such as CT or MR.  Acknowledgments The authors thank Lisa 
White and Jim Randall for developing the piecewise linear muscle model used to model the mouth. Range/RGB 
facial data were provided courtesy of Cyberware, Inc., Monterey, CA. The .rst two authors thank the Natural 
Scienceand Engineering Research Council of Canada for .nancial support. DT is a fellow of the Canadian 
Institute for Advanced Research.  References [1] T. Akimoto, Y. Suenaga, and R. Wallace. Automatic creation 
of 3D facial models. IEEE Computer Graphics and Applications, 13(5):16 22, September 1993. [2] James 
Doyle and James Philips. Manual on Experimental Stress Anal­ysis. Society for Experimental Mechanics, 
.fth edition, 1989. [3] Irfan A. Essa. Visual Interpretation of Facial Expressions using Dy­namic Modeling. 
PhD thesis, MIT, 1994. [4] Frick and Hans. Human Anatomy, volume 1. Thieme Medical Pub­lishers, Stuttgart, 
1991. [5] H. Gray. Anatomy of the Human Body. Lea &#38; Febiber, Philadelphia, PA, 29th edition, 1985. 
[6] Brian Guenter. A system for simulating human facial expression. In State of the Art in Computer Animation, 
pages 191 202. Springer-Verlag, 1992. [7] T. Kurihara and K. Arai. A transformation method for modeling 
and animation of the human face from photographs. In State of the Art in Computer Animation, pages 45 
57. Springer-Verlag, 1991. [8] Y.C. Lee, D. Terzopoulos, and K. Waters. Constructing physics-based facial 
models of individuals. In Proceedings of Graphics Interface 93, pages 1 8, Toronto, May 1993. [9] N.Magneneat-Thalmann,H.Minh,M.Angelis,andD.Thalmann.De­sign, 
transformation and animation of human faces. Visual Computer, 5:32 39, 1989. [10] D. Metaxas and E. Milios. 
Reconstruction of a color image from nonuniformly distributed sparse and noisy data. Computer Vision, 
Graphics, and Image Processing, 54(2):103 111, March 1992. [11] M. Nahas, H. Hutric, M. Rioux, and J. 
Domey. Facial image synthesis using skin texture recording. Visual Computer, 6(6):337 343, 1990. [12] 
M. Oka, K. Tsutsui, A. Ohba, Y. Kurauchi, and T. Tago. Real-time manipulation of texture-mapped surfaces. 
In SIGGRAPH 21, pages 181 188. ACM Computer Graphics, 1987. [13] F. Parke. Computer generated animation 
of faces. In ACM National Conference, pages 451 457. ACM, 1972. [14] F. Parke. Parameterized models for 
facial animation. IEEE Computer Graphics and Applications, 2(9):61 68, November 1982. [15] F. Parke. 
Parameterized models for facial animation revisited. In SIG-GRAPH Facial Animation Tutorial Notes, pages 
43 56. ACM SIG-GRAPH, 1989. [16] Elizabeth C. Patterson, Peter C. Litwinowicz, and N. Greene. Fa­cial 
animation by spatial mapping. In State of the Art in Computer Animation, pages 31 44. Springer-Verlag, 
1991. [17] S. Platt and N. Badler. Animating facial expression. Computer Graph­ics, 15(3):245 252, August 
1981. [18] D. Terzopoulos. The computation of visible-surface representations. IEEE Transactions on 
Pattern Analysis and Machine Intelligence, PAMI-10(4):417 438, 1988. [19] D. Terzopoulos and M. Vasilescu. 
Sampling and reconstruction with adaptive meshes. In Proceedings of Computer Vision and Pattern Recognition 
Conference, pages 70 75. IEEE, June 1991. [20] D. Terzopoulos and K. Waters. Physically-based facial 
modeling, analysis, and animation. Visualization and Computer Animation, 1:73 80, 1990. [21] K. Waters. 
A muscle model for animating three-dimensional facial expression. Computer Graphics, 22(4):17 24, 1987. 
[22] K.Waters.Aphyscialmodeloffacialtissueandmusclearticulationde­rived from computer tomography data. 
In Visualization in Biomedical Computing, pages 574 583. SPIE, Vol. 1808, 1992. [23] K. Waters and D. 
Terzopoulos. Modeling and animating faces using scanned data. Visualization and Computer Animation, 2:123 
128, 1991. [24] L. Williams. Performance-driven facial animation. In SIGGRAPH 24, pages 235 242. ACM 
Computer Graphics, 1990. [25] J.YauandN.Duffy.3-Dfacialanimationusingimagesamples.In New Trends in Computer 
Graphics, pages 64 73. Springer-Verlag, 1988.  Plate 5: Complete, functional head model of Heidi with 
physics-Plate 8: George in four scenes from Bureaucrat Too . based face and geometric eyes, teeth, hair, 
neck, and shoulders (in Monument Valley).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218411</article_id>
		<sort_key>63</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Automated learning of muscle-actuated locomotion through control abstraction]]></title>
		<page_from>63</page_from>
		<page_to>70</page_to>
		<doi_number>10.1145/218380.218411</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218411</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Channels and controllers</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010590</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Buses and high-speed links</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P237369</person_id>
				<author_profile_id><![CDATA[81100560226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Radek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grzeszczuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto, 10 King's College Road, Toronto, Ontario, Canada, M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109166</person_id>
				<author_profile_id><![CDATA[81100294834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Demetri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto, 10 King's College Road, Toronto, Ontario, Canada, M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>378531</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[L. S. Brotman and A. N. Netravali. Motion interpolation by optimal control. Proc. ACM SIGGRAPH, 22(4):309-407,1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. I. Castro. The Sharks of North American Waters. Texas Unviersity Press, 1983.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. E Cohen. Interactive spacetime control for animation. Proc. ACM SIGGRAPH, 26(2):293-301,1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. H. Ernst. Venomous Reptiles of North America. Smithsonian Institution Press, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>44534</ref_obj_id>
				<ref_obj_pid>44533</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[C.J. Gob and K. L. Teo. Control parameterization: A unified approach to optimal control problems with general constraints. Automatica, 24:3-18,1988.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R. Grzeszczuk. Automated learning of muscle based locomotion through control abstraction. Master's thesis, Dept. of Comp. Sci., Univ. of Toronto, Toronto, ON, January 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[R. McNeill Alexander. Exploring Biomechanics: Animals in Motion. Scientific American Library, New York, 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378508</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[G.S.E Miller. The motion dynamics of snakes and worms. Proc. ACM SIGGRAPH, 22(4): 169-177,1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. T. Ngo and J. Marks. Spacetime constraints revisited. Proc. ACM SIGGRAPH, 27(2):343-351,1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. G. Pandy, E C. Anderson, and D. G. Hull. A parameter optimization approach for the optimal control of large-scale musculoskeletal systems. Transactions of the ASME, 114(450), November 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Best EB. and Abernethy R. B. Heaviside's dolphin. In Handbook of Marine Mammals, volume 5, pages 289-310. Academic Press, 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[W.H. Press, B. Flannery, et al. Numerical Recipes: The Art of Scientific Computing, Second Edition. Cambridge University Press, 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J.E. Rendal, Allen G. R., and Steene R. C. Fishes of the Great Barrier Reef and Coral Sea. Univ. of Hawaii Press, Honolulu, HI, 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[G. Risdale. Connectionist modeling of skill dynamics. Journal of Visualization and Computer Animation, 1 (2):66-72,1990.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[K. Sims. Evolving virtual creatures. Proc. ACM SIGGRAPH, pages 15-22,1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667945</ref_obj_id>
				<ref_obj_pid>1667943</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos, X. Tu, and R. Grzeszczuk. Artificial fishes: Autonomous locomotion, perception, behavior, and learning in a simulated physical world. Artificial Life, 1 (4):327-351, 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192170</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[X. Tu and D. Terzopoulos. Artificial fishes: Physics, locomotion, perception, behavior. Proc. ACM SIGGRAPH, pages 43-50, 1994.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[M. van de Panne and E. Fiume. Sensor-actuator networks. Proc. ACM SIGGRAPH, 27(2):335-343,1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and M. Kass. Spacetime constraints. Proc. ACM SIG- GRAPH, 22(4): 159-167, 1988.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automated Learning of Muscle-Actuated Locomotion Through Control Abstraction Radek Grzeszczuk and Demetri 
Terzopoulos Department of Computer Science, University of Toronto 1 Keywords: learning, locomotion, 
control, arti.cial life, physics­ based modeling Abstract: We present a learning technique that automatically 
syn­thesizes realistic locomotion for the animation of physics-based models of animals. The method is 
especially suitable for animals with highly .exible, many-degree-of-freedom bodies and a consid­erable 
number of internal muscle actuators, such as snakes and .sh. The multilevel learning process .rst performs 
repeated loco­motion trials in search of actuator control functions that produce ef.cient locomotion, 
presuming virtually nothing about the form of these functions. Applying a short-time Fourier analysis, 
the learn­ing process then abstracts control functions that produce effective locomotion into a compact 
representation which makes explicit the natural quasi-periodicities and coordination of the muscle actions. 
The arti.cial animals can .nally put into practice the compact, ef.cient controllers that they have learned. 
Their locomotion learn­ing abilities enable them to accomplish higher-level tasks speci.ed by the animator 
while guided by sensory perception of their vir­tual world; e.g., locomotion to a visible target. We 
demonstrate physics-based animation of learned locomotion in dynamic models of land snakes, .shes, and 
even marine mammals that have trained themselves to perform SeaWorld stunts. Introduction The animation 
of animals in motion is an alluring but dif.cult prob­lem. With the advent of visually realistic models 
of humans and lower animals, even small imperfections in the locomotion patterns can be objectionable. 
The most promising approach to achieving a satisfactory level of authenticity is to develop physically 
realistic ar­ti.cial animals that employ internal actuators, or muscles, to closely approximate the movements 
of natural animal bodies. As these ani­mal models become increasingly complex, however, animators can 
no longer be expected to control them manually. Sophisticated models must eventually assume the responsibility 
for their own sen­sorimotor control. Like real animals, they should be capable of learning to control 
themselves. This paper addresses a natural question: Is it possible for a physics-based, muscle-actuated 
model of an animal to learn from .rst principles how to control its muscles in order to locomote in a 
natural fashion? Unlike prior work on motion synthesis, we target state-of-the-art animate models of 
at least the level of realism and complexity of the snakes and worms of Miller [8] or the .sh of Tu and 
Terzopoulos [17]. In both of these cases, the muscle controllers 110 King s College Road, Toronto, Ontario, 
Canada, M5S 1A4 E-mail: {radekjdt}@cs.toronto.edu Permission to make digital/hard copy of part or all 
of this work for personal or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage, the copyright notice, the title of the publication and 
its date appear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to 
republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. that produce locomotion were carefully hand crafted using knowl­edge gleaned from the biomechanics 
literature [7] and long hours of experimentation. Our goal in this paper is to devise algorithms that 
can provide such animal models the ability to learn how to locomote automatically, in a way that is inspired 
by the remarkable ability of real animals to acquire locomotion skills through action and perception. 
At the foundation of our approach lies the notion that natural locomotion patterns are energetically 
ef.cient. This allows us to formalize the problem of learning realistic locomotion as one of optimizing 
a class of objective functionals, for which there are various solution techniques. We formulate a bottom-up, 
multilevel strategy for learning muscle controllers. At the early stages of the learning process,the 
animate model has no apriori knowledge about howtolocomote. Itisasiftheanimalhadafullyfunctionalbody,but 
no motor control center in its brain . Through practice repeated locomotion trials with different muscle 
actions the animal learns how to locomote with increasing effectiveness, by remembering actions that 
improve its motor skills as measured by the objective functional. Repeated improvements eventually produce 
life-like locomotion. When basic locomotive skill is achieved, the animate models ab­stract the low-level 
muscle control functions that they have learned and train themselves to perform some speci.c higher-level 
motor tasks. The learning algorithm abstracts detailed muscle control functions into a highly compact 
representation. The representation now emphasizes the natural quasi-periodicities of effective muscle 
actionsandmakesexplicit thecoordinationamongmultiple muscles that has led to effective locomotion. Finally, 
the arti.cial animals can put into practice the compact, ef.cient controllers that they have learned 
in order to accomplish the sorts of tasks that animators would have them do. We are particularly interested 
in realistic motion synthesis for three dimensional models of animals that are highly deformable and 
can move continuously within their virtual worlds. Plates 1 5 show frames from animations of animal models 
that we have created, which have learned to locomote and perform interesting motor tasks automatically. 
We use spring-mass systems to construct our models, following the work of [8, 17]. This results in biomechanical 
models with numerous degrees of freedom and many parameters to control. The reader should peruse [8, 
17] to become familiar with the details of the models. An example will serve to illustrate the challenges 
of controlling highly deformable body models: Fig. 1 illustrates a biomechani­cal model of a Sonoral 
coral snake [4] that we use in one of our animations (Plate 1). The body consists of 10 segments. Each 
segment has two pairs of longitudinal muscle springs that are under the active control of the snake s 
brain. All other springs are passive dynamic elements that maintain the structural integrity of the body. 
The snake can actuate its body by varying the rest lengths of the 40 muscle springs over time. To simplify 
matters slightly, paired mus­cles on either side in each segment are actuated synchronously, and this 
yields a total of 20 actuators. Clearly, it is counterproductive to provide the animator direct control 
over so many actuators. Instead, we would like the snake to train itself to control its body. We will 
&#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 left muscle pair right muscle pair DOFs : 126 springs 
stiffness : 50.0 size of the state space : 252 Figure 1: The snake biomechanical model consists of nodal 
masses (points) and springs (lines). It has twenty independent actuators (muscle springs): ten on the 
left side of the body and ten on the right side. Each actuator comprises a pair of synchronous muscles. 
The numbers along the body indicate nodal masses in cross sectional planes. The cross-springs, shown 
in only one segment, maintain the structural integrity of the body. develop algorithms that will enable 
its brain to exercise its body until it discovers the actuator coordination needed to achieve ef.­cient 
serpentine locomotion. The snake will monitor the progress of the learning cycle using an objective functional 
that incorporates sensory feedback about its actions. An advantage of our approach from the point of 
view of the animator is its generality. In principle, it is applicable to all animate models motivated 
by internal muscles, whether highly deformable, or articulate. In this paper, we demonstrate its power 
using 4 differ­ent, highly deformable animal body models in varied media terra .rma, water, and air (see 
Appendix A). Another advantage is that the approach allows us to equip our models with sensors that enable 
them to perceive their environment. Sensory perception is mod­eled through the objective functional to 
be optimized. The sensory contribution to the objective functional represents the animal s per­ception 
of the degree to which its goal has been achieved. Making the arti.cial animal perceptually aware allows 
it to handle tasks that depend on dynamic events in the environment and gives the animator a potent tool 
with which to control the model. 1.1 Related Work The issue of control is central to physics-based animation 
research. Optimal control methods formulate the control problem in terms of an objective functional which 
must be minimized over a time inter­val, subject to the differential equations of motion of the physical 
model [1]. The spacetime constraints method [19] has attracted a certain following (e.g., [3]), but it 
is problematic because, in princi­ple, it treats physics as a penalty constraint (that can be stretched 
like a spring ) and, in practice, the need to symbolically differen­tiate the equations of motion renders 
it impractical for all but the simplest physical models. We pursue a different approach toward locomotion 
control that is suitable for complex physical models. The approach is inspired by the direct dynamics 
technique which was described in the control literature by Goh and Teo [5] and earlier references cited 
therein. Directdynamicsprescribesagenerate-and-teststrategythat optimizes a control objective functional 
through repeated forward dynamic simulation and motion evaluation. The direct dynamics technique was 
developed further to control articulated musculoskeletal models in [10] and it has seen applica­tion 
in the mainstream graphics literature to the control of planar articulated .gures [18, 9]. Pandy et al. 
[10] search the model ac­tuator space for optimal controllers, but they do not perform global optimization. 
Van de Panne and Fiume [18] use simulated annealing for global optimization. Their models are equipped 
with simple sen­sors that probe the environment and use the sensory information to in.uence control decisions. 
Ngo and Marks [9] stimulus-response control algorithm presents a similar approach. They apply the ge­netic 
algorithm to .nd optimal controllers. The genetic algorithm is also used in the recent work of Sims [15]. 
Risdale [14] reports an early effort at controller synthesis for articulated .gures from training examples 
using neural networks. A characteristic of prior methods that tends to limit them to rel­atively simple 
planar models with few actuators is that they attempt to tackle the control problem at only a single 
level of abstraction. Typically, they deal with the control problem at an abstract level, say, in terms 
of a small number of controller network weights [18, 15] or whole body motions [9]. We advocate a multilevel 
controller learning technique that can handle complex models even though it seeks, based on .rst principles, 
optimal muscle actuation functions in a very concrete representation that makes the weakest possible 
assumptions. Thus the learning process is bootstrapped essentially from scratch. Earlier versions of 
our work were presented in [6, 16]. 1.2 Overview We describe our multilevel learning technique in the 
following two sections. Section 2 presents the strategy for learning low level con­trollers. Low level 
control learning is time consuming because of the high dimensionality of the search space. It is therefore 
prudent to reuse controllers. To this end, Section 3 presents the strategy for abstracting high level 
controllers. The abstraction step dramat­ically reduces dimensionality, stores the reduced description 
in the animal s memory, and permits the control problems to be de.ned in terms of higher level motor 
goals. This approach leads naturally to reusable solutions. We search for good low level control solutions 
for a set of simple tasks and use them as building blocks to achieve higher level goals. Section 4 presents 
a thorough experimental evaluation of our learning approach and more results.  2 Learning Low Level 
Control Our low-level learning technique repeatedly generates a controller, applies it to drive a short-time 
forward simulation of the dynamic bodymodel,andmeasuresits effectivenessatproducinglocomotion using an 
objective functional. Typically, this low-level motor learn­ing cycle is lengthy (as it can be in real 
animals, such as humans). However, it is simple and ultimately quite effective. 2.1 Biomechanical Models, 
Muscles, Actuators, Controllers The biomechanical models that we employ are constructed of nodal masses 
and springs, as is detailed in Appendix A. Their dynamics is speci.ed by the Lagrangian equations of 
motion X fs miX+lie+ ij=fi (1) xixi j2Ni where node ihas mass mi, position xi(t)=[xi(t),yi(t),zi(t)], 
velocity eis an external force. x, and damping factor li,and where fiSpring Sij, which connects node 
ito neighboring nodes j2Ni, exerts the force fijs(t)=-(cijeij+lijeeij)rij/jjrijjjon node i(and it exerts 
the force -fijson node j), where cijis the elastic constant, lijis the damping constant, and eij(t)=jjrijjj-lijis 
the deformation of the spring with separation vector rij(t)=xj-xi. The natural length of the spring is 
lij. Some of the springs in the biomechanical model play the role of contractile muscles. Muscles contract 
as their natural length lijdecreases under the autonomous control of the motor center of the arti.cial 
animal s brain [17]. To dynamically contract a muscle, the brain must supply an activation function a(t)to 
the muscle. This continuous time function has range [0,1], with 0 corresponding to a fully relaxed muscle 
of length lrand 1to a fully ijcontracted muscle of length lcMore speci.cally, for a muscle ij. r spring, 
lij=alc+(1-a)l ijij. Typically, individual muscles form muscle groups, called actu­ators, that are activated 
in unison. Referring to Fig. 1 for example, the 40 muscles in the snake model are grouped pairwise in 
each segment to form 10 left actuators and 10 right actuators. Each actu­ator iis activated by a scalar 
actuation function ui(t), whose range is again normalized to [0,1]. The actuation function transforms 
straightforwardly into activation functions for each muscle in the actuator. Thus, to control the snake 
s body we must specify the actuation functions u(t)=[u(t),,ui(t),,u(t)]0,where 1N N=20. The continuous 
vector-valued function of time u(t)is called the controller and its job is to produce locomotion. Controllers 
may be stored within the arti.cial animal s motor control center. 2.2 Objective Functional A continuous 
objective functional Eprovides a quantitative measure of the progress of the locomotion learning process. 
The functional is the weighted sum of a term Euthat evaluates the controller u(t) and a term Evthat evaluates 
the motion v(t)that the controller produces in a time interval t0.t.t1, with smaller values of E indicating 
better controllers u. Mathematically, Zt1 E(u(t))=( 1Eu(u(t))+ 2Ev(v(t)))dt,(2) t0 where 1and 2are scalar 
weights. Fig. 2 illustrates this schemat­ically. CONTROL FUNCTION TRAJECTORY  forward simulation time 
evaluation metric  evaluation metric for the control function for the trajectory + EU EV OPTIMIZATION 
FUNCTION Figure 2: The objective function guiding the optimization is a weighted sum of terms that evaluate 
the trajectory and the con­trol function. It is important to note that the complexity of our models pre­cludes 
the closed-form evaluation of E. As the .gure indicates, to compute E, the arti.cial animal must .rst 
engage u(t)to produce a motion v(t)with its body (in order to evaluate term Ev). This is done through 
forward simulation of the biomechanical model over the time interval t0.t.t1using the controller u(t). 
We may want to promote a preference for controllers with cer­tain qualities via the controller evaluation 
term Eu. For example, we can guide the optimization of Eby discouraging large, rapid .uctuations of u, 
since chaotic actuations are usually energy inef.­cient. We encourage lower amplitude, smoother controllers 
through the function  d2u 21 du 2 Eu = 1 + 2 , (3) 2dt dt2 with weighting factors 1and 2. The two component 
terms in (3) are potential energy densities of linear and cubic variational splines in time, respectively. 
The former penalizes actuation amplitudes, while the latter penalizes actuation variation. actuation 
 The distinction between good and bad control functions also depends on the goals that the animal must 
accomplish. In our learning experiments we used trajectory criteria Evsuch as the .nal distance to the 
goal, the deviation from a desired speed, etc. These and other criteria will be discussed shortly in 
conjunction with speci.c experiments. 2.3 Time and Frequency Domain Discrete Controllers To solve the 
low level control problem, we must optimize the ob­jective functional (2). This cannot be done analytically. 
We convert this continuous optimal control problem to an algebraic parameter optimization problem [5] 
by parameterizing the controller through discretization using basis functions. Mathematically, we express 
M X jj ui(t)= uB(t), (4) ij=1 j where the uare scalar parameters and the Bj(t), 1.j.M i are (vector-valued) 
temporal basis functions. There are two quali­tatively different choices of basis functions local and 
global. j In the local discretization, the parameters uare nodal variables i and the Bj(t)can be spline 
basis functions. The simplest case is j when the uare evenly distributed in the time interval and the 
Bj(t) i are tent functions centered on the nodes with support extending to nearest neighbor nodes, so 
that u(t)is the linear interpolation of the nodal variables (Fig. 3, top). Smoother B-splines can be 
used in a similar fashion. Since the nodal parameters are naturally ordered in a time sequence, we will 
refer to locally discretized controllers as time domain controllers. TIME DOMAIN 12 3 45 678 9 uu1 u 
 1 3 75 8 21u9 u (t)1 u (t) 2 FREQUENCY DOMAIN 12 3 45 678 9 u 1(t) u (t) 2 u1 t 112233 u1B u1B u1B 
445566 u1B u1B u1B 778899 u1B u1B u1Bu2 t 112233 u2B u2B u2B 445566 u2B u2B u2B 778899 u2B u2B u2B Figure 
3: Simple time domain controller (top) with two control functions u1(t)and u2(t). Each function is a 
piecewise linear polynomialgeneratedby9controlpoints. Simplefrequencydomain controller (bottom) with 
two control functions, each a sum of 9 sinusoidal basis functions Bj(t)=cos(wjt+). cj In the global discretization, 
the support of the Bj(t)covers the entire temporal domain t0.t.t1. A standard choice is sinusoidal basis 
functions Bj(t)=cos(wjt+)where wjis the cjj angular frequency and cjis the phase, and the parameters 
uare i amplitudes (Fig. 3, bottom). We will refer to controllers discretized globally using sinusoidal 
bases of different frequencies and phases as frequency domain controllers. The time domain and frequency 
domain representations offer different bene.ts and drawbacks. The time domain controller yields a faster 
low-level learning rate. This issue is discussed in detail in Section 4.1. The frequency domain controller, 
on the other hand, does not require a change of basis during the abstraction process described in Section 
3. It can also sometimes be extended arbitrarily in time since it favors periodicity. 2.4 Optimization 
of the Discrete Objective Function Since u(t)has Nbasis functions, the discretized controller is repre­sented 
using NMparameters. Substituting (4), into the continuous objective functional (2), we approximate it 
by the discrete objective 1 M0 function E([u1,,u]). N Learning low level control amounts to using an 
optimization algorithm to iteratively update the parameters of a time domain or frequency domain controller 
so as to maximize the discrete objective function and produce increasingly better locomotion. We have 
used both simulated annealing and the simplex method to optimize the objective function. The reader should 
refer to a text such as [12] for details about these optimization methods. Simulated annealing has three 
features that make it particularly suitable for our application. First, it is applicable to problems 
with a large number of variables yielding search spaces large enough to make exhaustive search prohibitive. 
Second, it does not require gradient information about the objective function. Analytic gra­dients are 
not directly attainable in our situation since evaluating Erequires a forward dynamic simulation of the 
animal. Third, it avoids getting trapped in local suboptima of E. In fact, given a suf.ciently slow annealing 
schedule, it will .nd a global optimum of the objective functional. Robustness against local suboptima 
can be important in obtaining control functions that produce realistic motion. The bene.t of using the 
simplex method over simulated annealing in some cases is its faster convergence rate. On the other hand, 
since it is a local optimization technique, strictly speaking, it can be applied successfully only to 
the class of optimization prob­lems in which the topography of Eis globally convex. Section 4.1 will 
describe in more detail the advantages and pitfalls of both methods when applied to the low level learning 
problem. All of the biomechanical models described in Appendix A have demonstrated the ability to learn 
effective low level time domain locomotion controllers. Plates 1, 2, and 3 show frames from ani­mations 
with controllers that have been learned by the snake, ray, and shark models, which produce natural and 
effective locomo­tion. Plate 3 illustrates a race among four sharks that have learned for different durations. 
The shark that is furthest from the cam­era has learned how to locomote for the shortest period of time, 
which yields muscle control functions that are essentially random and achieve negligible locomotion. 
Sharks closer to the camera have learned for progressively longer periods of time. The closest shark, 
which locomotes the best wins the race.  3 Abstracting High Level Control It is time consuming to learn 
a good solution for a low level controller because of the high dimensionality of the problem (large NM), 
the lack of gradient information to accelerate the optimization of the objective functional, and the 
presence of suboptimal traps that must be avoided. Consequently, it is costly to produce animation by 
perpetually generating new controllers. The learning procedure must be able to abstract compact higher 
level controllers from the low level controllers that have been learned, retain the abstracted controllers, 
and apply them to future locomotion tasks. The process of abstraction takes the form of a dimensional­ity 
reducing change of representation. More speci.cally, it seeks to compress the many parameters of the 
discrete controllers to a compact form in terms of a handful of basis functions. Natural, steady-state 
locomotion patterns tend to be quasi-periodic and they can be abstracted very effectively without substantial 
loss. The nat­ural choice, therefore, is to represent abstracted controllers using the global sinusoidal 
basis functions discussed earlier. For the fre­quency domain controller, the dimensionality reduction 
is achieved trivially by retaining all basis functions whose amplitudes ujexceed i a low threshold and 
suppressing those below threshold. This results in a small set of signi.cant basis functions with associated 
ampli­tudes that constitute the abstracted controller. To abstract a time domain controller, we apply 
the fast Fourier transform (FFT) [12] to the parameters of the time domain controller and then suppress 
the below-threshold amplitudes. 3.1 Using Abstracted Controllers Typically, our arti.cial animals are 
put through a basic training regimen of primitive motor tasks that it must learn, such as locomot­ing 
at different speeds and executing turns of different radii. They learn effective low level controllers 
for each task and retain compact representations of these controllers through controller abstraction. 
The animals subsequentlyput the abstractions that they have learned into practice to accomplish higher 
level tasks, such as target track­ing or leaping through the air. To this end, abstracted controllers 
are concatenated in sequence, with each controller slightly over­lapping the next. To eliminate discontinuities, 
temporally adjacent controllers are smoothly blended together by linearly fading and summing them over 
a small, .xed region of overlap, approximately 5% of each controller (Fig. 4). BASIC ABSTRACTED CONTROLLERS 
 move forward controller turn up controller turn down controller turn right controller Figure 4: Higher 
level controller for jumping out of water is con­structed from a set of abstracted basic controllers. 
[m] 3.0 2.0 1.0 [m] Figure 5: The solid curve indicates the path of the .sh tracking the goal. Black 
dots mark consecutive positions of the goal and white dots mark the starting point of a new controller. 
Currently we use abstracted controllers in two ways. In one scenario, the animated model has learned 
a repertoire of abstracted controllers that it applies in a greedy fashion to navigate in the di­rection 
of a target. It modi.es its locomotion strategy periodically by invoking the abstracted controller that 
gives it the greatest im­mediate gain over the subsequenttime interval. For example, Fig. 5 shows a path 
generated by the shark model using this method as it is presented with a series of targets. The shark 
has learned six basic abstracted controllers to accomplish this task: do-nothing, go­straight, sharp-turn-left, 
sharp-turn-right, wide-turn-left, and wide­turn-right. Itthendiscoveredhowtosequencethesecontrollers,and 
for what durations to apply them in order to locomote to successive targets indicated by black dots. 
The circles on the path in Fig. 5 indicate positions where the shark reevaluated its current strategy. 
Changes in the path s direction indicate that the shark has switched to a different controller which 
provides a bigger immediate gain. Plate 4 shows rendered frames from the locomotion animation with the 
targets rendered as red buoys. This method is inexpensive and can be made to work in real time. Unfortunately, 
the greedy strategy is bound to fail on a problem that requires careful planning.   Plate 5: SeaWorld 
tricks learned by the arti.cial dolphin. The second method overcomes the limitations of the greedy strategy 
by learning composite abstracted controllers that accom­plish complex locomotion tasks. Consider the 
spectacular stunts performed by marine mammals that elicit applause at theme parks like SeaWorld . We 
can treat a leap through the air as a complex task that can be achieved using simpler tasks; e.g., diving 
deep be­neath a suitable leap point, surfacing vigorously to gain momentum, maintaining balance during 
the ballistic .ight through the air, and splashing down dramatically with a belly .op. We have developed 
an automatic learning technique that con­structs a macro jump controller of this sort as an optimized 
sequence of basic abstracted controllers. The optimization process is, in prin­ciple, similar to the 
one in low level learning. It uses simulated annealing for optimization, but rather than optimizing over 
nodal parameters or frequency parameters, it optimizes over the selection, ordering, and duration of 
abstracted controllers. Thus the animal model applying this method learns effective macro controllers 
of the type shown at the bottom of Fig. 4 by optimizing over a learned repertoire of basic abstracted 
controllers illustrated at the top of the .gure.  3.2 Composing Macro Controllers We .rst train the 
arti.cial dolphin so that it learns controllers for 5 basic motor tasks: turn-down, turn-up, turn-left, 
turn-right, and move-forward. We then give it the task of performing a stunt like the one described above 
and the dolphin discovers a combination of controllers that accomplishes the stunt. In particular, it 
discovers that it must build up momentum by thrusting from deep in the virtual pool of water up towards 
the surface and exploit this momentum to leap out of the water. Plate 5(a) shows a frame as the dolphin 
exits the water. The dolphin can also learn to perform tricks while in the air. Plate 5(b) shows it using 
its nose to bounce a large beach-ball off a support. The dolphin can learn to control the angular momentum 
of its body while exiting the water and while in ballistic .ight so that it can perform aerial spins 
and somersaults. Plate 5(c) shows it in the midst of a somersault in which it has just bounced the ball 
with its tail instead of its nose. Plate 5(d) shows the dolphin right after splashdown. In this instance 
it has made a dramatic belly.op splash. By discovering controllers that enable it to control its body 
in these complex ways, the dolphin can amuse and entertain the animator, who would be hard pressed to 
design similar controllers byhandforaphysics-basedmodelwith asmanycontrolparameters as the dolphin model 
has. To train the dolphin to perform a variety of stunts we introduced additional style terms into the 
objective function that afford extra control on the animal s trajectory in the air. For example, a simple 
jump was learned by optimizing over the maximum height at some point in time. In order to train the dolphin 
to jump a hurdle, we introduced a term to control its orientation as it clears the hurdle at the apex 
of its trajectory, it should be in a horizontal orientation and it should face downward upon reentry. 
The somersault controller wasdiscoveredby addinga termthat encouragedmaximumangular velocity of the body 
in .ight. We can maximize or minimize the area of the animal s body that hits the water upon reentry 
for crowd­drenching splashes or for high scores from the judges for a clean dive. Different style terms 
can, in principle, be added inde.nitely to the control function. The only limitation seems to be the 
increasing complexity of the optimization. We have noted that although it can produce some interesting 
results, simulated annealing is not espe­cially well suited to this kind of optimization problem. We 
suspect that this is due to the combinatorial nature of the problem and the fact that simulated annealing 
does not take advantage of partial so­lutions that it .nds along the way, but instead starts with a new 
set of parameters at every iteration of the optimization process. Unfortu­nately, at a high level of 
abstraction sometimes even small changes in the set of parameters produce drastically different trajectories. 
Genetic algorithms may perform better on such problems.  4 Additional Experiments and Results This section 
presents a more thorough experimental study of our approach and reports additional results. 4.1 Performance 
of the Optimizers Fig. 6(a) compares the performance of the simplex method and simulated annealing in 
seeking optimal time and frequency domain controllers for the shark model given the task of locomoting 
to a speci.ed goal location; i.e., Ev =jjdgjj2,where dgis the separa­tion vector between the nose node 
of the shark and the goal point. The term Eu(3) in the objective functional (2) was disabled for these 
tests ( 2 =0). For simplicity, the 4 muscles in each segment were grouped into a single actuator (N=3actuators 
total), with the left muscle pair receiving contraction signals that are exactly out of phase with the 
right muscle pair. A time interval was discretized using M=15parameters; hence, the dimensionality of 
the search space was NM=45. Both methods converge to good time domain controllers. The simplex method 
yields a .nal objective functional value of Eo = 049after approximately 500 iterations. Simulated annealing 
.nds a slightly better solution, Eo =042, but only after 3500 iterations. For frequency domain controllers, 
the results differ substantially. Simulated annealing performs almost as well as for the time domain 
controller, yielding an objective function value of Eo =052after 3500 iterations.2However, the simplex 
method does much worse it fails to get below Eo =065. Fig. 6(b c) compares the convergence for simulated 
annealing on both types of controllers. The results are better for the objective function represented 
in the time domain (Fig. 6(b)) than for the frequency domain representation (Fig. 6(c)). For the time 
domain representation we need approximately 700 iterations to get very close to the global minimum. The 
number of iterations for the frequency domain representation is much greater. The above results suggest 
that it is much harder to optimize the objective using frequency domain controllers. To understand why, 
we plotted Eagainst pairs of randomly chosen parameters uj i (labeled xand yin the plots), using both 
time and frequency domain representations. We stepped the selected parameters through a range of values 
while keeping the other parameters constant and evaluated the objective function repeatedly to obtain 
a 3D surface plot (each repetition required a forward simulation of the locomotion). The plot in Fig. 
7(a) reveals the simple convex topography of Efor time domain controllers, while the plot in Fig. 7(b) 
reveals the much more irregular, nonconvex topography of Efor frequency domain controllers. Evidently, 
small changes in the local basis functions of the time domain controller produce small, well-behaved 
changes in the value of the objective function. By contrast, small changes in the global basis functions 
of the frequency domain controller produce relatively larger, more irregular changes in the value of 
the objective function. The many local minima in the topography of the objective func­tion associated 
with the frequency domain controller lead to failure of the simplex method and they present more of a 
challenge to sim­ulated annealing. The convex structure of the objective function associated with the 
time domain controller allows both annealing and simplex to converge very quickly. Moreover, they often 
yield better time domain controllers than frequency domain controllers. We conclude that the time domain 
controller representation is a worthwhile one.  4.2 In.uencing Controllers via the Objective Functional 
In Section 2.2 we discussed how the term Euin (2) that evaluates controllers allows us to in.uence the 
motion. For example, we can discourage chaotic motions. This section investigates the effect of different 
Euterms in more detail. Again, we employ the shark with the same goal Evas in the previous section. Fig. 
8(a) shows the results obtained with the time domain repre­ j+1 sentation for =0, hence the discrete 
term Eu = 1/2h2(u­ 2 i j u)2,where his the timestep between nodal parameters. The objec­ i tive function 
was evaluated for 1 =00,01,02(top to bottom). As the value of increases, both the amplitude and the frequency 
1 2We start the simulated annealing procedure at the temperature T0 0.5.The annealing schedule is Ti+1 
.Ti where 0:.:1. Usually .0.9,but Fig. 6(b c) shows the results obtained with different schedules. The 
maximum number of steps before the temperature drop is 10NMand the minimum number of accepted perturbations 
before the temperature drop is NMFor N15it takes about one hour on an SGI Indigo workstation to get a 
solution for each control function. At each step the values of all parameters are perturbed randomly. 
The perturbation is bounded by 10% of the range of admissible values. So, for example, if the maximum 
contractionofthe muscleis 20%of itsrelaxedlength,eachperturbationwill beat most 2%. The bound on the 
perturbation remains .xed over the annealing process. This yields much faster convergence than if we 
decreased the magnitude of the perturbation with temperature.  1 0.9 0.8 0.2 0.2 0.7  objective 
function 0.6  -0.2 -0.2 0.5  0.2 0.2 0.4  function evaluations -0.2 -0.2 (a) 1  0.6  sequence 
(Fig. 9(b)) the posterior and anterior segment muscles (a) (b) 0.9  objective function Figure 8: In.uence 
of Euon controller: (a) =0;(b) 2 0.8 = . 0.7 0.5 on the right side of the body are essentially unused, 
while all the 0.4 0.4 0 500 1000 1500 2000 2500 3000 3500 0 500 1000 1500 2000 2500 3000 3500 function 
evaluations function evaluations (b) (c) Figure 6: (a) Performance comparison of the simplex method 
and of simulated annealing. Convergence rate of simulated annealing on the time domain controller (b) 
and on the frequency controller (c) with cooling rates: T0 =08, T=085, and T2 =09. 1 0.9 0.7 0.8 
0.6 0.7 0.5 0.6 0.4 0.5 0.2  muscles on the left side of the body move approximately in phase. 
It suf.ces to use two primary modes of the Fourier transform to get an effective abstracted controller 
for the turning snake. For straight locomotion (Fig. 9(c)) it is dif.cult to interpret the time domain 
actuation functions. However, if we look at the learned abstracted controller for straight locomotion, 
we can clearly see that the main modes have the same frequency, but their phase is shifted slightly, 
as expected. When a real snake turns, it .rst coils itself in the direction of the turn then switches 
back to its normal serpentine mode of locomotion. This pattern is revealed in the automatically learned 
turn controller shown in Fig. 9(d). First, all the muscles on the right side of the body relax and all 
the muscles on the left side contract. Then they resume their action for straight serpentine locomotion. 
 0 0.2 -0.2 y  0 -0.2 x  (a) (b) Figure 7: Topographyofobjectivefunction(in 2dimensions)oftime domain 
representation(a) and frequencydomain representation(b). of the learned actuation functions drop and 
the shark learns loco­motion controllers that result in less energy expenditure. Fig. 8(b) shows the 
results obtained with 1 =0, hence the discrete term j+1 jj.12 Eu = 2/2h4(u-2u+u)with 2 =00,0002,0006 
iii (top to bottom). As the value of increases, the amplitude of the learnedactuationfunctionsremains 
constantandonlythefrequency decreases.  4.3 Abstracted Learning Results Next, we report results for 
the shark and the snake in abstracting learned time domain controllers for straight locomotion and left 
turns. Right turn controllers were obtained by swapping signals sent to left and right actuators. To 
obtain good abstracted controllers for the shark, it was suf­.cient for both straight motion and left 
turn to retain the single dominant mode of the FFT. Fig. 9(a) shows learned controllers for the shark 
swimming straight. In this experiment, the left and right muscle pairs in each segment constitute independent 
actuators, but note how the animal has learned to actuate its left muscles approx­imately out of phase 
with those on the right side. The posterior segment muscles contract with roughly half the frequency 
of the other muscles and the muscles on either side of the body are ac­tivated in sequence with a slight 
phase shift. For the swim left  A Biomechanical Model Structure and Simulation Animals such as snakes, 
worms, .shes, and marine mammals, with highly .exible bodies are well suited to mechanical modeling using 
spring-mass systems. All of our animal body models consist of an internal biomechanical model with contractile 
muscles coupled to an external, texture-mapped, NURBS surface display model. Fig. 1 shows the spring-mass 
system for the coral snake (Mi­cruroides euryxanthus), which is similar to the one in [8]. Plate 1 shows 
the display model. The muscle springs can contract to 30% of their relaxed length. The body mass is distributed 
evenly among all nodes. Fig. 10 shows the spring-mass system for the Leopard shark (Triakis semifasciata) 
[2], which is similar to the .sh model in [17]. Plates 3 and 4 show the display model. The 4 posterior 
muscles can contract to 10% of their relaxed length; the 8 other muscles to 20%. The .gure speci.es the 
nodal mass distribution. We model a Heaviside s dolphin (Cephalorhynchus heavisidii) [11] (Plate 5 shows 
the display model) straightforwardly by turning the shark spring-mass system on its side, such that the 
muscles serve as caudal (tail) .n elevator and depressors. We equip the dolphin with functional pectoral 
.ns that allow it to roll, pitch, and yaw in the water (see [17] for details about .ns). Fig. 11 shows 
the spring-mass system for the Kuhl s stingray (Dasyatis kuhlii) [13]. Plate 2 shows the display model. 
Four left and 4 right elevator muscles and an equal number of depressor muscles are capable of .exing 
the wings by contracting to 20% of their relaxed length. Mass is distributed evenly among all the nodes. 
To model snake locomotion, we use directional friction against the ground which generates reaction forces 
that move the body forward, as described in [8]. To model marine animal locomotion, we compute hydrodynamic 
reaction forces acting on each of the 0.2 0.2 2.0 t [ s ] 2.0 t [ s ] -0.2 -0.2 0.0 freq [Hz]2.0 4.0 
0.2 0.0 2.0 4.0 freq [Hz] 0.2 t [ s ]2.0 t [ s ]2.0 -0.2 -0.2 (a) (b) 0.3 0.3 t [ s ]2.0 t [ s ]1.5 -0.3 
-0.3  0.0 1.0 2.0 freq [Hz] 0.0 1.0 2.0 freq [Hz] 0.3 0.3 2.0 t [ s ] 1.5 t [ s ] -0.3 -0.3 (c) (d) 
Figure 9: Learned controller for the swim straight (a) and the left turn (b) for the shark. Learned controller 
for the straight motion (c) and the left turn (d) for the snake. For each part: (top) learned time domain 
controller (dotted lines indicate actuator functions for left side of body, solid lines indicate actuator 
functions for right side); (center) primary modes of controller FFT (radius of circles indicates mode 
amplitudes, radial distances from center of surrounding circle indicate frequencies, angular positions 
within surrounding circle indicate phases); (bottom) abstracted controller obtained by retaining primary 
modes. model s faces, as described in [17]. These forces produce external nodal forces fiin the equations 
of motion (1). We use a stable, ef.cient semi-implicit Euler method [12] to numerically integrate these 
ODEs. It is implicit in the internal forces on the lhs of (1) and explicit in the external forces fi. 
 Acknowledgements We thank Xiaoyuan Tu, who developed the original .sh biomechan­ical model, for her 
software and cooperation, and Geoffrey Hinton for valuable discussions. This work was made possible by 
a grant from the Natural Sciences and Engineering Research Council of Canada. DT is a fellow of the Canadian 
Institute for Advanced Research.  References [1] L. S. Brotman and A. N. Netravali. Motion interpolation 
by optimal control. Proc. ACM SIGGRAPH, 22(4):309 407, 1988. [2] J. I. Castro. The Sharks of North American 
Waters. Texas Unviersity Press, 1983. [3] M. F. Cohen. Interactive spacetime control for animation. Proc. 
ACM SIGGRAPH, 26(2):293 301, 1992. [4] C.H.Ernst. Venomous Reptiles of North America. Smithsonian Insti­tution 
Press, 1992. left back left center left front muscle pair muscle pair muscle pair point masses : 21 0.22 
DOFs : 69 size of the state space : 138 2.2 actuators : 6 springs 0.22 1.1 1.1 2.2 11.0 2.2 stiffness 
: 35.0 right back right center right front muscle pair muscle pair muscle pair Figure 10: The shark 
biomechanical model has six actuators con­sisting of a pair of muscles that share the same activation 
function. The numbers along the body indicate the mass of each point in the corresponding cross sectional 
plane. The cross-springs that main­tain the structural integrity of the body are indicated in one of 
the segments only. right depressors right elevators 1.0 point masses : 231.0 DOFs : 63 size of the state 
space : 126 1.0 actuators : 161.0 springs stiffness : 60.0 1.0 1.0 Figure 11: The ray biomechanical model 
has four sets of actuators: left and right depressors and left and right elevators. The numbers along 
the body indicate the mass of each point in the corresponding crosssectionalplane. Thecross-springsthatmaintainthestructural 
integrity of the body are indicated in one of the segments only. [5] C. J. Goh and K. L. Teo. Control 
parameterization: A uni.ed approach to optimal control problems with general constraints. Automatica, 
24:3 18, 1988. [6] R. Grzeszczuk. Automated learning of muscle based locomotion through control abstraction. 
Master s thesis, Dept. of Comp. Sci., Univ. of Toronto, Toronto, ON, January 1994. [7] R. McNeill Alexander. 
Exploring Biomechanics: Animals in Motion. Scienti.c American Library, New York, 1992. [8] G.S.P. Miller. 
The motion dynamics of snakes and worms. Proc. ACM SIGGRAPH, 22(4):169 177, 1988. [9] J. T. Ngo and J. 
Marks. Spacetime constraints revisited. Proc. ACM SIGGRAPH, 27(2):343 351, 1993. [10] M. G. Pandy, F. 
C. Anderson, and D. G. Hull. A parameter optimiza­tion approach for the optimal control of large-scale 
musculoskeletal systems. Transactions of the ASME, 114(450), November 1992. [11] Best P.B. and Abernethy 
R. B. Heaviside s dolphin. In Handbook of Marine Mammals, volume 5, pages 289 310. Academic Press, 1994. 
[12] W. H. Press, B. Flannery, et al. Numerical Recipes: The Art of Scienti.c Computing, Second Edition. 
Cambridge University Press, 1992. [13] J. E. Rendal, Allen G. R., and Steene R. C. Fishes of the Great 
Barrier Reef and Coral Sea. Univ. of Hawaii Press, Honolulu, HI, 1990. [14] G. Risdale. Connectionist 
modeling of skill dynamics. Journal of Visualization and Computer Animation, 1(2):66 72, 1990. [15] K. 
Sims. Evolving virtual creatures. Proc. ACM SIGGRAPH, pages 15 22, 1994. [16] D. Terzopoulos, X. Tu, 
and R. Grzeszczuk. Arti.cial .shes: Au­tonomous locomotion, perception, behavior, and learning in a simu­lated 
physical world. Arti.cial Life, 1(4):327 351, 1994. [17] X. Tu and D. Terzopoulos. Arti.cial .shes: Physics, 
locomotion, perception, behavior. Proc. ACM SIGGRAPH, pages 43 50, 1994. [18] M. van de Panne and E. 
Fiume. Sensor-actuator networks. Proc. ACM SIGGRAPH, 27(2):335 343, 1993. [19] A. Witkin and M. Kass. 
Spacetime constraints. Proc. ACM SIG-GRAPH, 22(4):159 167, 1988.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218414</article_id>
		<sort_key>71</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Animating human athletics]]></title>
		<page_from>71</page_from>
		<page_to>78</page_to>
		<doi_number>10.1145/218380.218414</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218414</url>
		<keywords>
			<kw><![CDATA[computer animation]]></kw>
			<kw><![CDATA[dynamic simulation]]></kw>
			<kw><![CDATA[human motion]]></kw>
			<kw><![CDATA[motion control]]></kw>
			<kw><![CDATA[physically realistic modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14028670</person_id>
				<author_profile_id><![CDATA[81100049661]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jessica]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Hodgins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Computing, Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P297086</person_id>
				<author_profile_id><![CDATA[81100297836]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wayne]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Wooten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Computing, Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P60759</person_id>
				<author_profile_id><![CDATA[81100138869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Brogan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Computing, Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133156</person_id>
				<author_profile_id><![CDATA[81100311781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Computing, Georgia Institute of Technology, Atlanta, GA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, N. I., Phillips, C. B., Webber, B. L. 1993. SimulatingHumans. Oxford: Oxford University Press.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baumgarte, J. 1972. Stabilization of Constraints and Integrals of Motion in Dynamical Systems. Computer Methods in Applied Mechanics and Engineering 1:1-16.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192259</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Breen, D. E., House, D. H., Wozny, M. J., 1994. Predicting the Drape of Woven Cloth Using Interacting Particles. Proceedings of SIGGRAPH, 365-372.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>849669</ref_obj_id>
				<ref_obj_pid>846238</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brogan, D. C., Hodgins J. K. 1995. Group Behaviors for Systems with Significant Dynamics. To appear in IEEE/RSJ International Conference on Intelligent Robot and Systems.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Brotman, J. S., Netravali, A. N. 1988. Motion Interpolation by Optimal Control. Proceedings of SIGGRAPH, 309-315.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, A., Calvert, T. W. 1989. Goal-Directed, Dynamic Animation of Human Walking. Proceedings of SIGGRAPH, 233-242.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134017</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Carignan, M., Yang, Y., Magnenat-Thalmann, N., Thalmann, D. 1992. Dressing Animated Synthetic Actors with Complex Deformable Clothes. Proceedings of SIGGRAPH, 99-104.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Cavagna, G. A., Thys, H., Zamboni, A. 1976. The Sources of External Work in Level Walking and Running.Journal of Physiology 262:639- 657.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cavanagh, E, Sanderson, D. 1986. The Biomechanics of Cycling: Studies of the Pedaling Mechanics of Elite Pursuit Riders. In Science of Cycling, Edmund R. Burke (ed), Human Kinetics: Champaign, Ill.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. E 1992. Interactive Spacetime Control for Animation. Proceedings of SIGGRAPH, 293-302.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Dempster, W. T., Gaughran, G. R. L. 1965. Properties of Body Segments based on Size and Weight. American Journal of Anatomy 120: 33-54.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Frohlich, C. 1979. Do springboard divers violate angular momentum conservation? American Journal of Physics 47:583-592.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Gregor, R. J., Broker, J. E, Ryan, M. M. 1991. Biomechanics of Cycling Exercise and Sport Science Reviews Williams &amp; Wilkins, Philadelphia, John Holloszy (ed), 19:127-169.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hodgins, J. K. 1991. Biped Gait Transitions. In Proceedings of the IEEE International Conference on Robotics and Automation, 2092- 2097.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83536</ref_obj_id>
				<ref_obj_pid>83528</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hodgins, J., Raibert, M. H. 1990. Biped Gymnastics. International Journal of Robotics Research 9(2):115-132.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hodgins, J. K., Raibert, M. H. 1991. Adjusting Step Length for Rough Terrain Locomotion. IEEE Transactions on Robotics and Automation 7(3): 289-298.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155325</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Hodgins, J. K., Sweeney, E K, Lawrence, D. G. 1992. Generating Natural-looking Motion for Computer Animation. Proceedings of Graphics Interface '92,265-272.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ko, H., Badler, N. I. 1993. Straight-line Walking Animation based on Kinematic Generalization that Preserves the Original Characteristics. In Proceedings of Graphics Interface '93.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>141254</ref_obj_id>
				<ref_obj_pid>141248</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Laurent, B., Ronan, B., Magnenat-Thalmann, N. 1992. An Interactive Tool for the Design of Human Free-Walking Trajectories. In Proceedings of Computer Animation '92, 87-104.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Lien, S., Kajiya, J. T. 1984. A Symbolic Method for Calculating the Integral Properties of Arbitrary Nonconvex Polyhedra. IEEE Computer Graphics and Applications 4(5):35-41.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[McMahon, T. A. 1984. Muscles, Reflexes, and Locomotion. Princeton: Princeton University Press.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Magnenat-Thalmann, N. 1994. Tailoring Clothes for Virtual Actors, Interacting with Virtual Environments, Edited by MacDonald, L., and Vince, J., John Wiley &amp; Sons, 205-216.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166160</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ngo, J. T., Marks, J. 1993. Spacetime Constraints Revisited. Proceedings of SIGGRAPH, 343-350.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791474</ref_obj_id>
				<ref_obj_pid>791214</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[O'Brien, J. E, Hodgins, J. K., 1995, Dynamic Simulation of Splashing Fluids. Proceedings of Computer Animation '95, 198-205.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Pai, D. 1990. Programming Anthropoid Walking: Control and Simulation. Cornell Computer Science Tech Report TR 90-1178.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Playter, R. R., Raibert, M. H. 1992. Control of a Biped Somersault in 3D. In Proceedings of the IEEE International Conference on Robotics and Automation, 582-589.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6152</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Raibert, M. H. 1986. Legged Robots That Balance. Cambridge: MIT Press.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122755</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Raibert, M. H., Hodgins, J. K. 1991. Animation of Dynamic Legged Locomotion. Proceedings of SIGGRAPH, 349-356.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. W. 1987. Flocks, Herds, and Schools: A Distributed Behavioral Model. Proceedings of SIGGRAPH, 25-34.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Rosenthal, D. E., Sherman, M. A. 1986. High Performance Multibody Simulations Via Symbolic Equation Manipulation and Kane's Method. Journal of Astronautical Sciences 34(3):223-239.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192167</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Sims, K. 1994. Evolving Virtual Creatures. Proceedings of SIG- GRAPH, 15-22.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1667946</ref_obj_id>
				<ref_obj_pid>1667943</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Sims, K. 1994. Evolving 3D Morphology and Behavior by Competition. Artificial Life IV, 28-39.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Takei, Y., 1990. Techniques Used by Elite Women Gymnasts Performing the Handspring Vault at the 1987 Pan American Games. International Journal of Sport Biomechanics 6:29-55.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Terzopoulos, D., Fleischer, K. 1988. Modeling Inelastic Deformation: Viscoelasticity, Plasticity, Fracture. Proceedings of SIGGRAPH, 269- 278.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Wooten, W., Hodgins, J. K. 1995. Simulation of Human Diving. To appear in Proceedings of Graphics Interface '95.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[van de Panne M., Fiume, E. 1993. Sensor-Actuator Networks. Proceedings of SIGGRAPH, 335-342.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97904</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[van de Panne M., Fiume, E., Vranesic, Z. 1990. Reusable Motion Synthesis Using State-Space Controllers. Proceedings of SIGGRAPH, 225-234.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Witkin, A., Kass, M. 1988. Spacetime Constraints. Proceedings of SIGGRAPH, 159-168.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animating Human Athletics Jessica K. Hodgins Wayne L. Wooten David C. Brogan James F. O Brien College 
of Computing, Georgia Institute of Technology ABSTRACT This paper describes algorithms for the animation 
of men and women performing three dynamic athletic behaviors: running, bicycling, and vaulting. We animate 
these behaviors using control algorithms that cause a physically realistic model to perform the desired 
maneuver. For example, control algorithms allow the simulated humans to maintain balance while moving 
their arms, to run or bicycle at a variety of speeds, and to perform a handspring vault. Algorithms for 
group behaviors allow a number of simulated bicyclists to ride as a group while avoiding simple patterns 
of obstacles. We add secondary motion to the animations with spring­mass simulations of clothing driven 
by the rigid-body motion of the simulated human. For each simulation, we compare the computed motion 
to that of humans performing similar maneuvers both qualitatively through the comparison of real and 
simulated video images and quantitatively through the comparison of simulated and biomechanical data. 
Key Words and Phrases: computer animation, human motion, motion control, dynamic simulation, physically 
realistic modeling.  INTRODUCTION People are skilled at perceiving the subtle details of human motion. 
We can, for example, often identify friends by the style of their walk when they are still too far away 
to be recognizable otherwise. If synthesized human motion is to be compelling, we must create actors 
for computer animations and virtual environments that appear realistic when they move. Realistic human 
motion has several components: the kinematics and dynamics of the .gure must be physically correct and 
the control algorithms must make the .gure perform in ways that appear natural. We are interested in 
the last of these: the design of control strategies for natural-looking human motion. In particular, 
this paper describes algorithms that allow a rigid­body model of a man or woman to stand, run, and turn 
at a variety of speeds, to ride a bicycle on hills and around obstacles, and to perform a gymnastic vault. 
Figure 1 shows two examples of the animated behaviors. The rigid-body models of the man and woman Collegeof 
Computing,GeorgiaInstituteofTechnology,Atlanta,GA 30332-0280. [jkhjwlwjdbroganjobrienj]@cc.gatech.edu 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
 Figure 1: Images of a runner on the track in the 1996 Olympic Stadium and a gymnast performing a handspring 
vault in the Georgia Dome. are realistic in that their mass and inertia properties are derived from data 
in the biomechanics literature and the degrees of freedom of the joints are chosen so that each behavior 
can be completed in a natural-looking fashion. Although the behaviors are very different in character, 
the control algorithms are built from a common toolbox: state machines are used to enforce a correspondence 
between the phase of the behavior and the active control laws, synergies are used to cause several degrees 
of freedom to act with a single purpose, limbs without required actions in a particular state are used 
to reduce disturbances to the system, inverse kinematics is used to compute the joint angles that would 
cause a foot or hand to reach a desired location, and the low-level control is performed with proportional­derivative 
control laws. We have chosen to animate running, bicycling, and vaulting because each behavior contains 
a signi.cant dynamic component. For these behaviors, the dynamics of the model constrain the motion and 
limit the space that must be searched to .nd control laws for natural-looking motion. This property is 
most evident in the gymnastic vault. The gymnast is airborne for much of the maneuver, and the control 
algorithms can in.uence the internal motion of the joints but not the angular momentum of the system 
as a whole. The runner, on the other hand, is in contact with the ground much of the time and the joint 
torques computed by the control algorithms directly control many of the details of the motion. Because 
the dynamics do not provide as many constraints on the motion, much more effort went into tuning the 
motion of the runner to look natural than into tuning the motion of the gymnast. Computer animations 
and interactive virtual environments require a source of human motion. The approach used here, dynamic 
simulation coupled with control algorithms, is only one of several options. An alternative choice, motion 
capture, is now widely available in commercial software. The dif.culty of designing control algorithms 
has prevented the value of simulation from being demonstrated for systems with internal sources of energy, 
such as humans. However, simulation has several potential advantages over motion capture. Given robust 
control algorithms, simulated motion can easily be computed to produce similar but different motions 
while maintaining physical realism (running at 4mjs rather than 6 mjs for example). Real-time simulations 
also allow the motion of an animated character to be truly interactive, an important property for virtual 
environments in which the actor must move realistically in response to changes in the environment and 
in response to the actions of the user. And .nally, when the source of motion is dynamic simulation we 
have the opportunity to use multiple levels of simulation to generate either secondary motion such as 
the movement of clothing and hair or high-level motion such as obstacle avoidance and group behaviors. 
 BACKGROUND Research in three .elds is relevant to the problem of animating human motion: robotics, 
biomechanics, and computer graphics. Researchers in robotics have explored control techniques for legged 
robots that walk, run, balance, and perform gymnastic maneuvers. While no robot has been built with a 
complexity similar to that of the human body, control strategies for simpler machines provide basic principles 
that can be used to design control strategies for humanlike models. Raibert and his colleagues built 
and controlled a series of dynamic running machines, ranging from a planar machine with one telescoping 
leg to three-dimensional machines that ran on two or four legs. These machines walked, jumped, changed 
gait, climbed stairs, and performed gymnastic maneuvers ([14 16], [26 28]). The control algorithms for 
human running described in this paper build on these control algorithms by extending them for systems 
with many more controlled degrees of freedom and more stringent requirements on the style of the motion. 
Biomechanics provides the data and hypotheses about human motion required to ensure that the computed 
motion resembles that of a human performing similar maneuvers. The biomechanics literature contains motion 
capture data, force plate data, and muscle activation records for many human behaviors. These data were 
used to tune the control algorithms for running, bicycling, and balancing. Cavagna presents energy curves 
for walking and running as well as studies of energy usage during locomotion[8]. McMahon provides graphs 
of stance duration, .ight duration, and step length as a function of forward speed[21]. Gregor surveys 
biomechanical studies of bicyclists[13]. Takei presents biomechanical data of elite female gymnasts performing 
a handspring vault and relates the data to the scores that the gymnasts received in competition[33]. 
Many researchers in computer graphics have explored the dif.cult problems inherent in animating human 
motion. The Jack system developed at the University of Pennsylvania contains kinematic and dynamic models 
of humans based on biomechanical data[1]. It allows the interactive positioning of the body and has several 
built-in behaviors including balance, reaching and grasping, and walking and running behaviors that use 
generalizations of motion capture data[18]. Bruderlin and Calvert used a simpli.ed dynamic model and 
control algorithms to generate the motions of a walking human[6]. The leg model included a telescoping 
leg with two degrees of freedom for the stance phase and a compound pendulum model for the swing phase. 
A foot, upper body, and arms were added to the model kinematically, and were made to move in an oscillatory 
pattern similar to that observed in humans. Pai programmed a walking behavior for a dynamic model of 
a human torso and legs in a high-level fashion by describing a set of time-varying constraints, such 
as, maintain ground clearance during leg swing, lift and put down a foot, keep the torso vertical, 
and support the torso with the stance leg [25]. None of these approaches to generating motion for animation 
are automatic because each new behavior requires additional work on the part of the researcher. In recent 
years, the .eld has seen the development of a number of techniques for automatically generating motion 
for new behaviors and new creatures. Witkin and Kass[38], Cohen[10], and Brotman and Netravali[5] treat 
the problem of automatically generating motion as a trajectory optimization problem. Another approach 
.nds a control algorithm instead of a desired trajectory ([37], [36], [23], [31], and [32]). In contrast, 
the control algorithms described in this paper were designed by hand, using a toolbox of control techniques, 
our physical intuition about the behaviors, observations of humans performing the tasks, and biomechanical 
data. While automatic techniques would be preferable to hand design, automatic techniques have not yet 
been developed that can .nd solutions for systems with the number of controlled degrees of freedom needed 
for a plausible model of the human body. Furthermore, although the motion generated by automatic techniques 
is appealing, much of it does not appear natural in the sense of resembling the motion of a biological 
system. We do not yet know whether this discrepancy is because only relatively simple models have been 
used or because of the constraints and optimization criteria that were chosen. DYNAMIC BEHAVIORS The 
motion of each behavior described in this paper is computed using dynamic simulation. Each simulation 
contains the equations of motion for a rigid-body model of a human and environment (ground, bicycle, 
and vault), control algorithms for balancing, running, bicycling, or vaulting, a graphical image for 
viewing the motion, and a user interface for changing the parameters of the simulation. The user is provided 
with limited high-level control of the animation. For example, the desired velocity and facing direction 
for the bicyclist and runner are selected by the user. During each simulation time step, the control 
algorithm computes desired positions and velocities for each joint based on the state of the system, 
the requirements of the task and input from the user. Proportional-derivative servos compute joint torques 
based on the desired and actual value of each joint. The equations of motion of the system are integrated 
forward in time taking into account the internal joint torques and the external forces and torques from 
interactions with the ground plane or other objects. The details of the human model and the control algorithm 
for each behavior are described below. Human Models The human models we used to animate the dynamic 
behaviors were constructed from rigid links connected by rotary joints with one, two or three degrees 
of freedom. The dynamic models were derived from the graphical models shown in .gure 2 by computing the 
mass and moment of inertia of each body part using algorithms for computing the moment of inertia of 
a polygonal object of uniform density[20] and density data measured from cadavers[11]. We also veri.ed 
that the model could perform maneuvers that rely on the parameters of the dynamic system using data from 
Frohlich[12]. The controlled degrees of freedom of the models are shown in .gure 2. Each internal joint 
of the model has a very simple muscle model, a torque source, that allows the control algorithms to apply 
Shoulder-3D Shoulder-3D Z X Y   Elbow-1D Y Wrist-2D Z Waist-3D X Y X Figure 2: The controlled degrees 
of freedom of the human model. The gymnast represented in the .gure has 15 body segments and a total 
of 30 controlled degrees of freedom. The runner has 17 body segments and 30 controlled degrees of freedom 
(two-part feet with a one degree of freedom joint at the ball of the foot and only one degree of freedom 
at the ankle), The bicyclist has 15 body segments and 22 controlled degrees of freedom (only one degree 
of freedom at the neck, hips, and ankles). The directions of the arrows indicates the positive direction 
of rotation for each degree of freedom. The polygonal models were purchased from Viewpoint Datalabs. 
a torque between the two links that form the joint. The equations of motion for each system were generated 
using a commercially available package[30]. The points of contact between the feet and the ground, the 
bicycle wheels and the ground, and the gymnast s handsandthevaultaremodeledusingconstraints. Theerrorsforthe 
constraints are the relative accelerations, velocities, and positions of one body with respect to the 
other. The constraints are stabilized using Baumgarte stabilization[2].  Running Running is a cyclic 
behavior in which the legs swing fore and aft and provide support for the body in alternation. Because 
the legs perform different functions during the phases of the locomotion cycle, the muscles are used 
for different control actions at different times in the cycle. When the foot of the simulated runner 
is on the ground, the ankle, knee, and hip provide support and balance. During the .ight phase, a leg 
is swung forward in preparation for the next touchdown. These distinct phases and corresponding changes 
in control actions make a state machine a natural tool for selecting the control actions that should 
be active at a particular time. The state machine and transition events used for the simulation of running 
are shown in .gure 3. To interact with the animation of the runner, the user speci.es desired values 
for the magnitude of the velocity on the ground plane and the facing direction. The control laws for 
each state compute joint torques that move the velocity and facing direction toward these desired values 
while maintaining balance. The animated runner can run at speeds between 2.5 m/s and 5 m/s and runs along 
a user-de.ned path. We call the leg that is on the ground or actively being positioned for touchdown 
the active leg. The other leg is called the idle leg. During .ight, the active leg is swung forward in 
anticipation of touchdown. Using the degrees of freedom of the leg in a synergistic fashion, the foot 
is positioned at touchdown to correct for errors in forward speed and to maintain balance. Forward speed 
is controlled by placing the average point of support during stance underneath the hip and taking into 
account the change in contact point from heel to metatarsus during stance. At touchdown, the desired 
distance from ball of foot heel touches leaves ground ground knee extended knee bends percentage of 
stance phase ball of foot touches complete ground Figure 3: A state machine is used to determine the 
control actions that should be active for running given the current state of the system. The transition 
events are computed for the active leg. At liftoff the active and idle legs switch roles. The states 
correspond to the points of contact on the ground: .ight, heel contact, heel and toe contact, and toe 
contact. The other two states, loading and unloading, are of very short duration and ensure that the 
foot is .rmly planted on the ground or free of the ground before the control actions for stance or .ight 
are invoked. the hip to the heel projected onto the ground plane is xhh 1j2(tsx..cos(O)lf )k(x. .x.d) 
(1) yhh 1j2(tsy..sin(O)lf )k(y. .y.d) (2) where ts is an estimate of the period of time that the foot 
will be in contact with the ground (based on the previous stance duration), .x and .yare the velocities 
of the runner on the plane, .xd and .yd are the desired velocities, Ois the facing direction of the runner, 
lf is the distance from the heel to the ball of the foot, and kis a gain for the correction of errors 
in speed. The length of the leg at touchdown is .xed and is used to calculate the vertical distance from 
the hip to the heel, zhh. Thedisturbancescausedbytheimpactoftouchdowncan be reduced by decreasing the 
relative speed between the foot and the ground at touchdown. This technique is called ground speed matching 
in the biomechanical literature. In this control system, ground speed matching is accomplished by swinging 
the hip further forward in the direction of travel during .ight and moving it back just before touchdown. 
The equations for xhh, yhh,and zhh, and the inverse kinematics of the leg are used to compute the desired 
knee and hip angles at touchdown for the active leg. The angle of the ankle is chosen so that the toe 
will not touch the ground at the same time as the heel at the beginning of stance. During stance, the 
knee acts as a passive spring to store the kinetic energy that the system had at touchdown. The majority 
of the vertical thrust is provided by the ankle joint. During the .rst part of stance, heel contact, 
the toe moves toward the ground because the contact point on the heel is behind the ankle joint. Contact 
of the ball of the foot triggers the transition from heel contact to heel and toe contact. The transition 
from heel and toe contact to toe contact occurs when the time since touchdown is equal to a percentage 
of the expected stance duration (30-50% dependingon forward speed). After the transition, the ankle joint 
is extended, causing the heel to lift off the ground and adding energy to the system for the next .ight 
phase. Throughout stance, proportional-derivative servos are used to compute torques for the hip joint 
of the stance leg that will cause the attitude of the body (roll, pitch, and yaw) to move toward the 
desired values. The desired angle for roll is zero except during turning when the body leans into the 
curve. The desired angle for pitch is inclined slightly forward, and the desired angle for yaw is set 
by the user or the higher-level control algorithms for grouping behaviors and obstacle avoidance. The 
idle leg plays an important role in locomotion by reducing disturbancestothebodyattitude causedbytheactivelegasitswings 
Z Figure 4: The four degrees of freedom of the bicycle model. The direction of the arrows indicates 
the positive direction of rotation for each degree of freedom. The polygonal model is a modi.cation of 
a model purchased from Viewpoint Datalabs. forward and in toward the centerline in preparation for touchdown. 
The idle leg is shortened so that the toe does not stub the ground, and the hip angles mirror the motion 
of the active leg to reduce the net torque on the body: CC.(j.j(3) xd xlo xd xlo ) CC.(j.j(4) yd ylo 
yd ylo ) where Cxd and Cyd are the desired rotations of the idle hip with respect to the pelvis, Cxlo 
and Cylo are the rotation of the idle hip at the previous liftoff, jxd and jyd are the desired position 
of the active hip, and jand jare the position of the active hip at the xlo ylo previous liftoff. The 
mirroring action of the idle leg is modi.ed by the restriction that the legs should not collide as they 
pass each other during stance. The shoulder joint swings the arms fore and aft in a motion that is synchronized 
with the motion of the legs: IkCy I0 (5) yd where Iyd is the desired fore/aft angle for the shoulder, 
kis a scaling factor, Cy is the fore-aft hip angle for the leg on the opposite side of the body, and 
I0 is an offset. The other two degrees of freedom in the shoulder (xand z) and the elbows also follow 
a cyclic pattern with the same period as Iyd . The motion of the upper body is important in running because 
the counter oscillation of the arms reduces the yaw oscillation of the body caused by the swinging of 
the legs. However, the details of the motion of the upper body are not constrained by the dynamics of 
the task and amateur athletes use many different styles of arm motion when they run. Observations of 
human runners were used to tune the oscillations of the arms to produce a natural-looking gait. The control 
laws compute desired values for each joint and proportional-derivative servos are used to control the 
position of all joints. For each internal joint the control equation is Tk(Od.O)kv(O. d.O.) (6) The desired 
values used in the proportional-derivative servos are computed as trajectories from the current value 
of the joint to the desired value computed by the control laws. Eliminating large step changes in the 
errors used in the proportional-derivative servos smoothes the simulated motion.  Bicycling The bicyclist 
controls the facing direction and speed of the bicycle by applying forces to the handlebars and pedals 
with his hands and feet. The rider is attached to the bicycle by a pivot joint between the bicycle seat 
and the pelvis (.gure 4). Spring and damper systems connect the hands to the handlebars, the feet to 
the pedals, and the crank to the rear wheel. The connecting springs are two-sided and the bicyclist is 
able to pull up on the pedals as if the bicycle were equipped with toe-clips and a .xed gear (no freewheel). 
The connection between the crank and the rear wheel includes an adjustable gear ratio. The bicycle wheels 
have a rolling resistance proportional to the velocity. The control algorithm adjusts the velocity of 
the bicycle by using the legs to produce a torque at the crank. The desired torque at the crank is Tcrank 
k(v.vd) (7) where kis a gain, vis the magnitude of the bicyclist s velocity, and vis the desired velocity. 
The force applied by each leg depends d on the angle of the crank because we assume that the legs are 
most effective at pushing downwards. For example, the front leg can generate a positive torque and the 
rear leg can generate a negative torque when the crank is horizontal. To compensate for the crank position, 
the desired forces for the legs are scaled by a weighting function between zero and one that depends 
on the crank position, Ocrank : sin(Ocrank)1 w (8) 2Ocrank is zero when the crank is vertical and the 
right foot is higher than the left. If Tcrank >0, the force on the pedal that the legs should produce 
is wTcrank fl (9) l (1 .w)Tcrank fr (10) l where fand fare the desired forces from the left and right 
legs lr respectively, and lis the length of a crank arm. If Tcrank is less than zero, then the equations 
for the left and right leg are switched. An inverse kinematic model of the legs is used to compute hip 
and knee torques that will produce the desired pedal forces. To steer the bicycle and control the facing 
direction, the control algorithm computes a desired angle for the fork based on the errors in roll and 
yaw: Ofork .ka(C.Cd).ka.C.k((j.jd)k(. j.(11) where C, Cd,and .Care the roll angle, desired roll, and 
roll velocity and j, jd,and j.are the yaw angle, desired yaw, and yaw velocity. ka, ka., k(,and k(. are 
gains. The desired yaw angle is set by the user or high-level control algorithms; the desired roll angle 
is zero. Inverse kinematics is used to compute the shoulder and elbow angles that will position the hands 
on the handlebars with a fork angle of Ofork . Proportional-derivativeservosmovetheshoulderand elbow 
joints toward those angles. These control laws leave the motion of several of the joints of the bicyclist 
unspeci.ed. The wrists and the waist are held at a nearly constant angle with proportional-derivative 
controllers. The ankle joints are controlled to match data recorded from human subjects[9]. Vaulting 
and Balancing To perform a vault, the gymnast uses a spring board to launch herself toward the vaulting 
horse, pushes off the horse with her hands, and lands on her feet on the other side of the horse. The 
vault described here, a handspring vault, is one in which the gymnast performs a full somersault over 
the horse while keeping her body extended in a layout position. This vault is structured by a state machine 
with six states: hurdle step, board contact, .rst .ight, horse contact, second .ight, and landing. The 
animation of the handspring vault begins during the .ight phase preceding the touchdown on the springboard. 
The initial conditions were estimated from video footage (forward velocity is 6.75 m/s and the height 
of the center of mass is 0.9 m). The simulated gymnast lands on a spring board that de.ects based on 
a linear spring and damper model. When the springboard reaches maximum de.ection, the control system 
extends the knees, pushing on the springboard and adds energy to the system. As the springboard rebounds, 
it launches the gymnast into the air and the .rst .ight state begins. Using a technique called blocking,the 
control system positions the hips forward before touchdown on the springboard so that much of the horizontal 
velocity at touchdown is transformed into rotational and vertical velocity at liftoff. During the .rst 
.ight state, the control system prepares to put the gymnast s hands on the horse by positioning her shoulders 
on the line between the shoulders and the desired hand position on the vault: Iyd .y .. (12) where Iyd 
is the desired shoulder angle relative to the body, .y is the angle between vertical and a vector from 
the shoulder to the desired hand position on the vault, and .is the pitch angle of the body (with respect 
to vertical). Because the shoulders are moving toward the vaultduring.ight, thiscontrollawperforms groundspeedmatching 
between the hands and the horse. The wrists are controlled to cause the hands to hit the horse palm down 
and parallel to the surface of the horse. During the next state, the gymnast s hands contact the vault 
and the arms are held straight. No torque is applied at the shoulder or the wrist and the angular and 
forward velocity of the gymnast carries her over the horse as she performs the handspring. When the hands 
leave the vault, the second .ight phase begins. During the second .ight state, the control system maintains 
a layout position with the feet spread slightly to give a larger area of support at touchdown. When the 
feet hit the ground, the control system must remove the horizontal and rotational energy from the somersault 
and establish an upright, balanced position. The knees and waist are bent to absorb energy. Vaulters 
land on soft, 4 cm thick mats that help to reduce their kinetic energy. In our simulation, the behavior 
of the mat is approximated by reducing the stiffness of the ground. When the simulated gymnast s center 
of mass passes over the center of the polygon formed by the feet, a balance controller is activated. 
After the gymnast is balanced, the control system straightens the knees and hips to cause the gymnast 
to stand up. The balance controller not only allows the gymnast to stand up after a landing but also 
compensates for disturbances resulting from the motion of other parts of the body while she is standing. 
For example, if the gymnast bends forward, the ankles are servoed to move the center of mass of the gymnast 
backwards. The balance controller also allows the simulated gymnast throw her arms back in a gesture 
of success after the vault and to take a bow.  HIGHER-LEVEL BEHAVIORS The control algorithms provide 
the animator with control over the velocity and facing direction of the runner and bicyclist. However, 
choreographing an animation with many bicyclists or runners would be dif.cult because the animator must 
ensure that they do not run into each other while moving as a group and avoiding obstacles. Building 
on Reynolds[29], we implemented an algorithm that allows bicyclists to move as a group and to avoid simple 
con.gurations of obstacles on the terrain. The performance of the algorithm for a simulation of a bike 
race on a hill is shown in .gure 5. In contrast to most previous implementations of algorithms for group 
behaviors, we use this algorithm to control a group where the members have signi.cant dynamics. The problem 
of controlling these individuals more closely resembles that faced by biologicalsystemsbecauseeachindividualhaslimited 
acceleration, velocity, and turning radius. Furthermore, the control algorithms for bicycling are inexact, 
resulting both in transient and steady-state errors in the control of velocity and facing direction. 
 The algorithm for group behaviors computes a desired position for each individual by averaging the location 
and velocity of its visible neighbors, a desired group velocity, and a desired position with respect 
to the visible obstacles. The details of this computation are presented in Brogan and Hodgins[4]. This 
desired position is then used as an input to the control algorithm for the bicyclist. The desired position 
is known only to the individual bicyclist and his navigational intent is communicated to the other cyclists 
only through their observation of his actions. The desired position for the bicycle that is computed 
by the algorithm for group behaviors is used to compute a desired velocity and facing direction: vd kpekv(vgl 
.v) (13) where vd is the desired velocity in the plane, vis the actual velocity, eis the error between 
the current position of the bicyclist and the desired position, kp is the proportional gain on position, 
kv is the proportional gain on velocity, and vgl is the group s global desired velocity (speci.ed by 
the user). SECONDARY MOTIONS While we are often not consciously aware of secondary motions, they can 
add greatly to the perceived realism of an animated scene. This property is well known to traditional 
animators, and much of the work in creating believable hand animation focuses on animating the motion 
of objects other than the primary actors. This effect can be duplicated in computer animation by identifying 
the objects in the environment that should exhibit passive secondary behavior and including a simulation 
suitable for modeling that type of behavior. In some cases, the simulated motion of the passive secondary 
objects can be driven by the rigid body motion of the primary actors. As examples of this approach, we 
have simulated sweatpants and splashing water. The behavior of the sweatpants is computed by using the 
motion of the simulated runner to drive a passive system that approximates the behavior of cloth. Similarly, 
the motion of splashing water is driven by the motion of a platform diver when it impacts the water ([24] 
and [35]). Ideally, all objects that do not have active control could be implemented in this fashion. 
Unfortunately, computational resources and an incomplete understanding of physical processes restrict 
the size and types of the passive systems that we are able to simulate. Several methods for physically 
based animation of cloth have been described in the literature ([7], [22], [3], and [34]). Carignan[7] 
implemented a system that uses the motion of a kinematic human walker developed by Laurent[19] to drive 
the action of the cloth. Our approach is similar to that described by Terzopoulos and Fleischer[34]. 
We use an elastic model to de.ne the properties of the cloth. Collisions are detected using a hierarchical 
object grouping algorithm and resolved using inverse dynamics to compute reaction forces. Although our 
cloth model is not signi.cantly different from previous methods, our approach of using dynamically correct 
rigid body motion to drive the passive system results in an animated scene where all the motion is governed 
by a consistent set of physically based rules. DISCUSSION This paper presents algorithms that allow 
an animator to generate motion for several dynamic behaviors. Animations of platform diving, unicycle 
riding and pumping a swing have been described Figure 5: Images of an athlete wearing sweat pants running 
on a quarter mile track in the 1996 Olympic Stadium, a gymnast performing a handspring vault in the Georgia 
Dome, a bicyclist avoiding a jogger, a group of bicyclists riding around a corner during a race, a group 
of runners crossing the .nish line, and a comparison between a simulated and a human runner on a treadmill. 
In each case, the spacing of the images in time is equal with the stadium runner at intervals of 0.066 
s, the gymnast at 0.5 s, the single bicyclist at 1.0 s, the group of bicyclists at 0.33 s, the group 
of runners at 0.5 s and the composite of the simulated and human runner at 0.066 s. elsewhere ([35], 
[17]). Taken together with previous work, these dynamic behaviors represent a growing library. While 
these behaviors do not represent all of human motion or even of human athletic endeavors, an animation 
package with ten times this many behaviors would have suf.cient functionality to be interesting to students 
and perhaps even to professional animators. Several open questions remain before the value of simulation 
as a source of motion for animation and virtual environments can be conclusively demonstrated: How can 
we make it easier to generate control algorithms for a new behavior? This paper partially addresses that 
question by presenting a toolbox of techniques that can be used to construct the control algorithms for 
a set of diverse behaviors. However, developing suf.cient physical intuition about a new behavior to 
construct a robust control algorithm remains time consuming. We hope that these examples represent a 
growing understanding of the strategies that are useful in controlling simulations of human motion and 
that this understanding will lead to the development of more automatic techniques. What can we do to 
reduce the number of new behaviors that need to be developed? One idea that has been explored by researchers 
in the domain of motion capture and keyframing is to perform transitions between behaviors in an automatic 
or semiautomatic fashion. Such transitions may be much more amenable to automatic design than the design 
of entire control algorithms for dynamic simulations. What rules can we add to the system to improve 
the naturalness of the motion? The techniques presented here are most effective for behaviors with a 
signi.cant dynamic component because the dynamics constrain the number of ways in which the task can 
be accomplished. When the gross characteristics of the motion are not constrained by the dynamics of 
the system, the task can be completed successfully but in a way that appears unnatural. For example, 
the simulated runner can run while holding his arms .xed at his sides, but an animation of that motion 
would be amusing rather than realistic. Humans are strong enough and dextrous enough that simple arm 
movements such as picking up a coffee cup can be completed in many different ways. In contrast, only 
good athletes can perform a handspring vault and the variations knee (rad) 3 3 2 2 1 1 0 0 Figure 
6: A phase plot of the hip and knee angles seen in the simulated runner (left) and measured in human 
subjects (right). The simulated motion is qualitatively similar to the measured data. Variables Mean 
Human Min Max Simulation Mass (kg) 47.96 35.5 64.0 64.3 Height (m) 1.55 1.39 1.66 1.64 Board contact 
(s) 0.137 0.11 0.15 0.105 First .ight (s) 0.235 0.14 0.30 0.156 Horse contact (s) 0.245 0.19 0.30 0.265 
Second .ight (s) 0.639 0.50 0.78 0.632 Horizontal velocity (m/s) Board touchdown 6.75 5.92 7.25 6.75 
Board liftoff 4.61 3.97 5.26 4.01 Horse touchdown 4.61 3.97 5.26 4.01 Horse liftoff 3.11 2.48 3.83 2.83 
Vertical velocity (m/s) Board touchdown -1.15 -1.54 -.71 -1.13 Board liftoff 3.34 2.98 3.87 3.81 Horse 
touchdown 1.26 0.74 2.39 2.13 Horse liftoff 1.46 0.56 2.47 1.10 Aver. vertical force (N) Board contact 
2175 1396 2792 5075 Horse contact 521 309 752 957  Table 1: Comparison of velocities, contact times, 
and forces for a simulated vaulter and human data measured by Takei. The human data was averaged from 
24 subjects. The simulated data was taken from a single trial. seen in their performances are relatively 
small. When the dynamics do not signi.cantly constrain the task, the control algorithms must be carefully 
designed and tuned to produce motion that appears natural while matching the key features of the behavior 
when performed by a human. The tuning process might be aided by data from psychophysical experiments 
that would provide additional constraints for the motion. Can human motion be simulated interactively? 
To be truly interactive, the motion of synthetic actors in virtual environments footage of a human runner 
and images of the simulated runner. This comparison represents one form of evaluation of our success 
in generating natural-looking motion. Figure 6 shows biomechanical data for running and represents another 
form of validation. Table 1 compares data from female gymnasts[33] and data from the vault simulation. 
From the perspective of computer graphics, the .nal test would must be computed in real time (simulation 
time must be less than wall clock time). Our implementation of the bicyclist runs ten times slower than 
real time on a Silicon Graphics Indigo2 Computer with an R4400 processor. We anticipate that with improved 
dynamic simulation techniques, and the continued increase in workstation speed, a three-dimensional human 
simulation will run in real time within a few years. A related question is whether the behaviors are 
robust enough for the synthetic actors to interact in a natural fashion with unpredictable be a form 
of the Turing Test. If simulated data and motion capture data were represented using the same graphical 
model, would the audience occasionally choose the simulated data as the more natural motion? The user 
may .nd it easy to identify the motion source because motion capture data often has noise and registration 
problems with limbs that appear to change length and feet that slide on the ground. Simulated motion 
also has characteristic .aws, for example, the cyclic motion of the runner is repetitive allowing the 
eye to catch oscillations in the motion that are not visible in the human users. The runner can run at 
a variety of speeds and change direction, but abrupt changes in velocity or facing direction will cause 
him to fall down. The planning or reactive response motion of the human runner. The animations described 
in this paper and a Turing test comparison with motion capture data can be seen on the WWW algorithms 
that lie between the locomotion control algorithms and at http://www.cc.gatech.edu/gvu/animation/Animation.html 
the perceptual model of the simulated environment will have to take in account the limitations of the 
dynamic system and control system. One goal of this research is to demonstrate that dynamic simulation 
of rigid-body models can be used to generate natural­looking motion. Figure 5 shows a side-by-side comparison 
of video  ACKNOWLEDGMENTS The authors would like to thank Debbie Carlson and Ron Metoyer for their 
help in developing our simulation and rendering environ­ment, Jeremy Heiner and Tom Meyer for their modeling 
expertise, Amy Opalak for digitizing the motion of the bicyclist, John Sny­der and the User Interface 
and Graphics Research group at Mi­crosoft for allowing the use of their collision detection system, and 
the CAD Systems Department at the Atlanta Committee for the Olympic Games for allowing us to use models 
of the Olympic venues. This project was supported in part by NSF NYI Grant No. IRI-9457621, by Mitsubishi 
Electric Research Laboratory, and by a Packard Fellowship. Wayne Wooten was supported by a Intel Foundation 
Graduate Fellowship.  REFERENCES [1] Badler, N. I., Phillips, C. B., Webber, B. L. 1993. Simulating 
Humans. Oxford: Oxford University Press. [2] Baumgarte, J. 1972. Stabilization of Constraints and Integrals 
of Motion in Dynamical Systems. Computer Methods in Applied Mechanics and Engineering 1:1 16. [3] Breen, 
D. E., House, D. H., Wozny, M. J., 1994. Predicting the Drape of Woven Cloth Using Interacting Particles. 
Proceedings of SIGGRAPH, 365 372. [4] Brogan, D. C., Hodgins J. K. 1995. Group Behaviors for Systems 
with Signi.cant Dynamics. To appear in IEEE/RSJ International Conference on Intelligent Robot and Systems. 
[5] Brotman, J. S., Netravali, A. N. 1988. Motion Interpolation by Optimal Control. Proceedings of SIGGRAPH, 
309 315. [6] Bruderlin, A., Calvert, T. W. 1989. Goal-Directed, Dynamic Anima­tion of Human Walking. 
Proceedings of SIGGRAPH, 233 242. [7] Carignan, M., Yang, Y., Magnenat-Thalmann, N., Thalmann, D. 1992. 
Dressing Animated Synthetic Actors with Complex Deformable Clothes. Proceedings of SIGGRAPH, 99 104. 
[8] Cavagna, G. A., Thys, H., Zamboni, A. 1976. The Sources of External Work in Level Walking and Running. 
Journal of Physiology 262:639 657. [9] Cavanagh, P., Sanderson, D. 1986. The Biomechanics of Cycling: 
Studies of the Pedaling Mechanics of Elite Pursuit Riders. In Science of Cycling, Edmund R. Burke (ed), 
Human Kinetics: Champaign, Ill. [10] Cohen, M. F. 1992. Interactive Spacetime Control for Animation. 
Proceedings of SIGGRAPH, 293 302. [11] Dempster, W. T., Gaughran, G. R. L. 1965. Properties of Body Segments 
based on Size and Weight. American Journal of Anatomy 120: 33 54. [12] Frohlich, C. 1979. Do springboard 
divers violate angular momentum conservation? American Journal of Physics 47:583 592. [13] Gregor, R. 
J., Broker, J. P., Ryan, M. M. 1991. Biomechanics of Cycling Exercise and Sport Science Reviews Williams 
&#38; Wilkins, Philadelphia, John Holloszy (ed), 19:127 169. [14] Hodgins, J. K. 1991. Biped Gait Transitions. 
In Proceedings of the IEEE International Conference on Robotics and Automation, 2092 2097. [15] Hodgins, 
J., Raibert, M. H. 1990. Biped Gymnastics. International Journal of Robotics Research 9(2):115 132. [16] 
Hodgins, J. K., Raibert, M. H. 1991. Adjusting Step Length for Rough Terrain Locomotion. IEEE Transactions 
on Robotics and Automation 7(3): 289 298. [17] Hodgins, J. K., Sweeney, P. K, Lawrence, D. G. 1992. Generating 
Natural-looking Motion for Computer Animation. Proceedings of Graphics Interface 92, 265 272. [18] Ko, 
H., Badler, N. I. 1993. Straight-line Walking Animation based on Kinematic Generalization that Preserves 
the Original Characteristics. In Proceedings of Graphics Interface 93. [19] Laurent, B., Ronan, B., Magnenat-Thalmann, 
N. 1992. An Interactive Tool for the Design of Human Free-Walking Trajectories. In Proceed­ings of Computer 
Animation 92, 87 104. [20] Lien, S., Kajiya, J. T. 1984. A Symbolic Method for Calculating the Integral 
Properties of Arbitrary Nonconvex Polyhedra. IEEE Computer Graphics and Applications 4(5):35 41. [21] 
McMahon,T.A.1984. Muscles, Re.exes, and Locomotion. Princeton: Princeton University Press. [22] Magnenat-Thalmann, 
N. 1994. Tailoring Clothes for Virtual Actors, Interacting with Virtual Environments, Edited by MacDonald, 
L., and Vince, J., John Wiley &#38; Sons, 205 216. [23] Ngo, J. T., Marks, J. 1993. Spacetime Constraints 
Revisited. Proceedings of SIGGRAPH, 343 350. [24] O Brien, J. F., Hodgins, J. K., 1995, Dynamic Simulation 
of Splashing Fluids. Proceedings of Computer Animation 95, 198 205. [25] Pai, D. 1990. Programming Anthropoid 
Walking: Control and Simulation. Cornell Computer Science Tech Report TR 90-1178. [26] Playter, R. R., 
Raibert, M. H. 1992. Control of a Biped Somersault in 3D. In Proceedings of the IEEE International Conference 
on Robotics and Automation, 582 589. [27] Raibert, M. H. 1986. Legged Robots That Balance. Cambridge: 
MIT Press. [28] Raibert, M. H., Hodgins, J. K. 1991. Animation of Dynamic Legged Locomotion. Proceedings 
of SIGGRAPH, 349 356. [29] Reynolds, C. W. 1987. Flocks, Herds, and Schools: A Distributed Behavioral 
Model. Proceedings of SIGGRAPH, 25 34. [30] Rosenthal, D. E., Sherman, M. A. 1986. High Performance Multi­body 
Simulations Via Symbolic Equation Manipulation and Kane s Method. Journal of Astronautical Sciences 34(3):223 
239. [31] Sims, K. 1994. Evolving Virtual Creatures. Proceedings of SIG-GRAPH, 15 22. [32] Sims, K. 1994. 
Evolving 3D Morphology and Behavior by Competi­tion. Arti.cial Life IV, 28 39. [33] Takei, Y., 1990. 
Techniques Used by Elite Women Gymnasts Performing the Handspring Vault at the 1987 Pan American Games. 
International Journal of Sport Biomechanics 6:29-55. [34] Terzopoulos, D., Fleischer, K. 1988. Modeling 
Inelastic Deformation: Viscoelasticity, Plasticity, Fracture. Proceedings of SIGGRAPH, 269 278. [35] 
Wooten, W., Hodgins, J. K. 1995. Simulation of Human Diving. To appear in Proceedings of Graphics Interface 
95. [36] van de Panne M., Fiume, E. 1993. Sensor-Actuator Networks. Proceedings of SIGGRAPH, 335 342. 
[37] van de Panne M., Fiume, E., Vranesic, Z. 1990. Reusable Motion Synthesis Using State-Space Controllers. 
Proceedingsof SIGGRAPH, 225 234. [38] Witkin, A., Kass, M. 1988. Spacetime Constraints. Proceedings of 
SIGGRAPH, 159 168. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218417</article_id>
		<sort_key>79</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[TicTacToon]]></title>
		<subtitle><![CDATA[a paperless system for professional 2D animation]]></subtitle>
		<page_from>79</page_from>
		<page_to>90</page_to>
		<doi_number>10.1145/218380.218417</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218417</url>
		<keywords>
			<kw><![CDATA[2D animation]]></kw>
			<kw><![CDATA[cel animation]]></kw>
			<kw><![CDATA[vector-based sketching]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P136466</person_id>
				<author_profile_id><![CDATA[81100194446]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jean-Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fekete]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LRI, CNRS URA 410, B&#226;timent 490, Universit&#233; de Paris-Sud, F91405 Orsay Cedex]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P92</person_id>
				<author_profile_id><![CDATA[81100316052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[&#201;rick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bizouarn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[2001 S.A., 2, rue de la Renaissance, F92184 ANTONY Cedex]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P80</person_id>
				<author_profile_id><![CDATA[81100136173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[&#201;ric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cournarie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[2001 S.A., 2, rue de la Renaissance, F92184 ANTONY Cedex]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P280094</person_id>
				<author_profile_id><![CDATA[81100151238]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Thierry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Galas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[2001 S.A., 2, rue de la Renaissance, F92184 ANTONY Cedex]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P84789</person_id>
				<author_profile_id><![CDATA[81100276635]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Fr&#233;d&#233;ric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taillefer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[2001 S.A., 2, rue de la Renaissance, F92184 ANTONY Cedex]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>102723</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adobe Systems Incorporated. PostScriptLanguage Reference Manual. Addison-Wesley, Reading, MA, USA, second edition, 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Adobe Systems Incorporated, 1585 Charleston Road, E O. Box 7900, Mountain View, CA 94039-7900, USA, Tel: (415) 961-4400. Adobe Premiere 1.0 User Guide, 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>24056</ref_obj_id>
				<ref_obj_pid>24054</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Austin Henderson, Jr. and Stuart K. Card. Rooms: The Use of Multiple Virtual Workspaces to Reduce Space Contention in a Window-Based Graphical User Interface. TOGS, 5(3):211-243, 1986.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192496</ref_obj_id>
				<ref_obj_pid>192426</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Thomas Baudel. A Mark-Based Interaction Paradigm for Free-Hand Drawing. In Proceedings of the ACM SIGGRAPH Symposium on User Intelface Software and Technology, Marina del Rey, 1994. ACM.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Nathaniel S. Borenstein and Ned Freed. MIME (Multipurpose Internet Mail Extension): Mechanism for Specifying and Describing the Format of Internet Message Bodies. Request for Comments: 1341, June 1992.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360357</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[N. Burtnyk and M. Wein. Interactive Skeleton Techniques for Enhancing Motion Dynamics in Key Frame Animation. Communications of the ACM, 19:564-569, 1976.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cambridge Animation. The Animo System.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Edwin E. Catmull. A hidden-surface algorithm with antialiasing. Computer Graphics (SIGGRAPH '78 Proceedings), 12(3):6-11, August 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807414</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Edwin E. Catmull. The problems of computer-assisted animation. Computer Graphics (SIGGRAPH '78 Proceedings), 12(3):348-353, August 1978.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Charles X. Durand. The "TOON" project: requirements for a computerized 2D animation system. Computers and Graphics, 15(2):285-293, 1991.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155328</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Jean-Daniel Fekete. A Multi-Layer Graphic Model for Building Interactive Graphical Applications. In Proceedings of Graphics Intelface '92, pages 294-300, May 1992.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>161322</ref_obj_id>
				<ref_obj_pid>161308</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Paula Ferguson. The Xll Input Extension: Reference Pages. The X Resource, 4(1):195-270, December 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Fundamentals of Interactive Computer Graphics. Addison-Wesley Publishing Company, second edition, 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74369</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Michel Gangnet, Jean-Claude Herv6, Thierry Pudet, and Jean- Manuel Van Thong. Incremental Computation of Planar Maps. In Jeffrey Lane, editor, Computer Graphics (SIGGRAPH '89 Proceedings), volume 23, pages 345-354, July 1989.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Michel Gangnet, Jean-Manuel Van Thong, and Jean-Daniel Fekete. Automated Gap Closing for Freehand Drawing. {Technical Sketch} Siggraph'94, Orlando, 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142180</ref_obj_id>
				<ref_obj_pid>142174</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Jakob Gonczarowski. A Fast Approach to Auto-tracing (with Parametric Cubics). In Robert A. Morris and Jacques Andr6, editors, Raster Imaging and Digital Typography III Papers from the second RIDT meeting, hem in Boston, Oct. 14- 16,1991, pages 1-15, New York, 1991. Cambridge University Press.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Claude Huhardeaux. The Label 35 System. Personal Communication, 1989.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563871</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Mark Levoy. A Color Animation System Based on the Multiplane Technique. In Computer Graphics (SIGGRAPH '77 Proceedings), volume 11, pages 65-71, Summer 1977.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Mark Levoy. Area Flooding Algorithms. In Two-Dimensional Computer Animation, Course Notes 9 for SIGGRAPH 82. ACM Press, New York, NY 10036, USA, July 1982.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63219</ref_obj_id>
				<ref_obj_pid>63218</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Mark A. Linton, John M. Vlissides, and Paul R. Calder. Composing user interfaces with InterViews. IEEE Computer, 22(2):8-22, February 1989.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Microsoft Corporation. SoftImage Toonz Feature Summary. Part No. 098-58493, One Microsoft Way, Redmond, WA 98052-6399, 1995.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Bill Perkins. The Creative Toonz System. Personal Communication, 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801153</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Michael Plass and Maureen Stone. Curve Fitting with Piecewise Parametric Cubics. Computer Graphics (SIGGRAPH '83 Proceedings), 17(3):229-239, July 1983.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Thomas Porter and Tom Duff. Compositing Digital Images. In Hank Christiansen, editor, Computer Graphics (SIGGRAPH '84 Proceedings), volume 18, pages 253-259, July 1984.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>57171</ref_obj_id>
				<ref_obj_pid>57167</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Richard L. Potter, Linda J. Weldon, and Ben Shneiderman. Improving the Accuracy of Touch Screens: an Experimental Evaluation of Three Strategies. In Proceedings of ACM CHI' 88 Conference on Human Factors in Computing Systems, pages 27-32, Washington, DC, 1988.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Thierry Pudet. Real Time Fitting of Hand-Sketched Pressure Brushstrokes. In Eurographics'94. Proceedings of the European Computer Graphics Conference and Exhibition, Amsterdam, Netherlands, 1994. North-Holland.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Barbara Robertson. Digital Toons. Computer Graphics WorM, pages 40-46, July 1994.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Barbara Robertson. Disney Lets CAPS Out of the Bag. Computer Graphics WorM, pages 58-64, July 1994.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>24053</ref_obj_id>
				<ref_obj_pid>22949</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Robert W. Scheifler and Jim Gettys. The X Window System. ACM Transactions on Graphics, 5(2):79-109, 1986.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90941</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Philip J. Schneider. An Algorithm for Automatically Fitting Digitized Curves. In Andrew S. Glassner, editor, Graphics Gems I, pages 612-626,797-807. Academic Press, 1990.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166118</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Thomas W. Sederberg, Peisheng Gao, Guojin Wang, and Hong Mu. 2D Shape Blending: An Intrinsic Solution to the Vertex Path Problem. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH '93 Proceedings), volume 27, pages 15-18, August 1993.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192191</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Michael A. Shantsis. A Model for Efficient and Flexible Image Computing. In Andrew Glassner, editor, Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 147-154. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0- 89791-667-0.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Peter Van Sommers. Drawing and Cognition. Cambridge University Press, Cambridge, 1984.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Jean-Michel Spiner. PEGS Technical Description VI.00, 1994.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806813</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Bruce A. Wallace. Merging and transformation of raster images for cartoon animation. Computer Graphics (SIGGRAPH '81 Proceedings), 15(3):253-262, August 1981.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TicTacToon: A Paperless System for Professional 2D Animation ´´ Jean-Daniel Fekete. Erick Bizouarn 
Eric Cournarie Thierry Galas Fr´ed´eric Taillefer 2001 S.A. 2, rue de la Renaissance, F92184 ANTONY Cedex 
 .LRI, CNRS URA 410, B atiment 490 Universit´e de Paris-Sud, F91405 ORSAY Cedex Abstract TicTacToon is 
a system for professional 2D animation studios that replaces the traditional paper-based production process. 
TicTac-Toon is the .rst animation system to use vector-based sketching and painting: it uses an original 
method to transform a pen trajectory with varying pressure into a stroke of varying thickness, in real­time. 
TicTacToon provides resolution independence, a virtually in­.nite number of layers, the ability to dynamically 
manage perspec­tive and sophisticated support for reuse of drawings. Other innova­tions include replacement 
of the rostrum model with a 3D model and integration into the overall 2D animation production process. 
TicTacToon is in daily use by 2D animation studios for a wide range of productions, from commercials 
to television series and even a feature .lm. The user interface enables professionals to sketch and draw 
as they do on paper. Over 100 professional ani­mators have used the system over a period of two years 
and most need less than an hour before beginning productive work. TicTac-Toon eliminates most tedious 
tasks and frees professional animators for more creative work. Keywords: 2D animation, vector-basedsketching, 
cel animation 1 Introduction The .eld of professional 2D animation has not pro.ted much from advances 
in computer-assisted animation. Most professional stu­dios still animate by hand, using a process that 
has changed little since the 1950 s. In striking contrast to related .elds such as com­mercials, art 
and 3D animation, 2D animation studios use comput­ers in a supporting rather than a central role. Walt 
Disney Feature Animation [28] is the exception; they have been using a computer­assisted system since 
1987. The key issues identi.ed by Edwin Cat­mull [9] 17 years ago remain issues today. Why are computer 
graphics tools so dif.cult to apply to 2D an­imation? It is not enough to simply solve technical problems. 
The studio must also be convinced. Today s creation process is essen­tially a production line, in which 
a studio of 50 to 300 people work together to produce tens of thousands of drawings for a single fea­ture 
.lm or television episode. Everyone has a speci.ed role and followsdetailedprocedurestomovefromonestagetothenext. 
Any This work was supported by the French Centre National de la Cin´e­matographie and by the Media Program 
of the EU. attempt to computerize the traditional set of tasks must take into ac­count overhead costs 
in both time and quality. This paper begins by describing the animation process, to illus­trate the problems 
faced by 2D animation studios. We then examine other solutions, partial or global, that have already 
been proposed. We then describe TicTacToon and evaluate it based on how well it addresses speci.c technical 
issues, handles user interface concerns and .ts within the social organization of an animation studio. 
1.1 The Traditional Animation Process Figure 1 shows the traditional animation process. Most steps in­volve 
an exposure sheet, which lists all the frames in a scene. Each line includes the phoneme pronounced by 
each character and the or­der and position in which the camera will shoot the .gures and the background. 
Each scene requires a set of stages, of which only the background can be painted in parallel with the 
animation stages (from key-frame to paint). The stages include: Story Board: Splits script into scenes 
with dialog and music. Sound Track: Records dialog and music in prototype form. Sound Detection: Fills 
the dialog column of an exposure sheet. Layout: Manages the drawing of backgrounds and main character 
positions, with speci.cations for camera movement and other animation characteristics. Background Painting: 
Paints the background according to the layout. Key Frame Animation: Draws extreme positions of characters 
as speci.ed by the layout. Provides instructions for the in­ betweeners. In-Betweening: Draws the missing 
frames according to the key­frame animator s instructions. Cleaning: Cleans up the drawings to achieve 
.nal quality of the strokes. Paint: Photocopies the clean drawings onto acetate celluloid (cels) and 
paints zones with water color. Check: Veri.es animation and backgrounds according to the lay­out and 
approves for shooting. Permission to make digital/hard copy of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage, the copyright notice, the title of the publication and its date appear, and notice 
is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. Record: Shoots frame-by-frame 
on .lm or video, using a rostrum (explained later).  2 State of the Art Robertson s [27] survey of commercial 
computer graphics systems identi.es two main types of animation systems: Ink and Paint and Automated 
In-Betweening. In both cases, all artwork is drawn on pa­per and later digitized and managed by the computer 
after the clean­ing stage (Figures 1(a) and 1(b)). 2.1 Ink and Paint Systems Ink and Paint systems perform 
the following steps, starting from scanned images of each animated character: .Remove noise from images. 
.Close gaps between strokes to prepare for paint. .Paint using seed-.ll. .Compose images of characters 
and backgrounds, applying zoom, rotation and pans as in a rostrum. .Record on .lm or video. 2.2 Automated 
In-Betweening Systems Automated In-Betweening systems begin with a scanned key frame and perform the 
following steps: .Clean and vectorize, sometimes using semi-automatic meth­ods. .Match pairs of drawings 
or match drawings to a template (see below). .Paint, if colors are not part of the template. .Interpolate 
in-betweens according to an exposure sheet. .Render and compose images of characters and backgrounds, 
applying zoom, rotation and pans as in a rostrum. .Record on .lm or video. .Put in database for reuse. 
 2.3 Technical Issues Catmull [9] and, more recently, Durand [10] discuss several tech­nical issues to 
address in order to provide an effective system. We review some of them here. 2.3.1 Input of drawings 
All commercial systems involve scanning drawings. Ink and Paint systems like PEGS [34] and Toonz [21] 
 must scan every draw­ing whereas Automated In-Betweening systems like Animo [7] need only scan the 
key drawings. Catmull mentions tablets as a possible solution, but Durand argues that, as sophisticated 
as they may be at this time, [they] could not offer the same versatility as tra­ditional tools. Toonz 
and PEGS scan drawings at a multiple of the .nal resolu­tion to avoid jaggies. PEGS can optionally micro-vectorize 
scanned animations. The Animo system provides tools for automatic tracing from the scanned key drawing. 
In all cases, an operator is required for checking the results. Story Board Sound Track Sound Detection 
Layout Key Frame Animation In-Betweening Cleaning Paint Background Paint Check Shoot  (a) The Ink 
and Paint Process Story Board Sound Track Sound Detection Layout Key Frame Animation In-Betweening Cleaning 
Paint Background Paint Check Shoot  (b) The Automated In-Betweening Process  Story Board Sound Track 
Sound Detection Layout Key Frame Animation In-Betweening Cleaning Paint Background Paint Check Shoot 
 Exposure Sheet + Database (c) The TicTacToon Process Figure 1: Work .ow of the different stages in 
animation Work done on computers is marked with a dark background 2.3.2 Automated In-Betweening The 
two main classes of Automated In-Betweening systems are based on templates [6, 7] or explicit correspondence 
[31]. In template-based systems, a designer creates a template for each char­acter. Animators must then 
restrict the character s movements ac­cording to the template. An operator must specify the correspon­dence 
between each key frame and the template. Explicit corre­spondence systems use two key drawings and generate 
in-betweens on a curve-by-curve basis. An operator is also required to specify the correspondence between 
the curves on the two key drawings. Sometimes, a zero-length curve must be created when a new detail 
appears or disappears between them. 2.3.3 Animation Painting Ink and Paint systems use an area .ooding 
algorithm [19]. All sys­tems optimize the painting when successivedrawings of a character contain zones 
that are mostly aligned and have the same color. Automated In-Betweening systems associate graphic attributes 
with zones during the speci.cation of correspondence. 2.3.4 Construction of Image Layers Ink and Paint 
systems only manipulate pixel images. They support transforms to the geometry (pan, zoom, rotate), as 
well as to the color intensity, as described in [35]. In addition, all the commercial systems can apply 
special effects speci.ed in the software expo­sure sheet to layers (color transforms, blur, transparency, 
etc.) Vector-basedsystemsuseavariantofthe scan-linealgorithm[13, pages 92 99] to transform their graphical 
structure into a pixel im­age. For graphical attributes, .at colors and constant transparency are simple 
to implement. Automated In-Betweening systems such as [7] can also use textures and shading on animated 
characters since they can maintain space coherence. 2.3.5 Composition of Image Layers Ink and Paint 
systems compose the layers using the alpha-blending arithmetic[24]. Vector-basedsystemscanchoosetocomposelayers 
at the pixel level or manage the composition during scan-conversion [8]. Commercial products provide 
no information about this point. For recording, all current computer-assisted systems use the ros­trum 
model to specify the way images are composed, positioned and turned relative to the camera. A rostrum 
includes a camera, a set of movable transparent trays holding the cels and a background area (Figure 
2). The camera axis is always perpendicular to the layers of cels, but can be panned, moved forward or 
back, and zoomed in or out. Cels can also be moved or rotated on the trays. The rostrum model makes managing 
an animation s perspective dif.cult. The animator must use traditional techniques for drawing perspective 
and translate it into the physical movement of the dif­ferent layers of the rostrum. The computer system 
can only check if the calculation was correct and help to .x errors. Although Levoy [18] has proposed 
using a 3D editor, no commercial system has im­plemented one. 2.3.6 Other Computer Tools Studios are 
not hostile to computer systems. They already use computer tools to assist them in stages such as storyboarding, 
sound detection, and layout. However, the data they produce must be re-entered into the process by hand. 
Storyboards can be .ne-tuned with an editing system (e.g. [2]) by mixing rough images with a sound track. 
The resulting storyboard must then be re-entered by hand. Camera Figure 2: A Rostrum. Sound Detection 
can be performed automatically by transforming the soundtrack into a listof phonemeswhich arethen betranscribed 
by hand onto an exposure sheet. Complicated layouts can be created with a 3D program, live action or 
rotoscoping. However, these cannot be integrated: the layout is printed on paper and used as in a traditional 
layout. Animation can be checked with pencil tests. The animators record several drawings on a computer 
and play the animation in real time. Each drawing must be input with a scanner or camera and a subset 
of the exposure sheet must be typed in. Although computers have been available at this stage for some 
time, they are not always ef.­cient. Animators must usually wait for the equipment, so they tend to draw 
more in-betweens than necessary. They later remove them, after checking the action on the pencil test 
machine.   3 Animation with TicTacToon TicTacToon is designed to support a paperless 2D animation pro­duction 
line. To be successful, we had to solve a number of tech­nical problems, provide a user interface that 
enhances the existing skills of professional animators and take into account the existing organizational 
structure of today s animation studios. This section describes the overall system and how it addresses 
each issue. 3.1 System Overview TicTacToon structures the stages of the production line into a set of 
tasks. A workstation is assigned to a speci.c stage and individual tasks are performed within rooms [3] 
listed below: .. Lobby Paint .. Storyboard/Database Render/Record .. Layout Scan .. Exposure Sheet Electronic 
Mail . Sketching/Animation Figure 1(c) shows that TicTacToon supports all stages of produc­tion, except 
sound track recording. Major features include: . a paperless process to avoid changing media, . resolution 
independence to use real perspective and to enable reuse of drawings at any zoom level, . a user interface 
that replicates the tools used by animators on paper, and . a 3D model to replace the rostrum model. 
Thesefeatures eliminate many tedious tasks and increase produc­tivity. TicTacToon also provides innovative 
solutions to the follow­ing problems: . drawing directly on a computer system, . painting on a vector-based 
system, and . providing an environment acceptable to traditional animators without too much training. 
TicTacToon runs on Digital Alpha AXP workstations using the X Window System [29] and a modi.ed version 
of the InterViews toolkit [20, 11]. TicTacToon does not require any special graphic hardware; it interacts 
with the digitizer using the X Input Extension protocol [12]. 3.2 Technical Issues The design of TicTacToon 
poses several technical problems. This section describes how TicTacToon handles sketching, painting and 
rendering. Sketching is vector-based and painting uses planar maps and a gap-closing technique. 3.2.1 
Vector-based sketching Vector-based sketching poses two main problems: .nding fast­enough algorithms 
and making a user interface suited to the anima­tor s work. Three steps are involved: providing graphical 
feedback as the user draws, vectorizing the curve when the pen goes up, and redrawing the curve in place 
of the traced feedback. Users should not notice the last two steps. The tablet sends an event every time 
the pen s state is modi.ed. The state contains a pressure level, ranging from 64 to 512, depend­ing on 
the digitizer, and X and Y coordinates, with a precision of 0.002 inches. The surface is 18 x12 . Current 
dig itizers can send up to 200 events per second if the user moves quickly enough. TicTacToon uses the 
brush/stroke model described in [26]. A brush can be any convex shape and is de.ned by a B´ezier path 
[1]. A stroke is a line drawn with a brush; the width is modulated by the amount of pressure applied 
to the pen. Pen pressure is normalized between 0 (no pressure) and 1 (full pressure). The brush shape 
is scaled according to the pen pressure and a linear modulation func­tion. This function is de.ned by 
two scaling factors for pressures 0 and 1. If the factors are equal, the result is a .xed-width pen. 
In gen­eral, though, the scale factors are 0 for pressure 0 and 1 for pressure 1. To compute the strokes, 
we use a variant of a fast curve .tting algorithm developed by Thierry Pudet. Compared to Plass [23], 
Schneider[30]andmorerecentlyGonczarowski[16], thisalgorithm tries to optimize the .tting time rather 
than the number of B´ezier segments that .t a set of sampled points. Least Squares Curve Fitting Previous 
curve .tting algo­rithms start with the samples, a tolerance parameter and perform the following steps: 
1. Filter the samples to reduce noise. 2. Find corners or other singular points. 3. De.ne an initial 
parameterization for the samples.  4. Perform a least-squares curve .tting. 5. If the distance between 
the curve and the samples greatly ex­ceeds the tolerance, split the samples in two parts, apply step 
3 to both parts and connect the resulting segments. 6. If the distance between the curve and the samples 
exceeds the tolerance, change the parameterization and restart at 4. 7. Otherwise, output the segment. 
 In step 5, two more operations should be performed when split­ting the samples: the splitting point 
should be chosen carefully and the resulting .tted curves should connect smoothly, i.e. the deriva­tive 
at the splitting point should be evaluated and the least-squares .t should be constrained to maintain 
the direction of the derivatives at the connecting ends. Step 5 is the bottleneck for this algorithm. 
Splitting the sam­ples at the point of maximum distance to the computed curve, as in [30, 16], is computationally 
expensive, since the distance must be computed for every sample. Yet, this step, together with step 6, 
is essential for minimizing the number of segments. Pudet avoids steps 6 and thus avoids computing the 
distance from the .tted curve to each sample point. Step 5 becomes: 5. If the distance between the curve 
and any sample point ex­ceeds the tolerance, split the samples in two parts at the mid­dle, apply step 
3 to both parts and connect the resulting seg­ments. 6. Otherwise, output the segment.  If the algorithm 
were used to .t cubic B´ezier segments, it would produce too many small segments. Instead, Pudet .ts 
quintic B´ezier segments, which have more freedom than cubic and tend to .t more samples without re-parameterization. 
Variable Width Stroke Pudet s algorithm performs a second curve .tting for the outline of the stroke. 
Our variation improves feedback by eliminating this second .tting. We recompute the curve s envelope 
when it is redrawn and cache it to avoid further re­computation. We draw the envelope of each curve by 
generating a single polygon for the whole outline of the curve s stroke. We have optimized the most common 
brush shapes, e.g., circles and ellipses. In orderto makepressureinformation independentofthe stroke 
s geometry, we de.ne a stroke pro.le that maps a number between 0 and 1 (the normalized curvilinear index 
or NCI) to the pressure at that point. For example, the NCI for a point halfway along the stroke would 
be .5. This technique speeds computation of the en­velope polygon and lets the stroke trajectory be modi.ed 
while still keeping pressure information. Another problem is noise: even the best digitizers suffer from 
both electronic interference and mechanical vibration. We apply a simple low pass .lter to the samples. 
The .tting in itself .lters the data, enabling the system to track the original gesture more accu­rately. 
In summary, curve .tting transforms the input samples into a list of quintic B´ezier segments and a stroke 
pro.le. Bene.ts include res­olution independence (i.e., smooth curves at all zoom levels), com­pact representation 
of the pen s trajectory and .ltering of the origi­nal stroke. We have tested the vector-based sketching 
technique with profes­sional animators for two years. This software requires a workstation with a fast 
FPU to achieve good performance; we used Digital AXP machines. The results have been very promising. 
The only problem reported involved performance when sketching and simultaneously running another program 
that loads the machine. Figure 3: A zone looks closed but is actually open at the upper left corner. 
We can envision a pathological case: drawing a very long stroke without releasing the pen. However, we 
found that this rarely occurs in practice. Animators only do it when cleaning a drawing and they still 
take more time to start a new stroke than for TicTacToon to draw the previous one. 3.2.2 Painting with 
planar maps and gap closing A drawing consists of a list of strokes. In order to paint it, we must de.ne 
a set of zones from these strokes, i.e. compute the topology of the drawing. We use the MapKernel library 
[14] to compute the planartopologyde.nedbyasetofB´ezierpaths. Thistopologyisin­crementally modi.ed when 
adding or removing edges and also sup­portshitdetection. Thesetofzonesisextractedfromtheplanarmap as 
a new set of B´ezier paths. To compute the planar map, TicTacToon uses the central path rather than the 
outlines of each stroke, which halvesthe planar map s size. This allows later modi.cations of the brush 
strokes, e.g., when reusing a character. However, using central paths causes a problem: a zone that appears 
closed may actually be open. Even if the edges of the strokes intersect, the central paths may not (see 
Figure 3). We use the algorithm we described in [15] that corrects this prob­lem and .nds the other small 
gaps that are always present in real drawings. Gaps are closed by adding invisible lines and changing 
the topology maintained by the MapKernel. This step takes sev­eral seconds and is usually run in the 
background to avoid making painters wait for the operation to .nish. Once the gaps have been closed, 
a painter simply selects a color and points to the relevant zones. Painted zones are inserted at the 
beginning of the drawing s display list as one graphical element. This element consists of a grouped 
list of background zones, described as B´ezier paths with a .ll color. Since painted zones are rendered 
differently, as discussed below, this element is given a special tag. 3.2.3 Rendering We use a painter 
s algorithm to render each graphic primitive into an image buffer which is then composed into the .nal 
image using alpha-blending arithmetic. Our graphical primitives are very similar to PostScript [1], with 
the following differences: .Instead of cubic B´ezier paths, we support B´ezier curves up to the 7th degree, 
though we only use quintics, cubics, quadratics and lines. . Pixel images can have an alpha channel. 
. Colors have an alpha component. . We use a brush shape and a stroke pro.le to modulate the stroke width. 
. Variations of attributes along and/or across the paths are sup­ported, thus providing shading facilities 
(see Figure 5). We Figure 4: A Lightbox used by animators. call them annotations. For now, they apply 
to color and trans­ parency. An intuitive interface is available to generate such annotations. The rendering 
algorithm proceeds by breaking down the strokes into elementary shaded polygons and scan-convert them. 
First, the centralB´ezierpath is discretized into line segmentsand normals are computed. Then, if the 
path only has a pressure pro­.le, the stroke outline is generated and scan-converted. Otherwise, if there 
is an annotation along the path, attribute values are computed for each vertex of the discretized path 
and the stroke is split into 4­sided shaded polygons. When there is an annotation across the path, each 
polygon must be sliced into smaller pieces. The polygons are then scan-converted into the image buffer. 
Af­ter the last polygon has been processed, the image buffer is merged with the .nal image. The image 
buffer is used for applying special effects such as blur. It also helps to alleviate one current limitation 
of the alpha-blending arithmetic, which assumesthat thearea ofa pixelcoveredbya poly­gon is randomly 
distributed over that pixel. Since the paths pro­duced by the paint program are contiguous and never 
overlap, if two polygons overlap the same pixel, the total area covered is the sum of each area, which 
violates the assumption. Our algorithm renders all the components of such structured graphics into the 
same image buffer and merges pixel values by adding each color/alpha compo­nent (the PLUS operator of 
[24]). The image buffer is then over­laid on the .nal image, using the regular OVER operator.  3.3 
User Interface Issues Animators usually spend eight hours a day drawing, so it is critical to make the 
user interface both comfortable and easy to use. We ob­served animators at work in order to understand 
their needs and then iteratively designed the system, responding to their feedback about a series of 
prototypes. This section describes how traditional animators work and then presents various aspects of 
TicTacToon s user interface, emphasiz­ing tools that support sketching and the layout module. Traditional 
animators work on a Lightbox (Figure 4). The disk is a sheet of translucent plastic that can be turned, 
using .nger holes at the top and bottom. A strip light behind the disk lights up all layers of tracing 
paper at once. Paper sheets are punched and inserted into peg bars, usually on top of the disk. The holes 
act as a reference and are used to ac­curately position the animations. Animators can work comfortably 
with this setup all day long. 3.3.1 Animation and Sketching Tools TicTacToon sAnimation Editor performs 
orenhancesthe following functions of a lightbox: . stacking drawings, . turning the light on or off, 
for stacks as well as individual drawings, . changing the position of underlying drawings without actually 
modifying the drawings themselves, . quickly.ippingbetweensuccessivesketchesto .ndandcheck the best stroke 
position for a movement, and . zooming, panning and turning the viewport. TicTacToon provides a set of 
tools to support sketching, .ipping among sketches and turning the viewpoint. Sketching: Animators begin 
drawing in-betweens by stacking the initial and .nal key drawings. They put a new sheet of paper on the 
lightbox, turn it on, and sketch an in-between, according to the key­frame animator s instructions. They 
regularly check their drawings by turning off the light. The in-between animator must sometimes superimpose 
different parts of key drawings. For example, if a char­acter jumps and his arm also moves, the movement 
of the arm may be seen more easily by superimposing the arms of the two key draw­ings. Translated into 
TicTacToon actions, an animator creates a new drawing, drags the two key drawings, drops them into position, 
and begins sketching with the digitizer pen. Different parts of the key drawing can also be superimposed 
on the current drawing. Flipping: Animators .ip between sketches to check that their an­imations are 
correct. This requires manual dexterity, since they place the drawings between their .ngers, and limits 
them to four or .ve drawings. They must also .ip non-sequentially, since drawings are stacked as key 
1, key 2 and in-betweens, rather than key 1, in­betweens and key 2. TicTacTooncan.ipanynumberofdrawingsinanyorder. 
Tocre­ate the impression of movement, it is important to switch between drawings in less than 5 milliseconds 
to maintain retinal persistency. We cannot guarantee this redraw speed since complex vector-based drawings 
can take an unbounded amount of time to draw. This is not a problem for most animators, but those who 
use a large number of strokes must check their animations with the pencil test module instead of .ipping. 
Alternatively, we could cache the drawings as Pixmaps. Turning the drawing: The human hand has mechanical 
constraints that limit its precision in some positions [33]. Animators can turn the disk to .nd the most 
comfortable and accurate drawing position. Most animators draw with the right hand and use the left for 
turning the drawing and turning the lightbox on and off. TicTacToonalsoletsanimatorsdrawwith onehandwhilelooking 
at the drawing on the screen. Special function keys are assigned to the keyboard so that the other hand 
need not move to perform the following functions: undo/redo, .ip up/.ip down, turn the viewport, and 
reset the viewport. Partial Editing of Strokes: We provide almost unlimited undo/redo. Animators make 
very few mistakes and would rather erase and redraw a stroke than twiddle with curve parts. We removed 
tools that enabled them to manipulate the B´ezier curves because they were distracting and never used 
constructively. We may reconsider this decision if new paradigms for re-editing strokes prove successful 
(e.g., [4]). Graphical feedback from the pen: We have experimented with several devices to provide feedback 
under the tracing pen. A ball­point pen can be inserted into some cordless digitizers. If the anima­tor 
places a sheet of paper on the digitizer, it is possible to draw with both the pen and the computer. 
However, animators do not like the feel of ball-point pens, which are too soft compared to their usual 
pencils. Moreover, they cannot undo a stroke on paper. We have also tried a cordless digitizer on an 
LCD screen. With current hardware, a distance of several millimeters separates the pen surface from the 
LCD screen. This produces a parallax error, simi­lar to that observed with touch-sensitive screens [25]. 
It is dif.cult for animators to continue their strokes because they often miss the expected starting 
position. Animators prefer to draw on a .at surface and look at the draw­ing on the screen. One animator 
gave a compelling argument as to why he preferred this approach. After three months of uninterrupted 
work using TicTacToon, he tried using paper again. He told us that he found it very annoying because 
his hand was always hiding some part of the drawing.  3.3.2 The Layout Module The Layout supports the 
whole animation process. It provides two views: the exposure sheet and a 3D view (Figure 8). The exposure 
sheet presents the logical structure of an animation: which charac­ters and backgrounds are present in 
a speci.c frame. The 3D view presents the graphical structure: the position of characters and back­grounds 
and the trajectory they follow. Unlike traditional animation, which provides only a static speci.­cation 
of a scene, TicTacToon begins with a playable version of each scene, using rough drawings. A layout animator 
can reuse anima­tions or backgrounds from a database and can check camera move­ments and synchronize 
them with character movements. A copy of the layout speci.cation is handed off to subsequent stages, 
which replace rough drawings with new work. Animators can check their drawings at any time, to ensure 
that they conform to the layout and that the action works. The layout stage can also check that work 
has been done correctly. Unlike the traditional rostrum model, TicTacToon positions an­imations in a 
3D world, in a way similar to Levoy [18]. However, the camera axis is always kept perpendicular to the 
drawings. This model acts like a theater background designed to be seen from only one perspective. As 
in 3D systems, the characters and camera are as­signed a general trajectory in 3D space. Since they can 
t turn around the X or Y axis, they use 2D instead of 3D general transforms. This model automatically 
maintains perspective and the correct stacking order. The columns of a traditional exposure sheet relate 
directly to the levels of a rostrum. The columns in TicTacToon s exposure sheet contain one character 
or a character part. An animation can be seen asalistofsuccessivedrawingsorgroupedintoacycle,whichis 
are­peatable list of consecutive drawings. For example, a walking char­acter is usually de.ned with a 
cycle of 8 drawings. TicTacToon ex­posure sheets handle cycles as structured objects that can be placed 
on a trajectory and tuned precisely. Prototype walks can be stored for most characters and reused as 
the basis for all their walks. Tele­vision serials only de.ne actions for a few speci.c angles: 0, 30o, 
60o,90o and the symmetricals. Storing reusable walks (and runs) saves time for both the layout and other 
animators. The Layout stage can check complex camera movements on pro­totypes and precisely tune the 
movement of each character. Ani­mators can modify prototype walks, as when a wounded character limps, 
and save time by reusing parts of the prototype walk. As in traditional animation, story boards consist 
of a set of small sketchesofimportantactionsin ascenewith anassociatedscript,in­cluding dialog, music, 
and intentions. Figure 6 shows a Story Board designed with TicTacToon. Traditional animation uses only 
eight mouth shapes (Figure 7). Wehavedevelopedamouth shapedetectionprogramthatavoidsus­ing phoneme recognition 
and is speaker and language independent. The success rate is around 90%, with errors only at the transitions. 
Mouth shapes can be edited, played and corrected and the result is directly inserted in the sound column 
of the exposure sheet.   Figure 5: Rendering effects on Figure 6: The story board Figure 7: The Voice 
Detection strokes. module. module.  Figure 8: The Layout module showing a front view, top view and 
side view of a scene. Over 100 professional animators in several studios have used TicTacToon over the 
past two years. So far, more than 90% were able to immediately switch from paper to TicTacToon. The unsuc­cessful 
animators included some who refused to try it, some who were afraid of computers in general and some 
who had dif.culties with speci.c aspects of the user interface. Animators have been the strongest supporters 
of TicTacToon and some have convinced their studios to adopt it. This is in strong contradiction with 
Durand in [10]. We addressed an early criticism of the digitizer s pen and re­placed it with a model 
that was both pressure sensitive and rigid. The other major criticism is the digitizer s surface which 
is too slippy for some and too soft for others. We .nd similar kinds of in­dividual preferences about 
pencil hardness and paper quality. Al­though most animators adapt to the digitizer s surface after about 
a month, we are still investigating possible improvements.  3.4 Social organization of the work To 
succeed, an animation system must provide more than useful technical features and a good user interface; 
it must also .t within the existing work context of an animation studio. Both formal and informal communication 
are essential for the smooth running of a studio. Since animators express themselves with cartoons as 
well as words, we provide a metamail compliant mailer [5] with TicTac-Toon. We receive mail messages 
from production sites all over the world, including formal (bug reports, request for information) and 
informal (jokes, tricks and caricatures), illustrating the same range of communication as would be found 
in a traditional animation stu­dio. TicTacToon supports a number of operations that help manage the production 
line. Animators exchange their work via exposure sheets and a distributed database. They also use electronic 
mail to notify the necessary people when their work is ready to be processed and get feedback about their 
own work.We do not want the studio staff to have to become system operators. TicTacToon does not re­quire 
prior computer skills and we have hidden operating system commands such as copying directories and naming 
.les. TicTacToon is designed to enhance the animation process, not just raise productivity. As 3D production 
studios increase the scope of their work, we expect that they will require the same level of or­ganizational 
support needed by today s 2D studios.  4 Discussion Current tools to support animation have both advantages 
and dis­advantages over traditional techniques. This section examines the advantages and disadvantages 
of two types of computer tools, Ink and Paint systems and Automated In-Betweening systems, and then discusses 
the relative advantages and disadvantages of TicTacToon. 4.1 Ink and Paint Systems 4.1.1 Advantages Speed 
up painting: Hand painting is the most tedious task in tra­ditional animation: using a computer can speed 
up the process by a factor of 5 [34]. The average speed is 100 drawings per day depend­ing upon the number 
of zones per drawing and the number of colors being used. In traditional animation, a maximum of 40 drawings 
can be painted per day, with an average of 20. Remove limits on the number of layers: Computers can also 
han­dle more than the .ve composition layers found in traditional ani­mation, which is limited by the 
opacity of acetate celluloid. Simplify shooting: A virtual rostrum is easier to use than a real ros­trum. 
Allow images from other sources: Highest quality backgrounds are still hand-painted and scanned in. However, 
images from other sources like paint programs, rendering programs or digitized live action can also 
be imported. 4.1.2 Disadvantages Increase the cleaning time: Films require huge numbers of draw­ings, 
in the order of tens of thousands. Computer systems can add manual actions to traditional work and machine 
processing time can be slow. For example, Ink and Paint systems require cleaner draw­ings than traditional 
animation does. Animators must spend extra time cleaning to avoid having the computer spend more time 
.lter­ing, which increases the cost of the hand-cleaning stage. Consume large resources: Reuses, though 
possible, are only made by large studios. With the exception of backgrounds, most images are usually 
removed from disk after recording. Pixel images require a lot of storage, especially if the .nal format 
is 35mm .lm. The technology required to manage such images remains expensive and without a planned long 
term production (over two years), the extra storage and network cost is not guaranteed to be balanced 
by the in­crease in productivity. Some systems address this by vectorizing the images after the cleaning 
stage [34], most others don t [21, 22]. Add medium change costs: Finally, several marginal costs result 
from the transition between paper and computer: . A special staff is required to scan and check drawings. 
. The exposure sheet must be typed into the system. . Information is not usually propagated back from 
the computer tothepaperworld. Sinceartistsdonotreceivefeedback,some problems recur. The overhead of transferring 
between paper and computer ac­counts for a minimum of 10% for a well-organized process to 25% or more 
for less well-organized processes [17] in the .nal cost.  4.2 Automated In-Betweening Systems 4.2.1 
Advantages Reduce the number of hand-drawn in-betweens: Computer­assisted In-betweening is designed to 
reduce the number of hand­drawn in-betweens and increase reuse of animation. Once a drawing hasbeen vectorizedand 
the correspondencespeci.ed,an animation can be tuned precisely. Some actions can be reused at different 
tem­pos which also decreases the number of hand-drawn drawings. Allow procedural rendering: Since zones 
and edges composing successive key drawings are matched, the interpolation process can automaticallymaintain 
spacecoherencebetweenprocedurallycom­puted attributes used for rendering. For example, the checked tex­ture 
of a character s shirt will be correctly animated when he moves his arms. Unlike with traditional animation, 
many special effects can be animated automatically. Like Ink and Paint Systems, Automated In-Betweening 
Systems allow an unlimited number of layers, an easier shooting and access to images from other sources. 
Painting takes a small portion of the key frame matching process.  4.2.2 Disadvantages Change the nature 
of in-betweening and limit its complexity: Crafting an animation with an Automated In-Betweening system 
is slow: We have found that inputing and tuning simple in-betweens takeaboutthesametimeasdrawingthembyhand. 
Evenwithrecent advances in algorithms, it remains dif.cult to match a drawing to a  Figure 9: Model 
of the Klaxon character and two key drawings very distorted from the model. template or two successive 
drawings to each other. Template-based systems require key drawings to conform to the model. (Figure 
9 il­lustrates how much key frames can differ from the model.) Explicit correspondence systems must match 
each key drawing to the pre­vious and next drawings. In both cases, the animation is restricted to fairly 
standard drawings; matching unusual images is very time­consuming. Traditional animators use a special 
notation on the key frames to specify the rhythm of the animation. Automated In-Betweening systems make 
it dif.cult to control this rhythm, which lowers the quality of the animation. Generating automated in-betweens 
is time-consuming and transforms the animator s work from drawing images to removing and tuning the slowing 
in and slowing out of an action. Tuning in-betweens requires a cross of skills from a tradi­tional animator 
and a computer graphist who can precisely manipu­late B´ezier curve handles. Few such animators exist. 
Require a skill in modeling: Template-based systems require both 3D modeling as well as traditional animation 
skills. Crafting a good model is expensive and must be taken into account when estimating the cost of 
the production. Given this cost, these systems are only cost-effective for productions with few characters. 
Does not provide as much reuse as expected: Animations are less reusable than might be expected. Reuse 
must be planned at the sto­ryboard stage, which is not integrated with the computer part of the process. 
All these problems contribute to the perception by anima­tion studios that computer assisted in-between 
programs produce poor quality animation. Moreover, like Ink and Paint systems, they also add medium change 
costs.   4.3 TicTacToon 4.3.1 Advantages Avoids medium change costs: With TicTacToon, all stages of 
the work are performed on computer (see Figure 10, with one worksta­tion being allocated to a speci.c 
stage. This avoids the cost of trans­ferring from paper to computer. Offers resolution independence: 
Vector-based backgrounds pro­vide both resolution independence and more importantly, an alpha channel. 
We also integrate pixel-based images, which can be made with a paint program or scanned in. Allows reuses: 
TicTacToon animations are vector-based (B´ezier­based) and require 20KB to 100KB per character. We provide 
a database system to manage and qualify animations. Distributes information in a playable form: Each 
stage has ac­cess to all information, from the earliest written scenario to the database of reusable 
images. Animators can test scenes at any time, to clarify the author s intent. Paper need not be carried 
from place to place, risking loss or damage and animators can spend more time at their desks drawing. 
Provides interactive tools to increase animator s productivity: TicTacToon provides a number of features 
that support high quality animation. Compared to Automated In-Betweening systems, ani­mators spend more 
time on art work and can check their work in­teractively. The animations themselves can be much more 
com­plicated, sometimes containing hundreds of layers. Scenes can be played and checked before even starting 
character animation and background painting. As with traditional animation, TicTacToon permits varying 
width strokes. Animators using TicTacToon can work up to 30% faster than with traditional animation. 
If we also include cut and paste and reuse of existing images, the savings are even greater. TicTacToon 
share all the advantages of Ink and Paint systems. Painting speed is roughly the same. By improving the 
rostrum model, it further simpli.es the shooting of complex scenes, using hundredsoflayers. Also,TicTacToonacceptsscannedbackgrounds 
and provides tools to manage them, including cut overlays (chroma key), assembly of background parts, 
and correct/adjust colorimetry. These tools are the most tedious to use and implement. Pixel-based backgrounds 
can be made device-independent by scanning them at high resolution (we recommend 400 to 600 dpi). Although 
this requires more storage, the relative number of back­grounds is small (in the thousands) compared 
to the number of ani­mated drawings. 4.3.2 Disadvantages Changes the nature of layout: Some traditional 
layout animators reported problems using the layout program. They are more familiar with the rostrum 
model than the 3D model and have learned how to cheat with perspective. However, layout animators who 
work with special effects had no problems. Does not allow fast enough .ipping for some animators: Anima­tors 
who use a large number of strokes can .nd the redraw latency too slow when .ipping between animations. 
These animators must use the pencil test module to check their animation. Some animators would like an 
in-betweening module for simple animations (see the section on Future Work). Does not change the nature 
of painting: Painting is still tedious, although we have tuned the user interface to be as simple as 
possible. The current interface is much faster, but keeps the painter so busy clicking the mouse to paint 
and using the keyboard to change the drawing that the job seems much more painful. The studios hire a 
non-skilled person to perform this task. We would like to avoid this gap in skill since it has important 
economic and social implications. Limits the quality of vector-based backgrounds: Vector-based backgrounds 
does not yet achieve the level of quality of pixel based images made by the best paint programs or scanning. 
  5 Future work We are enhancing TicTacToon in the following ways: Speed up the painting process: We 
would like to handle cleaning and painting in the same stage. This would require automatically .nding 
the color of a zone, either from a template or from a previ­ously painted cel. Support some procedural 
rendering: We already allow more graphical effects than used on traditional character animation. How­ever, 
we are working on adding procedural rendering like computed texture for strokes and 3D effects. Optimize 
rendering: Currently, a single frame can take from a few seconds to several minutes to compute. Many 
optimizations can be performed, including those described by Wallace [35] and Shantsis [32]. Others are 
more speci.c to vector-based drawings, such as caching rendered images for later reuse. However, since 
an image recordertakes30 secondsto shootaframe ona 35mm.lm, it is only important to increase rendering 
speed when images take more than 30 seconds. Implement Computer-Assisted In-Betweening: Animators usu­ally 
ask for assistance when in-betweening objects such as bounc­ing balls, falling snow, and air bubbles 
in water. We are working on a special module for this. Connect with a 3D system: We are currently working 
on inferring 3D features from 2D drawings. 6 Conclusion TicTacToon is a system for professional 2D animation 
studios that replaces the traditional paper-based production process. It provides a practical solution 
to the problem of converting from analog to dig­ital 2D animation. TicTacToon is the .rst animation system 
to use vector-based sketching and painting. Other innovations include replacing the ros­trum model with 
a constrained 3D model and integration of the sys­tem into the overall 2D animation production process. 
TicTacToon eliminates most tedious tasks, freeing animators for more creative work. We believe that TicTacToon 
can be the .rst of a new genera­tion of tools to support digital animation.  Acknowledgments This work 
has been done in collaboration with the former Paris Re­search Laboratory of Digital. Thanks to Michel 
Gangnet, Henry Gouraud, Thierry Pudet, Jean-Manuel Van Thong for their sup­port and their work on MapKernel 
and Fitlib. At 2001, thanks to Jean-Louis Moser, Olivier Arnaud and Gregory Denis for their work on TicTacToon. 
Wendy Mackay and Michel Beaudouin-Lafon provided support and advice for the writing of this article, 
Wendy rewrote most of it into readable English. Digital has been support­ing our work from the beginning, 
with special thanks to Jacques Lefaucheux.  References [1] Adobe Systems Incorporated. PostScript Language 
Reference Manual. Addison-Wesley, Reading, MA, USA, second edi­tion, 1990. [2] Adobe Systems Incorporated, 
1585 Charleston Road, P. O. Box 7900, Mountain View, CA 94039-7900, USA, Tel: (415) 961-4400. Adobe Premiere 
1.0 User Guide, 1993. [3] D. Austin Henderson, Jr. and Stuart K. Card. Rooms: The Use of Multiple Virtual 
Workspaces to Reduce Space Con­tention in a Window-Based Graphical User Interface. TOGS, 5(3):211 243, 
1986. [4] Thomas Baudel. A Mark-Based Interaction Paradigm for Free-Hand Drawing. In Proceedings of the 
ACM SIGGRAPH Symposium on User Interface Software and Technology,Ma­rina del Rey, 1994. ACM. [5] Nathaniel 
S. Borenstein and Ned Freed. MIME (Multipurpose Internet Mail Extension): Mechanism for Specifying and 
De­scribing the Format of Internet Message Bodies. Request for Comments: 1341, June 1992. [6] N. Burtnyk 
and M. Wein. Interactive Skeleton Techniques for Enhancing Motion Dynamics in Key Frame Animation. Com­munications 
of the ACM, 19:564 569, 1976. [7] Cambridge Animation. The Animo System. [8] Edwin E. Catmull. A hidden-surface 
algorithm with anti­aliasing. Computer Graphics (SIGGRAPH 78 Proceedings), 12(3):6 11, August 1978. [9] 
Edwin E. Catmull. The problems of computer-assisted ani­mation. Computer Graphics (SIGGRAPH 78 Proceedings), 
12(3):348 353, August 1978. [10] Charles X. Durand. The TOON project: requirements for a computerized 
2D animation system. Computers and Graph­ics, 15(2):285 293, 1991. [11] Jean-Daniel Fekete. A Multi-Layer 
Graphic Model for Build­ing Interactive Graphical Applications. In Proceedings of Graphics Interface 
92, pages 294 300, May 1992. [12] Paula Ferguson. The X11 Input Extension: Reference Pages. The X Resource, 
4(1):195 270, December 1992. [13] James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. 
Fundamentals of Interactive Computer Graphics. Addison-Wesley Publishing Company, second edi­tion, 1990. 
[14] MichelGangnet,Jean-ClaudeHerv´e,ThierryPudet,andJean-ManuelVan Thong. Incremental Computation of 
Planar Maps. In Jeffrey Lane, editor, Computer Graphics (SIGGRAPH 89 Proceedings), volume 23, pages 345 
354, July 1989. [15] Michel Gangnet, Jean-Manuel Van Thong, and Jean-Daniel Fekete. Automated Gap Closing 
for Freehand Drawing. [Technical Sketch] Siggraph 94, Orlando, 1994. [16] Jakob Gonczarowski. A Fast 
Approach to Auto-tracing (with Parametric Cubics). In Robert A. Morris and Jacques An­dr´e, editors, 
Raster Imaging and Digital Typography II Pa­pers from the second RIDT meeting, held in Boston, Oct. 14 
16, 1991, pages 1 15, New York, 1991. Cambridge University Press. [17] Claude Huhardeaux. The Label 35 
System. Personal Commu­nication, 1989. [18] Mark Levoy. A Color Animation System Based on the Mul­tiplane 
Technique. In Computer Graphics (SIGGRAPH 77 Proceedings), volume 11, pages 65 71, Summer 1977. [19] 
Mark Levoy. Area Flooding Algorithms. In Two-Dimensional Computer Animation, Course Notes 9 for SIGGRAPH 
82. ACM Press, New York, NY 10036, USA, July 1982. [20] Mark A. Linton, John M. Vlissides, and Paul R. 
Calder. Com­posing user interfaces with InterViews. IEEE Computer, 22(2):8 22, February 1989. [21] Microsoft 
Corporation. SoftImage Toonz Feature Summary. Part No. 098-58493, One Microsoft Way, Redmond, WA 98052-6399, 
1995. [22] Bill Perkins. The Creative Toonz System. Personal Commu­nication, 1994. [23] Michael Plass 
and Maureen Stone. Curve Fitting with Piece­wise Parametric Cubics. Computer Graphics (SIGGRAPH 83 Proceedings), 
17(3):229 239, July 1983. [24] Thomas Porter and Tom Duff. Compositing Digital Images. In Hank Christiansen, 
editor, Computer Graphics (SIGGRAPH 84 Proceedings), volume 18, pages 253 259, July 1984. [25] Richard 
L. Potter, Linda J. Weldon, and Ben Shneiderman. Improving the Accuracy of Touch Screens: an Experimen­tal 
Evaluation of Three Strategies. In Proceedings of ACM CHI 88 ConferenceonHumanFactorsinComputingSystems, 
pages 27 32, Washington, DC, 1988. [26] Thierry Pudet. Real Time Fitting of Hand-Sketched Pressure Brushstrokes. 
In Eurographics 94. Proceedings of the Euro­pean Computer Graphics Conference and Exhibition,Amster­dam, 
Netherlands, 1994. North-Holland. [27] Barbara Robertson. Digital Toons. Computer Graphics World, pages 
40 46, July 1994. [28] Barbara Robertson. Disney Lets CAPS Out of the Bag. Com­puter Graphics World, 
pages 58 64, July 1994. [29] Robert W. Schei.er and Jim Gettys. The X Window System. ACM Transactions 
on Graphics, 5(2):79 109, 1986. [30] Philip J. Schneider. An Algorithm for Automatically Fitting Digitized 
Curves. In Andrew S. Glassner, editor, Graphics Gems I, pages 612 626, 797 807. Academic Press, 1990. 
[31] ThomasW.Sederberg,PeishengGao,GuojinWang,andHong Mu. 2D Shape Blending: An Intrinsic Solution to 
the Vertex Path Problem. In James T. Kajiya, editor, Computer Graph­ics (SIGGRAPH 93 Proceedings), volume 
27, pages 15 18, August 1993. [32] Michael A. Shantsis. A Model for Ef.cient and Flexible Im­age Computing. 
In Andrew Glassner, editor, Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994),Com­puter 
Graphics Proceedings, Annual Conference Series, pages 147 154.ACMSIGGRAPH, ACMPress,July1994. ISBN0­89791-667-0. 
[33] Peter Van Sommers. Drawing and Cognition. Cambridge Uni­versity Press, Cambridge, 1984. [34] Jean-Michel 
Spiner. PEGS Technical Description V1.00, 1994. [35] Bruce A. Wallace. Merging and transformation of 
raster im­ages for cartoon animation. Computer Graphics (SIGGRAPH 81 Proceedings), 15(3):253 262, August 
1981.  (a) Rough drawing. (b) Clean drawing. (c) The cleaned animation sequence. (d) Painted drawing. 
(e) Final scene. Figure 10: A character at different stages in the TicTacToon system 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218419</article_id>
		<sort_key>91</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Fourier principles for emotion-based human figure animation]]></title>
		<page_from>91</page_from>
		<page_to>96</page_to>
		<doi_number>10.1145/218380.218419</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218419</url>
		<keywords>
			<kw><![CDATA[Fourier analysis]]></kw>
			<kw><![CDATA[emotion]]></kw>
			<kw><![CDATA[human figure animation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31098130</person_id>
				<author_profile_id><![CDATA[81332533092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Munetoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Research Laboratory, 7-1-1 Omika Hitachi Ibaraki 319-12, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39026528</person_id>
				<author_profile_id><![CDATA[81100085512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anjyo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Research Laboratory, 7-1-1 Omika Hitachi Ibaraki 319-12, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P250826</person_id>
				<author_profile_id><![CDATA[81100252629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryozo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hitachi Research Laboratory, 7-1-1 Omika Hitachi Ibaraki 319-12, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Badler, N., Phillips C., and Webber, B.L. Simulating Humans. Oxford University Press 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>113039</ref_obj_id>
				<ref_obj_pid>113034</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Boulic, R., Magnenat-Thalmann, N., and Thalmann, D. A Global Human Walking Model with Real-time Kinematic Personification. The Visual Computer. 6, 344-358, 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74357</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, A. and Calvert, T.W. Goal-directed, Dynamic Animation of Human Walking. Proceedings of SIGGRAPH 89 (Boston, Massachusetts, July 31- August 4, 1989). In Computer Graphics, 23, 3 (July 1989), 233- 242.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bruderlin, A., Teo, C.G. and Calvert, T.W. Procedural Movement for Articulated Figure Animation. Computers &amp; Graphics. 18,453-461, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cohen, M.F. Interactive Spacetime Control for Animation. Proceedings of SIGGRAPH 92 (Chicago, Illinois, July 26- 31, 1992). In Computer Graphics, 26, 2 (July 1992), 293-302.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325244</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Girard, M. and Maciejewski, A.A. Computational Modeling for Computer Generation of Legged Figures. Proceedings of SIGGRAPH 85 (San Francisco, California, July 23-27, 1985). In Computer Graphics, 19, 3 (July 1985), 263-270.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Koga, Y., Kondo, K., Kuffner, J. and Latombe, J-C. Planning Motions with Intentions. Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24- 29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994. ACM SIGGRAPH, pp. 395-408, 1994.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166159</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[van de Panne, M. and Fiume, E. Sensor-Actuator Networks. Proceedings of SIGGRAPH 93 (Anaheim, California, August 1- 6, 1993). In Computer Graphics Proceedings, Annual Conference Series, 1993. ACM SIGGRAPH, pp. 335-342, 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Sturman, D. Interactive Keyframe Animation of 3-D Articulated Models. Graphics Intelface '86, Tutorial on Computer Animation, 1986.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Unuma, M., and Takeuchi, R. Generation of Human Motion with Emotion. Proceedings of Computer Animation '91, pp. 77 - 88, 1991.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31463</ref_obj_id>
				<ref_obj_pid>31462</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Wilhelms, J. Using Dynamic Analysis for Realistic Animation of Articulated Bodies. IEEE Computer Graphics and Applications 7, 12-27, 1987.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fourier Principles for Emotion-based Human Figure Animation Munetoshi Unuma Ken Anjyo Ryozo Takeuchi 
Hitachi Research Laboratory Hitachi, Ltd. Abstract This paper describes the method for modeling human 
figure locomotions with emotions. Fourier expansions of experimental data of actual human behaviors serve 
as a basis from which the method can interpolate or extrapolate the human locomotions. This means, for 
instance, that transition from a walk to a run is smoothly and realistically performed by the method. 
Moreover an individual's character or mood, appearing during the human behaviors, is also extracted by 
the method. For example, the method gets "briskness" from the experimental data for a "normal" walk and 
a "brisk" walk. Then the "brisk" run is generated by the method, using another Fourier expansion of the 
measured data of running. The superposition of these human behaviors is shown as an efficient technique 
for generating rich variations of human locomotions. In addition, step-length, speed, and hip position 
during the locomotions are also modeled, and then interactively controlled to get a desired animation. 
CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation; I.3.7 [Computer 
Graphics]: Three-dimensional Graphics and Realism, Animation; I.6.3 [Simulation and Modeling]: Applications 
Additional Keywords and Phrases: Human figure animation, Fourier analysis, Emotion 1. Introduction Human 
or more generally articulated figure animations have been seen in a variety of application fields including 
advertising, entertainment, education, and simulation. The primary research goal now to further the animations 
is providing a system which allows animators to easily and interactively design and get desired movements. 
Many approaches to this goal are available, including key­framing[8], physics or robotics based methods 
[1, 3, 6, 11], and space-time control [5]. However many open problems remain and the required reality 
or complexity of the human animation may be rather different according to its application. 7-1-1 Omika 
Hitachi Ibaraki 319-12 Japan {unuma, anjyo, takeuchi}@hrl.hitachi.co.jp Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 In a general system for articulated figure 
animations, an articulated body is modeled with a hierarchy of rotational joints each of which may have 
up to three degrees of freedom. Moreover, motion control of the model is also hierarchically prescribed, 
such as script-based specifications at the highest level of control and key-framing for joint angles 
control at the lowest. By the use of higher level control, the system should reduce the user's load of 
direct specifications for the desired movement [7, 9]. This heavy task would be reducible by introducing 
into the system a knowledge database about the variety of movements. A basic problem is then how to get 
and construct the database, which is particularly crucial for describing human behaviors. In other words 
the problem is how to model the behaviors, since we cannot measure all kinds of human behaviors in advance. 
We also note that human behavior is usually affected by an individual's emotion or character (to some 
extent), which must therefore be modeled. In this paper we consider the problem of how to model human 
behaviors with emotions for the purpose of advertising or entertainment use. Therefore the solution of 
the problem should allow interactive and real-time control, while providing variations of movements, 
including cartoon­like exaggerations or expressions. The database of the human behaviors should then 
be concise and small enough for quick response and for needing limited storage in the computer's memory. 
In existing methods to tackle the problem, dynamic simulation [3, 11] or procedural models [2, 4] often 
have been used. However, dynamic simulation techniques require experiments (usually off-line) from the 
animator to get the desired result, whereas procedural approaches employ trial and error steps for suitable 
choice of the parameters involved. These inconveniences may occur particularly when animating individualized 
behaviors (such as with mood, characters or emotions). This is mainly because the existing methods do 
not directly deal with "emotion" or "mood" appearing with the human behavior. For instance, in making 
a "brisk" walk with a physics-based approach, if the animator wants to change the degree of briskness 
of the human model, then he has to adjust the parameters which are physically meaningful but do not directly 
prescribe nuances of emotion or mood. The method we propose in this paper is for making "emotion-based" 
human animations. Characteristics of human behaviors are extracted in the method simply from the empirical 
data of actual human movements, without any physics-based simulations. Based on the Fourier series expansion 
of the original measured data, a functional model is defined for generating a rich variation of movements, 
far from the original. A prototype of the functional model was originally introduced in our former work 
[10], where the prototype was used only for describing the emotional aspect of human locomotions through 
the Fourier analysis and synthesis. In making a human figure animation, however, we must treat not only 
the emotional aspect but also the kinematic aspect. Therefore the functional model in this paper is further 
extended to provide intuitive parameters for simultaneously controlling emotional and kinematic human 
locomotions, where the kinematic control, for example, prescribe speed and step-length of the human figure 
model. In addition real-time and interactive control is performed with the functional model and this 
consequently provides wider variations of human figure animations than previous approaches.  2. Functional 
Models for Human Locomotions 2.1 Preparation We first prepare a skeleton model represented with a hierarchical 
structure of rotational joints. The number of joints of the model and the degrees of freedom depend largely 
on the desired reality or quality. In this paper we employ a relatively simple model with nineteen joints 
which are hierarchically defined in Fig. 1. As shown in Fig. 2, each joint then has three degrees of 
freedom for rotation around x, y and z axes in the local coordinate system at each joint, where the y-axis 
is in the same direction as the stick direction of the previous joint. m Now we assume that the rotational 
joint angles T x, mm T y and T z at the m-th joint are measured for all the joints except when m = 8 
and 9, with a motion capturing system. The eighth and ninth joint angles' data are not used in our method 
(see Fig.1(a)), since tiptoe's landing is treated differently than other parts of the body (see 3.1). 
The obtained data set of the m-th joint is then of a form like {(T m (i. t), T my(i. t), T mz(i. t)) 
| i = 1, 2,..., n}, where . t x denotes the time interval specified in measuring with the capturing system, 
and m . 8 or 9. In addition, we assume that the obtained data set represents an (almost) periodic behavior, 
such as walking and running.  2.2 Rescaled Fourier Functional Model Based on the discrete data of the 
m-th joint angle, let us first construct a functional model with continuous parameter of t, which represents 
the periodic behavior of the joint angle. In general, however, the period of the functional model is 
rather hard to estimate from the measured data of the joint angle, since the original data may be "noisy". 
Then we employ the Fourier series expansion (approximation) of the 15 17  (a) Rotational joints considered. 
(b) Connection of human joints. Figure 1. Skeleton model used. joint angle as the functional model. For 
simplicity, hereafter, mm m T (t) denotes T x (t), T y (t), or T z (t). The period of mT m (t) for each 
m is originally the same value TT , and the Fourier coefficients are obtained with the sample values 
T m (tp), tp . [- TT /2, TT /2]. In practice, however, we can estimate these coefficients, based on the 
sample values in a larger interval. After rescaling time parameter t, we may suppose that the period 
of T (t) is normalized to be 2p . mThus we have the following expressions, which we call a rescaled Fourier 
functional model: T (t) = Am 0 + Ssin(n t + f m n). (1) m Amn n = 1 Similarly, based on the discrete 
sample data of the joint angles for another periodic behavior, we have . (t) = Bm 0 + S Bm n sin(n t 
+ . m n). (2) m n = 1 We use, again by rescaling, . (t) as described in (2), mwhereas the original period 
of . (t) may be different from mthat of T m. As explained later, the effect of "rescaling" in the above 
functional models occurs typically when we make a transition animation between two different locomotions, 
such as "from walking to running".  2.3 Interpolation, Extrapolation, and Transition We show, in this 
section, how the rescaled Fourier functional models are effectively used in making variations of human 
behaviors. Let us consider the following function of two variables s and t: . m (s, t) = {(1- s)Am 0 
+ sBm 0}+ S {(1- s)Am 0 + sBm 0}sin{n t +(1-s)f mn + s. m n}. (3) n = 1 In (3), if we suppose that 0 
= s = 1, . m is then an interpolant Figure 2. Local coordinate system of the model. between T m and 
. m in the mathematical sense that, as s varies from 0 to 1, . m continuously changes from T to m. . 
Moreover, in the context of human animations, we see mthat . m not only interpolates T and . , but also 
mm extrapolates them. This means that the animations obtained from . m still provide realistically human 
movements as the interpolant, while expressing exaggerated behaviors as the extrapolant. Fig. 3 shows 
an example; T m(t) (= . m(0, t)) represents a "normal" walk (Fig.3(a)) and . (t) (= . m(1, t)) shows 
a m"depressed" or "tired" walk (Fig.3(b)). If 0 < s < 1, we see "a little tired" walk (Fig.3(c)). If 
s > 1, then we see that the degree of being "tired" is amplified (Fig.3(d)). On the other hand, if s 
becomes less than 0, the model looks rather brisk (Fig.3(e)). A smooth transition from walk to run is 
also obtained, if we get . (t) from the measured data of a run, mwith T (t) being the above walk function. 
The interpolant m. m(s, t) (0 < s < 1) then gives the smooth transition. These examples illustrate well 
the rescaling effect involved in a rescaled Fourier functional model. For example, if we want to make 
the transition animation from walking to running by a traditional method, a skilled animator is required 
to consider many parameters, such as, speed, step, or one locomotion cycle. In particular the original 
data sets {T (t)} and {. (t)} have to be carefully mmsynchronized. This means, for instance, that the 
arms must reach their furthest swinging position at the same time in both data sets, before making the 
transition animation. Once we get the "rescaled" Fourier expressions as described in (1) and (2), the 
synchronization of the data sets are automatically made and the desired animation is easily obtained. 
Finally we also note that the number of sine functions (n in (1) - (3)) is rather small (our experiments 
show that n is usually 3, and 7 at most). This implies that human locomotions are characterized with 
the small number of Fourier coefficients and phases in the Fourier functional model. We have used . m 
(s, t) in (3) as an inter/extrapolant and for making a transition animation, based on the two different 
measured data. In this case parameter s travels in the frequency-phase domain. An alternative of . m 
(s, t) may be defined as (1- s)T (t) + . (t), where s moves in the mmjoint angle (time) domain. In both 
cases similar variations of human behaviors may be synthesized as long as we employ the rescaled Fourier 
functional models.  3. Fourier Characterizations for Human Animation 3.1 Step-Constraints Parameters 
Next we see that the rescaled Fourier functional model is endowed with the "step-constraints" parameters, 
which control kinematic aspects of human locomotions: step length, speed, hip position, etc. In the following, 
italics denote parameters' names appearing on the screen of the prototyping editor (see 3.3). Step: The 
step length is specified in the Fourier domain by adjusting the spectrum component. For example, when 
using T (t) in (1), replace Am n by stepAm n. Then mstep is controlled. If step becomes larger, the step 
length is longer (Fig. 4). Speed: Time-interval speed(= .t) can be independently specified. Based on 
the discrete values {T m(k.t )}k, the human animation is made. If speed tends to 0, then the human model 
smoothly stops walking. In particular we note that the above parameter step is easily introduced, because 
of the (rescaled) Fourier expressions of a measured data set. (a) Step = 1.0 (b)Step = 1.8 (c) Step 
= 0.5 Figure 4. "Step" effect. L(t) during a run  time (a) Trajectories of L(t) in one locomotion cycle 
(a) Normal walk (b) Tired walk  timetime (c) s = 0.5 (d) s = 2.0 (e) s = - 0.5 (b) With a large gait 
and a small (c) With a large gait and Figure 3. Interpolation and extrapolation of "tired" walk -jump 
a small jump (a) and (b) are obtained directly from the measured data Figure 5. Definition of hip position. 
sets, while (c) - (e) are by the method.  The next two parameters, gait and jump, relate to the hip 
position. To explain them, we must consider a human walking for a moment. Since one foot is always on 
the ground during the walk, we may assume that the following function L (t) is equal to the hip position 
LWalk (t): L (t) = max( Ll(t), Lr(t)), (4) where Ll(t) or Lr(t) means the vertical length of the left 
or right groin from the ground, respectively. Since locomotion is a periodic activity with a basic pattern 
of one locomotion stride, we need to observe only one step of the locomotion, as shown in Fig.5(a). Then 
we consider the case of running, in which L (t) is shown like Fig.5(a), and it does not express the hip 
position anymore. A run actually involves a flight state. Therefore, to define the hip position LRun 
(t) in the running case using L (t), we introduce the following parameters: Gait: The parameter prescribes 
the time of the flight state, taking values from 0 to 1. At every frame during the animation, the normalized 
value N(t) is compared with the value of gait, where N(t) = {Lmax - L(t)}/Lmax and Lmax = maxt{L(t)}. 
If gait = N(t), then we set LRun (t) = L(t). Otherwise we define . LRun (t) = L(t) + jump L (1- L(, 
(5) max where jump is another parameter described next. Intuitively, if gait becomes larger, the running 
movement looks more like a walk (Fig.6). Jump: This controls the height during the flight. Figs.5(b) 
and (c), for example, show typical cases. If jump is much larger 1.0, the flight state is rather exaggerated. 
 (a) gait = 0.06 (b) gait = 0.5 Figure 6. "Gait" ef fect.  3.2 Superposition of Human Behaviors In section 
2.3, we showed experimentally that the rescaled Fourier functional models successfully provide smooth 
transition of two different (measured) human movements. More generally, in this section, superposition 
of the functional models is used as an efficient technique for making emotion-based human figure animations. 
Let {F 1 (t)} and {F 2 (t)} be the rescaled Fourier mmfunctional models of two different measured data 
sets: F 1m(t) = A1m 0 + S A1 sin(n t + f 1m n), (6-1) mn n = 1 F 2 (t) = A2 A2 sin(n t + f 2 ). (6-2) 
mm 0 + Smn mn n = 1 Then we consider the following differences of the above Fourier coefficients: A12mn 
= A1 - A2 mnmn , (7-1) f 12m n = f 1m n -f 2mn . (7-2) The pairs {(A12m n , f 12 )}n (n = 0, 1, 2, ...) 
are mncalled the Fourier characteristics of the two measured data sets. As mentioned earlier, we may 
assume n is in practice rather small. Therefore it is believed that the Fourier characteristics provide 
a small and concise database of human behaviors. For example we can have the Fourier characteristics 
of "briskness", which are obtained from the two rescaled Fourier functional models of a normal walk and 
a "brisk" walk. Associated with these Fourier characteristics, we can define the Fourier characteristic 
function as . (t) = A12m 0 + S A12 sin(n t + f 12 ). (8) mmn mnn = 1 The superposition of human behaviors 
by our method is then stated as: Suppose that we have the Fourier characteristic function . (t) in (8), 
and that we have another measured mdata set with its rescaled Fourier functional model . (t) in m(2). 
Then, . and . give a rich variation of human m m behaviors through linear interpolation, extrapolation, 
and transition, if . and . express mutually meaningful mm human behaviors. Fig. 7 shows examples of this 
technique. In this case . (t) in the above statement expresses the running data for mFig. 7(a), while 
the Fourier characteristic function . (t) mdescribes "briskness" which is extracted from the data of 
a walk and of a "brisk" walk. Then we get a "brisk" run as shown in Fig. 7(b), where we employ . m(s, 
t) in (3) for linear transition. Similarly we have a "tired" or "depressed" run from the data of a run 
and "tiredness" in Fig. 7(c). In this way our experimental results assert that the superposition works 
well. (b) a "brisk" run (a) a run  (c) a "tired" run Figure 7. Superposition of human behaviors -"briskness" 
and "tiredness" are extracted from the measured data sets of walking. Formally, if we assume that F 2 
(t) in (8) is identically mzero, the techniques described in section 2.3 can be regarded as a special 
case of the superposition technique. In such a case, the rescaled Fourier functional model of a measured 
data set simply equals its Fourier characteristic function. 3.3 Results and Discussion The demonstration 
editor in Fig.8 is used for real-time previewing. The method runs in almost real-time (about 10 frames 
per second) on a R4000 workstation. The step­constraints parameters and the Fourier characteristics, 
such as of briskness, tiredness, or a run, are therefore interactively specified with the editor. Figs. 
9 and 10 show examples of the effects of these parameters and characteristics. We note that consistency 
of the parameters and characteristics observed in Fig. 9 is also derived from the rescaled Fourier functional 
models. The two still images in Fig. 10 are taken from an animation example, featuring the "TV robo". 
Combined with additional stage effects, such as a flash in Fig. 10(a), the proposed method succeeded 
in making the TV robo perform well on the bright and dark side of life. An additional feature of the 
rescaled Fourier functional model is demonstrated with Fig. 10(b), where a shivering walk is made. This 
is easily achieved by adding randomized higher frequency terms to the Fourier functional model. Adding 
noise as higher frequency terms to a rescaled Fourier functional model is useful for such dramatization. 
We also note that the generated animations show a wider variation than existing physics-based or procedural 
approaches. Currently the proposed method is not invertible. This means that the transition from running 
to walking by the method is unnatural, while the realistic transition from walking to running is made 
by the method. This problem must be addressed. The very limitation of the superposition technique in 
3.2 will be clarified under more explicit formulation.  4. Conclusion Based on the experimental data 
set of a few different human movements, the proposed method allows human movements to be interactively 
designed and generated in a wider variety. The key idea in the method is the use of Fourier series expansions 
of the measured data sets. Then smooth interpolation, extrapolation, and transition between different 
types of movements were made. In addition mood or characteristics of the human behaviors, such as "tiredness", 
and "briskness", were successfully extracted by the method. A promising direction of future research 
involves extensions of the method in order to describe non-periodic human behaviors and to model a crowd 
or throng of people. In addition, the integration of the proposed method and dynamics techniques will 
be indispensable for further applications.  Acknowledgements We would like to thank Yoshihiro Uehara 
of Nippon Television Network Corp. (NTV) for his generous support and encouragement, and Yoshinori Sugano 
of NTV and Shigeru Yamada of NTV Art Center Corp. for their production help. We are also grateful to 
Shinya Tanifuji and Motomi Odamura for their support of the publication of this research. Additional 
thanks go to Carol Kikuchi, Masa Tani and Koichi Tanikoshi for their comments and suggestions.  References 
[1] Badler, N., Phillips C., and Webber, B.L. Simulating Humans. Oxford University Press 1993. [2] Boulic, 
R., Magnenat-Thalmann, N., and Thalmann, D. A Global Human Walking Model with Real-time Kinematic Personification. 
The Visual Computer. 6, 344-358, 1990. [3] Bruderlin, A. and Calvert, T.W. Goal-directed, Dynamic Animation 
of Human Walking. Proceedings of SIGGRAPH 89 (Boston, Massachusetts, July 31- August 4, 1989). In Computer 
Graphics, 23, 3 (July 1989), 233­ 242. [4] Bruderlin, A., Teo, C.G. and Calvert, T.W. Procedural Movement 
for Articulated Figure Animation. Computers &#38; Graphics. 18, 453- 461, 1994. [5] Cohen, M.F. Interactive 
Spacetime Control for Animation. Proceedings of SIGGRAPH 92 (Chicago, Illinois, July 26- 31, 1992). In 
Computer Graphics, 26, 2 (July 1992), 293-302. [6] Girard, M. and Maciejewski, A.A. Computational Modeling 
for Computer Generation of Legged Figures. Proceedings of SIGGRAPH 85 (San Francisco, California, July 
23-27, 1985). In Computer Graphics, 19, 3 (July 1985), 263-270. [7] Koga, Y., Kondo, K., Kuffner, J. 
and Latombe, J-C. Planning Motions with Intentions. Proceedings of SIGGRAPH 94 (Orlando, Florida, July 
24- 29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994. ACM SIGGRAPH, pp. 395-408, 
1994. [8] van de Panne, M. and Fiume, E. Sensor-Actuator Networks. Proceedings of SIGGRAPH 93 (Anaheim, 
California, August 1- 6, 1993). In Computer Graphics Proceedings, Annual Conference Series, 1993. ACM 
SIGGRAPH, pp. 335-342, 1993. [9] Sturman, D. Interactive Keyframe Animation of 3-D Articulated Models. 
Graphics Interface '86, Tutorial on Computer Animation, 1986. [10] Unuma, M., and Takeuchi, R. Generation 
of Human Motion with Emotion. Proceedings of Computer Animation '91, pp. 77 - 88, 1991. [11] Wilhelms, 
J. Using Dynamic Analysis for Realistic Animation of Articulated Bodies. IEEE Computer Figure 8. Demonstration 
editor.  (a) run = 1.0; jump = 5.0 (b) run = 1.0, jump = 5.0, brisk = 1.37; (c) run = 1.0, jump = 5.0, 
brisk =1.37 step = 1.8, gait = 0.5 step = 1.8, gait = 0.06 Figure 9. Emotion-based running examples with 
step-constraints. (a) "Light" (b) "Shade"    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218421</article_id>
		<sort_key>97</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Motion signal processing]]></title>
		<page_from>97</page_from>
		<page_to>104</page_to>
		<doi_number>10.1145/218380.218421</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218421</url>
		<keywords>
			<kw><![CDATA[digital signal processing]]></kw>
			<kw><![CDATA[human animation]]></kw>
			<kw><![CDATA[motion control]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>C.3</cat_node>
				<descriptor>Signal processing systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31022945</person_id>
				<author_profile_id><![CDATA[81100008103]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Armin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruderlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, School of Computing Science, Simon Fraser University, Burnaby, B.C. V5A 1S6, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31100478</person_id>
				<author_profile_id><![CDATA[81100005753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apple Computer, Inc., 1 Infinite Loop, MS 301-3J, Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BERGERON, P., AND LACHAPELLE, P. Controlling facial expressions and body movements in the computer-generated animated short: Tony de Peltrie. In Computer Graphics (SIC- GRAPH '85), Course Notes: Techniques for Animating Characters (July 1985).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BRUDERLIN, A., AND CALVERT, T. Interactive animation of personalized human locomotion. In Graphics Intelface '93, Proceedings (May 1993), pp. 17-23.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BURT, E Multiresolution method for image merging. In Computer Graphics (SIGGRAPH '86), Course Notes: Advanced Image Processing (August 1986).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BURY, E, AND ADELSON, E. A multiresolution spline with application to image merging. ACM Transactions on Graphics 2, 4 (October 1983), 217-236.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617816</ref_obj_id>
				<ref_obj_pid>616027</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CALVERT, T., BRUDERLIN, A., DILL, J., SCHIPHORST, T., AND WELMAN, C. Desktop animation of multiple human figures. IEEE Computer Graphics &amp; Applications 13, 3 (1993), 18-26.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192272</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CASSELL, J. ET AL. Animated conversation: Rule-based generation of facial expression, gesture &amp; spoken intonation for multiple conversational agents. In Computer Graphics (SIC- GRAPH '94 Proceedings) (July 1994), pp. 413-420.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>163196</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CHUI, C. K. An Introduction to Wavelets, Series: Wavelet Analysis and its Applications. Academic Press, Inc., 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134083</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[COHEN, M. Interactive spacetime control for animation. In Computer Graphics (SIGGRAPH '92 Proceedings) (July 1992), vol. 26, pp. 293-302.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DEMORI, R., AND PROBST, D. Handbook of Pattern Recognition and Image Processing. Academic Press, 1986, ch. Computer Recognition of Speech.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359846</ref_obj_id>
				<ref_obj_pid>359842</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FUCHS, H., KEDEM, Z., AND USELTON, S. Optimal surface reconstruction from planar contours. Communications of the ACM 10, 10 (1977), 693-702.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Guo, S., ROBERGE, J., AND GRACE, T. Controlling movement using parametric frame space interpolation. In Computer Animation '93, Proceedings (1993), pp. 216-227.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37428</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[ISAACS, P., AND COHEN, M. Controlling dynamic simulation with kinematic constraints, behavior functions and inverse dynamics. In Computer Graphics (SIGGRAPH '87 Proceedings) (1987), vol. 21, pp. 215-224.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134087</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[KASS, M. Condor: Constraint-based dataflow. In Computer Graphics (SIGGRAPH '92 Proceedings) (1992), vol. 26, pp. 321-330.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808575</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[KOCHANEK, D., AND BARTELS, R. Interpolating splines with local tension, continuity and bias control. In Computer Graphics (SIGGRAPH '84 Proceedings) (1984), vol. 18, pp. 33-41.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192266</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[KOGA, Y., KONDO, K., KUFFNER, J., AND LATOMBE, J.-C. Planning motions with intentions. In Computer Graphics (SIGGRAPH '94 Proceedings) (July 1994), pp. 395-408.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LEBRUN, M. Digital waveshaping synthesis. Journal of the Audio Engineering Society 27, 4 (1979), 250-266.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122731</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LITWINOWICZ, E Inkwell: A 2 1/2-D animation system. In Computer Graphics (SIGGRAPH '91 Proceedings) (1991), vol. 25, pp. 113-122.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LIU, Z., GORTLER, S., AND COHEN, M. Hierarchical spacetime control. In Computer Graphics (SIGGRAPH '94 Proceedings) (July 1994), pp. 35-42.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>196568</ref_obj_id>
				<ref_obj_pid>196564</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MEYER, K., APPLEWHITE, H., AND BIOCCA, F. A survey of posistion trackers. Presence: Teleoperators and Virtual Environments 1, 2 (Spring 1992), 173-200.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[ODGEN, J., ADELSON, E., BERGEB, J., AND BURT, E Pyramidbased computer graphics. RCA Engineer 30, 5 (1985), 4-15.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. ET AL. State of the art in facial animation. In Computer Graphics (SIGGRAPH '90), Course Notes (August 1990).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122756</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[PHILLIPS, C., AND BADLER, N. Interactive behaviors for bipedal articulated figures. In Computer Graphics (SIC- GRAPH '91 Proceedings) (1991), vol. 25, pp. 359-362.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[RASURE, J., AND KUBICA, S. The Khoros application development environment. Tech. rep., Khoral Research, Inc., 4212 Courtney NE, Albuquerque, NM, 87108, USA, 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134001</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SEDERBERG, T., AND GREENWOOD, E. A physically-based approach to 2-D shape blending. In Computer Graphics (SIC- GRAPH '92 Proceedings) (1992), vol. 26, pp. 26-34.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SERRA, B., AND BERTHOLD, M. Subpixel contour matching using continuous dynamic programming. IEEE Computer Vision and Pattern Recognition (1994), 202-207.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[STEIN, C., AND HITCHNER, H. The multiresolution dissolve. SMPTE Journal (December 1988), 977-984.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[STURMAN, D. A discussion on the development ofmotion control systems. In Graphics Interface '86, Tutorial on Computer Animation (1986).]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[UNUMA, M., AND TAKEUCHI, R. Generation of human motion with emotion. In Computer Animation '93, Proceedings (1993), pp. 77-88.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>921522</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[VELHO, L. Piecewise Descriptions of Implicit Sulfaces and Solids. PhD thesis, University of Toronto, Computer Science, 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>16589</ref_obj_id>
				<ref_obj_pid>16564</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WILHELMS, J. Virya-a motion control editor for kinematic and dynamic aniamtion. In Graphics Intelface '86, Proceedings (1986), pp. 141-146.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. Spacetime constraints. In Computer Graphics (SIGGRAPH '88 Proceedings) (1988), vol. 22, pp. 159-168.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., TERZOPOULUS, D., AND KASS, M. Signal matching through scale space. International Journal of Computer Vision (1987), 133-144.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>20332</ref_obj_id>
				<ref_obj_pid>20313</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[ZELTZER, D. Towards an integrated view of 3-D computer character animation. In Graphics Interface '85, Proceedings (1985), pp. 105-115.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Signal Processing Armin Bruderlin1 Simon Fraser University Abstract Techniques from the image 
and signal processing domain can be successfully applied to designing, modifying, and adapting ani­mated 
motion. For this purpose, we introduce multiresolution mo­tion .ltering, multitarget motion interpolation 
with dynamic time­warping, waveshaping and motion displacement mapping. The techniques are well-suited 
for reuse and adaptation of existing mo­tion data such as joint angles, joint coordinates or higher level 
motion parameters of articulated .gures with many degrees of free­dom. Existing motions can be modi.ed 
and combined interactively and at a higher level of abstraction than conventional systems sup­port. This 
general approach is thus complementary to keyframing, motion capture, and procedural animation. Keywords: 
human animation, motion control, digital signal processing. Introduction Motion control of articulated 
.gures such as humans has been a challenging task in computer animation. Using traditional key­framing 
[27], it is relatively straightforward to de.ne and modify the motion of rigid objects through translational 
and rotational tra­jectory curves. However, manipulating and coordinating the limbs of an articulated 
.gure via keyframes or the spline curves they de­.ne is a complex task that draws on highly developed 
human skills. More general, global control of the character of an animated motion would be useful in 
.ne-tuning keyframed sequences. Such global controlwouldmakeprede.nedsequencesmore useful,andlibraries 
of animated motion more valuable. Much of the recent research in motion control of articulated .gures 
has been directed towards reducing the amount of motion speci.cation to simplify the task of the animator. 
The idea is to build some knowledge about motion and the articulated structure into the system so that 
it can execute certain aspects of movement autonomously. This has lead to the development of higher level 
control schemes [5, 6, 15, 22, 33] where the knowledge is fre­quently speci.ed in terms of rules, and 
physically-based modeling techniques [8, 12, 18, 30, 31] in which knowledge is embedded in the equations 
of motion, constraints and possibly an optimization expression. Both approaches often suffer from lack 
of interactiv­ 1School of Computing Science, Simon Fraser University, Burnaby, B.C. V5A 1S6, Canada, 
(armin@cs.sfu.ca). 2Apple Computer, Inc., 1 In.nite Loop, MS 301-3J, Cupertino, CA 95014, USA, (amberinca@aol.com). 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
Lance Williams2 Apple Computer, Inc. ity: they don t always produce the motion which the animator had 
in mind, and complex models have a slow interactive cycle. To increase the expressive power of such models, 
more control param­eters can be introduced. Once again, higher-level editing tools for the trajectories 
of such control parameters would ease animators burdens and generalize their results. An alternative 
method to obtain movements of articulated .g­ures is performance animation where the motion is captured 
from live subjects. Although a variety of technologies have been de­veloped to fairly reliably measure 
performance data [19], the com­puter graphics literature makes scant mention of editing techniques for 
recorded motion. In the absence of effective editing tools, a recorded movement that is not quite right 
requires the whole data capture process to be repeated. Because of the complexity of articulated movements 
and the limitations of current motion control systems as outlined above, we believe that it is desirable 
to develop tools that make it easy to reuse and adapt existing motion data. For this purpose, we adopt 
techniques from the image and signal processing domain which provide new and useful ways to edit, modify, 
blend and align motion parameters of articulated .gures. These techniques represent a pragmatic approach 
to signal processing by providing analytic solutions at interactive speeds, and lend themselves to higher 
level control by acting on several or all degrees of freedom of an articulated .gure at the same time. 
In this paper, we treat a motion parameter as a sampled signal. A signal contains the values at each 
frame1 for a particular degree of freedom. These values could come from evaluating a spline curve in 
a keyframing system, or be derived from the tracked markers in a motion capture system. In animating 
articulated .gures we are often concerned with signals de.ning joint angles or positions of joints, but 
the signal-processing techniques we have implemented also apply to higher level parameters like the trajectory 
of an end­effector or the varying speed of a walking sequence. In Section 2, we present the method of 
multiresolution .ltering and its application to parameters of motion. Section 3 discusses multitarget 
interpolation, while pinpointing a severe problem of this technique when used for motion blending the 
absence of an automatic alignment or registration of movements. A solution to this problem is given based 
on the principle of dynamic time­warping. Section 4 introduces waveshaping as a rapid nonlinear signal 
modi.cation method useful for tasks such as mapping joint limits of articulated .gures. Section 5 concludes 
the editing tech­niques we have developed with motion displacement mapping, an extremely general tool 
which permits editing of densely-sampled motion data with the ease of keyframing. Each of these sections 
provides illustrative examples. Finally, conclusions are given in section 6. 1Sampled signals have values 
de.ned at regular intervals which,in a good animation system, should be completely decoupled from the 
nominal frame rate of the .nal product. We will speak of frames at the sample rate without intending 
any loss of generality. 2 Multiresolution Filtering In the motion capture realm, most systems have provision 
for non­linear impulse-noise removal .lters (Tukey .lters) as well as linear smoothing .lters for noise 
reduction in digitized data. There has been less published discussion of the use of signal processing 
oper­ations to edit or modify captured motion for creative purposes. The lag, drag, and wiggle recursive 
.lters in Inkwell [17] represent more relevant previous work in the application of signal process­ing 
to keyframed 2D animated motion. These .lters were used to stylize motion by invoking linear systems 
behavior without a more structured physical model, and permitted lively animated effects without unduly 
taxing the animator. In another related approach, Unuma et al. [28] apply Fourier transformations to 
data on human walking for animation purposes. Based on frequency analysis of the joint angles, a basic 
walking factor and a qualitative factor like brisk or fast are extracted. These factors are then used 
to generate new movements by interpolation and extrapolation in the frequency domain, such that now a 
walk can be changed continu­ously from normal to brisk walking. Multiresolution .ltering describes a 
range of digital .lter­bank techniques which typically pass a signal through a cascade of lowpass .lters 
to produce a set of short-time bandpass or low­pass signal components. By applying .ltering recursively 
to the output of successive .lter bank stages, and downsampling lowpass components as appropriate, these 
.lter banks can be quite ef.cient; they can produce short-time spectra at roughly the same nlog(n) expense 
as the Fast Fourier Transform. The method of multiresolution .ltering has been extensively exercised 
by Burt et al. [4, 20] as an image representation method advantageous for certain kinds of operations, 
such as seamless merging of image mosaics and intra-image interpolation (noise re­moval). It has also 
been applied to temporal dissolves between images [26]. Images may be stored as lowpass (Gaussian) or 
band­pass (Laplacian) pyramids of spatial .lterbands, where each level represents a different octave 
band of spatial frequencies. Opera­tions like merging two images are then performed band-by-band before 
reconstructing the image by adding up the resulting bands. In this way, the .ne detail of an image corresponding 
to the higher frequencies can be treated separately from the coarse image features encoded by the low 
frequencies. In the currently popular wavelet parlance [7], Burt s Gaussian pyramid is a multiresolution 
analysis in terms of a cubic B-spline scaling function. The corresponding Laplacian pyramid is simply 
a bandpass counterpart, where each successively higher level of detail has an interpolated copy of the 
level beneath subtracted from it. The Laplacian pyramid can be computed directly in this way, or via 
a modi.ed wavelet transform. Burt s method is more ef.cient for signals of more than one dimension [29]. 
As a general obser­vation, for synthesis and modi.cation (as well as many analysis Figure 1, where G0 
is the original image). This process is repeated until the image size is reduced to one pixel, which 
is the average intensity, or DC value. The bandpass pyramid is then calculated by repeatedly differencing 
2 successive lowpass images, with the subtrahend image being expanded .rst in each case (right of Fig­ure 
1, where L0 is the highest frequency band). The image can be reconstructed without manipulation by adding 
up all the bandpass bands plus the DC. The same procedure can be performed on two or more images at the 
same time, whereby operations like merging are executed band by band before reconstructing the .nal result. 
 Gn Ln-1 G2 L2 L1G1  G0 L0 . . . G0 = L0 + L1 + L2 + + Ln-1 + Gn Figure 1: Left: lowpass pyramid; right: 
bandpass pyramid. 2.1 Motion Multiresolution Filtering The principles of image multiresolution .ltering 
are now applied to motion parameters of an articulated .gure, motivated by the following intuition: low 
frequencies contain general, gross motion patterns, whereas high frequencies contain detail, subtleties, 
and (in the case of digitized motion) most of the noise. Each motion parameter is treated as a one-dimensional 
signal from which the lowpass (G) and bandpass (L) levels are calculated. An example is illustrated in 
Figure 2 based on the signal of the sagittal knee angle of two walking cycles generated with GAITOR [2]. 
70 15 60 10 0 5 tasks for computer vision), oversampled .lter banks like Burt s are more useful than 
strict subband decompositions (where the num­ ber of coef.cients does not exceed the number of samples 
in the original signal). A direct contrast is in the way small translations of an image are projected: 
sparse decompositions change radically with small offsets of the input image, whereas the Burt pyramids 
change smoothly. The reduction in coef.cients attendant on a sub­ knee angle (degrees) 50 40 0 30 -5 
20 -10 10 band .lterbank may speed numerical solution of some problems; a recent effort in the animation 
domain is the wavelet formulation of spacetime interpolation for physically-based keyframing by Liu et 
al. [18]. They did not use the the frequency decomposition to pro­vide direct manipulation of motion, 
and we believe Burt s method is more appropriate for this purpose. The .rst step in applying Burt s multiresolution 
analysis is to obtain the lowpass pyramid by successively convolving the image with a B-spline .lter 
kernel (e.g. 5 x5), while the image is sub­sampled by a factor of 2 at each iteration (as shown at the 
left of 30405060708090 30405060708090 frames Figure 2: Left: lowpass G0 (solid) and G3 (dashed; B-spline 
kernel of width 5); right: bandpass L0 (solid) and L2 (dashed) of the sagittal knee angle for two walking 
cycles. 2.1.1 Filtering Algorithm The length m(number of frames) of each signal determines how many frequency 
bands (fb) are being computed: let 2n m 2n+1,then fb=n Instead of constructing a pyramid of lowpass and 
bandpass sequences where each successive sequence is reduced by a factor of two, alternatively the sequences 
are kept the same length and the .lter kernel (w)isexpandedateachlevelbyinserting zerosbetween the values 
of the .lter kernel (a, b, cbelow) [3]. For example, with a kernel of width 5, w1 =[cbabc], w2 =[c0 b0 
a0 b0 c], w3 =[c000 b000 a000 b000 c],etc., where a=3/8, b=1/4and c=1/16. Since we are dealing with signals 
rather than images, the storage penalty compared to a true pyramid is not as signi.cant ( fbxiversus 
4/3 xi,where i=number of data points in original signal), while reconstruction is faster since the signal 
does not have to be expanded at each level. We now state the motion multiresolution algorithm in detail. 
Steps 1 to 5 are performed simultaneously for each motion parameter signal: 1. calculatelowpasssequenceofall 
fbsignals (0 kfb)by successively convolving the signal with the expanded kernels, where G0 is the original 
motion signal and Gfbis the DC: Gk+1 =wk+1 xGk; This can be calculated ef.ciently by keeping the kernel 
con­stant and skipping signal data points (iranges over all data points of a signal)2: 2 X k Gk+1(i)= 
w1(m)Gk(i 2m); m .2 2. obtain the bandpass .lter bands (0 kfb): Lk =Gk Gk+1; 3. adjust gains for each 
band and multiply Lk s by their current gain values (see example below). 4. blend bands of different 
motions (optional, see multitarget interpolation below). 5. reconstruct motion signal:  fb. 1 X G0 
=Gfb Lk k 0 2We implemented several treatments of the boundary of the signal, that is when i+2k mlies 
outside the domain of the signal. The two most promising approaches have proved to be re.ecting the signal, 
and keeping the signal values constant (i.e. equal to the .rst/last data point) outside its boundaries. 
 Figure 3: Adjusting gains of bands for joint angles; top: increasing middle frequencies; middle: increasing 
low frequencies; bottom: using negative gain value. 2.1.2 Examples An application of motion multiresolution 
.ltering is illustrated in Figure 3. Displayed like an equalizer in an audio ampli.er, this is a kind 
of graphic equalizer for motion, where the amplitude (gain) of each frequency band can be individually 
adjusted via a slider before summing all the bands together again to obtain the .nal motion. A step function 
shows the range and effect of changing frequency gains. We applied this approach successfully to the 
joint angles (70 degrees of freedom) of a human .gure. The same frequency band gains were used for all 
degrees of freedom. In the example illustrated at the top of Figure 3, increasing the middle frequen­cies 
(bands 2, 3, 4) of a walking sequence resulted in a smoothed but exaggerated walk. By contrast, increasing 
the high frequency band (band 0) added a nervous twitch to the movement (not shown in Figure 3), whereas 
increasing the low frequencies (bands 5, 6) generated an attenuated, constrained walk with reduced joint 
move­ment (Figure 3 middle). Note that the gains do not have to lie in the interval [0,1]. This is shown 
at the bottom of Figure 3, where band 5 is negativefor a motion-captured sequenceofa .gure knockingat 
the door, resulting in exaggerated anticipation and follow-through for the knock. We also applied the 
same .ltering to the joint posi­tions (147 degrees of freedom) of a human .gure. Increasing the gains 
for the middle frequency bands of a walking sequence pro­duced a slight scaling effect of the end effectors, 
and resulted in a squash-and-stretch cartoon walk (Figure 4). From the examples, it becomes apparent 
that some constraints such as joint limits or non-intersection with the .oor can be violated in the .ltering 
process. Our motion-editing philosophy is to employ constraints or optimization after the general character 
of the motion has been de.ned (see displacement mapping in section 5 below; or a Figure 4: Adjusting 
gains of bands for joint positions. more general optimization method [13]). Whereas being trapped in 
local minima is the bane of global optimization for most problems, animatedmotion is agoodexampleofan 
underconstrainedproblem where the closest solution to the animator s original speci.cation is likely 
the best. Of course, many animators disdain consistent physics, which is another good reason to decouple 
motion editing from constraint satisfaction. Finally, we suggest that a multiresolution approach could 
also be quite useful in de.ning motion sequences, rather than simply modifying them. Much like an artist 
creating a picture blocks out the background .rst with a big brush, then adds more and more detail with 
.ner and .ner brushes, a generic motion pattern could be de.ned .rst by low frequencies, and then .netuned 
by adding in higher frequency re.nements3. Multitarget Interpolation Multitarget interpolation refers 
to a process widely used in com­puter animation to blend between different models. The technique was 
originally applied in facial animation [1, 21]. We might have a detailed model of a happy face, which 
corresponds parametrically to similar models of a sad face, quizzical face, angry face, etc. The control 
parameters to the model might be high level (like raise left eyebrow by 0.7 ), very high level (like 
be happy ), or they might simply be the coordinates of the points on a surface mesh de.ning the shape 
of part of the face. By blending the corresponding pa­rameters of the different models to varying degrees, 
we can control the expression of the face. Figure 5: Example of multitarget motion interpolation. 3.1 
Multitarget Motion Interpolation We can apply the same technique to motion. Now we might have a happy 
walk, a sad walk, angry walk, etc., that can be blended freely to provide a new result. Figure 5 shows 
an example of blending two 3Personal communication,Ken Perlin, New York University, 1994. different motions 
of a human .gure, a drumming sequence and a swaying arm sideways sequence. In this case, the blend is 
linear, i.e. add 0.4 of the drum and 0.6 of the arm-sway. In general, the blend can be animated by following 
any trajectory in time. Guo et al. [11] give a good discussion of this approach which they term parametric 
frame space interpolation. Our approach generalizes on theirs in that the motion parameters such as joint 
angles to be blended are completely decoupled from one another, and have no implicit range limits. Each 
component of an arbitrary ensemble of input parameters can have an independent blending coef.cient assigned 
to it. As indicated in step (4) of the multiresolution algorithm above, we can mix multitarget interpolation 
and multiresolution .ltering to blend the frequency bands of two or more movements separately. This is 
illustrated in Figure 6 for the same two motions (a drum and an arm-sway) as in Figure 5. Adjusting the 
gains of each band for each motion and then blending the bands provides .ner con­trol while generating 
visually much more pleasing and convincing motion. Figure 6: Multitarget interpolation between frequency 
bands. However, there is a potential problem when applying multitar­get interpolation to motion which 
relates to the notion of parametric correspondence as stated above: for all our face models to cor­respond 
parametrically implies that the parameters of each of the models has a similar effect, so that if a parameter 
raises the left eye­brow of face number one, a corresponding parameter raises the left eyebrow in face 
number two. If our parameters are simply surface coordinates, it means that the points on each surface 
correspond, so if the point at U,Vcoordinates U1, V1 is at the tip of the left eyebrow, the point at 
the same coordinates in any other face will also be at the tip of the left eyebrow. In motion, parametric 
correspondence means much the same thing, except that now a correspondence with respect to time is required. 
If we are blending walk cycles, the steps must coincide so that the feet strike the ground at the same 
time for corresponding parameter values. If the sad walk is at a slower pace than the happy walk, and 
we simply blend them together without .rst establishing a correspondencebetweenthe steps,theblend will 
bea curiousdance of uncoordinated motions, and the feet will no longer strike the ground at regular intervals; 
indeed, they are no longer guaranteed to strike the ground at all (see Figure 7). Thus, multitarget motion 
interpolation must include both a distortion (remapping a function in time) and a blend (interpolating 
among different mapped values). In the visual domain a transformation like this is termed a morph. Another 
example is illustrated in Figure 8; here the motion sequences of two human .gures waving at different 
rates and in­tensities (a neutral and a pronounced wave) were .rst blended without timewarping. This 
resulted in a new wave with undesirable Figure 7: Blending two walks without (top) and with (bottom) 
correspondence in time. secondary waving movements superimposed. After timewarping the neutral to the 
pronounced wave, the blend produced the neutral wave at the pronounced rate. In the following section 
we describe an automatic method for establishing correspondence between signals to make multitarget motion 
interpolation meaningful and useful. Figure 8: Blending two waves without (top) and with (bottom) correspondence 
in time. 3.2 Dynamic Timewarping The .eld of speech recognition has long relied on a nonlinear signal 
matching procedure called dynamic timewarping to compare tem­plates (for phonemes, syllables or words) 
with input utterances [9]. Apart from being subject to the usual random error, each acoustic input signal 
also shows variations in speed from one portion to an­other with respect to the template signal. The 
timewarp procedure identi.es a combination of expansion and compression which can best warp the two signals 
together. In our case, timewarping is applied in the discrete time domain to register the corresponding 
motion parameter signals such as joint angles. In Figures 7 and 8, the timewarping was done simultane­ously 
for all 70 rotational degrees of freedom of the human .gure for the duration of the movement sequences. 
If we have a military march and a drunken stagger, two new gaits can immediately be de.ned from the timewarp 
alone: the military march at the drunken pace, and the drunken stagger at the military pace. Figure 9 
shows an example for one degree of freedom (knee angle) for the two walks warped in Figure 7. However, 
we are not limited to these two extreme warps, but may freely interpolate between the map­pings of the 
two walks, and between the amplitudes of the signals through these mappings independently. 70 60 50 40 
30 20 10 0 70 60 50 40 30 20 10 0  knee angle (degrees) 0 50 100 150 200 70 60 50 40 30 20 10 0 frames 
Figure 9: Top: sagittal knee angles curves of two walks; middle: red = blue curve warped to match green; 
bottom: red = green curve warped to match blue. 3.2.1 Timewarp Algorithm The problem can be decomposed 
and solved in two steps: .nding the optimal sample correspondences between the two signals, and applying 
the warp. The vertex correspondence problem is de.ned as.ndingthe globallyoptimalcorrespondencebetweenthe 
vertices (samples) of the two signals: to each vertex of one signal, assign (at least) a vertex in the 
other signal such that a global cost func­tion measuring the difference of the two signals is minimized. 
In this sense, the problem is related to contour triangulation [10] and shape blending [24], and is solved 
by dynamic programming optimization techniques. The solution space can be represented as a two-dimensional 
grid, where each node corresponds to one possible vertex assignment (see Figure 10). The optimal vertex 
correspon­dence solution is illustrated in the grid by a path from (0,0) to (9,9). n In general, there 
are O(n/n!) such possible paths4. When applying timewarping to recognition, the best .t to a canon of 
signals is computed; no subsequent use is made of a warped signal. The algorithms which perform the warp 
typically dosobyformingadiscrete,point-sampledcorrespondence[9]. For synthetic purposes, more continuous 
transformations and cost func­tions are appropriate [32, 25]. We adopted Sederberg s shape blend­ing 
algorithm [24] which guarantees a globally optimal solution by visiting every node in the grid once (O(n 
2) with constant amount of work per node). Upon reaching node (n,n), the optimal solu­tion is recovered 
by backtracking through the graph. Sederberg s physically-based approach measures the difference in shape 
of the two signals by calculating how much work it takes to deform one signal into the other. The cost 
function consists of the sum of local stretching and bending work terms, the former involving two, the 
latter three adjacent vertices of each signal. Intuitively, the larger the difference in distance between 
two adjacent vertices of 4This holds for the vertex correspondence problem,where we favor a diagonal 
move in the graph over a south-followed-by-east-move or an east-followed-by-a-south-move. For contour 
triangulation [10], where diagonal moves are denied, the complexity is O((2n)!/(n!n!)). cost function 
terms: - stretching work between 2 adjacent vertices in signal (difference in segment lengths). - bending 
work between 3 adjacent vertices in signal (difference in angles). Bj 0 1 2 3 4 5 6 7 8 9 0 1 signal 
B 2 3 4 Ai 5 signal A 6 7 0 1 2 3 4 5 6 7 8 9 8 9 Figure 10: Vertex correspondence problem and cost 
functions. one signal and the two vertices of the other (given by two adjacent nodes in the graph), the 
bigger the cost. Similarly, the larger the difference in angles between three adjacent vertices of one 
signal and the three vertices of the other (given by three adjacent nodes in the graph), the bigger the 
cost (for details, see [24]; an illustration is given in Figure 10). One additional check was introduced 
to make sure that we are really comparing corresponding angles in the two signals: if the middle of the 
three vertices used to calculate the angle is a local minimum in one signal and a local maximum in the 
other signal, then one of the angles (.) is inverted before calculating the cost term (.=360 deg .). 
The second part of the problem is to apply the warp given the optimal vertex correspondences. As in speech 
recognition [9], three cases are distinguished: substitution, deletion and insertion. This is indicated 
in the optimal path by a diagonal, horizontal and vertical line, respectively, between two nodes. For 
the following explanations, we assume that signal Bis warped into Aas shown in Figure 11, and the warped 
signal is denoted by Bw.Then if Bjand Aiare related by a substitution it follows that Bwi =Bj. In case 
of a deletion, where multiple samples of B,(Bj,Bj+1,,Bj+k), correspond to one Ai, Bwi =mean(Bj,Bj+1,,Bj+k). 
Finally, an insertion implies that one sample of B, Bj, maps to multiple samples of A,(Ai,Ai+1,,Ai+k). 
In this case, the values for Bwi,Bw,,Bware determined by calculating a cubic B­ i.1 i.k spline distribution 
around the original value Bj. substitution: 1:1 correspondence of successive samples. deletion: multiple 
samples of B map to a sample of A. insertion: a sample of B maps to multiple samples of A. Bj 0 1 2 
3 4 5 6 7 8 9 B 0 1 2 3 A 4 Ai 5 6 7 warped B 8 9 0 1 2 3 4 5 6 7 8 9 Figure 11: Application of timewarp 
(warp Binto A). 4 Waveshaping The transformations discussed so far are operations on the time history 
of a signal. Operations which are evaluated at each point in the signal without reference to its past 
or future trajectory are occasionally termed point processes. Such operations include scal­ing or offsetting 
the signal, but are more generally described as a functional composition. Familiar uses of functional 
composition in graphics include gamma correction and color-lookup, as well as tabular warping functions 
for images. Digital waveshaping is the term applied to functionalcompo­sition in computer sound synthesis. 
In this domain, a normalized input signal x(e.g. scaled to the range from 1to 1) is directed through 
a discrete shaping function f(or waveshaping table) to synthesize steady-state or time-varying harmonic 
sound spectra. Although waveshaping is in general a nonlinear operation, its ef­fects when applied to 
an input sine wave can be easily character­ized [16]. In practical terms, if fis de.ned as the identity 
function f(x)=x, the signal will pass through unchanged. If fis slightly changed, say, to having a subtle 
bump near 0, then the signal xwill be altered in that it will have slightly positive values where, and 
around where, it was zero before, thus xhas now some bumps as well. If fis de.ned as a partial cycle 
of a cosine function going from minimum to maximum over the [1,1]range, the values of xwill be exaggerated 
in the middle and attenuated at the extremes. If fis a step function, xwill be quantized to two values. 
4.1 Motion Waveshaping An example of how this idea can be adopted for animation is illus­trated in Figure 
12. Here the default identity shaping function has been modi.ed to limit the joint angles for a motion 
sequence of an articulated .gure waving. In the .gure, hard limits are imposed: values of xgreater than 
a limit value simply map to that value. An alternative is a soft limit: as values exceed the limit, they 
are mapped to values that gradually approach it. The implementation of our shaping function is based 
on interpolating cubic splines [14]; a user can add, delete and drag control points to de.ne the function 
and then apply it to all or some degrees of freedom of an articulated .gure. Anotherapplicationofwaveshapingis 
to mapthe shapeofinput motions to a characteristic function. The shaping function in Fig­ure 13 applied 
to the motion-captured data of a human .gure sitting and drinking introduced extra undulations to the 
original monotonic reaching motion. In this way, it is possible to build up a library of shaping functions 
which will permit rapid experimentation with different styles of movement.     Motion Displacement 
Mapping Displacement mapping provides a means to change the shape of a signal locally through a displacement 
map while maintaining con­tinuity and preserving the global shape of the signal. To alter a movement, 
the animator just changes the pose of an articulated .g­ure at a few keyframes. A spline curve is then 
.tted through these displacements for each degree of freedom involved, and added to the original movement 
to obtain new, smoothly modi.ed motion. The basic approach is illustrated in Figure 14. Step 1 is to 
de.ne the desired displacements (indicated by the three vertical arrows) with respect to the motion signal; 
in step 2, the system then .ts an inter­polating cubic spline [14] through the values of the displacements 
(note that the .rst and last data points are always displacement points). The user can then adjust the 
spline parameters in step 3 before the system calculates the displaced motion satisfying the displacement 
points (step 4). The displacement process can be applied iteratively until a de­sired result is achieved. 
Since the operation is cheap, a fast feedback loop is guaranteed. In the top part of Figure 15, we took 
the output of a multiresolution .ltering operation on joint angles of a human walking .gure, where some 
of the joint limits were violated and the feet did not make consistent contact with the ground, and read 
it into LifeForms [5], a system to animate articulated .gures. There we adjusted some of the joints and 
translated the .gure at a few keyframes for which displacement curves were quickly generated and applied 
to the motion of the .gure as described above. To re.ne the resulting motion, a second loop was executed; 
a frame of the .nal result is shown on the top right of Figure 15. The same technique was used in modifying 
the rotoscoped motion of a hu­man .gure sitting and drinking (Figure 15, middle). Here, three out of 
the 600 motion-captured frames were modi.ed to include some additional gestures of the arms and legs. 
In Figure 15, bot­tom, the joint angles for the arm and neck of a motion-captured knocking-at-a-door-sequence 
were changed for one frame via mo­tion displacement mapping to obtain a knock at a higher impact point. 
 Conclusions In this paper we have assembled a simple library of signal process­ing techniques applicable 
to animated motion. A prototype system has been implemented in the programming language C using the 
Figure 15: Examples of applying displacement curves. Khoros application development environment [23]. 
The immedi­ate goals of our motion-editing experiments have been ful.lled: the motion signal processing 
techniques provide a rapid interactive loop, and facilitate reuse and adaptation of motion data. By au­tomating 
some aspects of motion editing such as time-registration of signals or increasing the middle frequencies 
for several degrees of freedom at the same time, these techniques lend themselves to higher level motion 
control and can serve as building blocks for high-level motion processing. Of all the techniques introduced 
here, perhaps motion displace­ment mapping will prove to be the most useful; it provides a means by which 
a basic movement such as grasping an object from one place on a table can be easily modi.ed to grasping 
an object any­where else on the table. This allows simple and straightforward modi.cation of motion-capture 
data through a standard keyframing interface. Timewarping as a non-linear method to speed up or slow 
down motion is useful in blending different movements. It could also play an important role in synchronizing 
various movements in an animation as well as in synchronizing animation with sound. Multiresolution .ltering 
has been demonstrated as an easy tool to change the quality of a motion. Waveshaping represents a simple 
but ef.cient way to introduce subtle effects to all or some degrees of freedom. As the use of motion 
capture is becoming increasingly popular and libraries of motions are increasingly available, provid­ing 
alternate methods for modifying and tweaking movement for reuse can be of great value to animators. We 
believe that a wide range of animation tasks can be addressed with these techniques at a high level which 
is complimentary to and extends conventional spline tweaking tools: .blending of motions is straightforward 
by using multitarget interpolation with automatic time registration of movements. This is a convenient 
way to build up more complex motions from elementary ones. For more .ne-control, the frequency bands 
can .rst be computed for each motion before blending band-by-band while adjusting the frequency gains. 
.concatenating motions is another practical application of mul­titarget interpolation, giving the user 
control over blending interval (transition zone) and blending coef.cient. Multires­olution can be applied 
to concatenate band-by-band. .cappingof joint angles is a task easily accomplishedby wave­shaping. This 
tool is also well suited to apply user-de.ned undulations to all or some degrees of freedom to make a 
bland motion more expressive. .some animation tasks which can be achieved with multires­olution analysis 
include toning down a motion by increas­ing the low frequency gains, exaggerating a movement by increasing 
the middle frequencies, producing a nervous twitch by increasing the higher frequencies, and generating 
anticipation and follow-through by assigning negative gain values. Because of immediate feedback, the 
user can quickly experiment with different combinations of gain values for speci.c movement qualities. 
.editing of motion-captured data is very desirable yet very tedious in current systems. As mentioned 
above, displace­ment mapping provides an interface through which the ani­mator can conveniently change 
such data at a few selected keyframes while preserving the distinctive signature of the captured motion. 
 References [1] BERGERON,P., AND LACHAPELLE, P. Controlling facial ex­pressions and body movements in 
the computer-generated an­imated short: Tony de Peltrie. In Computer Graphics (SIG- GRAPH 85), Course 
Notes: Techniques for Animating Char­acters (July 1985). [2] BRUDERLIN,A., AND CALVERT, T. Interactive 
animation of personalized human locomotion. In Graphics Interface 93, Proceedings (May 1993), pp. 17 
23. [3] BURT, P. Multiresolution method for image merging. In Com­puter Graphics (SIGGRAPH 86), Course 
Notes: Advanced Image Processing (August 1986). [4] BURT,P., AND ADELSON, E. A multiresolution spline 
with application to image merging. ACM Transactionson Graphics 2, 4 (October 1983), 217 236. [5] CALVERT,T., 
BRUDERLIN,A., DILL,J., SCHIPHORST,T., AND WELMAN, C. Desktop animation of multiple human .gures. IEEE 
Computer Graphics &#38; Applications 13,3 (1993), 18 26. [6] CASSELL,J. ET AL. Animated conversation: 
Rule-based gen­eration of facial expression, gesture &#38; spoken intonation for multiple conversational 
agents. In Computer Graphics (SIG- GRAPH 94 Proceedings) (July 1994), pp. 413 420. [7] CHUI,C.K. An 
Introduction to Wavelets, Series: Wavelet Analysis and its Applications. Academic Press, Inc., 1992. 
 [8] COHEN, M. Interactive spacetime control for animation. In Computer Graphics (SIGGRAPH 92 Proceedings) 
(July 1992), vol. 26, pp. 293 302. [9] DEMORI,R., AND PROBST,D. Handbook of Pattern Recogni­tion and 
Image Processing. Academic Press, 1986, ch. Com­puter Recognition of Speech. [10] FUCHS,H., KEDEM,Z., 
AND USELTON, S. Optimal surface reconstruction from planar contours. Communications of the ACM 10, 10 
(1977), 693 702. [11] GUO,S., ROBERGE,J., AND GRACE, T. Controlling movement using parametric frame space 
interpolation. In Computer An­imation 93, Proceedings (1993), pp. 216 227. [12] ISAACS,P., AND COHEN, 
M. Controlling dynamic simulation with kinematic constraints, behavior functions and inverse dy­namics. 
In Computer Graphics(SIGGRAPH 87 Proceedings) (1987), vol. 21, pp. 215 224. [13] KASS, M. Condor: Constraint-based 
data.ow. In Com­puter Graphics (SIGGRAPH 92 Proceedings) (1992),vol. 26, pp. 321 330. [14] KOCHANEK,D., 
AND BARTELS, R. Interpolating splines with local tension, continuity and bias control. In Computer Graph­ics 
(SIGGRAPH 84 Proceedings) (1984), vol. 18, pp. 33 41. [15] KOGA,Y., KONDO,K., KUFFNER,J., AND LATOMBE,J.-C. 
Planning motions with intentions. In Computer Graphics (SIGGRAPH 94 Proceedings) (July 1994), pp. 395 
408. [16] LEBRUN, M. Digital waveshaping synthesis. Journal of the Audio Engineering Society 27, 4 (1979), 
250 266. [17] LITWINOWICZ, P. Inkwell: A 2 1/2 D animation system. In Computer Graphics (SIGGRAPH 91 
Proceedings) (1991), vol. 25, pp. 113 122. [18] LIU,Z., GORTLER,S., AND COHEN, M. Hierarchical spacetime 
control. In Computer Graphics(SIGGRAPH 94 Proceedings) (July 1994), pp. 35 42. [19] MEYER,K., APPLEWHITE,H., 
AND BIOCCA,F. A survey of posistion trackers. Presence: Teleoperators and Virtual Environments 1, 2 (Spring 
1992), 173 200. [20] ODGEN,J., ADELSON,E., BERGEB,J., AND BURT,P. Pyramid­based computer graphics. RCA 
Engineer 30, 5 (1985), 4 15. [21] PARKE,F. ET AL. State of the art in facial animation. In Computer Graphics 
(SIGGRAPH 90), Course Notes (August 1990). [22] PHILLIPS,C., AND BADLER, N. Interactive behaviors for 
bipedal articulated .gures. In Computer Graphics (SIG-GRAPH 91 Proceedings) (1991), vol. 25, pp. 359 
362. [23] RASURE,J., AND KUBICA, S. The Khoros application devel­opment environment. Tech. rep., Khoral 
Research, Inc., 4212 Courtney NE, Albuquerque, NM, 87108, USA, 1993. [24] SEDERBERG,T., AND GREENWOOD, 
E. A physically-based ap­proach to 2 D shape blending. In Computer Graphics (SIG-GRAPH 92 Proceedings) 
(1992), vol. 26, pp. 26 34. [25] SERRA,B., AND BERTHOLD, M. Subpixel contour matching using continuous 
dynamic programming. IEEE Computer Vision and Pattern Recognition (1994), 202 207. [26] STEIN,C., AND 
HITCHNER, H. The multiresolution dissolve. SMPTE Journal (December 1988), 977 984. [27] STURMAN, D. A 
discussion on the development of motion con­trol systems. In Graphics Interface 86, Tutorial on Computer 
Animation (1986). [28] UNUMA,M., AND TAKEUCHI, R. Generation of human mo­tion with emotion. In Computer 
Animation 93, Proceedings (1993), pp. 77 88. [29] VELHO,L. Piecewise Descriptions of Implicit Surfaces 
and Solids. PhD thesis, University of Toronto, Computer Science, 1994. [30] WILHELMS, J. Virya a motion 
control editor for kinematic and dynamic aniamtion. In Graphics Interface 86, Proceedings (1986), pp. 
141 146. [31] WITKIN,A., AND KASS, M. Spacetime constraints. In Com­puter Graphics (SIGGRAPH 88 Proceedings) 
(1988),vol. 22, pp. 159 168. [32] WITKIN,A., TERZOPOULUS,D., AND KASS, M. Signal match­ing through scale 
space. International Journal of Computer Vision (1987), 133 144. [33] ZELTZER, D. Towards an integrated 
view of 3 D computer character animation. In Graphics Interface 85, Proceedings (1985), pp. 105 115. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218422</article_id>
		<sort_key>105</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Motion warping]]></title>
		<page_from>105</page_from>
		<page_to>108</page_to>
		<doi_number>10.1145/218380.218422</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218422</url>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P18516</person_id>
				<author_profile_id><![CDATA[81100295587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Carnegie Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P310472</person_id>
				<author_profile_id><![CDATA[81100620346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zoran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popovic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Carnegie Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely. Feature-based image metamorphosis. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 35-42, July 1992.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Boulic, Zhiyong Huang, N.M. Thalmann, and D. Thaimann. Goal-oriented design and correction of articulated figure motion with the track system. Computers and Graphics (UK), 18:443-52, July-Aug. 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192223</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[A. Finkelstein and D. H. Salesin. Multiresolution curves. In Computer Graphics (SIGGRAPH '94 Proceedings), volume 28, pages 261-268, July 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[S. Guo, J. Roberg6, and T. Grace. Controlling movement using parametric frame space interpolation. In Proceedings of Computer Animation ' 93, pages 216-227, 1993.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[H. Ko and N. I. Badler. Straight line walking animation based on kinematic generalization that preserves the original characteristics. In Proceedings Graphics Interface '93, pages 9-16, May 1993.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192270</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E Litwinowicz and L. Williams. Animating images with drawings. In Computer Graphics (SIGGRAPH '94 Proceedings), pages 409-12, July 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614291</ref_obj_id>
				<ref_obj_pid>614257</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[K. Perlin. Real time responsive animation with personality. IEEE Transactions on Visualization and Computer Graphics, 1(1), March 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[I. J. Schoenberg. Cardinal spline interpolation. CBMS (Conf. Board of the Mathematical Sciences), 12, 1973.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Unuma and R. Takeuchi. Generation of human walking motion with emotion for computer animation. Trans. Inst. Electron. Inf. Commun. Eng. D-H (Japan), J76D-II:1822-31, Aug. 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378507</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A. Witkin and M. Kass. Spacetime constraints. In Computer Graphics (SIGGRAPH '88 Proceedings), volume 22, pages 159-168, August 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Motion Warping Andrew Witkin and Zoran Popovic´ Computer Science Department Carnegie Mellon University 
Pittsburgh, PA 15213 Keywords animation, motion capture  Abstract We describe a simple technique for 
editing captured or keyframed animation based on warping of the motion parameter curves. The animator 
interactively de.nes a set of keyframe-like constraints which are used to derive a smooth deformation 
that preserves the .ne structure of the original motion. Motion clips are combined by overlapping and 
blending of the parameter curves. We show that whole families of realistic motions can be derived from 
a single cap­tured motion sequence using only a few keyframes to specify the motion warp. Our technique 
makes it feasible to create libraries of reusable clip motion. 1 Introduction Systems for real-time 3-D 
motion capture have recently become commercially available. These systems hold promise as a means of 
producing highly realistic human .gure animation with more ease and ef.ciency than traditional techniques 
afford. Motion capture can be used to create custom animation, or to create libraries of reusable clip-motion. 
Clip-motion libraries could facilitate conven­tional animation, or serve as databases for on-the-.y assembly 
of animation in interactive systems. The ability to edit captured motion is vitally important. Cus­tom 
animation must be tweaked or adjusted to eliminate artifacts, to achieve an accurate spatial and temporal 
match to the computer gen­erated environment, or to overcome the spatial constraints of motion capture 
studios. To reuse clip motion we must to be able to freely alter the geometry (e.g. to .t a canned walk 
onto uneven terrain, to retarget a reaching motion, or to compensate for geometric varia­tions from model 
to model) and the timing (for speed control, syn­chronization, etc.) and we also need to be able to perform 
seamless transitions, e.g. from a walk to a run, or from sitting to standing to walking. To be useful, 
editing should be much easier than animat­ing from scratch, and should preserve the quality and naturalness 
of the original motion. Motion capture yields an unstructured representation a se­quence of sampled positions 
for each degree of freedom, or through pre-processing using inverse kinematics, sequences of joint angle 
The authors address is Computer Science Department, Carnegie Mel­lon University, 5000 Forbes Ave., Pittsburgh 
PA 15213. Andrew Witkin s email address is aw@cs.cmu.edu. Zoran Popovi´c s email address is zoran@cs.cmu.edu. 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
values. Editing this kind of iconic description poses a problem anal­ogous to that of editing a bitmapped 
image or a sampled hand-drawn curve(see.gure1.) One approach to editing is to .t curves to the raw data, 
produc­ing a keyframe-like description than can be modi.ed by editing the curve s control points. The 
drawback of this approach is that the .t curve is liable to need at least as many control points as would 
have been needed to keyframe the motion manually. To make a global change to the motion would require 
all or most of the control points to be adjusted, losing much of motion capture s advantage over hand 
animation. An alternative is to edit by transforming the iconic description, in a manner analogous to 
image morphing [1]. This is the approach we take here. We hypothesize that much of the aliveness of cap­tured 
motion, distinguishing it from most keyframe animation, re­sides in the high-frequency details, and that 
these details can survive smooth transformations perceptually intact, provided the transfor­mations are 
not too extreme. The methodology we propose is similar to that of conventional keyframing, in that the 
animator interactively modi.es the pose at selected frames. In fact, we are able to use a standard keyframe 
an­imation system as an interactive front end. However, we take the keyframes as constraints on a smooth 
deformation to be applied to the captured motion curves. The deformation satis.es the keyframe constraints 
while preserving the .ne details of the original motion. This simple technique allows a whole family 
of realistic motions to be created from a single prototype using just a handful of keyframes to control 
the motion warp. Although inspired by the need to manip­ulate captured motion, the techniques we describe 
are applicable to keyframed motion as well. The main contribution of the paper is to introduce motion 
warping as a means of editing captured motion and to demonstrate that even very complex motions such 
as a hu­man walk or a tennis swing can be radically reshaped using just a few keyframes without losing 
their realistic appearance. To create transitions between clips, we perform motion blends using a technique 
similar to that described in [7]: the motions to be joined are overlapped, with one or more critical 
correspon­dence points identi.ed. The combined motion is generated by time­warping the constituent motions 
to align the correspondence points, then blending using time-dependent weights. The remainder of the 
paper is organized as follows: in the next section, we brie.y describe related work. Section 3 describes 
the details of our warping and blending methods. In Section 4 we de­scribe our implementation and results. 
We conclude with a brief dis­cussion of the method s advantages and limitations, and directions for further 
work. 2 Background Keyframe animation is usually edited by adding, deleting, and mod­ifying keyframes, 
the same process used to create the animation initially. Consequently, motion editing has seldom been 
treated as a distinct topic. State-of-the-art animation systems such as Figure 1: Some of the captured 
motion curves of human walking. SOFTIMAGETM, AliasTM, and WavefrontTMdo provide simple mo­tion editing 
tools (e.g. curve .tting, global scaling and translation). A motion editing system described in [2] provides 
a variety of tools for manipulating keyframes, and for frame-by-frame repair to en­force constraints. 
Commercial motion capture services possess pro­prietary editing tools which, to the best of our knowledge, 
employ curve .tting and control-point adjustment rather than deformation. As noted in the preceding section, 
this approach is not well suited to large-scale transformations. As we have pointed out, a fair analogy 
can be drawn between our approach and image morphing [1], in that geometric deforma­tions are being applied 
to iconic data. Litwinovicz and Williams [6] perform motion transformations to map motion tracking data 
onto image morphing control points. A few researchers have proposed function .tting [5], or motion curve 
.ltering [9] to reduce data vol­ume. In a similar vein, Finkelstein and Salesin s multiresolution curve 
.tting technique [3] might also be adapted to motion editing: modi.cation of coarse-scale components 
would allow large-scale changes. However, it appears that the method would have to be sig­ni.cantly extended 
to handle multiple keyframe constraints. Several motion blending techniques have been reported previ­ously 
[7, 4, 2], although they do not appear to have been applied to captured motion data. The blending technique 
used here most closely resembles that of Perlin [7]. In his system, procedural mo­tions are are concatenated 
using eased blending curves, using noise functions to add high frequency texture to the motion. Blending 
is also used to create hybrid motions. Perlin mentions the possibility of applying his techniques to 
captured data. 3 Warping Articulated objects such as human .gures are usually represented as rotation 
hierarchies parameterized by a whole-body translation, a whole-body rotation, and a set of joint angles. 
Motion is described by a setof motion curves each giving the value of one of the model s parameters as 
a function of time. We wish to derive new motion curves based on a set of sparse keyframe-like constraints 
that are interactively speci.ed by the an­imator. Subject to the constraints, the new curves should be 
similar to the originals in the sense that .ne details of the motion are pre­served. We warp each motion 
curve independently, so we can consider just a single curve .(t). As in conventional keyframing, the 
con­straints include a set of (.i,ti ) pairs each giving the value that . must assume at the speci.ed 
time. In addition, we allow a set of (t. j,tj ) pairs acting as time warp constraints, each giving the 
time t. j to which the value originally associated with time tj should be dis­placed. The warped motion 
curve ..(t.) is de.ned by two functions, ..(t) = f (.,t),and t = g(t.). We map from t. to t rather than 
the other way around because this is the direction in which we will need to go: given an actual frame 
time, g tells us where to go in the un­timewarped motion curve to fetch .. .1 Constructing a suitable 
timewarp function is straightforward: we need a smooth well-behaved function g(t.) that interpolates 
the timewarp constraints, satisfying tj = g(t. j ), . j. Just about any inter­polating spline would do; 
we chose the Cardinal spline [8]. Notice that g need not be monotonic: a negative slope means that time 
is reversed, which is sometimes desirable. A potential problem is that spline overshoot could induce 
unwanted time reversals. We have not found this to be a problem in practice with Cardinal splines, al­though 
it might be more of an issue if C2 splines were used. We warp the values using a transformation of the 
form ..(t) = a(t).(t)+ b(t),where a(t)and b(t) are scaling and offset functions respectively. The two 
functions must satisfy .i .(ti ) = a(ti ).(ti ) + b(ti ), .i. A problem is that the values of a and b 
are not uniquely determined at the constraint points: we must somehow decide what mixture of scale and 
offset to use. After considering vari­ous schemes, we found that allowing the user to select manually 
whether to scale or shift works best. When scaling has been se­lected, we hold b(ti ) constant as the 
user modi.es a keyframe, ob­taining a(ti ) = (.i .(ti ) - b(ti ))/.(ti), and when shifting has been selected 
we hold a(ti ) constant and solve for b(ti ). Frequently, no scaling is desired, for instance to translate 
or rotate an entire motion. Scaling of joint angles is useful for exaggeration, in which case the offset 
function can be used to set the zero-point around which scal­ing takes place. Once the values of a(ti 
) and b(ti ) are obtained a(t) and b(t) can be constructed straightforwardly using an interpolating spline, 
as for the time warp. To concatenate motion clips with blending we overlap an inter­val at the end of 
the .rst clip with an interval at the beginning of the second, and progressively blending from the .rst 
clip to the sec­ond over the course of the overlap interval. To accomplish a seam­less transition, one 
or both segments must generally be warped to bring them into reasonable alignment. If the intervals to 
overlap are of unequal duration they must be time warped as well. The blend is a straightforward weighted 
sum of the two motion curves as de­scribed in [7]: .blend (t) = w(t).1(t)+ (1 - w(t)).2(t),where .1(t) 
and .2(t) are the motion curves being blended and w(t) is a normal­ized slow-in/slow-out weight function. 
 4 Results Our implementation uses SOFTIMAGE/Creative EnvironmentTM as a front end. From the animator 
s standpoint, setting up a motion warp is no different than creating ordinary keyframes: the anima­tor 
selects a frame to be a key, then poses the model interactively. (The user must also specify whether 
the scaling or offset is being adjusted offset is the default.) Time warp constraints can be im­posed 
by sliding keyframe markers on a time line, or by entering times directly. There is nothing to preclude 
timewarping each mo­tion curve independently, but thus far we have restricted ourselves to a single, 
global timewarp function. All of the warped clips we have created required between one and .ve motion-warping 
keyframes to create, far fewer in each case than would be required to specify even a highly simpli.ed 
version of the motion by direct keyframing. We have derived a large set of clip 1We could easily allow 
t. to depend on . as well as t, letting f and g per­form an arbitrary deformation of the (.,t) plane 
but this does not appear to be useful. Figure 2: A frame from the original walking sequence, and the 
cor­responding frames from a number of warped sequences. Clockwise from upper left: The original sequence; 
stepping onto a block; car­rying a heavy weight; walking on tiptoe; bending through a door­way; stepping 
around a post; trucking; stepping over an obstacle motions from a basic captured walking sequence. The 
derived mo­tions include: bending down to step through a low doorway; step­ping over a low obstacle; 
stepping onto and down from a higher ob­stacle; walking around a still higher obstacle; climbing stairs; 
walk­ing with a limp; a stooped walk; a trucking gait, and a sneaky walk. Figure 2 shows frames from 
these sequences. Figures 4 and 5 illustrate the low doorway in detail: .gure 4 shows the original and 
warped motion curves for the left and right hip joints, and .gure 5 shows selected frames from the original 
and warped sequences. One application of motion warping is on-the-.y motion synthesis for virtual environments 
or games. We explored this idea by warp­ing captured motion clips of a tennis player performing backhand, 
forehand, and overhead shots. Frames from several forehand shots are shown in .gure 3. We found that 
we could produce realistic ten­nis shots over a wide range of ball trajectories by manually choos­ing 
the most appropriate motion clip, and setting a single key plac­ing the racket on the ball at the desired 
moment of impact. The next step will be to automate clip selection and keyframing, possibly with blending 
between the stored clips, to create a parameterized tennis player. We have also used motion warping to 
edit a clip created by con­ventional keyframing: we warped a straight-line cyclic walk of a bipedal creature 
into an animation where the same creature traverses an irregular series of stepping stones. The same 
effect could have been acheived by modifying all of the original keyframes, instead of warping. However, 
many more keys were used to specify the mo­tion initially than were required to warp it.  5 Conclusion 
We have described a simple technique for editing of captured or keyframed motion by warping and blending. 
We demonstrated that a wide range of new realistic motions can be created by warping and joining captured 
motion clips, using only a few motion-warping keyframes to modify the prototype motions, and using simple 
blend­ing to join overlapping motion clips. A key advantage of motion warping is that it .ts well into 
the fa­miliar keyframe animation paradigm, allowing a wide range of ex­isting tools, techniques, and 
skills to be brought to bear. On the other hand, motion warping shares some limitations of standard keyfram­ing, 
for example the dif.culty of enforcing geometric constraints between keys. We believe that constraint 
techniques applicable to Figure 3: A frame from a captured motion sequence of a tennis fore­hand shot 
(green), and the corresponding frames from two warped sequences (red and blue.) Only a single keyframe 
at the moment of impact was required to produce the warped sequences. conventional keyframing can be 
applied to motion warping as well. A further limitation is that motion warping is a purely geometric 
technique, not based on any deep understand of the motion s struc­ture. Consequently, as with analogous 
image morphing techniques, extreme warps are prone to look distorted and unnatural. A physi­cally based 
technique in the spirit of [10] might overcome this lim­itation.  Acknowledgements We thank Softimage, 
Inc. and BioVision, Inc. for access to mo­tion capture data. This research was supported in part by a 
Sci­ence and Technology Center Grant from the National Science Foun­dation, #BIR-8920118, by the Engineering 
Design Research Cen­ter, an NSF Engineering Research Center at Carnegie Mellon Uni­versity, by the Phillips 
Laboratory, Air Force Material Command, USAF, under cooperative agreement number F29601-93-2-0001, by 
Apple Computer, Inc, and by an equipment grant from Silicon Graphics, Inc. References [1] T. Beier and 
S. Neely. Feature-based image metamorphosis. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 
92 Proceedings), volume 26, pages 35 42, July 1992. [2] R. Boulic, Zhiyong Huang, N.M. Thalmann, and 
D. Thal­mann. Goal-oriented design and correction of articulated .g­ure motion with the track system. 
Computers and Graphics (UK), 18:443 52, July-Aug. 1994. [3] A. Finkelstein and D. H. Salesin. Multiresolution 
curves. In Computer Graphics (SIGGRAPH 94 Proceedings), vol­ume 28, pages 261 268, July 1994. left hip 
original right hip warped Figure 4: Original and warped motion curves for left and right hip joints. 
The unshaded portion matches .gure 5. Vertical lines denote motion warping keyframes. [4] S. Guo, J. 
Roberg´e, and T. Grace. Controlling movement us­ing parametric frame space interpolation. In Proceedings 
of Computer Animation 93, pages 216 227, 1993. [5] H. Ko and N. I. Badler. Straight line walking animation 
based on kinematic generalization that preserves the original charac­teristics. In Proceedings Graphics 
Interface 93, pages 9 16, May 1993. [6] P. Litwinowicz and L. Williams. Animating images with draw­ings. 
In Computer Graphics (SIGGRAPH 94 Proceedings), pages 409 12, July 1994. [7] K. Perlin. Real time responsive 
animation with personality. IEEE Transactions on Visualization and Computer Graphics, 1(1), March 1995. 
[8] I. J. Schoenberg. Cardinal spline interpolation. CBMS (Conf. Board of the Mathematical Sciences), 
12, 1973. [9] M. Unuma and R. Takeuchi. Generation of human walking motion with emotion for computer 
animation. Trans. Inst. Electron. Inf. Commun. Eng. D-II (Japan), J76D-II:1822 31, Aug. 1993. [10] A. 
Witkin and M. Kass. Spacetime constraints. In Computer Graphics (SIGGRAPH 88 Proceedings), volume 22, 
pages 159 168, August 1988. 10 22 34 44 54 60 66 70 76 86 96 Figure 5: Selected frames from 
an original and warped motion se­quence. Time runs from top to bottom. On each row are shown the frame 
number, the original frame, and the warped frame. The four shaded rows correspond to motion warping keyframes. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218424</article_id>
		<sort_key>109</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Automatic reconstruction of surfaces and scalar fields from 3D scans]]></title>
		<page_from>109</page_from>
		<page_to>118</page_to>
		<doi_number>10.1145/218380.218424</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218424</url>
		<keywords>
			<kw><![CDATA[algebraic surfaces]]></kw>
			<kw><![CDATA[alpha-shapes]]></kw>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[range data analysis]]></kw>
			<kw><![CDATA[shape recovery]]></kw>
			<kw><![CDATA[triangulations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P42739</person_id>
				<author_profile_id><![CDATA[81100323675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chandrajit]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Bajaj]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Sciences, Purdue University, West Lafayette, IN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P83787</person_id>
				<author_profile_id><![CDATA[81100373633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fausto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bernardini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Sciences, Purdue University, West Lafayette, IN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14077788</person_id>
				<author_profile_id><![CDATA[81100195555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Guoliang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computing Center, Academia Sinica, P. O. Box 2719, Beijing, 100080, P. R., China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALFELD, P. A trivariate clough-tocher scheme for tetrahedral data. Computer Aided Geometric Design i (1984), 169-181.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93805</ref_obj_id>
				<ref_obj_pid>93803</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ALFELD, P. Scattered data interpolation in three or more variables. In Mathematical Methods in Computer Aided Geometric Design, T. Lyche and L. Schumaker, Eds. Academic Press, Boston, 1989, pp. 1-34.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>23798</ref_obj_id>
				<ref_obj_pid>23792</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[AURENHAMMER, F. Power diagrams: properties, algorithms and applications. SIAM J. Comput. i6 (1987), 78-96.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., BERNARDINI, F., AND XU, G. Reconstruction of surfaces and surfaces-on-surfaces from unorganized weighted points. Computer Science Technical Report CSD-TR-94-001, Purdue University, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., BERNARDINI, F., AND XU, G. Adaptive reconstruction of surfaces and scalar fields from dense scattered trivariate data. Computer Science Technical Report CSD-TR-95-028, Purdue University, 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>221662</ref_obj_id>
				<ref_obj_pid>221659</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., CHEN, J., AND XU, G. Modeling with cubic A- patches. ACM Transactions on Graphics (1995). To Appear.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134014</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., AND IHM, I. C1 smoothing of polyhedra with implicit algebraic splines. Computer Graphics 26, 2 (July 1992), 79-88. Proceedings of SIGGRAPH 92.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>212536</ref_obj_id>
				<ref_obj_pid>212531</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[BAJAJ, C., AND XU, G. Modeling scattered function data on curved surfaces. In Fundamentals of Computer Graphics, Z. T. J. Chen, N. Thalmann and D. Thalmann, Eds. Beijing, China, 1994, pp. 19-29.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[BARNHILL, R. E. Surfaces in computer aided geometric design: A survey with new results. Computer Aided Geometric Design 2 (1985), 1-17.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[BARNHILL, R. E., AND FOLEY, T. A. Methods for constructing surfaces on surfaces. In Geometric Modeling: Methods and their Applications, G. Farin, Ed. Springer, Berlin, 1991, pp. 1- 15.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172325</ref_obj_id>
				<ref_obj_pid>172315</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[BARNHILL, R. E., OPITZ, K., AND POTTMANN, H. Fat surfaces: a trivariate approach to triangle-based interpolation on surfaces. Computer Aided Geometric Design 9 (1992), 365- 378.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[BARNHILL, R. E., PIPER, B. R., AND RESCORLA, K. L. Interpolation to arbitrary data on a surface. In Geometric Modeling, G. Farin, Ed. SIAM, Philadelphia, 1987, pp. 281-289.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357349</ref_obj_id>
				<ref_obj_pid>357346</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[BOISSONAT, J. D. Geometric structures for three-dimensional shape representation. ACM Transactions on Graphics 3, 4 (Oct. 1984), 266-286.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>64991</ref_obj_id>
				<ref_obj_pid>64978</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[CLARKSON, K. L. A randomized algorithm for closest-point queries. SIAM J. Comput. 17 (1988), 830-847.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>93817</ref_obj_id>
				<ref_obj_pid>93803</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[DAHMEN, W. Smooth piecewise quadratic surfaces. In Mathematical Methods in Computer Aided Geometric Design, T. Lyche and L. Schumaker, Eds. Academic Press, Boston, 1989, pp. 181-193.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172335</ref_obj_id>
				<ref_obj_pid>172333</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[DAHMEN, W., AND THAMM-SCHAAR, T.-M. Cubicoids: modeling and visualization. Computer Aided Geometric Design 10 (1993), 93-108.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[DEY, T. K., BAJAJ, C. L., AND SUGIHARA, K. On good triangulations in three dimensions. Internat. J. Comput. Geom. Appl. 2, 1 (1992), 75-95.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172356</ref_obj_id>
				<ref_obj_pid>172345</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[DEY, T. K., SUGIHARA, K., AND BAJAJ, C. L. Delaunay triangulations in three dimensions with finite precision arithmetic. Comput. Aided Geom. Design 9 (1992), 457-470.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>28905</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[EDELSBRUNNER, H. Algorithms in Combinatorial Geometry, vol. 10 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag, Heidelberg, West Germany, 1987.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[EDELSBRUNNER, H., KIRKPATRICK, D., AND SEIDEL, R. On the shape of a set of points in the plane. IEEE Trans. on Information Theory 29, 4 (1983), 551-559.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>156635</ref_obj_id>
				<ref_obj_pid>174462</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[EDELSBRUNNER, H., AND MUCKE, E. E Three-dimensional alpha shapes. ACM Transactions on Graphics 13, 1 (Jan. 1994), 43-72.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142688</ref_obj_id>
				<ref_obj_pid>142675</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[EDELSBRUNNER, H., AND SHAH, N. R. Incremental topological flipping works for regular triangulations. In Proc. 8th Annu. ACM Sympos. Comput. Geom. (1992), pp. 43-52.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>18691</ref_obj_id>
				<ref_obj_pid>18690</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[FARIN, G. Triangular Bemstein-B6zier patches. Computer Aided Geometric Design 3 (1986), 83-127.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83600</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[FARIN, G. Curves and Sulfaces forComputerAided Geometric Design: A Practical Guide. Academic Press, 1990.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[FAUGERAS, O. D., HEBERT, M., MUSSI, P., AND BOISSONNAT, J. D. Polyhedral approximation of 3-D objects without holes. Computer Vision, Graphics and Image Processing 25 (1984), 169-183.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[FRANKE, R. Recent advances in the approximation of surfaces from scattered data. In Multivariate Approximation, C.K.Chui, L.L.Schumarker, and F.I.Utreras, Eds. Academic Press, New York, 1987, pp. 275-335.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139906</ref_obj_id>
				<ref_obj_pid>139834</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Guo, B. Surface generation using implicit cubics. In Scientific Visualizaton of Physical Phenomena, N. M. Patrikalakis, Ed. Springer-Verlag, Tokyo, 1991, pp. 485-530.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Guo, B. Non-splitting macro patches for implicit cubic spline surfaces. Computer Graphics Forum 12, 3 (1993), 434-445.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192233</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, f., DUCHAMP, f., HALSTEAD, M., JIN, n., MCDONALD, J., SCHWITZER, J., AND STUELZLE, W. Piecewise smooth surface reconstruction. In Computer Graphics Proceedings (1994), Annual Conference Series. Proceedings of SIGGRAPH 94, ACM SIGGRAPH, pp. 295-302.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H., DEROSE, T., DUCHAMP, T., MCDONALD, J., AND STUELZLE, W. Surface reconstruction from unorganized points. Computer Graphics 26, 2 (July 1992), 71-78. Proceedings of SIGGRAPH 92.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[HOPPE, n., DEROSE, T., DUCHAMP, f., MCDONALD, J., AND STUELZLE, W. Mesh optimization. In Computer Graphics Proceedings (1993), Annual Conference Series. Proceedings of SIGGRAPH 93, ACM SIGGRAPH, pp. 19-26.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[MOORE, D., AND WARREN, J. Approximation of dense scattered data using algebraic surfaces. In Proceedings of the 24th annual Hawaii International Conference on System Sciences (1991), V. Milutinovic and B. D. Shriver, Eds., vol. 1.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>725533</ref_obj_id>
				<ref_obj_pid>647363</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M. Modeling and visualizing volumetric and surface-on-surface data. In Focus on Scientific Visualization, H. Hagen, H. Muller, and G. M. Nielson, Eds. Springer, 1992, pp. 219-274.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>160683</ref_obj_id>
				<ref_obj_pid>160673</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M. Scattered data modeling. IEEE Computer Graphics &amp; Applications 13 (1993), 60-70.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>87568</ref_obj_id>
				<ref_obj_pid>87526</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M., FOLEY, T., LANE, D., FRANKE, R., AND HAGEN, H. Interpolation of scattered data on closed surfaces. Computer Aided Geometric Design 7, 4 (1990), 303-312.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617657</ref_obj_id>
				<ref_obj_pid>616017</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M., FOLEY, T. A., HAMANN, B., AND LANE, D. Visualizing and modeling scattered multivariate data. IEEE Computer Graphics &amp; Applications 11, 3 (May 1991), 47-55.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M., AND FRANKE, R. Scattered data interpolation and applications: A tutorial and survey. In Geometric Modeling: Methods and Their Applications, H. Hagen and D. Roller, Eds. Springer, 1990, pp. 131-160.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[O'ROURKE, J. Polyhedra ofminimal area as 3D object models. In Proc. of the International Joint Conference on Artificial Intelligence (1981), pp. 664-666.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>150958</ref_obj_id>
				<ref_obj_pid>147632</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[POTTMANN, H. Interpolation on surfaces using minimum norm networks. Computer Aided Geometric Design 9 (1992), 51- 67.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_obj_id>4333</ref_obj_id>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[PREPARATA, F. P., AND SHAMOS, M. I. Computational Geometry: an Introduction. Springer-Verlag, New York, NY, 1985.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>43653</ref_obj_id>
				<ref_obj_pid>43647</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[RESCORLA, K. C~ trivariate polynomial interpolation. Computer Aided Geometric Design 4 (1987), 237-244.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[TURK, G., AND LEVOY, M. Zippered polygonal meshes from range images. In Computer Graphics Proceedings (1994), Annual Conference Series. Proceedings of SIGGRAPH 94, ACM SIGGRAPH, pp. 311-318.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[VELTKAMP, R. C. 3D computational morphology. Computer Graphics Forum 12, 3 (1993), 115-127.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[WORSEY, A., AND FARIN, G. An n-dimensional clough-tocher interpolant. Constructive Approximation 3, 2 (1987), 99-110.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic Reconstruction of Surfaces and Scalar Fields from 3D Scans12 Chandrajit L. Bajaj3 Fausto Bernardini34 
Guoliang Xu5 Department of Computer Sciences Purdue University ABSTRACT We present an ef.cient and uniform 
approach for the automatic reconstruction of surfaces of CAD (computer aided design) models and scalar 
.elds de.ned on them, from an unorganized collection of scanned point data. A possible application is 
the rapid computer model reconstruction of an existing part or prototype from a three dimensional (3D) 
points scan of its surface. Color, texture or some scalar material property of the physical part, de.ne 
natural scalar .elds over the surface of the CAD model. Our reconstruction algorithm does not impose 
any convexity or differentiability restrictions on the surface of the original physical part or the scalar 
.eld function, except that it assumes that there is a suf.cient sampling of the input point data to unambiguously 
recon­struct the CAD model. Compared to earlier methods our algorithm has the advantages of simplicity, 
ef.ciency and uniformity (both CAD model and scalar .eld reconstruction). The simplicity and ef­.ciency 
of our approach is based on several novel uses of appropri­ate sub-structures (alpha shapes) of a three-dimensional 
Delaunay Triangulation, its dual the three-dimensional Voronoi diagram, and dual uses of trivariate Bernstein-B´ezier 
forms. The boundary of the CAD model is modeled using implicit cubic Bernstein-B´ezier patches, while 
the scalar .eld is reconstructed with functional cubic Bernstein-B´ezier patches. CR Categories and Subject 
Descriptors: I.3.5 [Computer Graph­ics]: ComputationalGeometryandObjectModeling;J.6[Computer-Aided Engineering]: 
Computer-Aided Design. Additional keywords: Geometric modeling, shape recovery, range data analysis, 
algebraic surfaces, triangulations, alpha-shapes. 1Thisworkhasbeensupportedin partbyNSFgrantCCR92-22467,AFOSRgrant 
F49620-94-1-0080, ONR grant N00014-94-1-0370 and ARO grant DAAH04-95-1­0008. 2See also http://www.cs.purdue.edu/research/shastra/shastra.html 
 3Department of Computer Sciences, Purdue University, West Lafayette, IN 47907­ 1398 USA. Email: fbajaj,fxbg@cs.purdue.edu 
4Additional partial support from CNR, Italy 5Computing Center, Academia Sinica, P. O. Box 2719, Beijing, 
100080, P. R. China. Email: xuguo@cs.purdue.edu Permission to make digital/hard copy of part or all of 
this work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, 
and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee.  INTRODUCTION 
In this paper we present an approach for the reconstruction of a surface, and scalar .elds de.ned over 
it, from scattered data points. The points are assumed sampled from the surface of a 3D object, and the 
sampling is assumed to be dense and uniform (these terms will be given a more precise meaning later in 
the paper). Laser range scanners are able to produce a dense sampling, usually organized in a rectangular 
grid, of an object surface. Some models also allow to measure the RGB components of the color (i.e. three 
scalar .elds) at each sampled point. When the object has a simple shape, this grid of points can be a 
suf.cient representation. However, objects with a more complex geometry, e.g. objects with holes, handles, 
pockets, cannot be scanned in a single pass, and the various scans are not easy to merge [42]. Other 
applications, for example recovering the shape of a bone from contour data extracted from a CT scan, 
require reconstruction of a surface from data points organized in slices. The approach of considering 
the input points as unorganized has the advantage of generating cross-derivatives by a uniform treatment 
of all spatial directions. The reconstruction problem we are considering may be formally stated as follows: 
Letan unorganizedcollection ofpoints P=f(xi,yi,zi)g R3 and associated values W=fwigR1, i=1n,be given. 
The points Pare assumed sampled from a domain D in R3 (the boundary of a three-dimensional object) while 
the values Ware assumed sampled from some scalar function F on the domain D. Construct a C1 smooth piecewise 
polynomial surface SD: fD(x,y,z)=0and a C1 smooth piecewise polynomial function (surface-on-surface) 
SF: fF(x,y,z)on some do­main that contains Psuch that, for i=1n: (a) jfD(xi,yi,zi)jED (b) jfF(xi,yi,zi) 
wijEF  where EDand EFare user-de.ned approximation parameters. The user can also choose the degree of 
the Bernstein-B´ezier polynomial patches used in the approximation. Additionally, generate different 
visualizations of the domain surface SDand the surface-on-surface SF . In this paper we reconstruct the 
C1 smooth domain SDusing a piecewise algebraic surface (the zero contour of a C1 trivariate piecewise 
polynomial function). The surface is constituted by &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 barycentric 
implicit Bernstein-B´ezier patches, which are guaran­teed to be single-sheeted within each tetrahedron. 
We have also developed a method similar to the one described here, but based on tensor-product Bernstein-B´ezier 
patches [5]. Some researchers have focused on reconstructing a piecewise­linear representation of the 
unknown surface D[38, 25, 13, 30, 43]. These papers provide a very nice survey of both the varied nature 
of applications and past approaches. Related prior work [2, 6, 7, 15, 16, 27, 28, 35, 33] of .tting with 
smooth implicit surface patches, minimally all require an input surface triangulation of the data points. 
The surface .tting paper of [32] is similar to ours in that it only assumes a suf.ciently dense set of 
input data points but differs from our approach in the adaptive nature of re.nement, in time ef.ciency 
and in the degree of the implicit surface patches used. The authors propose either a C0 reconstruction 
algorithm based on an adaptive tetrahedral decomposition, or a scheme that uses tri-quadratic (degree 
six) tensor product implicit surface patches with a Powell-Sabin type split to achieve C1 continuity. 
Our scheme effectively utilizes the incremental Delaunay tri­angulation for a more adaptive .t; the dual 
Voronoi diagram for ef.cient point location in signed distance computations and degree three implicit 
surface patches. Furthermore, in the same time it also computes a C1 smooth approximation SFof the sampled 
scalar function. A different, three-step solution is described in papers [30, 31, 29]. In the .rst phase, 
a triangular mesh that approximates the data points is created. In a second phase, the mesh is optimized 
with respect to the number of triangles and the distance from the data points. A third step constructs 
a smooth surface from the mesh. If the surface SDis given, the problem of constructing the scalar function 
SFis known as surface interpolation on a surface, and arises in several application areas, e.g. in modeling 
and visualizing the rain fall on the earth, the pressure on the wing of an airplane or the temperature 
on the surface of a human body. Note that the trivariate scalar function SFis a two dimensional surface 
in R4 since its domain is the two dimensional surface SD(and not all of R3). The problem is relatively 
recent and was posed as an open question by Barnhill [9]. A number of methods have been developed since 
then for its solution (for surveys see [10, 26, 37, 34]). Most of the proposed approaches interpolate 
scattered data over planar or spherical domain surfaces. In [12] and [35], the domain surface is generalized 
to a convex surface and a topological genus zero surface, respectively. Pottmann [39] presents a method 
which does not possess similar restrictions on the domain surface but requires it to be at least C2 differentiable. 
In [11] the C2 restriction is dropped, however the interpolation surface is constructed by trans.nite 
interpolation using non-polynomials. A similar non­polynomial trans.nite interpolant construction is 
used in [36], while the interpolation scheme in [41] requires at least C4 continuity. Another approach, 
based on interpolation with cubic (for C1)or quintic (for C2) polynomials, is described in [8]. 1 OVERVIEW 
OF THE ALGORITHM Our algorithm consists of the following three phases: 1. Preprocessing: Preprocess the 
data points so that a signed­distance function is de.ned and ef.ciently computable. I.e., given a query 
point q, the function must return the approxi­mate distance of the point from the domain surface SD, 
with a positive sign if qlies outside the object, and a negative sign otherwise. We use a-shapes [21] 
to compute a piecewise linear approximation of the domain SD, from which the ap­proximated signed distance 
is computed. Details on this part are given in Section 2. 2. Approximation: Incrementally decompose the 
space into tetrahedra. For each tetrahedron Tthat contains a portion of the domain D, compute Bernstein-B´ezier 
trivariate im­plicit approximants fDand fFfor both the domain Dand ;; the .eld F, based on data points 
and on the signed-distance function described above. Then compute the errors of the approximants for 
the given data points, and repeat the pro­cess, re.ning at each step the decomposition, until the errors 
meet the speci.ed requirements. The use of a global signed­distance function in the computation of the 
coef.cients of each patch guarantees C0 continuity of the reconstructed sur­faces. We use an incremental 
3D Delaunay triangulation scheme together with a suitable point-insertion scheme to avoid badly-shaped 
tetrahedra in the spatial decomposition. This part of the algorithm is further detailed in Section 3. 
3. Smoothing: Use a Clough-Tocher 12-way split to make the reconstructed surfaces C1-smooth. See Section 
4 for details. Our domain surface SDand surface-on-surface SFreconstruc­tion scheme does not impose any 
convexity or differentiability re­strictions on the original domain surface Dor function F, except that 
it assumes that there is a suf.cient sampling of the input point data to unambiguously reconstruct the 
domain surface D. While it is dif.cult to precisely bound the required sampling density, we ad­dress 
this issue in Section 2.4 and characterize the required sampling density in terms of an a-shape (subgraph 
of a Delaunay triangu­lation of the points) which matches the topology of the original (unknown) sampled 
surface D. Compared to the above methods our algorithm thus has the following advantages: 1. It uni.es 
the reconstruction of the domain surface Dand the scalar function Fde.ned on the domain surface; 2. 
It is adaptive and approximates large dense data sets with a relatively small number of C1 smooth patches. 
 Outline of the paper: The rest of our paper is as follows. In Sections 2, 3 and 4 we present a detailed 
description of Phases 1, 2 and 3 of the reconstruction algorithm as outlined above. In Section 5 we illustrate 
all the phases of the algorithm with the aid of a simple 2D example. In Section 6 we show some examples 
of reconstructed surfaces and surface-on-surfaces, and discuss possible directions of future investigation. 
More details on the algorithm and additional examples can be found in [4].  2 PHASE 1: PREPROCESSING 
AND THE SIGNED-DISTANCE FUNCTION As we mentioned in Section 1, our algorithm relies on the computa­tion 
of the signed-distance 8(q)of a query point qfrom the domain surface D. The absolute value of 8(q)is 
de.ned as the Euclidean minimal distance of the point qfrom the domain surface D, while its sign is arbitrarily 
de.ned to be positive when qlies outside the object whose boundary is D, and negative otherwise. In our 
implementation of the algorithm, we use a-shapes to compute a piecewise linear approximation LDof the 
domain D, and make use of the associated data-structures (3D Delaunay tri­angulation and Voronoi diagram) 
to ef.ciently locate qw.r.t. the object and compute the associated signed-distance. An alternative method 
for computing an approximated signed-distance function, based on propagation of normals, is described 
in [30]. Before describing the actual signed-distance computation, we brie.y review some concepts and 
results from Computational Ge­ometry used in the algorithm. The style of this presentation is informal. 
The reader can refer to the papers in the references for more details.  (a) (b) (c) (d) Figure 1: A 
set of points in 3D (a), and three different a-shapes. 2.1 Delaunay Triangulations Given a set Pof points 
in R3 one can build a tetrahedralization of the convex hull of P, that is, a partition of conv(P)into 
tetrahedra, in such a way that the circumscribing sphere of each tetrahedron Tdoes not contain any other 
point of Pthan the vertices of T. Such a tetrahedralization is called a (3D) Delaunay triangulation and, 
under non degeneracy assumptions (no three points on a line, etc.) it is unique. Many different techniques 
have been proposed for the computation of Delaunay triangulations (see [19, 40, 18]). For our purposes, 
an incremental approach is particularly well­suited, as it can be used in both the preprocessing phase 
and the incremental re.ning of the adaptive, approximating triangulation (see Section 3). The algorithm 
we use is the randomized, incremental, .ipping­based algorithm proposed in [22], with heed paid to robustness 
issues due to .nite precision calculations [18]. At the beginning the triangulation is initialized as 
a single tetrahedron, with vertices at in.nity , that contains all points ofP. At each step a point from 
P is inserted as a new vertex in the triangulation, the tetrahedron in which plies is split and the Delaunay 
property is re-established by .ipping tetrahedra. This algorithm uses a data structure, called the history 
DAG, that maintains the collection of discarded tetrahedra. The DAG is used to locate the tetrahedron 
in which the point to be inserted lies. When a tetrahedron is split or groups of tetrahedra are .ipped, 
they become internal nodes of the DAG while the newly created tetrahedra become their children in the 
DAG. To locate a point, one starts at the root of the DAG (the single tetrahedron of the initial triangulation) 
and follows links down to a leaf. It is possible to build the Delaunay triangulation of a set of n ddl2e 
points in Rdin O(nlog n+n)expected time. The second term in this expression is of the same order as the 
maximum number of possible simplices. In practice, the running time of the algorithm (for d= 2, 3) is 
much better than this theoretic bound (the actual running time depends on the distribution of points). 
 2.2 Voronoi Diagrams Voronoi diagrams are well known tools in computational geometry (see [3] for a 
survey). They provide an ef.cient solution to the Post Of.ce Problem, that is an answer to the query: 
what is the closest point p2Pto a given point q? Voronoi diagrams are related to Delaunay triangulations 
by duality. It is easy to build a Voronoi diagram once one has the corresponding triangulation, and vice-versa. 
A Voronoi diagram is a partition of the space into convex cells. There is a cell for each point of p2P,and 
the cell of a point pis the set of points that are closer to pthan to any other point of P. So, all one 
has to do to answer the closest­point query is to locate the cell the query point lies in. Ef.cient point-location 
data structures can be built on top of the Voronoi diagram. Using the randomized approach described in 
[14], one builds the point-location data-structure (called an RPO-tree,for Randomized Post Of.ce tree) 
on top of the Voronoi diagram in 2+e O(n)expected time, for any .xed E0, and is then able to answer the 
closest-point query in O(log n)expected time. The data 2+e structure requires O(n)space in the worst 
case. We use the RPO-tree data structure for our point location and signed-distance computations. 2.3 
a-Shapes Given the Delaunay triangulation Tof a point set P, regarded as a simplicial complex, one can 
assign to each simplex (2T(vertices, edges, triangles and tetrahedra) a size de.ned in the following 
way. Let T0be the smallest sphere whose boundary contains all vertices of (. Then the size of (will be 
de.ned to be equal to the square of the radius of T0,and (will be said to be con.ict-free if T0does not 
contain any point of P other than the vertices of (. The subcomplex Saof simplices (2Twith either one 
of the following properties: (a) The size of (is less than aand (is con.ict-free (b) (is a face of Tand 
T2Sa,  is called the a-shape of P. a-Shapes have been introduced in the plane in [20] and then extended 
to the three-dimensional space in [21]. One can intuitively think of an a-shape as the subcomplex of 
T obtained in the following way: imagine that a ball-shaped eraser, p whose radius is a, is moved in 
the space, assuming all possible positions such that no point of Plies in the eraser. The eraser removes 
all simplices it can pass through, but not those whose size is smaller than a. The remaining simplices 
(together with all their faces) form the a-shape for that value of the parameter a.Two extreme cases 
are the 0-shape, which reduces to the collection of points P,and the 1-shape, that coincides with the 
convex-hull of P. Notice that there exists only a .nite number of different a-shapes. The collection 
of all possible a-shapes of Pis called the family of a-shapes of P(see Figure 1), and can be computed 
in time proportional to the number of simplices in T.We use the a-shape computation for our generating 
an initial piecewise linear approximation LDof the domain surface D(see Section 2.4).  2.4 Signed-Distance 
Computation Obviously the domain surface Dis unknown, so we need to build some suitable approximation 
of it to classify points as either internal or external to the object being reconstructed, and to compute 
a distance from it. In the preprocessing phase the Delaunay triangulation of the set of input points 
Pis computed, and then the Voronoi diagram and the family of a-shapes of Pare constructed. During the 
process, the history DAG and the RPO-tree data structures are built to allow a fast location of the tetrahedron 
and Voronoi cell a query point q lies in. Note that all these data structures are intimately related. 
Tetrahedra in the Delaunay triangulation are classi.ed as either internal or external (and assigned a 
corresponding sign) based on a particular a-shape chosen as a good linear approximationLD to the surface 
to be reconstructed. The computation of the signed­distance is then reduced to locating the query point 
qin both the Delaunay triangulation, to decide its sign s=±1, and in the Voronoidiagram,to .ndtheclosestpoint 
p2P. The approximated signed-distance sjpqjis then returned. A dif.culty in the process outlined above 
is the choice of a suitable value for a. We assume that the input data is dense enough so that there 
exists an asuch that the a-shape approximates the object with the same topology as the original unknown 
surface D. In our current scheme a suitable ais selected interactively. The boundary of the selected 
a-shape must possess the following properties: (a) It does not contain any singular (i.e. isolated) 
vertex;  (b) There are no missing edges, i.e. there can be missing triangles in the boundary, but if 
two adjacent triangles are missing, their common edge must be in the a-shape.  These properties make 
a slightly weaker condition than requiring that there exist an a-shape that correctly approximates the 
object and that has a complete boundary. In our experience it is sometime dif.cult to .nd an avalue such 
that these stronger conditions are satis.ed, even for reasonably dense samplings. When an a-shape with 
the above properties is determined, it is easy to distinguish between internal and external tetrahedra 
in the underlying triangulation T. One does a breadth .rst search on the dual graph of T, starting with 
a tetrahedron that is known to be external (e.g. one that has a vertex at in.nity) and continuing with 
adjacent tetrahedra. These tetrahedra are marked as external (positive sign) and put in a queue for further 
processing. When one hits a tetrahedron Tbelonging to Sa, Tis marked as internal and not enqueued. The 
same happens when, visiting an adjacent tetrahedron Tof a positive tetrahedron (, one .nds that the common 
face (or all three edges of the common face) belongs to Sa.This means that going from (to Tone crosses 
the boundary, so Tis marked as internal (negative sign) and not enqueued. When the data points are not 
very dense or uniform, the error caused by using the approximated distance computation described above 
can be too large. In these cases, it is possible to improve the error by returning the distance of the 
query point from LD, instead of P.  3 PHASE 2: INCREMENTAL REFINEMENT AND APPROXIMATION In Phase 2 
of the algorithm a 3D Delaunay triangulation Tis ini­tialized and incrementally re.ned, and C0-continuous 
piecewise­polynomial functions (approximants) fDand fFare generated. For each tetrahedron T2Tthat contains 
a portion of Dwe compute two Bernstein-B´ezier trivariate polynomials f;Dand f;F,to approximate the part 
of domain surface and scalar .eld contained in T, respectively. The coef.cients of the polynomials are 
com­puted using data points within Tand the signed-distance function described in Section 2.4. After 
computing the two polynomials, the errors of the approx­imants are estimated and, if one or both the 
errors are too large, the current triangulation Tis re.ned, until the errors are within the given bounds. 
The triangulation re.nement is done by adding at each step a new point to split the tetrahedron with 
the maximum error, and using the incremental Delaunay triangulation algorithm to update the triangulation. 
Before describing in further details the computation of the ap­proximating functions, we recall some 
facts and terminology related to Bernstein-B´ezier trivariate forms. 3.1 Bernstein-B´ezier (BB) Form 
Let p1,p2,p3,p4 2R3 be af.ne independent. Then the tetrahe­dron Twith vertices p1,p2,p3,and p4,is T=[p1p2p3p4].For 
4 X any p=aipi2T, a=(a1,a2,a3,a4)Tis the barycentric i=1 coordinate of p.Let p=(x,y,z)T , pi=(xi,yi,zi)T 
. Then the barycentric coordinates relate to the Cartesian coordinates via the following relation 2 3 
2 3 2 3 x x1 x2 x3 x4 a 1 6 4 y z 7 5 = 6 4 y1 z1 y2 z2 y3 z3 y4 z4 7 5 6 4 aa 2 3 7 5 (1) 1 1 1 1 1 
a 4 Any polynomial f(p)of degree ncan be expressed as a Bernstein­B´ezier (BB) form over Tas X f(p) 
= bhBhn (a), 2Z+ 4 jhj= n where n n!hhhh 1 234 Bh(a)= aaaa 1 234 P 1! 2! 3! 4! 4 is a Bernstein polynomial,j 
j= iwith =( 1,2,3,4)T i=1 a=(a1,a2,a3,a4)Tis the barycentric coordinate of p, bh= bh1 h2 h3 h4 (as a 
subscript, we simply write 1 2 3 4 for ( 1,2,3, 4 are called B´ezier ordinates, and Z+ stands for the 
set of all four dimensional vectors with nonnegative integer components. The points 1 2 3 4 ph=p1 +p2 
+p3 +p4,j j=n nnnn are called the regular points of T. The points (ph,bh)2R4 are called B´ezier points 
and their piecewise linear interpolation B´ezier net. The following lemma gives necessary and suf.cient 
conditions for continuity. P Lemma 3.1 ([24]). Let f(p)= aBhn(a)and g(p)= jhj= nh P bhB hn(a)be two polynomials 
de.ned on two tetrahedra jhj= n [p1p2p3p4]and [p10 p2p3p4], respectively. Then (i) fand gare C0 continuous 
at the common face [p2p3p4]if and only if ah=bh,for all =0 2 3 4,j j=n(2) , T 4)) p 1 p Positive 
1 Negative  Layer 3 p Layer 3 p 4 4  p 3 p 3 Figure 2: The layers of B´ezier ordinates in a 
tetrahedron. (left) Three-sided patch. (right) Four-sided patch. p p 1 1  p  4 p p 3 3 Figure 3: 
The splitting of a tetrahedron (right) into four sub­tetrahedra (left). Only one of the resulting sub-tetrahedra 
is shown. (ii) fand gare C1 continuous at the common face [p2p3p4]if and only if (2) holds and, for all 
=023 4, jj=n1, bh+e1 = 1ah+e1 + 2ah+e2 + 3ah+e3 + 4ah+e4 (3) where =( 1,2,3,4)Tis de.ned by the following 
rela­tion p 0 1 = 1p1 + 2p2 + 3p3 + 4p4,j j=1 The relation (3) will be called coplanar condition. The 
following Lemmas give suf.cient conditions for a patch to be single-sheeted (see [6] for proofs and further 
details). Lemma 3.2 Let T=[p1p2p3p4]. The regular points of Tcan be thought of as organized in triangular 
layers, that we can number from 0 to ngoing from p1 to the opposite face [p2p3p4](see Fig­ure 2). If 
the B´ezier ordinates are all positive (negative) on layers 0,,k1 and all negative (positive) on layers 
k+1,,n (0 kn), then the patch is single-sheeted (i.e., any line through p1 and p2[p2p3p4]intersects the 
patch only once). Lemma 3.3 Let T=[p1p2p3p4]. The regular points of Tcan be thought of as organized in 
quadrilateral layers, that we can number from 0 to ngoing from edge [p1p2]to the opposite edge [p3p4] 
(seeFigure2). IftheB´ezierordinatesareallpositive(negative)on layers 0,,k1 and all negative (positive) 
on layers k+1,,n (0 kn), then the patch is single-sheeted (i.e., any line through p2[p1p2]and q2[p3p4]intersects 
the patch only once). In the Lemmas above, the B´ezier ordinates on layer kcan have any sign. Patches 
satisfying the conditions of Lemma 3.2 will be called three-sided; those satisfying the conditions of 
Lemma 3.3 will be called four-sided.  3.2 Outline of Phase 2 of the Algorithm We are now ready to present 
in details the steps required to compute the approximant functions fDand fF . 1. Build an initial bounding 
tetrahedron T, such that PT.Set T=fTgand V= vertices of T.Mark Tas new. 2. For each new tetrahedron 
T2T, compute the signed­distance at all its regular points ph. If the values of 8(ph), j8j=n, do not 
satisfy either Lemma 3.2 or Lemma 3.3, then set 1D; =; =1. Otherwise, compute local approximants  1F 
X D Dn f;(p)= bhBh(a) (4) jhj=n X F Fn f;(p)= bhBh(a) (5) jhj=n for the domain surface Dand scalar .eld 
Fas follows: For the domain approximant, the coef.cients bDare com­ h puted by .rst interpolating the 
computed values of the signed­distance function: D f;(ph)=8(ph),jj=n (6) The tetrahedron Tis then split 
into four sub-tetrahedra T1 T4 (see Figure 3) by joining the baricenter of Twith its four ver­tices (Tkis 
the sub-tetrahedron opposite to vertex pk). The regular points on the faces of the sub-tetrahedra coincide 
with those of the original tetrahedron T. For these points we use the coef.cients computed from (6). 
Notice that on the shared face of two adjacent tetrahedra these coef.cients will coin­cide, as fD, restricted 
to that face, interpolates the signed distance at a number of points equal to the number of its co­ef.cients. 
All interior coef.cients of the sub-tetrahedra are computed by solving the least squares problem fD(pi)=0,pi2PnTk,k=14 
;kf;D(ph)=8(ph),jj=n,60,k=14 k= k (7) where we use the values of the signed-distance at regular points 
(of each sub-tetrahedron Tk) in addition to the data points contained in T. The signed-distance data 
helps in avoiding multiple sheets in the approximating patch. For the scalar .eld approximant we compute 
a least squares approximation of the .eld values at data points within T: f;D (pi)=wi,pi2PnT (8) Notice 
that the .eld approximant is not globally continuous. Continuity will be achieved by averaging and interpolating 
values of the approximant at the vertices Vof Tin a subse­quent phase, described in Section 4. 3. If 
the coef.cients computed in the step above do not satisfy the conditions of either Lemma 3.2 or Lemma 
3.3, set 1D; = 1F; =1. Otherwise, compute the approximation error for both functions: q P 4 P k=1 pi2Pn; 
f;D(pi)2 1D; = Card(PnT) qP (fF(pi)wi)2 ; pi2Pn; F 1 = ; Card(PnT) DF (if TnP=.,then set 1; =0and 1; 
=0), and keep track of the following two quantities: 1D =max f1D g 0 0 ;;2T and FF 1000 =max f1g ; ;2T 
4. If both 1D ED000 EFthen the algorithm stops 00and 1F the incremental re.nement phase and begins the 
smoothing phase. Otherwise, either (0or (00is selected for further re­.nement(accordingtoauser-de.nablestrategy. 
E.g.: choose always (0.rst, assigning priority to the surface, as variations of the scalar .eld Fgenerally 
correspond to variations of the surface). The circumcenter qof the selected tetrahedron is computed and 
added to the set Vof vertices of the tri­angulation, qis inserted in Tand Tis updated with splits and 
.ippings to accommodate the new vertex and restore the Delaunay property (adding the center of the circumscribing 
sphereof (is utilizing the empty sphere property of Delaunay triangulations and in general yields good 
aspect ratio tetrahe­dra in the .nal triangulation [17]). At the same time the subset PnTof points that 
lie within each modi.ed tetrahedron T is updated. This is done by considering the points originally within 
the modi.ed simplex, and reclassifying them with re­spect to the splitting/.ipping planes. Then mark 
all split/.ipped tetrahedra as old and all newly created ones as new and go back to step 2.  4 PHASE 
3: ACHIEVING C1 CONTINUITY VIA A 3D CLOUGH-TOCHER SCHEME The functions fD(p)and fF(p)computed in phase 
2 of the algo­rithm are not C1 continuous. To achieve C1 continuity, we apply a subdivision scheme to 
the tetrahedra of T, and compute C1-smooth Bernstein-B´ezier patches on the re.ned triangulation. We 
base our trivariate scheme on the n-dimensional Clough-Tocher scheme given by Worsey and Farin [44, 23]. 
In this scheme, one computes for each vertex in the original triangulation an average of the values of 
the functions fDand fFand their gradients, for all patches that share that vertex (the surface approximant 
is already C0, so only the gradient needs to be averaged). In addition, the average gradient at the middle 
point of each edge is computed. Each tetrahedron is then split into twelve sub-tetrahedra by inserting 
the incenter of each tetrahedron and a point on each face (the point on the face shared by two adjacent 
tetrahedra must be collinear with their incenters [44]), and joining these points with the original vertices. 
A cubic trivariate polynomial is built on each sub-tetrahedron. The coef.cients of the twelve resulting 
patches are computed based on the value of the function at each vertex, the average gradient at vertices 
and mid-edge points, and the continuity constraint. The resulting patches are C1 continuous and interpolate 
the averaged values and gradient of the functions. Another trivariate Clough-Tocher scheme (see [1]) 
splits each tetrahedron into four sub-tetrahedra. However the interpolants in each sub-tetrahedra are 
now of quintic degree and furthermore re­quire C2 data at the vertices of the main tetrahedron. Since 
our data at the vertices of the tetrahedral mesh comes from the averaging of locally computed low degree 
interpolants, the higher order deriva­tives tend to be un-reliable in general. We therefore prefer to 
use the lower degree cubic scheme that uses only .rst order derivatives at the vertices. An alternative 
approach to build a C1 interpolant with cubic patches has been presented in [8], and its application 
to our method is described in [4]. 5 A SIMPLE 2D EXAMPLE We present in this section an example of the 
three phases of the algorithm. For presentation purposes, the steps are illustrated with the aid of a 
2D example. The method is in fact perfectly suited for being applied in 2D reconstruction, and we chose 
to describe it only for the 3D case to keep the notation simple and because the most interesting applications 
arise from the study of .elds on the surface of 3D objects. Restricting ourselves to a bi-dimensional 
example allows us to illustrate the various steps with pictures which we believe are more easily understood. 
The generalization of the techniques involved should be clear from the text. In the following we refer 
to Figures 4(a) (n). Figure (a) shows the sample points P2R2. Figure (b) shows the associated function 
values W. The computed Delaunay triangulation and associated Voronoi diagram are depicted in Figure (c). 
These data structures will be used for fast point location in signed-distance computation. The chosen 
a-shape is shown in Figure (d). Four steps of the approximation phase are illustrated in Figures (e) 
though (i). Notice the adaptive subdivision of the plane. The implicit Bernstein-B´ezier patches are 
shown in red. Empty triangles are light-blue and those containing a patch are grey. These triangles lie 
on the zero plane, so their intersection with the patches form the implicit curve fD =0. Figures (l) 
and (m) show the .nal reconstructed C1 implicit patches, after Clough-Tocher subdivision, for both the 
domain and the scalar .eld. The zero contour of fDis .nally shown in Figure (n). 6 EXAMPLES AND CONCLUSIONS 
Some examples of reconstruction of 3D objects and associated scalar .elds are presented in this Section. 
The data for the human femur in Figure 5, 9223 points, comes from contouring of a CT scan. The algorithm 
does not use the fact that the data is arranged in slices. The reconstructed C1 surface is made by 400 
cubic patches. The reconstruction algorithm took about 10 minutes on a SGI Indigo2. The engine in Figure 
6 has been reconstructed from a data set containing 9800 points. The number of patches generated in the 
approximation phase is 382, with an error equal to 1/100 of the size of the object. Each patch is of 
degree 3, and is therefore de.ned by 20 coef.cients. At the same time, an approximate C1 scalar .eld 
(pressure form a simulated experiment) over the surface has also been computed. Several techniques can 
be used to visualize this surface-on-surface data. In Figure 6(c) we show iso-pressure regions. With 
the normal projection method, each point pon the domain surface SDis projected in the direction normal 
to SD,toa distance proportional to the value fF(p)of the .eld at that point. The projected surface is 
visible in transparency in Figure 6(d), with iso-contours of the pressure projected on it. The data for 
the head of Spock is a subsampling (about 104 points have been used) of scan data obtained with a laser 
3D digitizer. The reconstructed surface is constituted by 1100 cubic patches.  REFERENCES [1] ALFELD, 
P. A trivariate clough-tocher scheme for tetrahedral data. Computer Aided Geometric Design 1 (1984), 
169 181. [2] ALFELD, P. Scattered data interpolation in three or more vari­ables. In Mathematical Methods 
in Computer Aided Geomet­ric Design, T. Lyche and L. Schumaker, Eds. Academic Press, Boston, 1989, pp. 
1 34. [3] AURENHAMMER, F. Power diagrams: properties, algorithms and applications. SIAM J. Comput. 16 
(1987), 78 96.  (a) (b) (c) (d) (e) (f) (g) (h) (i) (l) (m) (n)   Figure 4: An example of reconstruction 
of the boundary of a two-dimensional shape and an associated scalar .eld. (a) (b) (c) Figure 5: (a) 
Data set for the upper part of a human femur. Data from a CT scan. (b) Final decomposition (wireframe). 
(c) Reconstructed object. (a) (b) (c) (d)  Figure 6: A jet engine. (a) C0 reconstructed domain. Patches 
are visible in different colors. (b) Reconstructed domain (after C1 smoothing). (c)Iso-pressurecontoursandregionsofasurface-on-surfacepressurefunctiondisplayedonthesurfaceofthejetengine. 
(d)Thereconstructed engine surface and visualization of the pressure surface function surrounding the 
jet engine using the normal projection method. (a) (b) (c) Figure 7: (a) A set of dense, scattered, 
noisy data points. (b) and (c) C1 smooth reconstructed surface. In (b) patches have been randomly colored. 
[4] BAJAJ,C., BERNARDINI,F., AND XU, G. Reconstruction of [16] DAHMEN,W., AND THAMM-SCHAAR, T.-M. Cubicoids: 
mod­surfacesandsurfaces-on-surfacesfromunorganizedweighted eling and visualization. Computer Aided Geometric 
Design points. Computer Science Technical Report CSD-TR-94-001, 10 (1993), 93 108. Purdue University, 
1994. [17] DEY,T. K., BAJAJ,C. L., AND SUGIHARA, K. On good tri­ [5] BAJAJ,C., BERNARDINI,F., AND XU, 
G. Adaptive reconstruc-angulations in three dimensions. Internat. J. Comput. Geom. tion of surfaces and 
scalar .elds from dense scattered trivariate Appl. 2, 1 (1992), 75 95. data. Computer Science Technical 
Report CSD-TR-95-028, [18] DEY,T.K., SUGIHARA,K., AND BAJAJ, C. L. Delaunay trian-Purdue University, 
1995. gulations in three dimensions with .nite precision arithmetic. [6] BAJAJ,C., CHEN,J., AND XU, 
G. Modeling with cubic A-Comput. Aided Geom. Design 9 (1992), 457 470. patches. ACM Transactions on Graphics 
(1995). To Appear. [19] EDELSBRUNNER,H. Algorithms in Combinatorial Geometry, [7] BAJAJ,C., AND IHM,I. 
C1 smoothing of polyhedra with vol. 10 of EATCS Monographs on Theoretical Computer Sci­implicit algebraic 
splines. Computer Graphics 26,2 (July ence. Springer-Verlag, Heidelberg, West Germany, 1987. 1992), 79 
88. Proceedings of SIGGRAPH 92. [20] EDELSBRUNNER,H., KIRKPATRICK,D., AND SEIDEL,R. On [8] BAJAJ,C., 
AND XU, G. Modeling scattered function data on the shape of a set of points in the plane. IEEE Trans. 
on curved surfaces. In Fundamentals of Computer Graphics, Information Theory 29, 4 (1983), 551 559. Z. 
T. J. Chen, N. Thalmann and D. Thalmann, Eds. Beijing, [21] EDELSBRUNNER,H., AND MUCKE, E. P. Three-dimensional 
China, 1994, pp. 19 29. alpha shapes. ACM Transactions on Graphics 13,1 (Jan. [9] BARNHILL, R. E. Surfaces 
in computer aided geometric de-1994), 43 72. sign: A survey with new results. Computer Aided Geometric 
 [22] EDELSBRUNNER,H., ANDSHAH,N.R.Incrementaltopological Design 2 (1985), 1 17. .ipping works for regular 
triangulations. In Proc. 8th Annu. [10] BARNHILL,R. E., AND FOLEY, T. A. Methods for constructing ACM 
Sympos. Comput. Geom. (1992), pp. 43 52. surfaces on surfaces. In Geometric Modeling: Methods and [23] 
FARIN, G. Triangular Bernstein-B´ezier patches. Computer their Applications, G. Farin, Ed. Springer, 
Berlin, 1991, pp. 1 Aided Geometric Design 3 (1986), 83 127. 15. [24] FARIN,G. Curves and Surfaces for 
Computer Aided Geometric [11] BARNHILL,R. E., OPITZ,K., AND POTTMANN,H. Fat sur- Design: A Practical 
Guide. Academic Press, 1990. faces: a trivariate approach to triangle-based interpolation on surfaces. 
Computer Aided Geometric Design 9 (1992), 365 [25] FAUGERAS,O. D., HEBERT,M., MUSSI,P., AND BOISSONNAT, 
 378. J. D. Polyhedral approximation of 3-D objects without holes. Computer Vision, Graphics and Image 
Processing 25 (1984), [12] BARNHILL,R. E., PIPER,B. R., AND RESCORLA, K. L. Interpo­169 183. lation to 
arbitrary data on a surface. In Geometric Modeling, G. Farin, Ed. SIAM, Philadelphia, 1987, pp. 281 
289. [26] FRANKE, R. Recent advances in the approximation of sur­faces from scattered data. In Multivariate 
Approximation, [13] BOISSONAT, J. D. Geometric structures for three-dimensional C.K.Chui, L.L.Schumarker, 
and F.I.Utreras, Eds. Academic shape representation. ACM Transactions on Graphics 3,4 Press, New York, 
1987, pp. 275 335. (Oct. 1984), 266 286. [27] GUO, B. Surface generation using implicit cubics. In Scienti.c 
[14] CLARKSON, K. L. A randomized algorithm for closest-point Visualizaton of Physical Phenomena, N. 
M. Patrikalakis, Ed. queries. SIAM J. Comput. 17 (1988), 830 847. Springer-Verlag, Tokyo, 1991, pp. 485 
530. [15] DAHMEN, W. Smooth piecewise quadratic surfaces. In Mathe­[28] GUO, B. Non-splitting macro patches 
for implicit cubic spline matical Methods in Computer Aided Geometric Design,T. Ly­ surfaces. Computer 
Graphics Forum 12, 3 (1993), 434 445. che and L. Schumaker, Eds. Academic Press, Boston, 1989, pp. 181 
193. [29] HOPPE,H., DEROSE,T., DUCHAMP,T., HALSTEAD,M., JIN, H., MCDONALD,J., SCHWITZER,J., AND STUELZLE, 
W. Piece­wise smooth surface reconstruction. In Computer Graphics Proceedings (1994), Annual Conference 
Series. Proceedings of SIGGRAPH 94, ACM SIGGRAPH, pp. 295 302. [30] HOPPE,H., DEROSE,T., DUCHAMP,T., 
MCDONALD,J., AND STUELZLE, W. Surface reconstruction from unorganized points. Computer Graphics 26, 2 
(July 1992), 71 78. Pro­ceedings of SIGGRAPH 92. [31] HOPPE,H., DEROSE,T., DUCHAMP,T., MCDONALD,J., AND 
STUELZLE, W. Mesh optimization. In Computer Graphics Proceedings (1993), Annual Conference Series. Proceedings 
of SIGGRAPH 93, ACM SIGGRAPH, pp. 19 26. [32] MOORE,D., AND WARREN, J. Approximation of dense scat­tered 
data using algebraic surfaces. In Proceedings of the 24th annual Hawaii International Conference on System 
Sciences (1991), V. Milutinovic and B. D. Shriver, Eds., vol. 1. [33] NIELSON, G. M. Modeling and visualizing 
volumetric and surface-on-surface data. In Focus on Scienti.c Visualization, H. Hagen, H. Muller, and 
G. M. Nielson, Eds. Springer, 1992, pp. 219 274. [34] NIELSON, G. M. Scattered data modeling. IEEE Computer 
Graphics &#38; Applications 13 (1993), 60 70. [35] NIELSON,G. M., FOLEY,T., LANE,D., FRANKE,R., AND HAGEN, 
H. Interpolation of scattered data on closed surfaces. Computer Aided Geometric Design 7, 4 (1990), 303 
312. [36] NIELSON,G. M., FOLEY,T. A., HAMANN,B., AND LANE,D. Visualizing and modeling scattered multivariate 
data. IEEE Computer Graphics &#38; Applications 11, 3 (May 1991), 47 55. [37] NIELSON,G. M., AND FRANKE, 
R. Scattered data interpola­tion and applications: A tutorial and survey. In Geometric Modeling: Methods 
and Their Applications, H. Hagen and D. Roller, Eds. Springer, 1990, pp. 131 160. [38] O ROURKE, J. Polyhedra 
of minimal area as 3D object models. In Proc. of the International Joint Conference on Arti.cial Intelligence 
(1981), pp. 664 666. [39] POTTMANN, H. Interpolation on surfaces using minimum norm networks. Computer 
Aided Geometric Design 9 (1992), 51 67. [40] PREPARATA,F.P., AND SHAMOS,M. I. Computational Ge­ometry: 
an Introduction. Springer-Verlag, New York, NY, 1985. [41] RESCORLA,K. C1 trivariate polynomial interpolation. 
Com­puter Aided Geometric Design 4 (1987), 237 244. [42] TURK,G., AND LEVOY, M. Zippered polygonal meshes 
from range images. In Computer Graphics Proceedings (1994), Annual Conference Series. Proceedings of 
SIGGRAPH 94, ACM SIGGRAPH, pp. 311 318. [43] VELTKAMP, R. C. 3D computational morphology. Computer Graphics 
Forum 12, 3 (1993), 115 127. [44] WORSEY,A., AND FARIN, G. An n-dimensional clough-tocher interpolant. 
Constructive Approximation 3, 2 (1987), 99 110.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218427</article_id>
		<sort_key>119</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[Creation and rendering of realistic trees]]></title>
		<page_from>119</page_from>
		<page_to>128</page_to>
		<doi_number>10.1145/218380.218427</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218427</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31098907</person_id>
				<author_profile_id><![CDATA[81100649819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Weber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Teletronics International, Inc., and Dynamics Research Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P149346</person_id>
				<author_profile_id><![CDATA[81332520565]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Penn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Army Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Alonso, E. Finn. Physics. Addison-Wesley, Reading, Massachusetts, 1970, pp. 160-166.]]></ref_text>
				<ref_id>ALON70</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Aono, T. Kunii. Botanical Tree Image Generation. IEEE Computer Graphics and Applications. May, 1984, pp.10-34, Volume 4, No.5.]]></ref_text>
				<ref_id>AONO84</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325249</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal. Modeling the Mighty Maple. Proceedings of SIGGRAPH '85 (San Francisco, California, July 22-26, 1985). In Computer Graphics Proceedings, Annual Conference Series, 1985, ACM SIGGRAPH, pp. 305-311.]]></ref_text>
				<ref_id>BLOO85</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. Chandler, A. Cook, G. DeWolf, G Jones, K Widin. Taylor's Guide to Trees. Houghton Mifflin Company, Boston, 1988.]]></ref_text>
				<ref_id>CHND88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[F. Chan, F. Ching, W Collins, M. Evans, W. Flemer, J. Ford, F. Galle, R Harris, R. Korbobo, F. Lang, F. Mackaness, B. Mulligan, R. Ticknor. Trees. The American Horticultural Society, Mount Vernon, Virginia, 1982.]]></ref_text>
				<ref_id>CHAN82</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[G. Collingwood, W. Brush. Knowing Your Trees. The American Forestry Association, Washington, DC., 1974.]]></ref_text>
				<ref_id>COLL74</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Foley, A. vanDam, S. Feiner, J. Hughes. Computer Graphics, Principles and Practice, Second Edition. Addison- Wesley, Reading, Massachusetts, 1992.]]></ref_text>
				<ref_id>FOLE92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[R. Floyd, L. Steinburg. An Adaptive Algorithm for Spatial Grey Scale, Proceedings SID. 1976, pp. 75-77.]]></ref_text>
				<ref_id>FLOY76</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D. Halliday, R, Resnick. Fundamentals of Physics, 3rd Ed. J. Wiley &amp; Sons, New York, 1988, pp. 306-322.]]></ref_text>
				<ref_id>HALL88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[E. Haustein. The Cactus Handbook. Hamlin, London, 1991.]]></ref_text>
				<ref_id>HAUS91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Den Hartog. Strength of Materials. Dover, Mineola, 1977, pp. 79-88.]]></ref_text>
				<ref_id>HART77</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[H. Honda. Description of the Form of Trees by the Parameters of the Tree-like Body: Effects of the Branching Angle and the Branch Length on the Shape of the Tree-like Body. Journal of Theoretical Biology. 1971, pp. 331-338.]]></ref_text>
				<ref_id>HOND71</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[H. Honda, P. Tomlinson, J. Fisher. Computer Simulation of Branch Interaction and Regulation by Unequal Flow Rates in Botanical Trees. American Journal of Botany. 1981, pp. 569-585.]]></ref_text>
				<ref_id>HOND81</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Lindenmayer. Mathematical Models for Cellular Interactions in Development, I&amp;II. Journal of Theoretical Biology. 1968, pp. 280-315.]]></ref_text>
				<ref_id>LIND68</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15892</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[P. Oppenheimer. Real Time Design and Animation of Fractal Plants and Trees. Proceedings of SIGGRAPH '86 (Dallas, Texas, August 18-22, 1986). In Computer Graphics Proceedings, Annual Conference Series, 1986, ACM SIGGRAPH, pp. 55-64.]]></ref_text>
				<ref_id>OPPE86</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Page. Planet Earth: Forest. Time-Life Books, Alexandria, Virginia, 1983.]]></ref_text>
				<ref_id>PAGE93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[P. Prusinkiewicz, A. Lindenmayer. The Algorithmic Beauty of Plants. Springer-Verlag, New York, 1990.]]></ref_text>
				<ref_id>PRUS90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192254</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[P. Prusinkiewicz, M. James, R. M6ch. Synthetic Topiary. Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 351-358.]]></ref_text>
				<ref_id>PRUS94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[W. Reeves. Approximate and Probabilistic Algorithms for Shading and Rendering Structured Particle Systems. Proceedings of SIGGRAPH '85 (San Francisco, California, July 22-26, 1985). In Computer Graphics Proceedings, Annual Conference Series, 1985, ACM SIGGRAPH, pp. 313- 322.]]></ref_text>
				<ref_id>REEV85</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378505</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[P. de Reffye, C. Edelin, J. Fran~on, M. Jaeger, C. Puech. Plant models faithful to botanical structure and development. Proceedings of SIGGRAPH 88 (Atlanta, Georgia, August 1-5, 1988). In Computer Graphics Proceedings, Annual Conference Series, 1988, ACM SIGGRAPH, pp. 151-158.]]></ref_text>
				<ref_id>REFF88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A. Reilly. The Secrets of Trees. Gallery Books, New York, 1991.]]></ref_text>
				<ref_id>REIL91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Rohlf, J. Helman. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics. Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994). In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM SIGGRAPH, pp. 381-394, specifically Figures 14 and 17 on page 394.]]></ref_text>
				<ref_id>ROHL94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[G. Symonds. The Tree Identification Book. William Morrow &amp; Company, New York, 1958.]]></ref_text>
				<ref_id>SYMO58</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[E. Tootill, S. Blackmore. The Facts on File Dictionary of Botany. Market House Books LTD, Aylesbury, UK, 1984, p.155.]]></ref_text>
				<ref_id>TOOT84</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creation and Rendering of Realistic Trees Jason Weber1 Teletronics International, Inc. "From such small 
beginnings - a mere grain of dust, as it were - do mighty trees take their rise." Henry David Thoreau 
from "Faith in a Seed" ABSTRACT Recent advances in computer graphics have produced images approaching 
the elusive goal of photorealism. Since many natural objects are so complex and detailed, they are often 
not rendered with convincing fidelity due to the difficulties in succinctly defining and efficiently 
rendering their geometry. With the increased demand of future simulation and virtual reality applications, 
the production of realistic natural-looking background objects will become increasingly more important. 
We present a model to create and render trees. Our emphasis is on the overall geometrical structure of 
the tree and not a strict adherence to botanical principles. Since the model must be utilized by general 
users, it does not require any knowledge beyond the principles of basic geometry. We also explain a method 
to seamlessly degrade the tree geometry at long ranges to optimize the drawing of large quantities of 
trees in forested areas.  INTRODUCTION Historically, much of the effort in computer graphics has been 
directed toward rendering precisely defined geometrical shapes such as manufactured objects whose geometry 
must be clear-cut and well-defined. CAD tools that are often used to design these objects can also be 
used to specify the geometrical properties in terms of simpler surfaces or solid geometric primitives. 
The complexity of many objects is generally low enough to allow complex lighting and ray-tracing computations 
that approach photorealism. Natural objects offer a more profound challenge. A hillside may contain hundreds 
of trees, billions of grass blades, and countless rocks, pebbles, and ground variations. Each tree may 
easily be characterized by hundreds of thousands of leaves and thousands of branches, branchlets, and 
stems oriented in complex directions. A complex landscape could require an unimaginable large number 
of polygons to define every minute facet. As a result, complex natural backgrounds containing vegetation 
are often neglected in high quality image generation and scene simulation because of the difficulty of 
properly defining and rendering them in a reasonable time. Emphasis is placed on the buildings, vehicles, 
and assorted manufactured objects that are often the focus of the dominant action in a scene. Because 
of speed requirements, two-dimensional texture-mapped trees drawn as rotating billboards are common today 
in many real-time applications, but their appearance can be objectionable. This is especially evident 
when a viewer is in motion. See [ROHL94] for examples of 2D trees. As simulations become more realistic, 
the deficiencies in the background objects become more apparent. We present a model to create and render 
trees. In designing this model, we have set guidelines focused on the requirements of scene simulation. 
The foremost requirement is the appropriate level of resolution and quality. For items to appear realistic 
in a dynamic simulation, the viewer must get the proper sense of rotational as well as translational 
motion when passing or circling objects. Realism also depends on the accuracy of textural effects due 
to leaves and branches within the tree shadowing each other at various times of the day. Therefore, all 
trees must be three-dimensional. Fortunately, as background objects, trees would rarely be taller than 
5 to 20 percent of screen height. Therefore, fine details such as leaf curvature and vein structure are 
not important. But, a tree's branch structure must be very accurate at this resolution. Leaves do not 
completely conceal the underlying branches of dormant or sparsely foliated trees. 1 weber@teleport.com, 
now employed at Dynamics Research Corporation 2 joseph@belvior-arl-irisgt.army.mil Permission to make 
digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage, the copyright notice, the 
title of the publication and its date appear, and notice is given that copying is by permission of ACM, 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Joseph Penn2 Army Research 
Laboratory The model must be capable of creating a wide variety of actual tree types and related vegetation 
such as shrubs, bushes, and palms, as well as cacti and even large grasses. Shrubs, for example, can 
be easily simulated with the model since they really only differ from trees in that they are usually 
shorter and have multiple trunks originating directly from the ground [REIL91]. The model must be able 
to handle random parameters so that a very large number of structural variations can be generated from 
the design specifications of a particular tree species. It should also implement time­dependent oscillations 
due to wind and other perturbations. Use of the model must be understandable by a common user with only 
a general knowledge of basic geometry, such as directly observable angles and lengths. This excludes 
the use of any model parameters requiring understanding of difficult principles such as differential 
equations. Likewise, the model must be stable and easy to use. User-entered free-form equations could 
easily cause unpredictable behavior. Aspects of the model that may be difficult to control should be 
isolated from the user and be represented by intuitive parameters. However, the model should not be constrained 
in a way that interferes with the user's freedom of design. The specification for the tree must be compact 
and be able to recreate and render the tree geometry efficiently. This includes the ability to degrade 
geometry to low resolution at long ranges, where increased speed is necessary to render large forested 
areas. Any degradation must use negligible overhead and be seamless, even in dynamic simulations where 
ranges to trees are continuously changing. This model was designed to successfully meet these criteria. 
We demonstrate the tree model in our natural environment scene generator. We have developed a compact 
but varied library of specification files for generating trees that are used in simulating a wide variety 
of landscapes. The following section briefly discusses other tree models. The third section gives an 
overview of our observations of trees. The fourth section goes into the specific details and equations 
explaining how our parameters are used to create the geometric description. The fifth section explains 
our method of drawing optimally-degraded instances of the trees at longer ranges. The sixth section is 
a very short description of our project and how we use the trees in our application. An appendix includes 
a listing of our parameters and four sample tree specifications. 2 PREVIOUS MODELS We will make some 
comparisons and contrasts to other tree models here and throughout the paper. We cannot fully explain 
the previous work in this space and will direct the reader to definitive references. Honda introduced 
a model using parameters to define the skeleton of a tree [HOND71]. He clearly illustrated the difference 
between the monopodial and dichotomous branching. In dichotomous branching, the branches tend to split 
apart in different directions from the original. Monopodial branching tends to act similarly except that 
one branch continues inline from the original. Honda assumes that monopodial branching is a special case 
concerning structures that are parallel to the line of gravity. Lindenmayer introduced a string rewriting 
system [LIND68] for cellular interaction commonly called the L-system. This is later applied to plants 
and trees and is extensively described in his book with Prusinkiewicz [PRUS90] which describes the system 
with a few extensions, such as allowing for context-sensitivity and random variations. Basically, the 
string starts with a seed of a single character. A set of rules defines how to substitute characters 
during an iteration of rewriting. Presumably, any one character may be converted into several characters. 
This process is continued iteratively and the string grows. After a designated number of iterations, 
these strings can be interpreted as geometric commands. Rules can be selected to produce the monopodial 
or dichotomous branching, as desired. Aono and Kunii stated that the L-system was not capable of producing 
complex three-dimensional patterns of branching [AONO84]. They demonstrated their models which also introduced 
interesting features such as attraction, inhibition, and statistical variations of angles. They made 
a detailed evaluation of the arrangement of branches or leaves on a parent stem. Prusinkiewicz and Lindenmayer's 
book, printed later, argued that Aono and Kunii's rejection of the L-system was no longer justified based 
on their recent improvements. Oppenheimer used fractals to form trees. He used parameters such as branching 
angle, branch-to-parent size ratio, stem taper rates, helical twist, and branches per stem segment. These 
specifications resemble our approach. The Oppenheimer model, however, following the fractal theory of 
self-similarity, uses the same specifications for every recursive level. He introduces random variations 
to alleviate some of the self-similarity [OPPE86]. We believe the self-similarity of fractals to be an 
unnecessary constraint that limits models to a relatively small number of basic trees. Oppenheimer's 
images appear to be influenced by Bloomenthal whose paper concentrated on the quality of the surface 
geometry assuming that a reasonable tree skeleton exists [BLOO85]. Bloomenthal splined between points 
on the skeleton and used a ramiform to represent branch splitting. He also used a bark texture map created 
from a digitized x-ray of a plaster cast. However, such detail is only useful when the tree is viewed 
at very close ranges. Reeves and Blau created trees and grasses by utilizing a particle system [REEV85]. 
They primarily emphasized the forest environment instead of concentrating on the structural detail of 
individual plants. In addition, they decided to focus more on the visual results than the specific details 
of actual botanical data. For our application, we followed similar guidelines. De Reffye et al. have 
had impressive results with a strict botanical model [REFF88]. Their system models growth to a certain 
age using probabilities of death, pause, ramification, and reiteration. They admit that it takes a considerable 
knowledge of both botany and of their model to create images with great fidelity to nature. Since all 
the models strive to achieve the same result, realistic trees, they will all have some characteristics 
in common. Although our model does not draw from any of the previous models, comparisons will be made 
for the benefit of the reader. APPROACH / OVERVIEW We visualize the structure of a tree as a primary 
trunk consisting of a variably curved structure similar to a cone. In some trees, this single structure 
may split multiple times along its length, forming additional similarly curved structures which can likewise 
split along their length [CHND88]. This is how we visualize dichotomous branching. The attributes of 
these "clones" closely match that of the remaining length of their twin, except that they are generated 
using different random seeds. After splitting, some will tend to curve more to compensate for the directional 
change caused by the splitting angle. Monopodial or "child" branches are formed from the trunk and any 
existing clones. These branches can have entirely different attributes from their "parents". Many attributes, 
such as length, are defined relative to the corresponding attribute of their parents. For example, a 
child branch's length is specified as a fraction of its parent's length. These branches, themselves, 
can have sub-branches and so on. For the resolution requirements of simulation, these levels of recursion 
can be generally be limited to three or four. It is important to point out that nearly all of the other 
models consider each branching, whether monopodial or dichotomous, to be discrete levels. They often 
require nine or ten of these levels. While this is primarily convention, it will be significant in optimized 
rendering (Section 5). Also, branch level control can assist in designing a tree. We usually begin by 
deactivating the rendering of all levels but the first (the trunk). Once the trunk's appearance is acceptable, 
we activate and design the second level, and so on, ascending degrees of complexity to the third and 
fourth levels. This allows us to view the general shape and structure of the tree without the visual 
confusion and performance loss due to drawing minor branches and leaves. In many cases, foliated trees 
can be drawn reasonably well in a final rendering without displaying any of the minor branches. Specific 
trees appear to form particular shapes [CHND88,CHAN82]. These shapes are usually the result of the lengths 
of the primary branches according to their position on the trunk of the tree, e.g., a conically shaped 
tree has larger main branches near the base of the trunk. Alternatively, it is sometimes easier to define 
the general shape of the crown by envisioning an invisible envelope around the tree which inhibits growth 
of branches. In addition, many trees have branches that show a preference to curve towards a vertical 
direction, either up or down, presumably responding to the competing influences of light and gravity. 
Cross-sectional variations can be particularly noticeable in the trunk. The scale of the cross-section 
does not necessarily taper linearly as with a perfect cone. Some cacti can even have periodic scaling 
in addition to simple random variations. The radial distance about any particular cross­section can also 
vary randomly and/or periodically. In addition, the radius of the trunk clearly flares at the base of 
the many trees. Wind causes complex oscillatory motion throughout the tree that varies in amplitude and 
frequency determined by the length and thickness of the trunk and branches. These are the characteristics 
we have observed and incorporated into our model. We model enough of the significant effects that a great 
variety of trees and related objects can be incorporated into any simulation that requires natural environments. 
Plate 1 shows twenty-four trees rendered with the model. 4 TREE CREATION The appendix lists most of 
the parameters currently used by our tree model. It will be used for reference throughout this paper. 
For the benefit of readers who wish to experiment with the demo program, the intuitive multi­character 
variable names used in the parameter files will also be used in the equations throughout this paper. 
We should stress that many of these parameters have standard botanical names which we have neglected. 
We are not trying to create a new convention, but merely attempting to clarify the meanings of the parameters 
using simple geometric names recognized by our potential end users. Many of the parameters are repeated 
for each level of recursion to permit greater control and flexibility. Additional parameters, mostly 
dealing with seasonal color and lighting properties, are not listed and will not be discussed. The parameters 
are referred to in the text by name and appear as bold italic, as in Shape. Where necessary, parameters 
are prefixed by a number that distinguishes similar parameters at different levels of recursion. Generalized 
parameters can appear in the text with an n prefix, such as nTaper referring to 0Taper, 1Taper, 2Taper 
and 3Taper. This refers non-specifically to any of the like parameters. Many parameters are followed 
by a variation parameter with the same name and a 'V' suffix, such as nLength and nLengthV. The variations 
are usually positive numbers indicating the magnitude of variation about the previous parameter. However, 
since a few special trees, like palms, require exceptions to common trends [REIL91], some parameters 
use the negative sign as a flag to activate a special mode. All angular parameters are specified in degrees. 
Likewise, angles in the equations are in degrees, unless otherwise stated. Except where noted, our equations 
describe structures based on our physical observations and research in tree reference manuals (see References). 
Additionally, four trees parameter lists are given for comparison in Appendix. These specifications were 
designed using photographs in tree reference manuals. These trees, Quaking Aspen, Black Tupelo, Weeping 
Willow, and California Black Oak, can be seen in Plates 1q, 2, 5, and 1a, respectively. As trees vary 
widely and can be hard to identify even by experts [SYMO58], these specific definitions could be used 
to represent many different species of trees. Figure 1 is a diagram demonstrating some of the parameters. 
It does not show a complete tree, but rather exaggerates certain components to clarify their construction. 
4.1 The Curved Stem Our model is based on two elements, the stem and the leaf. We will use the generic 
term "stem" to refer to the trunk or branches at any level of recursion. The unit stem is a narrow near-conical 
tube whose relative z-axis is coincident with its central axis. Note that each stem has it own relative 
coordinate system. For a main branch whose z-axis points out perpendicularly to the trunk's z-axis, the 
branch's y-axis points up toward the sky and its x-axis points parallel to the ground surface, according 
to the right-hand rule. The tube of a stem at a recursive level n is divided into a number of near-cylindrical 
segments defined by nCurveRes. Each segment is stored as a nearly-circular cross-section. These cross-sections 
are later connected together to draw a triangular mesh. If nCurveBack is zero, the z-axis of each segment 
on the stem is rotated away from z-axis of the previous segment by (nCurve/nCurveRes ) degrees about 
its x-axis. If nCurveBack is non-zero, each of the segments in the first half of the stem is rotated 
( nCurve/(nCurveRes/2) ) degrees and each in the second half is rotated ( nCurveBack/(nCurveRes/2) ) 
degrees. This two part curve allows for simple S-shaped stems. In either case, a random rotation of magnitude 
(nCurveV/nCurveRes ) is also added for each segment. A special mode is used when nCurveV is negative. 
In that case, the stem is formed as a helix. The declination angle is specified by the magnitude of nCurveVary. 
 4.2 Stem Splits A stem generally extends out to the periphery of the tree, potentially splitting off 
cloned stems along its length. A cloned stem is considered at the same recursive level as its twin and 
inherits all of its properties. The frequency of splitting is defined by nSegSplits. This is the number 
of new clones added for each segment along the stem and is usually between 0 and 1, with 1 referring 
to a dichotomous split on every segment. A value of 2 would indicate a ternary split. There is no pre-determined 
limit to the number of splits per segment; but, since each clone can also generate its own clones at 
the next segment, the resulting number of stems can easily reach undesirable levels. For instance, with 
a nCurveRes of 5 and nSegSplits of 2, one stem will eventually split off into 81 separate clones: (nSegSplits+1)nCurveRes-1=34. 
Note in the top center diagram in Figure 1 where a tree has 0SegSplits of 1 and 0CurveRes of 3. The resulting 
splitting results in a trunk with four total stems: (1+1)3-1 = 4. There is an additional parameter nBaseSplits 
that specifies the equivalent of nSegSplits at the end of the first segment of the trunk. This allows 
for an independent number of splits at the base of the tree, thus permitting trees that seem to have 
multiple trunks with few further splitting tendencies. Fractional values of nSegSplits will cause additional 
splits to be evenly distributed throughout all segments of all stems in that particular level of recursion. 
For example, an nSegSplits of 1.2 will form one clone on 80% of the level n segments and two clones on 
20% of the segments. Note that this yields an average number of 1.2 splits per segment. Using random 
numbers simplistically to distribute the fractional part of nSegSplits is unacceptable because when, 
by chance, several consecutive segments all get the extra split, they can form an unnaturally large number 
of stems in close proximity on part of the tree. To evenly distribute the splits, we use a technique 
similar to Floyd-Steinburg Error Diffusion [FLOY76]. For each recursive level, a global value holds an 
"error value" initialized to 0.0. Each time nSegSplits is used, this error is added to create a SegSplitseffective 
which is rounded to the nearest integer. The difference (SegSplitseffective -nSegSplits) is subtracted 
from the error. So, if a value is rounded up, it is more likely that the next value will be rounded down 
(and vice versa). 0SplitAngle  Figure 1: Tree Diagram If there are any clones, then the z-axes of the 
stem and its clones each rotate away from the z-axis of the previous segment by anglesplit = (nSplitAngle±nSplitAngleV 
) - declination limited to a minimum of 0, where the "declination" angle (defined here as the angle of 
a stem from the tree's positive z-axis) can be found by taking the inverse cosine of the z component 
of a unit z vector passed through the current matrix transformation of the relative coordinate system. 
The first clone continues the original mesh and cannot rotate around the z-axis or it would twist the 
mesh (i.e., if one rotated one of the circular faces on a cylinder about the longitudinal axis, the resulting 
section of geometry would render as a hourglass shape). This anglesplit is later distributed over the 
lengthchild = lengthchild,max * ( lengthparent - 0.6 * offsetchild) for further levels of branches, where 
lengthbase is the fractional bare area at the base of the tree calculated as (BaseSize*scaletree ) and 
scaletree defined as (Scale±ScaleV) in meters . The trunk has no parent, so its length is defined by 
lengthtrunk = ( 0Length ± 0LengthV )*scaletree If nDownAngleV is positive, the z-axis of a child rotates 
away from the z­axis of its parent about the x-axis at an angle of (nDownAngle±nDownAngleV). However, 
if nDownAngleV is negative, the variation is distributed along the height of the tree by downanglechild 
= nDownAngle ± [ nDownAngleV * ( 1 - 2 * ShapeRatio( 0, (lengthparent - offsetchild) / (lengthparent 
- lengthbase) ) ) ] This can be used to linearly change the down angle based on the position of the child 
along its parent, as with the Black Tupelo's main branches seen in Plate 2b. Note how they are angled 
upward near the crown of the tree and angled downward near the bottom. If nRotate is positive, each child 
formed along the parent is placed in a helical distribution by rotating about the z axis of its parent 
relative to the previous child by the angle (nRotate±nRotateV). In the special case where nRotate is 
negative, each child is rotated about its parent's z-axis relative to its parent's y-axis by the angle 
(180+nRotate±nRotateV) on alternating sides of the parent branch. This allows for a nearly coplanar child 
stem distribution. Since the y-axis of any stem with a small downangle points back toward its parent, 
the planar distribution from such a stem is aligned with that parent. This makes it easy to design trees 
where sub-branches tend to spawn parallel to the ground surface. This effect is most obvious in the tree 
shown in Plate 1v. Aono and Kunii go into detail about the proper divergence and branching angle [AONO84]. 
These correspond to our rotation and down angles, respectively. They note the Schimper-Braun law which 
states that this divergence angle is a fraction of 360 degrees based on a Fibonacci sequence of 1/2, 
1/3, 2/5, 3/8, ... , resulting in possible angles of 180, 120, 144, 135, and so on. Our results show 
that any number near 140 degrees works well in most situations. Aono and Kunii also note that the branching 
angle (our down angle) appears to be smaller for branches that form later as the tree matures. De Reffye 
attributes this to gravity affecting the increased mass of older branches and simulates the effect including 
elastic curvature using Young's modulus [REFF88]. The change in the branching angle can result in large 
angles at the base of the tree and smaller angles along the height of the tree. We implement this linearly 
with the negative nDownAngleV as noted above. However, Aono and Kunii state that changing their model 
to implement this effect does not add much realism. We find the effect, as implemented in our model, 
to be very substantial, especially in dormant or sparsely foliated trees.  4.4 Stem Radius For all levels 
except the trunk, the radius at the base of a stem is defined as a function of the radius of its parent 
stem. The trunk's radius is proportional to the scale of the entire tree. radiustrunk = lengthtrunk * 
Ratio * 0Scale trunk )RatioPower radiuschild = radiusparent * ( lengthchild / lengthparentbranches The 
maximum radius of a stem is explicitly limited to the radius of the parent at the point from which it 
was spawned. The radius of the stem can be tapered along its length. In the simplest form, this can be 
used to render the stem as a bent cone. However, there are other variations that allow for other cases 
according to the following chart: nTaper Effect 0 Non-tapering cylinder 1 Taper to a point (cone) 2 Taper 
to a spherical end 3 Periodic tapering (concatenated spheres) Any fractional value from 0 to 3 is permitted 
to allow adjustment for a desired effect. The periodic tapering can be seen in the cactus of Plate 1(L) 
which has an 0Taper of 2.2. For a normalized position Z from 0 to 1 along the length of a stem, the following 
equations compute radiusZ, the tapered radius in meters: unit_taper = nTaper 0 = nTaper < 1 unit_taper 
= 2 -nTaper 1 = nTaper < 2 unit_taper = 0 2 = nTaper < 3 taperZ = radiusstem * ( 1 - unit_taper * Z ) 
(purely tapered radius) and when 0 = nTaper < 1 radiusZ = taperZ or when 1 = nTaper = 3 Z2 = ( 1 - Z 
) * lengthstem depth = 1 (nTaper < 2) OR ( Z2 < taperZ) depth = nTAPER - 2 otherwise Z3 = Z2 nTaper < 
2 Z3 = |Z2 - 2 * taperZ * integer( Z2 / (2 * taperZ) + 0.5 )| otherwise radiusZ = taperZ (nTaper<2) AND 
( Z3 = taperZ) radiusZ = (1-depth) * taperZ + depth * sqrt( taperZ2 - (Z3 - taperZ)2 ) otherwise where 
'depth' is a scaling factor used for the periodic tapering. This periodic tapering is useful for some 
cacti, where annual growth can accumulate in segments [HAUS91]. Similarly, it can be used as a rough 
approximation of the scales on palm trees. In addition to tapering, the trunk may also vary its radius 
by other means. Flaring creates an exponential expansion near the base of the trunk. At the unit position 
Z from 0 to 1 along the length of a stem, the following flareZ scales the radiusZ computed above. y = 
1 - 8 * Z flareZ = Flare * ( 100y - 1 ) / 100 +1 where the value of y is limited to a minimum of zero. 
Note that this equation scales the radius by a minimum of 1 and a maximum of about (1 + Flare). The trunk 
can also have an irregular non-circular cross-section. This can be very apparent in the large supporting 
"knees" of cypress trees [REIL91]. These variations are also clearly present on some cacti, which can 
have pronounced ribs or ridges [HAUS91]. Lobes specifies the number of peaks in the radial distance about 
the perimeter. Even numbers can cause obvious symmetry, so odd numbers such as 3, 5, and 7 are preferred. 
The LobeDepth specifies the magnitude of the variations as a fraction of the radius as follows: lobeZ 
= 1.0 + LobeDepth * sin ( Lobes * angle ) given a specific "angle" from the x-axis about the z-axis. 
Note that a LobeDepth of zero effectively turns lobing off. The lobeZ value cumulatively scales radiusZ 
as did flareZ. Finally, a simple scaling factor (0Scale±0ScaleV ) can also be applied to the trunk. Bloomenthal 
modeled this flaring and lobing using an "equipotential curve surrounding the points of intersection 
of the tree skeleton with the plane of the contour" [BLOO85], essentially the blended circumference of 
several circles moving further away from the center of the trunk near the base of the tree. 4.5 Leaves 
The recursive proliferation of children is limited by Levels. This specifies the maximum level of stems 
that will be created starting from 0 for the trunk, usually to 3 or 4. If Leaves is non-zero, then leaves 
are used as the last level of recursion. The leaves use the nDownAngle, nDownAngleV, nRotate, and nRotateV 
from the that level of recursion. Any leaves or stems beyond level 3 will simply use the parameters of 
level 3. Our most common configuration is to set Levels to 3 and Leaves to a non-zero value which would 
give you the following levels of recursion: trunk (0), branches (1), sub-branches (2), and leaves (3). 
Some trees, such as the weeping willow, require sub-sub-branches as well. Leaves specifies the density 
in the same manner as nBranches did for stems. As with stems, the actual density used is also dependent 
upon other factors such as the length of the parent branch relative to the maximum length for the parent's 
level. Given that the leaves are at the second level of recursion or further, the following density is 
used: leaves_per_branch = Leaves * ShapeRatio( 4 (tapered), offsetchild/lengthparent ) * quality given 
a quality factor supplied by the parent program that is usually near 1. This quality factor is also used 
to scale the leaves to maintain consistent coverage. This distribution of leaves has the natural effect 
of preferentially placing leaves near the outside of the tree. For a negative value of Leaves, a special 
mode is used in which the leaves are placed in a fan at the end of the parent stem, as with some palm 
fronds. The angle over which the leaves fan out is specified by nRotate. Note that when in fan mode, 
nRotate is not needed for its original purpose. A negative value can also be applied to nBranches with 
similar results, but we have not yet modeled any trees requiring this attribute. We realize that these 
negative flags can become a bit confusing, but they are only used in a few special cases. Many users 
will never need them. Leaves can assume many different shapes [CHND88]. We allow for a few common geometries 
of leaves based on LeafShape. This parameter is used as an index to a list of pre-defined leaf shapes, 
such as oval, triangle, 3-lobe oak, 3-lobe maple, 5-lobe maple, any 3 leaflets. Each shape can be sized 
and scaled. For optimum coverage versus computational expense, the oval leaves are most commonly used. 
The pre-defined leaf geometries are stored with unit width and length. They are scaled as they are used 
to create the tree geometry. The length of the leaves, in meters, is determined by [LeafScale / sqrt(quality)]. 
The width, in meters, is determined by [LeafScale*LeafScaleX / sqrt(quality)]. 4.6 Pruning Pruning is 
used to force a tree to fit inside a specific envelope. We originally avoided such a feature since we 
felt that the shape of a tree should proceed from its underlying structure, not from the use of artificial 
boundaries. We now concede that under the influence of certain environmental conditions or to control 
an "uncooperative" tree, pruning can be very useful. Prusinkiewicz demonstrates pruning applied to the 
L-system model [PRUS94]. Essentially, growth of branches is blocked by the edge of the envelope boundary. 
Since the L-system progressively grows connecting nodes, the model can simply hinder growth near a boundary. 
Our model must approach the problem differently. Since our stems often reach from the trunk to the tree's 
outer edges simply chopping off the ends of the offending branches,will make the tree's appearance suffer. 
While this may be the effect from some actual physical pruning, we would rather use pruning as a tool 
to influence the shape of a tree through the underlying structure. To do this, every stem must adjust 
its length to fit inside the envelope. Each stem must "know this new length" before it spawns any children 
since each child's length is dependent on its parent's length. Generally, the child branches are recursively 
spawned during the formation of the parent's segment from which they grow. This is necessary since the 
child must use a geometric transformation relative to the transformation of that segment. At that point, 
the ultimate extent of the parent is not known, so there must first be a non-recursive probing pass for 
each stem to measure and rescale its length and then a fully-recursive second pass to actually form the 
geometry and spawn the children. Note that the probing must also that each of the stem's clones fits. 
If any stem or clone punctures the boundary, its length can be iteratively reduced and re-probed until 
it fits. While the model is capable of using any arbitrary envelope such as the topiary dinosaur in Prusinkiewicz's 
paper, the general user should be more comfortable with an easily selected simple envelope. Figure 2 
shows a pruning envelope. scaletree * PruneWidth greater than 1 in this example (concave) less than 1 
in this example (convex) Figure 2: Pruning Diagram The envelope covers a pseudo-ellipsoidal shape with 
a top at scaletree and bottom at (BaseSize*scaletree) in meters. The maximum width of the envelope is 
(PruneWidth*scaletree) in meters. This maximum width occurs at a position along the tree's z-axis as 
specified by PruneWidthPeak. This peak is defined as the distance from the bottom of the envelope as 
a fraction of the total height of the envelope. A PruneWidthPeak of 0.5 would center the peak as with 
a standard ellipsoid. The curvature of the envelope can also be independently controlled above and below 
the peak using PrunePowerHigh and PrunePowerLow, respectively. A power of 1 gives a linear envelope from 
PruneWidth to 0 over the distance from PruneWidthPeak to the top or bottom of the envelope. A power of 
2 gives a rounded concave envelope, while a power of 0.5 gives a rounded convex envelope. To determine 
whether a given transformed point (x,y,z) is inside the envelope, the boolean 'inside' is computed as: 
r = sqrt ( x2 + y2 + z2 ) ratio = ( scaletree - z ) / ( scaletree *(1-BaseSize ) ) inside = [ r / scaletree 
< PruneWidth * ShapeRatio( 8 (envelope), ratio ) ] where ShapeRatio( 8, ratio ) is defined as -PruneWidthPeak 
) ]PrunePowerHigh [ ratio / ( 1 when ratio < 1 -PruneWidthPeak, or [ ( 1 - ratio ) / ( 1 -PruneWidthPeak 
) ]PrunePowerLow when ratio = 1 -PruneWidthPeak. ShapeRatio( 8, ratio ) always returns 0 when ratio is 
not in the range of 0 to 1. The Shape parameter can use this index of 8 for a custom shape even if pruning 
is not turned on. This allows the user to define a shape not covered by the predefined shapes. If Shape 
is 8 and pruning is on, the tree will tend to match the customized shape even before pruning takes place. 
This may cause some strongly curved stems to fall short of the envelope's edge. The effects of pruning 
can be diminished by using the PruneRatio. This defines a weighted average between the unpruned original 
length and the completely pruned length. A PruneRatio of 1 activates full pruning while a PruneRatio 
of 0.0 effectively turns pruning off. Thus with values between 0 and 1, partial pruning can be utilized 
to avoid artificially smooth boundaries. Plate 7 demonstrates the use of the pruning envelope to control 
the weeping willow. 4.7 Wind Sway We model stem bending as the deflection of an elastic rod with a circular 
cross-section fixed on one end. This rod has a uniformly distributed force applied to it. The solution 
of this is a classic problem of mechanics where applying the Myosotis method yields useful solutions 
for the deflection [HART77]. We then consider this rod as a kind of pendulum [HALL88]. The entire system 
is then modeled as the superposition of coupled oscillators whose periods and phase angles differ so 
that the paths of points on a stem are very complex Lissajous figures [ALON70]. These results confirm 
our general observation that light to moderate winds induce trees to move so that branches sway at various 
directions and rates of oscillations. We currently model the oscillatory effects observed for light to 
moderate winds only. In our model, tree movement is simulated by introducing time-variant curvature changes 
to the stem segments. This effect is added to the structural curvature introduced by nCurve and nCurveBack 
causing rotations between segments about both the x and y axes. With wind speeds varying from windspeed 
to (windspeed+ windgust), the sway angles swayx and swayy at unit position Z from 0 to 1 of a segment 
along the length of a stem are computed at any "time" (in seconds) using: a0 = 4 * lengthstem ( 1 - Z 
) / radiusZ (degrees) a1 = windspeed / 50 * a0 (degrees) a2 = windgust / 50 * a0 + a1/2 (degrees) bx 
= sway_offsetx + radiusstem / lengthstem * time/15 (radians) by = sway_offsety + radiusstem / lengthstem 
* time/15 (radians) swayx = [ a1 * sin( bx ) + a2 * sin ( 0.7 * bx ) ] / nCurveRes (degrees) swayy = 
[ a1 * sin( by ) + a2 * sin ( 0.7 * by ) ] / nCurveRes (degrees) The angles sway_offsetx and sway_offsety 
are randomly selected for each stem in the tree. When the wind sway is activated, each tree geometry 
description must be reformed for each frame in an animation to adapt to the new angles. By using the 
same random seed, a specific tree will always have the same basic geometry, perturbed only by the wind-activated 
curvature variations. The angles swayx and swayy cause rotations between segments about the x and y axes, 
respectively. 4.8 Vertical Attraction With even hemispherical illumination (sky shine), tree shoots 
grow upwards because they are negatively geotropic and positively phototropic. An upward growth tendency 
is usually a subtle effect and can be implemented using the declination and orientation of each segment 
in each stem. For sub-branches and beyond, this curving effect is used in addition to the other curvature 
effects. The trunk and main branches do not use these functions since any such effect can be more easily 
controlled through the previous curve parameters. The AttractionUp parameter specifies the upward tendency. 
Zero denotes no effect and negative numbers cause downward drooping as in the Weeping Willow. A magnitude 
of one results in a tendency of each stem to curve just enough so that its last segment points in a vertical 
direction. Higher magnitudes cause stems to curve toward the vertical much sooner. Very high magnitudes 
such as 10 may result in snaking oscillations due to over-correction. This is not necessarily an undesirable 
result since branches on some trees exhibit a distinctly sinusoidal shape characteristic. Once the effects 
of nCurve are introduced to a segment, curve_upsegment is computed for each segment as declination = 
cos-1( transform_zz ) (radians) orientation = cos-1( transform_yz ) (radians) curve_upsegment = AttractionUp 
* declination * cos ( orientation ) / nCurveRes (radians) where transform_zz is the z component of a 
unit z vector passed through the current viewing transformation and transform_yz is the z component of 
a unit y vector passed through the current viewing transformation. This curve_upsegment is added to the 
segment's curvature. 4.9 Leaf Orientation Left alone, the modeled leaves will generally assume seemingly 
random orientations. However, in reality, leaves are oriented to face upwards and outwards, presumably 
to optimize the available direct (sun) and scattered (sky) light. We can use the declination and orientation 
of each leaf to rotate them toward the light. The necessary rotations are computed based on the current 
viewing transformation and are applied prior to creating the leaf into the geometric description. The 
effect, fractionally controlled by "bend", is applied by obtaining the leaf's position (leafx, leafy, 
leafz) and normal (leaf, leaf, leaf) in tree coordinates from the current transform matrix, nxnynz then 
computing the current and desired angles: thetaposition = atan2 ( leafy, leafx ) thetabend = thetaposition 
- atan2 ( leafny, leafnx ) then computing the change: rotatez ( bend * thetabend ) then recomputing declination, 
orientation, and normal vector using new transform: phibend = atan2 ( sqrt( leaf2 + leaf2 ), leaf ) nx 
ny nz rotatez ( -orientation ) rotatex ( bend * phibend ) rotatez ( orientation ) Plate 6 shows the bending 
effect applied to a Sassafras tree. The modified leaf orientations greatly increase the diffusive reflections 
from the tree. The increased variations improve the overall appearance.  5 DEGRADATION AT RANGE A tree 
generated with our algorithm may have on the order of 5000 to 100,000 facets. The detail can be increased 
automatically for even higher resolution images, such as the Weeping Willow in Plate 3 boosted to over 
1 million facets. Currently, a high-end graphics workstation may be capable of only about 50,000 facets 
in real time. The high resolution of the trees is necessary to have an accurate representation at close 
ranges of 10 to 50 meters or in equivalent magnified views of greater ranges, as in narrow fields-of-view. 
However, at long ranges, such as 1000 meters, a much lower resolution tree could be rendered faster with 
little or no loss in apparent quality. At first thought, it may seem useful to form multiple geometric 
descriptions of the same tree at different "levels of detail". At longer ranges, progressively lower 
resolution geometric descriptions would be used. This approach has two problems. First, each instance 
of a tree consumes resources. An average tree's geometric description may use about 1Mb of RAM. Also, 
it may require 1 to 10 seconds to form the data. These numbers become much more significant when multiplied 
by, perhaps, 100 instances. While this could be managed, a more critical second problem arises with the 
quantization of the resolution. In a still picture, the changes between resolutions would not be very 
apparent since the variably resolved trees appear as different trees. However, in a dynamic simulation, 
specific trees would switch from one resolution to the next. This would result in wide "resolution waves" 
flowing through forest canopies. This is unacceptable for realistic simulation. A method is needed that 
uses a single geometric description and renders it at an optimal resolution for any range. The changes 
between the differently resolved geometries must be very fine, preferably corresponding with removal 
or modification of each facet one at a time. There should be negligible overhead (CPU and RAM) involved 
with this reinterpretation of the specified geometry. Since the trees are not arbitrary objects, we can 
fit a range-degradation algorithm to their expected geometry. Each tree geometry is organized into four 
discrete geometric descriptions: 3 stem levels and the leaves. Any stems beyond the third level are grouped 
with the third level. The deeper levels of stems are rarely visible at long ranges and are often obscured 
by the leaves. Oppenheimer recognized that he could use polygonal tubes for large-scale details and vectors 
(lines) for the smaller details [OPPE86]. He warns that artifacts can occur if the "cutover" level is 
not deep enough. He also states that many small branches can be rendered as triangular tubes. Our method 
of rendering makes similar approximations for efficiency. To most efficiently use the CPU and memory, 
our technique does not convert the geometry, it merely re-interprets it. With progressively increasing 
ranges, a tree will re-interpret stem meshes as lines and leaf polygons as points. With longer ranges, 
some individual stems and leaves will disappear altogether. The specific geometry at any range can be 
rendered properly by altering limits and increments in the loops that draw the data. Although we speak 
of removing items one by one, we do not actually mark or delete them. We merely change the loop parameters 
that scan the stored geometry so that items are skipped. Any number of arbitrarily-ranged trees can be 
drawn in any order. The time and space overhead required to compute and hold these boundary limits is 
negligible. A 100,000 facet tree geometry may be rendered at 2 kilometers as about 30 lines and 1000 
points. This allows vast expanses of trees to be drawn very quickly. A viewer can then move close to 
any of these trees and see them at their full resolution. Since the items in each geometric description 
are ordered in the same manner as they were created, they generally start from the bottom of the tree 
and work up. The items are not randomly organized; therefore, we cannot simply remove objects one at 
a time in order from the top or bottom of the list. This could cause the top of the tree to be heavily 
degraded while the bottom remained unchanged, or vice versa. Instead, we group the items of a type of 
geometry into groups of a small size which we will call "masses". The number of elements per mass is 
determined by an appropriate "mass_size". We use a mass_size of 16 for all the stems and 4 for the leaves. 
Curve fitting equations give a value between 0 and mass_size. To explain, we will use the general term 
"primitive" to refer non-specifically to polygons, lines, or points and the general term "item" to refer 
non-specifically to leaves, trunk, branches, or sub-branches. The total number of elements in the geometric 
description of any item is given as "total_numberitem". Of this, we wish to draw a certain fraction of 
these items using a specific primitive. The portion to be drawn is specified by the non-integer "numberprimitive,item", 
which is between 0 and mass_sizeprimitive,item. For example, a mass_sizelines,1 of 16 divides up main 
branch lines into masses of 16. A numberlines,1 of 5 says that for every 16 cross-sections of recursion 
level 1, there should be lines connecting the first five. Fractional numbers will draw an additional 
item for a percentage of the masses. If there were 160 main-branch cross-sections (10 masses) and numberlines,1 
of 5.3, then the first 3 masses would show 6 of 16 lines and the last 7 masses would show 5 of 16 lines. 
A loop to draw the reduced portion of the item using a specific primitive would be: int_numberprimitive,item= 
integer( numberprimitive,item ) massesprimitive,item = total_numberitem / mass_sizeprimitive,item changeprimitive,item 
= massesprimitive,item * ( numberprimitive,item - int_numberprimitive,item ) for mass = 0 to massesprimitive,item 
{ start = mass * mass_sizeprimitive,item end = start + int_numberprimitive,item if mass < changeprimitive,item 
end = end + 1 for index = start to end drawprimitive,item( index ) } To compute the necessary numberprimitive,item, 
we need to first convert the range to a calibrated scale. This adjusts for the current image size and 
vertical field of view. A modified range value, r2, is computed as: r2 = range * 1000 / heightimage * 
Field_Of_Viewy / 60 This compensates for the effect of a telephoto lens that causes a tree to appear 
to be much closer. The following equations outline how numberprimitive,item is computed for different 
levels at different ranges. First, we use the general quality factor supplied by the parent program (usually 
between 0 and 1) to determine some general scaling factors: s = quality / 2 tree is evergreen, or in 
summer and fall s = quality otherwise d = 100 in spring d = 200 otherwise Then, we compute the polygons, 
lines, and points needed for each display item as follows. Level 0 Stems (trunk) r2 <100 don't draw trunk 
lines (can appear as seam) 100 < r2 draw all trunk lines r2 < 300 numberpolygons,0 = mass_sizepolygons,0 
300 < r2 < 800 numberpolygons,0 = mass_sizepolygons,0 * [1.5 - r2/600] 800 < r2 don't draw trunk polygons 
at all Level 1 Stems (main branches) r2 < 200s numberpolygons,1 = mass_sizepolygons,1 * [1.5 - r2/600] 
(bounded 0 to mass_sizepolygons,1) numberlines,1 = mass_sizelines,1 200s < r2 < 2000s don't draw polys 
numberlines,1 = mass_sizelines,1 * [ 2.2 - 1.2 (r2/200s)0.3 ] 2000s < r2 draw nothing for main branches 
Level 2 Stems (other branches) r2 < 50s numberpolygons,2 = mass_sizepolygons,2 numberlines,2 = mass_sizelines,2 
50s < r2 < 100s numberpolygons,2 = mass_sizepolygons,2 * [ 2 - r2/50s] numberlines,2 = mass_sizelines,2 
100s < r2 < 500s don't draw polys numberlines,2 = mass_sizelines,2 * [ 2 - (r2/100s)0.5 ] 500s < r2 draw 
nothing for secondary branches Leaves r2 < d/4 numberpolygons,3 = mass_sizepolys,3 numberpoints,3 = mass_sizepoints,3 
d/4 r2 < d numberpolygons,3 = mass_sizepolys,3 * [ 4/3 - r2 / (3d/4) ] numberpoints,3 = mass_sizepoints,3 
d < r2 numberpolygons,3 = 0 numberpoints,3 = mass_sizepoints,3 * [ 1.5 - r2 / 2d ] (minimum of 1) The 
effects of these equations can be seen in Table 2 which summarizes the total number of triangles, lines, 
and points. Triangles refer to elements of the triangular meshes which comprise the polygons. Item 5m 
30m 60m 120m 240m 600m 1200m Level 0 Triangles 1440 1440 1440 1440 1440 760 0 Level 0 Lines 0 0 0 36 
36 36 36 Level 1 Triangles 960 960 960 0 0 0 0 Level 1 Lines 240 240 240 223 153 35 0 Level 2 Triangles 
17736 14580 0 0 0 0 0 Level 2 Lines 5912 5912 5363 2648 0 0 0 Leaf Triangles 53248 53248 49800 28200 
0 0 0 Leaf Points 13312 13312 13312 13312 11944 1664 1664 Table 2: Number of elements drawn at specific 
ranges in summer on Quaking Aspen Plate 4 shows the Quaking Aspen rendered at the ranges listed in Table 
2, excluding 5 meters, progressively zoomed by powers of two. 6 APPLICATION Our project involves the 
development of software to produce accurate and realistic high resolution imagery in both the visible 
and infrared spectrums. The emphasis is on positioning vehicles in the context of natural environments 
for studies of detection and recognition by both humans and machines. The backgrounds and vehicles must 
be of equally high fidelity to alleviate any bias in the testing. The software utilizes readily available 
elevation maps and creates synthetically-generated shading variations for numerous natural effects. Readily 
available feature maps are usually at a poor resolution and often only describe trees as deciduous, coniferous, 
neither, or both. This is inadequate for our needs. We create our own feature maps from any available 
information such as scenario data (meteorological, topographical, vegetative type and placement, etc.), 
satellite imagery, and photographs, both aerial and at ground level. We describe vegetation using a 16-bit 
raster feature map where 14 bits specify 14 trees or related objects (not mutually-exclusive), and 2 
bits specify 3 types of grass (mutually­exclusive). Any of the 14 trees (from a larger list) and 3 grass 
types can be selected differently for any specific scene. For any one of the 14 selected trees, any number 
of variations can be specified. These variations use the same parameter file, but are generated from 
a different random seed. During rendering, these variations of a tree type are spread randomly over positions 
where the appropriate bit for that tree is set in the vegetation feature map. This can prevent large 
forests of similar tree types from appearing too uniform and self-similar. Other feature maps are used 
for soils, rocks, waterways, and roadways. Currently, we generally resolve elevation and features maps 
at 1 or 2 meters per sample. The grass resembles that of Reeves and Blau's particle grass [REEV85]. In 
our case, the grass is drawn as curved lines composed of Gouraud-shaded segments. Shadowing within the 
trees is produced using a standard shadow map technique [FOLE92]. The shadow map can be used to mark 
which geometric components (polygons, lines, points) will be shadowed before rendering takes place. This 
technique is only valid if each rendered instance of each specific geometric description has the same 
rotational orientation about its z-axis. Otherwise, the shadows would be rotated with the tree. This 
restriction is usually acceptable since each tree selected can have multiple variations, each of which 
can be randomly scaled for each instance. Plate 3 shows various images of a visual simulation from different 
points of view. Plate 3f was made from an altered scenario with higher tree density. A moderate haze 
was applied to the image in Plate 3b. 7 CONCLUSION We have introduced a model based on geometrical observables 
to create and render three-dimensional trees for simulating natural scenery. A wide variety of complex 
realistic tree structures can be generated quickly using a small number of parameters. The resulting 
images appear quite similar to images of real botanical trees. We have demonstrated the efficient use 
of the model with our synthetic scene generator. We explained a range degradation methodology to smoothly 
degrade the tree geometry at long ranges. This is used to optimize the drawing of large quantities of 
trees in forested areas. Our attention in designing the model was focused on allowing a general user 
to create trees that generally match images from books or photographs. The user needs no knowledge of 
botany or complex mathematical principles, only a basic understanding of geometry. We concentrated on 
the general structural appearance of a tree instead of the biological and biophysical principles that 
produced its structure. Currently, the rendering of our trees at close range is not quite fast enough 
to meet the needs of real time simulation. A high end graphics workstation may only be able to draw one 
very close tree or a few dozen long range trees in real time. However, newer hardware will inevitably 
bring higher performance. In the near future, tree models such as this will be important in many areas 
of computer graphics.   ACKNOWLEDGEMENTS All images were created with the CREATION software developed 
by Teletronics and the Modeling Simulation Branch of the US Army Research Laboratory. Thanks to the US 
Army Research Lab who supported this project and helped with this paper, specifically Teresa Kipp, John 
Ho (retired), Gertrude Kornfeld (retired), Hung Nguyen, E. "Glenn" Dockery, Michael Lander, Janice Colby, 
and Giap Huynh, and also Dickson Fang and Scott Hawley of Teletronics. Thanks also to Dynamics Research 
Corporation.  REFERENCES [ALON70] M. Alonso, E. Finn. Physics. Addison-Wesley, Reading, Massachusetts, 
1970, pp. 160-166. [AONO84] M. Aono, T. Kunii. Botanical Tree Image Generation. IEEE Computer Graphics 
and Applications. May, 1984, pp.10-34, Volume 4, No.5. [BLOO85] J. Bloomenthal. Modeling the Mighty Maple. 
Proceedings of SIGGRAPH '85 (San Francisco, California, July 22-26, 1985). In Computer Graphics Proceedings, 
Annual Conference Series, 1985, ACM SIGGRAPH, pp. 305-311. [CHND88] P. Chandler, A. Cook, G. DeWolf, 
G Jones, K Widin. Taylor's Guide to Trees. Houghton Mifflin Company, Boston, 1988. [CHAN82] F. Chan, 
F. Ching, W Collins, M. Evans, W. Flemer, J. Ford, F. Galle, R Harris, R. Korbobo, F. Lang, F. Mackaness, 
B. [PAGE93] J. Page. Planet Earth: Forest. Time-Life Books, Alexandria, Mulligan, R. Ticknor. Trees. 
The American Horticultural Virginia, 1983. Society, Mount Vernon, Virginia, 1982. [PRUS90] P. Prusinkiewicz, 
A. Lindenmayer. The Algorithmic Beauty [COLL74] G. Collingwood, W. Brush. Knowing Your Trees. The of 
Plants. Springer-Verlag, New York, 1990. American Forestry Association, Washington, DC., 1974. [PRUS94] 
P. Prusinkiewicz, M. James, R. Me ch. Synthetic Topiary. [FOLE92] J. Foley, A. vanDam, S. Feiner, J. 
Hughes. Computer Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, Graphics, Principles and 
Practice, Second Edition. Addison-1994). In Computer Graphics Proceedings, Annual Wesley, Reading, Massachusetts, 
1992. Conference Series, 1994, ACM SIGGRAPH, pp. 351-358. [FLOY76] R. Floyd, L. Steinburg. An Adaptive 
Algorithm for Spatial [REEV85] W. Reeves. Approximate and Probabilistic Algorithms for Grey Scale, Proceedings 
SID. 1976, pp. 75-77. Shading and Rendering Structured Particle Systems. [HALL88] D. Halliday, R, Resnick. 
Fundamentals of Physics, 3rd Ed. Proceedings of SIGGRAPH '85 (San Francisco, California, J. Wiley &#38; 
Sons, New York, 1988, pp. 306-322. July 22-26, 1985). In Computer Graphics Proceedings, [HAUS91] E. Haustein. 
The Cactus Handbook. Hamlin, London, 1991. Annual Conference Series, 1985, ACM SIGGRAPH, pp. 313­[HART77] 
J. Den Hartog. Strength of Materials. Dover, Mineola, 1977, 322. pp. 79-88. [REFF88] P. de Reffye, C. 
Edelin, J. Françon, M. Jaeger, C. Puech. [HOND71] H. Honda. Description of the Form of Trees by the Plant 
models faithful to botanical structure and development. Parameters of the Tree-like Body: Effects of 
the Branching Proceedings of SIGGRAPH 88 (Atlanta, Georgia, August 1-5, Angle and the Branch Length on 
the Shape of the Tree-like 1988). In Computer Graphics Proceedings, Annual Body. Journal of Theoretical 
Biology. 1971, pp. 331-338. Conference Series, 1988, ACM SIGGRAPH, pp. 151-158. [HOND81] H. Honda, P. 
Tomlinson, J. Fisher. Computer Simulation of [REIL91] A. Reilly. The Secrets of Trees. Gallery Books, 
New York, Branch Interaction and Regulation by Unequal Flow Rates in 1991. Botanical Trees. American 
Journal of Botany. 1981, pp. [ROHL94] J. Rohlf, J. Helman.. IRIS Performer: A High Performance 569-585. 
Multiprocessing Toolkit for Real-Time 3D Graphics. [LIND68] A. Lindenmayer. Mathematical Models for Cellular 
Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, Interactions in Development, I&#38;II. Journal 
of Theoretical 1994). In Computer Graphics Proceedings, Annual Biology. 1968, pp. 280-315. Conference 
Series, 1994, ACM SIGGRAPH, pp. 381-394, [OPPE86] P. Oppenheimer. Real Time Design and Animation of Fractal 
specifically Figures 14 and 17 on page 394. Plants and Trees. Proceedings of SIGGRAPH '86 (Dallas, [SYMO58] 
G. Symonds. The Tree Identification Book. William Morrow Texas, August 18-22, 1986). In Computer Graphics 
&#38; Company, New York, 1958. Proceedings, Annual Conference Series, 1986, ACM [TOOT84] E. Tootill, 
S. Blackmore. The Facts on File Dictionary of SIGGRAPH, pp. 55-64. Botany. p.155. Market House Books 
LTD, Aylesbury, UK, 1984, APPENDIX: Parameter List Parameter Shape BaseSize Description general tree 
shape id fractional branchless area at tree base Quaking Aspen 7 0.4 Black Tupelo 4 0.2 Weeping Willow 
3 0.05 CA Black Oak 2 0.05 Scale,ScaleV,ZScale,ZScaleV Levels size and scaling of tree levels of recursion 
13, 3, 1, 0 3 23, 5, 1, 0 4 15, 5, 1, 0 4 10, 10, 1, 0 3 Ratio,RatioPower Lobes,LobeDepth Flare radius/length 
ratio, reduction sinusoidal cross-section variation exponential expansion at base of tree 0.015, 1.2 
5, 0.07 0.6 0.015, 1.3 3, 0.1 1 0.03, 2 9, 0.03 0.75 0.018, 1.3 5, 0.1 1.2 0Scale,0ScaleV 0Length,0LengthV, 
0Taper 0BaseSplits 0SegSplits,0SplitAngle,0SplitAngleV 0CurveRes,0Curve,0CurveBack,0CurveV extra trunk 
scaling fractional trunk, cross-section scaling stem splits at base of trunk stems splits &#38; angle 
per segment curvature resolution and angles 1, 0 1, 0, 1 0 0, 0, 0 3, 0, 0, 20 1, 0 1, 0, 1.1 0 0, 0, 
0 10, 0, 0, 40 1, 0 0.8, 0, 1 2 0.1, 3, 0 8, 0, 20, 120 1, 0 1, 0, 0.95 2 0.4, 10, 0 8, 0, 0, 90 1DownAngle,1DownAngleV 
1Rotate,1RotateV,1Branches 1Length,1LengthV,1Taper 1SegSplits,1SplitAngle,1SplitAngleV 1CurveRes,1Curve,1CurveBack,1CurveV 
main branch: angle from trunk spiraling angle, # of branches relative length, cross-section scaling stem 
splits per segment curvature resolution and angles 60, -50 140, 0, 50 0.3, 0, 1 0, 0, 0 5, -40, 0, 50 
60, -40 140, 0, 50 0.3, 0.05, 1 0, 0, 0 10, 0, 0, 90 20, 10 -120, 30, 25 0.5, 0.1, 1 0.2, 30, 10 16, 
40, 80, 90 30, -30 80, 0, 40 0.8, 0.1, 1 0.2, 10, 10 10, 40, -70, 150 2DownAngle,2DownAngleV 2Rotate,2RotateV,2Branches 
2Length,2LengthV, 2Taper 2SegSplits,2SplitAngle,2SplitAngleV 2CurveRes,Curve,2CurveBack,2CurveV secondary 
branch: angle from parent spiraling angle, # of branches relative length, cross-section scaling stem 
splits per segment curvature resolution and angles 45, 10 140, 0, 30 0.6, 0, 1 0, 0, 0 3, -40, 0, 75 
30, 10 140, 0, 25 0.6, 0.1, 1 0, 0, 0 10, -10, 0, 150 30, 10 -120, 30, 10 1.5, 0, 1 0.2, 45, 20 12, 0, 
0, 0 45, 10 140, 0, 120 0.2, 0.05, 1 0.1, 10, 10 3, 0, 0, -30 3DownAngle,3DownAangleV 3Rotate,3RotateV,3Branches 
3Length,3LengthV, 3Taper 3SegSplits,3SplitAngle,3SplitAngleV 3CurveRes,3Curve,3CurveBack,3CurveV tertiary 
branch: angle from parent spiraling angle, # of branches relative length, cross-section scaling stem 
splits per segment curvature resolution and angles 45, 10 77, 0, 10 0, 0, 1 0, 0, 0 1, 0, 0, 0 45, 10 
140, 0, 12 0.4, 0, 1 0, 0, 0 1, 0, 0, 0 20, 10 140, 0, 300 0.1, 0, 1 0, 0, 0 1, 0, 0, 0 45, 10 140, 0, 
0 0.4, 0, 1 0, 0, 0 1, 0, 0, 0 Leaves,LeafShape LeafScale,LeafScaleX AttractionUp PruneRatio PruneWidth,PruneWidthPeak 
PrunePowerLow,PrunePowerHigh number of leaves per parent, shape id leaf length, relative x scale upward 
growth tendency fractional effect of pruning width, position of envelope peak curvature of envelope 25, 
0 0.17, 1 0.5 0 0.5, 0.5 0.5, 0.5 6, 0 0.3, 0.5 0.5, 0 0.5, 0.5 0.5, 0.5 15, 0 0.12, 0.2 -3 1 0.4, 0.6 
0.001, 0.5 25, 0 0.12, 0.66 0.8 0 0.5, 0.5 0.5, 0.5   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218430</article_id>
		<sort_key>129</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Depicting fire and other gaseous phenomena using diffusion processes]]></title>
		<page_from>129</page_from>
		<page_to>136</page_to>
		<doi_number>10.1145/218380.218430</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218430</url>
		<keywords>
			<kw><![CDATA[advection]]></kw>
			<kw><![CDATA[diffusion]]></kw>
			<kw><![CDATA[fire]]></kw>
			<kw><![CDATA[gaseous phenomena]]></kw>
			<kw><![CDATA[light transport]]></kw>
			<kw><![CDATA[multiple scattering]]></kw>
			<kw><![CDATA[particle systems]]></kw>
			<kw><![CDATA[smoke]]></kw>
			<kw><![CDATA[turbulence]]></kw>
			<kw><![CDATA[warped blobbies]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.6.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31029451</person_id>
				<author_profile_id><![CDATA[81100148921]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto, 10 King's College Circle, Toronto, Ontario, Canada, M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117278</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Toronto, 10 King's College Circle, Toronto, Ontario, Canada, M5S 1A4]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E Blasi, B. Le Saec, and C. Schlick. "A Rendering Algorithm for Discrete Volume Density Objects". Computer Graphics Forum, 12(3):201-210, 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. E Blinn. "Light Reflection Functions for Simulation of Clouds and Dusty Surfaces". ACM Computer Graphics (SIGGRAPH '82), 16(3):21-29, July 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. D. Buckmaster, editor. Frontiers in Applied Mathematics. The Mathematics of Combustion. SIAM, Philadelphia, 1985.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[N. Chiba, K. Muraoka, H. Takahashi, and M. Miura. "Twodimensional Visual Simulation of Flames, Smoke and the Spread of Fire". The Journal of Visualization and Computer Animation, 5:37-53,1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Chomiak. Combustion. A Study in Theory, Fact and Application. Abacus Press/Gordon and Breach Science Publishers, New York, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. J. Duderstadt and W. R. Martin. Transport Theory. John Wiley and Sons, New York, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97918</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. S. Ebert and R. E. Parent. "Rendering and Animation of Gaseous Phenomena by Combining Fast Volume and Scanline A-buffer Techniques". ACM Computer Graphics (SIGGRAPH '90), 24(4):357-366, August 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90972</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Inakage. "A Simple Model of Flames". In Proceedings of Computer Graphics International 89, pages 71-81. Springer-Verlag, 1989.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[A. Ishimaru. VOLUME 1. Wave Propagation and Scattering in Random Media. Single Scattering and Transport Theory. Academic Press, New York, 1978.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. T. Kajiya and B. E von Herzen. "Ray Tracing Volume Densities". ACM Computer Graphics (SIGGRAPH '84), 18(3):165-174, July 1984.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[E. Langu6nou, K.Bouatouch, and M.Chelle. Global illumination in presence of participating media with general properties. In Proceedings of the 5th Eurographics Workshop on Rendering, pages 69-85, Darmstadt, Germany, June 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78965</ref_obj_id>
				<ref_obj_pid>78964</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Levoy. "Efficient Ray Tracing of Volume Data". ACM Transactions on Computer Graphics, 9(3):245-261, July 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[J. J. Monaghan. "Why Particle Methods Work". SlAM Journal of Scientific and Statistical Computing, 3(4):422-433, December 1982.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K. Perlin. "An Image Synthesizer". ACM Computer Graphics (SIG- GRAPH '85), 19(3):287-296, July 1985.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[C.H. Perry and R. W. Picard. "Synthesizing Flames andtheir Spread". SIGGRAPH' 94 Technical Sketches Notes, July 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42249</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[W. H. Press, B. E Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C. The Art of Scientific Computing. Cambridge University Press, Cambridge, 1988.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves. "Particle Systems. A Technique for Modeling a Class of Fuzzy Objects". ACM Computer Graphics (SIGGRAPH '83), 17(3):359-376, July 1983.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[H.E. Rushmeier andK. E. Torrance. "The Zonal Method for Calculating Light Intensities in the Presence of a Participating Medium". ACM Computer Graphics (SIGGRAPH '87), 21 (4):293-302, July 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[G. Sakas. "Fast Rendering of Arbitrary Distributed Volume Densities". In E H. Post and W. Barth, editors, Proceedings of EURO- GRAPHICS '90, pages 519-530. Elsevier Science Publishers B.V. (North-Holland), September 1990.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>157618</ref_obj_id>
				<ref_obj_pid>157615</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[G. Sakas. "Modeling and Animating Turbulent Gaseous Phenomena Using Spectral Synthesis". The Visual Computer, 9:200-212,1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[G. Sakas and M. Gerth. "Sampling and Anti-Aliasing of Discrete 3-D Volume Density Textures". In E H. Post and W. Barth, editors, Proceedings of EUROGRAPHICS '91, pages 87-102. Elsevier Science Publishers B.V. (North-Holland), September 1991.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[J. Stam. Forthcoming Ph.D. thesis, Department of Computer Science, University of Toronto, 1995.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Stam. "Multiple Scattering as a Diffusion Process". In Proceedings of the 6th Eurographics Workshop on Rendering, Dublin, Ireland, June 1995.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166163</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[J. Stam and E. Fiume. "Turbulent Wind Fields for Gaseous Phenomena". In Proceedings of SIGGRAPH '93, pages 369-376. Addison- Wesley Publishing Company, August 1993.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. R. Wallace, K. E. Elmquist, and E. A. Haines. "A Ray Tracing Algorithm for Progressive Radiosity". ACM Computer Graphics (SIGGRAPH '89), 23(3):315-324, July 1989.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Depicting Fire and Other Gaseous Phenomena Using Diffusion Processes Jos Stam Eugene Fiume Department 
of Computer Science, University of Toronto1 Abstract Developing a visually convincing model of .re, 
smoke, and othergaseousphenomenais amongthe mostdif.cult andattractive problems in computer graphics. 
We have created new methods of animating a wide range of gaseous phenomena, including the particularly 
subtle problem of modelling wispy smoke and steam, using far fewer primitives than before. One signi.cant 
innovation is the reformulation and solution of the advection-diffusion equation for densities composed 
of warped blobs . These blobs more accurately model the distortions that gases undergo when advected 
by wind .elds. We also introduce a simple model for the .ame of a .re and its spread. Lastly, we present 
an ef.cient formulation and implementation of global illumination in the presence of gases and .re. Our 
models are speci.cally designed to permit a signi.cant degree of user control over the evolution of gaseous 
phenomena. Keywords: .re, smoke, gaseous phenomena, diffusion, advec­tion, warped blobbies, light transport, 
multiple scattering, particle systems, turbulence. Introduction The interplay of light with gases, aerosols 
and dust spans across the most visually delicate and the most explosive of natural phenom­ ena. The depiction 
of these phenomena has been of great interest to computer graphics for over a decade, and their application 
to computer animation is clear. Any graphical model of a gaseous phenomenon must have three components: 
a representation of the gas, a model for its spa­ tiotemporal behaviour, and an illumination model to 
determine its appearance. Graphical models for gaseous phenomena have fallen into two classes. Models 
from one class combine the representa­ tion of the gas and its motion into a solid texture, animating 
the gas by varying parameters of the texture [14, 7]. Obtaining con­ vincing motion with this approach 
becomes dif.cult because of the non-physical nature of the parameters and the cost associated with visualizing 
the solid texture. These parameters can be related, however, to a statistical model of turbulence [20]. 
We prefer the alternative approach, which keeps the three com­ ponents intact, and animates the gas using 
wind .elds. The earliest 110 King s College Circle, Toronto, Ontario, Canada, M5S 1A4 E-mail: fstamjelfg@dgp.toronto.edu 
 Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
related work in computer graphics is by Reeves [17], in which particle systems were used to evoke a variety 
of visual effects, in­cluding .re. Recent work has focussed on the depiction of speci.c phenomena such 
as steam, mist, clouds or .re (e.g., [24, 4]). In thesemodels,awind .eldbothadvects(i.e., displaces)anddiffuses 
blobs of density over time. Thus an immediate bene.t is that the motion of a gas can be seen in real-time. 
However, the regular shape of the blobs often makes the gas look arti.cial. In this work, we improve 
the blob model by nonuniformly modifying the shape of the .eld over time. The .eld is warped by the surrounding 
wind .eld in a manner consistent with a real blob of gas (see Figure 8). The corresponding results are 
more convincing, using an order of magnitude fewer particles than our previous work [24]. The rendering 
of gases has been an active area of computer graphics research since the seminal work of Blinn [2]. Most, 
if not all, of the algorithms .rst sample the density and the optical prop­erties of the gas on a grid 
and then solve for illumination. In a .rst pass, the effects of multiple scattering are resolved. Depending 
on the scattering distribution functions employed, different possibil­ities arise. For isotropic or constant 
scattering, a zonal radiosity­style method can be employed [18]. For arbitrary scattering, re­searchers 
have used a spherical harmonics expansion [10], a dis­cretization of the angles [11] or brute-force ray-tracing 
[1]. These approximations are known respectively as the PNmethod, discrete ordinates and Monte-Carlo 
simulation in the radiative transfer lit­erature [6]. Once the effects of multiple scattering are computed, 
a .nal image is obtained by voxel-traversal algorithms [7, 19, 12]. However, the grid approximation has 
two de.ciencies: the grid is memory intensive for complicated gas geometries,and it introduces samplingartifactsthatareevidentinanimation. 
Theappearanceof artifacts can be alleviated by pre.ltering the data on the grid prior to voxel traversal 
rendering [21], at the cost of diminished visual detail. In view of these limitations, we have developed 
a new render­ing algorithm which is an extension of the ray-tracing progressive re.nement solution for 
radiosity [25]. Speci.cally, shooting op­erations both to and from gaseous blobs are derived. In addition, 
we use a diffusion approximation to resolve the effects of multi­ple scattering. Once the scattered intensity 
is resolved, it can be integrated to yield images of the gas from arbitrary viewpoints us­ing an ef.cient 
blob tracer [24]. Our algorithm has been designed to capture the essential visual features of gaseous 
phenomena as ef.ciently as possible. We are fascinated with the depiction of .re. The general phe­nomenon 
of .re and combustion in spite of their fundamental importance and practical applications, are far from 
being fully un­derstood. This is due, above all, to their interdisciplinary character and great complexity 
[5]. Perhaps because of this, there has been little actual progress in visual models for .re. A simple 
laminar .ame was texture mapped onto a .ame-like implicit primitive and then volume-traced by Inakage 
[8]. Recently, two groups of re­searchers have worked on the problem of the spread of .re.1 Perry and 
Picard apply a velocity spread model from combustion science to propagate their .ames [15]. On the other 
hand, Chiba et al. compute the exchange of heat between objects by projecting the environment onto a 
plane [4]. The spread of the .ame is a function of both the temperature and the concentration of fuel. 
In this paper, we present a similar model in three dimensions for the creation, extinguishing and spread 
of .re. The spread of the .re is entirely controlled by the amount of fuel available, the geometry of 
the en­vironment and the initial conditions (i.e., where we light the match). Although our model relies 
on physical equations, it should not be confused with a physically accurate simulation of .re. Indeed, 
the latter is still an active area of research in combustion science. This paper focuses on producing 
convincing physically-motivated depictions of the motion and appearance of .re. The structure of the 
paper is as follows. In the next section, we discuss the general methodology we use to model gaseous 
phenomena. In Section 3 we introduce diffusion equations for the evolution of the density, temperature 
and diffuse intensity of gases. In Section 4 an ef.cient solution to the diffusion equations is derived 
using warped blobs. In Section 5 we outline our solution to the intensity .eld. Then, in Section 6 we 
advance our simple .ame model, discussing the results in Section 7. Finally, in Section 8 we discuss 
our conclusions, pointing towards future research.  Overview of the Method A gas is described by physical 
quantities that vary as a function of space and time. These quantities include the density of gas par­ticles, 
and the surrounding velocity and temperature .elds. Gen­erally, the relationships governing these quantities 
are strongly coupled and are known as the Navier-Stokes equations. For ex­ample, the temperature .eld 
introduces gradients in the velocity .eld, but the velocity .eld advects and diffuses the temperature 
.eld. With reacting gases such as .re, additional equations are needed to account for the underlying 
chemical reactions. The full physical simulation of this set of equations is prohibitively expen­sive. 
Their nonlinear nature and the wide range of scales required would severely strain computational speed 
and memory available on even the most powerful computers. Even if such a simulation were available, it 
would be of limited interest to computer graphics, because of the user s inability to control the phenomenon. 
Certain effects would not be achievable. The degree of user control must be balanced against the need 
for the turbulent behaviour of a gas. Turbulence is dif.cult to model without an underlying physical 
or statistical model driving it. In our model, an animator governs the behaviour of a gas by specifying 
a wind .eld. The animator manages the global motion of the gas by using smooth .elds, and controls the 
small scale behaviour by modifying the statistical parameters and scale of a turbulent wind .eld as in 
[24]. The latter .eld adds complexity to the motion. In addition, we assume that the motion .eld is incompressible 
so that its density Pfis constant. Once the wind .eld is given, we employ an advection-diffusion type 
equation to compute the evolution of both the density .eld and the temperature .eld. Diffusion type equations 
are employed for two reasons. First, they capture the main characteristics of many transport phenomena. 
Second, they are simple enough to be understood by an animator with a limited knowledge of physics. Indeed, 
most of us are familiar with diffusion processes since they are ubiquitous in everyday life (e.g., milk 
dissolving in a coffee cup). Given a user-speci.ed wind .eld u(xt), the evolution over time of a scalar 
.eld e(xt)is 1This is of course in the context of computer graphics. There is an enormous literature 
on .re in various areas, including forestry, chemistry, and physics. assumed to be governed by the following 
diffusion process: De = er 2e+Se-Le Dt where De/Dt=ae/at+ureis the total derivative giving the variation 
over time of the .eld eon a particle advected by the wind .eld u. Apart from the wind .eld, the evolution 
is characterized by a diffusion coef.cient e, sources Seand sinks Le. For rapidly evolving phenomena 
such as the propagation of light, only the steady state De/Dt=0 of this equation is considered. By approximating 
the scalar .eld by a set of fuzzy particles, a user is able to visualize the effect of the wind .eld 
in real-time on a graphics workstation. Finally, when the user is satis.ed with a particular behaviour, 
the fuzzy particles can be rendered by a global illumination algorithm for high-quality animations. 3 
Diffusion Processes 3.1 Density and Temperature The main physical characteristics of a gas are described 
by its den­sity P, its temperature T, its velocity uand its radiative properties.2 The evolution of the 
density is given by a diffusion equation. The diffusion coef.cient in this case models the mixing caused 
by the small scales of the turbulence not modelled by our wind .elds [24]. The sink term is modelled 
as simple decay over time at a constant rate: L=OP. The source is either speci.ed by a user or is re­lated 
to a simple model of chemical reactions (see Section 6). A similar equation for the evolution of the 
temperature is reached by assuming that the kinetic energy is negligible compared to the heat released 
by the gas, and that buoyancy and pressure .uctuations are small [5]: DT cpPf= Tr 2T+ST-LT Dt Often, 
this reduced equation is utilized in theoretical investigations of the propagation of .ames [3]. It is 
too simple to yield physically accurate simulations. However, we employ this equation because it captures 
some essential features of the .ame. The speci.c heat at constant pressure cpcharacterizes the ef.ciency 
of the .uid to release heat. The density of the .uid Pfis supposed to be constant, since our wind .elds 
are incompressible3. The exact form of the source, sink and absorption terms depend on the type of gas. 
We explore this speci.cally in the case of .re in Section 6. 3.2 Intensity Field A gas modi.es the intensity 
.eld of light I>(x s)by scattering, absorption and emission. The emission of a gas in local thermody­namic 
equilibrium is proportional to black-body emission [6]:   h()i .1 2hhc Q>=E>B>(T)=E>5exp -1 )c)kT where 
E>models the contribution of each wavelength )to the the emission, his Planck s constant, kis Boltzmann 
s constant and cis the speed of light. The total emission over all frequencies is proportional to the 
fourth power of the temperature: Z 1 B>(T)d)= SBT4 (1) 0 2To shorten the notation, explicit dependence 
of the .elds on all its variables is not always included. 3The density pfof the .uid should not be confusedwith 
the density of microscopic particles padvected and diffused by the .uid. where SBis Stefan-Boltzmann 
s constant. In order to shorten the notations somewhat, we drop the explicit dependence on wave­length 
from the following radiative properties. The scattering properties of a gas are characterized by its 
albedo O and its phase function p(ss0). The albedo gives the fraction of light that is scat­tered versus 
that which is absorbed. The phase function models the spherical distribution of the scattered light. 
Although various distributions exist for different types of gases, we assume that the following reduced 
description is suf.cient 0 s0 p(s)=1 +p¯ ss(2) R .1 where p¯=3/47pp(p)dpis the .rst moment of the phase 
.1 function and characterizes the anisotropy of the scattering. Values of ¯pnear +1 indicate a preference 
for forward scattering. Values near -1 indicate predominantly backward scattering. A direct consequenceofthe 
simpledistribution is thatthescatteredintensity depends only weakly on its angular variable: Z 1 000 
0p¯1 SfI(xs)g= p(ss)I(s)ds=I(x)+I(x)s(3) 47 3 4 where I0 and I1 are known as the average intensity and 
average .ux respectively. These two functions actually correspond to the .rst four coef.cients of the 
intensity .eld into a spherical harmon­ics basis. In fact, the derivations outlined in this paper can 
be generalized to higher-order expansions[22]. The number of interactions per unit length of the light 
.eld with the gas is proportional to the density of the gas and is equal to tP, where tis called the 
extinction cross-section. The gas diminishes the intensity of light traveling along a ray xu =x0 -usby 
absorption and outscatter, and increases the inten­sity through self-emission and inscatter. The intensity 
reaching a point x0 along a direction sis then equal to two contributions. The .rst is a portion of the 
intensity leaving the point of intersection xb of the ray with a background surface. The second contribution 
is the light created within the gas: Z b out I(xs)=I(xb)T(0 b)+ tP(xu)T(0 u)J(xus)ds(4) 0 R where T(vw)=exp(-wtP(xu)du)is 
the transparency,and v the source intensity is the sum of single-scattering, multiple scat­tering and 
self-emission: J=Js+Jd+(1 -O)Q (5) The single-scattering intensity Jsaccounts for the .rst scatter of 
the incident intensity Iientering the gas: Js =OSfIig.The diffuse intensity Jdon the other hand, is entirely 
created within the gas through the phenomenon of multiple scattering. Since we have restricted ourselves 
to simple scattering distributions, the diffuse intensity in fact satis.es a diffusion equation (see 
Appendix A). This is a well known approximation in transport theory and is valid when the number of interactions 
of light with the gas is high, speci.cally when the dimensionless number tPl0 is high, where l0 is a 
length characterizing the scales involved. For atmospheric scattering the approximation is usually considered 
to be valid when the density is higher than 0 01 [9]. 4 Blob Solution of Diffusion Equations Diffusion 
equations can be solved numerically using stable .nite difference schemes [16]. In this case, the solution 
is sampled on a grid. Hence, the method can only resolve scales that are bigger than the grid spacing. 
These values, then, are interpolated to yield a smoothed version of the exact solution. Unfortunately, 
for three-dimensional problems grid-based schemes are intractable, because of memory limitations. Consequently, 
we develop an alternative method of solution by generalizing the above smoothing process. Generally, 
the approximate solution can be represented as the convolution of the exact solution with a smoothing 
kernel WW(r): Z eW(xt)= WW(jx-x0 j)e(x0 t)dx where the corresponds to the grid spacing. The smoothing 
kernel is normalized and tends towards a delta distribution as -0. The latter two conditions are required 
such that the exact solution is recovered when the grid spacing goes to zero. Because the smoothing kernel 
depends solely on distance, the approximation is of order two [13]. Therefore, the .eld ecan always be 
replaced by its smoothed equivalent to within the order of accuracy of the smoothing process itself, 
e.g., 2 (e )W =eW W+O() (6) N Insteadofagrid,we considerasetofsamples fxk(t)gk:1 evolving over time. 
By assigning a mass mk(t)to each sample, we repre­sent the density as P(xt)= N mk(t) (x-xk(t))Then, a 
Pk:1 smoothed approximation of the density .eld is given by: N X PW(xt)= mk(t)WW(jx-xk(t)j) k:1 i.e., 
the density is a superposition of blobs centred at the sample locations. This representation induces 
one for other .elds using Eq. 6, i.e., (Pe)W =PWeW. Hence, 1 X N eW(xt)= mk(t)ek(t)WW(jx-xk(t)j) PW(xt) 
k:1 where ek(t)=e(xk(t)t). An approximate solution to the diffusion equation is obtained naturally when 
the samples move along the wind .eld. Because this equation is linear, it is enough that each blob satis.es 
it. A suf.cient (and convenient) condition for each blob kto satisfy the diffusion equation is that the 
smoothing kernel is a Gaussian: 2 1 r WWk t (r)= exp­ 3 23 2 (27)(t)2 (t) kk p with standard deviation 
proportional to et. The diffusion then expands the size of the blobs over time. The sink term is satis.ed 
if the coef.cients ek(t)satisfy dek(t)=-Le(xk(t)t) dt We use the source term of the density to generate 
new blobs at each time step of the simulation. From the source term, we can de.ne a probability density 
distribution for each time t: Z ip(xt)=Sp(xt)Sp(x0 t)dx0 Quite often, the source term is constant on 
a given domain, and the density distribution is uniform. The location xkof the new blob hence is determined 
by generating a random point from this distribution. The initial mass of each blob is a function of the 
 s+.t0 s  t-t0 t Figure 1: Blob Warping number N0 of new blobs per time step and of the initial size 
0 of each blob: 32 3 mk =(27)0 /N0Sp(xkt).t where .tis the time step. The animator determines the number 
of new blobs and the initial size. The preceding procedure is a generalization of our blob solution for 
the evolution of a density distribution in a moving .uid [24]. However, after a large amount of diffusion, 
that is, with suf.cient simulation time, the increasing variance causes the spherical shape for each 
blob to become apparent. Real gases, and especially .re, are poorly approximated by a superposition of 
spherical blobs, with the exception perhaps of very billowy smoke. The problem is due to the fact that 
as the blobs get bigger, the shape of the actual distribution is not uniform but is in fact advected 
by the velocity .eld (see Figure 1). To account for this nonuniformity, we shall more accurately track 
the shape of each blob as the advection of a regular blob over a .xed time t0. To achieve this, we could 
take samples in the blob at some time t-t0 and then trace each sample over an interval t0 through the 
wind .eld. However, this introduces sampling artifacts. A better approach is to backtrace from the warped 
blob toward the initial blob (see Figure 1). For each point x, there corresponds a point x.1 obtained 
by tracing the point back through the wind .eld for a time t0: Z t.t0 000 x.1 =x-u(x(t)t)dt(7) t To include 
this warping in our simulations, we replace the smooth­ing kernel by one evaluated at the backtraced 
points: .1 .1 WWkt(jx-xk(t)j)--WWkt. tjx-x(t)j 0k where Tis a factor accounting for the spread of the 
blob due to advective effects; it is a function of the magnitude and scale of the wind .eld at the point 
x. In our implementation, it is a user­speci.ed constant. The extra cost of the blob-warping method is 
the evaluation of the above integral, which can be achieved by a simple Eulerian scheme. Resolving the 
Intensity Field As in the density and temperature .elds, we represent the source intensity into a superposition 
of blobs: 1 X N J(xs)= mkJk(s)WW(jx-xkj)P(x) k:1 The angular variation of each coef.cient in this expansion 
is deter­mined by the shape of the phase function and is therefore (see Eq. 3): 01 Jk(s)=J+Js kk These 
coef.cients are computed by utilizing an extension of the shooting algorithm used in diffuse environments 
[25]. The surfaces of the environment are .rst discretized into an ensemble of patches. At each step 
of the algorithm, the outgoing intensity Ioutfrom one Figure 2: Patch to Patch Blob to Patch Patch to 
Blob patch is shot into the environment and is collected at the other patches of the environment. We 
extend this work by including shooting operations from a patch to the blobs and conversely, from a blob 
to the patches. In addition, we develop shooting operations from non-physical lights, such as directional 
sources, to the blobs. These shooting operations resolve the single scattering intensity. The diffuse 
intensity is obtained by solving Eq. 11. A summary of the algorithm is given next. Set source intensity 
to emission: J=(1 -O)Q. Collect intensity from light sources: J+J+OSfIlightsg. do Shoot from patches 
to other patches. Collect intensity from patches:J+J+OSfIpatchesg. Compute diffuse intensity: Jd. Add 
to source intensity: J+J+Jd. Shoot source intensity to the patches. Until converged This algorithm is 
similar to the one presented in [11]. 5.1 Shooting Operations To derive our shooting operations, we consider 
the basic inter­change of intensity between a surface area A2 andanarea dA1 separatedbya distance d12 
along a direction s12: inout I12 =IF12 (8) 2 where F12 is the form factor between the two surfaces. When 
A2 is a disk, the form factor can be approximated by [25]: A2 F12 =(n1 s12)(-n2 s12) A2 +d2 12 where 
n1 and n2 denote the normals at A2 and dA1, respectively. The contribution of the intensity leaving A2 
to the outgoing inten­ r1Iin sity Ioutis then given by Iout =12 ,where r1 is the re.ectance 1 12 of the 
receiving patch. A single shooting operation from a patch to a blob is a special case of Eq. 8, since 
an in.nitesimal receiving surface at the centre of the blob can always be aligned along s12, i.e., n1 
s12 =1. The contribution to the source intensity at the blob is then out J12(s)=IF12O(1 +p¯ss12 ) 2 Therefore, 
the .rst two coef.cients in the angular expansion of the source intensity are updated as 00 out out 12 
pI2 J+J1 +OIF12 and J11 +J11 +O ¯F12 s12 Similarly, the shooting operation from a blob to a patch is 
achieved by considering a disk of area A()=272 at the centre of the blob aligned with the direction s12, 
i.e., -n2 s12 =1. The outgoing intensity at the patch then is equal to IoutIout ++r1J2(s12)F12 (9) 11 
This step can be sped up by constructing a hierarchical tree-data structure of the blobs [24]. Instead 
of shooting from a single blob, we shoot from the centre of mass of each blob cluster. uj+1 -1 uj zj 
 Figure 3: Inverse Warp of the Ray We have also included shooting operations from non-physical light 
sources to the blobs, since these lights are extremely useful in creating many visual effects. The contribution 
of a directional light source of intensity I0 and direction s0 is achieved by: dir 0 J1(s)+J1(s)+IdirO(1 
+p¯ss0) The shooting operations from point lights and spotlights can be derived likewise. Shadowing by 
surfaces is included in these calculations by multiplying each form factor by an occlusion term. Similarly, 
partial shadowing caused by the gas is included by multiplying the form factor by the transparency between 
the shooting and receiving elements. Aliasing can be avoided by shooting from a subdivision of the shooting 
blob/patch [25]. We could derive similar shooting operations between blobs. However, since usually there 
is a large overlap of blobs, the disk form factor is not accurate. Also, in the case of many blobs, these 
shooting operations become very expensive. Instead, we solve for the .rst two coef.cients in the angular 
expansion of the source intensity using the diffusion approximation (see Eq. 11). 5.2 Multiple Scattering 
When the blob representation of the diffuse intensity is inserted into the diffusion equation (Eq. 11) 
we get the following equation: N X mkJ0 rd(x)rWk(x)-Od(x)Wk(x)+Sd(x)=0 d,k k:1 where Wk(x)=WW(jx-xkj)/P(x). 
By setting x=xl, k for l=1 ... Nwe get a system of N-linear equations for the 0 unknowns Jd,kwhich can 
be solved using an LU-decomposition for example [16]. Once we compute a solution, the coef.cients of 
the source intensity are updated: 0 00 Jk+Jk+Jd,kand J1 k+Jk1 +Jd1 (xk) where J1 dis given by Eq. 12. 
This method is in fact equivalent to a .nite-element solution for the diffusion equation. When the gas 
does not intersect any surfaces, the boundary conditions are naturally satis.ed by the blob representation. 
For more details about the method and the boundary conditions, see [23]. 5.3 Integrating the Transport 
Equation Once an approximation of the source intensity is computed, the intensity .eld at any point in 
the environment is obtained by inte­grating the scattering equation along a ray (Eq. 4). By truncating 
the domain of the blobs, we reduce the number of blobs intersect­ing a particular ray. These intersections 
de.ne a partition of the ray into disjoint intervals [ujuj1], with j=0 ... M-1[24]. To take into account 
the warping of the blob, we transform each interval backwards as illustrated in Figure 3. The density 
on each interval is approximated by its value at the point z.j1 calculated by backwarping the midpoint 
zj=(xuj +xuj)/2 of the interval: 1 X Pj=mkWWk jz.1 -x.1 j jk k where the sum is over the blobs which 
overlap the j-th interval. Consequently, both the transparency and the source intensity on each interval 
are approximated by: Tj:exp(-t(uj1 -uj)Pj)and X :1/Pj jz.1 -x.1 j Jj(s)mkJk(s)WWk jk k respectively. 
The evaluation of the integral can be performed by traversing the intervals from front to back: for all 
rays in ray-trace tree do I=0 Ttot =1 for j=0 ... M-1do I+I+Ttot(1 -Tj)Jj(s) Ttot+TtotTj if Ttot<EPS 
then exit end for I+I+TtotIout(xb) end for  6 A Simple Fire Model Flames result from the combustion 
of fuels and oxidizers. As the molecules of these compounds meet at a suf.ciently high tempera­ture, 
a chemical reaction becomes possible. The resultant burning compounds are called the .ame. We are not 
interested in a complete physical model for this reaction, but rather with those mechanisms essential 
to a good visual representation. In particular, we shall de­rive simple but effective models for the 
evolution of density .elds giving the concentration of .ames, smoke, and fuel. Flames and smoke can be 
subsequently rendered. We .rst describe the source and sink terms appearing in the diffusion equations 
for the density of the fuel, .ame and smoke. Given the density Pfuel of the fuel and its temperature 
Tfuel at a given point, the rate of production of the .ame density P.ame is given by the Arrhenius formula. 
Assuming a constant concentration of oxidants [3], () Ta Sp,.ame =Lp,fuel =Vaexp -Pfuel (10)Tfuel Tais 
the activation temperature, which is directly related to the energy Eareleased during the reaction by 
Ta =Ea/R,where Ris the universal gas constant. The term Vais a frequency , depending on the exact nature 
of the combustibles, characterizing the rate of the reaction. Most naturally occurring .res create smoke 
particles as the .ame cools down. To our knowledge, no de.nite analytical models of this exist. To model 
the creation of a density of smoke Psmoke, we use an equation similar to that used to produce burning 
fuel: () T.ame Sp,smoke =Vbexp -P.ame Ts where Tsis the temperature below which smoke particles start 
to form, and Vbis another material-dependent constant. The initial temperature of the .ame is related 
to the heat re­leased during the reaction modelled by Eq. 10. The source term appearing in the diffusion 
equation for its evolution is then ST,.ame =cpSp,.ame Ta. The temperature of the .ame decreases mainly 
through radiating heat. Since emission dominates in .ames, the loss of heat is equal to the contributions 
of the radiation from all angles and all wavelengths: ZZ 1 LT,.ame = t,>PQ>(T)dsd)=47PO.ame SBT 04 where 
the average absorption cross-section is de.ned by RZ 1 1 t,>E>B>(T)d)1 O.ame = 0 R 1 = 4 t,>Q>d) B>(T)d)SBT 
00 Notice that we have used Eq. 1. The temperature of the fuel rises because of radiated heat from nearby 
.ames. The source term for the temperature of the fuel is then the fraction of this radiation which is 
incident upon the fuel. This is similar to a shooting operation from a .ame blob to a patch (solid fuel) 
or to a blob (liquid fuel) (see Eq. 9): 4ST,fuel =PfuelOfuelF12 SBT.ame where Ofuel is the average absorption 
cross-section of the fuel. To achieve real-time simulation, we only shoot from a couple of blob­clusters. 
The loss of temperature due to radiation is similar to the loss term for the temperature of the .ame: 
LT,fuel =27PfuelOfuel SBT4 6.1 Implementation We have implemented the .re model for a fuel map de.ned 
on solid objects. This corresponds to non-burning objects coated with a .ammable substance. Note that 
we do not model the change in geometry caused by the burning process. The user speci.es the fuel density 
as a texture map and assigns burning properties (such as the speci.c heat) to each object. This is analogous 
to attributing re.ection properties to an object in a renderer. To begin a .re simulation, the user metaphorically 
strikes a match by indicating the origin(s) of combustion, and the simulation commences. The .ame model 
simulation is summarized in the following algorithm. for each time frame do for each solid object in 
the scene do Update temperature of the object Generate .ame blobs and update fuel map end for all .ame 
blobs do Update temperature and density Generate smoke blobs Move and diffuse end for all smoke blobs 
do Update density Move and diffuse end end The temperature of the fuel is updated using a Crank-Nicholson 
.nite difference scheme, since the domain of the fuel texture map is two-dimensional and there is no 
advection [16]. Typical reso­lutions for our simulations were 20 .20. The evolution of the temperature 
and the density of the .ame was performed using our blob method. We computed the evolution of the density 
of the smoke likewise. We allowed an animator to explore the parameter space by mapping the various physical 
quantities of the model into a graphical interface.  7Results We have developed an interactive implementation 
of the above models. As in [24], the user speci.es wind .elds and the effects of turbulence and the scale 
of the wind .elds on blobs can be imme­diately visualized. Various parameters (such as .eld positioning, 
scaling, and magnitude) can be modulated in real time. Figure 4 illustrates a spread simulation. The 
simulation is syn­chronized to the passage of real time so that an accurate evolution of the simulation 
can be subsequently rendered. Figure 5 gives one frame of a .re scene. As described above, a smoke density 
can also be produced. Rendered smoke can be seen at four stages in Figure 6. Each frame rendered at video 
resolution took approximately 20 minutes of CPU time on an SGI Indigo 2 with a R4000 processor. This 
includes rendering smoke and .re, the illumination caused by .re, shadowing and self-shadowing, using 
roughly 1000 blobs. Other nuances of our models can be gleaned from different renderings of steam densities. 
Figure 7 illustrates the effect of multiple scattering at different albedos, with one image having only 
single scattering performed. A constant phase function was employed, and the images in this .gure contain 
approximately 150 blobs. Solving for the diffuse intensity required one LU­decomposition,which took approximately 
one second of CPU time. Figure 10 shows four frames from an animation of an observer .ying around a backlit 
cloud with predominantly forward scattering (¯p=0 75). Figure 8(left) depicts the use of unwarped spherical 
blobs that are allowed to expand uniformly in all directions. At right, we see a much more convincing 
depiction in which the blobs are themselves warped due to advection. The cost of warping is directly 
related to solving Equation 7. Ten steps of an Euler integration proved adequate. In practice, then, 
the rendering time only increased by a factor 10. Figure 9 illustrates a more artistic use of our .re/smoke 
model. 8 Conclusions and Future Work We have presented a general model for the representation, anima­tion, 
and illumination of gaseous phenomena, paying particular attention to a model for .re. While the mathematics 
characterizing these phenomena is technically complicated, their implementa­tion is ef.cient, and a nonexpert 
user can control the evolution of complex gaseous phenomena, such as multiple .res with turbulent smoke 
and steam swirls. An issue for further work is ease of control over these phe­nomena. We stated at the 
outset that the user s control is largely based on the prior speci.cation of turbulent wind .elds. For 
mod­elling purposes, this is quite appropriate for interactive simulations involving steam. However, 
the .re model has a large set of interde­pendent parameters which are not necessarily easy to manipulate. 
Thus getting the .re to look just right can be problematic. In many cases, such as in the creation of 
heat .elds, it is quite feasible to generate wind .elds dynamically. Here, wind .elds can be specialized 
to a one or more blobs, giving them additional buoyancy, for example. Also, we would like to model the 
automatic destruction of objects while burning. We are contemplating the use of more accurate models 
for the reaction causing the .ame, so that we can model explosions more effectively. Acknowledgements 
The .nancial support of the Natural Sciences and Engineering Re­search Council of Canada and of the Information 
Technology Re­search Centre of Ontario is gratefully acknowledged. The helpful suggestions of the referees 
are greatly appreciated.  A Diffusion Approximation The diffusion approximation for diffuse intensity 
is given by [6] 00 r d(x)rJd(x)-Od(x)Jd(x)+Sd(x)=0 (11) where d(x)=((1 -O ¯tP(x)).1 p/3) Od(x)=(1 -O)tP(x)and 
01 Sd(x)= tP(x)J(x)-d(x)rJ(x)+Q(x) ss The directional coef.cient in the angular expansion is proportional 
to the gradient of Jd0: 01 J1 d(x) = d(x)(-rJd(x)+tP(x)Js(x)) (12) For the exact form of the boundary 
condition see [23], for example.  References [1] P. Blasi, B. Le Saec, and C. Schlick. A Rendering Algorithm 
for Discrete Volume Density Objects .Computer Graphics Forum, 12(3):201 210, 1993. [2] J. F. Blinn. 
Light Re.ection Functions for Simulation of Clouds and Dusty Surfaces .ACM Computer Graphics (SIGGRAPH 
82), 16(3):21 29, July 1982. [3] J. D. Buckmaster, editor. Frontiers in Applied Mathematics. The Mathematics 
of Combustion. SIAM, Philadelphia, 1985. [4] N. Chiba, K. Muraoka, H. Takahashi, and M. Miura. Two­dimensional 
Visual Simulation of Flames, Smoke and the Spread of Fire . The Journal of Visualization and Computer 
Animation, 5:37 53, 1994. [5] J. Chomiak. Combustion. A Study in Theory, Fact and Application. Abacus 
Press/Gordon and Breach Science Publishers, New York, 1990. [6] J.J. Duderstadt and W.R.Martin. Transport 
Theory. John Wiley and Sons, New York, 1979. [7] D. S. Ebert and R. E. Parent. Rendering and Animation 
of Gaseous Phenomena by Combining Fast Volume and Scanline A-buffer Tech­niques .ACM ComputerGraphics(SIGGRAPH 
90), 24(4):357 366, August 1990. [8] M. Inakage. A Simple Model of Flames . InProceedings of Com­puter 
Graphics International89,pages71 81.Springer-Verlag, 1989. [10] J. T. Kajiya and B. P. von Herzen. Ray 
Tracing Volume Densities . ACM Computer Graphics (SIGGRAPH 84), 18(3):165 174, July 1984. [11] E. Langu´enou, 
K.Bouatouch, and M.Chelle. Global illumination in presence of participating media with general properties. 
In Proceed­ings of the 5th Eurographics Workshop on Rendering, pages 69 85, Darmstadt, Germany, June 
1994. [12] M. Levoy. Ef.cient Ray Tracing of Volume Data .ACM Transac­tions on Computer Graphics, 9(3):245 
261, July 1990. [13] J. J. Monaghan. Why Particle Methods Work .SIAM Journal of Scienti.c and Statistical 
Computing, 3(4):422 433, December 1982. [14] K. Perlin. An Image Synthesizer .ACM Computer Graphics (SIG-GRAPH 
85), 19(3):287 296, July 1985. [15] C.H.PerryandR.W.Picard. SynthesizingFlamesandtheirSpread . SIGGRAPH 
94 Technical Sketches Notes, July 1994. [16] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. 
Vetterling. Numerical Recipes in C. The Art of Scienti.c Computing. Cambridge University Press, Cambridge, 
1988. [17] W. T. Reeves. Particle Systems. A Technique for Modeling a Class of Fuzzy Objects . ACM Computer 
Graphics (SIGGRAPH 83), 17(3):359 376, July 1983. [18] H. E. Rushmeier and K. E. Torrance. The Zonal 
Method for Calculat­ing Light Intensities in the Presence of a Participating Medium .ACM Computer Graphics 
(SIGGRAPH 87), 21(4):293 302, July 1987. [19] G. Sakas. Fast Rendering of Arbitrary Distributed Volume 
Densi­ties . In F. H. Post and W. Barth, editors,Proceedings of EURO-GRAPHICS 90, pages 519 530. Elsevier 
Science Publishers B.V. (North-Holland), September 1990. [20] G. Sakas. Modeling and Animating Turbulent 
Gaseous Phenomena Using Spectral Synthesis .The Visual Computer, 9:200 212, 1993. [21] G. Sakas and M. 
Gerth. Sampling and Anti-Aliasing of Discrete 3-D Volume Density Textures . In F. H. Post and W. Barth, 
editors,Pro­ceedings of EUROGRAPHICS 91, pages 87 102. Elsevier Science Publishers B.V. (North-Holland), 
September 1991. [22] J. Stam. Forthcoming Ph.D. thesis, Department of Computer Science, University of 
Toronto, 1995. [23] J. Stam. Multiple Scattering as a Diffusion Process . InProceedings of the 6th Eurographics 
Workshop on Rendering, Dublin, Ireland, June 1995. [24] J. Stam and E. Fiume. Turbulent Wind Fields for 
Gaseous Phenom­ena . InProceedings of SIGGRAPH 93, pages 369 376. Addison-Wesley Publishing Company, 
August 1993. [25] J. R. Wallace, K. E. Elmquist, and E. A. Haines. A Ray Trac­ing Algorithm for Progressive 
Radiosity .ACM Computer Graphics (SIGGRAPH 89), 23(3):315 324, July 1989.  Figure 5: A smokeless rendered 
.re image.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218432</article_id>
		<sort_key>137</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[Versatile and efficient techniques for simulating cloth and other deformable objects]]></title>
		<page_from>137</page_from>
		<page_to>144</page_to>
		<doi_number>10.1145/218380.218432</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218432</url>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[collision detection]]></kw>
			<kw><![CDATA[collision response]]></kw>
			<kw><![CDATA[deformable surfaces]]></kw>
			<kw><![CDATA[mechanical simulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14015481</person_id>
				<author_profile_id><![CDATA[81100007341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pascal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Volino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIRAlab, C.U.I., University of Geneva, 24, Rue du G&#233;n&#233;ral Dufour, 1211 Gen&#232;ve - Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P192065</person_id>
				<author_profile_id><![CDATA[81100186080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Courchesne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[H.E.C., University of Montreal, 5255, Av. D&#233;celles H3T-IU6 Montreal -Canada and MIRAlab, C.U.I., University of Geneva, 24, Rue du G&#233;n&#233;ral Dufour, 1211 Gen&#232;ve - Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P206538</person_id>
				<author_profile_id><![CDATA[81100651801]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nadia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnenat Thalmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIRAlab, C.U.I., University of Geneva, 24, Rue du G&#233;n&#233;ral Dufour, 1211 Gen&#232;ve - Switzerland and H.E.C., University of Montreal, 5255, Av. D&#233;celles H3T-IU6 Montreal -Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Amirbayat, J.W.S. Hearle, "The Complex Buckling of Flexible Sheet Material - Partl : Theoretical Approach", Int. J. Mech. Sci., 28(6), pp 339- 358, 1986.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90974</ref_obj_id>
				<ref_obj_pid>90967</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Aono, "A Wrinkle Propagation Model for Cloth", Computer Graphics International Proc., Springer-Verlag, 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Baraff, "Curved Sulfaces and Coherence for Non-Penetrating Rigid Body Simulation", Computer Graphics (proc. SIGGRAPH'90), 24(4), pp 19- 28, 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134084</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[D. Baraff, A. Witkin, "Dynamic Simulation of Non-Penetrating Flexible Bodies", Computer Graphics (proc. SIGGRAPH'92), 26(2), pp 303-308, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Baraff, A. Witkin, "Global Methods for Simulating Flexible Bodies", Computer Animation Proc., Springer-Verlag, pp 1-12, 1994.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D.E. Breen, D.H. House, M.J. Wozny, "Predicting the Drape of Woven Cloth using Interacting Particles", Computer Graphics (proc. SIGGRAPH'94), .28(4), pp 365-372, 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>140548</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[R.Barzel, "Physically-Based Modeling for Computer Graphics", Academic Press, 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[C.R. Calladine, "Theory of Shell Structures", Cambridge University Press, 1983.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J.F. Canny, D. Manoeha, "A new approach for Sulface Intersection", International journal of Computational Geometry and Applications, 1(4), pp 491-516, 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134017</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Carignan, Y. Yang, N. Magnenat Thalmann, D. Thalmann, "Dressing Animated Synthetic Actors with Complex Deformable Clothes", Computer Graphics (proc. SIGGRAPH'92), 26(2), pp 99-104, 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[E.F. Denby, "The Deformation of Fabrics during Wrinkling - A Theoretical Approach", Textile Reserch Journal, Landcaster PA., 46, pp 667-670, 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>653825</ref_obj_id>
				<ref_obj_pid>645465</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[S.G. Dhande, P.V.M. Rao, S. Tavakkoli, C.L. Moore, "Geometric Modeling of Draped Fabric Sulfaces", IFIP Trans. Graphics Design and Visualisation, North Holland, pp 349-356, 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134027</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[T. Duff, 'Tnterval Arithmetic and Recursive Subdivision for Implicit Functions and Constructive Solid Geometry", Computer Graphics (proc. SIGGRAPH'92), 26(2), pp 131-138, 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>653818</ref_obj_id>
				<ref_obj_pid>645465</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K.L. Gay, L. Ling, M. Damodaran, "A Quasi-Steady Force Model for Animating Cloth Motion", IFIP Trans. Graphics Design and Visualisation, North Holland, pp 357-363, 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1076572</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[H. Gould, J. Tobochnik, "An introduction to computer simulation methods: Applications to physical systems", Reading Mass., Addison-Wesley, 1988.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[D.R. Haumann, R.E. Parent, "The Behavioral Test-Bed: Obtaining Complex Behavior With Simple Rules", The Visual Computer, Springer- Verlag, 4, pp 332-347, 1988.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D.E. Breen, D. H. House, P. H. Getto, "A Physically-based Particle Models of woven cloth", The Visual Computer, 8(5-6), Springer-Verlag, Heidelberg, pp 264-277, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90703</ref_obj_id>
				<ref_obj_pid>90692</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[T.L. Kunii, H.Gotoda, "Modeling and Animation of Garment Wrinkle Formation processes", Computer Animation Proc., Springer-Verlag, pp 131- 146, 1990.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[B. Lafleur, N. Magnenat Thalmann, D. Thalmann, "Cloth Animation with Self-Collision Detection", Proc. of the IFIP conference on Modeling in Computer Graphics (proc. SIGGRAPH'91), Springer, pp 179-187, 1991.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[M.C. Lin, D. Manoeha, 'Tntelference Detection between Curved Objects for Computer Animation", Computer.Animation Proc., Springer-Verlag, pp 43-55, 1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[A.H. Manich, M.D. De Castellar, "Elastic Recovery of Polyester Staple Fiber Rotor Spun Yarn", Textile Research Journal, Landcaster PA., 62, pp 196-199, 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Moore, J. Wilhelms, "Collision Detection and Response for Computer Animation", Computer Graphics (proc. SIGGRAPH'88), 22(4), pp 289-298, 1988.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[W.E. Morton, J.W.S. Hearle, "Physical properties of textile fibers", Manchester and London, The textile institute, Butterworths, 1962.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[L.F. Palazzi, D.R. Forsey, "A Multilevel Approach to Smface Response in Dynamically Deformable Models", Computer Animation Proc., Springer- Verlag, pp 21-30, 1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[D. Rosenthal, "Resistance and Deformation of Solid Media", N-Y, Pergamon Press, 1974.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[M. Shinya, M.C. Forgue, "Interference Detection through Rasterisation", The journal of Visualisation and Computer Animation, J. Wiley &amp; Sons, 4(2), pp 132-134, 1991.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166158</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J.M. Snyder, A.R. Woodbury, K. Fleisher, B. Currin, A.H. Barr, "Interval Methods for Multi-Point Collisions between Time-Dependant Curved Smfaces", Computer Graphics annual series, pp 321-334, 1993.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37427</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos, J.C. Platt, H. Bar, "Elastically Deformable Models", Computer Graphics (proc. SIGGRAPH'87), 21, pp 205-214, 1987.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos, K. Fleischer, "Modeling Inelastic Deformation: Viscoelasticity, Plasticity, Fracture", Computer Graphics (proc. SIGGRAPH'88), 22, pp 269-278, 1988.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[S. Timoshenko, J. N. Goodier, "Theory of Elasticity", 3rd ed., N-Y, McGraw-Hill, 1970.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97883</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[B. Von Herzen, A.H. Barr, H.R. Zatz, "Geometric Collisions for Time- Dependant Parametric Surfaces", Computer Graphics (proc. SIGGRAPH'90), 24(4), pp 39-48, 1990.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[P. Volino, N. Magnenat Thalmann, "Efficient Self-Collision Detection on Smoothly Discretised Sulface Animations using Geometrical Shape Regularity", Computer Graphics Forum (EuroGraphics Proc.), 13(3), pp 155-166, 1994.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>135556</ref_obj_id>
				<ref_obj_pid>129873</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[R.C. Webb, M.A. Gigante, "Using Dynamic Bounding Volume Hierarchies to improve Efficiency of Rigid Body Simulations", Computer Graphics International Proc., Springer-Verlag, pp 825-841, 1992.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[R.C. Webb, M.A. Gigante, "Distributed, Multi-Person, Physically-Based Interaction in Virtual Worlds", Computer Graphics International Proc., Springer-Verlag, pp 41-48, 1993.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15891</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[J. Well, "The synthesis of Cloth Objects", Computer Graphics (proc. SIGGRAPH'86), 4, pp 49-54, 1986.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>653807</ref_obj_id>
				<ref_obj_pid>645465</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[H.M. Werner, N. Magnenat Thalmann, D. Thalmann, "User Interface for Fashion Design", Graphics Design and Visualisation, IFIP Trans. North Holland, pp 197-204, 1993.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>565650</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[A. Witkin, W. Welch, "Fast Animation and Control of Non-Rigid Structures", Computer Graphics (proc. SIGGRAPH'90), 24, pp 243-252, 1990.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325224</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[F. Yamaguchi, "An Unified Approach to Intelference Problems using a Triangle Processor", Computer Graphics (proc. SIGGRAPH'85), 19, pp 141-149, 1985.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Y. Yang, N. Magnenat Thalmann, "An Improved Algorithm for Collision Detection in Cloth Animation with Human Body", Computer Graphics and Applications (Pacific Graphics Proc.), World Scientific Publishing, 1, pp 237-251, 1993.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[M. Zyda, D. Pratt, W. Osborne, J. Monahan, "Real-Time Collision Detection and Response", The journal of visualisation and Computer Animation, 4(1), J. Wiley &amp; Sons, pp 13-24, 1993.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Versatile and Efficient Techniques for Simulating Cloth and Other Deformable Objects Pascal VOLINO (*), 
Martin COURCHESNE (**) (*) and Nadia Magnenat THALMANN (*) (**) ABSTRACT We are presenting techniques 
for simulating the motion and the deformation of cloth, fabrics or, more generally, deformable surfaces. 
Our main goal is to be able to simulate any kind of surface without imposing restrictions on shape or 
geometrical environment. In particular, we are considering difficult situations with respect to deformations 
and collisions, like wrinkled fabric falling on the ground. Thus, we have enhanced existing algorithms 
in order to cope with any possible situation. A mechanical model has been implemented to deal with any 
irregular triangular meshes, handle high deformations despite rough discretisation, and cope with complex 
interacting collisions. Thus, it should deal efficiently with situations where nonlinearities and discontinuities 
are really non marginal. Collision detection has also been improved to efficiently detect self-collisions, 
and also to correctly consider collision orientations despite the lack of surface orientation information 
from preset geometrical contexts, using consistency checking and correction. We illustrate these features 
through simulation examples. Keywords: deformable surfaces, collision detection, collision response, 
mechanical simulation, animation. 1 - INTRODUCTION Many efforts have already been made to represent 
the natural motion of deformable objects under relatively simple contact constraints with the environment 
and restricted deformation situations. But real life situations provide a wide range of (*) MIRAlab, 
C.U.I., University of Geneva 24, Rue du Général Dufour 1211 Genève - Switzerland Tel: (41)22 705 7763 
Fax: (41)22 705 7780 E-mail: pascal@cui.unige.ch thalmann@uni2a.unige.ch  (**) H.E.C., University of 
Montreal 5255, Av. Décelles H3T-IU6 Montreal - Canada E-mail: martin@athos.miralab.hec.ca Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 complexities, such 
as crumpling when an object falls to the ground, high deformations, wrinkling and friction when for example 
a synthetic actor puts on a cloth. Previous work has emphasized the precision of the mechanical model 
and the realism of the deformations while restricting animation contexts. However, to create scenarios 
with wrinkle and crumple situations where the deformations are ruled by lots of bendings and collisions, 
we need an efficient model able to deal with high deformations despite rough triangulation, where friction 
and collision are handled in a robust way. In addition, the model should not restrict simulated objects 
to particular situations shapes, therefore allowing objects to be composed of both regular and irregular 
meshes. Previously, collision detection was often handled in very simple ways, subject to geometrical 
optimizations made possible by the simplicity of the situation (for example, some parts of the garment 
colliding with some parts of the body). The response was directed by simple geometrical considerations 
(vertex-to-triangle repulsion and friction, inside-outside orientation). But if we consider very general 
wrinkling situations, collision detection becomes a very difficult and time consuming task, because the 
lack of geometrical context prevents the use of optimizations. It also becomes difficult maintaining 
orientation consistency between the detected collisions. Our main goal is to develop a very versatile 
and robust mechanical model, that is specially designed to rapidly and easily simulate deformable surface 
motion in any situation. It will be associated with a powerful and general collision detection and handling 
system that does not require any predefined context for efficiently and accurately computing collisions 
and self-collisions. Remaining as general as possible, the model should be directly applicable to any 
object such as complex garments formed by several panels stacked on several layers or, more generally, 
any other kind of surface not necessarily discretised into uniform triangulation.  2 - PREVIOUS WORK 
 Previous works on deformable object animation using physically based models have permitted animation 
of cloth-like objects in many kinds of situations. Weil [35] pioneered cloth animation using an approximated 
model based on relaxation of the surface. Haumann et al. produced animations with flags or leaves moving 
in the wind, or curtains blowing in a breeze [16]. Kunii and Gotoda used a hybrid model incorporating 
physical and geometrical techniques to model garment wrinkles [18]. Aono simulated wrinkle propagation 
on a handkerchief using an elastic model [2]. Terzopoulos and Fleischer developed a general elastic model 
and applied it to a wide range of objects including cloth [28] [29]. Interaction of clothes with synthetic 
actors in motion [19] [10] [39] marked the beginning of a new era in cloth animation in more complex 
situations. However, there were still a number of restrictions on the simulation conditions on the geometrical 
structure and the mechanical situations, imposed by the simulation model or the collision detection. 
Deformable objects may be represented by different geometrical models, Triangular grids are most common, 
but polynomial surfaces [37] [4] and particle systems [6] are also used for solutions to specific mechanical 
simulations. Yelling nice and accurate deformations, they constrain both the initial shape and the allowed 
deformations. Each model requires different techniques for modeling complex objects such as panels-and­seaming 
for cloth objects [39]. Furthermore, global mechanical models such as finite elements and finite difference 
are not suitable for situations involving constraints and nonlinearities as non-marginal situations all 
over the surfaces. These situations happen when modeling the highly nonlinear deformations required for 
wrinkles and crumples [11], and when there are numerous collisions and much friction. For coping with 
these mentioned situations, we provide a model resulting directly from the integration of Newton's motion 
equation. It allows us to efficiently and accurately integrate the effects caused by nonlinearities and 
collisions, and provides a small and efficiently computed adaptive time step, the best way for handling 
numerous discontinuities. We also remove modeling constraints by allowing simulation of any kind of non 
regular triangular meshes. Collision detection and response has been used mainly for stopping cloth from 
penetrating the body and, more marginally, for preventing self-collisions between different parts of 
the cloth. The first time-consuming problem was to extract the possible colliding elements from the whole 
set of elements composing the cloth and the body surfaces. Many techniques have been developed, based 
on different ideas and adapted for various surface representations. For example, mathematical algorithms 
have been developed for situations where the surfaces are represented by curved patches or parametrical 
surfaces, as described in [3], [4], [31], [13], [27]. In the case of representing surfaces by a huge 
set of flat polygons, techniques based on rasterisation [26] or the tracking of the closest distance 
on the convex hull [22], [20] have been developed. Unfortunately, these techniques are not well suited 
for efficient detection on deformable surface animations, as they require either expensive z­buffer rendering 
or constructing the convex hull of the objects at each frame. Closer to meeting our requirements are 
algorithms based on voxelisation or hierarchical octree subdivisions, as described in [39]. They are 
quite simple and efficient, but require a heavy data structure update at each frame. Hierarchical groupings 
for bounding-box tests have also been developed [33], [34], which are very efficient for handling a huge 
number of surface elements, but not well suited for surface self-collision detection. We propose an efficient 
algorithm for detecting collisions, especially self-collisions on animated discretised surfaces [32]. 
It takes advantage of adjacency between the elements of a hierachisation, built once during preprocessing, 
for selectively performing collision tests using a very simple surface curvature criteria. Besides being 
detected, collisions have to be handled in an accurate way for collision response. With cloth animation, 
this problem was relatively simple as long as the only situation considered was having the cloth already 
and constantly worn by the body [39]. Thus, a simple vertex-to-triangle interference using some proximity 
criteria could be considered. Furthermore, as the geometrical situation was quite constant and collision 
existed mainly when the cloth penetrated the body and wrinkles penetrated each other, the considered 
surfaces being already "oriented". Simple geometrical considerations could determine that a vertex had 
crossed, or was at the correct side of a polygon. Unfortunately, as we are now considering very general 
and non-restrictive situations where any surface can collide with any other, considering what and how 
elements are colliding becomes a nontrivial problem for obtaining correct collision response. Difficulties 
arise when considering the lack of preset orientation information among the surfaces. This leads us to 
consider techniques for tracking collision orientation, as well as techniques which cope with and correct 
any orientation inconsistencies that may arise. 3 - THE MECHANICAL MODEL The main idea of our model 
is to integrate Newton's motion equation in a direct way to keep quickly evaluated time steps small and 
very frequent collision detection. More sophisticated and time consuming models based on global minimizations 
or Lagrangian dynamics formulations allowing higher time step would represent a waste of time. Thus, 
discontinuous responses such as collisions will be handled in an accurate way. Furthermore, this direct 
formulation allows us easy and precise inclusion of any nonlinear mechanical behavior. With such a model, 
we can also act directly on the position and speed of the elements, and thus avoid handling collisions 
through strong repulsion forces that perturb the simulation. The animated deformable object is represented 
as a particle system by sets of vertices forming irregular triangles, thus allowing surfaces of any shape 
to be easily modeled and simulated.  3.1 - Description of the physical object The object is considered 
to be isotropic and of constant thickness. Elastic properties of the object are mainly described by the 
standard parameters [23] that are: E the Young modulus . the Poisson coefficient . the density T the 
thickness Rough discretisation, however, alters the behavior of the surface. In particular, heterogeneous 
triangulations are "rigidifying" the whole surface, preventing easy buckling. These effects have to be 
corrected through tuning and adjustments of the mechanical parameters. In particular, textile easily 
buckles into double curvature, but buckle formation requires a change of area that increases with the 
size of the discretised elements [8]. To facilitate buckle formation on roughly discretised objects without 
loosing textile stretching stiffness, we use a variable Young modulus for reducing the stretching stiffness 
for compression and small extension. 3.2 - The motion equation Using Newton's second law F=ma, the motion 
equation consists of a pair of coupled first-order differential equations for position and velocity. 
The system of equations is resolved using the second order (midpoint method) of the Euler-Cromer method 
[15]. The constraints implied in deformable object motion are divided in two categories: * Continuous 
constraints including internal and some external ones such as wind and gravity. * Discontinuous constraints 
resulting from collisions with other objects.  Discontinuous constraints induce instantaneous change 
in the state of the object. Considering the collision frequency, the interruption of the simulation every 
time a collision occurs would take much computation. Rather than considering complicated methods for 
solving differential equations with discontinuities [7] which may not be efficient for very complex collision 
situations, we prefer handling collisions separately. The problem of solving the differential equations 
has been simplified considerably using a two phase process, similarly to House et al. [17]: a -Considering 
only the continuous constraints, differential equations are solved using a time step that ensures mechanical 
stability for every vertices, computed from the acceleration of the vertices versus the length of their 
connected edges. However, we do not recompute acceleration at each step for vertices which do not require 
such a small time step. b -Then, collisions are detected and discontinuous constraints are handled, through 
direct correction of position and speed complying momentum transfer laws.  3.3 - Internal strains Internal 
strains are either in-plane, from planar extension and shearing, or out-of-plane, from bending and twisting. 
Considering the irregularity of the triangle mesh, the force evaluation should be independent from the 
size and shape of the triangles. * Elastic and shearing strain A triangle is considered as a thin flat 
object in a plane stress situation. Each edge of the triangle is taken as a strain gauge giving strain 
measurement on the cloth surface (fig. 1). A set of three measurements, called "strain rosette" [30], 
is enough for completely evaluating the strain. The unit elongations given by each edge at an angle . 
i are related using: [25] e. i = e u cos2 . i+ e v sin2. i+ 2 . uv sin. i cos. i (1) We compute the unit 
elongations (e u, e v) and shear (. uv) in an arbitrary, conveniently defined (u,v) coordinate system. 
Then, the Hook law for a uniform isotropic material [30] directly gives the stress components: E E s 
u,v = t uv = G . uv = e u,v+.e v,u . uv . (2) (3) 1-. 2 s t uv v  s u m2 m 1 s u v t uv m0 
t uv s v u Fig. 1 : Stress evaluation in a triangle The stress components on a triangle are convert 
into in-plane forces along its edges. The force applied on the edge j of the triangle i is: Fij = T Lj 
 mj·u s u -mj ·v t uv u + mj·v s + mj ·u t uv v v (4) where mj is a unit vector perpendicular to the 
edge j in the triangle plane. This force is then equally distributed on the two extremity vertices. * 
Bending strain Curvature force are very weak compared to in-plane forces. As we intend to consider high 
deformation, the force evaluation must consider the possible case where the radius of curvature is less 
than the size of the triangles. The edge between two triangles is used as a hinge for curvature manifestation 
(fig. 2a) providing information on single curvature only. It is known from the Mohr circle that it is 
always possible to decompose any twist strain into a combination of pure bending strains [8]. Considering 
the arbitrary orientation of the edges, even if we have no control on them, twist strain is taken into 
account via the additive property of curvature. a) 3 Fig. 2 : Curvature evaluation. Using the angle 
between the normals (fig. 2a), we look for the maximum curvature radius (R) for which the corresponding 
arc fits inside the triangles. Referring to fig. 2b, h is less than or equal to the height of the triangles 
and L is greater than h*a, with a<1. This adaptation will allow R to reach values smaller than the size 
of the triangles. The local curvature (K) is the inverse of R. To prevent K and the bending force from 
reaching infinity, we limit K to a maximal value. If the angle continues to grow, a specific high bending 
constraint handling will be performed. The associated momentum in width units is: T3 M = D'DK = E D' 
 K . 2 (5) where D, the flexural rigidity, is associated with D' an the extra parameter which is needed 
to allow fine tuning of bending strain. Using D'=1 would be appropriated for continuous solid sheets 
but would not reflect the real comportment of textile [1]. The material is still isotropic. The force 
corresponding to M is obtained using the triangle dimensions and normal. 3.4 - Time effect Textile material 
is not purely elastic and its response to stress depends on its straining history. Several phenomena, 
some of which are described in [23] and [21], yield time effects that appear as recovery behavior, creep, 
stress relaxation of stress, etc. Integral and analytical theories aim to describe mathematically the 
macroscopic behavior of material. Since permanent or semi­permanent damage is closely related to the 
quality and straining history of fiber and textile's structure, an exact representation of them is impossible. 
Rather than trying to idealize the behavior with complex equations, we developed a simple empirical equation 
to model the consequences. Basically, pure elastic behavior for a deformation "x(t)-x(0)" of an element 
at a time t is described by Hook's law: F(t) = k ( x(t) - x(0) ) (6) which is added a linear viscoelastic 
response according to Newton's law: x(t) - x(t-dt)Fv(t) = . x dt (7) where k and . x are elasticity 
and viscosity constants. When deformation exceeds a given ratio, we switch from the viscoelastic behavior 
described above to a plastic behavior for which the equilibrium x(0) is moved to a new value, modeling 
permanent deformation. A relaxation time has also been defined, defining how fast the equilibrium evolves 
back to its original value, as soon as the deforming constraints are released.  3.5 - Collision management 
We correct the non-constrained simulation by detecting and taking the collision effects into account. 
Rather than computing "collision forces" through inverse kinematics from the momentum conservation law 
[10], we directly integrate the constraints by position and speed corrections on the concerned vertices 
accordingly to momentum conservation. Thus, we avoid dealing with high reaction forces that alter the 
mechanical simulation. For instance, if a collision is detected between two elements, we compute the 
new positions of these elements that satisfy both the collision geometrical constraints and the mass 
center invariance. If some elements are implied into several collisions, we iterate the process until 
all the constraints are satisfied. Then, the speed of the elements is evaluated accordingly to momentum 
conservation, using perfectly inelastic collision for the normal speed along the collision plane, and 
coulombian friction for the tangential speed. This technique ensures very fast collision response computation, 
each collision being handled independently and by avoiding high reaction forces. However, robustness 
is required for the collision detection algorithm, to maintain consistency in complex interdependent 
collisions situations where the collision response may not be able to solve completely all the constraints. 
 3.6 - Stability control As we are dealing with nonlinear models put into widely varying conditions, 
some situations (for example, deformation caused by collisions) might lead to numerical instability. 
Once we detect increasing instability by monitoring local mechanical energy variations, we artificially 
distribute kinetic energy through momentum transfers in the neighborhood of the concerned elements. This 
transfer accelerates the propagation and the fairing of the perturbation. This technique increases the 
global robustness of the system for difficult conditions. 4 - COLLISION DETECTION AND HANDLING For dealing 
with complex collision situations such as crumpling, we need efficient collision and self-collision detection, 
as well as a robust collision handling. We are now discussing these two aspects.  4.1 - Fast self-collision 
detection Collision and particularly self-collision detection is often the bottleneck of simulation applications 
in terms of calculation time, because of the scene complexity that involves a huge number of geometrical 
tests for determining which elements are colliding. In our case, the problem is complicated further because 
we are handling discretised surfaces that may contain thousands of polygons. We also are considering 
general situations where we cannot make any hypotheses about region proximities. Finally, we have to 
efficiently detect self-collisions within the surfaces. This prevents the use of standard bounding box 
algorithms because potentially colliding regions of a surface are always touching each other by adjacency. 
We have developed a very efficient algorithm for handling this situation [32]. This algorithm is based 
on hierarchisation and takes advantage of the adjacency which, combined with a surface curvature criteria, 
let us skip large regular regions from the self­collision detection. We then get a collision evaluation 
time that is roughly proportional to the number of colliding elements, and independent of the total number 
of elements that compose our deforming surfaces.  Fig. 3 : Hierarchical collision and self-collision 
detection on cloth. Less than 5% of the detection time is spent for self-collisions. 4.2 - Handling 
different kinds of collisions Once the possible colliding triangles of our surfaces are located, we extract 
different types of geometrical collisions: * Proximities They are represented by couples of elements 
that are closer than a threshold distance. That may be triangle-to-vertex, edge-to­edge, and more marginally 
edge-to-vertex and vertex-to-vertex proximities. They illustrate collision interaction. They are used 
for computing collision response. * Interferences They are represented by edges-triangle couples that 
are crossing each other. They illustrate situations where two surfaces are interpenetrating. They reveal 
inconsistent collision situations that have to be corrected.  4.3 - Collision consistency Collision 
response implies the correction of position and velocity to prevent contact and crossing. However, this 
problem cannot be efficiently resolved in complicated situations such as interaction between multiple 
collisions. It may occasionally happen that some vertices move to "the wrong side" of a colliding surface, 
a situation with which the collision response must cope. Usually, the "right side" of a vertex from a 
triangle is determined by "inside-outside" orientation assumptions for the surfaces, made possible for 
some simple collision situations or geometrical contexts (for example, the vertices of the cloth have 
to be pushed outside the body, and colliding wrinkles have the same surface orientation). However, as 
we intend to simulate cloth or any other deformable surface in any situation, such orientation information 
is not available, and we cannot deduce from vertex­triangle proximity at which side of the triangle the 
vertex should be. Consistent collision orientations X O XO OO X O  O XXO ? O Unconsistent collision 
orientations X O OO O  X OO X O O XX X = Wrong side of the triangle O = Right side of the triangle 
X = Wrong side O = Right side Fig. 4 : Orientation ambiguity and collision consistency. Our contribution 
has been to create algorithms able to correctly orient the detected collisions so as to correct any wrong 
situation. We use a combination of techniques described below. * Remnant proximities As we said, vertices 
may marginally cross the colliding surfaces, but the response must return them to the correct side, even 
if they are temporarily out of the scope of the collision detection. For solving this problem, a proximity 
will be kept in memory for a certain time after its last detection, even when the concerned elements 
move far away from each other. During this time, the proximity will be geometrically updated with respect 
to the displacements of the objects. If this collision then gets detected again, its orientation is still 
known according to its "history". * Cinematical tracking For each newly detected proximity, we compute 
the relative movement of the concerned elements, and we can know whether a crossing has happened just 
before detection or not. If not, the elements are still at right side of each other. * Consistency checking 
and correction Even using the previously mentioned techniques, it is always possible that some collisions 
get incorrect orientation due to inaccurate response. This occurs mainly in complicated cases that lead 
to geometrically incorrect situations, and therefore erroneous detection. Our algorithm should not be 
perturbed by such false situations, and should be able to correct the wrong collision orientations, whenever 
they happen. Usually a contact region between two surfaces is represented by several collisions. The 
elements concerned by this group of collisions define "collision regions" on both surfaces. Our main 
 Fig. 5 : Complex collisions (Calc time : 5 hrs). Fig. 6 : Falling without, and with collision consistency 
correction. idea is to ensure global consistency of the collision behavior within and between these regions. 
Regions are efficiently computed by neighborhood walking and labelling through all detected collisions. 
We update incrementally the regions as the surfaces are moving through the detected collisions. For a 
consistent collision situation between two surfaces: * The collision orientation should remain constant 
within the collision regions of the two surfaces, so that the whole collision group behave consistently. 
 * The two regions should be oriented accordingly, so that the surfaces are repulsing each other if they 
are at the correct side of each other, or attracting if they are not.  For determining the region orientation, 
we use a statistical evaluation of all the collision orientations within the region, according to their 
reliability (whether they have been deduced from remnance or tracking). We then force all the collisions 
of the region to the same orientation. Using this process whenever any inconsistency has been detected 
(using edge-to-triangle interference), we can efficiently correct the situation by forcing every detected 
collision to behave accordingly to the majority's choice.  4.4 - Incremental collision detection In 
situations where large surfaces collide but where the deformations and relative movements remain small, 
and when the overall geometrical configuration remains constant, recomputing proximities which have simply 
evolved in position is meaningless. We provide some incremental algorithms that will update the existing 
proximities between each animation step, using some quick geometric computations for each of them, instead 
of using the whole global detection computation. Depending of the situation, different actions are provided: 
* The proximity direction and distance is recomputed according to the displacement of the concerned elements. 
 * The proximity may evolve through neighboring elements (sliding). * The proximity is forgotten (or 
remnant) when the concerned elements are not in the detection range. * Some new proximities may appear 
from the neighboring elements of existing collisions.  We could imagine using exclusively incremental 
detection in situations involving only sliding of two permanently colliding surfaces, thus permitting 
very fast evaluation computation. However, new collision "zones" on topologically variable situations 
will not be detected. A good compromise is to alternate incremental detection with full detection, according 
to the simulation conditions. 5 - RESULTS The mechanical simulation and collision detection algorithms 
have been implemented in the C language on Silicon Graphics workstations. An animation system has been 
designed to handle moving objects coming from various animation sources (fixed and frame-by-frame animated 
objects, objects transformed by mathematical transformations, and of course objects animated by mechanical 
simulation) that interact with the other objects. Any object may be subjected to mechanical simulation 
(provided it is discretised into a triangular mesh). A cloth object is imported directly from existing 
panel design software, and is assembled using the same mechanical software. Once this process is finished 
and the cloth becomes a single object, it may be handled as any other mechanical object. We have used 
this animation system for testing our algorithms. Mainly, the following tests have been performed.  
5.1 - Mechanical properties We have simulated sets of objects with different mechanical data which illustrate 
the different mechanical properties that can be handled: In order to simulate some "exceptional" conditions 
where our cloth, or any kind of other object, is subject to variable interaction that will cause much 
random wrinkling and deformation, we put our objects in a rotating cylinder that animates them the same 
way a drying machine would. The dryer is primarily a test which validates the efficiency and the robustness 
of the collision detection algorithm. Any kind of collision configuration may occur, and interaction 
between different collisions is high. Secondly, it is a good test for the validation of the mechanical 
model under the high deformations caused by collisions between deforming objects, as well as for collision 
response and friction. It also verifies the numerical stability of the model. Fig. 7 : Crumpling garments 
in the dryer. Calc time : 8 hrs for 1 min animation.  5.2 - Cloth assembly and simulation To define 
cloth objects, we use existing software for designing 2D panels, as described in [36]. The cloth is assembled 
according to its seaming borders by a 3D mechanical simulation where "elastics" provide forces to join 
the seaming lines together. We have considerably improved the seaming process by using well-tuned viscoelastic 
forces, which also simulate transversal viscosity and damping for directing the panel borders straight 
to the destination. Once the seaming lines are close enough, we engage a "hard" seam by topologically 
merging together the corresponding vertices and edges to obtain a single surface. As there is no constraint 
with respect to the discretisation of our surfaces, we can imagine building any kind of object using 
this process. Designed this way, a cloth object is handled in the simulation system the same way as any 
other mechanical object. There are no constraints according to the geometrical context (the cloth may 
not be worn by a body). We have complete freedom to decide how to manipulate this object. 6 - CONCLUSION 
AND FUTURE WORK We have developed an efficient set of techniques that allows us to simulate any kind 
of deformable surface in various mechanical situations. Our main contribution was to design algorithms 
that could handle very general, context-free situations: First by implementing a robust process for mechanical 
simulation that can cope with difficult situations involving high deformations and numerous collisions, 
and secondly by linking it to a powerful collision detection system that, in addition to good performance 
for collision and self-collision detection, is able to deal with the lack of geometrical context for 
correctly orienting collision response. Our main idea was to keep the system, which was basically designed 
for cloth simulation, as general and versatile as possible, by not restricting simulations to special 
contexts, or objects to certain shapes. Besides providing performance and robustness improvements, this 
powerful tool can be used to build applications involving garments in some very particular situations, 
like grasping and folding. Because of the versatility of the simulation algorithm, we can further imagine 
not only simulating the cloth worn by the body, but also trying to realistically reproduce an actor grasping 
his/her clothes and dressing himself/herself. ACKNOWLEDGMENTS This work is supported by the Swiss National 
Research Foundation and the European ESPRIT HUMANOID2 project. Thanks to all the people that have contributed 
to it, and particularly to Jean-Claude Mousally for his assistance in preparing some illustrations and 
Hans-Martin Werner for reviewing the English text. Fig. 8 : Seaming garment panels around tho body (Calc 
time : 10 min). Fig. 9 : Calc time : 5 hrs for 15 sec animation. [29] : D. Terzopoulos, K. Fleischer, 
"Modeling Inelastic Deformation:BIBLIOGRAPHY Viscoelasticity, Plasticity, Fracture", Computer Graphics 
(proc. [1] : J. Amirbayat, J.W.S. Hearle, "The Complex Buckling of Flexible Sheet Material - Part1 : 
Theoretical Approach", Int. J. Mech. Sci., 28(6), pp 339­358, 1986. [2] : M. Aono, "A Wrinkle Propagation 
Model for Cloth", Computer Graphics International Proc., Springer-Verlag, 1990. [3] : D. Baraff, "Curved 
Surfaces and Coherence for Non-Penetrating Rigid Body Simulation", Computer Graphics (proc. SIGGRAPH'90), 
24(4), pp 19­28, 1990. [4] : D. Baraff, A. Witkin, "Dynamic Simulation of Non-Penetrating Flexible Bodies", 
Computer Graphics (proc. SIGGRAPH'92), 26(2), pp 303-308, 1992. [5] : D. Baraff, A. Witkin, "Global Methods 
for Simulating Flexible Bodies", Computer Animation Proc., Springer-Verlag, pp 1-12, 1994. [6] : D.E. 
Breen, D.H. House, M.J. Wozny, "Predicting the Drape of Woven Cloth using Interacting Particles", Computer 
Graphics (proc. SIGGRAPH'94), .28(4), pp 365-372, 1994. [7] : R.Barzel, "Physically-Based Modeling for 
Computer Graphics", Academic Press, 1992. [8] : C. R. Calladine, "Theory of Shell Structures", Cambridge 
University Press, 1983. [9] : J.F. Canny, D. Manocha, "A new approach for Surface Intersection", International 
journal of Computational Geometry and Applications, 1(4), pp 491-516, 1991. [10] : M. Carignan, Y. Yang, 
N. Magnenat Thalmann, D. Thalmann, "Dressing Animated Synthetic Actors with Complex Deformable Clothes", 
Computer Graphics (proc. SIGGRAPH'92), 26(2), pp 99-104, 1992. [11] : E. F. Denby, "The Deformation of 
Fabrics during Wrinkling - A Theoretical Approach", Textile Reserch Journal, Landcaster PA., 46, pp 667-670, 
1976. [12] : S.G. Dhande, P.V.M. Rao, S. Tavakkoli, C.L. Moore, "Geometric Modeling of Draped Fabric 
Surfaces", IFIP Trans. Graphics Design and Visualisation, North Holland, pp 349-356, 1993. [13] : T. 
Duff, "Interval Arithmetic and Recursive Subdivision for Implicit Functions and Constructive Solid Geometry", 
Computer Graphics (proc. SIGGRAPH'92), 26(2), pp 131-138, 1992. [14] : K.L. Gay, L. Ling, M. Damodaran, 
"A Quasi-Steady Force Model for Animating Cloth Motion", IFIP Trans. Graphics Design and Visualisation, 
North Holland, pp 357-363, 1993. [15] : H. Gould, J. Tobochnik, "An introduction to computer simulation 
methods: Applications to physical systems", Reading Mass., Addison-Wesley, 1988. [16] D.R. Haumann, R.E. 
Parent, "The Behavioral Test-Bed: Obtaining Complex Behavior With Simple Rules", The Visual Computer, 
Springer-Verlag, 4, pp 332-347, 1988. [17] : D. E. Breen, D. H. House, P. H. Getto, "A Physically-based 
Particle Models of woven cloth", The Visual Computer, 8(5-6), Springer-Verlag, Heidelberg, pp 264-277, 
1992. [18] T.L. Kunii, H.Gotoda, "Modeling and Animation of Garment Wrinkle Formation processes", Computer 
Animation Proc., Springer-Verlag, pp 131­146, 1990. [19] : B. Lafleur, N. Magnenat Thalmann, D. Thalmann, 
"Cloth Animation with Self-Collision Detection", Proc. of the IFIP conference on Modeling in Computer 
Graphics (proc. SIGGRAPH'91), Springer, pp 179-187, 1991. [20] : M.C. Lin, D. Manocha, "Interference 
Detection between Curved Objects for Computer Animation", Computer.Animation Proc., Springer-Verlag, 
pp 43-55, 1993. [21] : A.H. Manich, M.D. De Castellar, "Elastic Recovery of Polyester Staple Fiber Rotor 
Spun Yarn", Textile Research Journal, Landcaster PA., 62, pp 196-199, 1992. [22] : M. Moore, J. Wilhelms, 
"Collision Detection and Response for Computer Animation", Computer Graphics (proc. SIGGRAPH'88), 22(4), 
pp 289-298, 1988. [23] : W.E. Morton, J.W.S. Hearle, "Physical properties of textile fibers", Manchester 
and London, The textile institute, Butterworths, 1962. [24] : L.F. Palazzi, D.R. Forsey, "A Multilevel 
Approach to Surface Response in Dynamically Deformable Models", Computer Animation Proc., Springer-Verlag, 
pp 21-30, 1994. [25] : D. Rosenthal, "Resistance and Deformation of Solid Media", N-Y, Pergamon Press, 
1974. [26] : M. Shinya, M.C. Forgue, "Interference Detection through Rasterisation", The journal of Visualisation 
and Computer Animation, J. Wiley &#38; Sons, 4(2), pp 132-134, 1991. [27] : J.M. Snyder, A.R. Woodbury, 
K. Fleisher, B. Currin, A.H. Barr, "Interval Methods for Multi-Point Collisions between Time-Dependant 
Curved Surfaces", Computer Graphics annual series, pp 321-334, 1993. [28] : D. Terzopoulos, J.C. Platt, 
H. Bar, "Elastically Deformable Models", Computer Graphics (proc. SIGGRAPH'87), 21, pp 205-214, 1987. 
SIGGRAPH'88), 22, pp 269-278, 1988. [30] : S. Timoshenko, J. N. Goodier, "Theory of Elasticity", 3rd 
ed., N-Y, McGraw-Hill, 1970. [31] : B. Von Herzen, A.H. Barr, H.R. Zatz, "Geometric Collisions for Time-Dependant 
Parametric Surfaces", Computer Graphics (proc. SIGGRAPH'90), 24(4), pp 39-48, 1990. [32] : P. Volino, 
N. Magnenat Thalmann, "Efficient Self-Collision Detection on Smoothly Discretised Surface Animations 
using Geometrical Shape Regularity", Computer Graphics Forum (EuroGraphics Proc.), 13(3), pp 155-166, 
1994. [33] : R.C. Webb, M.A. Gigante, "Using Dynamic Bounding Volume Hierarchies to improve Efficiency 
of Rigid Body Simulations", Computer Graphics International Proc., Springer-Verlag, pp 825-841, 1992. 
[34] : R.C. Webb, M.A. Gigante, "Distributed, Multi-Person, Physically-Based Interaction in Virtual Worlds", 
Computer Graphics International Proc., Springer-Verlag, pp 41-48, 1993. [35] : J. Weil, "The synthesis 
of Cloth Objects", Computer Graphics (proc. SIGGRAPH'86), 4, pp 49-54, 1986. [36] : H.M. Werner, N. Magnenat 
Thalmann, D. Thalmann, "User Interface for Fashion Design", Graphics Design and Visualisation, IFIP Trans. 
North Holland, pp 197-204, 1993. [37] : A. Witkin, W. Welch, "Fast Animation and Control of Non-Rigid 
Structures", Computer Graphics (proc. SIGGRAPH'90), 24, pp 243-252, 1990. [38] : F. Yamaguchi, "An Unified 
Approach to Interference Problems using a Triangle Processor", Computer Graphics (proc. SIGGRAPH'85), 
19, pp 141-149, 1985. [39] : Y. Yang, N. Magnenat Thalmann, "An Improved Algorithm for Collision Detection 
in Cloth Animation with Human Body", Computer Graphics and Applications (Pacific Graphics Proc.), World 
Scientific Publishing, 1, pp 237-251, 1993. [40] : M. Zyda, D. Pratt, W. Osborne, J. Monahan, "Real-Time 
Collision Detection and Response", The journal of visualisation and Computer Animation, 4(1), J. Wiley 
&#38; Sons, pp 13-24, 1993.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218434</article_id>
		<sort_key>145</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Feature-based control of visibility error]]></title>
		<subtitle><![CDATA[a multi-resolution clustering algorithm for global illumination]]></subtitle>
		<page_from>145</page_from>
		<page_to>152</page_to>
		<doi_number>10.1145/218380.218434</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218434</url>
		<keywords>
			<kw><![CDATA[clustering]]></kw>
			<kw><![CDATA[feature-based error metric]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[hierarchical radiosity]]></kw>
			<kw><![CDATA[multi-resolution visibility]]></kw>
			<kw><![CDATA[progressive multi-gridding]]></kw>
			<kw><![CDATA[visibility error]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.5.3</cat_node>
				<descriptor>Algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.1.2</cat_node>
				<descriptor>Analysis of algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010148.10010149</concept_id>
				<concept_desc>CCS->Computing methodologies->Symbolic and algebraic manipulation->Symbolic and algebraic algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010258.10010260.10003697</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Learning paradigms->Unsupervised learning->Cluster analysis</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P84813</person_id>
				<author_profile_id><![CDATA[81100402503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fra&#326;ois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sillion]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNRS, iMAGIS, B.P. 53, 38041 Grenoble Cedex 9, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40026929</person_id>
				<author_profile_id><![CDATA[81100408270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drettakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ERCIM-INRIA, iMAGIS, B.P. 53, 38041 Grenoble Cedex 9, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192179</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James Arvo, Kenneth Torrance, and Brian Smits. A framework for the analysis of error in global illumination algorithms. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH '94 (Orlando, FL), pages 75-84, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Michael E Cohen and John R. Wallace. Radiosity and Realistic Image Synthesis. Academic Press, Boston, 1993.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>31469</ref_obj_id>
				<ref_obj_pid>31468</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Goldsmith and J. Salmon. Automatic creation of object hierarchies for ray tracing. IEEE Computer Graphics and Applications, 7(5):14-20, May 1987.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Eric A. Haines. Shaft culling for efficient ray-traced radiosity. In R Brunet and EW. Jansen, editors, Photorealistic Rendering in Computer Graphics, pages 122-138. Springer Verlag, 1993. Proceedings of the Second Eurographics Workshop on Rendering (Barcelona, Spain, May 1991).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan, David Saltzman, and Larry Aupperle. A rapid hierarchical radiosity algorithm. Computer Graphics, 25(4):197-206, August 1991. Proceedings SIGGRAPH '91 in Las Vegas.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Nicolas Holzschuch, Francois Sillion, and George Drettakis. An efficient progressive refinement strategy for hierarchical radiosity. In Fifth Eurographics Workshop on Rendering, Darmstadt, Germany, June 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Arjan J. F. Kok. Grouping of patches in progressive radiosity. In Proceedings of Fourth Eurographics Workshop on Rendering, pages 221-231. Eurographics, June 1993. Technical Report EG 93 RW.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192176</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dani Lischinski, Brian Smits, and Donald R Greenberg. Bounds and error estimates for radiosity. In Computer Graphics P1vceedings, Annual Conference Series: SIGGRAPH '94 (Orlando, FL), pages 67-74, July 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>142453</ref_obj_id>
				<ref_obj_pid>142443</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Dani Lischinski, Filippo Tampieri, and Donald R Greenberg. Discontinuity meshing for accurate radiosity. IEEE Computer Graphics and Applications, 12(6):25-39, November 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166143</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Dani Lischinski, Filippo Tampieri, and Donald R Greenberg. Combining hierarchical radiosity and discontinuity meshing. In Computer Graphics P~vceedings, Annual Conference Series: SIGGRAPH '93 (Anaheim, CA), pages 199-208, August 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Holly Rushmeier, Charles Patterson, and Aravindan Veerasamy. Geometric simplification for indirect illumination calculations. In P~vceedings Graphics Interface ' 93. Morgan Kaufmann, 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1098652</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Serra. Image analysis and mathematical morphology : 1. Academic Press, London, 1982.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>614311</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Francois Sillion. A unified hierarchical algorithm for global illumination with scattering volumes and object clusters, to appear in IEEE Transactions on Visualization and Computer Graphics, 1(3), September 1995. (a preliminary version appeared in the fifth Eurographics workshop on rendering, Darmstadt, Germany, June 1994).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Francois Sillion, George Drettakis, and Cyril Soler. A clustering algorithm for radiance calculation in general environments. In Sixth Eurographics Workshop on Rendering, Dublin, Ireland, June 1995.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>561383</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Francois Sillion and Claude Puech. Radiosity and Global Illumination. Morgan Kaufmann publishers, San Francisco, 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192277</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Brian Smits, James Arvo, and Donald R Greenberg. A clustering algorithm for radiosity in complex environments. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH '94 (Orlando, FL), pages 435-442, July 1994.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134080</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Brian E. Smits, James R. Arvo, and David H. Salesin. An importance-driven radiosity algorithm. Computer Graphics, 26(4):273-282, July 1992. Proceedings of SIGGRAPH '92 in Chicago.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166148</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Seth J. Teller and Patrick M. Hanrahan. Global visibility algorithms for illumination computations. In Computer Graphics P1vceedings, Annual Conference Series: SIGGRAPH '93 (Anaheim, CA), pages 239-246, August 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74366</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[John R. Wallace, Kells A. Elmquist, and Eric A. Haines. A ray tracing algorithm for progressive radiosity. Computer Graphics, 23(3):315-324, July 1989. Proceedings SIGGRAPH '89 in Boston.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 bounds are always valid, their use results in unnecessary work being done to narrow down other error 
bounds by increasing the subdivi­sion. Global visibility algorithms can be used to exploit the struc­ture 
of architectural scenes and produce guaranteed visibility infor­mation [18], but they are not suited 
to large collections of indepen­dent objects. For exchanges between surfaces, discontinuity mesh­ing 
also provides explicit visibility information, and indeed consid­erably improves the ef.ciency of HR 
[10]. However for Monte-Carlo or clustering approaches it is either impossible or impractical to calculate 
analytic visibility and error bounds must be used. For exchanges between clusters, an approximate visibility 
estimate can be derived using equivalent volume extinction properties [13], but the error introduced 
in the process has not yet been analyzed. Visibility error is admittedly dif.cult to evaluate, since 
the com­putation of visibility itself is a costly process. Still, controlling this source of error is 
imperative since the quality of shadows plays a sig­ni.cant role in determining the user s perception 
of image quality. In complex environments where clustering is most useful, a domi­nant part of computation 
time is spent in visibility calculations in­volving small, geometrically complex objects. Resulting visibility 
variations produce .ne detail shadows, which may be of little inter­est to the user, or may be lost in 
the implicit averaging over a surface patch. Paper overview The preceding discussion has shown that a 
key issue in designing ef.cient lighting simulation systems is to provide adequate con­trol mechanisms 
to ensure that just enough work is done to meet the user s quality criteria. It appears that control 
of visibility error has not yet been attempted, despite its great potential for tightening global bounds 
and reducing computation costs. The goal of this pa­per is twofold: .rst, a new approach to visibility 
error estimation is proposed, based on features, that legitimates the use of a multi­resolution visibility 
algorithm. Second, quality control mechanisms are discussed for interactive simulation systems development. 
We begin in Section 2 with the introduction of features to evalu­ate image quality, and show why existing 
error metrics are incapable of determining when a given level of detail is satisfactorily repre­sented. 
A simple metric is then proposed to illustrate how to take into account the user s interest in a minimal 
feature size. This leads to Section 3 where we explain how to compute multi-resolution vis­ibility information 
using a spatial hierarchy augmented with equiv­alent extinction properties. Selection of a hierarchical 
level for vis­ibility computation can then be based on the resulting feature size on the receiver. In 
this paper an application to clustering algorithms is discussed, but multi-resolution visibility is equally 
promising for Monte Carlo techniques. The construction of a suitable hierarchy is discussed in Section 
4. In Section 5 we show that the multi­resolution visibility algorithm successfully generates images 
(cur­rently for isotropic clusters) in which only selected features sizes are accurately represented, 
resulting in computational savings. Section 6 presents more quality controls for clustering algorithms, 
speci.cally intra-cluster visibility determination in linear time and progressive multi-gridding. We 
conclude in Section 7. 2 Feature-Based Error Analysis To a large extent the quality of an image is judged 
based on how well features of different sizes are represented. It is not easy to charac­terize what constitutes 
an illumination feature. For the purposes of this paper, we will consider image features to be the connected 
re­gions of varying illumination related to shadows (regions in umbra or penumbra). 2.1 Lp metrics are 
inadequate for feature detection A major dif.culty for accurate lighting simulation is that in general 
the exact solution is not known at the time of computation. Thus the estimation of the error in a proposed 
approximation is particularly dif.cult, and must rely on the computation of error bounds for all algorithmic 
operations. Even in the case where an exact solution is available, it is not a simple task to de.ne the 
quality of a given approximation. This is done by choosing a particular error metric to quantify the 
distance between an approximate solution and the true solution. A good metric should therefore convey 
a sense of the user s requirements. A central observation in this paper is that when simulating a complex 
scene, the user is typically interested in capturing illumination variations down to a certain scale. 
Very small details are not as important, or at least not in all areas of the scene. We strive to de.ne 
a control mechanism that will avoid any work that would only generate such small details. In each column 
below a cluster of cubes similar to this one is placed between a light source and a wall. The size of 
the cubes increases from left to right.   9.51 10.7 11.0 10.5 Figure 1: Comparison of approximate 
illumination solutions using different clusters. Top: reference images (illumination of the wall). Middle: 
approximate images using a coarse mesh. Bottom: L2 er­ror norms. Note that the four images have similar 
L2 error values, and all hide some illumination information. However the varying size of the missing 
features cannot be discovered. Figure 1 illustrates the issue by showing shadows cast on a wall by four 
different groups of objects. Four approximate images, all computed using the same mesh size, are shown 
below the exact images. Consider a user who is interested in shadows of a speci.c size, e.g. those of 
the image on the extreme right, but is satis.ed by the averaging of the smaller, detailed shadows on 
the left1.The user thus does not wish more work to be done for the detail shad­ows, but wishes to have 
a more accurate representation at the larger scale. The subdivision criterion used in a HR algorithm 
for instance should be capable of halting the subdivision for the left-hand group, while ordering further 
computation for the group on the right. Thus an error measure should distinguish between the four cases. 
Traditional error metrics are incapable of making such a distinc­tion. As an example consider the commonly 
used family of error metrics expressing the distance between a reference function f and an approximate 
function ffas the Lp norm 1 Iff-fIp =|ff(x) -f(x)|pdxp 1Perhaps a more realistic example would be a situation 
where a user is viewing an of.ce scene from the doorway, and in which accurate shadows for chairs and 
desks are important, but averaged, low quality shadows from details such as pens on a desk are satisfactory. 
Lp norms simply add error contributions from all points on a surface (or in an image), and do not take 
into account higher-level proper­ties of the radiance distributions, such as the size and shape of illu­mination 
features. This is illustrated by the similar values obtained for the four groups in Figure 1. Appendix 
A shows that in fact for a point light source the L1 or L2 error introduced by averaging all visibility 
variations depends only on the average visibility, and not on the size or shape of the shadows. 2.2 
A proposal for an error metric based on feature size Our hypothesis is that illumination features (shadows 
or bright ar­eas) are important only as far as they have a signi.cant visual im­pact. Therefore it is 
possible to de.ne a feature size on a receiving surface, and decide that features smaller than that size 
are unim­portant : their absence should not contribute to the error. In the remainder of the paper we 
refer to the radiosity function over a surface as an image . This terminology should not mask the important 
fact that the entire discussion takes place in three­dimensional object space. In order to demonstrate 
the relevance of the feature-based approach, we assume for now that we have access to all the information 
in a reference solution. The multi-resolution visibility technique of Section 3 will show how the ideas 
developed here can still be used in the absence of such a reference. A simple error metric based on features 
is de.ned by segment­ing the image f into two components by means of a feature mask Fs(f,x): a binary 
function that equals one at points x that belong to a feature (of size greater thans) of function f. 
Computation of feature masks from the reference solution is described in the next section. For points 
in the mask region we compute an Lp norm of the difference between the approximate function and the reference 
function. For points outside the feature mask, we are content with an average value (since features present 
there are smaller than s). Thus in our current implementation we compute average values at each point, 
for both the approximate and reference functions, using a box .lter of size saround the point of interest, 
and compute an Lp norm of the difference between the averages. The feature-based error metric (FBEM) 
is summarized by the fol­lowing formula, where fs represents the .ltered version of f: sp Iff-fIp = |ffs(x) 
-fs(x)|[1 -Fs(f,x)] dx 1 p pFs + |ff(x) -f(x)|(f,x)dx (1) 2.3 Examples Table 1 shows the FBEM values 
computed for the four groups of Figure 1 and different values of the minimum feature size s.The object-space 
size of typical shadows in these images is respectively 11, 16.5, 22 and 31. For small s values, all 
FBEM values are high since the metric is equivalent to an L2 metric in the limit of s =0. As s increases, 
FBEM values decrease more rapidly for the groups containing smaller objects, as expected. There appears 
to be a resid­ual error of about 3 due to the mesh size used for the approximate solutions. Assume the 
user is interested in clearly seeing features of size 30 or greater, while being content with an average 
for all features smaller that this size. The extreme right-hand image of Figure 1 re­quires more work 
since the FBEM value for s =30 is high. The ap­proximation for the other three images is deemed satisfactory 
since the error is low. Thus, using the FBEM presented above, it is possible to reveal the presence of 
features greater than a given threshold in the approx-Table 1: Feature-based error metric (FBEM) for 
the four approxi­mate images of Figure 1 and .ve different feature sizes. The four measures are equivalent 
for small feature sizes, and decrease at dif­ferent rates as a function of s. Images are shown again 
for clarity. Feature size:   5 14.76 16.34 17.25 17.31 16 9.37 12.24 15.76 15.80 24 4.78 6.50 9.06 
14.74 30 4.23 3.16 6.90 13.37 40 3.65 2.33 3.35 6.94 imate images, opening the way for selective subdivision 
based on the user s minimum feature size of interest. Of course this could not be used as is in a subdivision 
criterion for HR, since it uses a refer­ence solution, but it is useful for a posteriori validation of 
control mechanisms. 2.4 Computation of feature masks According to the de.nition of features given above, 
computing a feature mask amounts to identifying connected regions of signif­icant size. Mathematical 
morphology provides tools to isolate fea­tures based on their size [12]. Consider a binary image, representing 
for example the characteristic function of an object. We de.ne the action of an Erosion operator as follows: 
all points outside the ob­ject (white) are untouched. All points inside the object that have a neighbor 
outside become white. All other points remain black. An Expansion operator is de.ned similarly by including 
in the objects all outside points that have a neighbor in the object. Figure 2 shows a reference image 
and images obtained after a number of erosions (top) or expansions (middle).        0 3 6 9 12 
15     Figure 2: Effect of repeated applications of the erosion (top), ex­pansion (middle) and combined 
erosions/expansion (bottom) oper­ations on a binary image. The reference image appears in the left column, 
and the number of applications of the operators increases from left to right. For the bottom row we apply 
nerosions followed by n expansions. Clearly an object of diameter 2dwill disappear after derosions are 
applied in sequence. Thus applying a sequence of n erosions followed by n expansions will successfully 
eliminate all small re­gions, but keep larger regions (slightly modifying their shape in the process). 
This process is illustrated in the bottom row of Figure 2. Computing the effect of the erosion operator 
on a binary image is straightforward using bitwise operations: the result is the logical OR of the image 
and the four translated copies of itself (by one pixel) in the +x, -x, +yand -ydirections. For the expansion 
operator the logical operator AND is used. In our examples, the original binary image is computed by 
recording all areas of the receiver that have a partial or occluded view of the light source. This expensive 
operation was performed only once during the creation of the reference image. Feature masks are computed 
by applying the proper number (p/2for a feature size of p) of successive erosions and expansions to eliminate 
unwanted features. Figure 3 shows some feature masks for the four groups used above. Original mask FSize12 
FSize 18 FSize24           Figure 3: Some feature masks for the images in Figure 1.  3 A 
Multi-resolution Visibility Algorithm In the previous section we presented the concept of a feature size 
and introduced an error metric which permits the evaluation of im­age quality determined by how well 
illumination features are repre­sented. We now use these fundamental concepts to develop a multi­resolution 
(MR) visibility algorithm. With this algorithm, expen­sive high quality visibility calculations are only 
performed when they are expected to help in the accurate representation of features deemed interesting 
by the user. Hierarchical spatial subdivision structures are often used in the calculation of global 
illumination algorithms, in particular when form-factor estimation is performed with ray-tracing [19, 
4, 5, etc.]. In radiosity clustering algorithms the hierarchy of clusters is also used for radiometric 
calculations, by letting clusters represent their contents for some energy transfers [13, 16]. The following 
multi­resolution visibility algorithm naturally extends previous clustering approaches by allowing clusters 
to also represent their contents in some visibility calculations. If a speci.c feature size shas been 
cho­sen, it is unnecessary to consider the contents of a cluster for visi­bility if these contents will 
produce features smaller than s. 3.1 Approximate visibility computation between clus­ters using an extinction 
model Let us assume that we have grouped all objects in the scene into a hi­erarchy of clusters. Approximate 
visibility calculations can be per­formed using an analogy between clusters and absorbing volumes [13]. 
The approximation (asymptotically exact for homogeneous isotropic clusters when the size of the objects 
goes to zero) con­sists of associating an extinction coef.cient .with each cluster. The transmittance 
function between two points P and Qin the scene is then given by - .(u)du T(P,Q)=e PQ - .ili i.C(PQ) 
=e where C(PQ)is the set of clusters traversed by the ray joining P and Q, .i is the extinction coef.cient 
of cluster i,and li is the length traveled inside cluster iby the ray. Extinction coef.cients express 
the probability that a random ray is intercepted in the cluster, and are computed as .i = jAj 4Vi where 
the area of all surface patches contained in cluster i is summed and divided by the cluster s volume. 
Since a surface con­tributes to the extinction of only one cluster, the attenuation due to overlapping 
clusters is correctly obtained by adding their extinction contributions.  3.2 Multi-Resolution Visibility 
In the rest of this section we consider the emitter-blocker-receiver con.guration shown in Figure 4, 
which consists of two surfaces, the emitter E and the receiver R, in two-dimensions. This restriction 
is for presentation purposes only and is removed later. R R E AE B A B (a) (b) Figure 4: De.nition 
of shadow features created by a blocker. (a) The umbra region is unbounded since the blocker is larger 
than the emitter: there is always an umbral region on the receiver. (b) For some positions of the blocker 
the receiver has no umbral region. If a blocker (which for now we also consider to be a surface) is placed 
between the emitter and the receiver, umbra and penum­bra regions are created in space. Depending on 
the position of the blocker, there may or may not be an umbral region on the receiver. (Figure 4). Given 
the de.nition discussed above the size of the um­bral zone on the receiver AB in Figure 4(a) , if it 
exists, is the fea­ture size. The blocker may actually be a hierarchical representation of a collection 
of objects (a cluster) as pictured in Figure 5(a). In this case, at each level of the hierarchy an extinction 
coef.cient is stored allowing the approximate calculation of the attenuation of a ray if it passes through 
the cluster, as described previously. Multi-resolution visibility can be performed by avoiding the de­scent 
into the hierarchy after a certain level. When the required con­ditions are met the extinction coef.cient 
is used instead, thus avoid­ing the intersection of the ray with all the descendants of this cluster. 
Evidently, the effect is that visibility is no longer exact, but an aver­age estimation of transmittance. 
It is here that a large potential gain in computation time can be achieved. In scenes where the small 
de­tail objects (e.g., models of phones, keyboards, small objects on a desk etc.), comprise the largest 
part of the geometric complexity, the intersection with these objects can quickly become the overwhelm­ing 
expense of visibility (and overall) computation. By considering the higher level clusters for visibility 
computation instead of the nu­merous contents, when such a choice is dictated by the chosen fea­ture 
size, this expense can be completely avoided. R R E A B (a) (b) Figure 5: Visibility estimation through 
a cluster. (a) the blocker is a hierarchy of clusters. (b) an equivalent blocker is used to estimate 
the maximum feature size on the receiver. Recall the discussion in Section 2 in which the user wishes 
to ac­curately represent all features of size greater than s on the receiver. To achieve this, all that 
is required is to descend suf.ciently far into the hierarchy so that the large shadows are accurately 
calculated, while performing the approximate calculation for small, detail shad­ows. To facilitate such 
a choice each cluster is augmented with a de­scription of the maximum blocker size BSIZE of its contents 
(we give a precise de.nition of this in the following section). It then suf­.ces to place a .ctitious 
blocker of size BSIZE , at the center of the actual cluster CD in Figure 5(b). The descent in the cluster 
hierar­chy can be terminated if the projected umbral region of the .ctitious blocker (AB in Figure 5) 
is smaller than the chosen feature size s. Contiguous regions which let light traverse must also be consid­ered 
as feature creators since a feature can be considered negative (umbra in a bright region), or positive 
(lit areas inside a dark re­gion). We thus extend our de.nition of features from Section 2 by de.ning 
BSIZE to be the maximum of the connected regions of light or shadow. This is consistent with the symmetric 
expression of visi­bility error with respect to umbra and light presented in Appendix A.  3.3 Characterization 
of a Cluster for MR Visibility All that is required in order to apply the preceding algorithm is the 
determination of BSIZE for each cluster. The restriction to two­dimensions is now lifted, and the treatment 
for three-dimensional clusters is described. For now clusters are assumed to contain ob­jects placed 
so that the cluster density can be considered isotropic, and thus does not depend on the direction of 
incidence of a ray. The goal is to determine a representative size for a blocking clus­ter, which will 
allow the calculation of the maximum feature size given a speci.c emitter-receiver con.guration. At .rst 
glance it may seem natural to take BSIZE to be the size of the largest object con­tained in the cluster. 
However there is one important consideration: it is the connected region of shadow on the receiver which 
we wish to consider. Furthermore, as discussed above, the regions of light potentially blocked by the 
contents of the cluster and the regions of light which pass through must be considered separately. A 
preprocessing step is performed to calculate BSIZE for all clus­ters in the hierarchy. For each cluster, 
all the contained objects are orthographically projected into a binary image. This opera­tion is performed 
for a given cluster and a given direction, result­ing in a view-independent characterization. The consequence 
of the isotropic cluster assumption is that a single orthographic projection is suf.cient. For non-isotropic 
clusters the BSIZE parameter is a function of the direction of interest. A simple solution in that case 
would be to interpolate from a number of sampled directions. Our current research focuses on more ef.cient 
representations for such directional information [14]. The erosion and expansion operators from Section 
2.4 are then used to compute the maximum sizes for blockers and free regions inside a cluster. Erosions 
(respectively expansions) are computed until all objects have disappeared (respectively until all free 
space has disappeared). The number of erosion or expansion operations de.nes the value of BSIZE for the 
blocked and free regions respec­tively. In our implementation we do the projections, erosions and expansions 
using Graphics hardware.  4 A Hierarchical Structure for Clustering and Multi-Resolution Visibility 
Previous automatic clustering approaches have used spatial data structures developed for ray-tracing 
(hierarchical bounding boxes [3] were used in [16], while in [13] a K-D tree was used). In this sec­tion 
we show that given the calculation of average visibility based on extinction coef.cients in the manner 
of [13], it is bene.cial to develop a special-purpose hierarchical data structure, such that the resulting 
clusters have properties suitable for cluster-based hierar­chical radiosity and multi-resolution visibility. 
By de.nition, clusters are constructed to represent as accurately as possible the collection of objects 
they contain. By introduc­ing computation of visibility using extinction coef.cients and also multi-resolution 
visibility, apart from the representation of energy transfer of the contained objects as a whole, the 
clusters also need to correctly represent the transmission properties of the collection of contained 
objects. These two modes of representation place different constraints on the cluster hierarchy. From 
the point of view of energy exchanges, good clusters allow tight bracketing of radiance or visibility 
func­tions (thus surfaces with similar orientation that do not shadow each other are preferred). From 
the point of view of visibility approxima­tion, good clusters are ones for which the extinction property 
is plau­sible (thus homogeneous isotropic clusters are preferred). Given these constraints, we have identi.ed 
two key properties for clusters: (a) proximity and (b) homogeneity of the contained objects. Main­taining 
proximity is a natural way to group objects when the cluster is used to represent radiative transfers. 
Also, for multi-resolution computation it is important that objects contained in a cluster are close 
so that the averaging performed does not introduce unaccept­able artifacts. Homogeneity here means that 
we want a cluster to group objects of similar size, and is crucial for the resulting quality of the average 
visibility computation. As a simple measure of proximity, we use the percentage of empty space resulting 
from a clustering operation (i.e., the addition of an object or a cluster to another cluster). Thus we 
prefer clusters in which the empty space is minimized. To ef.ciently group objects of similar size we 
use a hierarchy of n levels of uniform grids. We start with level 0, which is a single voxel the size 
of the bounding box of the scene and then at each level i we create a grid which is subdivided into 2i 
voxels along each axis. We then insert each object into the level for which its bounding box .ts the 
voxel size. Once these grids have been constructed, we start at the lowest level n, containing the smallest 
objects. We group the objects en­tirely contained in each voxel, by attempting to minimize the empty 
space, in accordance to the proximity criterion described above. In addition, objects which are very 
small compared to the grid size are grouped into an appropriate cluster, even if the resulting cluster 
is largely empty. Once all the voxels of a level have been treated, we attempt to add the objects not 
entirely contained in a single voxel at this level to the clusters already constructed, again using the 
same criteria. We then insert the clusters created to the grid of the level immediately above, and iterate. 
Once the cluster hierarchy has been created, the data structure is augmented with average transmission 
behavior by propagating the average extinction values up the hierarchical structure as in [13]. When 
multi-resolution visibility is used, the BSIZE estimation is also performed for each cluster in the hierarchy 
in the manner de­scribedinSection3. Figure 6 presents results obtained with the new hierarchy using .rst 
a surface visibility algorithm similar to that of [16], and then the average visibility proposed in [13]. 
The scene consists of 5380 poly­gons. It is interesting to observe the signi.cant time gain achievable 
by the average visibility algorithm given a suitable hierarchy (we observe a factor of 4), while approximate 
shadows are preserved. Surface vis: 1 216 s. Volume vis: 376 s. Figure 6: Timings (in seconds) using 
the new hierarchy construc­tion. Throughout the paper all timing information was obtained on an Indigo 
R4000 computer. Constructing a suitable hierarchy for cluster-based hierarchical radiosity with extinction 
and multi-resolution visibility is a dif.cult problem. The results indicate that the .rst solution presented 
here, based on proximity and homogeneity, results in the construction of hierarchies well suited to approximate 
and multi-resolution visibil­ity calculations. 5 Results of multi-resolution visibility We have implemented 
the hierarchy construction, the calculation of BSIZE and the multi-resolution visibility algorithm in 
a hierarchical radiosity clustering testbed. To evaluate the results of the multi-resolution visibility 
approach we have computed images of test environments using different val­ues for the feature sizes of 
interest s on a receiver. The .rst test scene is shown in Figure 7 (left). It contains the four clusters 
used in Section 2 and a light source (in yellow). The right-hand image is the illumination obtained on 
the back wall and serves as a reference image. For all these images visibility was al­ways computed solely 
using extinction properties (thus we do not attempt to characterize the error introduced by averaged 
transmis­sion visibility itself). Figure 8 shows four images, where the desired feature size pa­rameter 
(see Section 2.2) is changed. For each image the computa­tion time in seconds is given. A very low error 
threshold was used to ensure that the mesh density was maximal for all images. Thus the decrease in computation 
time as the desired feature size becomes larger measures the speedup in the visibility calculation. We 
next show that multi-resolution visibility is consistent with the feature-based error metric (FBEM) from 
Section 2.2, by com­puting the FBEM for the images described above. Although the four clusters have been 
grouped in a single image for simplicity, we ap­ply the error metric only on the region of the image 
corresponding to each cluster, to obtain an FBEM value for each of the four groups. For the four images, 
we show for each cluster the value of L2 er­ror norm (back row) and the value of the FBEM for a feature 
size s equal to that used in the MR Visibility algorithm. We note that 3D view of test scene. Reference 
sol. (2069 s). Figure 7: Reference image used in the error comparisons. as we increase s the L2 norm 
for all clusters increases, as more and more averaging is being performed. Still the increase appears 
later for larger objects, as expected. The FBEM values are always of sim­ilar magnitude, despite the 
fact that very different levels of averag­ing are being used in different clusters in a given image. 
This shows that the multi-resolution visibility algorithm accomplishes its pur­pose: given a desired 
feature size, it ensures that the corresponding FBEM remains low while allowing time gains. Figure 9 
shows that even greater speedups can be achieved when a medium error threshold allows MR visibility to 
reduce the amount of subdivision. The explicit incorporation of MR visibility in re.ne­ment criteria 
is a promising path for further acceleration. 6 Control of Image Quality for Clustering Recent algorithms 
separate the computation of high-quality images into two phases: a coarse quality global illumination 
calculation is .rst performed using elaborate algorithms such as discontinuity meshing or clustering 
in a global pass. A view-dependent, poten­tially very expensive local pass follows [9, 16]. This local 
pass is typically a ray-casting operation: at each pixel the energy from all the links is collected, 
allowing the calculation of high-quality shad­ows. The cost of this local pass is often many times larger 
than that of the light-transfer calculation using clusters. In essence this pass may eradicate all computation 
time bene.t achieved by using the clusters in the .rst place, and exclude any possibility for interactiv­ity 
with quality and error control. In contrast, we maintain a progressive re.nement philosophy, by providing 
explicit quality controls, allowing computational cost to be focused on desired characteristics of the 
resulting image. The .rst component of this approach is the multi-resolution visibility presented above. 
This technique, coupled with the use of impor­tance [17] to assign appropriate feature sizes to different 
objects could plausibly replace the global/local pass approach while afford­ing more interactivity. We 
present next two supplementary quality controls: .rst, the correct treatment of intra-cluster visibility 
and second, progressive multi-gridding permitting rapid interactive re­sponse for hierarchical radiosity. 
6.1 Intra-cluster visibility Previous clustering algorithms compute a bound on energy transfer that ignores 
visibility (bound of 1 on the visibility error), both be­tween the two clusters but also in the distribution 
of light on each side [16, 13]. This potentially results in light leaks at the scale of the cluster. 
This behavior is not only visually displeasing but also .awed: since bounds are computed on irradiance 
values, that irra­diance is distributed to many surfaces which should be shadowed, 4 5 6 6 1 2 3 1 2 
3 4 1 2 3 4 5 1 2 3 4 5 A B C D 0 Feature size: 2.0 (1984 s). A B C D 0 Feature size: 2.8 (1648 s). A 
B C D 0 Feature size: 3.5 (1459 s). A B C D 0 Feature size: 5.0 (1356 s). Figure 8: Results for the 
multiresolution visibility algorithm. thereby creating energy. If visibility information inside the 
cluster with respect to a source cluster can be computed (with some approximation) in time linear in 
the number of contained objects, the overall time and space com­plexity of O(slog s) for clustering is 
not modi.ed [16]. We propose the use of an item buffer to quickly evaluate this vis­ibility. The cluster 
s contents are projected in the direction of the light source using a z-buffer to determine visible surfaces 
from that direction. By counting instances of an item number in the buffer we obtain an estimate of the 
projected area of each patch visible from the direction of the source. This is used as the projected 
area in ker­nel calculations when computing energy bounds. Note that the reso­lution of the item-buffer 
can be adapted to the contents of each clus­ter, provided we know the size of the smallest object in 
each cluster. Thus the aliasing problems inherent to the item-buffer approach can be reduced. The same 
technique is also used at the other end of a link, to evaluate the energy leaving a cluster.  In the 
images of Figure 10 we present an example where a link (shown in purple) has been created from the light 
source to a clus­ter of books. Ignoring intra-cluster visibility (left) results in the cre­ation of energy 
since all books are fully illuminated. Using the vis-Figure 9: Increasing the desired fea­ture size reduces 
both the amount of subdivision and the cost of visibility computations. (left) Fsize = 0, 621s. (middle) 
Fsize = 4, 245s. (right) Fsize = 8, 148s. Tree courtesy of CIRAD, modelled with 7967 poly­gons using 
AMAP. ibility buffer to modulate the energy distribution (right), energy is preserved while improving 
the appearance of the image. 6.2 Progressive multi-gridding In hierarchical radiosity algorithms subdivision 
is controlled by an error threshold on individual interactions. A global bound on er­ror is dif.cult 
to determine and it is consequently dif.cult for the user to choose an error threshold so as to achieve 
a certain quality. The problem is exacerbated with clustering, since the subdivision of links is amortized 
with time, and thus successive iterations may become much more expensive as the allowed error decreases. 
This sharp and unpredictable increase in iteration time may then destroy interactivity. As a remedy we 
develop a progressive multi-gridding approach. By analyzing the distribution of error bounds on the links 
created, we can predict how many of these links would survive a given de­crease in the error threshold, 
and thus estimate the time required for a subsequent iteration with the new error threshold. In a manner 
more intuitive to the user the amount of computation can be spec­i.ed (in the form of a maximum number 
of links to re.ne) and the system proceeds to deduce the new threshold to use for hierarchical re.nement. 
This analysis can be performed at a marginal cost by recording a histogram of the distribution of error 
bounds computed. Speci.­cally, the interval between 0 and the current error threshold e is di­vided into 
a number of bins, each associated with a counter. Ev­ery time a link is created (or left unchanged) during 
the hierarchi­cal subdivision procedure, we increment the counter for the bin cor­responding to the link 
s error bound. At the start of the next pro­gressive multi-gridding iteration, the new error threshold 
is chosen such that the sum of all counter for bins with higher error levels is less than a user-speci.ed 
limit k. This effectively chooses an error threshold such that at most klinks are re.ned. This multi-gridding 
algorithm does not accelerate the computation but guarantees a con­tinuous update of the simulation. 
 7 Conclusions Important advances towards the goal of providing interactive sys­tems capable of treating 
very complex environments have been made by hierarchical radiosity and clustering algorithms. Nonethe­less 
several important shortcomings of previous approaches were identi.ed in this paper: (a) visibility error 
is typically ignored, (b) traditional error metrics do not allow the user to specify a desired level 
of detail and (c) progressive re.nement of the simulation is dif­.cult to achieve. In this paper we introduced 
a new approach to error estimation based on illumination features, which allows the user to choose a 
level of detail relevant to a given simulation. The quality of a so­lution then relates to how well features 
of the user-determined size have been represented. The principles introduced by the feature-based analysis 
were used to develop a multi-resolution visibility algorithm. The hier­archy constructed for clustering 
contains transmission information as in [13] and is further augmented with an estimate of the largest 
equivalent blocker size from its contents. This information is used to limit the cost of visibility calculations. 
An algorithm which ef.­ciently constructs a suitable hierarchy was also presented. The re­sults of the 
implementation for isotropic environments show signi.­cant computational speedup using MR visibility 
when the user does not require the accurate representation of small features. Two additional quality 
control mechanisms were introduced: intra-cluster visibility which corrects potential light-transfer 
error suffered by previous clustering algorithms, and progressive multi­gridding which is essential for 
interactive clustering systems. We believe that the introduction of feature-based error and qual­ity 
evaluation is an important step which will lead to signi.cant ac­celeration of global illumination algorithms. 
Multi-resolution visi­bility is an example of such an achievement. In future work the extension of our 
approach to non-isotropic en­vironments must be completely developed. Promising .rst results in representing 
directional information for clustering have been ob­tained [14]. We have not yet addressed the analysis 
of error caused by the use of extinction coef.cients and the effect of visibility corre­lations between 
clusters and their contents. Research in these areas is extremely important for the development of reliable 
quality con­trols. It will be interesting to observe the results of the application of our approach to 
Monte Carlo methods. A more in-depth study of feature-based error metrics must be performed. Finally 
better algo­rithms for hierarchy construction should be investigated. 8 Acknowledgements George Drettakis 
was hosted successively by INRIA (Grenoble, France), UPC (Barcelona, Spain) and GMD (St. Augustin, Germany) 
as an ERCIM postdoctoral fel­low, and was .nanced in part by the Commission of the European Communities. 
Pat Hanrahan provided parts of the hierarchical radiosity code; Jean-Daniel Boissonnat suggested the 
use of erosion/expansion operations. The scene in Figure 6 was assem­bled using pieces of the Berkeley 
Soda Hall model; thanks to Seth Teller and all mem­bers of the UC Berkeley walkthrough group. References 
 [1] James Arvo, Kenneth Torrance, and Brian Smits. A framework for the analysis of error in global illumination 
algorithms. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH 94 (Orlando, FL), pages 
75 84, 1994. [2] Michael F. Cohen and John R. Wallace. Radiosity and Realistic Image Synthesis. Academic 
Press, Boston, 1993. [3] J. Goldsmith and J. Salmon. Automatic creation of object hierarchies for ray 
trac­ing. IEEE Computer Graphics and Applications, 7(5):14 20, May 1987. [4] Eric A. Haines. Shaft culling 
for ef.cient ray-traced radiosity. In P. Brunet and F.W. Jansen, editors, Photorealistic Rendering in 
Computer Graphics, pages 122 138. Springer Verlag, 1993. Proceedings of the Second Eurographics Work­shop 
on Rendering (Barcelona, Spain, May 1991). [5] Pat Hanrahan, David Saltzman, and Larry Aupperle. A rapid 
hierarchical radios­ity algorithm. Computer Graphics, 25(4):197 206, August 1991. Proceedings SIGGRAPH 
91 in Las Vegas. [6] Nicolas Holzschuch, Fran¸ cois Sillion, and George Drettakis. An ef.cient pro­gressive 
re.nement strategy for hierarchical radiosity. In Fifth Eurographics Workshop on Rendering, Darmstadt, 
Germany, June 1994. [7] Arjan J. F. Kok. Grouping of patches in progressive radiosity. In Proceedings 
of Fourth Eurographics Workshop on Rendering, pages 221 231. Eurographics, June 1993. Technical Report 
EG 93 RW. [8] Dani Lischinski, Brian Smits, and Donald P. Greenberg. Bounds and error es­timates for 
radiosity. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH 94 (Orlando, FL), pages 
67 74, July 1994. [9] Dani Lischinski, Filippo Tampieri, and Donald P. Greenberg. Discontinuity meshing 
for accurate radiosity. IEEE Computer Graphics and Applications, 12(6):25 39, November 1992. [10] Dani 
Lischinski, Filippo Tampieri, and Donald P. Greenberg. Combining hierar­chical radiosity and discontinuity 
meshing. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH 93 (Anaheim, CA), pages 
199 208, Au­gust 1993. [11] Holly Rushmeier, Charles Patterson, and Aravindan Veerasamy. Geometric sim­pli.cation 
for indirect illumination calculations. In Proceedings Graphics Inter­face 93. Morgan Kaufmann, 1993. 
[12] J. Serra. Image analysis and mathematical morphology : 1. Academic Press, London, 1982. [13] Fran¸A 
uni.ed hierarchical algorithm for global illumination with cois Sillion. scattering volumes and object 
clusters. to appear in IEEE Transactions on Visu­alization and Computer Graphics, 1(3), September 1995. 
(a preliminary version appeared in the .fth Eurographics workshop on rendering, Darmstadt, Germany, June 
1994). [14] Fran¸ cois Sillion, George Drettakis, and Cyril Soler. A clustering algorithm for radiance 
calculation in general environments. In Sixth Eurographics Workshop on Rendering, Dublin, Ireland, June 
1995. [15] Fran¸ cois Sillion and Claude Puech. Radiosity and Global Illumination. Morgan Kaufmann publishers, 
San Francisco, 1994. [16] Brian Smits, James Arvo, and Donald P. Greenberg. A clustering algorithm for 
radiosity in complex environments. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH 
94 (Orlando, FL), pages 435 442, July 1994. [17] Brian E. Smits, James R. Arvo, and David H. Salesin. 
An importance-driven ra­diosity algorithm. Computer Graphics, 26(4):273 282, July 1992. Proceedings of 
SIGGRAPH 92 in Chicago. [18] Seth J. Teller and Patrick M. Hanrahan. Global visibility algorithms for 
illumi­nation computations. In Computer Graphics Proceedings, Annual Conference Series: SIGGRAPH 93 (Anaheim, 
CA), pages 239 246, August 1993. [19] John R. Wallace, Kells A. Elmquist, and Eric A. Haines. A ray tracing 
algorithm for progressive radiosity. Computer Graphics, 23(3):315 324, July 1989. Pro­ceedings SIGGRAPH 
89 in Boston. A Visibility error using L1 and L2 norms Consider a patch P illuminated by a point source 
at point y.To quantify the visibility error on the receiver patch, we compute the L1 and L2 norms of 
the difference between the visibility function v(x,y) de.ned for x . P, and its average value v¯over 
patch P.If P+ is the region of patch P that receives light, v¯is equal to the ratio of the areas of P+ 
and P. Separating the integrals into one over P+ and one over (P -P+),we .nd Iv -v¯I1 =2¯v (1 -v¯) (2) 
and similarly for the L2 norm Iv -v¯I2 = v¯(1 -v¯) (3) Note that both estimates only depend on the average 
visibility across patch P, not on the distribution of the visibility function. Also note the dependency 
in v¯(1 - v¯), yielding a small error for either almost complete occlusion or almost complete visibility. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218437</article_id>
		<sort_key>153</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[Live paint]]></title>
		<subtitle><![CDATA[painting with procedural multiscale textures]]></subtitle>
		<page_from>153</page_from>
		<page_to>160</page_to>
		<doi_number>10.1145/218380.218437</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218437</url>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39077181</person_id>
				<author_profile_id><![CDATA[81100250413]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Perlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Research Laboratory, Courant Institute of Mathematical Sciences, New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15025597</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA, Instituto de Matem&#225;tica Pura e Aplicada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>192181</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Deborah Berman, Jason Bartell, and David Salesin. Multiresolution painting and compositing. Computer Graphics, Annual Conference Series (SIGGRAPH '94 Proceedings), pages 85-90, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Peter J. Burt. The laplacian pyramid as a compact image code. IEEE Transactions on Communications, 31:532-540, April 1983.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358553</ref_obj_id>
				<ref_obj_pid>358523</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Alain Fournier, Don Fussell, and Loren Carpenter. Computer rendering of stochastic models. Communications of the ACM, 25(6):371-384, June 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808605</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[John Peter Lewis. Texture synthesis for digital painting. Computer Graphics (SIGGRAPH '84 Proceedings), 18(3):245- 252, July 1984.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lively pictures. Byte Magazine, 20(1):171-174, 1995. Live Picture- Product Review.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Stephane Mallat. Multifrequency channel decompositions of images and wavelet models. IEEE Trans. on Acoust. Signal Speech Process., 37(12):2091-2110, 1989.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Joan M. Ogden, Edward H. Adelson, J.R. Bergen, and Peter J. Burt. Pyramid-based computer graphics. RCA Engineer, 30(5):4-13, September-October 1985.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Alex Pentland and Bradley Horowitz. A practical approach to fractal-based image compression. In Proceedings of Data Compression Conference, pages 176-185, held in Snowbird, UT, 1991. IEEE Computer Society Press.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. An image synthesizer. Computer Graphics (SIG- GRAPH '85 Proceedings), 19(3):287-293, 1985.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin and Luiz Velho. A wavelet representation for unbounded resolution painting. Technical report, New York University, New York, 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Pyramidal parametrics. Computer Graphics (SIGGRAPH '83 Proceedings), 17(3):1-11, July 1983.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Xres, the alternative to photoshop? Mac Format Magazine, 23, pages 72-74, 1995. XRes- Graphics Software Review.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Live Paint: Painting with Procedural Multiscale Textures Ken Perlin Media Research Laboratory Courant 
Institute of Mathematical Sciences New York University Luiz Velho IMPA Instituto de Matem´atica Pura 
e Aplicada Abstract We present actively procedural multiresolution paint textures. Tex­ture elements 
may be linearly combined to create complex com­posite textures that continue to re.ne themselves when 
viewed at successively greater magni.cation. Actively procedural textures constitute a powerful drawing 
tool that can be used in a multires­olution paint system. They provide a mechanism to generate an in.nite 
amount of detail with a simple and compact representation. We give several examples of procedural textures 
and show how to create different painting effects with them. Introduction The introduction of multiresolution 
paint systems is a recent de­velopment in the .eld of computer graphics, [10], [1], [5], [12]. In this 
type of system, the user can view and modify an image at any desired resolution. This is possible because 
the internal image representation supports multiple levels of detail. In multiresolution paint systems 
it is possible to make modi.ca­tions at different image magni.cations. The user can quickly make coarse 
changes over large areas of the picture, as well as .ne and precise changes over small areas. Although 
this capability provides a great degree of control over the painting process, it is the painting tool 
that ultimately determines what goes into the picture. For this reason, it is necessary to develop tools 
that can take full advantage of multiresolution paint systems. Standard painting tools are designed to 
operate at .xed resolu­tion and, therefore, can generate only a certain amount of image detail, commensurate 
with resolution. When the user wants to cre­ate .ne detail over a large area of the image, s/he must 
paint at a high magni.cation level. This task is time consuming, even if a multiresolution paint system 
is used. Thepowerofmultiresolution paintsystemsis enhancedifpaint­ing tools are able to exploit the underlying 
multiresolution image 1Courant Institute of Mathematical Sciences, New York University, 719 Broadway 
12th Floor, New York, NY, 10003. perlin@cs.nyu.edu 2IMPA Instituto de Matem´atica Pura e Aplicada, Estrada 
Dona Castorina 110, Rio de Janeiro, RJ, Brazil, 22460-320. lvelho@visgraf.impa.br Permission to make 
digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage, the copyright notice, the 
title of the publication and its date appear, and notice is given that copying is by permission of ACM, 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 representation by operating 
at multiple levels of detail. In this way, when the user wants to paint a complex pattern over a large 
area of the picture, s/he can perform the operation at a coarse level very quickly and still create as 
much detail as desired. In this paper, we develop a framework for the design and im­plementation of multiresolution 
painting tools. This framework is based on general procedural textures de.ned over both spatial and scale 
domains. These procedures are executed at multiple resolution levels in order to add texture details 
to the picture. Basic texture elements can be linearly combined to create complex composite textured 
brushes. This framework .ts naturally into the context of multiresolution paint systems. The structure 
of this paper is as follows: Section 2 reviews the principles of multiresolution paint systems; Section 
3 gives a general overview of multiresolution textures; Section 4 shows where texture is invoked in the 
system; Section 5 explains how the texture re.nement method works; Section 6 illustrates the system in 
action with examples and pretty pictures; Section 7 discusses the use of images in our framework; and 
Section 8 concludes with .nal remarks and a discussion of future work. 2 Multiresolution Paint Systems 
A multiresolution paint system allows the user to view and modify an image at multiple resolution levels. 
The painting process consists of a cycle in which the following tasks are repeatedly executed: modify 
image at level x 1r move up or down a level For this, the system must support: Multiresolution Image 
Representation;  Painting and Compositing at Multiple Levels.  We can classify multiresolution paint 
systems with respect to the way they implement the image data structure and the operations mentioned 
above. 2.1 Multiresolution Image Representations There are two basic ways to represent an image at multiple 
resolu­tions: with a lowpass pyramid or with a bandpass pyramid. A lowpass pyramid consists of multiple 
copies of the image at different resolutions, [11]. In Figure 1(a) we show an example of a lowpass pyramid 
with 4 levels. A bandpass pyramid consists of a coarse resolution version of the image, together with 
a sequence of image details that are required to produce .ner resolution versions of the image from coarse 
versions, [2]. The bandpass pyramid can be constructed by taking differences between consecutive levels 
of the lowpass pyramid. InFigure1(b)weshowanexampleofabandpasspyramid with 4 levels. The images in the 
bandpass pyramid may contain negative values. For this reason, the .gure shows zero values as middle 
gray. (a) (b)  Figure 1: Lowpass (a) and Bandpass (b) Pyramids Note that the lowpass pyramid is a 
redundant representation since the same information is present at all levels of the pyramid. The bandpass 
pyramid, on the other hand, is not redundant because it keeps only the information necessary to go from 
coarse to .ne resolution levels. A Wavelet pyramid is a kind of bandpass pyramid which dis­criminates 
image details along the horizontal, vertical and diagonal directions, [6]. 2.2 Multiresolution Painting 
and Compositing There are two main ways in which image operations can be incor­porated into a multiresolution 
paint system: by re-execution or by lazy evaluation. If the system allows the image to be represented 
at arbitrarily high resolutions, then it is not possible to perform image operations at all levels simultaneously 
during the interaction cycle. Multires­olution paint systems generally deal with this as follows: while 
the user is painting, the system only updates the currently visible level. Changes to the image are cached, 
to be propagated to other resolution levels at a later time. What distinguishes different im­plementations 
of image operations is the strategy used to postpone the propagation of changes. If a re-execution strategy 
is used, changes are cached as pre­scriptions for redrawing. When the user moves to another level of 
detail, all cached operations are re-executed at that level. If a lazy evaluation strategy is used, modi.cations 
to the image are iteratively propagated through the image pyramid when the user magni.es the view to 
successively more detailed levels. Changes that will affect higher resolutions are evaluated only when 
the user .rst magni.es the view suf.ciently to see these resolutions.  2.3 Classi.cation of Multiresolution 
Paint Systems The multiresolution image data structures and operations described above have a direct 
correspondence with one another. Accordingly, there are two types of multiresolution paint systems: I 
 Lowpass pyramid + Re-execution strategy II Bandpass pyramid + Lazy evaluation strategy Examples of 
type I systems are Live Picture [5] and XRes [12]. Examples of type II systems are the Haar wavelet [1] 
and the B­spline wavelet [10] paint programs. The general idea of multiresolution painting tools applies 
equal­ly well to painting systems of types I and II. Although in this paper we will emphasize implementation 
techniques that are more suited to systems of type II, the same techniques could be used with proper 
changes in systems of type I. 3 Multiresolution Textures In this section we introduce the concept of 
multiscale textures and discuss how they are implemented. 3.1 Motivation Let us say that a user of a 
paint system wishes to paint with a rock generating brush. After painting, the user should subsequently 
be able to zoom in and continue to see ever .ner details of the rock texture. Alternatively, if the user 
paints with a checkerboard brush, then no matter how far s/he zooms into the checkerboard texture, the 
boundaries between the white and black squares should always remain sharp. Let us now suppose that the 
user paints some rock, then zooms in a bit and paints some translucent checkerboard over the rock. As 
the user zooms in,arbitrarily small rock details should still be visible behind the checkerboard, but 
attenuated. Both of the composited textures should continue to reveal more high frequency detail as the 
user s view zooms in. The user should, for example, be able to paint with a brush which consists of one 
part checkerboard and two parts rock, or in fact to freely mix any such texture elements. In this manner, 
the user should be able to paint with and to composite many different layers of procedural texture, with 
the ex­pectation that all visible layers will appear in the proper proportion at all levels of detail. 
 3.2 How to do this In order to implement this behavior we have developed a model for multiscale painted 
texture that allows procedural textures to be combined additively. The key insight is that texture must 
be added in two distinct ways: (1) Whenever the user paints a texture, the system must display that texture 
s initial appearance. (2) As the user successively magni.es the view, the system must continue to add 
detail to the painted texture.  It is the responsibility of the texture procedure to perform these two 
functions. To make this all happen properly and ef.ciently, we need two tools: Procedural Bandpass Pyramids 
and Procedural Ink. 3.3 Procedural Bandpass Pyramids To do sharpening properly, we use procedural bandpass 
pyramids. First let us brie.y review bandpass image pyramids. Each level of a bandpass pyramid gives 
the difference in detail between successive submagni.cations of an image. Consider an image of resolution 
2nx2n. If we want to view this image at a resolution of 2n.1x 2n.1, we can blur it using a smoothing 
.lter and then decimate. If the image is then blown up again with a good interpolation .lter, it will 
appear blurry. Information has been lost. We must add a correction to each pixel of this blurry image 
in order to recreate the original image. This correction is itself a 2nx2nimage containing the image 
details that were lost. If we apply the same process to the decimated image, recur­sively,we cancreatea 
sequenceofsuchdetailimageswith descend­ nnn.1n.1 ing resolutions: 2x2, 2x2, ...2x2. This sequence of 
images constitutes a bandpass pyramid [2]. Once we have its bandpass pyramid, we can reconstitute an 
image at any submagni.cation. Beginning with a single pixel, we magnify and add the 2x2bandpass image, 
then repeat with the 4x4bandpass image, and so on. The pixels of the 2kx2kimage of a bandpass pyramid 
are called level-k bandpass coef.cients . A multiresolution paint system can show an image at various 
scales. Bandpass coef.cients make up the difference between the less detailed view that is visible when 
the image is small, and the more detailed image that is available after magnifying by two. If we look 
at it this way, we can see that as we continue to magnify the user s view of a texture, our texture procedure 
should corre­spondingly add in the next level of bandpass coef.cients, so that the texture will always 
have sharp detail. Images are .nite. At some point a multiresolution paint system will magnify the view 
beyond any stored image s resolution. For levels beyond this, the image simply becomes blurrier, since 
it can contribute no more bandpass coef.cients. In other words, the image s bandpass pyramid is of .nite 
depth. But procedural textures are not so restricted. We can de.ne a procedural bandpass pyramid which 
given a type, and values for x, y,and level, returns the difference in a texture s appearance when viewed 
at successive magni.cations. Procedural bandpass pyramids can be of in.nite depth. Here is a simple example. 
It is well known that the appearance of rock can be synthesized by 1/fnoise [9]. The difference in this 
texture s appearance between successive magni.cations is just the addition of attenuated random noise 
at the higher magni.cation. A procedure that simulates a bandpass pyramid with this behavior is quite 
simple to de.ne: pseudorandom(x'y'level)add texture(ROCK'x'y'level) 2level/2 where the pseudorandom function 
is implemented by the same per­mutation method used to choose pseudorandom gradients from Z 3 for the 
Noise function [9]. The tricky aspect of this process is that we need to sharpen each painted texture 
only the .rst time that the view is magni.ed to a new level of greatest detail. For example, if the user 
paints a texture, then zooms in and out a few times, we do not want to add bandpass coef.cients to the 
texture again and again. The result would be incorrect. For this reason, texture propagation is controlled 
by procedural ink .  3.4 Procedural Ink In the sections that follow, we will use the phrase texture 
instance to refer to a primitive texture component. Some examples are: rock of a particular scale, a 
checkerboard of a certain size, sawtooth stripes of a particular width, or a source image texture painted 
on at a certain scale. We build all textures as combinations of texture instances. Procedural ink is 
texture in latent state. We represent it as a vector of amplitudes; each element of the ink vector modulates 
the amplitude of one texture instance. The .rst ink channel is reserved for ink.alpha the attenuation 
of the ink vector itself. All elements of the ink vector will be attenuated by this ink.alpha. Textures 
are mixed by compositing and layering ink vectors. The resulting composite ink vector is then used to 
control a texture generator procedure. Here is a key point: Because all elements of an ink vector are 
premultiplied by ink.alpha, we can mix textures simply by adding various ink vectors. As noted above, 
we want to do sharpening only the .rst time that a viewer magni.es a painted area to a new view. In order 
to accomplish this we need a form of lazy evaluation. This is where the ink comes in. Ink controls when 
the texture gets sharpened. The key property of ink is that it .ows downhill toward levels of evergreaterdetail. 
Asink.owsdown,itcausesthesystemtore.ne those texture instances which the ink modulates. More precisely, 
each element of the ink vector is used to scale the bandpass coef.cients that must be added in order 
to sharpen one texture instance. Ink is only used for texture re.nement at the moment that it .rst enters 
a view level. This will happen only in one case: when there is ink at the current view, and then the 
user magni.es the view. At this time, the used ink leaves the coarser level, and pools at the more detailed 
level, waiting for the user to further magnify the view. Note that there can be considerable delay between 
the time that a texture is painted and the time that this lazy-evaluation sharpen­ing occurs. For example, 
the user might paint at some level, then decrease the magni.cation, zooming out to paint something some­where 
else. Meanwhile, the ink is still sitting at the former level. When the user later returns to that view 
and magni.es, only then will the ink continue its downward journey. 4 Where Texture is called Texture 
is called in two places: once as the user interactively paints, and again when the view is magni.ed, 
in order to add bandpass coef.cients. 4.1 Texturing during painting When the user paints at a sample 
using drawing ink, and with opacity drawing alpha, then the color, alpha and ink at the sample are modi.ed 
as follows: (1) color := texture(drawing ink) OVERdrawing alpha color (2) alpha := 1.0 OVERdrawing alpha 
alpha (3) ink := drawing ink OVERdrawing alpha ink  where b OVERta denotes linear interpolationa+t(b-a), 
texture is a compound procedural texture as de.ned in subsection 5.2, and ink is a prescription for evaluating 
the procedural texture. This is the point where the ink is .rst injected into the system, and where the 
.rst, coarse view of the texture is painted. Note that the new color is merged directly into the sample. 
There is no need to explicitly store back-to-front layers.  4.2 Texture re.nement during magni.cation 
Texture is also invoked when the user increases the magni.cation level of the view. As the user s view 
changes from a coarser level to a more detailed level, ink .ows down to this new level, so that more 
texture detail can be induced. When the view is magni.ed from a coarser to a more detailed level, we 
need to propagate both color and ink to the new level. To describe this process, we de.ne: detail: the 
portion of a sample s color at the more detailed level that was too .nely detailed to be visible at the 
coarser level. This quantity is generated by the multiresolution paint system at the moment that magni.cation 
was reduced. If this is the .rst time that we have ever visited the more detailed level, then this quantity 
will be zero. In our implementation, this quantity is computed from B-spline wavelets.  coarse: a magni.ed 
view of what is currently visible at the coarser level.  new ink: a magni.ed view of the new ink from 
all coarser levels which now needs to .ow down to this more detailed level.  Let us .rst review the 
magni.cation procedure in a multiresolu­tion paint system that uses lazy evaluation, but that does not 
support procedural texture. In such a system, ink is just a vector of [red, green, blue, alpha]. Let 
us consider the situation where the user has painted with new ink at some coarser level after the last 
time s/he had visited the next more detailed level. Now the user wants to magnify the view. Because the 
system has lazy evaluation, this new ink will not yet be incorporated into the more detailed level. To 
incorporate this new ink, we do the following steps at the more detailed level: (1) color := coarse +(1 
-new ink.alpha)* detail (2) ink := new ink OVERnew ink.alpha ink Notice that we do not do an OVER operation 
in step (1). This is because the coarse value already incorporates all new ink that has been painted 
at all coarser levels. Only the detail is out of date. After this, we erase (i.e. set to zero) all the 
ink at the coarse level that has .owed to the more detailed level. The general effect is that as the 
user s view is progressively magni.ed, ink at coarser levels continually .ows down to more detailed levels. 
As it does so, its appearance becomes progressively blurrier. To support procedural texture, we need 
only to add a term to step (1): (1) color := coarse +texture(new ink) +(1 -new ink.alpha)* detail Note 
that all elements of new ink are already attenuated by new ink.alpha. The result is that all added texture 
is attenuated by new ink.alpha. That is why we simply add texture, instead of overlaying it on top of 
the detail. Now as the ink .ows down, it is continually re.ned. 5 From Ink to Texture Texture comes in 
different types . Each type requires a different re.nement method. In the following sections, we describe 
how texture is combined, show various texture procedures, and give some examples. 5.1 Types and Instances 
As described above, a procedural texture is built up by adding bandpass coef.cients to each texture instance 
modulated by the ink vector, at successive levels of detail. The type of a texture instance identi.es 
the method that is used for adding bandpass coef.cients at each level. The method chosen will determine 
the general look of the texture. Each texture instance is identi.ed by: - its type -its base magni.cation 
level (equal to the view level at which the user painted the texture). A unique element of the ink vector 
is allocated to each texture instance, the .rst time that instance is painted. This ink element contains 
the amplitude that will be used to scale the bandpass coef­.cients as that texture instance is re.ned. 
 5.2 Adding Texture All texture instances are combined linearly. The texture procedure simply loops through 
the ink vector and sums the contribution from each instance i: X amplitudei*add texture(typei 'x'y'level 
-base leveli) i Note that this arrangement allows us to linearly combine dif­ferent ink vectors. As any 
ink element is attenuated, the detail values added to its corresponding texture instance will be equally 
attenuated.  5.3 Some Examples of Texture Types For each type of texture, there is a corresponding procedural 
band­pass pyramid; a function that computes how much detail must be added into that texture at each level. 
We now describe the procedural bandpass pyramid used to construct various speci.c types of tex­ture. 
Each type is speci.ed in Boldface, followed by its re.nement method. The variable dlevel below refers 
to the difference between the current level and the texture instance s base magni.cation level. Note 
that dlevel = 0 when the .rst coarse texture is painted, and that dlevel >0 whenever the texture is subsequently 
sharpened. White: if dlevel =0 then 1 else 0 Rock: dlevel/2 pseudorandom(x,y,dlevel)/ 2 Stripes: sqr 
wave(x)/ (dlevel +0.4) 0 1 Sawtooth: saw wave(x)/ (dlevel +1) 0 14 Squares: sqr wave(x)* sqr wave(y)/ 
(dlevel +0.1) 0 14 where sqr wave and saw wave are de.ned as follows: square wave(x) if i= 0ori =n/2-1 
then 1 else if i=n/2 ori =n-1 then -1 else 0 saw wave(x) if i=0 then 1 else if i= n-1 then -1 else 0 
 with n := 2dlevel+1andi := xmod n. In practice each of the above power curves is computed only once, 
and then stored in a table, which is subsequently indexed by dlevel. These particular power curves depend 
on the B-spline reconstruction kernel that we use, [10]. A system with a different reconstruction kernel 
would require different curves. White is handled as a special case, since White texture requires no sharpening. 
For this reason, in practice we modulate all White texture instances in one element of drawing ink. We 
change the amplitude of this element each time the user lightens or darkens the brush. 6 Examples In 
this section we give some examples of the procedural multires­olution textures and their use. First we 
show some examples of magnifying and compositing textures. Then we present a more ad­vanced example: 
creating an entire multiresolution terrain model by painting with procedural textures. 6.1 Simple Examples 
Figure 2(a) shows the name of a beautiful city written with a saw­tooth generating brush. Figure 2(b) 
is a magni.ed view into the dot above the letter "i". Figure 2(c) is magni.ed even further. Each im­age 
is four times the magni.cation of the previous image. We note that the intensity ramps in the horizontal 
direction are piecewise linear. Figure 3 shows successive magni.cations of a translucent blend­ing of 
three active textures: rock, squares, and horizontal stripes. Note how in Figure 3(c) the squares texture 
smoothly blends into the stripes texture. Figure 4 shows the effect of a smoothing brush. This brush 
sim­ply averages neighboring values in the image, and blends the result with White ink. The White ink 
element modulates brightness; the alpha ink element modulates opacity. The most important effect of this 
brush is that it locally reduces or eliminates texture sharpening. Figure 4(a) shows rock texture. Figure 
4(b) shows an X drawn over this with a smoothing brush. Figure 4(c) is a magni.ed view into just below 
the central cross of the X .  6.2 Advanced Example: A Terrain Model Terrain modeling is an important 
application where fractal images may be used to describe elevation data [3], [4]. This example exploits 
the expressiveness of procedural textures in a system that combines interactive painting with 3D visualization. 
Figure 5 shows a terrain being interactively modeled in the system. On the right, the user sees an interactive 
height .eld view of the intensity image. First we will describe this interaction tool, and then we will 
discuss the example in Figure 5. The user sees a perspective view of intensity, as a height .eld mesh. 
The mesh is rendered back to front, so no Z buffer is required. The brightness at each location on this 
mesh is composed of a weighted sum of that location s height and directional derivatives. The user can 
interactively pan and tilt this view with the mouse. In order to maintain interactivity on platforms 
that do not have polygon transformation hardware, we use progressive re.nement. The user can change the 
view and see the results in real time over a coarse mesh approximation. Then while the user goes back 
to painting, the terrain model gradually increases to full resolution over several seconds, as a background 
activity. Any further changes to the view during this time will interrupt the re.nement process and startitagain. 
In .gure 5(a) we see a height .eld created by magnifying the view into a squiggle drawn with a rock texture. 
In .gure 5(b) we have drawn a new rock texture instance as a wavy horizontal across the image. In the 
height .eld view, this appears as a mountainous ridge across the terrain. We also have drawn with a dark 
erasing brush near the bottom of the image to simulate the .at terrain of a river. In .gure 5(c) we magnify 
the view into the bay that appeared in the lower left of .gure 5(b). Then, we sprinkle some individual 
bright squares near the river. In the height .eld view these appear as tall buildings. Note that the 
edges of these buildings will be perfectly sharp, no matter how close we get. In this same .gure, we 
have also used a transparent brush to paint some squares generating texture at several scales, in order 
to simulate a large cityscape. This can be seen at the left edge of the image. As we paint, the opacity 
of the brush controls the height of the buildings at the brush. The more time we spend over any area, 
the taller the buildings grow. This entire process took less than a minute of painting. 7 Using Images 
In this section we discuss how images can be used to complement our framework for multiresolution procedural 
textures. 7.1 Images as Procedural Brushes In addition to using textures that are strictly procedural, 
we can use a multiresolution image as a texture source [7]. This enables many useful paint operations 
and provides a way to seamlessly incorporate multiresolution images into the system. A multiresolution 
source image is represented internally by a bandpass pyramid, ie. a coarse image at the base resolution 
level and a sequence of detail images for the other levels. The corresponding data structure is: -size 
of the base image (nxm) -number of levels of the bandpass pyramid -bandpass pyramid The encapsulation 
of images in a procedural brush is done by associating a source image with a texture instance. When the 
user selects a source image to paint, a new texture is instantiated: an index of the ink vector is allocated, 
a reference from this index to the data structure representing the selected image is established, and 
the base level is set to the current resolution level. An image texture is similar to a procedural texture 
with the exception that the values are copied from its bandpass pyramid instead of being generated algorithmically. 
The operation is divided in two parts: at the base level pixels are copied from the base image onto the 
canvas; when the user changes level and the image needs to be re.ned, the bandpass coef.cients are added. 
Before using a source image as a procedural brush, we need .rst to construct a bandpass pyramid for it. 
For this purpose, we employ a modi.ed version of the wavelet transform engine used in the system. This 
can be done as an independent operation by a program that builds a library of source image brushes or 
as a built-in operation of the paint system to create a source image brush from any rectangular region 
of the canvas. Source images can be replicated by the procedural brush to cover a larger region of the 
canvas by using a rectangular tiling arrangement. Image replication has to be taken into account in the 
construction of the bandpass pyramid. If we wish the texture to tile the source image, then we must employ 
toroidal end condi­tions when building the pyramid, with both horizontal and vertical wraparound.  7.2 
Synthesizing Textures from Image Samples The design of procedural textures usually requires some kind 
of programming [9]. A powerful alternative to this way of creat­ing procedural textures is the automatic 
generation of a procedural bandpass pyramid from a sample image of the texture. For this, we need a mechanism 
to predict the coef.cients of the next level of the pyramid based on the current ones. In some cases, 
the prediction rules are very simple. For example, in the case of perfectly sharp edges and fractals 
the coef.cients are multiples of each other as shown in subsection 5.3. In the general case, the mappings 
between coef.cients at dif­ferent levels are inherently non-linear and multi-modal. We are currently 
investigating a technique to generate these mappings from a statistical analysis of a sample texture. 
This technique is similar to the one used for image compression in [8]. The method is based on a vector 
quantization of the bandpass pyramid and a subsequent analysis of the correlation between the codes generated 
by the vec­tor quantizer. This analysis allows us to build a prediction table for each code in a codebook, 
which gives a mapping from coarser to more detailed bandpass coef.cients. The texture procedure then 
uses the prediction tables to generate new bandpass coef.cients during re.nement. 8 Conclusions In this 
paper we have introduced the concept of actively procedural multiresolution paint textures. These are 
live picture elements that continue to re.ne themselves when viewed at magni.cations greater than the 
one at which they were originally painted. 8.1 Summary We presented a framework to implement these active 
textures and incorporate them as procedural brushes into a multiresolution paint system. We have described 
a simple way to linearly combine primitive texture elements in order to create complex composite textures. 
We haveexplainedhowtousesourceimagesinproceduralbrushes. We have discussed how to automatically generate 
multiresolution pro­cedural textures from an image sample of the texture. We have given several examples 
of the use of our framework in a multiresolution paint system. In conclusion, active procedural texture 
constitutes a powerful new painting tool to more fully exploit the power of multiresolution paint systems. 
 8.2 Future Work Future work should go in several main directions: Enhancing the capabilities of the 
current framework;  Adding more texture generators of interest;  Developing an environment for the 
design of procedural tex­tures;  Investigating extrapolation techniques to generate missing image details. 
 The current framework could be enhanced by incorporating the notion of layers and interpreted code, 
as in [9], to combine texture instances in arbitrary ways. This capability would allow the generation 
of arbitrarily complex in.nite resolution textures from simple primitives and operators. We also plan 
to build up a family of texture generators for simulating terrains of architectural interest. This would 
include treetops, shrubbery, the appearance of rows of houses including slanted roofs and chimneys and 
even roof texture and so on. The idea is that an Architect could work up a sketch of a landscaping, 
or cluster of dwellings, or a city, using broad strokes perhaps just to play with the feel of how various 
arrangements would look. From a production standpoint, it would be good to have a com­plete environment 
for designing multiresolution procedural tex­tures. This would include programming, testing and debugging 
facilities that can work together with the paint system. A .nal area of investigation is the use of techniques 
for analyzing the correlation between levels of the bandpass pyramid described in Section 3 in order 
to perform extrapolation of coarse resolution images when detail information is not available. Acknowledgements 
The authors wish to thank Lance Williams for fruitful discussions and for pointing out the work of [8]. 
Many thanks also to Stephane Mallat for several valuable suggestions. Special thanks to Karl Sims for 
inspiring comments on multiscale noise. This work was partially supported by grants from MCT/CNPq ConselhoNacionalde 
DesenvolvimentoCient´i.co e Tecnol´ogico and from the National Science Foundation. REFERENCES [1] Deborah 
Berman, Jason Bartell, and David Salesin. Multires­olution painting and compositing. Computer Graphics, 
An­nual Conference Series (SIGGRAPH 94 Proceedings), pages 85 90, 1994. [2] Peter J. Burt. The laplacian 
pyramid as a compact image code. IEEE Transactions on Communications, 31:532 540, April 1983. [3] Alain 
Fournier, Don Fussell, and Loren Carpenter. Computer rendering of stochastic models. Communications of 
the ACM, 25(6):371 384, June 1982. [4] John Peter Lewis. Texture synthesis for digital painting. Com­puter 
Graphics (SIGGRAPH 84 Proceedings), 18(3):245 252, July 1984. [5] Lively pictures. Byte Magazine, 20(1):171 
174, 1995. Live Picture Product Review. [6] Stephane Mallat. Multifrequency channel decompositions of 
images and wavelet models. IEEE Trans. on Acoust. Signal Speech Process., 37(12):2091 2110, 1989. [7] 
Joan M. Ogden, Edward H. Adelson, J.R. Bergen, and Pe­ter J. Burt. Pyramid-based computer graphics. RCA 
Engineer, 30(5):4 13, September October 1985. [8] Alex Pentland and Bradley Horowitz. A practical approach 
to fractal-based image compression. In Proceedings of Data Compression Conference, pages 176 185, held 
in Snowbird, UT, 1991. IEEE Computer Society Press. [9] Ken Perlin. An image synthesizer. Computer Graphics 
(SIG-GRAPH 85 Proceedings), 19(3):287 293, 1985. [10] Ken Perlin and Luiz Velho. A wavelet representation 
for unbounded resolution painting. Technical report, New York University, New York, 1992. [11] Lance 
Williams. Pyramidal parametrics. Computer Graphics (SIGGRAPH 83 Proceedings), 17(3):1 11, July 1983. 
[12] Xres, the alternative to photoshop? Mac Format Magazine, 23, pages 72 74, 1995. XRes Graphics Software 
Review.  (a) (b) (c) Figure 2: Sawtooth Texture (a) (b) (c) Figure 3: Blending of rock, squares, and 
horizontal stripes (a) (b) (c)   Figure 4: The effect of a smoothing brush (a)  (b) (c)  Figure 
5: Interactive design of a terrain model     
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218439</article_id>
		<sort_key>161</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[Spherical wavelets]]></title>
		<subtitle><![CDATA[efficiently representing functions on the sphere]]></subtitle>
		<page_from>161</page_from>
		<page_to>172</page_to>
		<doi_number>10.1145/218380.218439</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218439</url>
		<keywords>
			<kw><![CDATA[sphere]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>G.1.0</cat_node>
				<descriptor>Numerical algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Nonlinear approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor>Transform methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003717</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computation of transforms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10003636</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Approximation algorithms analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003739</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Nonlinear equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003724</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Numerical differentiation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40023875</person_id>
				<author_profile_id><![CDATA[81100117380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schr&#246;der]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematics, University of South Carolina]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P300467</person_id>
				<author_profile_id><![CDATA[81100340025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sweldens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematics, Department of Computer Science, Katholieke Universiteit Leuven, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALFELD, P., NEAMTU, M., AND SCHUMAKER, L. L. Bernstein- Bdzier polynomials on circles, sphere, and sphere-like surfaces. Preprint.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CARNICER, J. M., DAHMEN, W., AND PElqA, J. M. Local decompositions of refinable spaces. Tech. rep., Insitut far Geometrie und angewandete Mathematik, RWTH Aachen, 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CHRISTENSEN, E H., STOLLNITZ, E. J., SALESIN, D. H., AND DEROSE, T. D. Wavelet Radiance. In Proceedings of the 5th Eurographics Workshop on Rendering, 287-302, June 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[COHEN, A., DAUBECHIES, I., AND FEAUVEAU, J. Biorthogonal bases of compactly supported wavelets. Comm. Pure Appl. Math. 45 (1992), 485-560.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[COHEN, A., DAUBECHIES, I., JAWERTH, B., AND VIAL, P. Multiresolution analysis, wavelets and fast algorithms on an interval. C. R. Acad. Sci. Paris Sdr. I Math. I, 316 (1993), 417-421.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DAHLKE, S., DAHMEN, W., SCHMITT, E., AND WEINREICH, I. Multiresolution analysis and wavelets on S2 and S3. Tech. Rep. 104, Institut far Geometrie und angewandete Mathematik, RWTH Aachen, 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[DAHMEN, W. Stability of multiscale transformations. Tech. rep., Institut ftir Geometrie und angewandete Mathematik, RWTH Aachen, 1994.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DAHMEN, W., PROSSDORF, S., AND SCHNEIDER, R. Multiscale methods for pseudo-differential equations on smooth manifolds. In Conference on Wavelets: Theory, Algorithms, and Applications, C. K. C. et al., Ed. Academic Press, San Diego, CA, 1994, pp. 385-424.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130655</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[DAUBECHIES, I. Ten Lectures on Wavelets. CBMS-NSF Regional Conf. Series in Appl. Math., Vol. 61. Society for Industrial and Applied Mathematics, Philadelphia, PAL, 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DUTTON, G. Locational Properties of Quaternary Triangular Meshes. In Proceedings of the Fourth International Symposium on Spatial Data Handling, 901-910, July 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>78958</ref_obj_id>
				<ref_obj_pid>78956</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[DYN, N., LEVlN, D., AND GREGORY, J. A Butterfly Subdivision Scheme for Surface Interpolation with Tension Control. Transactions on Graphics 9, 2 (April 1990), 160-169.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949560</ref_obj_id>
				<ref_obj_pid>949531</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[FEKETE, G. Rendering and Managing Spherical Data with Sphere Quadtrees. In Proceedings of Visualization 90, 1990.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[FREEDEN, W., AND WINDHEUSER, U. Spherical Wavelet Transform and its Discretization. Tech. Rep. 125, Universit,it Kaiserslautern, Fachbereich Mathematik, 1994.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[GIRARDI, M., AND SWELDENS, W. A new class of unbalanced Haar wavelets that form an unconditional basis for Lv on general measure spaces. Tech. Rep. 1995:2, Industrial Mathematics Initiative, Department of Mathematics, University of South Carolina, 1995. (ftp ://ftp. math. scarolina, edu/pub/imi_95/imi95_2, ps).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192202</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[GONDEK, J. S., MEYER, G. W., AND NEWMAN, J. G. Wavelength Dependent Reflectance Functions. In Computer Graphics Proceedings, Annual Conference Series, 213-220, 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166146</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S., SCHRODER, R, COHEN, M., AND HANRAHAN, R Wavelet Radiosity. In Computer Graphics Proceedings, Annual Conference Series, 221-230, August 1993.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199410</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S. J., AND COHEN, M. F. Hierarchical and Variational Geometric Modeling with Wavelets. In Proceedings Symposium on Interactive 3D Graphics, 35-42, April 1995.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192169</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LIU, Z., GORTLER, S. J., AND COHEN, M. F. Hierarchical Spacetime Control. Computer Graphics Proceedings, Annual Conference Series, 35-42, July 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222932</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M. Multiresolution Analysis for Smfaces of Arbitrary Topological Type. PhD thesis, University of Washington, 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[LOUNSBERY, M., DEROSE, T. D., AND WARREN, J. Multiresolution Surfaces of Arbitrary Topological Type. Department of Computer Science and Engineering 93-10-05, University of Washington, October 1993. Updated version available as 93-10-05b, January, 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MITREA, M. Singular integrals, Ha~ffy spaces and Clifford wavelets. No. 1575 in Lecture Notes in Math. 1994.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>160683</ref_obj_id>
				<ref_obj_pid>160673</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. M. Scattered Data Modeling. IEEE Computer Graphics and Applications 13, 1 (January 1993), 60-70.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[SCHLICK, C. A customizable reflectance model for everyday rendering. In Fourth Eurographics Workshop on Rendering, 73-83, June 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[SCHRODER, P., AND HANRAHAN, P. Wavelet Methods for Radiance Computations. In Proceedings 5th Eurographics Workshop on Rendering, June 1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SCHRODER, P., AND SWELDENS, W. Spherical wavelets: Texture processing. Tech. Rep. 1995:4, Industrial Mathematics Initiative, Department of Mathematics, University of South Carolina, 1995. (ftp ://ftp. math. scarolina, edu/pub/imi_95/imi95_4, ps ).]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122739</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SILLION, F. X., ARVO, J. R., WESTIN, S. H., AND GREENBERG, D. E A global illumination solution for general reflectance distributions. Computer Graphics (SIGGRAPH '91 Proceedings), Vol. 25, No. 4, pp. 187-196, July 1991.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SWELDENS, W. The lifting scheme: A construction of second generation wavelets. Department of Mathematics, University of South Carolina.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SWELDENS, W. The lifting scheme: A customdesign construction of biorthogonal wavelets. Tech. Rep. 1994:7, Industrial Mathematics Initiative, Department of Mathematics, University of South Carolina, 1994. (ftp ://ftp. math. scarolina, edu/pub/imi_94/imi94_7, ps ).]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197963</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[WESTERMAN, R. A Multiresolution Framework for Volume Rendering. In Proceedings ACM Workshop on Volume Visualization, 51-58, October 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[WESTIN, S. H., ARVO, J. R., AND TORRANCE, K. E. Predicting reflectance functions from complex surfaces. Computer Graphics (SIGGRAPH '92 Proceedings), Vol. 26, No. 2, pp. 255-264, July 1992.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spherical Wavelets: Ef.ciently Representing Functions on the Sphere Peter Schr¨Wim Sweldens*z oder*y 
 University of South Carolina Abstract Wavelets have proven to be powerful bases for use in numerical 
analysis and signal processing. Their power lies in the fact that they only require a small number of 
coef.cients to represent gen­eral functions and large data sets accurately. This allows compres­sion 
and ef.cient computations. Classical constructions have been limited to simple domains such as intervals 
and rectangles. In this paper we present a wavelet construction for scalar functions de.ned on the sphere. 
We show how biorthogonal wavelets with custom properties can be constructed with the lifting scheme. 
The bases are extremely easy to implement and allow fully adaptive subdivi­sions. We give examples of 
functions de.ned on the sphere, such as topographic data, bidirectional re.ection distribution functions, 
and illumination, and show how they can be ef.ciently represented with spherical wavelets. CR Categories 
and Subject Descriptors: I.3.0 [Computer Graphics]: General; G.1.0 [Numerical Analysis]: General Numerical 
Algorithms; G.1.1 Interpolation Smoothing; G.1.2 Approximation Nonlinear Ap­proximation. Additional 
Key Words and Phrases: wavelets, sphere.  1 Introduction 1.1 Wavelets Over the last decade wavelets 
have become an exceedingly pow­erful and .exible tool for computations and data reduction. They offer 
both theoretical characterization of smoothness, insights into the structure of functions and operators, 
and practical numerical tools which lead to faster computational algorithms. Examples of their use in 
computer graphics include surface and volume illumi­nation computations [16, 29], curve and surface modeling 
[17], and animation [18] among others. Given the high computational de­mands and the quest for speed 
in computer graphics, the increasing exploitation of wavelets comes as no surprise. While computer graphics 
applications can bene.t greatly from wavelets, these applications also provide new challenges to the 
underlying wavelet technology. One such challenge is the con­struction of wavelets on general domains 
as they appear in graphics applications. Classically, wavelet constructions have been employed on in.­nite 
domains (such as the real line R and plane R2). Since most practical computations are con.ned to .nite 
domains a number of boundary constructions have also been developed [5]. However, *Department of Mathematics. 
yDepartment of Computer Science. zDepartment of Computer Science, Katholieke Universiteit Leuven, Belgium. 
 Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
 Figure 1: The geodesic sphere construction starting with the icosa­hedron on the left (subdivision level 
0) and the next 2 subdivision levels. wavelet type constructions for more general manifolds have only 
recently been attempted and are still in their infancy. Our work is inspired by the ground breaking work 
of Lounsbery et al.[20, 19] (hereafter referred to as LDW). While their primary goal was to ef.ciently 
represent surfaces themselves we examine the case of ef.ciently representing functions de.ned on a surface, 
and in particular the case of the sphere. Although the sphere appears to be a simple manifold, techniques 
from R2 do not easily extend to the sphere. Wavelets are no excep­tion. The .rst construction of wavelets 
on the sphere was introduced by Dahlke et al.[6] using a tensor product basis where one factor is an 
exponential spline. To our knowledge a computer imple­mentation of this basis does not exist at this 
moment. A continuous wavelet transform and its semi-discretization were proposed in [13]. Both these 
approaches make use of a (<,0)parameterization of the sphere. This is the main difference with our method, 
which is parameterization independent. Aside from being of theoretical interest, a wavelet construction 
for the sphere leading to ef.cient algorithms, has practical appli­cations since many computational problems 
are naturally stated on the sphere. Examples from computer graphics include: manipula­tion and display 
of earth and planetary data such as topography and remote sensing imagery, simulation and modeling of 
bidirectional re.ection distribution functions, illumination algorithms, and the modeling and processing 
of directional information such as envi­ronment maps and view spheres. In this paper we describe a simple 
technique for constructing biorthogonal wavelets on the sphere with customized properties. The construction 
is an incidence of a fairly general scheme referred to as the lifting scheme [27, 28]. The outline of 
the paper is as follows. We .rst give a brief review of applications and previous work in computer graphics 
in­volving functions on the sphere. This is followed by a discussion of wavelets on the sphere. In Section 
3 we explain the basic ma­chinery of lifting and the fast wavelet transform. After a section on implementation, 
we report on simulations and conclude with a discussion and suggestions for further research. 1.2 Representing 
Functions on the Sphere Geographical information systems have long had a need to repre­sent sampled data 
on the sphere. A number of basic data structures originated here. Dutton [10] proposed the use of a geodesic 
sphere Figure 2: Recursive subdivision of the octahedral base shape as used by LDW for spherelike surfaces. 
Level 0 is shown on the left followed by levels 2 and 4. construction to model planetary relief, see 
Figure 1 for a picture of the underlying subdivision. More recently, Fekete [12] described the use of 
such a structure for rendering and managing spherical geo­graphic data. By using hierarchical subdivision 
data structures these workers naturally built sparse adaptive representations. There also exist many 
non-hierarchical interpolation methods on the sphere (for an overview see [22]). An important example 
from computer graphics concerns the rep­resentation of functions de.ned over a set of directions. Perhaps 
the most notable in this category are bidirectional re.ectance distri­bution functions (BRDFs) and radiance. 
The BRDF, fr(w i,w o), x,wdescribes the relationship at a point wxon a surface between incoming radiance 
from direction w iand outgoing radiance in direction wo. It can be described using spherical harmonics, 
the natural extension of Fourier basis functions to the sphere, see e.g. [30]. These basis functions 
are globally supported and suffer from some of the same dif.culties as Fourier representations on the 
line such as ringing. To our knowledge, no fast (FFT like) algorithm is available for spheri­cal harmonics. 
Westin et al. [30] used spherical harmonics to model BRDFs derived from Monte Carlo simulations of micro 
geometry. Noting some of the disadvantages of spherical harmonics, Gondek et al.[15] used a geodesic 
sphere subdivision construction [10, 12] in a similar context. The result of illumination computations, 
the radiance L(wx,w ), is a function which is de.ned over all surfaces and all directions. For example, 
Sillion et al. [26] used spherical harmonics to model the directional distribution of radiance. As in 
the case of BRDF representations, the disadvantages of using spherical harmonics to represent radiance 
are due to the global support and high cost of evaluation. Similarly no locally controlled level of detail 
can be used. In .nite element based illuminations computations wavelets have proven to be powerful bases, 
see e.g. [24, 3]. By either reparame­terizing directions over the set of visible surfaces [24], or mapping 
them to the unit square [3], wavelets de.ned on standard domains (rectangular patches) were used. Mapping 
classical wavelets on some parameter domain onto the sphere by use of a parameterization provides one 
avenue to con­struct wavelets on the sphere. However, this approach suffers from distortions and dif.culties 
due to the fact that no globally smooth parameterization of the sphere exists. The resulting wavelets 
are in some sense contaminated by the parameterization. We will examine the dif.culties due to an underlying 
parameterization, as opposed to an intrinsic construction, when we discuss our construc­tion. We .rst 
give a simple example relating the compression of sur­faces to the compression of functions de.ned on 
surfaces.  1.3 An Example LDW constructs wavelets for surfaces of arbitrary topological type which are 
parameterized over a polyhedral base complex. For the case of the sphere they employed an octahedral 
subdivision domain (see Figure 2). In this framework a given goal surface such as the earth is parameterized 
over an octahedron whose triangular faces are successively subdivided into four smaller triangles. Each 
vertex can now be displaced radially to the limit surface. The resulting sequence of surfaces then represents 
the multiple levels of detail representation of the .nal surface. Figure 3: A simple example of re.nement 
on the line. The ba­ S S sis functions at the top can be 0.510.5 1-0.5 -0.5 expressed as linear combina­ 
tions of the re.ned functions at the bottom. As pointed out by LDW compressing surfaces is closely related 
to compressing functions on surfaces. Consider the case of the unit 2 S2 sphere and the function f(s)=f(0,<)=cos0with 
s2. We can think of the graph of this function as a surface over the sphere whose height (displaced along 
the normal) is the value of the function f. Hence an algorithm which can compress surfaces can also compress 
the graph of a scalar function de.ned over some surface. At this point the domain over which the compression 
is de.ned becomes crucial. Suppose we want to use the octahedron O.De.ne the projection T: O--S2, s=T(p)=p/kpk. 
We then have f (p)=f(T(p))with p2O. Compressing f(s)with wavelets on the sphere is now equivalent to 
compressing f (p)with wavelets de.ned on the octahedron. While fis simply a quadratic function over the 
sphere, f is considerably more complicated. For example a basis over the sphere which can represent quadratics 
exactly (see Section 3.4) will trivially represent f. The same basis over the octahedron will only be 
able to approximate f . This example shows the importance of incorporating the un­derlying surface correctly 
for any construction which attempts to ef.ciently represent functions de.ned on that surface. In case 
of compression of surfaces themselves one has to assume some canon­ical domain. In LDW this domain was 
taken to be a polyhedron. By limiting our program to functions de.ned on a .xed surface (sphere) we can 
custom tailor the wavelets to it and get more ef.­ciency. This is one of the main points in which we 
depart from the construction in LDW.  2 Wavelets on the Sphere 2.1 Second Generation Wavelets Wavelets 
are basis functions which represent a given function at multiple levels of detail. Due to their local 
support in both space and frequency, they are suited for sparse approximations of func­tions. Locality 
in space follows from their compact support, while locality in frequency follows from their smoothness 
(decay towards high frequencies) and vanishing moments (decay towards low fre­quencies). Fast O(n)algorithms 
exist to calculate wavelet coef.­cients, making the use of wavelets ef.cient for many computational problems. 
In the classic wavelet setting, i.e., on the real line, wavelets are de.ned as the dyadic translates 
and dilates of one particular, .xed function. They are typically built with the aid of a scaling function. 
Scaling functions and wavelets both satisfy re.nement relations (or two scale relations). This means 
that a scaling function or wavelet at a certain level of resolution (j) can be written as a linear combination 
of scaling basis functions of the same shape but scaled at one level .ner (level j+1), see Figure 3 for 
an example. The basic philosophy behind second generation wavelets is to build wavelets with all desirable 
properties (localization, fast trans­form) adapted to much more general settings than the real line, 
e.g., Functions 'j,k,k2K j primal scaling functions 'j,k,k2K j dual scaling functions Jj,m,m2M j primal 
wavelets Jj,m,m2M j dual wavelets Biorthogonality relationships h'j,k, 'j,k0i=°k,k0 hJj,m, Jj0,m0i=° 
m,m0° j,j0 h'j,k, Jj,mi=0 hJj,m, 'j,ki=0 'j,kand 'j,k0are biorthogonal Jj,mand Jj0,m0are biorthogonal 
Vj1 Wj Wj1 Vj Vanishing moment relations Jj,mhas Nvanishing moments Jj,mhas eNvanishing moments 'j,kreprod. 
polyn. degree <N 'j,kreprod. polyn. degree <eN Re.nement relations 'j,k= P l2K(j+1)hj,k,l'j+1,l 'j,k= 
P l2K(j+1) hj,k,l 'j+1,lJj,m= P l2K(j+1)gj,m,l'j+1,l Jj,m= P l2K(j+1) gj,m,l 'j+1,lVj=clos spanf'j,kjk2K 
j gWj=clos spanfJj,mjm2M j gVjEWj=Vj+1 scaling function re.nement eq. dual scaling function re.nement 
eq. wavelet re.nement equation dual wavelet re.nement equation with V0 the coarsest space with W0 the 
coarsest space wavelets encode difference between levels of approximation Wavelet transforms Aj,k=hf, 
'j,ki 'j,m=hf, Jj,mi scaling function coef.cient wavelet coef.cient Forward Wavelet Transform (Analysis) 
Aj,k= P l2K(j) hj,k,lAj+1,l 'j,m= P l2M(j) gj,m,lAj+1,l scaling function coeff., .ne to coarse wavelet 
coeff., .ne to coarse Inverse Wavelet Transform (Synthesis) Aj+1,l= P k2K(j)hj,k,lAj,k + P m2M(j)gj,m,l'j,m 
scaling function coeff., coarse to .ne Table 1: Quick reference to the notation and some basic relation­ships 
for the case of second generation biorthogonal wavelets. wavelets on manifolds. In order to consider 
wavelets on a surface, we need a construction of wavelets which are adapted to a measure on the surface. 
In the case of the real line (and classical con­structions) the measure is dx, the usual translation 
invariant (Haar) Lebesgue measure. For a sphere we will denote the usual area measure by d. Adaptive 
constructions rely on the realization that translation and dilation are not fundamental to obtain the 
wavelets with the desired properties. The notion that a basis function can be written as a .nite linear 
combination of basis functions at a .ner, more subdivided level, is maintained and forms the key behind 
the fast transform. The main difference with the classical wavelets is that the .lter coef.cients of 
second generation wavelets are not the same throughout, but can change locally to re.ect the changing 
(non translation invariant) nature of the surface and its measure. Classical wavelets and the corresponding 
.lters are constructed with the aid of the Fourier transform. The underlying reason is that translation 
and dilation become algebraic operations after Fourier transform. In the setting of second generation 
wavelets, translation and dilation can no longer be used, and the Fourier transform thus becomes worthless 
as a construction tool. An alternative construc­tion is provided by the lifting scheme.  2.2 Multiresolution 
Analysis We .rst introduce multiresolution analysis and wavelets and set some notation. For more mathematical 
detail the reader is referred to [9]. All relationships are summarized in Table 1. Consider the function 
space L2 =L2(S2 ,d), i.e., all functions of .nite energy de.ned over S2. We de.ne a multiresolution anal­ysis 
as a sequence of closed subspaces VjCL2, with j.0, so that I VjCVj+1, (.ner spaces have higher index) 
S II j>0 Vjis dense in L2, III for each j, scaling functions <j,kwith k2K(j)exist so that f<j,kjk2K(j)gis 
a Riesz basis1 of Vj. Think of K(j)as a general index set where we assume that K(j)C rj K(j+1). In the 
case of the real line we can take K(j)=2Z, rj rj while for an interval we might have K(j)=f0,2,...,1-2g. 
Note that, unlike the case of a classical multiresolution analysis, the scaling functions need not be 
translates or dilates of one particular function. Property (I) implies that for every scaling function 
<j,k, coef.cients fhj,k,lgexist so that P <j,k =hj,k,l<j+1,l. (1) l The hj,k,lare de.ned for j.0, k2K(j),and 
l2K(j+1). Each scaling function satis.es a different re.nement relation. In the classical case we have 
hj,k,l =hlr2k, i.e., the sequences hj,k,l are independent of scale and position. Each multiresolution 
analysis is accompanied by a dual multires­olution analysis consisting of nested spaces V jwith bases 
given by dual scaling functions <j,k, which are biorthogonal to the scaling functions: h<j,k,< j,k0i=tk,k0for 
k,k0 2K(j), R where hf,gi=fgdis the inner product on the sphere. The dual scaling functions satisfy 
re.nement relations with coef.cients  fhj,k,lg. In case scaling functions and dual scaling functions 
coincide, (<j,k =< j,kfor all jand k) the scaling functions form an orthog­onal basis. In case the multiresolution 
analysis and the dual mul­tiresolution analysis coincide (Vj =V jfor all jbut not necessarily <j,k =< 
j,k) the scaling functions are semi-orthogonal. Orthog­onality or semi-orthogonality sometimes imply 
globally supported basis functions, which has obvious practical disadvantages. We will assume neither 
and always work in the most general biorthog­onal setting (neither the multiresolution analyses nor the 
scaling functions coincide), introduced for classical wavelets in [4]. One of the crucial steps when 
building a multiresolution analysis is the construction of the wavelets. They encode the difference between 
two successive levels of representation, i.e., they form a basis for the spaces Wjwhere VjEWj =Vj+1. 
Consider the set of functions f1j,mjj.0,m2M(j)g,where M(j)CK(j+1) is again an index set. If 1. the set 
is a Riesz basis for L2(S2), 2. the set f1j,mjm2M(j)gis the Riesz basis of Wj,  we say that the 1j,mde.ne 
a spherical wavelet basis. Since WjC Vj+1,we have P 1j,m =gj,m,l<j+1,lfor m2M(j).(2) l An important 
property of wavelets is that they have vanishing moments. The wavelets 1j,mhave Nevanishing moments if 
Ne e independent polynomials Pi,0 6iNexist so that h1j,m,Pii=0, for all j.0,m2M(j). Here the polynomials 
Piare de.ned as the restriction to the sphere of polynomials on R3. Note that inde­pendent polynomials 
on R3 can become dependent after restriction 222 to the sphere, e.g., f1,x,y,zg. 1A Riesz basis of some 
Hilbert space is a countable subset ffkgso that every P element fof the space can be written uniquely 
as f= ckfk, and positive P k 2 22 constants Aand Bexist with Akfk6jckj6Bkfk. k For a given set of wavelets 
we have dual basis functions 1 j,m which are biorthogonal to the wavelets, or h1j,m,1j0,m0i= 00 tm,m0tj,j0for 
j,j00,m2M(j),m2M(j). This implies h1j,m,<j,ki=h< j,k,1j,mi=0for m2M(j)and k2K(j), and for f2L2 we can 
write the expansion PP f= j,m h1 j,m,fi1j,m = Ij,m1j,m (3) j,m Given all of the above relationships we 
can also write the scaling functions <j+1,las a linear combination of coarser scaling functions and wavelets 
using the dual sequences (cf. Eqs. (1,2)) PP <j+1,l =hj,k,l<j,k+g j,m,l1j,m. km If not stated otherwise 
summation indices are understood to run over k2K(j), l2K(j+1),and m2M(j). Given the set of scaling function 
coef.cients of a function f, fAn,k =hf,< j,kijk2K(n)gwhere nis some .nest resolution level, the fast 
wavelet transform recursively calculates the fIj,mj 0 6jn,m2M(j)g,and fA0,kjk2K(0)g, i.e., the coarser 
approximations to the underlying function. One step in the fast wavelet transform computes the coef.cients 
at a coarser level (j) from the coef.cients at a .ner level (j+1) PP Aj,k =hj,k,lAj+1,land Ij,m =g j,m,lAj+1,l. 
ll A single step in the inverse transform takes the coef.cients at the coarser levels and reconstructs 
coef.cients at a .ner level PP Aj+1,l =hj,k,lAj,k+gj,m,lIj,m. km  3 Wavelet Construction and Transform 
We .rst discuss the lifting scheme [27, 28]. After the introduc­ing of the algebra we consider two important 
families of wavelet bases, interpolating and generalized Haar. At the end of this section we give a concrete 
example which shows how the properties of a given wavelet basis can be improved by lifting it and lead 
to better compression. Lifting allows us to build our bases in a fully biorthogonal frame­work. This 
ensures that all bases are of .nite (and small) support and the resulting .lters are small and easy to 
derive. As we will see it is also straightforward to incorporate custom constraints into the resulting 
wavelets. 3.1 The Lifting Scheme The whole idea of the lifting scheme is to start from one basic multiresolution 
analysis, which can be simple or even trivial, and construct a new, more performant one, i.e., the basis 
functions are smoother or the wavelets have more vanishing moments. In case the basic .lters are .nite 
we will have lifted .lters which are also .nite. We will denote coef.cients of the original multiresolution 
anal­ysis with an extra superscript o(from old or original), starting with h oo o j,k,l, j,k,l, gj,k,lj,k,l 
 the .lters ho , and g.The lifting scheme now states that a new set of .lters can be found as P hoo k 
hj,k,l =j,k,l,gj,m,l =gj,m,l -sj,k,mhj,k,l, P o h o g j,m,l =g j,m,l,hj,k,l =j,k,l+sj,k,mg j,m,l, m and 
that, for any choice of fsj,k,mg, the new .lters will automat­ically be biorthogonal, and thus lead to 
an invertible transform. The scaling functions <j,lare the same in the original and lifted multiresolution 
analysis, while the dual scaling function and primal wavelet change. They now satisfy re.nement relations 
PP o lk 1j,m =gj,m,l<j+1,l-sj,k,m<j,k (4) PP h o < j,k = j,k,l< j+1,l+sj,k,m 1j,m. lm Note that the dual 
wavelet has also changed since it is a linear combination (with the old coef.cients g o) of a now changed 
dual scaling function. Equation (4) is the key to .nding the fsj,k,mjkg coef.cients. Since the scaling 
functions are the same as in the original multiresolution analysis, the only unknowns on the right hand 
side are the sj,k,m. We can choose them freely to enforce some desired property on the wavelets 1j,m. 
For example, in case we want the wavelet to have vanishing moments, the condition that the integral of 
a wavelet multiplied with a certain polynomial Piis zero can now be written as PP o 0 =gj,m,lh<j+1,l,Pii-sj,k,mh<j,k,Pi. 
i lk For a .xed jand m, this is a linear equation in the unknowns fsj,k,mjkg. If we choose the number 
of unknown coef.cients sj,k,mequal to the number of equations N, we need to solve a linear system for 
each jand mof size NxN. A priori we do not know if this linear system can always be solved. We will come 
back to this later. The fast wavelet transform after lifting can be written as P o lIj,m =g j,m,lAj+1,l 
PP h o Aj,k = j,k,lAj+1,l+sj,k,mIj,m, lm i.e., as a sequence of two steps. First the old dual high and 
low pass .lters. Next the update of the old scaling function coef.cients with the wavelet coef.cients 
using the fsj,k,mjkg. The inverse transform becomes P(P)P hoo Aj+1,l = j,k,lAj,k-sj,k,mIj,m+gj,m,lIj,m. 
kmmInstead of writing everything as a sequence of two steps involving fsj,k,mjkgwe could have formed 
the new .lters hand g.rst and then applied those in a single step. Structuring the new .lters as two 
stages, however, simpli.es the implementation considerably and is also more ef.cient. Remarks: 1. The 
multiple index notation might look confusing at .rst sight, but its power lies in the fact that it immediately 
corresponds to the data structure of the implementation. The whole transform can also be written as one 
giant sparse matrix multiplication, but this would obscure the implementation ease of the lifting scheme. 
 2. Note how the inverse transform has a simple structure directly related to the forward transform. 
Essentially the inverse trans­form subtracts exactly the same linear combination of wavelet coef.cients 
from Aj,kas was added in the forward transform. 3. It is also possible to keep the dual scaling function 
.xed and put the conditions on the dual wavelet. The machinery is exactly the same provided one switches 
primals and duals and thus toggles the tildes in the equations. We refer to this as the dual lifting 
scheme, which employs coef.cients sj,k,m. It allows us to improve the performance of the dual wavelet. 
Typically, the number of vanishing moments of the dual wavelet is important to achieve compression. Also, 
the lifting scheme and the dual lifting scheme can be alternated to bootstrap one s way up to a desired 
multiresolution analysis (cakewalk construction). 4. The construction in LDW can be seen as a special 
case of the lifting scheme. They use the degrees of freedom to achieve pseudo-orthogonality (i.e., orthogonality 
between scaling func­tion and wavelets of one level within a small neighborhood) starting from an interpolating 
wavelet. The lifting scheme is more general in the sense that it uses a fully biorthogonal setting and 
that it can start from any multiresolution analysis with .nite .lters. The pseudo-orthogonalization requires 
the solution of linear systems which are of the size of the neighborhood (typi­cally 24 by 24). Since 
many wavelets may in fact be the same caching of matrix computations is possible. 5. After .nishing 
this work, the authors learned that a similar con­struction was obtained independently by Dahmen and 
collabo­rators. We refer to the original papers [2, 8] for details.  Lazy wavelet on the real line Interpolating 
wavelet on the real line Primals Duals Primals Duals j j scaling functions scaling functions scaling 
functions scaling functions  j j wavelets wavelets wavelets  wavelets j+1 j+1 scaling functions 
scaling functions scaling functions scaling functions  Figure 4: For the Lazy wavelet all primals are 
Kronecker functions (1 at the origin, 0 otherwise), while all duals are unit pulses (Dirac distributions). 
Going from a .ner to a coarser scale is achieved by subsampling with the missing samples giving the wavelet 
spaces r jrj (K(j)=2Z, M(j)=2(Z +1/2), and xj,k =k). The well known linear B-splines as primal scaling 
and wavelet functions with Diracs as duals can be reached with dual lifting ( s j,k,m = 1/2 tkr2-j-1,m+1/2 
tk+2-j-1,m), resulting in < j,k =t(-k) and 1 j,m =-1/2 t(-m-2rjr1)+t(-m)-1/2 t(-m+ 2rjr1). 6. Evidently, 
the lifting scheme is only useful in case one has an initial set of biorthogonal .lters. In the following 
sections we will discuss two such sets.  3.2 Fast Lifted Wavelet Transform Before describing the particulars 
of our bases we give the gen­eral structure of all transforms. Forward (analysis) and inverse (synthesis) 
transforms are always performed level wise. The for­mer begins at the .nest level and goes to the root 
while the latter starts at the root and descends to the leaf level. AnalysisIcom­putes the unlifted wavelet 
coef.cients at the parent level while AnalysisIIperforms the lifting if the basis is lifted otherwise 
it is empty. Similarly, SynthesisIperforms the inverse lifting, if any, while SynthesisIIcomputes the 
scaling function coef.cients at the child level. Analysis Forlevel=leafleveltorootlevel AnalysisI(level) 
AnalysisII(level) Synthesis Forlevel=rootleveltoleaflevel SynthesisI(level) SynthesisII(level) The transforms 
come in two major groups: (A) Lifted from the Lazy wavelet: this involves interpolating scaling functions 
and a vertex based transform; (B) Lifted from the Haar wavelet: this involves a face based transform. 
We next discuss these in detail.  3.3 Interpolating Scaling Functions We .rst give a trivial example 
of a wavelet transform: the Lazy wavelet [27, 28]. The Lazy wavelet transform is an orthogonal transform 
that essentially does not compute anything. However, it is fundamental as it is connected with interpolating 
scaling functions. The .lters of the Lazy fast wavelet transform are given as oo oo hj,k,l =hj,k,l =tk,land 
gj,m,l =g j,m,l =tm,l. Consequently, the transform does not compute anything, it only subsamples the 
coef.cients. Figure 4 (left) illustrates this idea for the case of the real line. Scaling functions f<j,kjj0,k2K(j)gare 
called in­terpolating if a set of points fxj,kjj0,k2K(j)gwith e 3 Figure 5: Neighbors used in our bases. 
Members of the index sets used in the transforms are shown m f in the diagram (m2M(j),2 fv1,v2,f1,f2,e1,e2,e3,e4g=Km). 
e 1 2 xj,k =xj+1,kexists, so that 8k,k0 2K(j): <j,k(xj,k0)=tk,k0. An example for such functions on 
the real line is shown on the right side of Figure 4. In case of interpolating scaling functions, we 
can always take the dual scaling functions to be Dirac distributions, < j,k(x)=t(x-xj,k), which are immediately 
biorthogonal (see the dual scaling functions on the right of Figure 4). This leads to trivial inner products 
with the duals, namely evaluation of the function at the points xj,k. The set of .lters resulting from 
interpolating scaling functions and Diracs as their formal dual, can be seen as a dual lifting of the 
Lazy wavelet. This implies that hj,k,k0 =tk,k0, hj,k,m =s j,k,m, g j,m,k =-s j,k,m, gj,m,m0 =tm,m0. The 
wavelets are given by 1j,m =<j+1,mand the dual wavelets by P 1j,m =t(-xj+1,m)-s j,k,mt(-xj,k). k The 
linear B-spline (right side of Figure 4) can be seen to be the dual lifting of the Lazy wavelet. Since 
we applied dual lifting the primal wavelet does not yet have a vanishing moment. Below we present other 
choices for the .lter coef.cients hj,k,m. Typically one can choose the sj,k,mto insure that 1 j,mhas 
vanish­ing moments (this will lead to the Quadratic scheme), or that <j,k is smooth (this will lead to 
the Butter.y scheme). At this point we have an interpolating multiresolution analysis, which was dually 
lifted from the Lazy wavelet. A disadvantage of this multiresolution analysis is that the functions cannot 
provide Riesz bases for L2. The dual functions do not even belong to L2. This is related to the fact 
that the wavelet does not have a vanishing integral since it coincides with a scaling function. Consequently, 
unconditional convergence of the expansion (3) is not guaranteed. One can now apply the primal lifting 
scheme to try to overcome this drawback by ensuring that the primal wavelet has at least 1 vanishing 
moment. Note that this is only a necessary and not a suf.cient condition. This yields P hj,k,l =tk,l+ 
sj,k,mg j,m,l m P gj,m,l =tm,l-sj,k,mhj,k,l. k The resulting wavelet can be written as P 1j,m =<j+1,m 
-k sj,k,m<j,k.(5) In the situation in Figure 4 setting sj,k,m =1/4 tm,k+2-j-1 + 1/4 tm,kr2-j-1 results 
in 1j,mhaving a vanishing integral. This choice leads us to the well known (2,2)biorthogonal wavelet 
of [4]. 3.4 Vertex Bases Up to this point we have treated all index sets involved in the various .lters 
as abstract sets. We now make these index sets more concrete. In order to facilitate our description 
we consider all index sets as de.ned locally around a given site xj+1,m. A diagram is given in Figure 
5. The index of a given site is denoted m2M(j) and all the neighboring vertices (xj,kwith k2K(j)) needed 
in the transform have indices v, f,and erespectively. To give some more intuition to these index sets 
recall wavelets on the real line as in Figure 4. In that case the set K(0)3lwould consist of all integers, 
while M(-1)3mwould contain the odd and K(-1)3kthe even integers. For vertex based schemes we may think 
of the sites m2M(j)as always living on the midpoint of some parent edge (these being the odd indices), 
while the endpoints of a given edge form the even indices (k2K(j)), and their union l2K(j)M(j)=K(j+1)gives 
the set of all indices. For each mthe .lters only range over some small neighborhood. We will refer to 
the elements in these neighborhoods by a local naming scheme (see Figure 5), k2KmCK(j). For example, 
the site m lies in between the elements of Km =fv1,v2g. For all vertex bases the unlifted scaling coef.cients 
are simply subsampled during analysis and upsampled during synthesis, while the wavelet coef.cients involve 
some computation. AnalysisI(j): 8k2K(j): Aj,k:=Aj+1,k P 8m2M(j): Ij,m:=Aj+1,m -s j,k,mAj,k k2K m SynthesisII(j): 
8k2K(j): Aj+1,k:=Aj,k P 8m2M(j): Aj+1,m:=Ij,m+ s j,k,mAj,k k2K m We now give the details of the wavelet 
coef.cient computations. Lazy: As mentioned above the Lazy wavelet does nothing but subsampling. The 
resulting analysis and synthesis steps then be­come Ij,m:=Aj+1,mand Aj+1,m:=Ij,m. respectively. The corresponding 
stencil encompasses no neighbors, i.e., the sums over sj,k,mare empty. Linear: This basic interpolatory 
form uses the stencil k2K= fv1,v2g(see Figure 5) for analysis and synthesis Ij,m:=Aj+1,m -1/2(Aj+1,v1 
+Aj+1,v2 ) Aj+1,m:=Ij,m+1/2(Aj,v1 +Aj,v2 ), respectively. Note that this stencil does properly account 
for the geometry provided that the msites at level j+1 have equal geodetic distance from the fv1,v2gsites 
on their parent edge. Here sj,v1,m = s j,v2,m =1/2. Quadratic: The stencil for this basis is given by 
Km = fv1,v2,f1,f2g(see Figure 5) and exploits the degrees of freedom implied to kill the functions x 
2, y 2,and z 2 (and by implication the constant function [1]). Using the coordinates of the neighbors 
of the involved sites a small linear system results 0 10101 1111 1 sj,v1,m B 2222 CBCB 2 C xxxx x Bj,v1 
j,v2 j,f1 j,f2 CBs j,v2,m C=Bj+1,m C (2222 A( A(2 A yyyy y j,v1 j,v2 j,f1 j,f2 sj,f1,m j+1,m 2222 2 
zzzz z sj,f2,m j,v1 j,v2 j,f1 j,f2 j+1,m 222 Since x+y+z=1 this system is singular (but solvable) and 
the answer is chosen so as to minimize the l2 norm of the resulting .lter coef.cients. Note that this 
is an instance of dual lifting with effective .lters sj,k,m =hj,k,m =-g j,m,k. Butter.y: This is the 
only basis which uses other than immediate neighbors (all the sites Kmdenoted in Figure 5). Here sv1 
=s v2 = 1/2, sf1 =s f2 =1/8,and se1 =s e2 =s e3 =s e4 =-1/16. It is inspired by a subdivision scheme 
of Dyn et al. [11] for the construction of smooth surfaces.  3.5 Lifting Vertex Bases All of the above 
bases, Lazy, Linear, Quadratic, and Butter.y can be lifted. In this section we use lifting to assure 
that the wavelet has at least one vanishing moment. It does not improve the ability of the dual wavelet 
to annihilate more functions. Consequently the ability of the bases to compress is not increased, but 
smaller error results when using them for compression (see the example in Section 3.8 and the results 
in Section 5). We propose wavelets of the form 1j,m =<j+1,m -sj,v1,m<j,v1 -sj,v2,m<j,v2 .(6) In words, 
we de.ne the wavelet at the midpoint of an edge as a linear combination of the scaling function at the 
midpoint (j+1,m)and two scaling functions on the coarser level at the two endpoints of the parent edge 
(j,v1,2). The weights sj,k,mare chosen so that the resulting wavelet has a vanishing integral R sj,k,m 
=Ij+1,m/2 Ij,kwith Ij,k = S2 <j,kd. During analysis lifting is a second phase (at each level j)after 
the Ij,mcomputation, while during synthesis it is a .rst step followed bytheregularsynthesisstep(Linear, 
Quadratic,orButter.yasgiven above). The simplicity of the expressions demonstrates the power of the lifting 
scheme. Any of the previous vertex basis wavelets can be lifted with the same expression. The integrals 
Ij,kcan be approximated on the .nest level and then recursively computed on the coarser levels (using 
the re.nement relations). AnalysisII(j): Aj,v1 +=sj,v1,mIj,m 8m2M(j): Aj,v2 +=sj,v2,mIj,m SynthesisI(j): 
 Aj,v1 -=sj,v1,mIj,m 8m2M(j): Aj,v2 -=sj,v2,mIj,m For the interpolating case in the previous section, 
the scaling function coef.cients at each level are simply samples of the function to be expanded (inner 
products with the <n,k). In the lifted case the coef.cients are de.ned as the inner product of the function 
to be expanded with the (new) dual scaling function. This dual scaling Figure 6: Images of the graphs 
of all vertex based wavelets. On the left is the scaling function (or unlifted wavelet) while the right 
shows the lifted wavelet with 1 vanishing moment. From top to bottom: Linear, Quadratic, and Butter.y. 
Positive values are mapped to a linear red scale while negative values are shown in blue. The gray area 
shows the support. Figure 7: Example Haar scaling functions on a triangular subdivi­sion. On the left 
are primal functions each of height 1. On the right are the biorthogonal duals each of height a(Ti)r1. 
Disjoint bases have inner product of 0 while overlapping (coincident supports) lead to an inner product 
of 1. (For the sphere all triangles are spherical triangles.) children Bio-Haar 1 Bio-Haar 2 Bio-Haar 
3 Figure 8: The Bio-Haar wavelets. Note that the heights of the functions are not drawn to scale. function 
is only de.ned as the limit function of a non-stationary subdivision scheme. The inner products at the 
.nest level therefore need to be approximated with a quadrature formula, i.e., a linear combination of 
function samples. In our implementation we use a simple one point quadrature formula at the .nest level. 
Figure 6 shows images of the graphs of all the vertex based 1 functions for the interpolating and lifted 
case.  3.6 The Generalized Haar Wavelets and Face Bases Consider spherical triangles resulting from 
a geodesic sphere con­struction Tj,kCS2 with k2K(j)(note that the face based K(j) are not identical to 
the vertex based K(j)de.ned earlier). They satisfy the following properties: S 1. S2 = Tj,kand this union 
is disjoint, i.e., the Tj,k k2K(j) provide a simple cover of S2 for every j, 2. for every jand k, Tj,kcan 
be written as the union of 4 child triangles Tj+1,l. Let a(Tj,k)be the spherical area of a triangle and 
de.ne the scaling functions and dual scaling functions as <j,k =XTand < j,k =a(Tj,k)r1XT. j,k j,k Here 
XTis the function whose value is 1 for x2Tand 0 otherwise. The fact that the scaling function and dual 
scaling function are biorthogonal follows immediately from their disjoint support (see Figure 7). De.ne 
the VjCL2 as Vj =clos spanf<j,kjk2K(j)g. The spaces Vjthen generate a multiresolution analysis of L2(S2). 
Now .x a triangle Tj,*. For the construction of the general­ized Haar wavelets, we only need to consider 
the set of children Tj+1,l=0,1,2,3 of Tj,*. We call these bases the Bio-Haar functions (see Figure 8). 
The wavelets (m=1,2,3) are chosen as 1j,m =2(<j+1,m -Ij+1,m/Ij+1,0 <j+1,0), so that their integral vanishes. 
A set of semi-orthogonal dual wavelets is then given by 1j,m =1/2(< j+1,m -< j,*). These bases are inspired 
by the construction of orthogonal Haar wavelets for general measures, see [14, 21] where it is shown 
that the Haar wavelets form an unconditional basis. The Bio-Haar wavelets have only 1 vanishing moment, 
but us­ing the dual lifting scheme, we can build a new multiresolution Aunts and parent Bio-Haar 1 
Bio-Haar 2  Bio-Haar 3  Figure 9: Illustration of the dual lifting of the dual Bio-Haar wavelets. 
New dual wavelets can be constructed by taking linear combinations of the original dual Bio-Haar wavelets 
and parent level dual scaling functions. Each such linear combination is signi­.ed by a row. Solving 
for the necessary weights s j,k,mrequires the solution to a small matrix problem whose right hand side 
encodes the desired constraints. analysis, in which the dual wavelet has more vanishing moments. Let 
Tj,k=4,5,6 be the neighboring triangles of Tj,*(at level j), and Km =f*,4,5,6g. The new dual wavelets 
are P 1 j,m =1/2(< j+1,m -< j,*)-k2K s j,k,m< j,k. m Note that this is a special case of Equation (4). 
The coef.cients s j,k,mcan now be chosen so that 1j,mhas vanishing moments. Figure 9 illustrates this 
idea. In the left column are the three dual Bio-Haar wavelets created before. The following four columns 
show the dual scaling functions over the parent and aunt triangles Tj,k=*,4,5,6. Each row signi.es one 
of the linear combinations. Similarly to the Quadratic vertex basis we construct dually lifted Bio-Haar 
wavelets which kill the functions x 2, y 2, z 2, and thus 1. This leads to the equations P k2K s j,k,mh< 
j,k,Pi=1/2h< j+1,m -< j,*,Pi m 222 with P=x,y,z,1. The result is a 4 x4 singular (but solvable) matrix 
problem for each m=1,2,3. The unknowns are the sj,k,m with k=*,4,5,6 and the entries of the linear system 
are moments of dual scaling functions. These can be computed recursively from the leaf level during analysis. 
The Bio-Haar and lifted Bio-Haar transforms compute the scal­ing function coef.cient during analysis 
at the parent triangle as a function of the scaling function coef.cients at the children and possibly 
the scaling function coef.cients at the neighbors of the parent triangle (in the lifted case). The three 
wavelet coef.cients of the parent level are stored with the children T1, T2,and T3 for convenience in 
the implementation. During synthesis the scaling function coef.cient at the parent and the wavelet coef.cients 
stored at children T1, T2,and T3 are used to compute scaling function coef.cients at the 4 children. 
As before, lifting is a second step during analysis and modi.es the wavelet coef.cients. During synthesis 
lifting is a .rst step before the inverse Bio-Haar transform is calculated. AnalysisII(j): P 8m2M(j): 
Ij,m-= s j,k,mAj,k k2K m SynthesisI(j): P 8m2M(j): Ij,m+= s j,k,mAj,k k2K m  3.7 Basis Properties The 
lifting scheme provides us with the .lter coef.cients needed in the implementation of the fast wavelet 
transform. To .nd the basis functions and dual basis functions that are associated with them, we use 
the cascade algorithm. To synthesize a scaling function <j0 ,k0 one simply initializes the coef.cient 
Aj0,k =tk,k0. The inverse wavelet transform starting from level j0 with all wavelet coef.cients Ij,mwith 
jj0 set to zero then results in Aj,kcoef.cients which converge to function values of <j0,k0 as j-1. In 
case the cascade algorithm converges in L2 for both primal and dual scaling functions, biorthogonal .lters 
(as given by the lifting scheme) imply biorthogonal basis functions. One of the fundamental questions 
is how properties, such as convergence of the cascade algorithm, Riesz bounds, and smooth­ness, can be 
related back to properties of the .lter sequences. This is a very hard question and at this moment no 
general answer is available to our knowledge. We thus have no mathematical proof that the wavelets constructed 
form an unconditional basis except in the case of the Haar wavelets. A recent result addressing these 
questions was obtained by Dahmen [7]. In particular, it is shown there which properties in addition to 
biorthogonality are needed to assure stable bases. Whether this result can be applied to the bases constructed 
here needs to be studied in the future. Regarding smoothness, we have some partial results. It is easy 
to see that the Haar wavelets are not continuous and that the Linear wavelets are. The original Butter.y 
subdivision scheme is guaran­teed to yield a C1 limit function provided the connectivity of the vertices 
is at least 4. The modi.ed Butter.y scheme that we use on the sphere, will also give C1 limit functions, 
provided a locally smooth (C1) map from the spherical triangulation to a planar tri­angulation exists. 
Unfortunately, the geodesic subdivision we use here does not have this property. However, the resulting 
functions appear visually smooth (see Figure 6). We are currently working on new spherical triangulations 
which have the property that the Butter.y scheme yields a globally C1 function. In principle, one can 
choose either the tetrahedron, octahedron, or icosahedron to start the geodesic sphere construction. 
Each of them has a particular number of triangles on each level, and therefore one of them might be more 
suited for a particular application or platform. The octahedron is the best choice in case of functions 
de.ned on the hemisphere (cfr. BRDF). The icosahedron will lead to the least area imbalance of triangles 
on each level and thus to (visually) smoother basis functions.  3.8 An Example We argued at the beginning 
of this section that a given wavelet basis can be made more performant by lifting. In the section on 
interpolating bases we pointed out that a wavelet basis with Diracs for duals and a primal wavelet, which 
does not have 1 vanishing moment, unconditional convergence of the resulting series expan­sions cannot 
be insured anymore. We now give an example on the sphere which illustrates the numerical consequences 
of lifting. p Consider the function f(s)=jsxjfor s=(sx,sy,sz)2S2. This function is everywhere smooth 
except on the great circle sx = 0, where its derivative has a discontinuity. Since it is largely smooth 
but for a singularity at 0, it is ideally suited to exhibit problems in bases whose primal wavelet does 
not have a vanishing moment. Figure 10 shows the relative l1 error as a function of the number of coef.cients 
used in the synthesis stage. In order to satisfy the same error threshold the lifted basis requires only 
approximately 1/3the number of coef.cients compared to the unlifted basis. relative l1 error 1E-01 1E-02 
1E-03 1E-04  Figure 10: Relative l1 error as a function of the number of pcoef.cients for the example 
function f(s)=jsxjand (lifted) Linear wavelets. With the same number of coef.cients the error is smaller 
by a factor of 3 or conversely a given error can be achieved with about 1/3 the number of coef.cients 
if the lifted basis is used.  4 Implementation We have implemented all the described bases in an interactive 
appli­cation running on an SGI Irix workstation. The basic data structure is a forest of triangle quadtrees 
[10]. The root level starts with 4 (tetrahedron), 8 (octahedron), or 20 (icosahedron) spherical trian­gles. 
These are recursively subdivided into 4 child triangles each. Naming edges after their opposite vertex, 
and children after the ver­tex they retain (the central child becomes T0) leads to a consistent naming 
scheme throughout the entire hierarchy. Neighbor .nding is a simple O(1)(expected cost) function using 
bit operations on edge and triangle names to guide pointer traversal [10]. A vertex is allocated once 
and any level which contains it carries pointers to it. Each vertex carries a single Aand Islot for vertex 
bases, while face bases carry a single Aand Islot per spherical triangle. Our actual implementation carries 
other data such as surface normals and colors used for display, function values for error computations, 
and copies of all Iand Avalues to facilitate experimentation. These are not necessary in a production 
system however. Using a recursive data structure is more memory intensive (due to the pointer overhead) 
than a .at, array based representation of all coef.cients as was used by LDW. However, using a recursive 
data structure enables the use of adaptive subdivision and results in simple recursive procedures for 
analysis and synthesis and a subdivision oracle. For interactive applications it is straightforward to 
select a level for display appropriate to the available graphics performance (polygons per second). In 
the following subsections we address particular issues in the implementation. 4.1 Restricted Quadtrees 
In order to support lifted bases and those which require stencils that encompass some neighborhood the 
quadtrees produced need to satisfy a restriction criterion. For the Linear vertex bases (lifted and unlifted) 
and the Bio-Haar basis no restriction is required. For Quadratic and lifted Bio-Haar bases no neighbor 
of a given face may be off by more than 1 subdivision level (every child needs a proper set of aunts 
). For the Butter.y basis a two-neighborhood must not be off by more than 1 subdivision level. These 
requirements are easily enforced during the recursive subdivision. The fact that we only need aunts (as 
opposed to sisters ) for the lifting scheme allows us to have wavelets on adaptively subdivided hierarchies. 
This is a crucial departure from previous constructions, e.g., tree Basis Analysis Synthesis Lifted Basis 
Analysis Synthesis Linear Quadratic Butter.y Bio-Haar 3.59 21.79 8.43 4.31 3.55 21.00 8.42 6.09 Linear 
Quadratic Butter.y Bio-Haar 5.85 24.62 10.64 42.43 5.83 24.68 10.62 36.08 Table 2: Representative timings 
for wavelet transforms beginning with 4 spherical triangles and expanding to level 9 (220 faces and 219 
+2 vertices). All timings are given in seconds and measured on an SGI R4400 running at 150MHz. The initial 
setup time (allocating and initializing all data structures) took 100 seconds. wavelets employed by Gortler 
et al.[16] who also needed to support adaptive subdivision.  4.2 Boundaries In the case of a hemisphere 
(top 4 spherical triangles of an octahedral subdivision), which is important for BRDF functions, the 
issues associated with the boundary need to be addressed. Lifting of vertex bases is unchanged, but the 
Quadratic and Butter.y schemes (as well as the lifted Bio-Haar bases) need neighbors, which may not exist 
at the boundary. This can be addressed by simply using another, further neighbor instead of the missing 
neighbor (across the boundary edge) to solve the associated matrix problem. It implicitly corresponds 
to adapting .lter coef.cients close to the boundary as done in interval constructions, see e.g. [5]. 
This construction automatically preserves the vanishing moment property even at the boundary. In the 
implementation of the Butter.y basis, we took a different approach and chose in our implementation to 
simply re.ect any missing faces along the boundary. 4.3 Oracle One of the main components in any wavelet 
based approximation is the oracle. The function of the oracle is to determine which co­ef.cients are 
important and need to be retained for a reconstruction which is to meet some error criterion. Our system 
can be driven in two modes. The .rst selects a deepest level to which to expand all quadtrees. The storage 
requirements for this approach grow expo­nentially in the depth of the tree. For example our implementation 
cannot go deeper than 7 levels (starting from the tetrahedron) on a 32MB Indy class machine without paging. 
Creating full trees, however, allows for the examination of all coef.cients throughout the hierarchies 
to in effect implement a perfect oracle. The second mode builds sparse trees based on a deep re.nement 
oracle. In this oracle quadtrees are built depth .rst exploring the expansion to some (possibly very 
deep) .nest level. On the way out of the recursion a local AnalysisIis performed and any subtrees whose 
wavelet coef.cients are all below a user supplied threshold are deallocated. Once the sparse tree is 
built the restriction criterion is enforced and the (possibly lifted) analysis is run level wise. The 
time complexity of this oracle is still exponential in the depth of the tree, but the storage requirements 
are proportional to the output size. With extra knowledge about the underlying function more powerful 
oracles can be built whose time complexity is proportional to the output size as well.  4.4 Transform 
Cost The cost of a wavelet transform is proportional to the total number of coef.cients, which grows 
by a factor of 4 for every level. For example, 9 levels of subdivision starting from 4 spherical triangles 
result in 220 coef.cients (each of Aand I) for face bases and 219 + 2 (each of Aand I) for vertex bases. 
The cost of analysis and synthesis is proportional to the number of basis functions, while the constant 
of proportionality is a function of the stencil size. Table 2 summarizes timings of wavelet transforms 
for all the new bases. The initial setup took 100 seconds and includes allocation and initialization 
of all data structures and evaluation of the A9,k. Since the latter is highly dependent on the evaluation 
cost of the function to be expanded we used the constant function 1 for these timings. None of the matrices 
which arise in the Quadratic, and Bio-Haar bases (lifted and unlifted) was cached, thus the cost of solving 
the associated 4x4 matrices with a column pivoted QR (for Quadratic and lifted Bio-Haar) was incurred 
both during analysis and synthesis. If one is willing to cache the results of the matrix solutions this 
cost could be amortized over multiple transforms. We make three main observations about the timings: 
(A) Lifting of vertex bases adds only a small extra cost, which is almost entirely due to the extra recursions; 
(B) the cost of the Butter.y basis is only approximately twice the cost of the Linear basis even though 
the stencil is much larger; (C) solving the 4 x4 systems implied by Quadratic and lifted Bio-Haar bases 
increases the cost by a factor of approximately 5 over the linear case (note that there are twice as 
many coef.cients for face bases as for vertex bases). While the total cost of an entire transform is 
proportional to the number of basis functions, evaluating the resulting expansion at a point is proportional 
to the depth (log of the number of basis functions) of the tree times a constant dependent on the stencil 
size. The latter provides a great advantage over such bases as spherical harmonics whose evaluation cost 
at a single point is proportional to the total number of bases used.  5 Results In this section we report 
on experiments with the compression of a planetary topographic data set, a BRDF function, and illumination 
of an anisotropic glossy sphere. Most of these experiments involved some form of coef.cient thresholding 
(in the oracle). In all cases this was performed as follows. Since all our bases are normalized with 
respect to the L1norm, L2 thresholding against some user supplied threshold . becomes p if jIj,mjsupp(1j,m).,Ij,m:=0. 
 Furthermore .is scaled by (max(f)-min(f))for the given function fto make thresholding independent of 
the scale of f. 5.1 Compression of Topographic Data In this series of experiments we computed wavelet 
expansions of topographic data over the entire earth. This function can be thought of as both a surface, 
and as a scalar valued function giving height (depth) for each point on a sphere. The original data, 
ETOPO5 from the National Oceanographic and Atmospheric Administration gives the elevation (depth) of 
the earth from sea level in meters at a resolution of 5 arc minutes at the equator. Due to the large 
size of this data set we .rst resampled it to 10 arc minutes resolution. All expansions were performed 
starting from the tetrahedron followed by subdivision to level 9. Figure 11 shows the results of these 
experiments (left and mid­dle). After computing the coef.cients of the respective expansions at the .nest 
level of the subdivision an analysis was performed. After this step all wavelet coef.cients below a given 
threshold were zeroed and the function was reconstructed. The thresholds were successively set to 2rifor 
i=0,...,17 resulting in the number of coef.cients and relative l1 error plotted (left graph). The error 
was computed with a numerical quadrature one level below the .nest subdivision to insure an accurate 
error estimation. The results are plotted for all vertex and face bases (Linear, Quadratic, Butter.y, 
Bio-Haar, lifted and unlifted). We also computed l2 and l1error norms and the resulting graphs (not shown) 
are essentially identical 1E+00 1E+00 1E+00 1E-01   relative l1 error 1E-01 1E-01 1E-02 1E-02 1E-03 
1E-02 1E+02 1E+03 1E+04 1E+05 1E+01 1E+02 1E+03 1E+04 1E+05 number of coefficients number of coefficients 
(although the l1error stays initially high before falling off due to deep canyon features). The plot 
reaches to about one quarter of all coef.cients. The observed regime is linear as one would expect from 
the bases used. The most striking observation about these error graphs is the fact that all bases perform 
similar. This is due to the fact that the un­derlying function is non-smooth. Consequently smoother bases 
do not perform any better than less performant ones. However, when drawing pictures of highly compressed 
versions of the data set the smoother bases produce visually better pictures (see Figure 12). Depending 
on the allowed error the compression can be quite dra­matic. For example, 7 200 coef.cients are suf.cient 
to reach 7% error, while 119 000 are required to reach 2% error. In a second set of experiments we used 
the deep re.nement oracle (see Section 4.3) to explore the wavelet expansion to 10 levels (potentially 
quadrupling the number of coef.cients) with successively smaller thresholds, once again plotting the 
resulting error in the middle graph of Figure 11. The error as a function of coef.cients used is the 
same as the relationship found by the perfect oracle. This validates our deep re.nement oracle strategy. 
Memory requirements of this approach are drastically reduced. For example, using a threshold of 2r9 during 
oracle driven re.nement to level 10 resulted in 4 616 coef.cients and consumed a total of 27MB (including 
10MB for the original data set). Lowering the thresholdto2r10 yielded 10 287 coef.cients and required 
43MB (using the lifted Butter.y basis in both cases). Finally Figure 12 shows some of the resulting adaptive 
data sets rendered with RenderMan using the Butter.y basis and a pseudo coloring, which maps elevation 
onto a piecewise linear color scale. Total runtime for oracle driven analysis and synthesis was 10 min­utes 
on an SGI R4400 at 150MHz. 5.1.1 Comparison with LDW The earth data set allows for a limited comparison 
of our re­sults with those of LDW. They also compressed the ETOPO5 data set using pseudo orthogonalized 
(over a 2 neighborhood) Linear wavelets de.ned over the octahedron. They subdivide to 9 levels (on a 
128MB machine) which corresponds to twice as many coef­.cients as we used (on a 180MB machine), suggesting 
a storage overhead of about 3 in our implementation. It is hard to compare the quality of the bases without 
knowing the exact basis used or the errors in the compressed reconstruction. However, LDW report the 
number of coef.cients selected for a given threshold (741 for  0.05 and 0.01 respectively. In the image 
on the left coastal re­gions are rather smoothed since they contain little height variation (England 
and the Netherlands are merged and the Baltic sea is deserti.cated). However, such spiky features as 
the Cape Verde Islands off the coast of Africa are clearly preserved. 0.02, 15 101 for 0.002, and 138 
321 for 0.0005). Depending on the basis used we generally select fewer coef.cients (6 000-15 000 for 
0.002 and 28 000 -65000 for 0.0005). As timings they give 588 seconds (on a 100 MHz R4000) for analysis 
which is signi.cantly longer than our smoothest basis (lifted Butter.y). Their recon­struction time ranges 
from 75 (741 coef.cients) to 1 230 (138 058 coef.cients) seconds which is also signi.cantly longer than 
our times (see Table 2). We hypothesize that the timing and storage differences are largely due to their 
use of .at array based data struc­tures. These do not require as much memory, but they are more compute 
intensive in the sparse polygonal reconstruction phase.  5.2 BRDF Compression In this series of experiments 
we explore the potential for ef.ciently representing BRDF functions with spherical wavelets. BRDF func­tions 
can arise from measurements, simulation, or theoretical mod­els. Depending on the intended application 
different models may be preferable. Expanding BRDF functions in terms of locally sup­ported hierarchical 
functions is particularly useful for wavelet based .nite element illumination algorithms. It also has 
obvious appli­cations for simulation derived BRDF functions such as those of Westin et al. [30] and Gondek 
et al. [15]. The domain of a complete BRDF is a hemisphere times a hemi­sphere. In our experiments we 
consider only a .xed incoming direction and expand the resulting function over all outgoing direc­tions 
(single hemisphere). To facilitate the computation of errors we used the BRDF model proposed by Schlick 
[23]. It is a simple Pad´ e approximant to a micro facet model with geometric shad­owing, a microfacet 
distribution function, but no Fresnel term. It has roughness (r2.0,1., where 0 is Dirac mirror re.ection 
and 1 perfectly diffuse) and anisotropy (p2.0,1.,where 0 is Dirac style anisotropy, and 1 perfect isotropy) 
parameters. To improve the numerical properties of the BRDF we followed the suggestion of Westin et al. 
[30] and expanded cos 0ofr(wi,0,). In the experiments we used all 8 bases but specialized to the hemisphere. 
The parameters were 0i =./3, r=0.05, and p=1. The results are summarized in Figure 11 (rightmost graph). 
It shows the relative l1 error as a function of the number of coef.cients used. This time we can clearly 
see how the various bases differentiate themselves in terms of their ability to represent the function 
within some error bound with a given budget of coef.cients. We make several observations -all lifted 
bases perform better than their unlifted versions, con.rming our assertion that lifted bases are more 
performant; -increasing smoothness in the bases (Butter.y) is more im­portant than increasing the number 
of vanishing moments (Quadratic); -dual lifting to increase dual vanishing moments increases com­ pression 
ability dramatically (Bio-Haar and lifted Bio-Haar); -overall the face based schemes do not perform as 
well as the vertex based schemes. Figure 13 shows images of the graphs of some of the expansions. These 
used the lifted Butter.y basis with an adaptive re.nement oracle which explored the expansion to level 
9 (i.e., it examined 219 coef.cients). The .nal number of coef.cients and associated relative l1 errors 
were (left to right) 19 coef.cients (l1 =0.35), 73 coef.cients (l1 =0.065), and 203 coef.cients (l1 =0.015). 
Total runtime was 170 seconds on an SGI R4400 at 150MHz.  5.3 Illumination To explore the potential 
of these bases for global illumination algo­rithms we performed a simple simulation computing the radiance 
over a glossy, anisotropic sphere due to two area light sources. We emphasize that this is not a solution 
involving any multiple re.ec­tions, but it serves as a simple example to show the potential of these 
bases for hierarchical illumination algorithms. It also serves as an example of applying a .nite element 
approach to a curved object (sphere) without polygonalizing it. Figure 14 shows the results of this simulation. 
We used the lifted Butter.y basis and the BRDF model of Schlick with r=0.05, p=0.05, and an additive 
diffuse component of 0.005. Two area light sources illuminate the red sphere. Note the .ne detail in 
the pinched off region in the center of the hot spot and also at the north pole where all grooves converge. 
  6 Conclusions and Future Directions In this paper we have introduced two new families of wavelets 
on the sphere. One family is based on interpolating scaling functions and one on the generalized Haar 
wavelets. They employ a generalization of multiresolution analysis to arbitrary surfaces and can be derived 
in a straightforward manner from the trivial multiresolution analysis with the lifting scheme. The resulting 
algorithms are simple and ef.cient. We reported on the application of these bases to the compression 
of earth data sets, BRDF functions and illumination computations and showed their potential for these 
applications. We found that -for smooth functions the lifted bases perform signi.cantly better than the 
unlifted bases; -increasing the dual vanishing moments leads to better com­pression; -smoother bases, 
even with only one vanishing moment, tend to perform better for smooth functions; -our constructions 
allow non-equally subdivided triangulations of the sphere. We believe that many applications can bene.t 
from these wavelet bases. For example, using their localization properties a number of spherical image 
processing algorithms, such as local smoothing and enhancement, can be realized in a straightforward 
and ef.cient way [25]. While we limited our examination to the sphere, the construction presented here 
can be applied to other surfaces. In the case of the sphere enforcing vanishing polynomial moments was 
natural because of their connection with spherical harmonics. In the case of a general, potentially non-smooth 
(Lipschitz) surface, polynomial moments do not necessarily make much sense. Therefore, one might want 
to work with local maps from the surface to the tangent plane and enforce vanishing moment conditions 
in this plane. Future research includes -the generalization to arbitrary surfaces, -the incorporation 
of smoother (C2) subdivision schemes as recently introduced by Dyn et al. (personal communication, 1995), 
-the use of these bases in applications such as the solution of differential and integral equations on 
the sphere as needed in, e.g., illumination or climate modeling.  Acknowledgments The .rst author was 
supported by DEPSCoR Grant (DoD-ONR) N00014-94-1-1163. The second author was supported by NSF EPSCoR 
Grant EHR 9108772 and DARPA Grant AFOSR F49620­93-1-0083. He is also Senior Research Assistant of the 
National Fund of Scienti.c Research Belgium (NFWO). Other support came from Pixar Inc. We would also 
like to thank Princeton Univer­sity and the GMD, Germany, for generous access to computing resources. 
Help with geometric data structures was provided by David Dobkin. Finally, the comments of the referees 
were very helpful in revising the paper.  References [1] ALFELD,P., NEAMTU,M., AND SCHUMAKER,L.L. Bernstein-B´ 
ezier polynomials on circles, sphere, and sphere-like sur­faces. Preprint. [2] CARNICER,J. M., DAHMEN,W., 
AND PE Local NA,J.M. decompositions of re.nable spaces. Tech. rep., Insitut f¨ ur Geometrie und angewandete 
Mathematik, RWTH Aachen, 1994. [3] CHRISTENSEN,P.H., STOLLNITZ,E. J., SALESIN,D. H., AND DEROSE, T. D. 
Wavelet Radiance. In Proceedings of the 5th Eurographics Workshop on Rendering, 287 302, June 1994. [4] 
COHEN,A., DAUBECHIES,I., AND FEAUVEAU,J. Bi­orthogonal bases of compactly supported wavelets. Comm. Pure 
Appl. Math. 45 (1992), 485 560. [5] COHEN,A., DAUBECHIES,I., JAWERTH,B., AND VIAL,P. Multiresolution 
analysis, wavelets and fast algorithms on an interval. C. R. Acad. Sci. Paris S´er. I Math. I, 316 (1993), 
417 421. [6] DAHLKE,S., DAHMEN,W., SCHMITT,E., AND WEINREICH,I. Multiresolution analysis and wavelets 
on S2 and S3. Tech. Rep. 104, Institut f¨ ur Geometrie und angewandete Mathematik, RWTH Aachen, 1994. 
[7] DAHMEN, W. Stability of multiscale transformations. Tech. rep., Institut f¨ ur Geometrie und angewandete 
Mathematik, RWTH Aachen, 1994. [8] DAHMEN,W., PR¨ OSSDORF,S., AND SCHNEIDER, R. Multiscale methods for 
pseudo-differential equations on smooth mani­folds. In Conference on Wavelets: Theory, Algorithms, and 
Applications, C. K. C. et al., Ed. Academic Press, San Diego, CA, 1994, pp. 385 424. [9] DAUBECHIES,I. 
Ten Lectures on Wavelets. CBMS-NSF Re­gional Conf. Series in Appl. Math., Vol. 61. Society for In­dustrial 
and Applied Mathematics, Philadelphia, PA, 1992. [10] DUTTON, G. Locational Properties of Quaternary 
Triangular Meshes. In Proceedings of the Fourth International Sympo­sium on Spatial Data Handling, 901 
910, July 1990. [11] DYN,N., LEVIN,D., AND GREGORY, J. A Butter.y Subdivi­sion Scheme for Surface Interpolation 
with Tension Control. Transactions on Graphics 9, 2 (April 1990), 160 169. [12] FEKETE, G. Rendering 
and Managing Spherical Data with Sphere Quadtrees. In Proceedings of Visualization 90, 1990. [13] FREEDEN,W., 
AND WINDHEUSER, U. Spherical Wavelet Transform and its Discretization. Tech. Rep. 125, Univer­sit¨ at 
Kaiserslautern, Fachbereich Mathematik, 1994. [14] GIRARDI,M., AND SWELDENS,W. A new class of unbalanced 
Haar wavelets that form an uncondi­tional basis for Lpon general measure spaces. Tech. Rep. 1995:2, Industrial 
Mathematics Initiative, Depart­ment of Mathematics, University of South Carolina, 1995. (ftp://ftp.math.scarolina.edu/pub/imi95/imi952.ps). 
 [15] GONDEK,J.S., MEYER,G. W., AND NEWMAN, J. G. Wave­length Dependent Re.ectance Functions. In Computer 
Graph­ics Proceedings, Annual Conference Series, 213 220, 1994. [16] GORTLER,S., SCHR¨ODER,P., COHEN,M., 
AND HANRAHAN, P. Wavelet Radiosity. In Computer Graphics Proceedings, Annual Conference Series, 221 230, 
August 1993. [17] GORTLER,S. J., AND COHEN, M. F. Hierarchical and Vari­ational Geometric Modeling with 
Wavelets. In Proceedings Symposium on Interactive 3D Graphics, 35 42, April 1995. [18] LIU,Z., GORTLER,S. 
J., AND COHEN, M. F. Hierarchical Spacetime Control. Computer Graphics Proceedings, Annual Conference 
Series, 35 42, July 1994. [19] LOUNSBERY,M. Multiresolution Analysis for Surfaces of Arbi­trary Topological 
Type. PhD thesis, University of Washington, 1994. [20] LOUNSBERY,M., DEROSE,T.D., AND WARREN, J. Multires­olution 
Surfaces of Arbitrary Topological Type. Department of Computer Science and Engineering 93-10-05, University 
of Washington, October 1993. Updated version available as 93-10-05b, January, 1994. [21] MITREA,M. Singular 
integrals, Hardy spaces and Clifford wavelets. No. 1575 in Lecture Notes in Math. 1994. [22] NIELSON, 
G. M. Scattered Data Modeling. IEEE Computer Graphics and Applications 13, 1 (January 1993), 60 70. [23] 
SCHLICK, C. A customizable re.ectance model for everyday rendering. In Fourth Eurographics Workshop on 
Rendering, 73 83, June 1993. [24] SCHR¨Wavelet Methods for ODER,P., AND HANRAHAN, P. Radiance Computations. 
In Proceedings 5th Eurographics Workshop on Rendering, June 1994. [25] SCHR¨ODER,P., AND SWELDENS, W. 
Spherical wavelets: Tex­ture processing. Tech. Rep. 1995:4, Industrial Mathematics Initiative, Department 
of Mathematics, University of South Carolina, 1995. (ftp://ftp.math.scarolina.edu/pub/imi95/imi954.ps). 
[26] SILLION,F. X., ARVO,J. R., WESTIN,S. H., AND GREENBERG, D. P. A global illumination solution for 
general re.ectance distributions. Computer Graphics (SIGGRAPH 91 Proceed­ings), Vol. 25, No. 4, pp. 187 
196, July 1991. [27] SWELDENS, W. The lifting scheme: A construction of second generation wavelets. 
Department of Mathematics, University of South Carolina. [28] SWELDENS, W. The lifting scheme: A custom­design 
construction of biorthogonal wavelets. Tech. Rep. 1994:7, Industrial Mathematics Initiative, Depart­ment 
of Mathematics, University of South Carolina, 1994. (ftp://ftp.math.scarolina.edu/pub/imi94/imi947.ps). 
[29] WESTERMAN, R. A Multiresolution Framework for Volume Rendering. In Proceedings ACM Workshop on Volume 
Visu­alization, 51 58, October 1994. [30] WESTIN,S. H., ARVO,J. R., AND TORRANCE,K. E. Pre­dicting re.ectance 
functions from complex surfaces. Com­puter Graphics (SIGGRAPH 92 Proceedings), Vol. 26, No. 2, pp. 255 
264, July 1992. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218440</article_id>
		<sort_key>173</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[Multiresolution analysis of arbitrary meshes]]></title>
		<page_from>173</page_from>
		<page_to>182</page_to>
		<doi_number>10.1145/218380.218440</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218440</url>
		<keywords>
			<kw><![CDATA[geometric modeling]]></kw>
			<kw><![CDATA[subdivision surfaces]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Spline and piecewise polynomial approximation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Sampling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003736.10003737</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Functional analysis->Approximation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003720</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computations on polynomials</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31038755</person_id>
				<author_profile_id><![CDATA[81100357649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045514</person_id>
				<author_profile_id><![CDATA[81100493833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[DeRose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17009957</person_id>
				<author_profile_id><![CDATA[81100301736]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duchamp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P112851</person_id>
				<author_profile_id><![CDATA[81100397561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hugues]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoppe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P197887</person_id>
				<author_profile_id><![CDATA[81100464422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lounsbery]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Alias Research, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P298076</person_id>
				<author_profile_id><![CDATA[81100357122]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Werner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stuetzle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Washington, Seattle, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>577958</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Aho, J.E. Hopcroft, and J.D. Ullman. Data structures and algorithms. Addison-Wesley, Reading, Mass., 1983.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. Eells and L. Lemaire. Another report on harmonic maps. Bull. London Math. Soc., 20:385-524, 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Eells and J.H. Sampson. Harmonic mappings of Riemannian manifolds. Amer. J. Math., 86:109-160, 1964.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192223</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Adam Finkelstein and David Salesin. Multiresolution curves. Computer Graphics (SIGGRAPH '94 Proceedings), 28(3):261-268, July 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Forsey and R. Bartels. Hierarchical B-spline fitting. ACM Transactions on Graphics. To appear.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378512</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. Forsey and R. Bartels. Hierarchical B-spline refinement. Computer Graphics, 22(4):205-212, 1988.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[David Forsey and Lifeng Wang. Multi-resolution surface approximation for animation. In Proceedings of Graphics Interface, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. Computer Graphics (SIGGRAPH '93 Proceedings), pages 19-26, August 1993.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[James R. Kent, Wayne E. Carlson, and Richard E. Parent. Shape transformation for polyhedral objects. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):47-54, July 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222932</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. Michael Lounsbery. Multiresolution Analysis for Surfaces of Arbitrary Topological Type. PhD thesis, Department of Computer Science and Engineering, University of Washington, September 1994. Available as ftp://cs.washington.edu/pub/graphics/LounsPhd.ps.Z.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Michael Lounsbery, Tony DeRose, and Joe Warren. Multiresolution analysis for surfaces of arbitrary topological type. Submitted for publication. Preliminary version available as Technical Report 93-10-05b, Department of Computer Science and Engineering, University of Washington, January, 1994. Also available as ftp://cs.washington.edu/pub/graphics/TR931005b.ps.Z.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, H. Yahia, and A. Verroust. Interactive texture mapping. Computer Graphics (SIGGRAPH '93 Proceedings), 27(3):27-34, August 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>67254</ref_obj_id>
				<ref_obj_pid>67253</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Stephane Mallat. A theory for multiresolution signal decomposition: The wavelet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11 (7):674-693, July 1989.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>33372</ref_obj_id>
				<ref_obj_pid>33367</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[J.S. Mitchell, D.M. Mount, and C.H. Papadimitriou. The discrete geodesic problem. SlAM Journal of Computing, 16(4):647-668, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[David M. Mount. Voronoi diagrams on the surface of a polyhedron. Department of Computer Science CAR-TR-121, CS-TR-1496, University of Maryland, May 1985.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and R Borrel. Multi-resolution 3D approximations for rendering. In B. Falcidieno and T.L. Kunii, editors, Modeling in Computer Graphics, pages 455-465. Springer-Verlag, June-July 1993.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Richard Schoen and Shing-Tung Yau. Univalent harmonic maps between surfaces. Inventiones math., 44:265-278, 1978.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218439</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R Schr6der and W. Sweldens. Spherical wavelets: Efficiently representing functions on the sphere. Computer Graphics, (SIGGRAPH'95 Proceedings), 1995.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[William Schroeder, Jonathan Zarge, and William Lorensen. Decimation of triangle meshes. Computer Graphics (SIGGRAPH '92 Proceedings), 26(2):65-70, July 1992.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-tiling polygonal surfaces. Computer Graphics (SIG- GRAPH '92 Proceedings), 26(2):55-64, July 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Greg Turk and Marc Levoy. Zippered polygon meshes from range images. Computer Graphics (SIGGRAPH '94 Proceedings), 28(3):311- 318, July 1994.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222231</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Amitabh Varshney. Hierarchical Geometric Approximations. PhD thesis, Department of Computer Science, University of North Carolina at Chapel Hill, 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multiresolution Analysis of Arbitrary Meshes Matthias Eck3 Tony DeRose3 Tom Duchamp3 Hugues Hoppet Michael 
Lounsbery+ Werner Stuetzle3 University of Washington, Seattle, WA t Microsoft Research, Redmond, WA 
+ Alias Research, Seattle, WA Abstract In computer graphics and geometric modeling, shapes are often 
represented by triangular meshes. With the advent of laser scanning systems, meshes of extreme complexity 
are rapidly becoming commonplace. Such meshes are notoriously expensive to store, transmit, render, and 
are awkward to edit. Multiresolution analysis offers a simple, uni.ed, and theoretically sound approach 
to dealing with these problems. Lounsbery et al. have recently developed a technique for creating multiresolution 
representations for a restricted class of meshes with subdivision connectivity. Unfortunately, meshes 
encountered in practice typically do not meet this requirement. In this paper we present a method for 
overcoming the subdivision connectivity restriction, meaning that completely arbitrary meshes can now 
be converted to multiresolution form. The method is based on the approximation of an arbitrary initial 
mesh ] by a mesh ] Jthat has subdivision connectivity and is guaranteed to be within a speci.ed tolerance. 
The key ingredient of our algorithm is the construction of a parametriza­tion of ] over a simple domain. 
We expect this parametrization to be of use in other contexts, such as texture mapping or the approximation 
of complex meshes by NURBS patches. CR Categories and Subject Descriptors: I.3.5 [Computer Graphics]: 
Computational Geometry and Object Modeling. -surfaces and object rep­resentations; J.6 [Computer-Aided 
Engineering]: Computer-Aided Design (CAD); G.1.2 [Approximation]: Spline Approximation. Additional Keywords: 
Geometric modeling, subdivision surfaces, wavelets. 1 Introduction In computer graphics and geometric 
modeling, shapes are often represented by triangular meshes. With the advent of laser scan­ning systems, 
meshes of extreme complexity are rapidly becoming commonplace. The objects shown in Color Plates 1(k) 
and 2(g) for instance, consist of 69,473 and 103,713 triangles, respectively. Such meshes are notoriously 
expensive to store, transmit, and ren­der. They are also awkward to edit, as many vertices typically 
must be moved to make a change of substantial spatial extent. Multiresolution analysis offers a promising 
new approach for addressing these dif.culties in a simple, uni.ed, and theoretically sound way. A multiresolution 
representation of a mesh, as recently developed by Lounsbery et al. [11], consists of a simple base mesh 
(Color Plate 1(e)) together with a sequence of local correction terms, Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 called wavelet coef.cients, capturing the 
detail present in the object at various resolutions. Color Plates 1(g) (h) show a sequence of intermediate 
resolution models incorporating an increasing number of wavelets. Multiresolution mesh representations 
are particularly convenient for a number of applications, including: Compression/simpli.cation: A multiresolution 
mesh can be compressed by removing small wavelet coef.cients. More­over, the threshold for removal can 
be chosen such that the resulting approximation is guaranteed to be within a speci.ed error tolerance 
of the original mesh. A number of examples are shown in the color plates.  Progressive display and transmission: 
An attractive method for displaying a complex object is to begin with a low reso­lution version that 
can be quickly rendered, and then progres­sively improve the display as more detail is obtained from 
disk or over a network. Using a multiresolution representation, this is simply achieved by .rst displaying 
the base mesh, and then progressively adding the contributions of wavelet coef.cients in order of decreasing 
magnitude.  Level-of-detail control: High performance rendering systems often use a level-of-detail 
hierarchy, that is, a sequence of ap­proximations at various levels-of-detail. The crudest approxi­mations 
are used when the viewer is far from the object, while higher detail versions are substituted as the 
viewer approaches. Multiresolution representations naturally support this type of display by adding successively 
smaller wavelet coef.cients as the viewer approaches the object, and by removing them as the viewer recedes. 
Moreover, the coef.cients can be added smoothly, thereby avoiding the visual discontinuities encoun­tered 
when switching between approximations of different res­olution. This use of multiresolution representations 
is illus­trated in Color Plates 1(k) and 1(l).  Multiresolution editing: Editing at various scales can 
proceed along the lines developed by Finkelstein and Salesin [4] by or­dering coef.cients according to 
their support, that is, by the spatial extent of their in.uence. Color Plates 2(e) and 2(f) show edits 
of a mesh at low and high levels of detail.  Although the multiresolution analysis of Lounsbery et 
al. [11] can be applied to meshes of arbitrary topological type, it has a serious shortcoming: it is 
restricted to meshes with subdivision connectiv­ity, that is, to meshes obtained from a simple base mesh 
by recursive 4-to-1 splitting (see Figure 1). Figure 6(a) shows an example of a mesh with subdivision 
connectivity it results from recursively splitting the faces of an octahedron four times. Unfortunately, 
few of the meshes encountered in practice have this restricted structure. In this paper we present a 
method for overcoming the subdivision connectivity restriction, meaning that completely arbitrary meshes 
can now be converted to multiresolution form. Our approach is to develop an algorithm for approximating 
an arbitrary mesh M (as in Color Plate 1(a)), which might not have subdivision connectivity, by a mesh 
M Jthat does (as in Color Plate 1(f)), and is guaranteed to be within a prescribed tolerance E lof M 
. We refer to this process as remeshing, and we call M Jthe remesh. Multiresolution analysis of an arbitrary 
mesh M thus proceeds in two steps: we .rst use remeshing to approximate M by a mesh M J with subdivision 
connectivity, and then use the method of Louns­bery et al. to convert M Jto multiresolution representation. 
(Al­though we cannot reproduce here all the results of Lounsbery et al. [11], we have included a brief 
summary in Appendix A.) The key ingredient of the remeshing procedure and the prin­cipal technical contribution 
of the paper is the construction of a parametrization of M over a base complex K Opossessing a small 
number of faces. We then sample the parametrization to produce the remesh. Considerable care is taken 
to create a parametrization and a sampling pattern so that the resulting remesh can be well ap­proximated 
with relatively few wavelet coef.cients. The construction of parametrizations for complex shapes over 
simple domains is a fundamental problem that occurs in numer­ous applications, including texture mapping, 
and the approxima­tion of meshes by NURBS patches. We therefore expect that our parametrization algorithm 
will have uses outside of remeshing. The remainder of the paper is organized as follows. In Section 2, 
we describe the relationship between our work and previously pub­lished methods. In Section 3, we give 
a high level overview of the major steps of the remeshing algorithm. The details of the algorithm are 
presented in Sections 4-7. In Section 8, we apply our method to meshes of varying complexity, and give 
examples of compression, level-of-detail control, and editing. We close with conclusions and future work 
in Section 9. Related Work The dif.culty of dealing with complicated shapes is evidenced by the extensive 
recent research on the topic. The problems of compression/simpli.cation and level-of-detail control have 
been addressed by Turk [20], Schroeder et al. [19], Hoppe et al. [8], Rossignac and Borrel [16], and 
Varsney [22]. Our approach differs from these methods in three principal respects. First, it provides 
guaranteed error bounds, whereas the approaches of Turk, Schroeder et al., and Hoppe et al. do not. Second, 
it pro­duces a single compact representation from which a continuous fam­ily of lower resolution approximations 
can be quickly and easily constructed, whereas the previous methods generate a discrete set of models 
of varying complexity. (We should note, however, that Turk, and Rossignac/Borrel, and Varsney present 
methods for in­terpolating between models.) Third, our representation can be sim­ply and conveniently 
edited at multiple scales, whereas it is hard to imagine how one would achieve similar results using 
the previous approaches. The editing of complex shapes was a central motivation for the introduction 
of hierarchical B-splines by Forsey and Bartels [6]. Forsey and Bartels [5] and Forsey and Wang [7] have 
subsequently developed methods for .tting hierarchical B-splines to meshes topo­logically equivalent 
to a disk. Finkelstein and Salesin [4] have demonstrated how wavelet representations of B-spline curves 
and tensor product surfaces can be used to achieve similar bene.ts. The main advantage of our method 
is its ability to deal with shapes of arbitrary topological type. The problem of parametrizing meshes 
has recently been con­sidered by Maillot et al. [12] in the context of texture mapping. However, the 
parametrizations they construct are not useful for our purpose: their surface tiles are not triangular, 
and their local parametrizations do not .t together continuously. Additionally, our local parametrizations, 
based on the well-established theory of har­monic maps, are simpler to compute than the ones used by 
Maillot et al., and seem to produce parametrizations of comparable quality (see Section 4). Finally, 
the technique of Schr¨ oder and Sweldens [18] could be used in place of Lounsbery et al. for multiresolution 
analysis of the remesh. 3 Overview of Remeshing The basic idea of remeshing is to construct a parametrization 
of M over a suitably determined domain mesh K O. This parametrization is then resampled to produce a 
mesh M Jthat has subdivision con­nectivity and is of the same topological type as M . Our remeshing algorithm 
consists of three steps, as illustrated in Color Plates 1(a)-1(h): 1. Partitioning: Partition M into 
a number of triangular regions T l)...)T T, as shown in Color Plate 1(d). We want the number r of regions 
to be small, because the lowest complexity ap­proximation we can construct has r faces, as shown in Color 
Plate 1(e). Basic tools used in partitioning are harmonic maps, maps that preserve as much of the metric 
structure (lengths, an­gles, etc.) of M as possible. Harmonic maps are described in Section 4. A detailed 
description of our partitioning algorithm is given in Section 5. Identifying each of the m vertices 
or nodes of the triangulation T l)...)T Twith a canonical basis vector of R mde.nes a mesh in R m, called 
the base complex, with a face corresponding to each of the r triangular regions. This mesh serves as 
the domain of the parametrization constructed in the next step. 2. Parametrization: For each region 
T iof M construct a (local) parametrization P i:P i-T iover the corresponding face P i of the base complex 
K O. The local parametrizations are made to .t together continuously, meaning that collectively they 
de­.ne a globally continuous parametrization P :K O-M .We want the coordinate functions of the parametrization 
to vary as little as possible since such functions have multiresolution ap­proximations with few signi.cant 
wavelet coef.cients, leading to high compression ratios. Harmonic maps in a sense mini­mize distortion 
and therefore are particularly well suited for this purpose. A description of the parametrization step 
is pre­sented in Section 6. 3. Resampling: Perform J recursive 4-to-1 splits on each of the faces of 
K O(see Figure 1). This results in a triangulation K Jof K Owith subdivision connectivity. The remesh 
M J,as shown in Color Plate 1(f), is obtained by mapping the vertices of K Jinto R 3using the parametrization 
P , and constructing an interpolating mesh in the obvious way; M Jtherefore has vertices lying on M , 
and has subdivision connectivity.  The resampling step is described more fully in Section 7, and it 
is shown that J can be determined so that M Jand M differ by no more than a speci.ed remeshing tolerance 
E l.  4 Harmonic maps A crucial building block of our remeshing algorithm is a method for constructing 
a parametrization of a (topological) disk D C M over a convex polygonal region P C R 2. This method is 
used in two places: in the construction of the triangulation T l)...)T Tof (a) (b) (c) Figure 1: 4-to-1 
splitting of a triangular face: (a) the initial face; (b) after one 4-to-1 split; (c) after two 4-to-1 
splits. M (see Section 5), and in the parametrization of M over the base complex K O(see Section 6). 
We want this parametrization to have small distortion; for example, if D is (close to) planar, we want 
the parametrization to be (close to) linear. Because the region may be geometrically complex (see, for 
example, Figure 2), some distortion is usually inevitable. While it is not clear in general how to .nd 
a parametrization P with small distortion, there is a closely related and well-studied problem that has 
a unique solution: Fix a homeomorphism 9 be­tween the boundary of D and the boundary of the polygonal 
region P ; then there is a unique harmonic map h :D -P that agrees with 9 on the boundary of D and minimizes 
metric dispersion (see Eells and Sampson [3], pages 114 115, and the survey article by Eells and Lemaire[2]). 
Metric dispersion is a measure of the extent to which a map stretches regions of small diameter in D 
. It is thus a measure of metric distortion. In addition to minimizing metric distortion, the harmonic 
map h has a number of important properties: (i) It is in.nitely differen­tiable on each face of D ; (ii) 
it is an embedding [17]; and (iii) it is independent of the triangulation of D . Because h :D -P is an 
embedding, the inverse h 0lis a parametrization of D over P .We will return below to the issues of choosing 
the boundary map 9 and of computing approximations to h . The dispersion minimizing property of harmonic 
maps is illus­trated in Figure 2, which shows a piecewise linear approximation of a harmonic map from 
a geometrically complex region onto a poly­gon. The relatively dense regions of the polygon correspond 
to the ears and nose of the cat. Notice that the aspect ratios of triangles tend to be preserved. Notice 
also that the map introduces a certain amount of area compression. This is inevitable because the region 
has a large area relative to its circumference, and consequently any embedding must introduce some distortion 
in edge lengths. The harmonic map tends to minimize such distortion while maintaining the embedding property 
and attempting to preserve aspect ratios of triangles. Harmonic maps can be visualized as follows. Imagine 
D to be composed of elastic, triangular rubber sheets sewn together along their edges. Stretch the boundary 
of D over the boundary of the polygon P according to the map 9 . The harmonic map minimizes the total 
energy E haTm[h ]of this con.guration of rubber sheets. Rather than constructing the harmonic map directly, 
we com­pute a piecewise linear approximation. Assume that n vertices V l)...)V , called corners, have 
been selected on the boundary D of D (see Figure 2), and (for technical reasons) assume that the de­gree 
of each of the remaining boundary vertices is at least 3. We choose the polygon P by mapping the corners 
of D onto the vertices of an n -gon in R 2. The vertices of the n -gon are positioned on a circle such 
that the sides subtend angles proportional to the arc lengths of the boundary segments of D joining the 
corresponding corners. We then de.ne 9 to be the piecewise linear map that sends the corners of D to 
the vertices of P , and is a homothety (i.e. an isometry up to a constant factor) between each boundary 
segment of D and the corresponding side of P (Figure 2). (a) Original mesh tile (b) Harmonic embedding 
 Figure 2: The harmonic map for the head of a cat. The neck of the cat is mapped onto the boundary of 
the polygon. The corner ver­tices (thoses sent to vertices of the polygon) are indicated by small balls. 
Now suppose that h is any piecewise linear map that agrees with 9 on the boundary. It is therefore uniquely 
determined by its values h (i )at the vertices of D . By explicitly integrating the functional E haTmover 
each face, one .nds that E haTmcan be reinterpreted as the energy of a con.guration of springs with one 
spring placed along each edge of D : . E haTm[h ]=1/ 2K i j h (i )0 h (j ) 2) (1) {i j}Edges(D) where 
the spring constants K i jare computed as follows: For each edge { i)j } , let Li jdenote its length 
as measured in the initial mesh D , and for each face { i)j)k } , let Areai j denote its area, again 
as measured in D . Each interior edge { i)j } is incident to two faces, say { i)j)k l} and { i)j)k 2} 
. Then 01 L2 K i j=i Lj 2 0 Li j 2/ Areai j 01 L2 L20 L2 i j i j/ Areai j The formula for spring 
constants associated to boundary edges has only one term. Although the spring constants K i jcan assume 
negative values, the function (1) is positive de.nite, and its unique minimum can be found by solving 
a sparse linear least-squares problem for the val­ues h (i ). In contrast to the harmonic map itself, 
its piecewise linear approximation is not always an embedding. In our experience, this problem occurs 
extremely rarely (3 times in the roughly 1000 har­monic maps we computed). In these cases we use uniform 
spring constants. For the remainder of this paper we refer to the unique piecewise linear function minimizing 
(1) as a harmonic map, although strictly speaking it is only an approximation. Others have developed 
similar approaches to embedding disk­like regions. One such approach, described by Kent et al. [9], is 
also based on minimizing the energy of a network of springs. They choose spring constants to be either 
all equal or inversely propor­tional to edge lengths. Maillot et al. [12] introduced another func­tional, 
also based on elasticity theory. Figure 3 illustrates the behavior of the various embedding schemes in 
a simple example where the region D (see Figure 3(a)) is a triangulation of a planar polygon P and 9 
: D - P is the identity. The harmonic map (Figure 3(b)) is the identity map and therefore has no metric 
distortion. The method of Kent et al. with either choice of spring constants produces considerable metric 
dis­tortion (Figure 3(c) and (d)). Figure 3: Comparison of various spring embeddings . From left to 
right: (a) Original mesh; (b) Harmonic map and embedding of Maillot et al. with a =1;(c) K i j =1;(d) 
K i j =1/ Li j;(e) Embedding of Maillot et al. with a =1/ 2. The mathematical properties of the functional 
proposed by Mail­lot et al. are not entirely clear. In particular, the smooth theory to which it is an 
approximation does not yield planar embeddings of geometrically complex regions. This led them to introduce 
a user­speci.ed tuning parameter a . In the example of Figure 3, the choice a =1also produces the identity 
map, whereas the choice a =1/ 2 leads to small distortion (see Figure 3(e)). The method of Maillot et 
al. appears produce results whose quality is comparable to ours (for appropriately chosen a ). However, 
their method requires non­linear optimization, whereas our method requires only the solution of a sparse 
linear least-squares problem. 5 Partitioning Our partitioning scheme is based on the concepts of Voronoi 
dia­grams and Delaunay triangulations. Let us .rst see how these con­cepts could be used to partition 
a dense triangulation of a planar region into a small number of large triangles. We could begin by se­lecting 
a set of relatively uniformly distributed vertices of the dense triangulation, and then compute the Delaunay 
triangulation for the selected vertices. One method for computing the Delaunay triangu­lation for a set 
of sites in the plane is to .rst construct the Voronoi di­agram. Its polyhedral dual is the Delaunay 
triangulation if Voronoi tiles meet three at a corner. By analogy, our approach is to .rst partition 
the faces of M into a set of Voronoi-like tiles T iusing a discrete approximation of the Voronoi diagram 
as described in Section 5.1. Unlike typical uses of Voronoi diagrams, we do not know the sites a priori 
 they are determined dynamically as the Voronoi diagram is constructed. We then construct the dual to 
the Voronoi diagram, resulting in a Delaunay-like partition of M into triangular regions T i, as described 
in Section 5.2. 5.1 Constructing the Voronoi diagram As mentioned above, we use a discrete version of 
the Voronoi di­agram to partition M into a set of Voronoi-like tiles. An ef.cient algorithm for constructing 
true Voronoi diagrams on the surface of a mesh has been developed by Mount [15], but it is rather dif.cult 
to implement, and is unnecessary for our purposes. We .rst describe an algorithm for computing tiles 
T l)...)T fgiven a set of sites logically positioned at the centroids of the site faces S ={ f l)...)f 
f} . We then present an algorithm for selecting a set S of site faces for which the induced Voronoi diagram 
is dual to a triangulation. The results of applying the Voronoi algorithm is shown in Color Plate 1(b). 
Constructing the Voronoi diagram for a given set of site faces A Voronoi tile T iconsists of all faces 
for which the closest site face is f i. Our measure of distance between faces is an approximation of 
geodesic distance over the surface. It is de.ned by constructing a dual graph to the mesh: the nodes 
of the graph correspond to faces of M , and the edges of the graph connect nodes corresponding to adjacent 
faces. We set the cost of edges in this dual graph to be the distance between centroids of the corresponding 
faces. The distance between two faces is de.ned as length of the shortest path in this dual graph. Constructing 
the Voronoi diagram is a multi-source shortest path problem in the dual graph, which we solve using a 
variant of Di­jkstra s algorithm [1]. The algorithm simultaneously grows the Voronoi tiles from their 
site faces until they cover M . Selecting the site faces In this section we describe an algorithm for 
selecting a set S of site faces such that the induced Voronoi di­agram, computed as above, is dual to 
a triangulation. Although our algorithm for selecting such site faces can be applied to any mesh M , 
let us assume for the moment that M does not possess bound­aries. With this assumption, the Voronoi diagram 
must satisfy the following conditions to be dual to a triangulation: 1. tiles must be homeomorphic to 
disks; 2. no pair of tiles may share more than one cut (a cut is a contigu­ous set of edges of M along 
which a pair of tiles touch); 3. no more than three tiles can meet at any vertex.  The algorithm begins 
by initializing S with a single randomly chosen site face. In the outer loop we then incrementally add 
faces to S until the induced tiling satis.es conditions (1) through (3) above. In the inner loop (tile 
growth), tiles associated with the faces in S are grown until either they cover M , in which case tile 
growth terminates, or until condition (1) is violated. Violation of condition (1) can be detected by 
examining only the neighborhood of the most recently added face. If condition (1) is violated, this face 
is added to S and tile growth is resumed. When tile growth is complete, conditions (2) and (3) are checked. 
If condition (2) is violated, a face along one of the offending shared cuts is selected as a new site 
face. If condition (3) is violated, one of the faces adjacent to the offending vertex is selected as 
a site. If all adjacent faces already are sites, the Voronoi algorithm fails. This has never happened 
in any of the examples we have run. If it were to happen, we would simply use the original mesh as the 
base mesh. To accommodate boundaries, we introduce a single .ctitious Voronoi tile, logically outside 
of M , that touches each of the bound­aries of M . Conditions (1) through (3) can then be applied without 
change. To ensure that the Delaunay-like triangulation covers M , we require that boundary tiles (those 
adjacent to the .ctitious tile) have sites on the boundary of M . This issue is addressed again in the 
next section. To achieve this requirement, the algorithm adds a new boundary site face whenever an interior 
tile touches a bound­ary. As before, when tile growth stops, conditions (2) and (3) are checked, and 
if violated, appropriate new sites are added. It sometimes happens that tiles have adjacent short cuts, 
a situ­ation that leads to Delaunay-like triangles with poor aspect ratios, and hence to poor compression 
rates. We therefore add to the list of conditions one that disallows such tiles. Adjacent cuts of a tile 
are deemed short if the sum of their lengths is less than 10% of the length of the boundary of the tile. 
When an offending pair of cuts is found, one of the faces they share is added as a new site. Properties 
of the Voronoi algorithm The time complexity of the Voronoi algorithm depends on the number s of sites 
that are needed, for which there is no general formula. For a .xed set of s sites, the Voronoi tiles 
can be constructed using an s -source version of Dijk­stra s algorithm. Like the ordinary single source 
Dijkstra algorithm, the s -source version can be implemented ef.ciently (0 (n logn ) time) using a priority 
queue, where the priority of a face is the dis­tance to the nearest site. Pi PkPj Figure 4: Construction 
of initial Delaunay paths on M . . Naively rerunning Dijkstra s algorithm from scratch each time a new 
site face is added would require 0 (sn logn )time. However, the algorithm can be sped up signi.cantly 
by incrementally updating the priority queue as new sites are added to S . Finally, because our site 
selection algorithm uses a greedy search, it cannot be expected to produce a minimal set of sites. 5.2 
Constructing the Delaunay triangulation The partition of M into Voronoi tiles obtained in the previous 
sec­tion has the property that its dual graph consists of 3-sided faces. However, mapping these 3-sided 
faces onto the surface is a non­trivial problem. The obvious approach of connecting pairs of Voronoi 
sites by the shortest paths on the surface as is done in constructing the Delaunay triangulation in 
the plane is not guar­anteed to produce a valid triangulation for arbitrary manifolds since the resulting 
paths can cross. Moreover, .nding the shortest paths between two points on a mesh is itself a dif.cult 
problem [14]. Our alternative uses harmonic maps twice: once to produce an initial Delaunay triangulation, 
and then again to improve the triangulation by straightening its edges. Constructing an initial Delaunay 
triangulation. The .rst step is to compute the harmonic map h ithat carries each Voronoi tile T iinto 
an appropriate planar polygon P i, as described in Section 4. The inverse of h iprovides a parametrization 
of T iover P iwhich we use to construct paths lying on M . Let T iand T jdenote two adjacent interior 
Voronoi tiles as illus­trated in Figure 4. The path of the initial Delaunay triangulation joining these 
tiles is constructed as follows: the cut shared by the tiles is mapped to an edge e i jof P iby the harmonic 
map h i; sim­ilarly, the cut is mapped to an edge e jiof P jby h j(see Figure 4). We construct a line 
L i jfrom the centroid of P ito the midpoint of e i j, and a line L jifrom the centroid of P jto the 
midpoint of e ji. The path is formed by mapping these lines onto M using the inverse harmonic maps. That 
is, the path is obtained by joining h 0l(L i j) i l and h 0jl(L ji). The construction of a path between 
an interior tile T iand a bound­ary tile T is slightly different, as indicated in Figure 4. In order 
for the Delaunay triangulation to cover M , it is necessary to con­struct paths from the boundary. (The 
site selection algorithm of Sec­tion 5.1 was designed with this goal in mind in that it guarantees lNote 
that this path does not connect the site faces as one might expect. We have found that the method described 
here produces more uniform tri­angulations than were obtained by connecting site faces. that boundary 
tiles have site faces on the boundary.) We therefore select a boundary vertex V of the site face f , 
and construct L ias the line from h (V )to the midpoint of e i. The line L iis con­structed as before 
from the centroid of P ito the midpoint of e i. Finally, two adjacent boundary tiles T and T eare connected 
by the path along the boundary between V and V e. The edges of the paths thus constructed are generally 
not edges of M . For convenience in constructing the parametrizations of Sec­tion 6, we re.ne M to include 
the path edges. Straightening the Delaunay edges. The edges of the initial Delaunay triangles constructed 
in the .rst step can have kinks where they cross the border between two Voronoi tiles (see Color Plate 
1(c)). To straighten a Delaunay edge, we construct a second harmonic map from the union of the two Delaunay 
triangles adja­cent to the edge into a planar quadrilateral, as described in Section 4. We then replace 
the edge by the image of the corresponding diagonal of the quadrilateral under the inverse harmonic map. 
This straight­ening step is applied to all Delaunay edges in an arbitrary order, resulting in a .nal 
triangulation T l)...)T T. Color Plate 1(d) shows the result of straightening the edges in Color Plate 
1(c).  6 Parametrization Identifying each of the m vertices or nodes of the triangulation T l)...)T 
Twith a canonical basis vector of R mde.nes the base com­plex K OC R m, with a face corresponding to 
each of the r trian­gular regions. The goal of this section is to construct a continuous parametrization 
P :K O -M of the initial mesh over K O. We map each triangle T ionto a triangular region of the plane, 
again using harmonic maps described in Section 4. We then af.nely map the tri­angular region onto the 
corresponding face P iof the base complex. The composition of the two maps is an embedding, and therefore 
its inverse P ide.nes a parametrization of T iover P i. By construction, the maps P iagree on shared 
boundaries, and thus the P icollectively de.ne a continuous parametrization P of M over K O . 7 Resampling 
In this section, we describe a method for producing a mesh M J with subdivision connectivity from the 
parametrization P :K O -M constructed in Section 6. We also show how to determine the subdivision level 
J so that M Jand M differ by no more than a speci.ed remeshing tolerance E l. For a given value of J 
, we .rst produce a triangulation K Jof K Oby performing J recursive 4-to-1 splits of the faces of K 
O.We then approximate P by a function P Jde.ned as the piecewise linear interpolant to P on K J; that 
is, P Jis such that P J(x J)=P (x J), iiwhere the points x J(called knots) denote the vertices of K J 
. i The simplest strategy for performing a 4-to-1 split of a face is to position the split points at 
midpoints of edges, as illustrated in Fig­ure 1. We refer to this process as parametrically uniform resampling 
since the faces of K Jare of equal size. Alternatively, we could at­tempt to place the knots so that 
the images of triangles of K J, that is, the triangles of the remesh M J, are of equal size. We refer 
to this as geometrically uniform resampling. As one of our fundamental objectives is high compression 
rate, we evaluate the performance of a resampling strategy by the number of wavelet coef.cients needed 
for a given compression tolerance E 2. This number is governed by at least two competing factors: 1. 
As mentioned in Section 3, the coordinate functions of P should be as slowly varying as possible; this 
is largely achieved by the distortion minimizing property of the harmonic map parametrizations. E 2 
Geom. Uniform Hybrid Param. Uniform 0.5% (2679) [5422] (1768) [3562] (2224) [4502] 1.0% (1100) [2180] 
(795) [1591] (1044) [2079] 2.0% (416) [809] (385) [758] (455) [881] 5.0% (112) [223] (130) [245] (143) 
[302] Table 1: Performance of the three sampling strategies on the cat model. Parentheses denote the 
number of signi.cant wavelet coef­.cients; square brackets denote the number of triangles. All exam­ples 
were run using E l =1. 0%. Errors are measured as a percentage of the object s diameter. 2. The triangles 
of M Jshould be of roughly uniform size. Lounsbery et al. de.ne wavelets so that the magnitude of a wavelet 
coef.cient is a measure of the unweighted least­squares error that would be incurred if the coef.cient 
were set to zero. By unweighted we mean that deviations on large tri­angles of M Jare counted no more 
heavily than deviations on small triangles. If M Jhas triangles of roughly uniform size, magnitudes of 
wavelet coef.cients are better measures of ge­ometric error. The strategy that has performed best in 
our experiments is a hy­brid strategy using geometrically uniform sampling in the .rst few splitting 
steps (the .rst three steps in all our examples), and para­metrically uniform sampling in subsequent 
steps. Intuitively, this strategy does a reasonable job of uniformly distributing the trian­gles on a 
coarse scale, while still remaining faithful to the harmonic parametrization on smaller scales. This 
intuition is supported by numerical results. Our tests have shown that hybrid resampling typically results 
in wavelet expan­sions with fewer signi.cant coef.cients than either parametrically uniform or geometrically 
uniform resampling. Moreover, the num­ber of subdivisions J necessary to satisfy a remeshing tolerance 
E l is often smaller and hence the remesh is often faster to compute and requires less storage. Table 
1 presents the results of an experiment for the cat mesh (shown in Color Plate 2(d)) for various wavelet 
compression tolerances E 2. Notice that hybrid resampling is partic­ularly advantageous for small tolerances. 
7.1 Geometrically uniform resampling The task of determining new knots x jE K Oso that the trian­ i 
gles generated are roughly uniform in size is an optimization prob­lem whose solution we approximate 
using the following recursive greedy algorithm. In the parametrically uniform resampling process the 
knot x jat i level j is simply computed as midpoint of the edge of the two (neigh­ j0lj0l boring) knots 
x and x at level j 0 1. Instead of performing (i)(i) uniform subdivision, we de.ne jjj0ljj0l A j x =(10 
A )1 x A 1 x with E (0) 1)) ii(i)i(i)i where the splitting parameter A jis determined as follows: Split 
the ij0lj0l two faces adjacent to the edge from x to x , as illustrated in (i)(i) jj Figure 5. Our 
goal is to .nd A jso that the regions R j =U i iili3 j and S = jU jmap to regions of equal area on M 
. ii2i4 In the current implementation we have simpli.ed the area com­putations by using a discrete approximation: 
We scatter a roughly uniform collection of points on the faces of M , then map these sam­ple points back 
to K Ousing P 0l(P 0lis the harmonic map, so it is already known). We then use binary search to compute 
the param­eter A jso that the number of sampled points in the regions R jand i i j0l x (i) j0l x (i) 
Figure 5: Computing the new knot x j i S jare nearly equal. i  7.2 Bounding the remeshing error In 
this section we describe how to determine J such that the remesh M Jand the initial mesh M deviate by 
no more than a remeshing tolerance E lin an L 0sense. That is, we seek to .nd the smallest J such that 
J P (x )0 P (x ) E l. x Our strategy for determining J will be to perform successive steps of 4-to-1 
splitting until the error bound is satis.ed. To bound the error for a given value of J , let E (x ):=P 
(x )0 P J(x )denote the (vector-valued) error function. First, note that the preimages of the triangles 
of M under P form a partition 7 of K O , and that P is a linear function on each triangle of 7 . Next, 
recall that P Jis linear within each of the triangles of the partition K Jof K O . Thus, E (x ), the 
difference between the two, is linear within each cell of the union partition 7 J =7 U K J . The squared 
norm of E (x )is therefore quadratic and convex up over each cell of 7 J , and so must achieve is maximum 
value at a vertex of 7 J . The L 0error for a given value of J can therefore easily be de­termined by 
evaluating E (x )at the vertices of 7 J . Using a local marching technique such as the one in Kent et 
al. [9], these vertices can be found in time proportional to the total number of vertices in 7 and K 
J .  8 Results Color Plates 1 and 2 illustrate the steps of the algorithm and present examples of its 
applications. Color Plate 1(a)-1(h) demonstrate the complete process of mul­tiresolution analysis for 
a mesh of genus 3. We .rst partition the original mesh of Color Plate 1(a) into Voronoi-like tiles shown 
in Color Plate 1(b). We then construct the initial Delaunay-like triangulation (Color Plate 1(c)), and 
straighten its edges (Color Plate 1(d)). The Delaunay triangles de.ne a simple base complex that serves 
as the domain for the parametrization of the mesh. Re­sampling this parametrization using the hybrid 
strategy described in Section 7 with a remeshing tolerance of E l =0. 75%required J =4subdivision steps 
and produced the remesh shown in Color Plate 1(f) consisting of 17,920 triangles. The lowest resolution 
ap­proximation, shown in Color Plate 1(e), is a piecewise linear em­bedding of the base complex. Color 
Plates 1(g) and 1(h) show more detailed approximations using, respectively, 366 and 2,614 faces. Table 
2 summarizes the remeshing process for a variety of other meshes. (All computing times were measured 
on a SGI Onyx Re­ality Engine 2 with 256MB of memory.) Note that the number of Voronoi tiles is in.uenced 
more by the geometry of the model than by the number of faces of M . Approximating the dinosaur and the 
phone with low tolerances would require high subdivision levels. This is due to the presence object # 
faces of M # Voronoi tiles . i # Delaunay triangles . i remesh. tol. . l subdiv. level J time mins holes3 
11,776 31 70 0.5 % 4 4.6 bunny 69,473 88 162 0.5 % 5 33.5 cat 698 7 9 1.0 % 6 0.8 dino 103,713 117 229 
1.0 % 5 39.3 phone 165,896 69 132 2.5 % 5 346.6 Table 2: Summary of results of the remeshing algorithm 
Color Plate object # faces of ] J compr. tol. . 2 # wavelet coeff. # faces time mins 1(g) holes3 17,920 
4.0 % 179 366 0.9 1(h) holes3 17,920 0.5 % 1,298 2,614 1.0 2(a) bunny 165,888 0.07 % 18,636 37,598 4.5 
2(b) bunny 165,888 0.7 % 2,268 4,639 3.7 2(c) bunny 165,888 1.5 % 952 1,921 3.5 2(i) dino 234,496 0.5 
% 2,329 4,725 5.0 2(l) phone 135,168 0.1 % 7,920 16,451 3.3  Table 3: Summary of results of the algorithm 
of Lounsbery et al. of jagged boundaries which can only be well approximated using a large number of 
subdivisions. Computing times are strongly dependent on the ratio of the num­ber of faces of M to the 
number of Delaunay triangles, since the bottleneck of the algorithm, the harmonic map computation, re­quires 
solving sparse least-squares problems whose time complexity is proportional to the square of the number 
of vertices in the trian­gles. Table 3 summarizes the results of wavelet compression applied to remeshed 
models. Each line of the table gives the number of faces of the remesh, the compression tolerance E 2used 
in the wavelet com­pression method described in Appendix A.2, the number of wavelet coef.cients, the 
number of faces of the resulting approximation, and the time required for .lterbank analysis and synthesis. 
The total de­viation between the compressed model and the original is bounded by E =E lE 2, the sum of 
the remeshing tolerance and the compres­sion tolerance. Note that for storage and transmission purposes, 
the relevant performance measure is the number of wavelet coef.cients rather than the number of faces, 
since only the wavelet coef.cients (and their indices) have to be stored or transmitted. Color Plates 
1(k)-1(l) illustrate level-of-detail control. The origi­nal model (Color Plates 1(f) and 1(k)) was created 
from laser range data using the mesh zippering algorithm of Turk and Levoy [21]. Color Plates 1(k) and 
1(l) show views of the original model and of lower resolution approximations from three different distances. 
Color Plates 2(a)-2(c) are close-ups of the approximations in Color Plate 2(l). Note the enormous reduction 
in the number of triangles when the multiresolution approximations are viewed from afar. Color Plates 
2(d)-(f) illustrate multiresolution editing. Color Plate 2(e) shows a large-scale modi.cation caused 
by changing a wavelet coef.cient at the coarsest level, whereas Color Plate 2(f) corresponds to changing 
two coef.cients at an intermediate level­of-detail. Color Plates 2(g)-(l) show the application of remeshing 
and mul­tiresolution analysis to two additional meshes. 9 Conclusion We have described an algorithm 
for solving the remeshing problem, that is, the problem of approximating an arbitrary mesh by a mesh 
with subdivision connectivity. Combined with the previous work of Lounsbery et al., our remeshing algorithm 
allows multiresolu­tion analysis to be applied to arbitrary meshes. Multiresolution rep­resentations 
support ef.cient storage, rendering, transmission, and editing of complex meshes in a simple, uni.ed, 
and theoretically sound way. We have applied our remeshing algorithm and multiresolution analysis to 
complicated meshes consisting of more than 100,000 triangles. Examples of compression, level-of-detail 
rendering, and editing are shown in the Color Plates. The key ingredient of our remeshing procedure 
and the prin­cipal technical contribution of the paper is the construction of a continuous parametrization 
of an arbitrary mesh over a simple do­main mesh. Parametrizing complex shapes over simple domains is 
a fundamental problem in numerous applications, including texture mapping and the approximation of meshes 
by NURBS patches. We therefore expect that our parametrization algorithm will have uses outside of multiresolution 
analysis. We intend to explore these uses in future work.  Acknowledgments This work was supported 
in part by a postdoctoral fellowship for thelead author (Eck) from the German Research Foundation (DFG),Alias 
Research Inc., Microsoft Corp., and the National ScienceFoundation under grants CCR-8957323, DMS-9103002, 
and DMS­9402734. We are grateful to Marc Levoy and his students at Stan­ford University for providing 
the bunny, dinosaur, and phone mod­els. References [1] A. Aho, J.E. Hopcroft, and J.D. Ullman. Data 
structures and algo­rithms. Addison-Wesley, Reading, Mass., 1983. [2] J. Eells and L. Lemaire. Another 
report on harmonic maps. Bull. London Math. Soc., 20:385 524, 1988. [3] J. Eells and J.H. Sampson. Harmonic 
mappings of Riemannian mani­folds. Amer. J. Math., 86:109 160, 1964. [4] Adam Finkelstein and David Salesin. 
Multiresolution curves. Com­puter Graphics (SIGGRAPH 94 Proceedings), 28(3):261 268, July 1994. [5] D. 
Forsey and R. Bartels. Hierarchical B-spline .tting. ACM Trans­actions on Graphics. To appear. [6] D. 
Forsey and R. Bartels. Hierarchical B-spline re.nement. Computer Graphics, 22(4):205 212, 1988. [7] David 
Forsey and Lifeng Wang. Multi-resolution surface approxima­tion for animation. In Proceedings of Graphics 
Interface, 1993. [8] H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. 
Computer Graphics (SIGGRAPH 93 Proceed­ings), pages 19 26, August 1993. [9] James R. Kent, Wayne E. Carlson, 
and Richard E. Parent. Shape trans­formation for polyhedral objects. Computer Graphics (SIGGRAPH 92 Proceedings), 
26(2):47 54, July 1992. [10] J. Michael Lounsbery. Multiresolution Analysis for Surfaces of Arbi­trary 
Topological Type. PhD thesis, Department of Computer Science and Engineering, University of Washington, 
September 1994. Avail­able as ftp://cs.washington.edu/pub/graphics/LounsPhd.ps.Z. [11] Michael Lounsbery, 
Tony DeRose, and Joe Warren. Multiresolu­tion analysis for surfaces of arbitrary topological type. Submit­ted 
for publication. Preliminary version available as Technical Re­port 93-10-05b, Department of Computer 
Science and Engineer­ing, University of Washington, January, 1994. Also available as ftp://cs.washington.edu/pub/graphics/TR931005b.ps.Z. 
[12] J. Maillot, H. Yahia, and A. Verroust. Interactive texture mapping. Computer Graphics (SIGGRAPH 
93 Proceedings), 27(3):27 34, Au­gust 1993. [13] Stephane Mallat. A theory for multiresolution signal 
decomposition: The wavelet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 
11(7):674 693, July 1989. [14] J.S. Mitchell, D.M. Mount, and C.H. Papadimitriou. The discrete geodesic 
problem. SIAM Journal of Computing, 16(4):647 668, 1987. A  AA ...  (a) B (b) B ... (c) Wavelet coefficients 
Wavelet coefficients Figure 6: Decomposition of a mesh. [15] David M. Mount. Voronoi diagrams on the 
surface of a polyhedron. Department of Computer Science CAR-TR-121, CS-TR-1496, Uni­versity of Maryland, 
May 1985. [16] J. Rossignac and P. Borrel. Multi-resolution 3D approximations for rendering. In B. Falcidieno 
and T.L. Kunii, editors, Modeling in Com­puter Graphics, pages 455 465. Springer-Verlag, June-July 1993. 
[17] Richard Schoen and Shing-Tung Yau. Univalent harmonic maps be­tween surfaces. Inventiones math., 
44:265 278, 1978. [18] P. Schr¨ oder and W. Sweldens. Spherical wavelets: Ef.ciently repre­senting functions 
on the sphere. Computer Graphics, (SIGGRAPH 95 Proceedings), 1995. [19] William Schroeder, Jonathan 
Zarge, and William Lorensen. Decima­tion of triangle meshes. Computer Graphics (SIGGRAPH 92 Pro­ceedings), 
26(2):65 70, July 1992. [20] Greg Turk. Re-tiling polygonal surfaces. Computer Graphics (SIG-GRAPH 92 
Proceedings), 26(2):55 64, July 1992. [21] Greg Turk and Marc Levoy. Zippered polygon meshes from range 
im­ages. Computer Graphics (SIGGRAPH 94 Proceedings), 28(3):311 318, July 1994. [22] Amitabh Varshney. 
Hierarchical Geometric Approximations.PhD thesis, Department of Computer Science, University of North 
Carolina at Chapel Hill, 1994. A Multiresolution Analysis of Subdivi­sion Meshes As mentioned in Section 
1, the main idea of multiresolution analysis is to decompose a function into a low resolution part and 
a set of cor­rection or detail terms at increasing resolutions. Multiresolution analysis was .rst formalized 
by Mallat [13] for functions de.ned on R . Lounsbery [10] and Lounsbery et al. [11] have recently ex­tended 
the notion of multiresolution analysis to functions de.ned on base complexes of arbitrary topological 
type. Their results can be used to construct multiresolution representations of meshes with subdivision 
connectivity. The purpose of this appendix is to sum­marize their basic results and algorithms at a high 
level. A.1 Background The two basic ingredients of multiresolution analysis are a sequence of nested 
linear function spaces and an inner product. Lounsbery et al. use a sequence of spaces V OC V lC111 associated 
with the base complex. To describe meshes, the approximation spaces V j consist of piecewise linear functions; 
speci.cally, V jis the space of continuous piecewise linear functions over a partition K jof K O created 
by performing j recursive steps of 4-to-1 splitting to the faces of K O, as shown in Figure 1. As j increases, 
the triangulation K jbecomes more dense, and so the functions in V jare better able to model arbitrary 
continuous functions on K O. The inner product used by Lounsbery et al. is the standard inner product 
de.ned as . ( f)9 :=f (x )9 (x )d x x where d x is the differential area of K Oembedded in R m, so that 
all faces have unit area. The inner product is used to de.ne the following orthogonal com­plement spaces, 
also called wavelet spaces, jjHl j l :={ f E V ( f)9 =0 9 E V } . Intuitively, l jcaptures the detail 
that is missed when a function in V jHlis approximated by a function in V j. Basis functions for V jare 
called scaling functions. In the piece­wise linear case, particularly simple scaling functions for V 
jare the hat functions on K j: the i -th hat function ¢ jiE V jis the unique function in V jthat is one 
at x jand zero at all other knots of K j. i A wavelet i(x )is a basis function for one of the wavelet 
spaces l . Lounsbery et al. [11] give constructions for wavelet bases on arbitrary base complexes K 
O.A wavelet basis for V jcon­sists of a basis for V Otogether with bases for the wavelet spaces l O)...)l 
j0l . The parametrization P JE V Jfor V jcan be expanded in the hat function basis as . JJ¢ JO P (x )=V 
ii) x E K (2) i where V Jdenote the vertex positions of M J.A multiresolution irepresentation of P Jrefers 
to its expansion in a wavelet basis J0l . . . P J(x )= V O i¢ O i(x ) w j i j i(x )) x E K O) ( ) i j=O 
i where w jdenote the wavelet coef.cients. i An algorithm known as .lterbank analysis can be used to 
con­vert between the hat function expansion and the multiresolution rep­resentation. The geometric interpretation 
of .lterbank analysis is shown in Figure 6. The full detail model, described by P J(x )is successively 
decomposed into a lower resolution approximation to­gether with a collection of coef.cients that multiply 
the wavelets. The result is a simple base mesh together with wavelet coef.cients at various levels of 
detail. The operators A and B in Figure 6 refer to sparse matrices whose entries are given by Lounsbery 
et al..The .lterbank analysis has an inverse process called .lterbank synthe­sis that recovers the full 
resolution model from its multiresolution representation. A.2 Lo Wavelet compression The L 0error caused 
by wavelet compression is the L 0norm of the difference function 8 (x )=P J(x )0 P pJ(x ), where P pJ(x 
)denotes the compressed approximation to P J(x ). This difference function is simply the sum of the wavelet 
terms that have been removed from P J(x ). Since 8 (x )is a piecewise linear function on K J, its L 0 
norm can be determined as in Section 7.2 by recording its values at the vertices of K J . Compression 
in principle proceeds by considering the wavelet coef.cients in order of increasing magnitude. A coef.cient 
is re­moved if doing so does not cause the L 0norm of 8 (x )to exceed E 2. If removal of a coef.cient 
would violate the error tolerance, the coef.cient is retained and the next coef.cient is examined. The 
procedure terminates when all coef.cients have been considered for removal. The examples presented in 
this paper have used a conser­vative approximation to this approach where a bound on the L 0 norm of 
8 (x )is maintained, rather than maintaining 8 (x )itself; we plan to implement the principled approach 
in the near future. (a) Original mesh M (11,776 faces) (b) Voronoi diagram (31 tiles) (c) Initial Delaunay 
triangulation (70 tri.) (d) Straightened Delaunay triangulation (e) Base mesh (70 faces) (f) Remesh 
MJ (J = 4; 17,920 faces) (g) Approx. (.=4.5.; 366 faces) (h) Approx. (.=1.0.; 2,614 faces) (i) Delaunay 
triangulation (162 tri.) (j) Base mesh (162 faces) (k) Original mesh (69,473 faces) (l) LOD using multiresolution 
approx.   Color Plate 1: (a-g) Example of partition, parameterization, resampling, and approximation 
of a mesh using multiresolution analysis; (h-k) Level-of-detail approximations of a dense mesh. (a) 
Approx. ( =0 57%; 37,598 faces) (b) Approx. ( =1 2%; 4,639 faces) (c) Approx. ( =2 0%; 1,921 faces) 
(d) Original mesh (698 faces) (e) Surface editing at a coarse level (f) Surface editing at a .ner level 
 (g) Original mesh (103,713 faces) (h) Base mesh (229 faces) (i) Approx. ( =1 5%; 4,725 faces) (j) Original 
mesh (165,896 faces) (k) Base mesh (132 faces) (l) Approx. ( =2 6%; 16,451 faces)   Color Plate 2: 
(a-c) Multiresolution approximations used in Color Plate 1(l); (d-f) Example of multiresolution surface 
editing; (g-l) More results of multiresolution surface approximation.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218441</article_id>
		<sort_key>183</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[Image snapping]]></title>
		<page_from>183</page_from>
		<page_to>190</page_to>
		<doi_number>10.1145/218380.218441</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218441</url>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39038559</person_id>
				<author_profile_id><![CDATA[81100342764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gleicher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Technology Group, Apple Computer, Inc. 1 Infinite Loop M/S 301-3J, Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adobe Systems, Inc. PhotoshopTM 3.0. Computer Program, 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578131</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ballard, D and Brown, C. Computer Vision. Prentice-Hall, 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>914766</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bier, E. Snap-Dragging: Interactive Geometric Design in Two and Three Dimensions. Ph.D. Thesis, University of California, Berkeley, 1989. Also as Xerox PARC report EDL-89-2.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15912</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bier, E and Stone, M. Snap-dragging. Proceedings of SIGGRAPH 86.Computer Graphics (20) 4: 233-240, 1986.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Burt, P and Adelson, E. A Multiresolution Spline With Application to Image Mosaics. ACM Transactions on Graphics (2) 4:217-236, October 1983.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11275</ref_obj_id>
				<ref_obj_pid>11274</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Canny, J. A Computational Approach to Edge Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI-8) 6:679-698, 1986.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cayley, A. On Contour and Slope Lines. London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, (18) 4:264- 269, 1859.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Chen, SE. An Image-Based Approach to Virtual Reality. Proceedings of SIGGRAPH 95. In Computer Graphics Proceedings, August 1995. This volume.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Danielson, R Euclidean Distance Mapping. Computer Graphics Image Processing (14) 3:227-248, November, 1980.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>39857</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fletcher, R. Practical Methods of Optimization. John Wiley and Sons, 1987.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>922879</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M. A Differential Approach to Graphical Manipulation. Ph.D. Thesis, Carnegie Mellon University, 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>205428</ref_obj_id>
				<ref_obj_pid>205424</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gleicher, M and Witkin, A. Drawing with constraints. The Visual Computer (11) 1, 1995.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22881</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Gonzales, R and Wintz, P. Digital Image Processing, second edition. Addison-Wesley, 1987.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Haralick, R and Shapiro, L. Survey: Image Segmentation Techniques. Computer Vision, Graphics, and Image Proc. (29) 100-132, 1985.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15921</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Heckbert, R Filtering by Repeated Integration. Proceedings of SIG- GRAPH 86.Computer Graphics (20) 4:315-321.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hou, H, and Andrews, H. Cubic Splines for Image Interpolation and Digital Filtering. IEEE Transactions on Acoustics, Speech and Signal Processing (ASSP-26) 6:508-517, 1978.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97253</ref_obj_id>
				<ref_obj_pid>97243</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Hudson, S. Adaptive semantic snapping - a technique for feedback at the lexical level. In Proceedings CHI '90, pages 65-70, 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Kass, M, Witkin A, and Terzopoulos, D. Snakes: Active Contour Models. Intern Journal of Computer Vision (1) 4:321-331, 1988.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Koenderink, J. The Structure of Images. Biological Cybernetics (50): 363-370, 1984.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134413</ref_obj_id>
				<ref_obj_pid>134396</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Leymarie, F and Levine, M. Fast Raster Scan Propagation on the Discrete Rectangular Lattice. Computer Graphics, Vision, Image Processing: Image Understanding (55) 1: 84-94, 1992.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Lucas, B and Kanade, T. An Iterative Image Registration Technique with an Application to Stereo Vision. Proceedings 7th HCAI 1981, pages 674-679, August 1981.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[MacViar-Whelan, P and Binford T. Intensity Discontinuity Location to Subpixel Precision. Proceedings 7th HCAI 1981, pages 752-754, August 1981.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Maxwell, J. On Hills and Dales. London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, (40) 269:421-427, 1870.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Miller, G and et. al. The Virtual Museum: Interactive 3D Exploration of a Multimedia Database. Journal of Visualization and Computer Animation (3) 183-197, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Mart, D and Hildreth, E. Theory of Edge Detection. Proc. Royal Society of London (207) 187-217, 1980.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192076</ref_obj_id>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Nalwa, V. A Guided Tour of Computer Vision. Addison-Wesley, 1993.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130597</ref_obj_id>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. Digital Image Processing, 2nd ed. J Wiley and Sons, 1990.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Press, W, Flannery, B, Teukolsky, S, and Vettering, W. Numerical Recipes in C. Cambridge University Press, 1986.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Smith, AR. Tint Fill. Proceedings of SIGGRAPH 79. Computer Graphics (13) 2:276-283.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. Sketchpad: A Man Machine Graphical Communication System. Ph.D. Thesis, Massachusetts Institute of Technology, 1963.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Tabatabai, A and Mitchell, O. Edge Location to Subpixel Values in Digital Imagery. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI-6) 2:188-201, March 1984.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11786</ref_obj_id>
				<ref_obj_pid>11783</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Torte, V and Poggio, T. On Edge Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI-8) 2:147-163, 1986.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>169065</ref_obj_id>
				<ref_obj_pid>169059</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Venolia., D. Facile 3D Manipulation. In Proceedings INTERCHI '93, pages 31-36, 1993]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Williams, L. Pyramidal Parametrics. Proceedings of SIGGRAPH 83. Computer Graphics (17) 3:1-11.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Witkin, A. Scale Space Filtering. In Alex Pentland, ed., From Pixels to Predicates. Ablex, 1984. Reprinted from Proceedings of IJCAI '83.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Snapping Michael Gleicher Advanced Technology Group Apple Computer, Inc. ABSTRACT Cursor snapping 
is a standard method for providing precise point­ing in direct manipulation graphical interfaces. In 
this paper, we introduce image snapping, a variant of cursor snapping that works in image-based programs 
such as paint systems. Image snapping moves the cursor location to nearby features in the image, such 
as edges. It is implemented by using gradient descent on blurred ver­sions of feature maps made from 
the images. Interaction techniques using cursor snapping for image segmentation and curve tracing are 
presented CR Descriptors: I.4.3 [Image Processing]: image processing soft­ware. I.3.6 [Computer Graphics]: 
interaction techniques. 1. INTRODUCTION Cursor snapping is a standard method for providing precise point­ing 
in direct manipulation graphical interfaces. Since its introduc­tion in Sutherland s Sketchpad [30], 
variants of snapping have been employed by almost all object-oriented drawing, modeling and CAD applications. 
In this paper, we present image snapping, a technique that extends cursor snapping to image-based applications 
such as paint systems. Image snapping retains the basic idea of traditional cursor snap­ping: the cursor 
follows the motion of the pointing device, but snaps to interesting locations that are nearby. With image 
snapping, features such as edges or locations of a speci.c color can serve as snapping targets, as shown 
in Figure 1. For traditional snapping in an object-oriented drawing application we have a list of objects 
that provide features to snap to. To provide snapping in images, a system might apply image understanding 
techniques to build an analytical, object-oriented model. Such an approach is typically impractical because 
of the complexity of image understanding. Instead, we use image processing techniques to determine likely 
targets on a per-pixel basis. Image snapping searches the image made from these probabilities, called 
the feature map, for targets near the pointer. 1 In.nite Loop M/S 301-3J, Cupertino, CA 95014 gleicher@apple.com 
 Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
This paper presents the methods for realizing image snapping. After an introduction to cursor snapping, 
we present techniques for searching an image for features. The task of identifying features is discussed 
with particular attention to edge detection. We then examine how cursor snapping applies to higher level 
tasks in image-based applications.  2. PRECISE POINTING IN DIRECT MANIPULATION Almost all object-oriented 
drawing, modelling and CAD programs provide some technique to aid the user with precise positioning. 
Bier [3] surveys approaches to this problem. Techniques aim to give the user the required precision, 
yet maintain the dynamic and free feel of direct manipulation. The most successful methods enhance direct 
manipulation positioning by guiding the position of the cursor from which drawing operations are carried 
out. In such schemes, the position of the software cursor is decoupled from the the screen location speci.ed 
by the hardware pointing device, which we call the pointer. The cursor follows the pointer, but snaps 
to nearby locations of interest. Most snapping interfaces, with the exception of Venolia s 3D system 
[33], move the cursor discontin­ uously: when the cursor gets within range, it jumps to its target. A 
common cursor positioning aid is the .xed grid. While easy to implement and use, it permits only operations 
that are grid-aligned. Grids could be used in an image editing program; however, to obtain precision 
with respect to features in an initial image, the fea­tures would have to be aligned with the grid. This 
is especially impractical when the images are acquired from the real world. Other snapping variants are 
scene sensitive, that is, the position of the cursor depends on the contents of the drawing or image. 
When the pointer comes within snapping range of an object, the object s gravitational attraction draws 
the cursor to its surface. Such methods were introduced in Sketchpad [30], where they were dubbed gravity. 
Gravity was developed further in the snap-drag­ging work of Bier [3][4]. Many popular drawing applications 
pro­ vide object snapping. It can provide precision greater than the resolution of the input device since 
snap targets are computed ana­lytically. Snapping also can facilitate reference to objects. Users can 
point near to objects, rather than being forced to point exactly on top of them. Snapping systems often 
provide the user with feedback of the cur­sor location and snapping state. The user can see the target 
of a drawing operation before it occurs, and correct for bad snaps by moving the cursor or overriding 
snapping. Snapping with proper feedback tells the user what locations can be snapped to. Hudson [17] 
suggests many applications of snap feedback.  Figure 1: Examples of Image Snapping. Image snapping attracts 
the cursor to image features such as object edges (a and b), pixels of a spe­ci.c color (blue pixels 
in c), or the centers of particular shapes found with template matching (campground symbols in d). The 
cursor state and pointer position are both always shown. 2.1 Implementing Snapping The central piece 
of a snapping implementation is the function that computes the position of the cursor given the position 
of the pointer. In a traditional snapping implementation, this cursor func­tion has arguments of the 
pointer position and the scene objects. The basic implementation of a cursor function iterates over the 
objects, .nding the location on each that is closest to the cursor. The closest result is returned. Complications 
arise from the need to cull objects for better performance and computing the points that might be snap 
targets. Computing snap points requires .nding the point on each object that is closest to the pointer. 
Sutherland is intentionally vague [30,p.30] in describing how to do this for a line and circle, noting 
that each new object type requires a new method. De.ning this method for more complex objects, such as 
curves, can be dif.cult. If a snapping implementation is to support snapping to object inter­sections, 
the cursor function must compute the intersection of any pair of objects, increasing the dif.culty of 
adding new object types. Gleicher [11] presents a generalized approach to implementing snapping using 
numerical constraint solving. The cursor is consid­ered as a free .oating point that minimizes its distance 
to the pointer location, as if it were connected by a damped spring. Each object has an implicit function 
that measures the distance of a point to the object, so that f(x,y)=0 de.nes the object s edge. When 
the cursor is close to an object, it is snapped to the object by treating the implicit function as a 
constraint on the cursor position. Inter­sections of objects are handled by solving multiple constraints 
simultaneously. The constraint snapping the cursor to an object is removed if the spring connecting the 
cursor to the pointer must pull too hard. We can think of generalized snapping as a physical pro­cess: 
the cursor is a ball that we position by pushing it around on a surface. Objects are grooves in the surface. 
If the ball comes close to a groove, it will fall in. If we push the ball gently, or in a direc­tion 
that allows it to stay in the groove, it will roll within the groove. If we push harder, the ball will 
come out of the groove.  3. IMPLEMENTING IMAGE SNAPPING The image snapping algorithm must compute the 
following: given the location of the pointing device, return the precise position of where the user is 
most likely to be pointing. We assume that posi­tions correspond to positions in the image, that is, 
that the transfor­mation from screen coordinates to image coordinates has already been applied. The image 
snapping function searches a feature map for snap tar­gets near the pointer. We postpone discussion of 
how to obtain the feature map until Section 4. For now, we imagine that some oracle will tell us if each 
pixel is part of a feature. This will not, in gen­eral, be a binary (thresholded) result. Varying values 
can indicate probabilities and varying amounts of feature in the image region represented by the pixel. 
We prefer to avoid thresholding the fea­ture maps as it allows for anti-aliasing and also because picking 
a proper threshold to reject improbable pixels can be dif.cult to do statically (see [27] for a discussion). 
We use the convention that in the image map, a maximum value indicates the absence of a feature and a 
minimum value indicates the most certain features. We can think of such an image map as a height .eld 
where the image features are canyons. To search for a feature near the pointer location, we could start 
at the pointer location and spiral outward, stopping when either a fea­ture pixel had been found, or 
when the spiral had gotten so large that anything it were to .nd would be out of snapping range. If the 
snapping radius is small, the number of pixels that need to be examined is manageable. If not, fast algorithms 
for computing Euclidean distance maps [9][20] can pre-compute the nearest non­ zero pixel in the feature 
map for each pixel in the image. Such an approach to image snapping has several drawbacks: 1. It has 
no methods of rejecting noise; 2. It uses a preset stopping criteria, globally thresholding the fea­ture 
map; 3. There is no way to trade off certainty and size for distance; 4. It stops at the .rst feature 
it .nds. Another, possibly better, fea­ture, might be equidistant, or just slightly farther away. Also, 
it will stop at the boundary of feature regions, rather than seeking the best location on the feature. 
 In cases where the feature map is binary, uncluttered, and assumed to be perfect, computing a map of 
nearest pixels using a Euclidean distance map algorithm provides a simple method for implement­ing image 
snapping. Typically, we prefer to use local stopping cri­teria rather than global thresholding, and perform 
some operation that allows the algorithm to take an entire neighborhood into account at once. The use 
of local extrema to de.ne features has a long tradition, see Maxwell[23] and Cayley[7] for example, and 
is a commonly used in edge detection, such as [25] and [6]. We therefore, suggest a different process: 
follow the image map gradients to a nearby image feature point. Basically, we view the feature map as 
a height .eld, drop a ball at the pointer position, and watch it roll down hill. If the ball reaches 
a satisfactory feature, we have found a point to snap to. Since the downhill direction is given by the 
gradient (actually, the negative of the gradient), such a search is easy to implement. This method is 
known as steepest descent [10][28]. Issues that make steepest descent unattractive for standard numerical 
problems do not apply to our task of .nding minima in the feature map: we are only interested in problems 
where we have good starting points, the discrete nature of the image provides a natural step size and 
accuracy bound, and the small step size and direction-based stopping criteria, discussed later, avoid 
troubling oscillations. 3.1 Computing Gradients and Blurring Texts, such as Pratt[27] and Nalwa[26], 
contain more complete discussions of the methods for measuring gradients in images. We review the basics 
here. Although an image is a continuous function over a region of the plane, in practice we only have 
a sampled rep­resentation. Therefore, to compute derivatives, we must use dis­crete approximations. The 
simplest method is to merely look at the adjacent pixels: . x . f x, y( ) = f x 1, y+( )  f x, y( ) 
(EQ 1) . y . f x, y( ) = f x y + 1,( )  f x, y( ) Other methods look at more pixels in order to better 
estimate the gradient. A common measure called the Sobel operator averages the .nite differences in different 
directions, examining a 3x3 region around the point to be evaluated. In matrix notation, the Sobel operators 
are: 1 2 1 101 1 1 S =- , S =- 202 (EQ 2) 000 y x 4 4 121 1 0 1 neighborhood over which an image feature 
has in.uence on the cursor. Our implementation uses B-Spline basis low-pass .lter kernels created by 
successively convolving a 2x2 box by itself[15][16]. Such .lters approximate Gaussian kernels. Blurring 
the feature map has many bene.ts. Foremost, it allows the effects of a larger neighborhood to be felt 
at a given pixel. The high frequencies in images are often noise, so blurring can help cancel these effects. 
While these high frequencies might represent .ne details in an image, too many small details can cause 
clutter. Also, blurring can add useful detail within the features. For example, thick line features get 
stripes down their center, large points get dots at their centers, and intersections are emphasized. 
These effects are illustrated in Figure 3 and Figure 4. ABC D where Sx is the operator for the gradient 
in the x direction, and Syis the operator for the y direction. Our implementation always computes image 
gradients using the Sobel operators. On a raw feature map, image gradients provide little information 
for snapping. A feature can only in.uence a point one pixel away, there is no provision for noise suppression, 
and no mechanism for trading off certainty and size for distance. Also, because we know nothing about 
the smoothness of the image function, there is no reason to believe that the steepest direction is most 
likely to lead us to the nearest, or darkest, edge. These problems are illustrated in the example of 
a 1D image shown in Figure 2. ABC D Figure 2: Cursor snapping in a 1 dimensional image shown as a height 
.eld. 4 starting points are shown. Each is given the opportunity to roll down hill. Point A does not 
roll anywhere since there is no gradient at its pixel. Point B correctly rolls downhill into the deep 
canyon. Points C and D roll down into the shallow canyons, although there are deeper ones nearby. In 
short, image snapping at pixel scale [19][35], is ineffective. Abrupt changes, or high frequencies, in 
feature mask intensities make it impossible to sense features that are far away. Blurring the feature 
map to examine it at greater scale is essential for image snapping. Torre and Poggio[32] provide an alternate 
motivation: without blurring, evaluating gradients is an ill-posed problem. Blurring is a standard image 
processing operation; most image pro­cessing texts such as Pratt[27] contain full discussions. Brie.y, 
blurring is accomplished by taking a weighted average of the neighborhood around each pixel. Writing 
the weights as a matrix gives a kernel that can be convolved with the image to produce the image of averages. 
Linear .lters are described by these kernel matrices. Larger kernels typically cause blurrier images. 
From the standpoint of image snapping, the larger the kernel, the larger the Figure 3: Cursor snapping 
in a blurred 1D image. A 3 pixel blur­ring operation has been applied to the image of Figure 2. In this 
case, each of the starting points snaps into a nearby deep canyon. a) b) Figure 4: Blurring a simple 
image shows how blurring emphasizes the centers of points, lines, and intersections. The darkest points 
in image b) are centers of the circle and cross. The bene.ts of blurring can be a problem. The larger 
area of atten­tion can be problematic in cluttered scenes. The high frequencies in an image may not be 
noise, but rather, might be important features. The effect of features migrating, for example to the 
center of a wide line, may not be desired. There is a trade-off in selecting how much blurring should 
be used for image snapping. A large amount of blurring allows the cursor to snap from larger distances 
and better reject noise at the expense of the ability to resolve .ne detail. As the blur factors grow 
larger, issues in how to represent the fea­ture maps arise. While a large blur allows a feature to affect 
a posi­tion many pixels away, the amount of this effect can be very small. We use .oating point numbers 
to represent feature maps; however, there is still an issue with noise rejection when the system is made 
sensitive enough to accommodate these very small values. We use a threshold to remove extremely small 
values.  3.2 Searching the Feature Map Gradients in the feature map indicate which direction a search 
should proceed in order to .nd the best feature point. The magni­tude provides little information on 
how to search, so we ignore it for searching. The search process moves a pixel at a time begin­ning with 
the mouse position. At each step, we evaluate the gradi­ent of the blurred feature map to determine which 
direction to move the search to one of the pixel s 8-connected neighbors. This quantization of the gradient 
direction means that we will not neces­sarily .nd the nearest point on the feature. We continue the process 
until it either moves too far from the initial starting point, meaning that there is no feature within 
the snapping radius, or when the search has found the best feature that it expects to .nd, e.g. a local 
extremum. Local extrema are identi.ed as places where the gradient vanishes [10]. Basically, when there 
is no downhill direction from a particu­lar location, that location must be a minimum. Because of sam­pling, 
this criterion is not suf.cient for image snapping. Local minima can actually occur between pixels. In 
such cases, the gradi­ents of neighboring pixels will point at one another, as shown in Figure 5. We 
therefore add the additional stopping criterion that if the gradient changes direction too much between 
steps, a feature has been found. By placing limits on the range of directions allowed in a search, we 
can avoid looping and cycling. Our algo­rithm de.nes this search range from the initial step direction. 
 Figure 5: A 2-pixel wide feature. The left shows a 2 pixel wide feature. The right shows this feature 
blurred, with arrows denoting the gradient direction. Notice that the local minima is in the center of 
the feature, between the pixels. Another issue in de.ning a stopping criterion is that within features 
the gradient is not zero. That is, within a feature, some points are more featureful than others. Without 
the direction change crite­rion of the last paragraph, the cursor might make a turn when it reached an 
edge, to search along the edge for a slightly darker spot. Dragging along a feature edge would cause 
the cursor to jump from one point to another. If some feature point, such as an inter­section or corner, 
is suf.ciently more featureful, its gravitational attraction will pull the cursor in as it approaches 
the feature. Notice that our stopping criterion for a search is very different than performing a thresholding 
on the feature map. Rather than basing the decision on the value of the feature point alone, we use infor­mation 
about the neighborhood around the point, as well as the particular search. A different problem occurs 
when we begin our search on a feature. Because we have no static criteria to classify a pixel as a feature 
or not, there is no simple way to distinguish between locations that are exactly on features and locations 
that are too far away from fea­tures. To perform such a classi.cation, when we encounter a start­ing 
point which does not provide a gradient direction to search in, we check to see if neighboring pixels 
point towards this location before discarding it as a non-target.  3.3 Sub-pixel Positioning When a 
local minima occurs between pixels, the direction change stopping criterion stops the search after the 
search has gone too far. In such a case, we know that the real stopping point lies between the last two 
pixels examined in the search. We can use the gradi­ents at these two points to better estimate where 
in between the centers of these two pixels the real stopping point would be. Given the gradients at the 
last two pixels of the search, the real stopping point would be the place along the line between the 
two pixels at which the projection of the gradient vector onto the search direction reaches a zero value. 
To estimate this location, we com­pute the projection of the two gradients onto the inter-pixel line, 
and approximate its change by assuming a linear transition. If the zero crossing is in-between the pixels, 
it is used as the sub-pixel location. This sub-pixel method only adds precision along the direction of 
search, so it is mainly useful for .nding the centers of edges as shown in Figure 6 and Figure 7. Interpolating 
4 neighbor­ ing gradients, as done for the precise edge detector of [22], might better localize points 
but has not been tried in our implementation.  aligned stopping positions. On the right, it shows the 
snap positions computed to sub-pixel accuracy. Notice that the points always lie on the line between 
the last two search points. The utility of sub-pixel positioning is not limited to cases where the feature 
detector is capable of localizing the features to this greater accuracy and the application can make 
use of the precision. In other cases, it adds important consistency when snapping to wide features. Consider 
image snapping without sub-pixel posi­tioning on the example of Figure 5. If the search begins above 
the line, the stopping point will be on the bottom of the line, while if the search begins below the 
line, it will stop on the top. This means that if the user crosses the line while dragging the cursor 
along the feature, zig-zagging will occur. Sub-pixel positioning avoids this: starting from either side 
of the edge would lead to the same stop­ping point. Even if the sub-pixel position is rounded to the 
nearest pixel, consistency can be achieved. 3.4 Achieving Large Snap-Radii Section 3.1 describes a trade-off 
in selecting the amount of blurring to apply to the feature map. To avoid losing the locality and .ne 
detail of the features, we might prefer to use small amounts of blur­ring, especially when we have con.dence 
in the map. The problem with small blurs in such cases is that the snapping radius is very small. In 
this section, we consider how we can achieve large snap­ping radii while still maintaining the features 
of small blurs. We begin by blurring the feature map by a small amount (we typi­cally use 3x3 or 5x5 
kernels) and assume that when close to a fea­ture, this blur provides good performance. This creates 
a small region around features that have gradient information pointing towards them. A simple thresholding 
criteria is possible: if the gra­dient has suf.cient magnitude to determine a meaningful search direction, 
it will guide us to a good feature. If not, we need to use an alternate method to decide which direction 
to direct the search. As the search proceeds and nears a feature, it can use the small­blur gradients 
when they are available. We implement two methods for determining search directions for long snaps. One 
way to enhance the snapping radius is to use different amounts of blurring. Such a technique is known 
as a multi-scale method, and is often used in computer vision, see [26, pp. 92-96] for a brief survey. 
A simple multi-scale image snapping technique would create a very blurry version of the feature map in 
addition to the slightly blurred one. If no gradient is found for a given point in the small blur version, 
the larger blur version is checked. Unfortunately, this simple scheme will not work because the jumps 
in amount of blur will create discontinuities in the gradient .eld. At .rst, the search will proceed 
towards the best feature in the blurry image. As it gets close and switches to the less-blurry image, 
the feature will migrate differently causing a change in the search direction. Meth­ods in computer vision 
typically address this by using a continuum of blurs [19][35]. For image snapping, we approximate these 
meth­ ods by using multiple blurs so that the changes among them are more gradual than jumping between 
a large and small blur. Com­puting and storing multiple blurred images is made practical by the use of 
an image pyramid [5][34] and interpolation. Altering image snapping to use a multi-scale method is straightfor­ward. 
We alter gradient evaluation so that if a zero value is pro­duced at a point, evaluation looks at higher 
levels of the pyramid until a non-zero gradient is found. There are still discrete jumps between levels, 
and therefore discontinuities. These small jumps do not seem to be a problem in practice. An alternate 
approach uses the distance map techniques of Section 3. Rather than building a map to the nearest feature, 
we create a map that stores the nearest point that has a gradient. Image snap­ping is trivially modi.ed 
to begin by consulting the map to .nd the location for starting a search. Such a scheme still uses local 
proper­ties to determine features and uses thresholding to determine where there is suf.cient information 
to begin a search. Our prototype implements both the multi-scale and distance map­ping approaches to 
extending snap-radii. Computing the pyramid and distance map are approximately equivalent in computation 
time. We are able to mix the two approaches, for example to use 2 levels of the image pyramid coupled 
with the distance map. The advantages of the map method are that it can travel long distances quickly 
because it initially teleports rather than searches, it has no issues with small values, and does not 
require level interpolation on the pyramid. Pyramid methods may be preferable because the larger blurs 
reduce noise and clutter in the open spaces and allow for controlling the trade-off between distance 
and certainty. Pyra­mid methods may be easier to update incrementally, but we have not implemented this 
yet. In uncluttered images, pyramids and distance maps both work well for extending the snap radius. 
In cluttered images, larger snap-radii may be problematic for any image snapping method because it becomes 
less clear which feature is the desired target. 3.5 Hysteresis In pictures that are cluttered, i.e. 
where there are many features close to together as in Figure 1b, snapping may be dif.cult as the algorithm 
may not select the features that the user desires. Tradi­tional snapping combats clutter with techniques 
that select features on criteria other than simple closeness. For example, cycling [3][12] allows the 
user to select among features within the snap­ ping radius. Such a method relies on the ability to tell 
different fea­tures apart, which is easy in a traditional, object-oriented, snapping implementation. 
To use such a method in image snapping would require somehow grouping pixels into individual features, 
which is not possible without image understanding. Hysteresis in snapping gives the cursor function a 
preference to stay snapped to the same object. This helps prevent the cursor from jumping around annoyingly. 
When multiple objects are found to be within the cursor radius, the last object snapped to is given prefer­ence, 
rather than simply selecting the closest. Hysteresis is extremely important when tracing features, which 
will be dis­cussed in Section 5.2. Figure 10 provides an example of where hysteresis would avoid a problem 
for the user. As described, cursor snapping does not provide any form of hyster­esis. Each snap operation 
is independent of past and future opera­tions. Like cycling, lack of object structures to snap to makes 
adding hysteresis dif.cult. Returning to the previously snapped point is not the same: we would like 
the cursor to follow smoothly along edges. To fake hysteresis, image snapping can use the heuris­tic 
that nearby feature points are likely to be on the same object. We have created a technique that attempts 
to lock the cursor onto its current feature. When locking is enabled by depressing a modi­.er key, subsequent 
mouse motions pull the cursor along the locked feature. The technique is called cursor pulling because 
it pulls the cursor from its previous position, rather than starting searches from the pointer location. 
Cursor pulling is inspired by generalized snapping that was reviewed in Section 2.1. Like generalized 
snapping, when the cur­sor is locked, we consider it in a groove. The positioning method switches from 
one that .nds grooves to one that pulls the cursor along in the groove from its previous position. We 
displace the cur­sor a small amount towards the pointer and start the descent pro­cess from this point. 
The basic idea is that by starting at points near a point on a particular object, we are more likely 
to end up at another point on the same object. The dif.cult decision in implementing cursor pulling is 
how far to displace the cursor before beginning the descent. The farther away from its initial starting 
point, the more likely it will end up farther away from this point, and the less likely it will return 
to the same object. On the other hand, if we start too close, the cursor is likely to return to the initial 
position. Rather than making a .xed deci­sion, we displace the cursor back as little as possible. To 
imple­ment this, we displace the cursor one pixel towards the pointer, and execute the descent process. 
If the cursor returns to its previous location, then we did not displace it enough, so we try again from 
slightly further away. We continue the process until either we .nd a position that descends to an acceptable 
location, or we give up the search because we have either gotten too far away from the origi­nal point, 
or have found a distance at which the cursor doesn t snap to anything. In the event that we fail to .nd 
a new location, we return the cursor to its previous location. We enhance locking by adding the heuristic 
that between two points on a feature should be only other feature pixels. We add a test that looks at 
the pixels along the line between the previous and current snap positions. If any of these pixels clearly 
do not qualify as feature pixels, we return the cursor to its previously snapped position. This technique 
requires a static threshold for pixels that are clearly not part of a feature. It also often causes the 
cursor to get stuck when there is a small break in a feature that is being traced. As we pull the cursor 
along a feature, it typically moves more slowly than the pointer. The user often waits for it to catch 
up over long distances. Sometimes, the cursor will stick in places where moving towards the pointer does 
not lead to new positions. In such cases, the mouse can be moved to coax the cursor along.  4. FEATURE 
IDENTIFICATION We now consider the problem of creating the feature map. Deter­mining which pixels the 
user is more likely to want to snap to con­sists of two problems: determining what types of features 
are likely to be of interest, and determining what pixels are part of these fea­tures. The .rst part 
is often application speci.c. For example, we might want to snap to spots of a certain color or to bright 
or dark areas of the image. For some applications, we might want to manu­ally create the feature map, 
attracting the cursor to features that an author can identify. Object edges serve as a generally useful 
set of features to snap to. The ability to snap to edges mimics the typical snap targets of tra­ditional 
snapping and is desirable for many applications. The dif.­culty in using edge detection stems from the 
fact that we are interested in perceived edges. Without the ability to identify objects, determining 
what is an object edge can be dif.cult. Edge detection is not always dif.cult. For example, if we have 
an image that is a line drawing or has been segmented into different objects (see Figures 7 and 8), accurately 
identifying edges is easy. Unfor­tunately, such segmentations are not always available, in fact, gen­erating 
them is a motivating application (see Section 5.3). Edge detection is a common problem in image processing 
and computer vision. Most textbooks in either .eld, for example Pratt s image processing text [27] or 
Nalwa s vision text [26], contain sur­ veys. Use of better edge detectors will improve the performance 
of image snapping. For image snapping, good edge detectors are characterized by locality, noise rejection, 
and sensitivity. While these criteria are similar to those typically desired for vision appli­cations, 
the fact that we can handle grayscale and thick lines leads to some differences from the criteria de.ned 
by Canny [6]. In par­ ticular, his goal to have a single pixel wide response for each edge is less important 
for image snapping. We have experimented with a variety of edge detectors for use in image snapping. 
Simple edge detection operators, such as the mag­nitude of the gradient, often work well. More sophisticated 
opera­tors, such as Marr-Hildreth[25] or Canny[6] offer better locality and noise rejection, however, 
they have the less desirable property of thinning the edges. Non-local techniques that attempt to link 
edge pixels together to raise certainty about the edges, such as Hough transforms[27], edge relaxation[2] 
or morphological meth­ ods[27] may further improve edge detector performance for image snapping. As shown 
in Section 3.3, image snapping can locate features to sub-pixel precision. Using this precision requires 
features that are localized to sub-pixel accuracy. Examples of edge detectors that provide such accuracy 
exist in the literature, such as [22] and [31]. We have experimented with a scheme that enlarges an image 
with interpolation, detects edges, then reduces the image back to its original size. However, most of 
our sub-pixel experiments have been performed on synthetic images. Another type of feature detector is 
a template matcher [27] that computes the likelihood that a pixel is the center of a particular shape. 
Using a template matcher output with image snapping pro­duces a method similar to numerical search-based 
tracking [21] where the matches and gradients have been pre-computed. Because we are typically interested 
in extreme points, not smoothly sliding along edges, we allow much higher tolerances on angle changes. 
Effective use of a template matcher typically requires using thresholding. Our primary experiments have 
used template matching to help a user point at symbols on a map, for example to identify campsites in 
a park. Although the example in Figure 1d is synthetically generated, we have had good results on scanned 
maps.  5. INTERACTION IN APPLICATIONS In this section, we look at some common tasks in image editing 
applications and examine how image snapping applies. Automat­ing image editing tasks is dif.cult without 
image understanding. Cursor snapping can provide precision for more manual approaches. Our implementation 
provides continuous feedback of the cursor position and a modi.er key to disable snapping at any time. 
Feed­back always shows the cursor location and whether or not it is snapped, in addition to the pointer 
location. If the cursor is incor­rectly snapped, holding down a modi.er key causes it to return to the 
pointer location. 5.1 Point Positioning Often, a user needs to indicate the position of a point to an 
applica­tion where precision is important, for example, identifying corre­spondence points for image 
morphs, .nding coordinates of particular features, measuring distances between points, or indicat­ing 
locations on a map. Drawing operations that use discrete points, such as rubber-band drawing of lines, 
are also enhanced by cursor snapping.  5.2 Curve Tracing In an object oriented drawing program, the 
cursor position is only used at discrete events. For example, when we draw a line segment or rectangle 
by rubber banding only the initial and .nal positions of the dragging operation are important. This means 
that if the cur­sor makes an incorrect snap during the drag, the user can correct the error without it 
ever being recorded. In image-based applications, an important class of operations does use the cursor 
path. Brushing, penciling, and lassoing are examples of such techniques. Having snapping during these 
continuous drag­ging operations can be extremely valuable for tracing feature curves, for example to 
circle objects. Because the path of the cur­sor is recorded (and is important), errors made by the snapping 
algorithm can be problematic. An example is shown in Figure 10. Even with feature locking techniques, 
as in Section 3.5, bad tran­ sient snaps are unavoidable. Curve tracing with snapping can be made workable 
through the addition of some simple interaction techniques. First, it is crucial to provide feedback 
of the snapping state to the user so that errors can be recognized immediately. Sec­ond, methods for 
backing up in the midst of a drawing operation must be provided. One interaction technique for this is 
to have the user back up over the curve to erase previous drawn portions. Another technique uses a modi.er 
key that switches curve drawing into backup mode. While the key is depressed, moving the cur­sor permits 
specifying points along the curve. This method does  Figure 8: This piece of a panoramic image of a 
store (left) contains images that have particular behaviors when clicked on. To do this, an item buffer 
(right) stores a segmentation image identifying objects. This segmentation was not created using image 
snapping, but was used to create the edges in Figure 7. With the background objects (large rectangles) 
repainted white and the foreground objects (comput­ ers) repainted black, the segmentation can serve 
as a feature map for image snapping, attracting the cursor to computers. Images Copy­right Apple Computer 
Inc, 1994. Figure 9: Lassoing a bird. The Canny edge detector (output shown (b)) performs well on some 
parts of the image, but not on others. Image snapping is used to trace the outline where meaningful snapping 
is easy (c). Other sections of the curve can be drawn manually and com­bined into a single lasso outline. 
use a modi.er key, however, it has the advantage that it allows the user to interactively .nd the point 
in the path that they want to roll back to. 5.2.1 Comparison with snakes Curve tracing is an important 
enough task that techniques have been developed to help automate it. One approach is to .t deform­able 
models to the image. A successful technique for this is the snake of Kass, Witkin and Terzopoulos [18]. 
The user places a snake curve near a feature and it automatically locks on to the fea­ture. A numerical 
optimization moves the entire curve, both attract­ing it to features and keeping its shape smooth. Snake 
implementations allow the user to interactively pull on the snake as it progresses. Snakes have a wide 
variety of uses beyond assisting users in tracing curves. A snake is an active object that seeks image 
features after it has been dropped on the image. Snakes provide a more highly auto­mated process for 
tracing curves than image snapping, at the expense of user control. Where image snapping enhances the 
familiar drawing process, snakes provide a new type of active object that the user can place, watch settle, 
then hopefully coax into having the correct shape. Snakes address the issue of editing a curve after 
it has been placed, which simply drawing with image snapping does not. Our system allows snakes and image 
snapping to be mixed: image snapping is used for drawing curves that can be turned into snakes for editing. 
This combination is useful as the snapping feedback can show the user what features the snake is likely 
to grab when activated. Our initial experiences show that this combination can be frustrating as the 
snake s internal forces rip it from a carefully placed initial curve. Improvements in the snake implementation 
may alleviate this problem. The issue does emphasize the differ­ence in the use of active and passive 
objects as interface elements.  5.3 Image Segmentation Another common task in image editing applications 
is identifying related regions of an image, known as segmenting it. Some of the many uses of segmentation 
that we have considered include identi­fying objects to cut them from their backgrounds, making mattes, 
and creating hit maps for interactive multimedia [8][24], as shown in Figure 8. There is a large literature 
on automatically segmenting images, for example see [14] for a survey. Some of these techniques can be 
used to become semi-automatic interaction techniques, like Smith s tint .ll [29] or the Magic Wand in 
Adobe Photoshop [1]. A more natural, but very manual, method for identifying a segment is lasso­ing, 
where a user traces around the edge of the region. Lassoing is a use of curve tracing, and is similarly 
enhanced by image snapping. Lassoing objects can be a tedious task. Cursor snapping can facili­tate it 
by freeing the user from having to precisely position the lasso. To further ease the task, our system 
allows the user to connect multiple curve segments to create a closed curve, eliminating the need to 
create the entire lasso in one motion. An example is shown in Figure 9. Figure 10: Circling the wheel. 
The counter-clockwise path of the pointer is shown in magenta, the path of the cursor in cyan. If the 
user is not careful, lack of hysteresis can cause bad snaps during circling, as seen on the left. The 
center shows a more careful tracing. The right shows an alternate interaction tech­nique: the user speci.es 
discrete points through which a curve will be .tted.  6. CONCLUSIONS Creating interaction techniques 
that help automate image editing tasks is dif.cult because of the challenges in understanding the content 
of images. Even operations that are trivial for users, such as seeing edges, can be dif.cult to perform 
in a robust and accurate fashion. This can lead to frustrating interfaces as the system fails to see 
things that are obvious to a user. It also suggests a partnership where visual processing is performed 
by the user, rather than attempting to develop fully automated solutions. Avoiding tedium and providing 
precision are issues in such manual interfaces. Image snapping attempts to address these issues by facilitating 
pre­cise pointing at features in the image. The dif.culty of identifying features means that image snapping 
is not perfect. Uncertainties in edge detection lead to noise in the fea­ture maps and blotching along 
feature edges. We are exploring the use of better feature detectors and more sophisticated image processing 
techniques to reduce these problems, enhance the qual­ity of interactions, and improve the range of images 
on which the techniques can be applied. The methods are unlikely to ever be perfect, therefore it is 
important to design interaction that accom­modates this unreliability. Dealing with cluttered images 
is particu­larly challenging. Image snapping brings a familiar feature of drawing programs to image editing. 
However, the interactions in image editing more often use the paths drawn by the cursor than discrete 
positions at snap points. As discussed in Section 5.2, cursor snapping does not apply as well to paths 
as to discrete points. De.ciencies in image snapping, such as the lack of hysteresis and imperfect feature 
iden­ti.cation, become more serious when tracing paths. This suggests the importance of developing new 
interaction techniques that will work better with image snapping. Figure 10 shows an example where an 
alternative interface avoids path tracing. This particular example exploits the fact that the system 
knows the user is identi­fying an ellipse. The multiple click interface is also useful when the positioning 
must be completely manual because the edge detec­tor fails completely (for example trying to circle the 
tire). Presently, we are exploring image snapping inside of a prototype image editing application. However, 
since the inputs and outputs of the methods are standard, it is possible to construct a general purpose 
implementation. For example, an implementation at the window system level could intercept the mouse positions 
and look at the frame buffer directly. Such an implementation might con­struct and blur feature maps 
on the .y. Our prototype implementation provides a testbed for exploring image snapping. Performing gradient 
descent on blurred feature maps creates precise pointing in images. We are exploring a variety of sources 
of features, including a number of edge detectors. Stan­dard image editing interaction techniques are 
enhanced with a familiar method for providing precision, although new interaction techniques should better 
address image snapping s shortcomings. ACKNOWLEDGMENTS Lance Williams encouragement and help with image 
processing were instrumental in the development of Image Snapping. Bruce Horn and Steve Rubin helped 
with Macintosh hacking. Heung-Yeung Shum, Pete Litwi­nowicz and Demetri Terzopoulos donated example code. 
Eric Chen and Ted Casey provided panoramic test images and scanning help. Bruce Horn, Frank Crow, Gavin 
Miller, Ken Turkowski, Heung-Yeung Shum, and Mike Kelley proofread drafts of this paper. REFERENCES 
[1] Adobe Systems, Inc. Photoshop 3.0. Computer Program, 1994. [2] Ballard, D and Brown, C. Computer 
Vision. Prentice-Hall, 1982. [3] Bier, E. Snap-Dragging: Interactive Geometric Design in Two and Three 
Dimensions. Ph.D. Thesis, University of California, Berkeley, 1989. Also as Xerox PARC report EDL-89-2. 
[4] Bier, E and Stone, M. Snap-dragging. Proceedings of SIGGRAPH 86.Computer Graphics (20) 4: 233-240, 
1986. [5] Burt, P and Adelson, E. A Multiresolution Spline With Application to Image Mosaics. ACM Transactions 
on Graphics (2) 4:217-236, Octo­ber 1983. [6] Canny, J. A Computational Approach to Edge Detection. IEEE 
Trans­actions on Pattern Analysis and Machine Intelligence (PAMI-8) 6:679-698, 1986. [7] Cayley, A. On 
Contour and Slope Lines. London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 
(18) 4:264­269, 1859. [8] Chen, SE. An Image-Based Approach to Virtual Reality. Proceedings of SIGGRAPH 
95. In Computer Graphics Proceedings, August 1995. This volume. [9] Danielson, P. Euclidean Distance 
Mapping. Computer Graphics Image Processing (14) 3:227-248, November, 1980. [10] Fletcher, R. Practical 
Methods of Optimization. John Wiley and Sons, 1987. [11] Gleicher, M. A Differential Approach to Graphical 
Manipulation. Ph.D. Thesis, Carnegie Mellon University, 1994. [12] Gleicher, M and Witkin, A. Drawing 
with constraints. The Visual Computer (11) 1, 1995. [13] Gonzales, R and Wintz, P. Digital Image Processing, 
second edition. Addison-Wesley, 1987. [14] Haralick, R and Shapiro, L. Survey: Image Segmentation Techniques. 
Computer Vision, Graphics, and Image Proc. (29) 100-132, 1985. [15] Heckbert, P. Filtering by Repeated 
Integration. Proceedings of SIG-GRAPH 86.Computer Graphics (20) 4:315-321. [16] Hou, H, and Andrews, 
H. Cubic Splines for Image Interpolation and Digital Filtering. IEEE Transactions on Acoustics, Speech 
and Signal Processing (ASSP-26) 6:508-517, 1978. [17] Hudson, S. Adaptive semantic snapping - a technique 
for feedback at the lexical level. In Proceedings CHI 90, pages 65-70, 1990. [18] Kass, M, Witkin A, 
and Terzopoulos, D. Snakes: Active Contour Models. Intern Journal of Computer Vision (1) 4:321-331, 1988. 
[19] Koenderink, J. The Structure of Images. Biological Cybernetics (50): 363-370, 1984. [20] Leymarie, 
F and Levine, M. Fast Raster Scan Propagation on the Dis­crete Rectangular Lattice. Computer Graphics, 
Vision, Image Pro­cessing: Image Understanding (55) 1: 84-94, 1992. [21] Lucas, B and Kanade, T. An Iterative 
Image Registration Technique with an Application to Stereo Vision. Proceedings 7th IJCAI 1981, pages 
674-679, August 1981. [22] MacViar-Whelan, P and Binford T. Intensity Discontinuity Location to Subpixel 
Precision. Proceedings 7th IJCAI 1981, pages 752-754, August 1981. [23] Maxwell, J. On Hills and Dales. 
London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, (40) 269:421-427, 1870. 
[24] Miller, G and et. al. The Virtual Museum: Interactive 3D Exploration of a Multimedia Database. Journal 
of Visualization and Computer Animation (3) 183-197, 1992. [25] Marr, D and Hildreth, E. Theory of Edge 
Detection. Proc. Royal Soci­ety of London (207) 187-217, 1980. [26] Nalwa, V. A Guided Tour of Computer 
Vision. Addison-Wesley, 1993. [27] Pratt, W. Digital Image Processing, 2nd ed. J Wiley and Sons, 1990. 
[28] Press, W, Flannery, B, Teukolsky, S, and Vettering, W. Numerical Recipes in C. Cambridge University 
Press, 1986. [29] Smith, AR. Tint Fill. Proceedings of SIGGRAPH 79. Computer Graphics (13) 2:276-283. 
[30] Sutherland, I. Sketchpad: A Man Machine Graphical Communication System. Ph.D. Thesis, Massachusetts 
Institute of Technology, 1963. [31] Tabatabai, A and Mitchell, O. Edge Location to Subpixel Values in 
Digital Imagery. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI-6) 2:188-201, March 
1984. [32] Torre, V and Poggio, T. On Edge Detection. IEEE Transactions on Pattern Analysis and Machine 
Intelligence (PAMI-8) 2:147-163, 1986. [33] Venolia., D. Facile 3D Manipulation. In Proceedings INTERCHI 
93, pages 31-36, 1993 [34] Williams, L. Pyramidal Parametrics. Proceedings of SIGGRAPH 83. Computer Graphics 
(17) 3:1-11. [35] Witkin, A. Scale Space Filtering. In Alex Pentland, ed., From Pixels to Predicates. 
Ablex, 1984. Reprinted from Proceedings of IJCAI 83.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218442</article_id>
		<sort_key>191</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Intelligent scissors for image composition]]></title>
		<page_from>191</page_from>
		<page_to>198</page_to>
		<doi_number>10.1145/218380.218442</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218442</url>
		<categories>
			<primary_category>
				<cat_node>I.4.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Dynamic programming</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10003809.10011254.10011258</concept_id>
				<concept_desc>CCS->Theory of computation->Design and analysis of algorithms->Algorithm design techniques->Dynamic programming</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010247</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Image segmentation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39080793</person_id>
				<author_profile_id><![CDATA[81100365390]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Mortensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University, Dept. of Comp. Sci., BYU, Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39038570</person_id>
				<author_profile_id><![CDATA[81100343021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Barrett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University, Dept. of Comp. Sci., BYU, Provo, UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>81144</ref_obj_id>
				<ref_obj_pid>81139</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A.A. Amini, T. E. Weymouth, and R. C. Jain, "Using Dynamic Programming for Solving Variational Problems in Vision," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 12, no. 2, pp. 855-866, Sept. 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578131</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D.H. Ballard, and C. M. Brown, Computer Vision. Englewood Cliffs, NJ: Prentice Hall, 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. Bellman and S. Dreyfus, Applied Dynamic Programming. Princeton, NJ: Princeton University Press, 1962.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Y. E Chien and K. S. Fu, "A Decision Function Method for Boundary Detection," Computer Graphics and Image Processing, vol. 3, no. 2, pp. 125-140, June 1974.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. Daneels, et al., "Interactive Outlining: An Improved Approach Using Active Contours," in SPIE Proceedings of Storage and Retrieval for Image and Video Databases, vol 1908, pp. 226-233, Feb. 1993.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E.W. Dijkstra, "A Note on Two Problems in Connexion with Graphs," Numerische Mathematik, vol. 1, pp. 269-270, 1959.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>132032</ref_obj_id>
				<ref_obj_pid>132030</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M.M. Fleck, "Multiple Widths Yield Reliable Finite Differences," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 14, no. 4, pp. 412-429, April 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M. Kass, A. Witkin, and D. Terzopoulos, "Snakes: Active Contour Models," in Proceedings of the First International Conference on Computer Vision, London, England, pp. 259- 68, June 1987.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D. Marr and E. Hildreth, "A Theory of Edge Detection," in Proceedings of the Royal Society of London--Series B: Biological Sciences, vol. 207, no. 1167, pp. 187-217, Feb. 1980.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360004</ref_obj_id>
				<ref_obj_pid>359997</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A. Martelli, "An Application of Heuristic Search Methods to Edge and Contour Detection," Communications of the ACM, vol. 19, no. 2, pp. 73-83, Feb. 1976.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>362594</ref_obj_id>
				<ref_obj_pid>362588</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[U. Montanari, "On the Optimal Detection of Curves in Noisy Pictures," Communications of the ACM, vol. 14, no. 5, pp. 335-45, May 1971.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[E. N. Mortensen, B. S. Morse, W. A. Barrett, and J. K. Udupa, "Adaptive Boundary Dectection Using 'Live-Wire' Two- Dimensional Dynamic Programming," in IEEE Proceedings of Computers in Cardiology, pp. 635-638, Oct. 1992.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30425</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[N. J. Nilsson, Principles of Artificial Intelligence. Palo Alto, CA: Tioga, 1980.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D. L. Pope, D. L. Parker, D. E. Gustafson, and E D. Clayton, "Dynamic Search Algorithms in Left Ventricular Border Recognition and Analysis of Coronary Arteries," in IEEE Proceedings of Computers in Cardiology, pp. 71-75, Sept. 1984.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134401</ref_obj_id>
				<ref_obj_pid>134396</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[D. J. Williams and M. Shah, "A Fast Algorithm for Active Contours and Curvature Estimation," CVGIP: Image Understanding, vol. 55, no. 1, pp. 14-26, Jan. 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218443</article_id>
		<sort_key>199</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Interactive physically-based manipulation of discrete/continuous models]]></title>
		<page_from>199</page_from>
		<page_to>208</page_to>
		<doi_number>10.1145/218380.218443</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218443</url>
		<keywords>
			<kw><![CDATA[grammars]]></kw>
			<kw><![CDATA[interactive techniques]]></kw>
			<kw><![CDATA[physically-based modeling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Discrete event</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>G.2.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10010354</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Discrete-event simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P200330</person_id>
				<author_profile_id><![CDATA[81100529740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mikako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Architecture, Carnegie Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18516</person_id>
				<author_profile_id><![CDATA[81100295587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Witkin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39038146</person_id>
				<author_profile_id><![CDATA[81100334025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baraff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Robotics Institute, Carnegie Mellon University, Pittsburgh, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>74356</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Analytical methods for dynamic simulation of non-penetrating rigid bodies. Computer Graphics (Proc. SIG- GRAPH), 23:223-232, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192168</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Fast contact force computation for nonpenetrating rigid bodies. Computer Graphics (Proc. SIGGRAPH), 28:23-34, 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618272</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[David Baraff. Interactive simulation of solid rigid bodies. IEEE Computer Graphics and Applications, 15:63-75, 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Christopher Carlson. Structure grammars and their application to design. Technical Report EDRC-01-09-89, Engineering Design Research Center, Carnegie Mellon University, Pittsburgh, PA, 1989.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[George Celniker and Dave Gossard. Deformable curve and surface finite-elements for free-form shape design. Computer Graphics, 25(4), July 1991. Proceedings SIGGRAPH '91.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ulrich Flemming. Wall representations of rectangular dissections and their use in automated space allocation. Environment and Planning B: Planning and Design, 5:215-232, 1978.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ulrich Flemming. More than the sum of parts: the grammar of Queen Anne houses. Environment and Planning B: Planning and Design, 14:323-350, 1987.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ulrich Flemming and Robert F. Coyne. A design system with multiple abstraction capabilities. InAvignon '90: Tools, Techniques &amp; Applications, volume 1, pages 107-122. EC2, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Philip E. Gill, Walter Murray, Michael A. Saunders, and Margaret H. Wright. User's guide for NPSOL (version 4.0): A fortran package for nonlinear programming. Technical Report SOL 86-2, Stanford University, Stanford, California, 1986.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Through-the-lens camera control. Computer Graphics, 26, 1992. Proc. Siggraph '92.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Michael Gleicher and Andrew Witkin. Drawing with constraints. The Visual Computer, 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905478</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[John Grason. Methods for the computer-implemented solution of a class of "floorplan". PhD thesis, Department of Electrical Engineering, Carnegie Mellon University, 1970.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>144744</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jeff Heisserman. Generative geometric design and boundary solid grammars. PhD thesis, Department of Architecture, Carnegie Mellon University, 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active contour models. Int. J. of Computer Vision, 1(4), 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Terry W. Knight. Designing with grammars. In Gerhard N. Schmitt, editor, CAAD Futures '91 Proceedings, pages 19-34, 1991.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T.W. Knight. Transformations of De Stijl art: the paintings of Georges Vantongerloo and Fritz Glarner. Environment and Planning B: Planning and Design, 16:51-98, 1989.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[H. Koning and J. Eizenberg. The language of the prairie: Frank Lloyd Wright's prairie houses. Environment and Planning B: Planning and Design, 8:295-323, 1981.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>62970</ref_obj_id>
				<ref_obj_pid>62959</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Sukhamay Kundu. The equivalence of the subregion representation and the wall representation for a certain class of rectangular dissections. Communications of the ACM, 31(6):752- 763, 1988.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Cornelius Lanczos. The Variational Principles of Mechanics. Dover Publications, Inc., 1970.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[David E LaPotin. Mason: A global floor-planning approach for VLSI design. Technical Report IBMC-11657, IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, January 1986.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[William J. Mitchell, Robin S. Ligget, Spiro N. Pollalis, and Milton Tan. Integrating shape grammars and design analysis. In CAAD futures '91 proceedings, pages 1-18, 1991.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[W.J. Mitchell, J.E Steadman, and Robin S. Ligget. Synthesis and optimization of small rectangular floor plans. Environment and Planning B: Planning and Design, 3(1):37-70, 1976.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Herbert A. Simon. Style in design. In Charles M. Eastman, editor, Spatial Synthesis in Computer-Aided Building Design, chapter 9, pages 287-309. Applied Science Publishers LTD, Ripple Road, Barking, Essex, England, 1975.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[G. Stiny. Introduction to shape and shape grammars. Environment and Planning B: Planning and Design, 7(3):343-351, 1980.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[G. Stiny and J. Gips. Shape grammars and the generative specification of painting and sculpture. Information Processing, pages 1460-1465, 1972.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[G. Stiny and W.J. Mitchell. The Palladian grammar. Environmerit and Planning B: Planning and Design, 5(1):5-18, 1978.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134065</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Mark C. Surles. An algorithm with linear complexity for interactive, physically-based modeling of large proteins. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 221-230, July 1992.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, Andrew Witkin, and Michael Kass. Energy constraints on deformable models: recovering shape and non-rigid motion. In Proc. AAAI-87, Seattle, 1987.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, Andrew Witkin, and Michael Kass. Symmetry-seeking models for 3D object reconstruction. International Journal of Computer Vision, 3(1), 1987.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91430</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Jeffrey A. Thingvold and Elaine Cohen. Physical modeling with B-spline surfaces for interactive design and animation. Computer Graphics, 24(2):129-138, March 1990.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[William Welch and Andrew Witkin. Variational surface modeling. Computer Graphics, 26, 1992. Proc. Siggraph '92.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91400</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin, Michael Gleicher, and William Welch. Interactive dynamics. Computer Graphics, 24(2):11-21, March 1990. Proc. 1990 Symposium on 3-D Interactive Graphics.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>565650</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and William Welch. Fast animation and control of non-rigid structures. Computer Graphics, 24(4):243- 252, July 1990. Proc. Siggraph '90.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>53631</ref_obj_id>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[D.F. Wong, H.W. Leong, and C.L. Liu. Simulated Annealing for VLSI Design. Kluwer Academic Publishers, 101 Philip Drive, Assinippi Park, Norwell, Massachusetts 02061, 1988.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Lin S. Woo, C.K. Wong, and D.T. Tang. Pioneer: a macrobased floor-planning design system. VLSI System Design, pages 32-43, August 1986.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Physically-Based Manipulation of Discrete/Continuous Models Mikako Harada Andrew Witkin 
David Baraff Department of Architecture Department of Computer Science Robotics Institute Carnegie Mellon 
University Pittsburgh, PA 15213 Abstract Physically-based modeling has been used in the past to support 
a va­riety of interactive modeling tasks including free-form surface de­sign, mechanism design, constrained 
drawing, and interactive cam­era control. In these systems, the user interacts with the model by exerting 
virtual forces, to which the system responds subject to the active constraints. In the past, this kind 
of interaction has been applicable only to models that are governed by continuous parameters. In this 
paper we present an extension to mixed con­tinuous/discrete models, emphasizing constrained layout problems 
that arise in architecture and other domains. When the object being dragged is blocked from further motion 
by geometric constraints, a local discrete search is triggered, during which transformations such as 
swapping of adjacent objects may be performed. The result of the search is a nearby state in which the 
target object has been moved in the indicated direction and in which all constraints are satis.ed. The 
transition to this state is portrayed using simple but effective an­imated visual effects. Following 
the transition, continuous dragging is resumed. The resulting seamless transitions between discrete and 
continuous manipulation allow the user to easily explore the mixed design space just by dragging objects. 
We demonstrate the method in application to architectural .oor plan design, circuit board layout, art 
analysis, and page layout. Keywords Interactive techniques, physically-based modeling, grammars. 1 Introduction 
Physically-based modeling has been a topic of interest in computer graphics for some time. A particular 
area of interest has been the use of physical simulation or related optimization techniques as a means 
of geometric interaction, allowing users to directly manip­ulate simulated objects subject to constraints. 
This approach has been applied to animation, image analysis, drawing, free-form sur­face modeling, mechanical 
design, and interactive molecular simu­lation [29, 28, 30, 5, 31, 33, 27]. Author s af.liations: Mikako 
Harada, (mh3i@andrew.cmu.edu), Department of Architecture; Andrew Witkin, (aw@cs.cmu.edu), De­partment 
of Computer Science; David Baraff, (baraff@cs.cmu.edu), Robotics Institute. Carnegie Mellon University, 
Pittsburgh PA 15213. Despite its successes, this approach has been subject to the se­vere limitation 
that it only supports manipulation of systems gov­erned by continuous variables. Neither discrete operations 
such as the addition, deletion, or replacement of parts nor discrete changes in object shapes or spatial 
relationships can be accommodated at all within the physically-based manipulation framework. This is 
an un­fortunate limitation because a great many problems of interest in­volve both continuous and discrete 
parameters. An important class of such problems are layout or arrangement problems in which a col­lection 
of objects are to be placed and reshaped subject to constraints that prevent interpenetration, require 
adjacency, bound dimensions, etc. Such problems arise in architecture for .oor planning and furni­ture 
layout, in mechanical and electromechanical design, and even in .ne art. We would like to extend the 
physically-based interaction paradigm to encompass the exploration of this kind of continu­ous/discrete 
problem space. We envision systems in which objects respond continuously to the user s pushing and dragging 
actions in a physical or quasi-physical manner, until they hit a wall imposed by some constraint. They 
then spontaneously undergo some discrete rearrangement or other transformation that does what the user 
asked (essentially, moving the dragged or pushed object point in the indicated direction) with the least 
perceived disruption in the overall arrangement. The change should occur rapidly enough to maintain interactivity, 
and should be visually portrayed in a fashion that the user can clearly understand. For example, in laying 
out a .oor plan there are many degrees of freedom as well as many constraints. Rooms may not overlap. 
A residential plan must include a living room, some number of bed­rooms, etc. The dining room should 
adjoin the kitchen. Rooms have minimum dimensions or areas. Typically, all but bathrooms must adjoin 
an exterior wall or courtyard wall. The whole house must .t within some pre-speci.ed envelope. Within 
these constraints, the size, shape, and arrangement of rooms is free to vary. Some rooms (e.g. a library 
or den) might be optional. Although plans may be judged objectively by criteria such as cost, subjective 
aesthetic crite­ria also play a large role, so the problem cannot be reduced to one of black box mixed-integer 
combinatorial optimization. (And such problems are, as a general rule, NP-hard.) If the basic arrangement 
is .xed, it is a straightforward matter us­ing existing physical manipulation techniques to push and 
pull on walls, adjusting dimensions within the size or area constraints. The new ingredient we want to 
add is this: when a wall is pushed to one of its constraint limits, the system should .nd a re-arrangement, 
(e.g. swapping the living room to the other side of the dining room) that allows the wall on which the 
user pushes to continue to move in the indicated direction. The disruption to the rest of the plan should 
be minimal, and it should be immediately clear to the user what hap­pened. For any particular instance 
of this class of problem, we are given a model whose state is given by a set of continuous and discrete 
(in­teger or boolean) variables, with a graphical representation that de­pends on state, and a set of 
equality or inequality constraint func­tions that likewise depend on state. Our approach to the problem 
handles continuous and discrete changes separately, although tran­sitions between the two modes are seamless 
and transparent to the user. The user s dragging motions are treated as a force to be applied to the 
model s continuous parameters subject to the con­straints. As long as this applied force is not entirely 
cancelled by constraint forces, the discrete variables remain frozen, and our method is just like that 
used in previous physical manipulation sys­tems. When the constraint forces cancel the user s applied 
force, the model is stuck. We then search for a new state of the discrete vari­ables. The main contribution 
of this paper lies in this area. We be­gin with a problem-speci.c search space that is de.ned by a set 
of transformations on the discrete variables. The form of these trans­formations depends on the structure 
of the models. The idea is to de.ne this set of transformations to do a reasonable job of captur­ing 
perceptual similarity, so that each single transformation makes a perceptually small change to the arrangement 
of objects. In the do­mains on which we have experimented, constructing the transforma­tions has not 
been dif.cult; we have done very well with simple op­erations such as swaps and moves. The set of transformations 
gives us a new space in which we may perform shallow searches. The search is limited to transformations 
that directly in.uence the ele­ment being dragged by the user, and is also subject to a depth limit. 
Subject to these restrictions, we look for a state that does the best job of moving whatever was grabbed 
by the user in the desired direction. To portray the transition to this new state we use simple animation 
tricks that provide smooth motion cues for the user. This combination of discrete and continuous mechanisms 
pro­vides seamless, pleasing interactions. The discrete search is hidden from the user. In the case of 
swapping rooms in a .oor plan, the user s experience is something like this: we grab one wall of the 
liv­ing room, pulling toward the opposing wall, which is shared with the dining room. The living room 
shrinks until it reaches its minimum width. If we continue to pull strongly on the wall, the living room 
pops through to the other side of the dining room, with a brief in­terpenetration. Most of the rest of 
the plan is undisturbed, but the kitchen, which is constrained to be adjacent to the dining room, has 
swapped places as well. The remainder of the paper is organized as follows: in the next section, we discuss 
the background of physical manipulation meth­ods, as well as the relevant architecture background of 
shape gram­mars. In the following section we review the math and algorithms of continuous constrained 
manipulation. We then describe the discrete portion in detail. Next, we present results in architecture, 
board lay­out, page layout, and grammar-based analysis exploration of paint­ings. We conclude with discussion 
and directions for future work.  2 Background An active research area in architecture involves the use 
of Shape Grammars as a means of formally capturing architectural style. De­spite considerable evidence 
that such grammars are capable of rep­resenting styles, it has not previously been clear how they could 
ac­tually be used to interactively explore design spaces or create de­signs. A main motivation of our 
work has been to develop means of interacting with shape grammars through direct geometric manipu­lation. 
The .rst portion of this section reviews prior work on shape grammars for architecture. Embedded in the 
discrete problem of searching grammar-based design spaces is a problem of exploring a continuous design 
space by varying sizes, positions, etc. within the design constraints. Our approach to this sub-problem 
closely follows previous work in the use of physically-based methods for interactive manipulation. The 
second portion of this section reviews this prior work. 2.1 Shape Grammars in Architecture Shape grammars 
were .rst introduced to architecture by Stiny and Gips in the 1970 s [25, 24]. Since then, shape grammars 
have been used theoretically to describe the historical style of architectural de­signs [26, 7, 17] and 
to specify new forms [15]. Shape grammars are appealing to architecture researchers because they are 
visual, and because they offer a formal mechanism for capturing the previously vague and ambiguous notion 
of style. The basic idea of shape grammars is to de.ne a set of rewrite rules that operate on geometric 
models by replacing, re-arranging or modifying elements. The grammar serves to implicitly de.ne a set 
of geometric models, i.e. all those that can be derived by some legal sequence of rule applications. 
The hope is that relatively sim­ple rule sets can be devised that capture some aspect of architectural 
style, for example, generating designs for all and only Queen Anne Houses [13]. Several computer programs 
have been developed based on the theoretical foundation of shape grammars.1 These systems have been used 
to demonstrate grammars representing .oor plan layouts [8], framing structures [21], and a class of building 
designs [13]. Interacting with grammar-based systems has been problematic. The only means of exploring 
the discrete space of designs de.ned by a grammar has been sequential manual rule selection by the user, 
with, in some systems, the ability to edit an existing derivation. This mode of editing can be frustrating 
and extremely dif.cult, because the relation between a rule and its geometric effect can be obscure, 
particularly for rules that occur early in the sequence. We believe that the formalism of shape grammars 
would be far more useful if the architect could explore the space by directly manipulating the grammar 
s geometric output rather than the sequence of rules that produced it. A further issue has been that 
these systems do not handle contin­uous dimensional variations (the .ip side of the problem we have faced 
in physically-based manipulation.) A few applications allow grammars to include continuous parameters, 
with some user control over parameter values [21]. However, the problem of solving si­multaneously for 
parameter values that satisfy constraints has been .nessed. No existing grammar-based system allows the 
user to di­rectly manipulate designs to produce geometric variations. Existing physically-based techniques, 
on the other hand, appear to be ideally suited to supporting such direct manipulation.  2.2 Physically-Based 
Modeling for Graphical Interaction Interactive physical simulations have been used as a technique for 
interaction with a variety of graphical models. In computer vision and image analysis, quasi-physical 
active contour models known as snakes are widely used for interactive edge .nding and tracking [14]. 
Snakes are essentially simulated springy wires that are at­tracted to edge features in images, and simultaneously 
subjected to manipulation forces imposed by the user. The use of constrained dynamics simulations for 
interactive ge­ometric modeling was described by Witkin et al. [32]. Flexible­surface simulations for 
interactive computer vision and free-form surface modeling are areas that have also been extensively 
inves­tigated [29, 28, 30, 5, 31, 33]. Constrained dynamics simulations 1Shape grammars in their original 
form could not be implemented di­rectly, due to problems of ambiguity. Several additional formalisms 
were in­troduced on which system implementations were based [4, 13]. Henceforth we use shape grammar 
in the broader sense of these extended geometric grammars. have also been used to support drawing applications 
[11] and for in-model s motion at time t; in particular, if k is zero the system is (mo­teractive camera 
control for animation [10]. Surles [27] describes mentarily) completely unconstrained in its motion. 
Let C(q(t)) . a system for interactive molecular simulations. Interactive simula-IRk be de.ned as the 
vector-collection of these k active constraints, tions that include contact constraints were described 
by Baraff [3]; .. C1(q(t)) the continuous simulation techniques in this paper most closely fol­ C2(q(t)) 
low that work. C(q(t)) = ... . . . Ck (q(t)) ... ,  3 Continuous Manipulation During any interval of 
time in which the discrete variables of the model stay .xed, the model is represented simply as a continuous 
function of state q(t),where q(t) . IRn is a vector of the model s n continuous variables. Over such 
an interval, the model may be subject to some constraints: that is, we may require the model to satisfy 
the conditions C1 (q(t)) = 0 C2 (q(t)) = 0 . . . (1) Cm (q(t)) = 0 where each function Ci is a scalar 
function of state, q. The evolution of this type of constrained system over time exactly parallels work 
on physical simulation of impenetrable objects [2]; accordingly, we cast the continuous manipulation 
of our models as a problem in con­strained physical simulation. Assuming that the model s initial state 
q(t0)satis.es Ci (q(t0)) = 0for 1 = i = m, the model evolves from time t0 according to the .rst-order 
differential equation q.(t) = d q(t) =M-1F(t) (2) dt where F(t) . IRn is the force acting on the model 
at timet and M is an n ×n diagonal mass matrix. By adjusting entries inM, state variables can be heavier 
or lighter (that is, harder or easier to change). Aside from user preferences, the matrix M is needed 
to properly scale the problem in the case when two or more of the state variables have vastly different 
scale/ranges from each other. The force F(t) has the form F(t) =Fa (t)+Fc (t) where Fa (t) represents 
the force applied by the user to alter the model, and Fc (t) represents a constraint force that is introduced 
so that the conditions of equation (1) can be maintained. To allow the model to be manipulated with as 
little interference as possible, we seek a constraint force Fc (t) that interferes with the user s applied 
force as little as possible. Treating our manipulation problem as a physical problem, we choose Fc (t) 
to be a workless, compressive constraint force [19]. At any particular time t,sucha constraint force 
depends on the applied force Fa (t) and the set of active constraints. A constraint Ci is said to be 
active at time t if Ci (q(t)) =0. The workless, compressive constraint forces we seek have the form .C1(q(t)) 
.Cm (q(t)) Fc (t) = f1 +···+ fm .q .q where the scalar variables fi are all nonnegative and fi is zero 
if the ith constraint is not active. Suppose at time t only k of the m constraints on the model are active; 
without loss of generality, let C1(q(t)) =C2(q(t)) =···= Ck (q(t)) = 0. Only the k active constraints 
have an effect on the and let the n ×k matrix J be de.ned by .C(q(t)) JT = . .q Using this notation, 
Fc (t) has the form Fc (t) = JT f where the vector f = ( f1, f2,..., fk ) satis.es f =0 (with 0 a k-vector 
of ze­ros). To prevent the active constraints from decreasing past zero, we require that C.(q(t)) =0. 
Using the chain rule, this yields .C(q(t))T C.(q(t)) = q.(t) =Jq.(t) =0. (3) .q Since q.(t) =M-1F(t) 
and F(t) =Fa (t)+Fc (t) =Fa (t)+JT f, we can rewrite equation (3) as JM-1JT f +JFa (t) =0. (4) For the 
constraint force to be workless, we require not only that f be nonnegative and satisfy equation (4), 
but also that the ith com­ponent of f be zero if C.i (q(t)) > 0. This last condition prevents the constraint 
force from gratuitously changing q(t) so that an active constraint becomes inactive. Baraff [2] discusses 
the implementa­tion of a fast, ef.cient algorithm for computing a vector f which sat­is.es all of these 
conditions; our system computes f using the de­scribed implementation. Once f is computed for a given 
t, the constraint force and thus the total force F(t) acting on the system is known. Standard numer­ical 
methods are then used to integrate the system q.(t) =M-1F(t) forward in time. Since constraint forces 
only prevent currently ac­tive constraints from being violated, and since numerical integration techniques 
take discrete steps from time t to time t +.t, a constraint j that is inactive at time t may occasionally 
be violated at time t +.t (i.e. Cj (q(t +.t)) may be negative). In this case, we employ stan­dard root 
.nding techniques to evolve the system forward to the time tc between t and t +.t at which Cj (q(tc )) 
becomes zero. The jth constraint then becomes an active constraint at this time, and a more accurate 
projection of q(t +.t) can be computed. The process is the same as the collision-resolution processes 
employed in physical simulation systems [1].  4 Discrete Manipulation We wish to extend the paradigm 
of continuous physically-based in­teraction to allow the state of the system to undergo discrete struc­tural 
change in response to the user s actions. Although such dis­continuous behavior is inherently non-physical 
we want to retain the spirit of continuous physical interaction, letting the user operate on the design 
simply by dragging objects subject to whatever con­straints are in force. In a continuous system, when 
the user drags an object into a wall or other constraint-imposed dead end, the object simply halts. In­stead, 
if the user continues to pull, we want the system to seek some discrete change that permits the object 
to move further in the indi­cated direction. Finding such a change is a search problem. Of the discrete 
space of alternative states, we must .nd a new state that satis.es several criteria: The new state must 
match our interpretation of the user s in­tent. In the case of simple dragging, this means that the ele­ment 
being dragged must be displaced in the direction indi­cated by the user.  The new state should be consistent 
with all problem con­straints, although it turns out we will want to brie.y portray illegal intermediate 
states as a visual cue.  The change should not surprise the user. Rather than an in­volved global rearrangement, 
we want the change to be sim­ple, local, and easily grasped. Ideally, we want the user to per­ceive some 
sort of direct mechanical connection between his or her action and the resulting change, as if, for example, 
nor­mally impenetrable rigid objects were allowed momentarily to squeeze past or pop through each other. 
 If several otherwise equivalent alternatives are found, we may wish to choose the one that rates best 
according to a problem­speci.c objective function.  We must .nd the new state quickly enough to maintain 
inter­activity, which rules out a large-scale combinatorial search.  We approach the problem by creating 
a set of domain-speci.c transformations that map states in the discrete space into other states. We use 
these to structure a local search and to de.ne a met­ric intended to capture perceptual similarity, with 
distance to a new state de.ned by the number of transformations required to reach it. In practice, this 
means that we want transformations that make small, local changes, such as swapping two adjacent elements. 
The user actions that triggered the search provide information that can be used to limit it, for instance, 
considering only those trans­formations that directly in.uence the element being dragged. In ad­dition, 
to maintain interactivity, we limit the depth of the search to at most a few levels. Subject to these 
restrictions we seek the closest state that satis.es the user s intent subject to the design constraints, 
possibly weighting the similarity of the old and new states against the absolute merit of the new one 
as de.ned by the design objective function. In the subsections that follows, we describe the method in 
detail. 4.1 Triggering a Local Search In our system, continuous manipulation is the main mode of inter­action. 
The search for a discrete change is triggered automatically when continuous manipulation hits a wall 
in the sense that no fur­ther continuous change can be made in response to the force Fa ap­plied by the 
user. Speci.cally, the discrete search is activated when the following conditions apply: there are user-applied 
forces (Fa = 0),  the applied forces do no work (FaT q. 0),  the applied forces are large enough to 
signal the direction of user action (1Fa1= k,where k is the smallest force needed to activate discrete 
manipulation), and  the con.guration is not identical to that preceding the most re­cently performed 
search.  The last condition is added to avoid thrashing after a local dis­crete search fails. If these 
conditions are met, we initiate a local search, using the object being pulled by the user and the direction 
in which it is pulled to limit the search. Although the choice of transformations is problem-speci.c, 
we generally want transformation rules that make minimal, local changes so that nearby states will appear 
globally similar to the orig­inal. Additional criteria are that the transformations should tend to yield 
nearby states that are consistent with the topological and ge­ometric constraints, and steerable in the 
sense that we can directly limit the search to states that are consistent with the user s intent. We 
limit the search to operations that move the target element in the desired direction, and also place 
an absolute limit on the depth of the search to maintain interactivity. Each candidate state is tested 
against the constraints and objective functions. An example of a set of transformation rules is described 
in the next section. 4.2 Constraints and Objectives In addition to the user s intent constraint which 
has already been built into the search, we must consider topological and geometric constraints that are 
part of the problem speci.cation. We also have two sorts of objective function: a relative one, measuring 
distance from the old state to a candidate new state, and an absolute one mea­suring the goodness of 
the overall design. We take a weighted sum of the two, choosing the best solution that satis.es the constraints. 
If no legal state is found within the depth limit of the search we re­main at the old state. In this 
last case, the user s experience is that the object being pulled can t go that away, in which case he 
or she is free to try something else. To satisfy the constraints, we consider topological constraints 
.rst, such as adjacency requirements. If a candidate state meets all the topological constraints, we 
then perform a numerical constraint solution in an effort to meet the applicable continuous geometric 
constraints, using a general purpose continuous constrained opti­mization program [9]. Since we are making 
incremental changes, the previous geometric parameter values generally give good initial estimates for 
.nding a new state that satis.es the constraints. Of the states that satisfy the topological and geometric 
con­straints, we choose the best one as measured by a compound objec­tive function. This objective function 
has discrete and continuous parts as well. We let the closeness of the new state to the old de­pend on 
continuous geometric measures of the change, as well as on the integer-valued transformation distance 
between them. We could use an additional objective function to modify the search, ei­ther a persistent 
objective function that measures the goodness of the design, or a temporary one that measures the degree 
to which the change matches what the user asked for, or both. After a new state is chosen, the numerical 
values obtained by op­timization become the initial continuous parameter values for the new state. We 
then return to the continuous manipulation phase.  4.3 Visualizing Discrete Changes Although we are 
trying to make the smallest possible discrete changes, any abrupt change to the displayed state of the 
model tends to be confusing. We have found that the addition of some simple visual cues is of great help 
to the user in understanding the discrete changes. Instead of switching directly to the new state we 
smoothly animate the transition, using highlighting and icons to help direct the viewer s attention. 
With some tuning of the appearance and timing of these visual effects we have been able to produce seamless 
transi­tions between the continuous and discrete modes of interaction, with the subjective impression 
that the discrete change, like the continu­ous one, takes place in direct mechanical response to the 
user s ac­tion.  Figure 2: Architectural room layout. (top) The user pulls on the dining room, and 
a discrete change occurs. (middle) Two intermediate frames from the resulting animated transition. (bottom) 
The new con.guration. near one point to change the order of subdivisions (i.e., from vertical .rst and 
horizontal second to horizontal .rst and ver­tical second, or vice versa). The rotate transformation 
takes a single node as its argument. It does not change the structure of the subregion tree. Rather, 
it swaps the x and y values of the minimum and maximum di­mensions assigned to the corresponding rectangle. 
This oper­ation is only applicable to leaf nodes. The search space for discrete changes is the set of 
subregion trees generated by applying sequences of these transformations to the original tree. We are 
interested in interactive manipulation rather than large-scale combinatorial optimization. To maintain 
interac­tivity we must restrict the search to a very small number of alterna­tive states. We limit the 
search in two ways. First, we consider only transformations that operate on nodes which are directly 
involved with the rectangle or edge being dragged by the user. Second, we limit the depth of the search: 
in the examples presented here we con­sider only transformation sequences of length two or less. For 
each new discrete state we consider, it is generally necessary to adjust the continuous parameters to 
satisfy the geometric con­straints if possible, and to obtain a local optimum of whatever ob­jective 
function is attached to the model. To perform the continu­ous constrained optimization we use a general 
purpose optimization package called NPSOL [9] to compute a new state vector q.The NPSOL package optimizes 
smooth nonlinear functions subject to both linear and nonlinear constraints. We have considered several 
objective functions for the new state vector q, including objective functions that seek to keep rectangular 
areas close to a desired size, functions that try to minimize the change in q from its prior value, and 
objective functions which minimize the total area of the rect­angular dissection. Like constraints, new 
types of objective func­tions are easily added by supplying code which evaluates an objec­tive function 
s value and gradient for a particular state q. 5.2 Examples Within the class of .oor planning problems, 
we show examples from four domains: architectural room layout, circuit board layout, page layout, and 
stylistic analysis of abstract painting. Architectural Room Layout A primary motivation of our research 
has been to build a system for architectural layout planning. In our architectural examples, rooms or 
functional areas are modeled as rectangles. The sides of the rect­angles are the walls surrounding a 
room. In addition to the con­straints to set bounds on the lengths and areas of each room, and the overall 
.oorspace occupied by the rooms, adjacency constraints can be de.ned between some rooms. For example, 
we might spec­ify that a kitchen must remain adjacent to the dining room, while an entranceway must always 
be adjacent to an exterior wall of the layout. Using the system, the user can pull on either a wall or 
the cen­ter of a room to change the position and size of a room. Constraints or preferences can be used 
to prevent rooms from becoming overly narrow during this manipulation. The user can also add constraints 
to .x walls in place and prevent them from moving from their cur­rent position. When the user pulls on 
an interior wall, the exterior walls of the layout remain .xed. When the user pulls on an exterior wall, 
the entire layout is scaled. During manipulation, constraints are visually indicated. For ex­ample, if 
a room s width has reached its lower bound, a bright line is drawn horizontally across the room, to indicate 
that no more hor­izontal compaction of the room is possible. Even if the user con­tinues pulling in the 
same direction, the constraint forces prevent Figure 3: Circuit board layout. the room s width from 
further decreasing. Similarly, a room whose area cannot be increased or decreased is bordered with a 
bright line to inform the user that the area of the room is currently bounded. When all progress is blocked 
by the constraints, a suf.ciently hard pull on a blocked component of the layout activates a search for 
a discrete change. After the system .nishes the search, a transition between the current state and the 
new state is shown as an anima­tion ; rooms .y from the current state to the new state (using simple 
linear interpolation). For a proposed discrete change, the objective function on the state q is the difference 
of the desired total area of the house, and the total area occupied by the house in state q. Two examples 
of architectural layout are shown; the .rst example shows a house with 14 rooms, while the second example 
has 24 rooms (21 actual rooms plus 3 ex­tra rectangular areas to produce indentations, yielding a nonrectan­gular 
exterior shape for the house). A sample layout for the second house is shown in .gure 2. Circuit Board 
Layout PC board layout is a heavily investigated application area of .oor planning. In this context, 
the goal is to arrange a set of circuit modules on a board while minimizing the total interconnecting 
wire length. The objective of this problem sounds much clearer than the architectural examples. However, 
there are many .exible parame­ters that the designer must consider. The design process involves the selection 
of a representative among many alternatives. As an example, in designing a board layout for a wearable 
computer, the primary objective is to make the computer wearable; i.e., it must be small, and comfortable 
to carry around. The designer s concern then shifts to interface issues, such as the position of switch, 
the position of a strip, and the balance of weight. We have taken a sample board layout from a design 
team working on a wearable computer. We have chosen an objective function that Figure 4: Page layout. 
seeks to minimize the total area occupied by the circuit board. The basic interactions for the board 
layout are the same as in the archi­tectural examples. We have added some icons to make it easier for 
users to recognize each module. Additionally, rotational transfor­mations are part of the allowable set 
of transformations during dis­crete search. A sample con.guration of the circuit board is shown in .gure 
3. Page Layout Another potential application area of our approach is in graphic de­sign for posters, 
covers and page layout. Figure 4 shows a sample image of typographic design, in which headlines, picture 
areas, and text areas are considered. In this example we apply aspect-ratio con­straints to headlines 
and pictures, and area as well as dimension con­straints to text blocks. Analysis of Abstract Painting 
Constraints are known to be one of the essential elements in de.ning an artistic style. In the rich combinatorial 
space of design, an artist s method of selective search through that space is thought to be a key element 
of style [23]. Our last example focuses on images from ab­stract painting. This example is taken from 
Knight [16], who has analyzed stylistic changes in the paintings by De Stijl artists. The purpose of 
this example is to show the usefulness of exploratory ma­nipulation as a tool to study styles. Knight 
studied artists paint- Figure 5: De Stijl style art. ings over periods of time. Dividing them into several 
stages, she de.ned generative grammars for each stage of each artist. Our ex­ample is based on her analysis 
on the .rst stage (1945 50) of Frits Glarner s work. Primary elements of Glarner s stage I paintings 
are rectangu­lar divisions; oblique divisions in each rectangle; and grey, black, white, and primary 
colors (red, yellow, and blue). The constraints in Glarner s work are characterized as follows: 1. The 
oblique division is angled slightly from 90 degrees. 2. The placement of the oblique is near the edge 
of a rectangle, forming a wedge shaped strip. 3. For each rectangular division, the larger wedge is 
grey or white, and the narrower wedge is grey, black, white or a pri­mary color. 4. The rectangular 
divisions appear regular. 5. Among six combinatorially possible relationships between two obliquely 
divided rectangles, only three are used.  Currently, our system is limited to the constraints listed 
above. Whatever changes a user makes, continuous or discrete, the image keeps the original style, as 
de.ned by the constraints. In future work, we would want to let the user easily impose new stylistic 
con­straints if they encounter transformations which are not consistent with the desired style. Conversely, 
if the constraints prevent us from seeing a legal instance, we may wish to modify or delete some of the 
constraints. According to the work by Knight, for example, a modi.cation to the second constraint above 
is one of the changes needed to go on to the next stage of Glarner s work. A sample piece of artwork 
in the De Stijl style generated by our system is shown in .gure 5.  5.3 Results Overall, our initial 
experiments in continuous and discrete manip­ulation have been successful. The system runs at a pleasingly 
in­teractive speed on the examples discussed. Continuous and discrete manipulations are integrated seamlessly. 
The usefulness of direct continuous manipulation was expected. We were grati.ed to .nd that continuous 
manipulation, combined with a variety of continu­ous constraints, yielded a powerful and simple style 
of manipula­tion. In particular, the complexity of interrelated constraints gener­ated continuous movements 
of the layout that were far too compli­cated for the user to have formulated on their own. The feeling 
when discrete changes are made is much the same: the user is freed from the burden of remembering and 
maintaining a large and complicated set of constraints. The work so far de.­nitely encourages us to try 
more changes in the system. The con­ditions which activate and set the direction of discrete search work 
well. The set of transformations we implemented seemed trivial at .rst, but we found that they were powerful 
enough to generate many alternate solutions. Although the system makes structural changes relatively 
quickly, the evaluation of each possible alterna­tive slows down as we work with more complex objective 
functions. The visualization of discrete changes using an interpolating anima­tion is very effective 
in helping the user to understand what discrete changes have occurred. Without this visualization, users 
hardly rec­ognize when and where changes have occurred.   6 Conclusion In this paper, we have introduced 
a new technique for interaction with discrete/continuous geometric models, allowing users to ex­plore 
problem spaces having both continuous and discrete param­eters. The user can continuously manipulate 
an object as long as the given constraints are satis.ed. When a point is reached at which no further 
continuous movement in the desired direction is possi­ble, a slightly stronger pull on the mouse triggers 
a discrete change. This change is based on a rapid, behind-the-scenes local search, con­strained by the 
user s pulling action. A small number of transforma­tion rules are de.ned to perform actual changes. 
Alternatives are compared in terms of goodness of the overall design and the mag­nitude of the change 
from the previous state, subject to continuous and discrete constraints. The best among the alternatives 
becomes the new state. Our approach is most useful for discrete/continuous exploration problems which 
are not governed by a cut-and-dried objective func­tion, but involve aesthetic or other subjective judgements 
as well, and that therefore cannot be solved by conventional combinatorial optimization methods. We have 
implemented a system that demonstrates seamlessly in­tegrated continuous and discrete manipulation. We 
have applied the technique to planar layout problems in architecture, PC board lay­out, page layout, 
and analysis of art. Our plans for additional work focus on ways to improve perfor­mance, and on applications 
to a broader class of models and prob­lem domains. From a performance standpoint, the discrete evalua­tion 
phase is the largest bottleneck, even though the discrete search is local and limited. We currently call 
an external nonlinear con­strained optimization package at each discrete evaluation; it may be that we 
can enhance performance by writing custom numerics code, or by using heuristics to further prune the 
search. Our current sys­tem is limited to axis-aligned rectangles; we have been able to han­dle somewhat 
more general shapes, such as L-shapes, as constrained sets of rectangles. We intend to move to much more 
general shapes and shape grammars. We are currently investigating scheduling problems as a an addi­tional 
application area. For example, in a resource allocation prob­lem, resources are usually limited in quantity, 
and can be discrete, such as humans and machines. The activities in a project have con­straints on sequential 
order (discrete) and completion time (con­tinuous). We believe that our methods will carry over directly 
to scheduling projects such as those that arise in construction project management. Another direction 
for future work is the extension of our system to 3D models. A .rst step is to handle .oor plans for 
multi-story houses, which can be treated as a set of 2-D plans which are linked by constraints. For instance, 
locations of stairways and structural walls must be consistent. Acknowledgements This research was supported 
in part by a Science and Technol­ogy Center Grant (#BIR-8920118) and a Research Initiation Award (#CCR-9308353) 
from the National Science Foundation, by the En­gineering Design Research Center, an NSF Engineering 
Research Center at Carnegie Mellon University, by the Phillips Laboratory, Air Force Material Command, 
USAF, under cooperative agreement number F29601-93-2-0001, by Apple Computer, Inc, and by an equipment 
grant from Silicon Graphics, Inc. References [1] David Baraff. Analytical methods for dynamic simulation 
of non-penetrating rigid bodies. Computer Graphics (Proc. SIG-GRAPH), 23:223 232, 1989. [2] David Baraff. 
Fast contact force computation for nonpene­trating rigid bodies. Computer Graphics (Proc. SIGGRAPH), 
28:23 34, 1994. [3] David Baraff. Interactive simulation of solid rigid bodies. IEEE Computer Graphics 
and Applications, 15:63 75, 1995. [4] Christopher Carlson. Structure grammars and their application to 
design. Technical Report EDRC 01 09 89, Engineering Design Research Center, Carnegie Mellon University, 
Pitts­burgh, PA, 1989. [5] George Celniker and Dave Gossard. Deformable curve and surface .nite-elements 
for free-form shape design. Computer Graphics, 25(4), July 1991. Proceedings SIGGRAPH 91. [6] Ulrich 
Flemming. Wall representations of rectangular dissec­tions and their use in automated space allocation. 
Environment and Planning B: Planning and Design, 5:215 232, 1978. [7] Ulrich Flemming. More than the 
sum of parts: the grammar of Queen Anne houses. Environment and Planning B: Planning and Design, 14:323 
350, 1987. [8] Ulrich Flemming and Robert F. Coyne. A design system with multiple abstraction capabilities. 
In Avignon 90: Tools, Tech­niques &#38; Applications, volume 1, pages 107 122. EC2, 1990. [9] Philip 
E. Gill, Walter Murray, Michael A. Saunders, and Mar­garet H. Wright. User s guide for NPSOL (version 
4.0): A fortran package for nonlinear programming. Technical Report SOL 86-2, Stanford University, Stanford, 
California, 1986. [10] Michael Gleicher and Andrew Witkin. Through-the-lens cam­era control. Computer 
Graphics, 26, 1992. Proc. Siggraph 92. [11] Michael Gleicher and Andrew Witkin. Drawing with con­straints. 
The Visual Computer, 1994. [12] John Grason. Methods for the computer-implemented solution of a class 
of .oor plan . PhD thesis, Department of Electrical Engineering, Carnegie Mellon University, 1970. [13] 
Jeff Heisserman. Generative geometric design and bound­ary solid grammars. PhD thesis, Department of 
Architecture, Carnegie Mellon University, 1991. [14] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. 
Snakes: Active contour models. Int. J. of Computer Vision, 1(4), 1987. [15] Terry W. Knight. Designing 
with grammars. In Gerhard N. Schmitt, editor, CAAD Futures 91 Proceedings, pages 19 34, 1991. [16] T.W. 
Knight. Transformations of De Stijl art: the paintings of Georges Vantongerloo and Fritz Glarner. Environment 
and Planning B: Planning and Design, 16:51 98, 1989. [17] H. Koning and J. Eizenberg. The language of 
the prairie: Frank Lloyd Wright s prairie houses. Environment and Plan­ning B: Planning and Design, 8:295 
323, 1981. [18] Sukhamay Kundu. The equivalence of the subregion represen­tation and the wall representation 
for a certain class of rectan­gular dissections. Communications of the ACM, 31(6):752 763, 1988. [19] 
Cornelius Lanczos. The Variational Principles of Mechanics. Dover Publications, Inc., 1970. [20] David 
P. LaPotin. Mason: A global .oor-planning approach for VLSI design. Technical Report IBMC 11657, IBM 
T.J. Watson Research Center, Yorktown Heights, NY 10598, Jan­uary 1986. [21] William J. Mitchell, Robin 
S. Ligget, Spiro N. Pollalis, and Milton Tan. Integrating shape grammars and design analysis. In CAAD 
futures 91 proceedings, pages 1 18, 1991. [22] W.J. Mitchell, J.P. Steadman, and Robin S. Ligget. Synthe­sis 
and optimization of small rectangular .oor plans. Envi­ronment and Planning B: Planning and Design, 3(1):37 
70, 1976. [23] Herbert A. Simon. Style in design. In Charles M. Eastman, editor, Spatial Synthesis in 
Computer-Aided Building Design, chapter 9, pages 287 309. Applied Science Publishers LTD, Ripple Road, 
Barking, Essex, England, 1975. [24] G. Stiny. Introduction to shape and shape grammars. Envi­ronment 
and Planning B: Planning and Design, 7(3):343 351, 1980. [25] G. Stiny and J. Gips. Shape grammars and 
the generative spec­i.cation of painting and sculpture. Information Processing, pages 1460 1465, 1972. 
[26] G. Stiny and W.J. Mitchell. The Palladian grammar. Environ­ment and Planning B: Planning and Design, 
5(1):5 18, 1978. [27] Mark C. Surles. An algorithm with linear complexity for in­teractive, physically-based 
modeling of large proteins. In Ed­win E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), 
volume 26, pages 221 230, July 1992. [28] Demetri Terzopoulos, Andrew Witkin, and Michael Kass. En­ergy 
constraints on deformable models: recovering shape and non-rigid motion. In Proc. AAAI-87, Seattle, 1987. 
[29] Demetri Terzopoulos, Andrew Witkin, and Michael Kass. Symmetry-seeking models for 3D object reconstruction. 
In­ternational Journal of Computer Vision, 3(1), 1987. [30] Jeffrey A. Thingvold and Elaine Cohen. Physical 
modeling with B-spline surfaces for interactive design and animation. Computer Graphics, 24(2):129 138, 
March 1990. [31] William Welch and Andrew Witkin. Variational surface mod­eling. Computer Graphics, 26, 
1992. Proc. Siggraph 92. [32] Andrew Witkin, Michael Gleicher, and William Welch. In­teractive dynamics. 
Computer Graphics, 24(2):11 21, March 1990. Proc. 1990 Symposium on 3-D Interactive Graphics. [33] Andrew 
Witkin and William Welch. Fast animation and con­trol of non-rigid structures. Computer Graphics, 24(4):243 
252, July 1990. Proc. Siggraph 90. [34] D.F. Wong, H.W. Leong, and C.L. Liu. Simulated Annealing for 
VLSI Design. Kluwer Academic Publishers, 101 Philip Drive, Assinippi Park, Norwell, Massachusetts 02061, 
1988. [35] Lin S. Woo, C.K. Wong, and D.T. Tang. Pioneer: a macro­based .oor-planning design system. 
VLSI System Design, pages 32 43, August 1986.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218444</article_id>
		<sort_key>209</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[An interactive tool for placing curved surfaces without interpenetration]]></title>
		<page_from>209</page_from>
		<page_to>218</page_to>
		<doi_number>10.1145/218380.218444</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218444</url>
		<keywords>
			<kw><![CDATA[collision detection]]></kw>
			<kw><![CDATA[contact point]]></kw>
			<kw><![CDATA[object placement/assembly]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39079343</person_id>
				<author_profile_id><![CDATA[81100167784]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Snyder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation, 1 Microsoft Way, Redmond WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97881</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Curved Surfaces and Coherence for Non-penetrating Rigid Body Simulation," Computer Graphics, 24(4), pp. 19-28, August 1990.]]></ref_text>
				<ref_id>BARA90</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122722</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Coping with Friction for Non-penetrating Rigid Body Simulation," Computer Graphics, 25(4), pp. 31-39, July 1991.]]></ref_text>
				<ref_id>BARA91</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baraff, David, "Fast Contact Force Computation for Nonpenetrating Rigid Bodies," Computer Graphics, 28(2), pp. 23-42, July 1994.]]></ref_text>
				<ref_id>BARA94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Barzel, Ronen, and A. Barr, "A Modeling System Based On Dynamic Constraints," Computer Graphics, 22(4), pp. 179-188, August 1988.]]></ref_text>
				<ref_id>BARZ88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>176968</ref_obj_id>
				<ref_obj_pid>176962</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Garcia-Alonso, Alejandro, N. Serrano, and J. Flaquer, "Solving the Collision Detection Problem," IEEE Computer Graphics and Applications, pp. 36-43, May 1994.]]></ref_text>
				<ref_id>GARC94</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15916</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kay, Tim, and K. Kajiya, "Ray Tracing Complex Scenes," Computer Graphics, 20(4), pp. 269-278, August 1986.]]></ref_text>
				<ref_id>KAY86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gleicher, Michael, and A. Witkin, "Through-the-Lens Camera Control," Computer Graphics, 26(2), pp. 331-340, July 1992.]]></ref_text>
				<ref_id>GLEI92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134085</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Metaxas, Dimitri, and D. Terzopoulos, "Dynamic Deformation of Solid Primitives with Constraints," Computer Graphics, 26(2), pp. 309-312, July 1992.]]></ref_text>
				<ref_id>META92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378528</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Moore, M. and Wilhelms, J., "Collision Detection and Response for Computer Animation," Computer Graphics, 22(4), pp. 289-298, August 1988.]]></ref_text>
				<ref_id>MOOR88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Press, W. H., B. R Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes, Cambridge University Press, Cambridge, England, 1986.]]></ref_text>
				<ref_id>PRES86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122745</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Sclaroff, Stan, and A. Pentland, "Generalized Implicit Functions for Computer Graphics," Computer Graphics, 25(4), pp. 247-250, July 1991.]]></ref_text>
				<ref_id>SCLA91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134094</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, and J. Kajiya, "Generative Modeling: A Symbolic System for Geometric Modeling," Computer Graphics, 26(2), pp. 369-378, July 1992.]]></ref_text>
				<ref_id>SNYD92</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Snyder, John, A. Woodbury, K. Fleischer, B. Currin, and A. Barr, "Interval Methods for Multi-point Collisions between Time-Dependent Curved Surfaces", Computer Graphics, 27(2), pp. 321-334, Aug. 1993.]]></ref_text>
				<ref_id>SNYD93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37429</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Witkin, Andrew, K. Fleischer, and A. Barr, "Energy Constraints on Parameterized Models," Computer Graphics, 21 (4), pp. 225-232, July 1987.]]></ref_text>
				<ref_id>WITK87</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Interactive Tool for Placing Curved Surfaces without Interpenetration John M. Snydery Microsoft Corporation 
 Abstract We present a surface representation and a set of algorithms that allow in­ teractive placement 
of curved parametric objects without interpenetration. Using these algorithms, a modeler can place an 
object within or on top of other objects, .nd a stable placement for it, and slide it into new stable 
place­ ments. Novel algorithms are presented to track points of contact between bodies, detect new points 
of contact, and delete vanishing contacts. Inter­ active speeds are maintained even when the moving body 
touches several bodies at many contact points. We describe a new algorithm that quickly brings a body 
into a stable con.guration with respect to a set of external forces, subject to the constraint that it 
not penetrate a set of .xed bodies. This algorithm is made possible by sacri.cing the requirement that 
a body behave physically over time. Intuitive control is still achieved by making incremental, "pseudo-physical" 
changes to the body s placement, while enforcing the non-interpenetration constraint after each change. 
CR Categories and Subject Descriptors: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling. 
Key Words: object placement/assembly, collision detection, contact point. 1 Introduction Geometric modeling 
can be dissected into two tasks: creating models for a set of parts or basic objects, and then assembling 
these parts into virtual combinations. For example, to model an automobile engine, one can .rst create 
a set of parts including the engine block, pistons, nuts, and bolts, and then assemble these into an 
engine. While the vast majority of work in geometric modeling has focused on the part-creation task, 
part-assembly is nonetheless an important and dif.cult problem. Parts can often be modeled independently, 
perhaps by different people; assembling them means having to deal explicitly with relationships between 
all the parts together, such as the constraint that no solid object penetrate another. Increasingly in 
research systems, a part is modeled not as a static geometric object, but as a high­ level parameterized 
model such as an elastic sheet with given initial geometry and material properties [WITK87,META92]. This 
increases the complexity of the part-assembly task, which must compute model parameters from spatial 
relationships between the various parts (for example, the shape of the tablecloth after it is dropped 
on the table). This paper focuses on a subset of the part-assembly problem: interactive placement of 
rigid, solid, curved objects into physically plausible con.gu­ rations (Figure 1). Parts in this context 
are rigid bodies parameterized by a rigid motion in 3D (i.e., a rotation and translation). The goal is 
to arrive at a statically balanced con.guration of solids near a user-speci.ed initial con.guration. 
Examples include placing a spoon in a cereal bowl or .lling it with cereal, placing a phone receiver 
on its hook, putting ice cubes in a y1 Microsoft Way, Redmond WA 98052. johnsny@microsoft.com Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Figure 1: The 
Placement Problem The algorithms described here allow, for example, repositioning of a bowl and spoon 
in an arbitrary con.guration (left) to produce a con.guration with the spoon resting stably in the bowl 
and the bowl resting stably on the .oor (right). Points of contact computed by the algorithm are shown 
in yellow; the spoon is rendered transparently to make these visible. glass, modeling a pile of fruit, 
.tting oddly shaped objects into a rigid box, etc.. Many applications exist for a tool that performs 
such interactive place­ment: modeling of static virtual environments, interactive .tting/packing of 3D 
shapes, games (e.g. virtual 3D puzzles), and keyframe animation for physically plausible but still controllable 
motions. Even this restricted problem is dif.cult without special tools. Tradition­ally, an object is 
placed by directly tying a graphics input device such as a mouse, trackball, or set of dials to its orientation 
and translation parameters. Users verify by eye that objects do not interpenetrate and are physically 
balanced. Because curved objects resting on each other can have many points of contact, modeling a con.guration 
like the one in Figure 1 can take many minutes of tedious interaction: one portion of the spoon is satisfacto­rily 
placed only after some other part begins interpenetrating or pulls away. Traditional methods also provide 
no help in reaching a stable con.guration. To assist the user in the placement task, one option is to 
take advantage of the substantial work done in computer animation to make objects move physically. These 
techniques can be used for the placement problem by simulating through time until stability is achieved; 
that is, the objects stop moving. Unfortunately, none of these techniques is suitable for interactive 
placement of curved, non-convex, parametric shapes; the kind of shapes that many commercial modelers 
can produce. Most approaches restrict the kinds of shapes that can be simulated [MOOR88,BARA90,SCLA91, 
see SNYD93 for a survey of these restrictions]; another approach that handles general parametric surfaces 
is too slow for interactive applications [SNYD93]. An additional disadvantage is that physical behavior 
over time can actually hinder the placement problem. A physical card house can fall .at when we place 
the next card; a spoon dropped into a bowl accelerates, bounces, and can end up far from where we wanted 
it. However, the placement problem is concerned with quick production of a desired con.guration of objects, 
not with how they arrived there. We seek an approach that arrives at a stable state from some arbitrary 
state, making changes that are intuitive and controllable, but not necessarily physically accurate. To 
make the approach fast, we deviate from accurate physical simulation in several ways: 1. Bodies have 
no velocity. In fact there is no explicit notion of time in the system at all. Instead, a body is moved 
through a series of discrete states, each of which obeys the non-interpenetration constraint. 2. While 
forces and torques are solved for at each state, they aren t the real forces that a dynamic body would 
experience instantaneously. They are chosen to speed convergence of the body to a nearby stable placement, 
and are called pseudo-forces and pseudo-torques.  3. The exact time when contact point transitions occur 
(e.g., vanishing contacts) is not calculated. Instead, transitions are made after each discrete step. 
 4. Small, non-physical adjustments are made to an active body s position and orientation in order to 
preserve the non-interpenetration constraint. 5. Only one body at a time, the active body, can move. 
As it moves, it interacts with other bodies which remain .xed and are treated as in.nitely massive. The 
user can change which body is currently active in order to move bodies sequentially into position.  
As is implied in some of the above choices, the approach presented here is a limited solution to the 
general placement problem of rigid bodies a very dif.cult problem for curved surfaces. Although the 
sequence of intermediate states does not correspond to a real motion of the object, the .nal state does 
represent a stable con.guration according to static force balance laws, assuming immovable non-active 
bodies. The algorithm thus handles the common case of placing a single object on or within another that 
is essentially .xed, like a teapot on a table or a spoon in a bowl. Force balance is only maintained 
for the active body; the force it imparts to the rest of the world is ignored. The algorithm therefore 
can not automatically produce physically balanced heaps of multiple bodies. However, visually plausible 
heaps can still be modeled from the ground up . A sequence of bodies is added to the accumulated heap; 
after a body reaches a stable state, it remains in that position as additional bodies are added (this 
technique was used to make Figures 12, 14, 15, and 16). Automatic maintenance of the non-interpenetration 
constraint and the ability to place the next object stably at least provide useful tools in such a modeling 
task. The approach trades off robustness in order to achieve interactivity, as compared to an approach 
like [SNYD93]. Problems can happen when an object contacts another at a curve or surface, where one object 
is concave in this contact region (e.g., a torus on a cone).1 Such con.gurations can still be produced, 
but sometimes require user intervention (see Section 6 for further discussion). As in [BARA94], our algorithms 
make use of heuristics for which a formal justi.cation is lacking, but which seem to work in practice. 
The main example involves converting collisions on polygonal approximations to collisions on analytic 
surfaces using numerical iteration. We have not found selection criteria for the approximation that provide 
a theoretical guarantee for convergence of this iteration. In practice though, heuristic selection of 
the approximation gives a workable solution. The placement algorithm described in this paper shares characteristics 
with optimization methods used previously [WITK87,BARZ88,GLEI92], but specializes these methods to the 
problem of interactive placement. The main dif.culty with direct application of such methods involves 
handling discontinuities in the energy functional that arise from collisions. This paper explains how 
to slide downhill ef.ciently in the presence of collisions with­out resorting to a full-blown and very 
slow physical simulation. Essentially, the main contribution of this paper is a description of which 
physical char­acteristics to sacri.ce in order to get good performance, and which to keep in order to 
get intuitive control. Several speci.c elements of the approach are new: we use a new surface representation 
that employs both polygonal and parametric representations, a new algorithm for fast tracking, creation, 
and deletion of contact points, and a new method that quickly converges to a stable, non-interpenetrating 
placement for a single body starting from some arbitrary non-interpenetrating state and subject to a 
set of external forces.  2 Summary An object is placed near its desired position such that it does not 
touch any other objects. This is called a noninterfering state. Interactive collision detection, described 
in Section 3, makes it easy for the user to select a noninterfering initial state. In the simplest mode 
of interaction, the user lets the body stabilize under the in.uence of a gravitational pseudo-force. 
The body is automatically moved through a series of discrete states, each of which obeys the non-interpenetration 
constraint, until a stable state is reached. An independent pseudo-force calculation is made at each 
state transition: external pseudo-forces are computed, components balanced by contact pseudo-forces are 
subtracted away, and the residual is applied to produce an incremental change in the body s position. 
Contact points are 1Note that the problem arises only when the object is concave at the contact region. 
Concave objects that contact at isolated points or are concave only away from the region of contact are 
handled without dif.culty. place body in a noninterfering initial state loop check for tracked point 
transitions compute pseudo-forces at each contact point, and on body update active body s placement using 
(solution step) adjust tracked points for new placement if unable, + /2 and start loop again (loop failure) 
while new polygonal collision exists and collisions < Nsimultaneous if unable to convert collision to 
a contact point then + /2 and start loop again (loop failure) else add tracked point endif endwhile 
if collisions were successfully added, restore old value of if objective function increases, + /2 (loop 
failure) if loop was completed successfully, increase by +max(. , max) render current state complete 
user interaction requests: interrupt, undo, direct repositioning, force or parameter change, etc. until 
stable Figure 2: Automatic Placement Algorithm: This algorithm produces a series of place­ments of the 
body that never violate the non-interpenetration condition and converge to a stable state a state in 
which contact forces balance user-speci.ed external forces such as gravity. relocated where the body 
now touches other bodies; the body s position and orientation may also be slightly adjusted to preserve 
all contacts. The relevant algorithms are described fully in Sections 4 and 5. The result of the algorithm 
is the .nal position of the active body and a list of contact points. The complete algorithm is outlined 
in Figure 2. The sequence of state transitions generated by the automatic placement algorithm can be 
interrupted at any point. The user can undo a portion of the sequence and change the forces acting upon 
the body in order to reach different resting states. In addition to turning gravity on or off, the user 
can apply external pseudo-forces to nudge the object in any direction or make it attach to speci.ed points 
on other objects. Alternatively, the body can be manually positioned relative to its current state,2 
and the automatic placement algorithm resumed from the new position. As in the case of selecting an initial 
state for the body, the user must choose a noninterfering state from which to resume automatic placement. 
2.1 Surface Representation A body is a rigidly movable solid object that interacts with other bodies. 
It is represented as a set of patches, each of which is an analytic parametric surface, whose union contains 
the boundary of the body. Each patch is de.ned so that its surface normal points toward the exterior 
of the body.3 In choosing a representation for patches, we were faced with the problem that polygonal 
collision detection is extremely ef.cient, but can t be used alone to .nd accurate collisions between 
smooth surfaces. We therefore use a hybrid representation: a polygonal approximation is used to detect 
new points of contact that arise between bodies, and a functional description is used to adjust these 
points so that they lie on the actual curved surface, and to track them as they move. The polygonal description 
is a mesh of triangles with (u,v) parametric coordinates per vertex. Each vertex lies on the analytic 
surface. The lo­cation of a collision can then be approximated by .nding two intersecting triangles and, 
for each triangle, barycentrically interpolating the parametric coordinates at each vertex to obtain 
the (u,v) coordinate at the point of in­tersection. The two points are used as starting points in an 
iterative method (multidimensional Newton-Raphson) which produces contact points on the actual surfaces. 
The polygonal approximation is also used during manual positioning to determine whether a body is in 
a noninterfering state. When interference is detected, the body is made transparent and a small dot dis­played 
at a point of intersection. The calculations take place at interactive 2In the prototype system, the 
active body is moved by turning a series of 6 dials which represent translation and rotation around the 
coordinate axes. 3Although the system does not currently support open bodies, the changes required are 
straight­forward. Contact points must record whether they are on surfaces, edges, or vertices. The systems 
of equations used in tracking must then make use of this information. Alternatively, surface boundaries 
can be handled by placing thin tubes around edges and spheres around vertices. Figure 3: Collision Inconsistencies 
for Analytic Surfaces vs. Polygonal Approxima­tions: (a) shows a collision between polygons but not surfaces, 
(b) shows a collision between surfaces but not polygons. rates as the user moves the body. Finally, the 
polygonal approximation is used to select a body or a point on a body, using a ray casting algorithm 
as in [KAY86]. Functional descriptions for parametric surfaces are created with a sym­bolic language, 
as described in [SNYD92]. It is also possible to develop special purpose code for classes of parametric 
surfaces such as bicubic patches. Evaluation of surface points and derivatives up to second order is 
required. Handling Inconsistencies Since analytic surfaces are approximated by triangles for collision 
detec­tion, two sorts of errors can occur as shown in Figure 3. When one of the objects is concave, a 
collision can happen between polygonal approxima­tions but not the analytic surfaces (Figure 3a). This 
is detected by a failure in the numerical iteration to move the contact point onto the analytic sur­faces 
(see Section 4.3) and ignored.4 A collision between analytic surfaces can also be missed with the polygonal 
approximations (Figure 3b). There are three cases for what then happens: the surfaces can continue to 
inter­penetrate until the polygonal approximations eventually collide, they can tunnel through each other, 
or they can reach a stable state in this situation. The .rst case is the typical one and causes no problem 
since the collision is eventually found. Users can ignore tunneling since the objects no longer vi­olate 
the non-interpenetration condition, or undo tunneled placement states and restart the algorithm with 
a smaller £(solution step) parameter, making it more likely a polygonal collision will be detected. The 
third case reduces the accuracy of the .nal placement: the objects violate the non-interpenetration condition 
by a distance bounded by the approximation errors of the two meshes. A conservative test for noninterpenetration 
can be developed using polygonal approximations for offset surfaces where the mesh is known to enclose 
the analytic surface. Why not just use polygons? There are three advantages of employing an analytic 
description of curved surfaces. First, much better accuracy can be achieved. The location of a surface 
is computed where it rests on points mathematically on the surface rather than on faces or edges of a 
faceted approximation. This can be important for CAD applications, or cases in which the camera is close 
to the resulting model. More accuracy is also achieved in reaching a stable state; meta-stable con.gurations 
that rest on facets are avoided. Second, using the smooth surface makes it easier to incrementally change 
a body s state while enforcing the non-interpenetration condition. When one curved body can continuously 
slide over the surface of another, many states are unreachable when a polygonal approximation is used. 
Consider a cylinder approximated as an extruded regular polygon of n sides resting lengthwise on a .at 
plane. Only n discrete rotations of this cylinder around its axis are stable (where it rests on one of 
the extruded edges of the regular polygon); the real cylinder is stable for any such rotation. Third, 
faster convergence to stability can be achieved. The analytic surfaces provide the exact normal vector 
at points of contact unavailable with a polygonal approximation. Using these normals in a force balance 
computation, we can move the surface a signi.cant amount between steps. The size of the step used with 
a purely polygonal approximation would necessarily be tied to the size of polygonal facets in the neighborhood 
of 4As will be discussed further in Section 4.3, such a collision is not really ignored, but is initialized 
as an extremal point: a point where surfaces are close but not in contact. Extremal points are tracked 
along with contact points. An extremal point is deleted if the separation distance increases, or is converted 
to a contact point if the distance becomes :0. Figure 4: Spheres of neglect: A sphere is placed around 
current contact points within which collisions are disregarded, so that only new contact points are found. 
the contact: an accurate approximation with many polygons would require a large number of steps.  3 
Detecting Collisions Quickly Our method for detecting collisions between triangulated bodies is similar 
to that presented in [GARC94], with several differences that tailor the algorithm to the interactive 
placement problem. See that reference for a detailed description of the collision detection problem. 
We approximate each body with a set of triangles organized in an object­partitioning bounding box hierarchy 
as in [KAY86]. At the terminal nodes of the hierarchy are the triangles, each of which is marked with 
a (body,patch) identi.er. Each node of the tree contains a bounding box in the form (xmin,xmax,ymin,ymax,zmin,zmax), 
and a list of child nodes, or information for a triangle in the case of a terminal node. Nodes can also 
contain infor­mation specifying a rigid motion for the subtree. In this case, the node s bounding box 
is in the post-transformed space; each of its child nodes store bounding boxes in local (pre-transformed) 
space. Bounding boxes are dy­namically transformed during traversal of the hierarchy (see Section 3.2). 
A similar hierarchy is then constructed for the set of bodies in the system (the world). The world is 
maintained by incrementally adjusting the active body s position in it; this is easier in an object partitioning 
hierarchy than in a spatial hierarchy. When a body moves, a transformation at a single node must be changed 
and bounding box changes propagated up the tree; the object s hierarchy remains unchanged.5 Because a 
body s hierarchy is constructed just once, we can afford to use substantial computation organiz­ing its 
triangles in a hierarchy that is ef.cient for collision detection. This processing is done before interaction 
begins (see Section 3.1). Two forms of collision detection are required for interactive placement. Simple 
collision detection computes whether an object interferes with any other objects, and is used in choosing 
a noninterfering state from which to begin or resume convergence to stability. A second form of collision 
detection is used to compute whether a body already in contact with other bodies intersects at any additional 
points. To do this, we de.ne spheres of neglect around the current contact points, as in [SNYD93]. During 
traversal, potential collisions within the spheres of neglect are discarded; the algorithm thus locates 
new collision points (Figure 4). The radius for each sphere is a small value determined by the distance 
the contact point moves from polygonal approximation to analytic surface during the contact creation 
process (Section 4.3). This is done because contact points are tracked on analytic surfaces but are used 
to discard polygonal collisions. Note that spheres of neglect are used only to ignore polygonal collisions; 
they do not effect contact point tracking or force calculations. 3.1 Constructing the Bounding Box Hierarchy 
To construct a bounding box hierarchy for a body, we apply the recursive algorithm maketree to the .at 
list of triangles comprising it: maketree(L) partition L into set of n lists Li create root node R for 
each i, insert maketree(Li ) as child of R return R 5We note that lazy evaluation of bounding box changes 
improves ef.ciency in the general case. When a transformation changes, the appropriate node s dirty .ag 
is set and propagated up the tree until the root or a node previously marked dirty is reached. At collision 
time, a dirty node s transformation is updated using a callback function, and its bounding box updated 
as the union of its children (which must be recursively updated if marked dirty). This sort of lazy evaluation 
avoids needless union-of­bounding-box computations as child nodes are sequentially marked dirty. A node 
s bounding box and transformation is updated exactly once, no matter how many of its children have changed. 
Of course, these subtleties are unimportant when only a single body is moved. a Figure 5: Object partitioning 
by gap .nding: The bounding boxes around four objects (A,B,C,D) haveagap, a, when projected onto the 
horizontal axis. Partitioning around this gap yields the two lists (A,B)and (C,D). Note that the objects 
have no gap when projected onto the vertical axis. At each invocation of maketree, the bounding box for 
the result node R is taken as the union of the bounding boxes of its child nodes. The heart of the construction 
process is the partitioning of a list of nodes L into sublists Li. Grouping nodes that are close together 
greatly improves culling during collision detection. We have used the following heuristic: the bounding 
boxes of each list element are projected into each of the three coordinate axes to form three lists of 
intervals. These lists are sorted in increasing order of interval lower bound, and then are searched 
for gaps (see Figure 5). We partition the list into two by using the coordinate axis that generates the 
gap of greatest width. All elements whose projected interval is less than the gap center are placed in 
one list, the rest in the other. If no gaps exist, which happens fairly frequently, we partition around 
the center of the interval in which projected bounding box centers have the greatest variance. The projection 
axis is chosen as the coordinate axis for which the standard deviation of interval centers is greatest. 
This kind of partitioning gives us a branching ratio n = 2 in the resulting hierarchy; other branching 
ratios are possible by partitioning along multiple axes. We have not tried other branching ratios. 3.2 
Traversing the Bounding Box Hierarchy Collision detection is computed by traversing the bounding box 
hierarchy of a pair of nodes to be collided, NA and NB. In our system, the active body is collided against 
the world, although the algorithm described here can compute collisions between any two collections of 
bodies or within a single collection. Pairs of nodes from the two hierarchies are examined in depth-.rst 
order according to the following basic algorithm: traversetrees(NA ,NB) initialize stack of active pairs 
with (NA,NB) while stack is nonempty pop off next pair (A,B) loop through child nodes of A and B: (Ai,Bj) 
if (Ai,Bj) collide if both are triangles, record collision else push pair onto active list Associated 
with each pair of nodes on the active stack is a transformation which transforms the second element of 
the pair into the coordinate system of the .rst. This transformation is updated whenever child nodes 
are inserted that contain transformations. Relatively few of these nodes are encountered during traversal 
since bodies typically contain hundreds or thousands of triangles, all of which are moved by changing 
one node s transformation. Three types of collision computations occur: bounding box vs. bounding box, 
bounding box vs. triangle, and triangle vs. triangle. If both nodes are nonterminal, a bounding box vs. 
bounding box collision is done by transforming the bounding box of the second object to the coordinate 
system of the .rst, and testing whether the two bounds overlap. If only one is nonterminal, a bounding 
box/triangle collision test is done, again in the coordinate system of the .rst object. Otherwise, a 
test for the intersection of two triangles is done. When two triangles collide, the location of the collision 
and pointers to the two triangles are recorded.6 Several changes to the basic algorithm are needed. The 
.rst is to cull nodes based on a list of spheres of neglect. To do this, we test the intersection of 
the 6The algorithm currently records an arbitrary point of intersection between the two triangles. AA 
B B  contact pair extremal pair Figure 6: Types of interface points: Two non-interpenetrating bodies 
in close prox­imity, A and B, can have two types of interface points. On the left, the bodies are in 
tangent contact at a contact pair. On the right, the bodies are slightly apart; the pair of points at 
minimal distance is the extremal pair. bounding boxes of A and B to see whether it is entirely inside 
some sphere of neglect. If so, we discard the pair. We also discard a collision between triangles if 
it lies within any sphere of neglect. The second change improves performance: when one node s bounding 
box is small with respect to the other, we do not subdivide it into its children, but compare it unsubdivided 
with the children of the bigger node. In our experiments, this sped up the algorithm up by factors of 
2-10, the larger number attained when a very small object is collided against a much larger one.  4 
Computing Points of Contact As in [SNYD93], places where bodies interact are handled using a .nite set 
of points even when the region of contact forms a curve or surface. In [SNYD93], points were uniformly 
distributed over the contact region; the approach here creates only enough points to prevent interpenetration 
and reach a stable state. These points are called interface points and consist of a pair of points on 
two bodies. Interface points are tracked (incrementally updated) as the body moves from state to state 
using numerical iteration. During collision detection, a sphere of neglect is placed around each interface 
point to avoid detecting collisions already being handled. We therefore store, for each interface point, 
a (u,v) parametric coordinate pair, a pair of (body,patch) identi.ers for the surfaces in contact, and 
a radius for the sphere of neglect. Two types of interface points are used: contact pairs and extremal 
pairs, shown in Figure 6. Bodies actually touch at contact pairs. Forces that balance external forces 
such as gravity are applied at the contact pairs. These forces are only applied to push objects away 
from interpenetrating, not to glue objects together. When such a gluing force is obtained, it is not 
applied but instead causes the contact pair to transition to an extremal pair. Extremal pairs are points 
on a pair of bodies at minimal distance. The bodies do not contact in a neighborhood around the extremal 
pair. Forces are not applied at extremal pairs. Extremal pairs allow interface points to vanish gradually. 
When a gluing contact force is detected, the contact pair is con­verted to an extremal pair, allowing 
the two bodies to separate. Contact can quickly be resumed if the separation distance becomes negative. 
Extremal pairs are deleted when the separation distance becomes large enough. 4.1 Tracking Contacts After 
the active body is moved, the automatic placement algorithm must track the interface points from their 
previous positions. The following two-phase tracking algorithm yields excellent numerical stability. 
The .rst phase, called the conditioning phase, is not necessary from a theoretical perspective, but makes 
the resulting system easier to solve in the next phase.7 In this phase, each interface point, represented 
as a pair of (u,v) parametric locations, is independently adjusted to satisfy the extremal point conditions 
using numerical iteration (the equations are described in Section 4.4). The initial condition for this 
iteration is the parametric location of the interface point before the body was moved. In the second 
phase, called the contact adjustment phase, we adjust both the body s rigid motion parameters and the 
locations of all the interface pairs to preserve the contact or extremal conditions. Numerical iteration 
takes place to simultaneously satisfy the extremal or contact conditions for all interface points. Initial 
conditions for this iteration are the body s current placement, and the parametric locations of the interface 
points after the conditioning phase. Note that the second phase may slightly alter the body s 7We are 
able to move bodies much more between states and still maintain the contact conditions with the use of 
the conditioning phase. AA A  BB B (a) polygonal collision (b) analytic intersection (c) gradient descent 
A A  B B (d) extremal pair (e) contact pair Figure 7: Steps in converting a polyhedral collision into 
a contact point. placement in order to satisfy the contact point conditions. When the numerical iteration 
in either tracking phase fails, the placement algorithm tries a smaller move of the active body from 
its last correct state (Figure 2). 4.2 Detecting Contact Transitions Before a body is moved, the placement 
algorithm checks for transitions of contact points. When a transition is detected, the interface point 
is either deleted in the case of a vanishing transition, or switched between contact and extremal. The 
following transitions (with their conditions for occurrence) are detected: 1. extremal to contact separation 
distance between bodies becomes negative 2. contact to extremal contact force attracts rather than 
repels the bodies 3. extremal vanishes  separation distance exceeds threshold  conditioning phase 
iteration fails  point becomes too close to another interface point  point falls off parametric domain 
of its patch   4. contact vanishes point becomes too close to another interface point  point falls 
off parametric domain of its patch  The user can optionally disable contact to extremal transitions 
to assure that the active body will stay in contact as it s moved. This effectively makes the selected 
contacts sticky . 4.3 Creating Contacts The automatic placement algorithm converts collision points 
on polygonal bodies to contact points on analytic parametric surfaces. A series of steps, illustrated 
in Figure 7, is performed, each of which uses the results of the previous step as the initial condition 
in a numerical iteration: 1. Interpolate the polygonal collision point to yield a pair of parametric 
points (u1,v1,u2,v2). The (body,patch) identi.er for each of the inter­secting triangles allows retrieval 
of the appropriate surface functions used in the following steps (Figure 7a). 2. Iterate onto the analytic 
intersection (Figure 7b). 3. Do a few steps of gradient descent to nudge the pair toward the points 
of maximal penetration (Figure 7c). This is necessary so that the next step iterates in the right direction 
(i.e., toward maximal penetration rather than maximal separation). The objective function is de.ned in 
Section 4.4, Equation 2. 4. Iterate onto the extremal pair (Figure 7d). This is a conditioning step 
exactly like the conditioning phase of tracking. 5. Iterate onto a contact pair (Figure 7e). In this 
step, the active body s position parameters and the locations of all interface points are adjusted. Equations 
for all interface points must be satis.ed simultaneously, since the active body s placement affects the 
contact conditions of the previous interface points.  Steps 2, 4, and 5 involve numerical iteration 
which can fail. A failure in Step 2 may occur because the polygonal approximations intersect but the 
analytic surfaces do not (refer to Figure 3a). If failure occurs, we try iterating onto an extremal pair. 
If this succeeds, and the resulting separation distance (de.ned in Section 4.4) is positive, the interface 
point is added as an extremal pair rather than a contact pair. This allows the polygonal collision to 
be ignored during the next collision detection query. If a failure in Step 4 occurs, we disregard what 
amounts to a conditioning step, and go on to step 5. A failure elsewhere, or after taking the above measures, 
results in a collision failure and causes the placement algorithm to try a smaller solution step in its 
next iteration. After a successful contact point creation, the sphere of neglect radius is determined 
by the distance the point moved from Step 1 to Step 5. A collision failure is returned if this distance 
exceeds a user-settable threshold. 4.4 Contact Point Iteration Interface point tracking and contact 
point creation both use multidimensional Newton iteration on a system of equations (see [PRES86] for 
a complete description).8 Given a system of nonlinear equations F(x) = 0 and a point near the solution, 
x0, a successive approximation to the true solution is achieved by solving the linear system of equations 
oF 0= F(x0)+ (x0)(x1 -x0) (1) ox where oF/ox is the Jacobian of F. We solve the above linear system using 
the singular value decomposition (or SVD, see [PRES86]), which allows non-square systems to be solved 
and is numerically robust.9 The process is then repeated to improve the approximation, yielding a sequence 
of iterates xi. The iteration fails if F(xi) does not get closer to 0 or a maximum number of iterations 
is exceeded without yielding a point suf.ciently close (in the residual sense) to a solution. It only 
remains to describe the relevant systems of equations (F from Equa­tion 1), and their parameters, x, 
for each iterative procedure. In the follow­ing, we have two points each on a rigidly movable surface 
Si(Qi,Xi,ui,vi), i =1,2, where Qi is a rotation in 3D10 and Xi is a translation in 3D: Si(Qi,Xi,ui,vi) 
Qi si(ui,vi)+ Xi si(ui,vi) represents the surface in its local coordinate system. There are three systems 
of equations used in the above Newton iteration: 1. intersection [iteration over (u1,v1,u2,v2)]: S1 -S2=0 
The pair of parametric points is moved to become a true intersection of the bodies, used in Step 2 of 
the contact point creation sequence. Note that this is a non-square system of equations: 3 equations 
in 4 variables, which is handled using SVD-augmented Newton iteration. 2. extremal [iteration over (u1,v1,u2,v2)]: 
(S1 -S2) XN1=0 N1+ N2=0 where N1 and N2 are the outward-pointing unit normals of the two surfaces. These 
conditions imply that the distance between the point pair is a local extremum (the vector between the 
points is in the direction of one surface s normal, and the two normals are anti-parallel). If the bodies 
are separated by a small distance, the iteration will cause the two points to become the points of minimum 
distance between the two surfaces. If the bodies interpenetrate, the iteration will cause the two to 
become the points of furthest penetration. This iteration is used in tracking and in Step 4 of the contact 
point creation sequence. 8We use the numerical package LAPACK to compute the SVD. 9To solve the linear 
system Ax = b, the SVD of matrix A is .rst computed. This yields three matrices whose product is A, A 
= UDVT ,where U and V are orthonormal, and D is a diagonal matrix. An ill-conditioned system is adjusted 
by setting to 0 those elements for which Di/Dmax,where Dmax is the largest diagonal element. We then 
compute x = VD-1 UT taking into account the rank of D, since some elements have been set to 0. The result 
is a numerically robust solution that minimizes the solution, kxk, if solutions exist, and minimizes 
the residual, kMx -bk, if not. 10Weusea unitquaternion, Q =(q1 q2 q3 q4), to represent this rotation. 
The coordinate system origin is at the center of mass of the body. 3. contact [iteration over (Q1,X1,u1,v1,u2,v2)] 
S1 -S2=0 N1+ N2=0 We change the position and orientation of the .rst body as well as the parametric location 
of the point of contact in order to achieve a true contact point where the surfaces touch and are tangent. 
This iteration is used in tracking and in Step 5 of the contact point creation sequence. Note that the 
position and orientation of the second body (Q2,X2)isheld constant for all systems of equations. Contact 
point creation also involves gradient descent iteration in Step 3. The objective function whose gradient 
is descended, the signed separation distance function G,is given by G (S2 -S1) .N1 (2) If the bodies 
don t touch, this function is positive within a neighborhood of the points of maximal separation. If 
the bodies interpenetrate, the function is negative inside the region of interpenetration.  5 Converging 
to a Stable Con.guration Convergence to a stable state is achieved by the application of pseudo­forces 
that cause an incremental change in the position of the active body. Using force-like quantities accomplishes 
two things: it leads to changes in object placement that the user can predict, and it provides much faster 
convergence to stable placement than other update rules.11 Pseudo-forces differ from physical forces 
because they are solved for statically and do not accelerate the object but are applied directly to update 
the object s position (see Section 5.3). Three kinds of pseudo-forces are used: external, which represent 
gravity and user-speci.ed nudges ,contact, which are applied at contact pairs to balance the external 
forces as much as possible, andresidual, which represent the resulting force (external -contact) applied 
to adjust the body s position. The body is considered to be at rest and automatic placement halted when 
the residual force is suf.ciently small. It sometimes happens that exact force balance can not be achieved.12 
We therefore use a second criterion to determine stability: the psuedo potential energy objective. The 
placement algorithm tries to minimize the energy objective: a state that increases the objective is rejected. 
When gravity is the sole external force, the energy objective is simply the height of the center of mass 
of the active body. The placement algorithm thus drops the object as far as possible. When the change 
in energy objective is suf.ciently small, the automatic placement algorithm is terminated.13 Other forces 
have different objectives, discussed in Section 5.1. The total energy objective is the sum of the energy 
objectives for all external psuedo-forces. 5.1 Computing External Pseudo-Forces The user can apply combinations 
of three types of external pseudo-forces to the active body: gravity, local,and connection forces (Figure 
8). Pseudo­forces from these three categories can be added or deleted at any time during automatic placement 
to position the active body. For each external pseudo­force, the algorithm computes three quantities: 
a force (F) and torque (T) on the body, and an objective term (E). These are summed to produce the total 
force, torque, and objective. In the following, the current location of the active body is represented 
by the rotation/translation pair (Q,X), as in Section 4.4. Force parameters in capital letters are in 
world coordinates; noncapitalized parameters are in body coordinates. External pseudo-forces always have 
unit length; the 11For example, to stabilize an object under the in.uence of gravity, the .rst update 
rule we tried was to translate the body down in z (direct gradient descent). Iteration was then done 
to satisfy the interface point conditions, which typically moves the body back up in z. In many experiments 
this algorithm was more than 10 times slower then the one proposed, which balances gravity with an approximation 
to the contact forces. 12Forexample, consider droppingatorusonto a .atground planesuch that the contact 
regionin the resting state forms a circle. The approach advocated here will create a number of contact 
points whose con.guration depends on the polygonal approximation used for the torus. It can easily happen 
that two contact points arise that do not form an exact diameter of the circle of contact. Any assignment 
of contact forces at these points will therefore produce a residual torque (note that the contact forces 
point up, normal to the plane). 13Using the torus/plane example, this termination criterion guarantees 
that the torus comes to rest in a stable state as soon as more than one contact point is generated. This 
is because it can no longer move in z; the energy objective remains unchanged, triggering the termination 
condition. P p  gravity local connection Figure 8: Types of external pseudo-forces. amount of movement 
is determined by the body movement step (£from Figure 2). Length measurements are scaled relative to 
the radius of the smallest enclosing sphere around the active object, R. A gravity pseudo-force is applied 
at the body s center of mass in a given direction D (e.g., D =(0,0,-1)). It thus produces no torque, 
and tends to translate the body along the D direction. The force, torque, and objective terms for this 
pseudo-force are given by: Fgravity D Tgravity 0 Egravity -(D .X)/R A local pseudo-force is applied at 
a given point on the active body, p,in some direction d in its local coordinate system. It allows the 
user to push and pull on the body. The force, torque, and objective terms are: Flocal Qd Tlocal (Qp)/R 
XFlocal Elocal 0 To specify a local pseudo-force, the user picks a point p on the active body and speci.es 
a push or pull force. A push force assigns to d the negative of the unit normal vector at p; a pull force 
assigns it the unnegated normal vector. The user can also directly control the direction with a 3D widget. 
Note that the objective term for a local pseudo-force is 0. In fact, the user typically turns off objective 
function processing when using local pseudo­forces, so that objects can be moved freely against gravity 
or other external forces without causing a loop failure when the objective function increases (refer 
to Figure 2).14 A connection pseudo-force is applied at a local point on the body, p in order to connect 
that point to a given point in world space P. The force, torque, and objective terms are: P -(Qp + X) 
Fconnect kP -(Qp + X)k Tconnect (Qp)/R XFconnect Econnect kP -(Qp + X)k/R 5.2 Solving for Contact and 
Residual Pseudo-Forces Let the total external pseudo-force and pseudo-torque on the body be given by 
Fe and Te. The contact pseudo-forces are given by n scalars fi where n represents the number of contact 
pairs. The direction of these forces is given by Ni where Ni is a unit vector representing the negative 
of the normal vector on the active body at the point of contact. To .nd the fi s, we solve the linear 
system of force balance equations given by: n X fi Ni = Fe i=1 n X fi (pi X Ni) = Te i=1 where pi is 
the vector from the body s center of mass to the contact point. This system has no solution if the the 
body isn t in a balanced state. We solve using SVD to obtain a solution which minimizes the L2 norm of 
the residual. 14In a typical placement scenario, the user .rst lets a body drop under the in.uence of 
gravity. When it is stable, objective function processing is turned off and local pseudo-forces are used 
to slide the contacting body where it s desired. Then local pseudo-forces can be deleted and objective 
processing re-enabled to reach a truly stable placement. While the contact forces produced in this way 
are a coarse approximation, unstable objects still fall over much as they might in the real world. To 
solve for the residual pseudo-force and pseudo-torque, Fr and Tr ,we subtract the sum of the previously 
computed contact forces from the external pseudo-forces and pseudo-torques: n X Fr Fe -fi Ni i=1 n X 
Tr Te -fi (pi XNi) i=1 The termination criterion kFr k<1,kTr k<1stops the body when it is balanced with 
respect to purely normal contact forces. Note that this criterion implies the body is physically stable, 
since a static force balance has been achieved. The user may also want to stop the body when it is stable 
with respect to frictional forces.15 To do this, we compute a frictional pseudo-force and pseudo-torque 
residual, Ff and Tf , by .rst solving the frictional force balance problem n X fiU Ui + fiV Vi = Fr i=1 
n X fU (pi XUi)+ fV (pi XVi)= Tr ii i=1 where Ui and Vi are independent tangent vectors at each contact 
point, and fU and fV are the 2n frictional forces along these directions to be solved for. ii This problem 
is solved using SVD and the resulting minimal L2 solution subtracted from Fr and Tr respectively to yield 
the new residual Ff , Tf .This approximation to the frictional force and torque is simpler, and less 
physical, than used in [BARA94]. When allowing friction forces in the termination criteria, we make the 
additional constraint that the magnitude of the frictional pseudo-force at each contact must be less 
than a .xed ratio of the magnitude of the normal pseudo-force (Coulomb model of friction).16 The .nal 
pseudo-force and pseudo-torque applied to the active body, FN and TN ,isgiven by (1 --a FN a)Fr Ff TN 
(1 -a)Tr -a Ff where ais a user-speci.ed constant related to the amount of friction desired. 5.3 Updating 
Body Placement The active body s position is updated by rotating through a small angle B in radians around 
an axis A and translating through a small displacement 6. The total amount of change is governed by the 
scale-invariant parameter £ the maximum distance to move the active body in one step, relative to the 
radius of its smallest enclosing sphere, R. Actual values for the update parameters are obtained from 
the rigid body equations of motion assuming the body was at rest before applying forces, which implies 
that the linear and angular velocities of the body are 0. We .rst scale the pseudo-forces and pseudo-torques 
to be applied, FN and TN , by the sum of their lengths so that roughly the same amount of change happens 
at each step: FN F N kFN k+ kTN k TN TFN kFN k+ kTN k Since for small £,sin(£) .£, normalizing by kFN 
k+ kTN kimplies that the amount of movement of a point on the active body s enclosing sphere due to both 
rotation and translation, scaled by 1/R, will be roughly £. 15This allowsa body to lean on otherseven 
thoughexternal forcesare unbalanced by purelynormal contact forces. 16Note that an assignment of frictionforcesthat 
produces a very small residual and doesnot violate the Coulomb constraint may exist, yet we may not .nd 
it. To .nd such a friction force assignment would require solving a quadratic optimization problem. In 
practice, we frequently .nd an acceptable force assignment with this simple method. The user can also 
stop the system at any time manually. experiment time iterations torus on cone 9.89 13 block into goblet 
8.0 25 goblet sideways on ground 6.34 28 knot on ground 3.86 12 jack on ground 6.6 36 bumpy sphere on 
ground 3.43 14 torus on ground 10.72 37 cone on ground 16.15 23 teapot on ground 6.89 20 lid on teapot 
51.23 29 straw in glass 14.03 40 ice cube in glass 13.78 39 44thballinbowl 6.58 9 bumpy sphere on cone 
21.26 50 apple on ground 3.1 12 Figure 9: Results for Various Experiments. Each experiment was run until 
the place­ment algorithm terminated. Time is clock time in seconds on a SGI Indigo Extreme 2 workstation 
(R4400 150MHz processor). Iteration counts are the number of main loops, including failure loops, from 
the placement algorithm in Figure 2. We then derive the linear and angular accelerations from the normalized 
force and torque. The linear acceleration, a,is given by a F N assuming the body has unit mass. The derivative 
of the angular velocity, w , (under the at-rest assumption) is given by I-1 F wTN where I is the inertia 
tensor of the body in world coordinates.17 A differential update is obtained through direct use of the 
linear and angular accelerations, scaled by £, to yield 6£Ra B£ k kw w A kw k This provides a memoryless 
change to the body placement, akin to simu­lation in an extremely viscous .uid. Note that 6is scaled 
by R since it is a length parameter in world coordinates. As before, the body state is encoded by a quaternion/vector 
pair (Q,X). After moving by £, the new body state, (Q0 ,X0), is given by Q0 (sin(B/2),cos(B/2) A).Q X0 
X + 6 where .denotes quaternion multiplication.   6Results Figure 9 shows performance results for some 
simple experiments. In these experiments, time per iteration on an SGI Indigo Extreme 2 worksta­tion 
varied from 0.1 seconds to several seconds. A few tens of iterations are typically necessary to stabilize 
an object from a nearby placement. Propor­tionally more iterations are required when an object must traverse 
a circuitous route over many bodies and contacts. The majority of the computation time (more than 90% 
excluding rendering) is consumed by collision detection and numerical iteration involved in contact point 
creation and tracking. Figure 1 and Figures 10-16 show some results of the placement algorithm. The placement 
tool allows the user to replicate the active body, making it easy to place multiple instances like the 
ice cubes or balls in Figures 14 and 15. Parameter values used in both the performance experiments and 
the modeling examples were ,=1.33, Nsimultaneous =3, a=0, and £max =0.05. The recovery parameter, ,from 
Figure 2, was chosen so that the solution step size recovers somewhat more slowly than it is re.ned.18 
Using Nsimultaneous = 3 handles the vast majority of simultaneous collisions, 17The inverse of the inertia 
tensor I -1 in world coordinates is equal to QI -1Q -1 where Q is b the rotation of the body, and I -1 
is the inertia tensor in body coordinates. Ib is appropriately scaled b so that the body has unit mass. 
Note that an accurate inertia tensor is often not required for computer graphics applications in which 
only the appearance of stability matters. The identity tensor often suf.ces. 18Recall from Figure 2 that 
cis halved if a loop failure occurs. but when regions of contact form curves or surfaces, allows the 
algorithm to check for stability without distributing points over the entire contact manifold. The value 
£max =0.05 is a good compromise between speed of convergence and predictability of movement. User setting 
of £max is sometimes useful to increase the speed of convergence or progress more slowly through a portion 
of the optimization in which there is an undesirably large change in the body s placement. Polygonal 
tessellations of surfaces were computed by uniform sampling in parameter space. The sampling density 
was typically chosen so that the surfaces appeared mostly free of polygonal artifacts when viewed from 
distances convenient for modeling. For example, the sphere used in the test of Figure 15 was tessellated 
using a 41 X21 mesh, the bowl with a 81 X121 mesh. We have reason to believe that the algorithm functions 
over a wide range of mesh accuracy. In one experiment, a knot-like shape was repeatedly dropped from 
the same position on a plane, using successively coarser uniformly sampled polygonal meshes. A stable 
placement involving three contact points was achieved using meshes containing from 9600 triangles to 
56 triangles. Time to convergence varied for these experiments varied from 7.27 seconds to 6.12 seconds. 
Below 56 triangles (mesh size 15 X5), the mesh was too poor an approximation to allow convergence of 
polygonal collisions to analytic contacts. A reasonable deterministic heuristic for tessellation would 
be to bound a measure of the approximation error such as maximum length of deviation; we have not investigated 
such a heuristic. The placement algorithm has been surprisingly robust in our experiments: almost all 
modeling tasks in the .gures were performed without interrupting the algorithm and without changing parameters 
from their default values. Occasional problems do occur. The algorithm can get stuck when it is not able 
to convert a polygonal collision to a contact point, so that £goes to 0. This happened in certain experiments 
when dropping a torus over a cone and the lid over the teapot. Both situations involve bodies that form 
a curve of contact at the stable con.guration, where one body is concave at the contact. The problem 
occurs after one contact has been created. As the body approaches the stable state, additional polygonal 
collisions are found and converted to analytic contact points. The iteration often moves the contact 
point a signi.cant distance over the surface, violating the maximum distance threshold. If the distance 
threshold is increased, the large radius of neglect for the point may cause missed collisions. If the 
radius of neglect is manually decreased, too many polygonal collisions can be computed, slowing the algorithm 
to a crawl. With additional intervention though, modeling tasks can still be performed in these situations. 
When the problem occurs, the user can interrupt the placement algorithm. Increasing the distance threshold 
parameter and then resuming often solves the problem, though the user must watch out for missed collisions. 
Collision detection can also be temporarily disabled after interruption and convergence attempted with 
the current contacts. Again, missed collisions are a possibility. Since the problems often occur very 
near to the stable position, a third strategy is to interactively reposition the object slightly from 
its stuck position and try again. If the problem is not corrected or results in missed collisions, the 
user can undo the bad states and try different strategies. The desired placement is usually achieved 
after two or three attempts. Such measures, although clearly not ideal, may not be intolerable in an 
interactive modeling environment. As might be expected, the algorithm also suffers from convergence to 
meta-stable placements. For example, when a sphere is dropped onto another precisely underneath, the 
algorithm converges with the sphere balanced on top. This is easily remedied by interactively giving 
the sphere a nudge (by applying a local force for one step) and resuming. 7 Conclusion Placing curved 
objects in physically plausible con.gurations has always been a dif.cult task for modeling systems, but 
one that can add much visual richness. This paper describes a new tool for interactive placement of non-interpenetrating 
curved objects that makes this task easier and more accurate. Two ideas make such a tool practical. The 
.rst is the use of a hybrid surface representation. A polygonal approximation allows quick detection 
of contacts that arise; a functional description converts these using numerical iteration to accurate 
points where the analytic surfaces touch. The second idea is to reach stability using an optimization 
technique that passes through a discrete series of states, based on a simple static force balance law 
rather than a dynamic simulation. This paper describes a way of moving a body toward a stable state which 
is predictable and fast. A number of areas for extending this work remain. Handling multiple active bodies 
is a straightforward extension. A slightly more general contact force solver is needed; the collision 
detection and interface point tracking algorithms described here require no modi.cation. We believe such 
an extension would be practical in an interactive setting only for a limited number of active bodies. 
For example, simultaneously manipulating all 44 balls from the model in Figure 15, including well over 
a hundred tracked contact points, does not seem practical for the near future. Nevertheless, the extension 
would be useful for placement of mechanical linkages containing a few curved parts. Handling interactions 
between rigid and .exible bodies may also be possible. Another problem we have only begun to investigate 
is placement from an interfering initial state to allow a tool that can extricate a penetrating body. 
Finally, we are investigating ways to automatically handle the problem case discussed in Section 6 without 
resorting to user intervention.  Acknowledgements I wish to thank Al Barr for getting me interested 
in this topic, and for his many insights into physical simulation. I also owe thanks to Jim Kajiya for 
many helpful discussions about the role of physical simulation in object assembly. James O Brien wrote 
most of the collision detection code. Michael Cohen provided an extremely valuable critical read of the 
document as did Jim Kajiya. I also acknowledge Bena Currin, Adam Woodbury, Kurt Fleischer, and the other 
members of Caltech s graphics group for geometric models used in this work and for helpful early discussions. 
This work wold not have been possible without the support of Microsoft, especially my manager Dan Ling. 
 References [BARA90] Baraff, David, Curved Surfaces and Coherence for Non-penetrating Rigid Body Simulation, 
Computer Graphics, 24(4), pp. 19-28, August 1990. [BARA91] Baraff, David, Coping with Friction for Non-penetrating 
Rigid Body Simulation, Computer Graphics, 25(4), pp. 31-39, July 1991. [BARA94] Baraff, David, Fast Contact 
Force Computation for Nonpenetrating Rigid Bodies, Computer Graphics, 28(2), pp. 23-42, July 1994. [BARZ88] 
Barzel, Ronen, and A. Barr, A Modeling System Based On Dynamic Constraints, Computer Graphics, 22(4), 
pp. 179-188, August 1988. [GARC94] Garcia-Alonso, Alejandro, N. Serrano, and J. Flaquer, Solving the 
Col­lision Detection Problem, IEEE Computer Graphics and Applications, pp. 36-43, May 1994. [KAY86] Kay, 
Tim, and K. Kajiya, Ray Tracing Complex Scenes, Computer Graphics, 20(4), pp. 269-278, August 1986. [GLEI92] 
Gleicher, Michael, and A. Witkin, Through-the-Lens Camera Control, Computer Graphics, 26(2), pp. 331-340, 
July 1992. [META92] Metaxas, Dimitri, and D. Terzopoulos, Dynamic Deformation of Solid Primitives with 
Constraints, Computer Graphics, 26(2), pp. 309-312, July 1992. [MOOR88] Moore, M. and Wilhelms, J., Collision 
Detection and Response for Computer Animation, Computer Graphics, 22(4), pp. 289-298, August 1988. [PRES86] 
Press, W. H., B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes, Cambridge University 
Press, Cambridge, England, 1986. [SCLA91] Sclaroff, Stan, and A. Pentland, Generalized Implicit Functions 
for Com­puter Graphics, Computer Graphics, 25(4), pp. 247-250, July 1991. [SNYD92] Snyder, John, and 
J. Kajiya, Generative Modeling: A Symbolic System for Geometric Modeling, Computer Graphics, 26(2), pp. 
369-378, July 1992. [SNYD93] Snyder, John, A. Woodbury, K. Fleischer, B. Currin, and A. Barr, Interval 
Methods for Multi-point Collisions between Time-Dependent Curved Surfaces , Computer Graphics, 27(2), 
pp. 321-334, Aug. 1993. [WITK87] Witkin, Andrew, K. Fleischer, and A. Barr, Energy Constraints on Pa­rameterized 
Models, Computer Graphics, 21(4), pp. 225-232, July 1987.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218445</article_id>
		<sort_key>219</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Artistic screening]]></title>
		<page_from>219</page_from>
		<page_to>228</page_to>
		<doi_number>10.1145/218380.218445</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218445</url>
		<keywords>
			<kw><![CDATA[artistic screening]]></kw>
			<kw><![CDATA[graphic design]]></kw>
			<kw><![CDATA[halftoning]]></kw>
			<kw><![CDATA[image reproduction]]></kw>
			<kw><![CDATA[microlettering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P290853</person_id>
				<author_profile_id><![CDATA[81100237726]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ostromoukhov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), EPFL/LSP CH-1015 Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15020442</person_id>
				<author_profile_id><![CDATA[81100044881]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Hersch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL), EPFL/LSP CH-1015 Lausanne, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Critchlow, Islamic Patterns, Thames &amp; Hudson, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R Fink, PostScript Screening: Adobe Accurate Screens, Mountain View, Ca., Adobe Press, 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[E. Fiume, A. Fournier, V. Canale, "Conformal Texture Mapping", Eurographics'87, North-Holland, 1987, 53-64]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Foley, A. van Dam, S. Feiner, J. Hughes, Computer Graphics: Principles and Practice, Addison-Wesley, Reading, Mass., 1990.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[R.D. Hersch, "Fill and Clip of Arbitrary Shapes", New Trends in Animation and Visualization, (D. Thalmann, N. Magnenat- Thalmann, Eds.), J. Wiley &amp; Sons, 1991, 3-12.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Peter R. Jones, "Evolution of halftoning technology in the United States patent literature", Journal of Electronic Imaging, Vol. 3, No. 3, 1994, 257-275.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[H. Massoudi, Calligraphie arabe vivante, Flammarion, Paris, 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[R.K. Molla, Electronic Color Separation, Montgomery, W.V., R.K. Printing and Publishing, 1988.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[M. Morgan, R.D. Hersch, V. Ostromoukhov, "Hardware Acceleration of Halftoning", Proceedings SID International Symposium, Anaheim, May 1993, published in SID 93 Digest, Vol XXIV, 151-154.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[L.A. Olzak, J.P. Thomas, "Seeing Spatial Patterns", in Handbook of Perception and Human Performance, (K.R. Boff, L. Kaufman, J.P. Thomas, Eds.), John Wiley &amp; Sons, Vol. 1, 1986, 7.1-7.57.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[D. Schattschneider, Visions of Symmetry, Note, Books, Periodic Drawings and Related Works of M.S. Escher, W.H. Freeman and Company, New York, 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[S.N. Schiller, D.E. Knuth, Method of controlling dot size in digital halftoning with multi-cell threshold arrays, US Patent 5305118, issued on April 19, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R. Schinziger, P.A.A. Laura, Conformal Mappings: Methods and Applications, Elsevier, 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134001</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[T.W. Sederberg, E. Greenwood, "A Physically Based Approach to 2-D Shape Blending" SIGGRAPH'92,ACM Computer Graphics, 26(2), 1992, 25-34.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[G. Stix, "Making money, desktop counterfeiting may keep the feds hopping", Scientific American, March 1994, 81-83.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27674</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[R. Ulichney, Digital Halftoning, MIT Press, 1987.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192184</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[G. Winkenbach, D.H. Salesin, "Computer-Generated Penand-Ink Illustration" Proceedings SIGGRAPH'94, Computer Graphics, Annual Conference Series, 1994, 91-100.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J.A.C. Yule, Principles of Color Reproduction, John Wiley &amp; Sons, New York, 1967.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Artistic Screening Victor Ostromoukhov, Roger D. Hersch Ecole Polytechnique Fe´de´rale de Lausanne (EPFL), 
Switzerland Abstract Artistic screening is a new image reproduction technique incorpo­rating freely 
created artistic screen elements for generating halftones. Fixed prede.ned dot contours associated with 
given intensity lev­els determine the screen dot shape s growing behavior. Screen dot contours associated 
with each intensity level are obtained by inter­polation between the .xed prede.ned dot contours. A user-de.ned 
mapping transforms screen elements from screen element de.nition space to screen element rendition space. 
This mapping can be tuned to produce various effects such as dilatations, contractions and non­linear 
deformations of the screen element grid. Discrete screen elements associated with all desired intensity 
levels are obtained by rasterizing the interpolated screen dot shapes in the screen element rendition 
space. Since both the image to be reproduced and the screen shapes can be designed independently, the 
design freedom offered to artists is very great. The interaction between the image to be reproduced and 
the screen shapes enables the creation of graphic designs of high artistic quality. Artistic screening 
is particularly well suited for the reproduction of images on large posters. When looked at from a short 
distance, the poster s screening layer may deliver its own message. Furthermore, thanks to artistic screen­ing, 
both full size and microscopic letters can be incorporated into the image reproduction process. In order 
to avoid counterfeiting, banknotes may comprise grayscale images with intensity levels produced by microletters 
of varying size and shape. Keywords Image reproduction, graphic design, halftoning, artistic screening, 
microlettering 1 Introduction Halftoning and screening techniques are aimed at giving the impres­sion 
of variable intensity levels by varying the respective surfaces of white and black within a small area. 
Traditional techniques use repetitive screen elements, which pave the plane and within which screen dot 
surfaces de.ne either white or black parts [16]. As long as the screen element period is small, or equivalently, 
the screen EPFL/LSP CH-1015 Lausanne, Switzerland victor@di.ep..ch, hersch@di.ep..ch http://diwww.ep..ch/w3lsp/screenart.html 
  Figure 1: Escher s Sky and Water woodcut (reproduced with per­mission, c1995 M.C. Escher, Cordon Art, 
Baarn, NL). frequency is high (for example 150 screen elements per inch), dis­tinct screen elements cannot 
be perceived by the human eye from a normal viewing distance [10]. However, in order to achieve such 
high screen frequencies, resolutions above 2400 dpi are required. With of.ce printers, respectively photocomposers, 
having resolu­tions between 240 and 800 dpi, respectively between 1200 and 2400 dpi, halftoning or screening 
effects cannot be completely hidden. This explains why so much effort has been invested in develop­ing 
halftoning techniques which reduce the impact of halftoning artifacts as much as possible [6]. We would 
like to take a different approach. Instead of looking at the halftoning layer as a pure functional layer 
producing undesired artifacts, we propose a new screening technique which enables the shape of screen 
dots to be tuned. By creating artistic screens which may take any desired shape, screening effects, which 
up to now were considered to be undesirable, are tuned to convey additional information for artistic 
purposes. The approach we follow is somewhat related to the pen and ink illustration techniques where 
pen strokes are used for sketching illustrations, at the same time creating texture and intensities. 
While computer-aided pen and ink illustration systems [17] aim to offer the same .exibility as traditional 
pen-based stroking, artistic screening, as presented in this contribution, is a new computer-based image 
reproduction technique, which opens a new design space for artistic realizations. For artistic screening, 
we extend the dynamics of screen dot Figure 2: Mosaic tilework faces walls surrounding the courtyard 
of the Attarine Medeza, Fez (Courtesy of R. and S. Michaud, Rapho).  shapes by using more sophisticated 
artistic shapes as screen dots. We would like to have full control over the evolution of the artistic 
screen dot shape and at the same time offer a halftoning method which is competitive with regard to conventional 
high-resolution clustered-dot screening. We have sought our inspiration in the work of medieval artists 
[1], who after having tiled the plane with repetitive polygonal patterns, created beautiful ornaments 
in each of the separate tiles (Fig. 2). Escher [11] further developed this technique by letting shapes 
circumscribed by a regular tile smoothly grow into one another (Fig. 1). The present work is also related 
to the decorative motives found in Islamic art which incorporate beautiful calligraphic work with letter 
shapes well-distributed over a given geometric surface (Fig. 3). Previous attempts to develop screen 
dots having non-standard shapes were aimed at improving the tone reproduction behavior at mid-tones [8]. 
Elliptic screen dots for example, have an improved tone reproduction behavior due to the fact that at 
the transition between 45% and 55% intensity, at .rst only two neighbouring dots touch each other and 
only after a certain increase of intensity does the screen dot touch all its four neighbours (Fig. 4). 
State of the art techniques for generating screen dot shapes are based on dither threshold arrays which 
determine the dot growing behavior. Since the dither threshold levels associated with the dither cells 
of a dither threshold array specify at which intensity the corresponding binary screen element pixels 
are to be turned on, the so generated screen dot shapes have the property of overlapping one another. 
In order to generate screen dots of any shape, which need not overlap one another and which may have 
self-intersecting contours, we propose a new way of synthesizing screen dot shapes. We de.ne the evolution 
of screen dot contours over the entire intensity range by interpolating over a set of prede.ned .xed 
dot contours which de.ne the screen dot shape at a set of .xed intensity levels. Once the evolving shape 
of the halftone dot boundary is de.ned exactly for every discrete intensity level, the screen elements 
associated with each intensity level are rasterized by .lling their associated screen dot contours (Section 
3). After having generated the screen elements, digital screening proceeds with the halftoning process 
described in more detail in Section 2. This halftoning process distinguishes itself from previ­ous halftoning 
methods described in the literature [6] by the fact that the screen elements associated with every intensity 
level are precomputed and that no comparisons between original gray levels and dither threshold levels 
are necessary at image generation time. Furthermore, it ensures smooth transitions of the artistic halftone 
pattern in regions of high intensity gradients by applying bi-linear Figure 3: Thoulthi classical calligraphy 
by Majed Al Zouhdi (Cour­tesy of H. Massoudy, [7]). interpolation between source image pixels. The results 
obtained with artistic screening (Section 5) demon­strate that contour-based generation of halftone screens 
effectively provides a new layer of information. We show how this layer of information can be used to 
convey artistic and cultural elements related to the content of the reproduced images. Since there is 
no limitation to the size of the halftone screen elements, they can be made as large as the image itself. 
The introduced mapping (Sec­tion 4) between screen element de.nition space and screen element rendition 
space enables the production of highly desirable, smooth deformations of screen dots, without affecting 
the image content. In addition to their nice visual properties, geometric transformations of screen element 
shapes are of high interest for creating microscopic letters for security purposes, for example on banknotes. 
Since artistic screening relies on the evolution of dot shapes at continuous intensity levels and since 
it allows building large screens (superscreens) containing arrays of screen subshapes, it is also able 
to produce traditional halftone screen dots having those frequencies and orientations which are required 
for traditional colour reproduc­tion. Artistic screening may therefore also be used at high resolu­tion 
as an alternative to current exact-angle clustered-dot screening techniques [2]. 2 The halftoning process 
Classical clustered-dot halftoning techniques rely on ordered dither threshold arrays. A dither threshold 
array is conceived as a discrete tile paving the output pixel plane. A dither threshold level is asso­ciated 
with each elementary cell of the dither threshold array. The succession of dither threshold levels speci.es 
the dot shape growing Figure 4: Traditional screen dot shapes, above with round and below with elliptic 
screen dots, produced by the artistic screening software package. 39 40   rz t 64 64 S x y y 1 8 516 
1 -1 21 1233 2925 8 16 5 374222 1229 244946 6 213325 45 58 38 0 28 53 54 2 2437 4246226 13 49 57 28 5413 
x 19 41 6263 5034 9 4553 58 38 21 11 32 61 59 17 4157 6263 5034 9 486043 19 -1 0 15 36 52 5155 2630 1532 
5261 60 59 17 5647 1148 43 4 44 55 30 204039 0 365647 (a) (b) 2731 3514 23 (c) 4 20 44 40 5139 26 18 
7 273523 10 3114 3 187 10 3 Figure 5: Spot function, dither matrix, and corresponding screen dot shapes. 
Two .xed prede.ned contours Interpolated contours Discretized screen element (a) (b)  Figure 6: Artistic 
screening with a screen dot pattern inspired by Escher reproduced on an image representing a grayscale 
wedge. behavior at increasing intensity levels (see Fig. 5). Dither thresh-rays are based on spot functions 
[2]. A spot function old levels can either be speci.ed manually or algorithmically [16]. de.nes the dither 
threshold levels for a dither element tile de.ned Previous algorithmic approaches for generating discrete 
dither ar-in a normalized coordinate space1 1.  et x yr z . S x y input image pixel boundaries input 
image pixel boundaries ...... ...... (a) input image pixel boundaries input image pixel boundaries 
...... ...... (b) Figure 7: Effect of rapid intensity transitions on (a) standard Figure 8: Rapid intensity 
transitions smoothed out by bi-linear clustered-dot screen elements and (b) artistic screen elements 
(en-interpolation of source image pixels at halftoning time on (a) stan­larged). dard clustered-dot screen 
elements and (b) artistic screen elements (enlarged). a) .xed prede.ned contours 25% 0%  b) scaled 
.xed prede.ned contours  n 25% i 50% 75% 100% c) interpolated contours 16.7% 41.7%  58.3% 66.7% 
 83.3% 91.7% d) interpolation parameters nzz nz z n z nzz 00 11 22 33 z 0=0% 1=25% 2=50% 3=75% 4=100% 
z Figure 9: Simple dot shape obtained by blending between a set of .xed contours. By discretizing this 
spot function, i.e, by computing its eleva­tion at the coordinates of the centers of individual screen 
cells, and by numbering successive intersection points according to their ele­vations (Fig. 5b), one 
obtains the dither threshold array used for the halftoning process. The comparisons between given source 
image pixel intensity levels and dither threshold levels determine the z. surface of a screen dot. For 
example, the dot shape associated with an input intensity level of 4064 is obtained by activating all 
screen element pixels with threshold values 40 or greater (Fig. 5c). With a given dither threshold array, 
the classical halftoning process consists of scanning the output bitmap, for each output pixel, .nding 
its corresponding locations both in the dither array and in the grayscale input image, comparing corresponding 
input image pixel intensity values to dither array threshold levels and accordingly writing pixels of 
one of two possible ouput intensity levels to the output image bitmap. Since artistic screening is not 
based on dither matrices, we pre­compute the screen elements (halftone patterns) representing each of 
the considered intensity levels. The halftoning process associ­ated with artistic screening consists 
of scanning the output bitmap, and for each binary output pixel, .nding its corresponding locations both 
in the grayscale input image and in the screen element tile. The input image intensity value determines 
which of the precomputed screen elements is to be accessed in order to copy its bit value into the current 
output bitmap location (Fig. 6a). This process may be accelerated by executing the same operations with 
several binary output pixels at a time [9]. In standard clustered-dot screening, due to the comparison 
be­tween source pixel intensity values and dither threshold values, rapid transitions within a single 
halftone screen element are pos­sible (Fig. 7a). They ensure that rapid intensity transitions occur­ring 
in the original image are preserved in the halftoned image. With artistic screen elements however, rapid 
transitions may in­troduce unacceptable distortions in the screen dot shape (Fig. 7b). Smoother transitions 
are obtained by computing for each output bitmap pixel the corresponding interpolated gray intensity 
value at the corresponding location in the source image pixmap (bi-linear in­terpolation). Smoother intensity 
variations will be associated with output bitmap neighbourhoods, which will in turn smooth out the transitions 
within single artistic screen elements (Fig. 8). If the original image is scanned at high resolution 
(300 dpi and higher), undesired sharp intensity transitions may be avoided by applying to it a low-pass 
.lter. There is a trade-off between the continuity of the halftone dot shapes and the faithful reproduction 
of sharp transitions. 3 Contour-based generation of discrete screen el­ements Spot functions generating 
simple screen dot shapes can be Sx y described easily. More complicated spot functions for generating 
shapes such as the dot shapes described in Fig. 6 are impossible to generate, since they cannot be described 
as single valued functions. In order to generate complicated dot shapes capable of repre­senting known 
subjects (birds, .shes) or objects (letter shapes), we de.ne the evolving screen dot shape by a description 
of its contours. For this purpose, we introduce .xed prede.ned screen dot contours which are associated 
with speci.c intensity levels. Shape blending techniques [14] are used to interpolate between those prede.ned 
screen dot contours at all other intensity levels. The .xed prede.ned contours, de.ned in a screen element 
def­  Figure 11: Screen tile containing subscreen shapes made of individual characters, at different 
intensity levels. inition space, are designed by a graphist using a shape drafting In order to control 
the speed at which the interpolated contour software package such as Adobe Illustrator. The graphist 
de.nes parts move from one .xed contour to the next, we introduce inter­his contours in the screen element 
de.nition coordinate space of polation parameters varying between 0 and 1 (Fig. 9d). The his preference. 
Figure 9a shows a set of .xed prede.ned contours  z p Pz coordinates of a control point at intensity 
interpolated between de.ning the evolution of a screen dot shape. two .xed contour control points and1 
associated with theFor ease of implementation, we assume that each .xed contour extremities of intensity 
range1is given byhas the same number of distinct contour parts and that the contour parts of the interpolated 
contours are obtained by blending between p Pz e p P e n i z p P z ii e p Pz p ii P h e p Pn i h i z 
p P i h e p P corresponding .xed contour parts. Curved contour parts may be 0 1 0 1 0 (1) described by 
polynomial splines. For convenience, we use a cubic where0 represents the screen element origin. Be´zier 
spline given by its control polygon to de.ne each curved Parameters are mapped to the range of intensity 
levelscontour part. In order to simplify the interpolation process, we z i z i h p P n i zzn i z by 
interactively de.ning the curves also assume that each straight line contour part is also de.ned by 1in 
the same way as gamma correction curves are de.ned in well known grayscalea Be´zier control polygon having 
its vertices aligned on the given halftoning packages (Adobe Photoshop for example). Figure 9straight 
line segment. The arrangement of contour parts in each shows the full intensity range, .xed prede.ned 
and intermediateof the .xed prede.ned contours governs the interpolation process contours as well as 
their associated interpolation parameters. (Fig. 9). In the range between intensity level 0 0 and intensity 
level 1 associated with the .rst .xed contour, the only operation which takes place is scaling. We therefore 
assume that the contour at level z z 0 is a .xed contour of in.nitely small size and that it has the 
same number of control points as the one at level 1. The tone reproduction behavior of a given printing 
process de­pends heavily on the dot gain, i.e. to what extent the printed dot has a larger surface than 
expected due to printer toner or ink spread properties. For a given printing process, the tone reproduction 
be­havior depends on the shape of the printed dot. When increasing the darkness (or, equivalently, decreasing 
the intensity) at light and mid-tones, the relative printed surface increase is larger for dots having 
a higher contour to surface ratio. Since the .xed contours de.ning the artistic screen dot shape may 
have any contour to sur­face ratio, the surface growth of the printed dot for a given intensity difference 
may vary considerably at different intensity levels. We can therefore use interpolation parameters as 
local gamma n i z correction factors [4]. If the created screen element is required to have a similar 
shape growing behavior in the light and in the dark tones, one may .rst design the .xed screen dot contours 
in the light tones and then in the dark tones. Taking into account the plane tiling behavior of a single 
screen element, the .xed contours associated with intensity levels, z t t 05 are drawn at a location 
whose center is translated by half a period in each direction from the original screen element center. 
The .xed contour parts located on the three quadrants outside the original screen element boundary are 
copied back into the original dx screen element (see Fig. 9a, 50% intensity). A single .xed contour associated 
with an intensity level equal dy to or close to 05 delimits the white growing region and the zt rendition 
space (a) applied to a small screen tile and (b) applied black growing region. to a large screen tile 
(super screen) made of repetitive subscreen Once all .xed contours have been designed in the screen elementelements. 
de.nition space, and the table of blending parameters is initalized 1.5 1 1 0.5 -1 -0.5 -1 -0.5 0.5 
1 -1 -0.5 -1.5 -1 a) b) c) Figure 12: Non-linear mapping between screen de.nition and screen rendition 
plane. n i0.5 z 12 with values , one merely needs to de.ne the corresponding result with an artistic 
screen dot shape inspired by Escher s drawing  screen element boundaries in the screen element rendition 
space, e.g. in the space associated with the output bitmap. The transfor­mation between screen element 
de.nition space and screen element rendition space enables the .xed prede.ned screen dot contours to 
be de.ned independently of the orientation and size of the .nal screen elements (Fig. 10). This transformation 
provides the basis both for screen element morphing (see Section 4) and for the gen­eration of screen 
elements having exact screen angles, as required by traditional colour reproduction techniques [18]. 
A square screen element de.ned by its supporting cathets and (Fig. 10b) whose desired orientation is 
given by angle can be approximated by an angle arctanas closely as required  dyhdx 0 dy dxdy hdx by 
increasing integer values and . The screen element s sub­division into a certain number of replicated 
subscreen dot shapes de.nes its screen frequency. In the screen element de.nition space, all subscreen 
dot shapes are identical. In the screen element ren­dition space however, rasterized discrete subscreen 
elements differ slightly one from another due to the different phase locations of their respective continuous 
contours (Fig. 10b). At high resolution, the so obtained exact angle screen elements are equivalent to 
the super-screening methods known in the .eld of colour reproduction [2], [12]. They have the advantage 
of offering the potential for colour reproduction with speci.cally designed screen shapes. Once the .xed 
prede.ned contour parts have been transformed from screen element de.nition to rendition space, the discrete 
screen elements may be generated for each discrete intensity level. For reproducing 256 intensity levels, 
the intensity interval be­tween 0 and 1 is divided by 255 and intermediate screen dot contours are successively 
generated at intensity levelsz zz. tt zz. 01255255255. At each discrete intensity, the screen dot contours 
are rasterized by applying well known shape rasterization techniques [4]. In the case of self-intersecting 
dot contours or dot contours having at a single intensity level multiple intersecting contours, care 
must be taken to use a scan-conversion and .lling algorithm supporting the non-zero winding number rule 
and generating non-overlapping complementary discrete shapes [5]. Furthermore, the .lling algorithm must 
be able to .ll shapes becom­ing smaller and smaller until they disappear. Figure 6 shows the (Fig. 1a), 
reproduced on a grayscale wedge. Small details, such as the wings of the bird, progressively fade out 
as the bird s shape size decreases. 4 Screen Morphing Since screen tiles can be as large as desired, 
they can be conceived so as to cover either the whole or a signi.cant part of the surface of the destination 
halftoned image. Such large screen tiles are divided into elementary subscreen shapes which may contain 
either identical or different shapes. For microlettering applications, each elementary subscreen shape 
may contain a different letter shape (Fig. 11). By de.ning the mapping from screen element de.nition 
space to screen element rendition space as a non-linear transformation, smooth, highly esthetic spatial 
variations of the subscreen shapes can be attained. For example, conformal mappings [13] [3] trans­form 
a rectangular grid of screen element sub-shapes into the sub­shapes of a deformed grid following electro-magnetic 
.eld lines (Fig. 12a). In that example, the conformal mapping is 1, where is a real scaling factor, represents 
complex  k e zz e e zx e iyk x y zwu wu e viv points lying in the originalplane and the corresponding 
complex points lying in the destinationplane. Alternatively, if one would like to enlarge a few screen 
sub­shapes at the expense of their surrounding subshapes, one may de.ne a circle of unit radius within 
which a geometric transforma­tion maps the original rectangular grid into a highly deformed grid (Fig. 
12b). A possible transformation is one that keeps the angle and modi.es the distance of points from the 
center of the circle (.sheye transformation). With the center of the circle as the origin of the coordinate 
system, the mapping expressed in polar coordinates is the following: 1if1 ; 11(2) ma 0 ar 0 n r h mm 
ff o r o r r rr otherwise where is a magnifying factor.  5 High-quality artistic screening In high-quality 
graphic applications, the shapes of artistic screen dots may be used as a vector for conveying additional 
information. This new layer of information may incorporate shapes which are related to the image. When 
reproduced in poster form, the screen elements of the screening layer will become suf.ciently large to 
produce the desired visual effect. In Figure 13, we show an example of a mosque rendered by screen dots 
made of calligraphic arabic letter shapes and oriental polygonal patterns. This screening layer adds 
a touch of islamic culture to the reproduced image. The next example (Fig. 14) shows a poster displaying 
a scene inspired from the well-known Kabuki theater shows. Such a poster could be used for example to 
advertise a Kabuki theatre perfor­mance. The beautiful Kanji letter shapes can be seen close up whereas 
the full poster can only be perceived from a certain view­ing distance. These two different views complement 
one another and each contributes towards transmitting the message to the public. In the last example, 
we show that artistic screening can bring new solutions for avoiding desktop counterfeiting [15]. Since 
1990, the US treasury protects banknotes by using microprinting techniques for generating letters having 
a size of approximatively 150 m in order to avoid reproduction by photocopy or scanners . (Fig. 15). 
In Figure 16, we show that by using artistic screen­ing techniques, microletters of the type shown in 
Figure 11 can be incorporated into the grayscale image. Furthermore, due to the conformal mapping function 
between screen element wtgz de.nition and rendition spaces (Fig. 12c), a non-repetitive screen is created 
which cannot be scanned easily without producing Moire´ effects.  6 Conclusions We have presented a 
new halftoning technique, where screen ele­ments are composed of artistic screen dot shapes, themselves 
cre­ated by skilled graphists. Fixed prede.ned dot contours associated with given intensity levels determine 
the screen dot shape s grow­ing behavior. Screen dot contours associated with each intensity level are 
obtained by interpolation between the .xed prede.ned dot contours. User-de.ned mappings transform screen 
elements from screen element de.nition space to screen element rendition space. These mappings can be 
tuned to produce various effects such as dilations, contractions and non-linear deformations of the subscreen 
element grid. By choosing an appropriate mapping, im­ages can be rendered while ensuring a highly esthetic 
behavior of their screening layer. Since artistic screening uses precomputed screen elements, its performance 
at image halftoning time is similar to that of other dithering algorithms. The time required for precomputing 
the screen elements associated with every intensity level depends on the size of the screen element tile. 
Limited size repetitive screen elements such as those used in Figures 13 and 14 can be gener­ated quickly 
(few minutes). On the other hand, very large screen elements morphed over the output image may require 
considerable computing power and time. Therefore, libraries of precomputed screen elements should be 
created. With such libraries, artistic screening can be made nearly as ef.cient as conventional halfton­ing. 
Artistic screening can be seen as a new image reproduction technique incorporating freely created artistic 
screen elements used for generating halftones. Since both the image to be reproduced and the screen shapes 
can be designed independently, the design freedom offered to artists is very great. In the examples of 
sections 4 and 5, we have shown that one may reproduce simple images with complicated screen elements 
morphed over the destination halftone image, real images with beautiful but repetitive screen shapes 
or   real images with complicated and morphed screen shapes. in order to simplify the design of the 
.xed prede.ned screen dot Artistic screening enables both full size and microscopic letters contours 
and the speci.cation of the transformation between screen to be incorporated into the image reproduction 
process. For exam-element de.nition and screen element rendition space. ple, next-generation banknotes 
may incorporate grayscale images Thanks to these novel computer-based screening techniques, with intensity 
levels produced by microletters of varying size and artistic screening may become an important graphic 
design tool. It shape. may have a considerable impact on future graphic designs. Currently, artistic 
screening is made possible by creating screen shapes with existing shape outlining tools and feeding 
them as input to the artistic screening software package. In the near future, we intend to add speci.c 
screen shape creation and morphing tools  7 Acknowledgements We would like to thank Nicolas Rudaz for 
having developed the QuickTime animation illustrating the basic concepts of Artistic Screening (see SIGGRAPH 
95 Proceedings on CD-ROM). We are grateful to H. Massoudy, Bella O., the British Museum, Rapho Press 
Agency, Paris and Cordon Art, Baarn, Holland for having kindly accepted to give us the permission to 
reproduce their originals.  REFERENCES [1] K. Critchlow, Islamic Patterns, Thames &#38; Hudson, 1989. 
[2] P. Fink, PostScript Screening: Adobe Accurate Screens, Moun­tain View, Ca., Adobe Press, 1992. [3] 
E. Fiume, A. Fournier, V. Canale, Conformal Texture Map­ping ,Eurographics 87, North-Holland, 1987, 53 
64 [4] J. Foley, A. van Dam, S. Feiner, J. Hughes, Computer Graphics: Principles and Practice, Addison-Wesley, 
Reading, Mass., 1990. [5] R.D. Hersch, Fill and Clip of Arbitrary Shapes ,New Trends in Animation and 
Visualization, (D. Thalmann, N. Magnenat-Thalmann, Eds.), J. Wiley &#38; Sons, 1991, 3 12. [6] Peter 
R. Jones, Evolution of halftoning technology in the United States patent literature ,Journal of Electronic 
Imaging, Vol. 3, No. 3, 1994, 257 275. [7] H. Massoudi, Calligraphie arabe vivante, Flammarion, Paris, 
1981. [8] R.K. Molla, Electronic Color Separation, Montgomery, W.V., R.K. Printing and Publishing, 1988. 
[9] M. Morgan, R.D. Hersch, V. Ostromoukhov, Hardware Ac­celeration of Halftoning , Proceedings SID International 
Sym­posium, Anaheim, May 1993, published in SID 93 Digest, Vol XXIV, 151 154. [10] L.A. Olzak, J.P. Thomas, 
Seeing Spatial Patterns , inHand­book of Perception and Human Performance, (K.R. Boff, L. Kaufman, J.P. 
Thomas, Eds.), John Wiley &#38; Sons, Vol. 1, 1986, 7.1 7.57.  Figure 16: Design of a banknote incorporating 
microletters as screen dot shapes. [11] D. Schattschneider, Visions of Symmetry, Note, Books, Peri­odic 
Drawings and Related Works of M.S. Escher, W.H. Freeman and Company, New York, 1990. [12] S.N. Schiller, 
D.E. Knuth, Method of controlling dot size in digital halftoning with multi-cell threshold arrays, US 
Patent 5305118, issued on April 19, 1994. [13] R. Schinziger, P.A.A. Laura, Conformal Mappings: Methods 
and Applications, Elsevier, 1991. [14] T.W. Sederberg, E. Greenwood, A Physically Based Ap­proach to 
2-D Shape Blending SIGGRAPH 92,ACM Computer Graphics, 26(2), 1992, 25 34. [15] G. Stix, Making money, 
desktop counterfeiting may keep the feds hopping ,Scienti.c American, March 1994, 81 83. [16] R. Ulichney, 
Digital Halftoning, MIT Press, 1987. [17] G. Winkenbach, D.H. Salesin, Computer-Generated Pen-and-Ink 
Illustration Proceedings SIGGRAPH 94,Computer Graphics, Annual Conference Series, 1994, 91 100. [18] 
J.A.C. Yule, Principles of Color Reproduction, John Wiley &#38; Sons, New York, 1967.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218446</article_id>
		<sort_key>229</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Pyramid-based texture analysis/synthesis]]></title>
		<page_from>229</page_from>
		<page_to>238</page_to>
		<doi_number>10.1145/218380.218446</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218446</url>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P61956</person_id>
				<author_profile_id><![CDATA[81100505719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Heeger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, Department of Psychology, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P131695</person_id>
				<author_profile_id><![CDATA[81100198422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Bergen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SRI David Sarnoff Research Center, Princeton, NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>72528</ref_obj_id>
				<ref_obj_pid>72527</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BENNIS, C., AND GAGALOWICZ, A. 2-D Macroscopic Texture Synthesis. Computer Graphics Forum 8 (1989), 291-300.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BERGEN, J.R. Theories of Visual Texture Perception. In Spatial Vision, D. Regan, Ed. CRC Press, 1991, pp. 114-133.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BERGEN, J. R., AND ADELSON, E. H. Early Vision and Texture Perception. Nature 333 (1988), 363-367.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BURY, P. Fast Filter Transforms for Image Processing. Computer Graphics and Image Processing 16 (1981), 20-51.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>247</ref_obj_id>
				<ref_obj_pid>245</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[BURY, P. J., AND ADELSON, E. H. A Multiresolution Spline with Application to Image Mosaics. ACM Transactions on Graphics 2 (1983), 217-236.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CHELLAPPA, R., AND KASHYAP, R. L. Texture Synthesis Using 2-D Noncausal Autoregressive Models. IEEE Transactions on Acoustics, Speech, and Signal Processing 33 (1985), 194-203.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CHUBB, C., AND LANDY, M. S. Orthogonal Distribution Analysis: A New Approach to the Study of Texture Perception. In Computational Models of Visual Processing, M. S. Landy and J. A. Movshon, Eds. MIT Press, Cambridge, MA, 1991, pp. 291-301.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[CROSS, G. C., AND JAIN, A. K. Markov Random Field Texture Models. IEEE Transactions on Pattern Analysis and Machine Intelligence 5 (1983), 25-39.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358553</ref_obj_id>
				<ref_obj_pid>358523</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[FOURNmR, A., FUSSEL, D., AND CARPENTER, L. Computer Rendering of Stochastic Models. Communications of the ACM 25 (1982), 371-384.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[FRANCOS, J. M., MEIRI, A. Z., AND PORAT, B. A Unified Texture Model Based on a 2D Wold-Like Decomposition. IEEE Transactions on Signal Processing 41 (1993), 2665- 2678.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>117688</ref_obj_id>
				<ref_obj_pid>117684</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FREEMAN, W. T., AND ADELSON, E. U. The Design and Use of Steerable Filters. IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (1991), 891-906.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[GAGALOWICZ, A. Texture Modelling Applications. The Visual Compuwr 3 (1987), 186-200.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[GAGALOWICZ, A., AND MA, S. D. Sequential Synthesis of Natural Textures. Computer Vision, Graphics, and Image Processing 30 (1985), 289-315.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., AND HAEBERLI, P. Direct WYSIWYG Painting and Texturing of 3D Shapes. Proceedings of SIC- GRAPH 90. In Computer Graphics (1990), vol. 24, ACM SIGGRAPH, pp. 215-223.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>13027</ref_obj_id>
				<ref_obj_pid>13021</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HECKBERT, P. S. Survey of Texture Mapping. IEEE Computer Graphics and Applications 6 (1986), 56-67.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[LANDY, M. S., AND BERGEN, J. R. Texture Segregation and Orientation Gradient. Vision Research 31 (1991), 679-691.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808605</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J.P. Texture Synthesis for Digital Painting. Proceedings of SIGGRAPH 84. In Computer Graphics (1984), vol. 18, ACM SIGGRAPH, pp. 245-252.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35069</ref_obj_id>
				<ref_obj_pid>35068</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J.P. Generalized Stochastic Subdivision. ACM Transactions on Graphics 6 (1987), 167-190.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74360</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[LEWIS, J.P. Algorithms for Solid Noise Synthesis. Proceedings of SIGGRAPH 89. In Computer Graphics (1989), vol. 23, ACM SIGGRAPH, pp. 263-270.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MALIK, J., AND PERONA, P. Preattentive Texture Discrimination with Early Vision Mechanisms. Journal of the Optical Society ofAmerica A 7 (1990), 923-931.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T., AND SPACH, S. A Context Sensitive Texture Nib. In Communicating with Virtual Worlds, N. M. Thalmann and D. Thalmann, Eds. Springer-Verlag, New York, 1993, pp. 151-163.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[OGDEN, J. M., ADELSON, E. H., BERGEN, J. R., AND BURY, P. J. Pyramid-Based Computer Graphics. RCA Engineer 30 (1985), 4-15.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[PEACHY, D. R. Solid Texturing of Complex Surfaces. Proceedings of SIGGRAPH 85. In Computer Graphics (1985), vol. 19, ACM SIGGRAPH, pp. 279-286.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. An Image Synthesizer. Proceedings of SIG- GRAPH 85. In Computer Graphics (1985), vol. 19, ACM SIGGRAPH, pp. 287-296.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628677</ref_obj_id>
				<ref_obj_pid>628318</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[PERONA, P. Deformable Kernels for Early Vision. IEEE Transactions on Pattern Analysis and Machine Intelligence (1995). To appear May 1995.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[POPAT, K., AND PICARD, R. W. Novel Cluster-Based Probability Model for Texture Synthesis, Classification, and Compression. In Proceedings of SPIE Visual Communications and Image Processing (1993), pp. 756-768.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[RUDERMAN, D. L., AND BIALEK, W. Statistics of Natural Images: Scaling in the Woods. Physical Review Letters 73 (1994), 814-817.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SIMONCELLI, E. P., AND ADELSON, E. H. Subband Transforms. In Subband Image Coding, J. W. Woods, Ed. Kluwer Academic Publishers, Norwell, MA, 1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SIMONCELLI, E. P., FREEMAN, W. T., ADELSON, E. H., AND HEEGER, D.J. Shiftable Multi-Scale Transforms. IEEE Transactions on Information Theory, Special Issue on Wavelets 38 (1992), 587-607.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>76600</ref_obj_id>
				<ref_obj_pid>76594</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[STRANG, G. Wavelets and Dilation Equations: A Brief Introduction. SIAM Review 31 (1989), 614-627.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion. Proceedings of SIGGRAPH 91. In Computer Graphics (1991), vol. 25, ACM SIGGRAPH, pp. 289- 298.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>11683</ref_obj_id>
				<ref_obj_pid>11682</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[TURNER, M. R. Texture Discrimination by Gabor Functions. Biological Cybernetics 55 (1986), 71-82.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L. Pyramidal Parametrics. Proceedings of SIC- GRAPH 83. In Computer Graphics (1983), vol. 17, ACM SIGGRAPH, pp. 1-11.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WITKIN, A., AND KASS, M. Reaction-Diffusion Textures. Proceedings of SIGGRAPH 91. In Computer Graphics (1991), vol. 25, ACM SIGGRAPH, pp. 299-308.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pyramid-Based Texture Analysis/Synthesis David J. Heeger* James R. Bergeny Stanford University SRI David 
Sarnoff Research Center Abstract This paper describes a method for synthesizing images that match thetextureappearanceofagivendigitizedsample. 
Thissynthesisis completely automatic and requires only the target texture as input. It allows generation 
of as much texture as desired so that any object can be covered. It can be used to produce solid textures 
for creat­ing textured 3-d objects without the distortions inherent in texture mapping. It can also be 
used to synthesize texture mixtures, images that look a bit like each of several digitized samples. The 
approach is based on a model of human texture perception, and has potential to be a practically useful 
tool for graphics applications. 1 Introduction Computer renderings of objects with surface texture are 
more inter­esting and realistic than those without texture. Texture mapping [15] is a technique for adding 
the appearance of surface detail by wrap­ping or projecting a digitized texture image onto a surface. 
Digitized textures can be obtained from a variety of sources, e.g., cropped from a photoCD image, but 
the resulting texture chip may not have the desired size or shape. To cover a large object you may need 
to repeatthe texture; thiscanleadtounacceptableartifacts eitherin the form of visible seams, visible 
repetition, or both. Texture mapping suffers from an additional fundamental prob­lem: often there is 
no natural map from the (planar) texture image to the geometry/topology of the surface, so the texture 
may be dis­torted unnaturally when mapped. There are some partial solutions to this distortion problem 
[15] but there is no universal solution for mapping an image onto an arbitrarily shaped surface. An alternative 
to texture mapping is to create (paint) textures by hand directly onto the 3-d surface model [14], but 
this process is both very labor intensive and requires considerable artistic skill. Another alternative 
is to use computer-synthesized textures so that as much texture can be generated as needed. Furthermore, 
some of the synthesis techniques produce textures that tile seamlessly. Using synthetic textures, the 
distortion problem has been solved in two different ways. First, some techniques work by synthesizing 
texture directly on the object surface (e.g., [31]). The second solu­tionis touse solid textures [19, 
23, 24]. A solid texture is a 3-d ar­ray of color values. A point on the surface of an object is colored 
by the value of the solid texture at the corresponding 3-d point. Solid texturing can be a very natural 
solution to the distortion problem: 'Department of Psychology, Stanford University, Stanford, CA 94305. 
heeger@white.stanford.edu http://white.stanford.edu ySRI David Sarnoff Research Center, Princeton, NJ 
08544. jrb@sarnoff.com Permission to make digital/hard copy of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage, the copyright notice, the title of the publication and its date appear, and notice 
is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 there is no distortion because there is no mapping. However, exist­ing techniques for synthesizing 
solid textures can be quite cumber­some. One must learn how to tweak the parameters or procedures of 
the texture synthesizer to get a desired effect. This paper presents a technique for synthesizing an 
image (or solid texture) that matches the appearance of a given texture sample. The key advantage of 
this technique is that it works entirely from the example texture, requiring no additional information 
or adjustment. The technique starts with a digitized image and analyzes it to com­pute a number of texture 
parameter values. Those parameter val­ues are then used to synthesize a new image (of any size) that 
looks (in its color and texture properties) like the original. The analysis phaseis inherentlytwo-dimensionalsincetheinputdigitizedimages 
are 2-d. The synthesis phase, however, may be either two-or three­dimensional. For the 3-d case, the 
output is a solid texture such that planar slices through the solid look like the original scanned image. 
In either case, the (2-d or 3-d) texture is synthesized so that it tiles seamlessly. 2 Texture Models 
Textures have often been classi.ed into two categories, determinis­tic textures and stochastic textures. 
A deterministic texture is char­acterized by a set of primitives and a placement rule (e.g., a tile .oor). 
A stochastic texture, on the other hand, does not have easily identi.able primitives (e.g., granite, 
bark, sand). Many real-world textures have some mixture of these two characteristics (e.g. woven fabric, 
woodgrain, plowed .elds). Much of the previous work on texture analysis and synthesis can be classi.ed 
according to what type of texture model was used. Some of the successful texture models include reaction­diffusion 
[31, 34], frequency domain [17], fractal [9, 18], and sta­tistical/random .eld [1, 6, 8, 10, 12, 13, 
21, 26] models. Some (e.g., [10]) have used hybrid models that include a deterministic (or pe­riodic) 
component and a stochastic component. In spite of all this work, scanned images and hand-drawn textures 
are still the princi­ple source of texture maps in computer graphics. This paper focuses on the synthesis 
of stochastic textures. Our approach is motivated by research on human texture perception. Current theories 
of texture discrimination are based on the fact that two textures are often dif.cult to discriminate 
when they produce a similar distribution of responses in a bank of (orientation and spatial-frequency 
selective) linear .lters [2, 3, 7, 16, 20, 32]. The method described here, therefore, synthesizes textures 
by match­ing distributions (or histograms) of .lter outputs. This approach de­pends on the principle 
(not entirely correct as we shall see) that all of the spatial information characterizing a texture image 
can be cap­tured in the .rst order statistics of an appropriately chosen set of lin­ear .lter outputs. 
Nevertheless, this model (though incomplete) cap­tures an interesting set of texture properties. Computational 
ef.ciency is one of the advantages of this approach compared with many of the previous texture analy­sis/synthesis 
systems. The algorithm involves a sequence of simple image processing operations: convolution, subsampling, 
upsam­pling, histograming, and nonlinear transformations using small lookup tables. These operations 
are fast, simple to implement, and amenable to special purpose hardware implementations (e.g., using 
DSP chips). 3 Pyramid Texture Matching The pyramid-based texture analysis/synthesis technique starts 
with an input (digitized) texture image and a noise image (typically uni­form white noise). The algorithm 
modi.es the noise to make it look like the input texture (.gures 2, 3, 4). It does this by making use 
of an invertible image representation known as an image pyramid, along with a function, match-histogram, 
that matches the his­togramsoftwoimages. Wewillpresentexamplesusingtwotypesof pyramids: the Laplacian 
pyramid (a radially symmetric transform) and the steerable pyramid (an oriented transform). 3.1 Image 
Pyramids A linear image transform represents an image as a weighted sum of basis functions. That is, 
the image, I(x,y), is represented as a sum over an indexed collection of functions, gi(x,y): X I(x,y)= 
yig(x,y), i i where yiare the transform coef.cients. These coef.cients are com­puted from the signal 
by projecting onto a set of projection func­tions, hi(x,y): X yi=hi(x,y)I(x,y). x,y For example, the 
basis functions of the Fourier transform are sinu­soids and cosinusoids of various spatial frequencies. 
The projection functions of the Fourier transform are also (co-)sinusoids. In many image processing applications, 
an image is decomposed into a set of subbands, and the information withing each subband is processed 
more or less independently of that in the other subbands. The subbands are computed by convolving the 
image with a bank of linear .lters. Each of the projection functions is a translated (or shifted) copy 
of one of the convolution kernels (see [28] for an in­troduction to subband transforms and image pyramids). 
An image pyramid is a particular type of subband transform. The de.ning characteristic of an image pyramid 
is that the ba­sis/projection functions are translated and dilated copies of one an­other (translated 
and dilated by a factor or 2jfor some integer j). The subbands are computed by convolving and subsampling. 
For each successive value of j, the subsampling factor is increased by a factor of 2. This yields a set 
of subband images of different sizes (hence the name image pyramid) that correspond to different fre­quency 
bands. In an independent context, mathematicians developed a form of continuous function representation 
called wavelets (see [30] for an introduction to wavelets), that are very closely related to image pyramids. 
Both wavelets and pyramids can be implemented in an ef.cient recursive manner, as described next. Laplacian 
Pyramid. The Laplacian pyramid [4, 5, 22] is com­puted using two basic operations: reduce and expand.The 
reduceoperation applies a low-pass .lter and then subsamples by a factor of two in each dimension. The 
expand operation upsam­ples by a factor of two (padding with zeros in between pixels) and then applies 
the same low-pass .lter. A commonly used low-pass .lter kernel (applied separably to the rows and columns 
of an im­ 1 age) is: 16(1,4,6,4,1). One complete level of the pyramid consists of two images, l0(a low-pass 
image), and b0(a high-pass image), that are computed as follows: l0 = Reduce(im) b0 = im -Expand(l0), 
 where im is the original input image. Note that the original image can be trivially reconstructed from 
l0and b0: reconstructed-im = b0 + Expand(l0). The next level of the pyramid is constructed by applying 
the same set of operations to the l0image, yielding two new images, l1and b1 . The full pyramid is constructed 
(via the make-pyramidfunction) bysuccessivelysplitting thelow-passimage liinto two new images, li+1(a 
new low-pass image) and bi+1(a new band-pass image). The combined effect of the recursive low-pass .ltering 
and sub/upsampling operations yields a subband transform whose basis functions are (approximately) Gaussian 
functions. In other words, the transform represents an image as a sum of shifted, scaled, and dilated 
(approximately) Gaussian functions. The projection func­tions of this transform are (approximately) Laplacian-of-Gaussian 
(mexican-hat) functions, hence the name Laplacian pyramid. Note that the pyramid is not computed by convolving 
the image directly with the projection functions. The recursive application of the reduce and expand 
operations yields the same result, but much more ef.ciently. In the end, we get a collection of pyramid 
subband images con­sisting of several bandpass images and one leftover lowpass im­age. These images have 
different sizes because of the subsam­pling operations; the smaller images correspond to the lower spa­tial 
frequency bands (coarser scales). Note that the original image can always be recovered from the pyramid 
representation (via the collapse-pyramid function) by inverting the sequence of op­erations, as exempli.ed 
above. Steerable Pyramid. Textures that have oriented or elongated structures are not captured by the 
Laplacian pyramid analysis be­cause its basis functions are (approximately) radially symmetric. To synthesize 
anisotropic textures, we adopt the steerable pyra­mid transform [25, 29]. Like the Laplacian pyramid, 
this transform decomposestheimageintoseveralspatialfrequencybands. Inaddi­tion, it further divides each 
frequency band into a set of orientation bands. The steerable pyramid was used to create all of the images 
in this paper. The Laplacian pyramid was used (in addition to the steerable pyramid, see Section 4) for 
synthesizing the solid textures shown in .gure 5. Figure 1(a) shows the analysis/synthesis representation 
of the steerable pyramid transform. The left-hand side of the diagram is the analysis part (make-pyramid) 
and the right hand side is the synthesis part (collapse-pyramid). The circles in between represent the 
decomposed subband images. The transform begins with a high-pass/low-pass split using a low-pass .lter 
with a radially symmetric frequency response; the high-pass band corresponds to the four corners of the 
spatial frequency domain. Each successive level of the pyramid is constructed from the previous level 
s low­pass band by a applying a bank of band-pass .lters and a low-pass .lter. The orientation decomposition 
at each level of the pyramid is steerable [11], that is, the response of a .lter tuned to any orienta­tion 
can be obtained through a linear combination of the responses H0 H0   b c  Figure 1: (a) System diagram 
for the .rst level of the steerable pyra­mid. Boxes represent .ltering and subsampling operations: H0is 
a high-pass .lter, L0and Liare low-pass .lters, and Biare oriented bandpass .lters. Circles in the middle 
represent the decomposed subbands. Successive levels of the pyramid are computed by apply­ing the Biand 
L1.ltering and subsampling operations recursively (represented by etc. at the bottom). (b) Several basis/projection 
functions of the steerable pyramid. Note that these are not the Bi .lters, although the Bi.lters do look 
similar to the top row of ba­sis/projection functions. (c) Input image. (d) Steerable pyramid subband 
images for this input image. of the four basis .lters computed at the same location. The steerabil­ity 
property is important because it implies that the pyramid repre­sentation is locally rotation-invariant. 
The steerable pyramid, unlike most discrete wavelet transforms used in image compression algorithms, 
is non-orthogonal and over­complete; the number of pixels in the pyramid is much greater than the number 
of pixels in the input image (note that only the low-pass band is subsampled). This is done to minimize 
the amount of alias­ing within each subband. Avoiding aliasing is critical because the pyramid-basedtexture 
analysis/synthesisalgorithm treats each sub­band independently. The steerable pyramid is self-inverting; 
the .lters on the synthe­sis side of the system diagram are the same as those on the analysis side of 
the diagram. This allows the reconstruction (synthesis side) to be ef.ciently computed despite the non-orthogonality. 
Although the steerable pyramid .lter kernels are nonseparable, any nonseparable .lter can be approximated 
(often quite well) by a sum of several separable .lter kernels [25]. Using these separable .lter approximations 
would further increase the computational ef.­ciency. A C code implementation of the steerable pyramid 
is available at http://www.cis.upenn.edu/ eero/home.html. Psychophysical and physiological experiments 
suggest that im­age information is represented in visual cortex by orientation and spatial-frequency 
selective .lters. The steerable pyramid captures some of the oriented structure of images similar to 
the way this in­formation is represented in the human visual system. Thus, textures synthesized with 
the steerable pyramid look noticeably better than those synthesized with the Laplacian pyramid or some 
other non­oriented representation. Other than the choice of pyramid, the algo­rithm is exactly the same. 
 3.2 Histogram Matching Histogram matching is a generalization of histogram equalization. The algorithm 
takes an input image and coerces it via a pair of lookup tables to have a particular histogram. The two 
lookup tables are: (1) the cumulative distribution function (cdf) of one image, and (2) the inverse cumulative 
distribution function (inverse cdf) of the other image. An image s histogram is computed by choosing 
a bin­size (we typically use 256 bins), counting the number of pixels that fall into each bin, and dividing 
by the total number of pixels. An image s cdf is computed from its histogram simply by accumulat­ing 
successive bin counts. The cdf is a lookup table that maps from the interval [0,256] to the interval 
[0,1]. The inverse cdf is a lookup table that maps back from [0,1] to [0,256]. It is constructed by resampling 
(with linear interpolation) the cdf so that its samples are evenly spaced on the [0,1] interval. These 
two lookup tables are used by the match-histogram function to modify an image (im1) to have the same 
histogram as another image (im2): Match-histogram (im1,im2) im1-cdf = Make-cdf(im1) im2-cdf = Make-cdf(im2) 
 inv-im2-cdf = Make-inverse-lookup-table(im2-cdf) Loop for each pixel do im1[pixel] = Lookup(inv-im2-cdf, 
 Lookup(im1-cdf,im1[pixel]))  3.3 Texture Matching The match-texture function modi.es an input noise 
image so that it looks like an input texture image. First, match the his­togram of the noise image to 
the input texture. Second, make pyra­mids from both the (modi.ed) noise and texture images. Third, loop 
through the two pyramid data structures and match the histograms of each of the corresponding pyramid 
subbands. Fourth, collapse the (histogram-matched) noise pyramid to generate a preliminary ver­sion of 
the synthetic texture. Matching the histograms of the pyra­mid subbands modi.es the histogram of the 
collapsed image. In or­der to get both the pixel and pyramid histograms to match we iter­ate, rematching 
the histograms of the images, and then rematching the histograms of the pyramid subbands. Match-texture(noise,texture) 
 Match-Histogram (noise,texture) analysis-pyr = Make-Pyramid (texture) Loop for several iterations 
do synthesis-pyr = Make-Pyramid (noise) Loop for a-band in subbands of analysis-pyr for s-band in 
subbands of synthesis-pyr do Match-Histogram (s-band,a-band) noise = Collapse-Pyramid (synthesis-pyr) 
 Match-Histogram (noise,texture)  Whenever an iterative scheme of this sort is used there is a con­cern 
about convergence. In the current case we have not formally investigated the convergence properties of 
the iteration, but our ex­perience is that it always converges. However, stopping the algo­rithm after 
several (5 or so) iterations is critical. As is the case with nearly all discrete .lters, there are tradeoffs 
in the design of the steerable pyramid .lters (e.g., .lter size versus reconstruction ac­curacy). Since 
the .lters are not perfect, iterating too many times introduces artifacts due to reconstruction error. 
The core of the algorithm is histogram matching which is a spa­tially local operation. How does this 
spatially local operation repro­duce the spatial characteristics of textures? The primary reason is that 
histogram matching is done on a representation that has intrin­sic spatial structure. A local modi.cation 
of a value in one of the pyramid subbands produces a spatially correlated change in the re­constructed 
image. In other words, matching the pointwise statistics of the pyramid representation does match some 
of the spatial statis­tics of the reconstructed image. Clearly, only spatial relationships that are represented 
by the pyramid basis functions can be captured in this way so the choice of basis functions is critical. 
As mentioned above, the steerable pyramid basis functions are a reasonably good model of the human visual 
system s image representation. If we had a complete model of human texture perception then we could presumably 
synthesize perfect texture matches. By anal­ogy, our understanding of the wavelength encoding of light 
in the retina allows us to match the color appearance of (nearly) any color image with only three colored 
lights (e.g., using an RGB monitor). Lights can be distinguished only if their spectral compositions 
dif­fer in such a way as to produce distinct responses in the three pho­toreceptor classes. Likewise, 
textures can be distinguished only if their spatial structures differ in such a way as to produce distinct 
re­sponses in the human visual system. 3.4 Edge Handling Proper edge handling in the convolution operations 
is important. For the synthesis pyramid, use circular convolution. In other words, for an image I(x,y)of 
size NxN, de.ne: I(x,y). I(xmodN,ymodN). Given that the synthesis starts with a ran­dom noise image, 
circular convolution guarantees that the resulting synthetic texture will tile seamlessly. For the analysis 
pyramid, on the other hand, circular convolution would typically result in spuriously large .lter responses 
at the im­age borders. This would, in turn, introduce artifacts in the synthe­sized texture. A reasonable 
border handler for the analysis pyramid is to pad the image with a re.ected copy of itself. Re.ecting 
at the border usually avoids spurious responses (except for obliquely ori­ented textures). 3.5 Color 
The RGB components of a typical texture image are not indepen­dent of one another. Simply applying the 
algorithm to R, G, and B separately would yield color artifacts in the synthesized texture. Instead, 
color textures are analyzedby .rst transforming the RGB values into a different color space. The basic 
algorithm is applied to each transformed color band independently producing three syn­thetic textures. 
These three textures are then transformed back into the RGB color space giving the .nal synthetic color 
texture. The color-space transformation must be chosen to decorrelate the color bands of the input texture 
image. This transformation is com­puted from the input image in two steps. The .rst step is to sub­tract 
the mean color from each pixel. That is, subtract the average of the red values from the red value at 
each pixel, and likewise for the green and blue bands. The resulting color values can be plotted as points 
in a three-dimensional color space. The resulting 3-d cloud of points is typically elongated in some 
direction, but the elongated direction is typically not aligned with the axes of the color space. The 
second step in the decorrelating color transform rotates the cloud so that its principle axes align with 
the axes of the new color space. The transform can be expressed as a matrix multiplication, y=Mx,where 
xis the RGB color (after subtracting the mean) of a particular pixel, yis the transformed color, and 
Mis a 3x3 matrix. The decorrelating transform Mis computed from the covariance matrix Cusing the singular-value-decomposition 
(SVD). Let Dbe a 3xN matrix whose columns are the (mean-subtracted) RGB val­ues of each pixel. The covariance 
matrix is: C=DDt,where Dtmeans the transpose of D. The SVD algorithm algorithm de­composes the covariance 
matrix into the product of three compo­nents, C=US2Ut. Here, Uis an orthonormal matrix and S2is a diagonal 
matrix. These matrices (C, Uand S2) are each 3x3, so the SVD can be computed quickly. The decorrelating 
transform is: S .1 M=Ut,where Sis a diagonal matrix obtained by taking the square-root of the elements 
of S2 . After applying this color transform, the covariance of the trans­formed color values is the identity 
matrix. Note that the transformed S .1Ut color values are: MD=USVt =Vt. It follows that the covariance 
of the transformed color values is: VtV=I. The color transform is inverted after synthesizing the three 
tex­ture images in the transformed color space. First, multiply the syn­thetic texture s color values 
at each pixel by M.1 . This produces three new images (color bands) transformed back into the (mean subtracted) 
RGB color space. Then, add the corresponding mean values (the means that were subtracted from the original 
input tex­ture) to each of these color bands.  4 Solid Textures Pyramid-based texture analysis/synthesis 
can also be used to make isotropic 3-d solid textures. We start with an input image and a block of 3-d 
noise. The algorithm coerces the noise so that any slice through the block looks like the input image. 
The solid texture synthesis algorithm is identical to that described above, except for the choice of 
pyramid: use a 2-d Laplacian pyra­mid for analysis and a 3-d Laplacian pyramid for synthesis. As usual, 
match the histograms of the corresponding subbands. Note that since the Laplacian pyramid is constructed 
using separable con­volutions, it extends trivially to three-dimensions. We have obtained better looking 
results using a combination of Laplacian and steerable pyramids. On the analysis side, construct a 2-d 
Laplacian pyramid and a 2-d steerable pyramid. On the synthe­sis side, construct a 3-d Laplacian pyramid 
and construct steerable pyramids from all two-dimensional (x-y, x-z, and y-z) slices of the solid. Match 
the histograms of the 3-d (synthesis) Laplacian pyra­mid to the corresponding histograms of the 2-d (analysis) 
Lapla­cian pyramid. Match the histograms of each of the many synthe­sis steerable pyramids to the corresponding 
histograms of the analy­sis steerable pyramid. Collapsing the synthesis pyramids gives four solids (one 
from the 3-d Laplacian pyramid and one from each set of steerable pyramids) that are averaged together. 
Some examples are shown in .gure 5. 5 Texture Mixtures Figure 6 shows some texture mixtures that were 
synthesized by choosing the color palette (decorrelating color transform) from one image and the pattern 
(pyramid subband statistics) from a second image. One can imagine a number of other ways to mix/combine 
textures to synthesize an image that looks a bit like each of the inputs: ap­ply match-texture to a second 
image rather than noise, com­bine the high frequencies of one texture with the low frequencies of another, 
combine two or more textures by averaging their pyramid histograms, etc. 6 Limitations and Extensions 
The approach presented in this paper, like other texture synthesis techniques, has its limitations. The 
analysis captures some but not all of the perceptually relevant structure of natural textures. Hence, 
this approach should be considered one of many tools for texturing objects in computer graphics. It is 
critical that the input image be a homogeneous texture. Fig­ure 7 shows two input textures (cropped from 
different areas of the same photoCD image) and two corresponding synthetic textures. Whenthe inputis 
inhomogeneous(dueto anintensity gradient,con­trast gradient, perspective distortion, etc.) then the synthesized 
tex­ture has a blotchy appearance. The approach also fails on quasi-periodic textures and on ran­dom 
mosaic textures (.gure 8). Although the results look inter­esting, they do not particularly resemble 
the inputs. We have had some success synthesizing quasi-periodic textures using a hybrid scheme (e.g., 
like [10]) that combines a periodic texture model with the pyramid decomposition. Methods that are speci.cally 
designed to capture long range statistical correlation [26] have also been suc­cessful with textures 
of this type. The issue with random mosaic textures is mainly one of scale. If the repeated micro-patterns 
are small enough, then the pyramid analysis/synthesis scheme works well (e.g., see the ivy example in 
.gure 3). Figure 9 shows more examples of failures. There are two as­pects of these images that the pyramid 
texture model misses. First, these textures are locally oriented but the dominant orientation is different 
in different parts of the image. In a sense, they are inho­mogeneous with respect to orientation. Second, 
they contain ex­tended, .ne structure (correlations of high frequency content over large distances). 
The pyramid scheme captures correlations of low frequency content over large distances, but it captures 
correlations of high frequency content only over very short distances. There is no general way to construct 
an anisotropic solid tex­ture from a 2-d sample. However, there are several options includ­ing: (1) constructing 
a solid texture as the outer product of a 2­d anisotropic color texture image and a 1-d (monochrome) 
signal; (2) composing (adding, multiplying, etc.) several solid textures as Peachy [23] did; (3) starting 
with an isotropic solid, and introducing anisotropy procedurally, like Perlin s marble [24] and Lewis 
wood­grain [19]; (4) starting with an isotropic solid, and using a paint pro­gram to introduce anisotropic 
touch-ups . Image pyramids and multi-scale image representations of one sort or another are the most 
often used data structures for antialiased texture mapping (e.g., Renderman, Silicon Graphics Iris GL, 
Gen­eral Electric and E&#38;S realtime .ight simulators, and reference [33]). Pyramid-based texture synthesis, 
therefore, can be naturally inte­grated into an antialiased texture mapping system. Finally, it may be 
possible to write an interactive tool for texture synthesis, with a slider for each parameter in the 
pyramid represen­tation. In our current implementation, each subband histogram is encoded with 256 bins. 
However the subband histograms of many natural images have a characteristic shape [27], suggesting that 
a very small number of parameters may be suf.cient. 7 Conclusion This paper presents a technique for 
created a two-or three­dimensional (solid) texture array that looks like a digitized texture image. The 
advantage of this approach is its simplicity; you do not have to be an artist and you do not have to 
understand a complex texture synthesis model/procedure. You just crop a textured region from a digitized 
image and run a program to produce as much of that texture as you want. Acknowledgements: The teapot 
images were rendered using Rayshade. Many of the source texture images were cropped from photoCDs distributed 
by Pixar and Corel. Special thanks to Eero Simoncelli for designing the .lters for the steerable pyramid, 
to Patrick Teo for writing a solid texturing extension to Rayshade, to Alex Sherstinsky for suggesting 
the solid texturing application, to Marc Levoy for his help and encouragement, and to Charlie Chubb and 
Mike Landy for stimulating discussions. Supported by an NIMH grant (MH50228), an NSF grant (IRI9320017), 
and an Al­fred P. Sloan Research Fellowship to DJH.  References [1] BENNIS,C., AND GAGALOWICZ, A. 2-D 
Macroscopic Tex­ture Synthesis. Computer GraphicsForum 8 (1989), 291 300. [2] BERGEN, J. R. Theories 
of Visual Texture Perception. In Spatial Vision, D. Regan, Ed. CRC Press, 1991, pp. 114 133. [3] BERGEN,J.R., 
ANDADELSON,E.H.EarlyVisionandTex­ture Perception. Nature 333 (1988), 363 367. [4] BURT, P. Fast Filter 
Transforms for Image Processing. Com­puter Graphics and Image Processing 16 (1981), 20 51. [5] BURT,P.J., 
AND ADELSON, E. H. A Multiresolution Spline with Application to Image Mosaics. ACM Transactions on Graphics 
2 (1983), 217 236. [6] CHELLAPPA,R., AND KASHYAP, R. L. Texture Synthesis Using 2-D Noncausal Autoregressive 
Models. IEEE Transac­tions on Acoustics, Speech, and Signal Processing 33 (1985), 194 203. [7] CHUBB,C., 
AND LANDY, M. S. Orthogonal Distribution Analysis: A New Approach to the Study of Texture Percep­tion. 
In Computational Models of Visual Processing,M. S. Landy and J. A. Movshon, Eds. MIT Press, Cambridge, 
MA, 1991, pp. 291 301. [8] CROSS,G.C., ANDJAIN,A.K.MarkovRandomFieldTex­ture Models. IEEE Transactions 
on Pattern Analysis and Ma­chine Intelligence 5 (1983), 25 39. [9] FOURNIER,A., FUSSEL,D., AND CARPENTER,L. 
Com­puter Rendering of Stochastic Models. Communications of the ACM 25 (1982), 371 384. [10] FRANCOS,J. 
M., MEIRI,A. Z., AND PORAT,B. A Uni­.ed Texture Model Based on a 2D Wold-Like Decomposition. IEEE Transactions 
on Signal Processing 41 (1993), 2665 2678. [11] FREEMAN,W. T., AND ADELSON, E. H. The Design and Use 
of Steerable Filters. IEEE Transactions on Pattern Anal­ysis and Machine Intelligence 13 (1991), 891 
906. [12] GAGALOWICZ, A. Texture Modelling Applications. The Vi­sual Computer 3 (1987), 186 200. [13] 
GAGALOWICZ,A., AND MA, S. D. Sequential Synthesis of Natural Textures. Computer Vision, Graphics, and 
Image Pro­cessing 30 (1985), 289 315. [14] HANRAHAN,P., AND HAEBERLI, P. Direct WYSIWYG Painting and 
Texturing of 3D Shapes. Proceedings of SIG-GRAPH 90. In Computer Graphics (1990), vol. 24, ACM SIGGRAPH, 
pp. 215 223. [15] HECKBERT, P. S. Survey of Texture Mapping. IEEE Com­puter Graphics and Applications 
6 (1986), 56 67. [16] LANDY,M.S., ANDBERGEN,J.R.TextureSegregationand Orientation Gradient. Vision Research 
31 (1991), 679 691. [17] LEWIS, J. P. Texture Synthesis for Digital Painting. Pro­ceedings of SIGGRAPH 
84. In Computer Graphics (1984), vol. 18, ACM SIGGRAPH, pp. 245 252. [18] LEWIS, J. P. Generalized Stochastic 
Subdivision. ACM Transactions on Graphics 6 (1987), 167 190. [19] LEWIS, J. P. Algorithms for Solid Noise 
Synthesis. Pro­ceedings of SIGGRAPH 89. In Computer Graphics (1989), vol. 23, ACM SIGGRAPH, pp. 263 270. 
[20] MALIK,J., AND PERONA, P. Preattentive Texture Discrimi­nation with Early Vision Mechanisms. Journal 
of the Optical Society of America A 7 (1990), 923 931. [21] MALZBENDER,T., AND SPACH, S. A Context Sensitive 
Texture Nib. In Communicating with Virtual Worlds,N. M. Thalmann and D. Thalmann, Eds. Springer-Verlag, 
New York, 1993, pp. 151 163. [22] OGDEN,J. M., ADELSON,E. H., BERGEN,J. R., AND BURT, P. J. Pyramid-Based 
Computer Graphics. RCA En­gineer 30 (1985), 4 15. [23] PEACHY, D. R. Solid Texturing of Complex Surfaces. 
Pro­ceedings of SIGGRAPH 85. In Computer Graphics (1985), vol. 19, ACM SIGGRAPH, pp. 279 286. [24] PERLIN, 
K. An Image Synthesizer. Proceedings of SIG-GRAPH 85. In Computer Graphics (1985), vol. 19, ACM SIGGRAPH, 
pp. 287 296. [25] PERONA, P. Deformable Kernels for Early Vision. IEEE Transactions on Pattern Analysis 
and Machine Intelligence (1995). To appear May 1995. [26] POPAT,K., AND PICARD, R. W. Novel Cluster-Based 
Prob­ability Model for Texture Synthesis, Classi.cation, and Com­pression. In Proceedings of SPIE Visual 
Communications and Image Processing (1993), pp. 756 768. [27] RUDERMAN,D. L., AND BIALEK, W. Statistics 
of Natural Images: Scaling in the Woods. Physical Review Letters 73 (1994), 814 817. [28] SIMONCELLI,E. 
P., AND ADELSON, E. H. Subband Trans­forms. In Subband Image Coding, J. W. Woods, Ed. Kluwer Academic 
Publishers, Norwell, MA, 1990. [29] SIMONCELLI,E. P., FREEMAN,W.T., ADELSON,E. H., AND HEEGER, D. J. 
Shiftable Multi-Scale Transforms. IEEE Transactions on Information Theory, Special Issue on Wavelets 
38 (1992), 587 607. [30] STRANG, G. Wavelets and Dilation Equations: A Brief Intro­duction. SIAM Review 
31 (1989), 614 627. [31] TURK, G. Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion. 
Proceedings of SIGGRAPH 91. In Com­puter Graphics (1991), vol. 25, ACM SIGGRAPH, pp. 289 298. [32] TURNER,M.R.TextureDiscriminationbyGaborFunctions. 
Biological Cybernetics 55 (1986), 71 82. [33] WILLIAMS, L. Pyramidal Parametrics. Proceedings of SIG-GRAPH 
83. In Computer Graphics (1983), vol. 17, ACM SIGGRAPH, pp. 1 11. [34] WITKIN,A., AND KASS, M. Reaction-Diffusion 
Tex­tures. Proceedings of SIGGRAPH 91. In Computer Graphics (1991), vol. 25, ACM SIGGRAPH, pp. 299 308. 
 Figure2:(Left)Inputdigitizedsampletexture:burledmappawood.(Middle)Inputnoise.(Right)Outputsynthetictexture 
thatmatchestheappearanceofthedigitizedsample.Notethatthesynthesizedtextureislargerthanthedigitizedsample; 
ourapproachallowsgenerationofasmuchtextureasdesired.Inaddition,thesynthetictexturestileseamlessly. Figure3:Ineachpairleftimageisoriginalandrightimageissynthetic:stucco,iridescentribbon,greenmarble,pandafur, 
slagstone,fguredyewwood. Figure4.Ineachpairleftimageisoriginalandrightimageissynthetic.redgravel..guredsepelewood.brocolli.barkpaper. 
denim.pinkwall.ivy.grass.sand.surf. Figure5:(TopRow)Originaldigitizedsampletextures:redgranite,berrybush,fguredmaple,yellowcoral.(BottomRows) 
Syntheticsolidtexturedteapots. Figure6.Texturemixturessynthesizedbychoosingthecolorpalettefromoneimageandthepatternfromasecondimage. 
 Figure7..Leftpair.Inhomogoneousinputtextureproducesblotchysynthetictexture..Rightpair.Homogenousinput. 
 Figure8.Examplesoffailures.woodgrainandredcoral. Figure9.Morefailures.hayandmarble. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218447</article_id>
		<sort_key>239</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Cellular texture generation]]></title>
		<page_from>239</page_from>
		<page_to>248</page_to>
		<doi_number>10.1145/218380.218447</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218447</url>
		<keywords>
			<kw><![CDATA[bump mapping]]></kw>
			<kw><![CDATA[constraints]]></kw>
			<kw><![CDATA[data amplification]]></kw>
			<kw><![CDATA[developmental models]]></kw>
			<kw><![CDATA[displacement mapping]]></kw>
			<kw><![CDATA[particle systems]]></kw>
			<kw><![CDATA[texture mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39076360</person_id>
				<author_profile_id><![CDATA[81332499016]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Fleischer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P62493</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Laidlaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P28731</person_id>
				<author_profile_id><![CDATA[81100456871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bena]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Currin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14034821</person_id>
				<author_profile_id><![CDATA[81100070192]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[James Arvo and David Kirk. Modeling plants with environmentsensitive automata. In Proceedings of Ausgraph '88, pages 27-33, 1988.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>140548</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ronen Barzel. Physically-Based Modeling: A Structured Approach. Academic Press, Cambridge, MA, 1992.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808602</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook. Shade trees. In Computer Graphics(SIGGRAPH '84 Proceedings), volume 18, pages 223-231, July 1984.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801253</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E C. Crow. A more flexible image generation environment. In Computer Graphics (SIGGRAPH '82 Proceedings), volume 16, pages 9- 18, July 1982.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Jody Duncan. The making of a rockbuster. Cinefex: the Journal of Cinematic Illusions, 58:34-65, June 1994.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Kurt Fleischer. Cells: Simulations of multicellular development. In Siggraph Video Review, 1994. A video presented at Siggraph 94.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>922416</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Kurt W. Fleischer. A Multiple-Mechanism Developmental Model for Defining Self-Organizing Structures. PhD dissertation, Caltech, Department of Computation and Neural Systems, June 1995.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kurt W. Fleischer and Alan H. Bart. A simulation testbed for the study of multicellular development: The multiple mechanisms of morphogenesis. In Artificial Life III. Addison-Wesley, 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134096</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Deborah R. Fowler, Hans Meinhardt, and Przemyslaw Prusinkiewicz. Modeling seashells. In Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 379-388, July 1992.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134093</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Deborah R. Fowler, Przemyslaw Prusinkiewicz, and Johannes Battjes. A collision-based model of spiral phyllotaxis. In Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 361-368, July 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>540426</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[C.W. Gear. Numerical Initial Value Problems in Ordinary Differential Equations. Prentice-Hall, Englewood Cliffs, NJ, 1971.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Goldstein. Classical Mechanics. Addison-Wesley, 1980.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74351</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ned Greene. Voxel space automata: Modeling with stochastic growth processes in voxel space. In Computer Graphics (SIGGRAPH '89 Proceedings), volume 23, pages 175-184, July 1989.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325167</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya. Anisotropic reflection models. In Computer Graphics (SIGGRAPH '85 Proceedings), volume 19, pages 15-21, July 1985.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Timothy L. Kay. Rendering fur with three dimensional textures. In Computer Graphics (SIGGRAPH '89 Proceedings), volume 23, pages 271-280, July 1989.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Hans Meinhardt. Models of Biological Pattern Formation. Academic Press, London, 1982.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Gavin Miller and Andrew Pearce. Globular dynamics: A connected particle system for animating viscous fluids. Computers and Graphics, 13(3):305-309,1989.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[J.D. Murray. MathematicalBiology. Springer-Verlag, New York, 2nd edition, 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[B. N. Nagorcka, V. S. Manoranjan, and J. D. Murray. Complex spatial patterns from tissue interactions - an illustrative model. Journal of Theoretical Biology, 128:359-374, 1987.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Garrett M. Odell, George Oster, R Alberch, and B. Burnside. The mechanical basis of morphogenesis. DevelopmentalBiology, 85, 1981.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192229</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Hans Kohling Pedersen. Displacement mapping using flow fields. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29,1994), pages 279-286. ACM Press, July 1994.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>76524</ref_obj_id>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[John Platt. Constraint Methods for Neural Networks and Computer Graphics. PhD dissertation, Caltech, Department of Computer Science, Pasadena, CA, 91125, 1989.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192254</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz, Mark James, and Radomi( M6ch. Synthetic topiary. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29,1994), pages 351-358. ACM Press, July 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83596</ref_obj_id>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Przemyslaw Prusinkiewicz and Aristid Lindenmayer. The Algorithmic Beauty ofPlants. Springer-Verlag, New York, 1990.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357320</ref_obj_id>
				<ref_obj_pid>357318</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[W. T. Reeves. Particle systems - a technique for modeling a class of fuzzy objects. ACM Trans. Graphics, 2:91-108,April 1983.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325250</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves and Ricki Blau. Approximate and probabilistic algorithms for shading and rendering structured particle systems. In Computer Graphics (SIGGRAPH '85 Proceedings), volume 19, pages 313-322, July 1985.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Craig W. Reynolds. Flocks, herds, and schools: A distributed behavioral model. In Computer Graphics (SIGGRAPH '87 Proceedings), volume 21, pages 25-34, July 1987.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97923</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Karl Sims. Particle animation and rendering using data parallel computation. In Computer Graphics (SIGGRAPH '90 Proceedings), volume 24, pages 405-413, August 1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122752</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Karl Sims. Artificial evolution for computer graphics. In Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 319-328, July 1991.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808571</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Alvy Ray Smith. Plants, fractals and formal languages. In Computer Graphics (SIGGRAPH '84 Proceedings), volume 18, pages 1-10, July 1984.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130368</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[John Snyder. Generative Modeling for Computer Graphics and CAD: Symbolic Shape Design using Interval Analysis. Academic Press, 1992.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37417</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[John M. Snyder and Alan H. Bart. Ray tracing complex models containing surface tessellations. In Computer Graphics (SIGGRAPH '87 Proceedings), volume 21, pages 119-128, July 1987.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134037</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Richard Szeliski and David Tonnesen. Surface modeling with oriented particle systems. In Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 185-194, July 1992.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, John Platt, and Kurt Fleischer. From goop to glop: Heating and melting deformable models. In Graphics Interface 89, 1989.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Alan Turing. The chemical basis of morphogenesis. Phil. Trans. B., 237, 1952.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Generating textures for arbitrary surfaces using reactiondiffusion. In Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 289-298, July 1991.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-tiling polygonal surfaces. In Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 55-64, July 1992.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134075</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Stephen H. Westin, James R. Arvo, and Kenneth E. Torrance. Predicting reflectance functions from complex surfaces. In Computer Graphics (SIGGRAPH '92 Proceedings), volume 26, pages 255-264, July 1992.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37429</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin, Kurt Fleischer, and Alan Bart. Energy constraints on parameterized models. In Computer Graphics (SIGGRAPH '87 Proceedings), volume 21, pages 225-232, July 1987.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122750</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Andrew Witkin and Michael Kass. Reaction-diffusion textures. In Computer Graphics (SIGGRAPH '91 Proceedings), volume 25, pages 299-308, July 1991.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Andrew R Witkin and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29,1994), pages 269-278. ACM Press, July 1994.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cellular Texture Generation Kurt W. Fleischer David H. Laidlaw Bena L. Currin Alan H. Barr California 
Institute of Technology Pasadena, CA 91125 email: fkurt, dhl, bena, barrg@druggist.gg.caltech.edu Abstract 
We propose an approach for modeling surface details such as scales, feathers, or thorns. These types 
of cellular textures require a rep­resentation with more detail than texture-mapping but are inconve­nient 
to model with hand-crafted geometry. We generate patterns of geometric elements using a biolog­ically-motivated 
cellular development simulation together with a constraint to keep the cells on a surface. The surface 
may be de.ned by an implicit function, a volume dataset, or a polygonal mesh. Our simulation combines 
and extends previous work in developmental models and constrained particle systems. Key Words: particle 
systems, developmental models, data am­pli.cation, constraints, texture mapping, bump mapping, displace­ment 
mapping  Introduction For several years computer graphics researchers and practitioners have been grappling 
with the problem of creating and displaying sur­faces having an organic appearance. Texture maps, bump 
maps, and related methods often attain the appearance of detailed geometry without actually creating 
it. These techniques do not suf.ce, how­ever, when the viewpoint is close enough that the three-dimensional 
(3-D) geometric structure of a surface texture is apparent. We are interested in making images of surfaces 
covered with interacting geometric elements, such as scales, feathers, thorns, and fur. We model these 
elements as small 3-D cells constrained to lie on a surface. The cells interact to form cellular textures: 
surface textures with 3-D geometry, orientation, and color. Our approach combines properties of particle 
systems, developmental models, and reaction-diffusion methods into one system. Figure 7 shows an example 
combining all of these approaches. There are a few challenges in making images of these types of materials: 
The geometry is often too pronounced for using texture-or bump-maps. Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Figure 1: Thorny Head: Both .at and thorn-shaped 
cells are con­strained to lie on a surface de.ned by a polygonal dataset of a human head. Flat cells 
are used in the neck and chest regions, while thorn-shaped cells are used on the head. The orientation 
of each thorn approaches that of its neighbors, leading to a continuous .eld of thorns that sweeps across 
the head. The size of the thorns is related to the level of detail of the model; smaller thorns are placed 
on smaller features. It is often dif.cult to map appropriate texture coordinates onto the global geometry 
and topology.  The placement, orientation, coloration, and shape of the in­dividual elements may depend 
on:  neighboring elements,  surface characteristics such as local curvature, or  global phenomena 
such as sunlight.    Cells Geometry Image Figure 2: The cellular particle simulator computes the locations, 
orientations, and other values associated with the cells. This information is converted to geometry and 
appearance parameters, which is then passed to a renderer to create the image. Note that the cell orientations 
(red arrows) become the orientations of the thorns. Using a geometric modeler, we created a geometric 
object that changes shape from a bump to a thorn based on a single parameter [31]. We use the cell state 
variable Sc0to control this parameter (Section 4). Becauseofthe potentially complicatedinterdependenciesofthe 
elements, it is dif.cult to create either geometric or textural models of such objects by hand. So we 
turn to automatic data-ampli.cation techniques, which are similar to the structured particle systems 
used to generate models of plants [26, 30]. Developmental Approach For generating organic patterns, it 
is natural to consider a biologically-based simulation. In previous work [8], we developed a biological 
developmental model to simu­late and study patterns generated by the motions and interactions of discretecells(Figure3). 
Thesearti.cialcellsmoveabout,grow,and divide in a simulated petri dish, the extracellular environment.The 
extracellular environment can contain physical barriers, diffusing chemicals, gravity, etc. The ability 
to form a variety of interesting patterns with the system has prompted us to explore its application 
to geometric texture generation (Figure 1). The textures we model are formed from many interacting geo­metric 
elements. Actual fur, scales, and thorns may be formed from single cells or multiple cells [19]. In either 
case, we assume that the texture patterns arise from the interactions of discrete elements capable of 
movement and orientation change, and model each of these elements as one cell. The patterns are formed 
as the cells ex­perience physical processes of collision, adhesion, and other local interactions. Software 
Structure The approach advocated by this paper is to automatically grow cellular textures by simulating 
discrete cells on surfaces. We then convert the resulting cellular information into model geometry and 
coloration, which is rendered. The images of this paper were generated using oriented, spherical cells, 
which are converted into thorns, scales, and other shapes for rendering (Figure 2). Overview The remainder 
of this paper is structured as follows. Section 2 describes related work. It is followed by an overview 
of the system architecture in Section 3. Section 4 describes the cellular particle system, and includes 
examples of cell programs that implement various behaviors. In Section 5 we discuss the particle converter,whichproducesgeometryfrom 
the cellpositions, orientations and other parameters. Results are presented in Section 6, which describes 
the examples shown in the .gures. The .nal section presents a discussion of the approach and some directions 
for future work. 2 Related Work This approach is a synthesis and extension of work ranging from morphological 
models to general texture mapping. In this section, we discuss our approach in the context of four related 
areas: Levels of Detail  Biologically Motivated Morphogenesis  Reaction-Diffusion Methods  Particle 
Systems  Levels of Detail Choosing the appropriate level of detail for image synthesis at a given viewing 
distance has long been recognized as an important topic in computer graphics [4, 14, 15]. At large scales,geometricmodelsare 
necessary;intermediate scales,texture mapping and similar techniques may be suf.cient; at the smallest 
scales, illumination models suf.ce to describe the microgeometry of the object [38]. The level of detail 
of the models addressed in this paper falls somewhere between the use of hand-crafted geometric models 
and bump-or texture-mapping. A range of geometric levels is available to us because of the modular nature 
of our technique. Complex, oriented textures have been created and rendered in many ways, notably with 
texels [15]. The texel approach is interme­diate between geometry and mapping techniques, but leaves 
open the question of how to arrange the texel elements appropriately. Our approach addresses this problem, 
and can produce models to be rendered using texels. Displacement mapping is another technique for adding 
geomet­ric detail to surfaces [3]. As with texels, the displacement mapping technique does not address 
the problem of determining which dis­placementsarenecessaryto createaspeci.ceffect, suchasa.eldof similarly 
oriented thorns. A possible application of our technique Figure 3: These images demonstrate the pattern 
formation capabil­ities of our 2-D cell simulator [8]. is to create such displacement maps, for example 
by creating .ow .elds [21]. Biologically Motivated Morphogenesis The cellular develop­ment system which 
forms the basis of this work [6, 7, 8] incorporates elements of several established biological models 
of morphogen­esis: Turing s morphogens [35], Odell s mechanical models [20], and Lindenmayer-system cell 
lineage determinants [24], as well as our own model of cell contact and adhesion. Much well-known computer 
graphics work is biologically based. The combination of developmental models with geometric con­straints 
enables the creation of many organic patterns. It has been explored in work on plant growth [13, 23], 
plant organ place­ment [10], and seashell patterning [9]. Interacting geometric elements were used by 
[10] to model the placement of plant organs. Our cells are a generalization of these elements, with many 
additional capabilities, including independent movement, adhesion, and changes in size and orientation 
due to cell-cell interaction. In [9], pigmentation patterns on seashells are modeled using reaction-diffusion 
equations on surfaces de.ned by sweeping a gen­erating curve along a logarithmic spiral. This shares 
with our work the concept of applying pattern formation models on 3-D surfaces. Their use of continuous 
reaction-diffusion equations to generate the patterns differs from our use of discrete cells. For the 
types of cellular texture we are investigating, the choice of a discrete model seems appropriate. Spatially-oriented 
models of plant growth are capable of gener­ating attractive plant images [1, 13]. The placement of geometric 
objects in the environment of plants affects their growth. The impor­tance of combining environmental 
and endogenous mechanisms in forming organic shapes in computer graphics has also been demon­strated 
using environmentally-sensitive L-systems [23], which al­low interaction between the environment and 
the development of a structure de.ned by an L-system. In an application to synthetic topiary, elements 
sense their global position and orientation, and are pruned according to a bounding surface. Our work 
also combines geometric environmental factors with an endogenous developmen­tal model to describe cell 
behavior. We differ from these plant models by the use of discrete motile cells that are able to move 
and rotate independently. Reaction-Diffusion Methods Reaction-diffusion equations were .rst proposed 
as a model for morphogenesis by Turing [35]. They are a continuous approximation to a sheet of many discrete 
cells interacting over time. Our system models discrete cells explicitly, and can generate patterns similar 
to continuous reaction-diffusion equations since it is actually a more detailed model of the same biological 
system. Reaction-diffusion equations have been successfully applied to the generation of texture maps 
[36, 40]. Because they are based on natural phenomena, they have an appealing organic quality. In addition, 
they avoid problems of parameterization and topology by creating the pattern directly on a surface. Our 
approach shares both of these bene.ts. In our 2-D implementation (Figure 3), we include both discrete 
cells and a continuous reaction-diffusion computation. The two modelsarealsoableto interact,sincethediscretecellscansenseand 
emit the continuously diffusing chemicals. The 3-D implementation does not yet support continuously diffusing 
and reacting chemicals. However, we are able to reproduce some forms of reaction-diffusion behavior using 
cell-cell interaction in the discrete model. Particle Systems Early particle systems [25, 28] had little 
or no interparticle interaction, unlike later work based on molecular models and other criteria [17, 
34]. Our work includes elements of Witkin and Heckbert s surface-constrained particles [41], and the 
orientation constraints of Szeliski and Tonneson [33]. Reynold s boids [27] introduced somewhat more 
sophisticated interacting particles with programmable behaviors. In addition, his boids, like our interacting 
cells, can sense and react to each other and to their environment. 3 Software Architecture We arrange 
the process into three software modules (Figure 2). cellular particle simulator with surface constraints: 
computes locations, orientations, sizes and other parameters of cells based on a behavioral speci.cation. 
Allows particles to be constrained to a surface (implicit, polygon mesh, or volume dataset isosurface). 
parameterized particle-to-geometry converter: converts cell po­sitions, orientations and other parameters 
into shape and ap­pearance parameters. renderer: takes shape and appearance parameters plus a scene description 
and renders the scene. The images in this paper were generated with John Snyder s ray tracer [32], which 
was chosen primarily for its speed on large datasets. The implementation of our framework involves the 
addition of cell-cell interactions, orientation constraints and surface constraints to a more traditional 
particle simulator. A simple version of a particle converter can be implemented using a geometric modeler 
to place a geometric object at each particle s location with the appropriate size and orientation. The 
cellular particle simulator and particle converter are described in further detail in the Sections 4 
and 5.  4 Cellular Particle Simulator The cellular particle system combines cell-cell interactions, 
cell­cell adhesion, oriented particles, and surface constraints into one uni.ed framework. Additional 
discussion of the cell simulator and its implementation can be found in [7, 8]. Our system allows the 
user to specify cell behaviors such as go to a surface and align with neighbors by combining modular 
cell programs. Cell programs are .rst-order differential equation terms that modify the cell s state 
(see Table 1). In Section 4.1 we discuss how to use the simulator, and then delve into a more detailed 
mathematical description of the cell pro­grams in Section 4.2. Section 4.3 describes the cell programs 
used to create simulations like those shown in the .gures. Section 4.4 presents methods for incorporating 
various types of surfaces into our surface constraint method. 4.1 Using the Simulator For a particular 
simulation run, the user de.nes the cell state variables,  the extracellular environment, and  the 
cell programs. Theuseralsospeci.esinitial placementsandotherinitial conditions for the cells. Users can 
control the simulation by writing cell programs to describe the behaviors of the cells, and by putting 
surfaces into the environment. More direct interaction is also possible during the simulationprocess. 
Theusercanhaltthesimulation,changethecell programs, and choose individual cells or groups of cells to 
modify or remove. The simulation is then restarted with the modi.cations. It is sometimes convenient 
to freeze certain cell values when they have reached a desirable state. Frozen values remain .xed while 
others continue to vary when the simulation is restarted. Particular seed cells can also be placed and 
frozen in situations where the user wishes to achieve a certain effect. For instance, frozen cells with 
a particular orientation could encourage fur to run in a particular direction on a surface.  4.2 De.nitions 
A cell is an entity that has position, orientation, shape, and an arbitrary length state vector for parameters 
such as chemical con­centrations in a reaction-diffusion simulation. It is a generalization of a particle 
in a particle system. Cell State Variables The state of a cell, S=( p,q,r,Sdie,Ssplit S c0,Sc1,Sc2,···,Sa0,Sa1,Sa2,···) 
is a vector containing values representing position (p), orientation (q), size (r), and concentrations 
of chemicals within the cell (Sci) or in the cell membrane (Sai). The variable Ssplitis used to trigger 
an event, the cell splitting. This occurs when the value of this state variable exceeds a threshold, 
esplit. Sdieis de.ned similarly, with an associated threshold, edie. In real cells, chemicals in the 
cell membranes of adjacent cells can bind together and enable cells to sense that they are in contact. 
The chemicals can also be adhesive. The binding of membrane chemicals is speci.c; some chemicals bind 
in complementary pairs, and others bind to themselves. Our model allows the user to specify the adhesive 
properties of the membrane chemicals, and provides the amount of each bound membrane chemical as an environmental 
parameter (described below). To de.ne a cell s motion, we specify cell programs that con­tribute to p 
0, the viscous force on the cell. This is the attempted motion of the cell, which is further modi.ed 
by the in.uence of collisions, adhesion, and viscous drag. We do not currently com­pute inertial dynamics, 
but instead use viscous dynamics (F = mv), which makes the cells easier to control and predict. Collision 
forces are computed using a polynomial penalty function (kxno where xo is the overlap between two cells). 
We represent cell orientation in 3-D using a quaternion. In the exposition that follows, we sometimes 
refer to the cell s local coordinate frame using the three basis vectors: ex,ey,ez.This is the coordinate 
frame obtained by rotating from the lab frame using quaternion q. Extracellular Environment The cell 
s external environment is a vector of parameters that are provided as an input to the cell programs. 
These parameters describe everything the cell can sense from its current position: A=( .a0,.a1,.a2,···,.p0,r.p0,.p1,r.p1,···, 
Av0,Av1,···, A.x ,A.y,A.z,.u0,.u1,···) The .ai values represent the amounts of membrane chemicals that 
are bound to membrane chemicals on neighboring cells. The value and gradient of a potential .eld, such 
as an implicit function or the concentration of a diffusing chemical, are provided in .pi and r.pi . 
These .elds are evaluated at the current location of the cell, and will generally have different values 
at different locations. Other scalar and vector .elds can be provided in .ui and Avi,which can also be 
functions of position. The orientation of a cell relative to its neighbors is made avail­able to the 
cell programs in the vectors A.i . This vector describes the rotation that would align this cell s ei 
axis with the average orientation of the adjacent cells. This parameter is used to align the orientations 
of cells, as shown in Figure 8(a) and others. The direction A.i speci.es the axis of rotation, and the 
magnitude spec­i.es the rotation angle (similar to angular velocity). As an example, consider the computation 
of the average relative x-axis for a cell b, computed as a sum over neighboring cells c: bc X -1(e 1 
ex xex bcA.x = bc cos·e) xx n kexexk x c2neighbors where ex cis the x-axis of the cell c, and cell b 
has n neighbors. Cell Programs Each cell has several cell programs,which are .rst order differential 
equations describing how its state changes over time. Examples are given in Table 1 and Section 4.3. 
A cell program is a function of the cell s current state Sand its environment as expressed by A. Different 
types of cells use different cell programs or different combinations of the same cell programs to de.ne 
their behaviors. Eveniftwocellssharethesamesetofcellprograms,they will generally behave differently because 
they experience different local conditions depending on their position. The entire system of differential 
equations to be solved is ob­tained by superposing ordinary differential equations from the cell programs 
for every cell. Additional equations arise from compu­tation in the environment (e.g., diffusion of chemicals, 
although this is not in the current 3-D implementation). In order to handle discontinuous changes, such 
as when cells are created or die, we use a piecewise ordinary differential equation solver [2, App. C]. 
Mathematical Basis for Cell Programs Differential equations are a general tool for creating dynamic behavior. 
In our cell pro­grams, we employ equations arising from physical models, as well as those arising from 
constraint solution techniques. Higher order linear differential equations, such as those for mechanical 
or chemical systems, can be rewritten as multiple .rst order differential equations (i.e., cell programs) 
with the addition of state variables. In this case, the simulation dynamics re.ect the dynamics of the 
equations. In order to write constraints as cell programs, we formulate them as energy functions1 to 
be minimized [39]. Each constraint is ex­pressed as an energy function Ei(S,A)ofthe state ofthe system, 
S, and the parameters describing the environment, A. Hard constraints could also .t into this framework 
using Lagrange multipliers [22]. Using relative constants ki to weight the soft constraints, we express 
the overall energy to minimize as: E.(S,A)= k1E1(S,A)+ ···+ knEN (S,A) We can minimize this energy according 
to gradient descent by modifying the state according to N X dS. Ei = ki dt S. i=1 Superposition of Cell 
Programs Because the overall energy is expressed as a sum, the cell programs dS.ddt are also sums of 
terms, one for each constraint. We .nd it convenient to write cell programs as incremental collections 
of constraints. We write this as dS.ddt += fconstraint termgin our example programs in Table 1. Multiple 
cell programs can thus be added together conveniently. 1Note that when we use constraint-based cell programs, 
the dynamics of our sim­ulation depends upon the gradient descent algorithm, and is not necessarily physically 
meaningful. Behavior Environment Requirements Cell Program Go to a surface. An implicit surface f (x)= 
0. p 0+= kf (p)rf(p) Die if too far from surface. An implicit surface f (x)= 0. S0 die+= 1 d f(p) edie 
Sdie Align an axis with a vector .eld. A vector .eld, v(x). ey is the cell s y-axis. Wv =k eyxv(p) keyxv(p)k 
cos -1(ey · v(p) kv(p)k) q 0+= (1d2) Wv q Align x-axis with neighbors. A.x, x-axis orientation relative 
to neighbors. q 0+= (1d2) A.x q Align z-axis with neighbors. A.z, z-axis orientation relative to neighbors. 
q 0+= (1d2) A.z q Maintain unit quaternion. q 0+= 4k(1 q·q)q Adhere to other cells Membrane chemical 
a2 which binds to itself. S0 a2+= 1.0 Sa2 Divide until surface is covered. .a2, amount of a2 which is 
bound. S0 split+= ,(,.a2) esplit Ssplit Set size relative to surface feature size .u0, a value which 
re.ects the size of the nearest feature on the surface. r0+= .u0 r Example of reaction-diffusion in discrete 
cells. .a0, .a2amounts of bound membrane chemicals. The user has speci.ed that the membrane chemical 
a0 binds to a1,and that a2 binds to itself. S0 c0+= 20 .a0d.a2+ 10 S2 c0d(1 + S2 c0) 0.5 Sc0+13 S0 a1+= 
0.95 .a0d.a2 S0 a1+= ,(3,Sc0) Sc0 S0 a1+= Sa1 S0 a0+= 5 Sa0 S0 a2+= 1 Sa2 Table 1: Example Cell Programs. 
Scalar or vector .elds are given as a function of spatial location, x, and are evaluated at the current 
location of the cell, p. See cell program descriptions in Section 4.3. 4.3 Example Cell Programs The 
cell programs listed in Table 1 exemplify the types of cell programs used to make the .gures in this 
paper. We describe each brie.y below. Remember that the contributions from multiple terms are added together 
to make a single differential equation for each state variable (using the += notation). Many of the cell 
programs shown here are of the form S0 i (t)= dSiddt = fSi(t) for some constant f,which causes Si to 
quickly approach the value f. Go to a surface. This cell program implements a constraint to keep a cell 
on the implicit surface f (x) = 0. An approximation to the gradient, rf (x), is also available in the 
environment. As the simulation runs, a cell with this program will descend the gradient and come to rest 
on the surface. The parameter k determines the speed with which the particle approaches the surface. 
Die if too far from surface. Recall that when the variable Sdie crosses the threshold edie, it triggers 
cell death (Section 4.2). In this cell program, we cause Sdieto rise towards the threshold quickly if 
the cell is greater than a certain distance from the surface. Com­puting this requires a measure of the 
distance from the surface. For the implicit surfaces used in the .gures, f (x) is an approximation to 
the distance from the surface. The equation in Table 1 causes cells at a distance greater than d to die. 
Similar cell programs can cause a cell to die if it becomes too large, its orientation strays too far 
from neighboring cells, or due to any other condition that is a function of the cell s environment and 
internal state. Align with a vector .eld. In this example, we align the cell s y­axis, ey with a given 
vector .eld v(x). The vector .eld is evaluated at the cell s current location, p. We .rst compute the 
vector Wv, which represents the transfor­mation required to rotate the ey into v. Wv is the axis of rotation, 
and the length of Wv speci.es the angle through which to rotate. The formulation works with any of the 
cell s axes. The form of the cell program comes from the equation q 0=(1d2) Wq, which de.nes the rate 
of change of a quaternion, q, for angular velocity W[12]. If we have multiple cell programs of this form, 
the Wi terms add, which allows us to constrain one, two, or all three axes of the cells. If the orientation 
constraints con.ict, the cell s orientation will approach the average orientation. If they don t con.ict, 
all constraints will become satis.ed. Align with neighbors. The three orientation constraints on the 
cell s x-, y-and z-axes fully constrain its orientation. Constraining two axes would be suf.cient in 
most cases, except where the vector .eld v(x) happenedto be collinearwith the x-or z-axis. Having the 
extra constraint keeps us from running into problems in that case, and also aids the convergence of the 
cell alignment process. Maintain unit quaternion. It is wise to add a constraint to ensure that the quaternion 
does not stray too far from a unit quaternion during the integration of the differential equations. We 
can do this with another simple constraint. Table 1 shows a term for this constraint that comes from 
minimizing the energy expression E =(1 q·q)2, which describes the deviation of qfrom a unit quaternion. 
Adhere to other cells. This equation causes the variable Sa2 (representing the surface chemical a2) to 
approach and stay at the value 1.0. A pair of cells expressing a2 will stick together once they come 
in contact, and a force is required to pull them apart. The environment vector for each cell will report 
the amount of chemical bound on each cell, .a2, which may be used in other cell programs to determine 
if the cell has contacted another cell. The amount bound is computed from the contact area between the 
two cells, and the concentrations on each cell. Divide until surface is covered. Divide until the amount 
of bound surface chemical a2 reaches the level . .a2reports the total amount bound from all cells that 
are in contact, which gives the cell a means of determining how many neighbors it has. Note that the 
mechanism has more general utility than just counting neighbors. For instance, a cell with twice the 
concentration of a0 will contribute more to .a2. The auxiliary function ,( ,.a0) is used in this cell 
program to compute a continuous version of the condition (.a0. ). The function ,(a,b) computes a continuous 
version of the boolean condition (b .a): ,(a,b) =(tanh((ba)) + 1)d2. The value of this function will 
be near one for (b ..a) and near zero for (a ..b). Set size relative to surface feature size. In Figure 
1, the cell sizes are related to the sizes of features in the polygonal database. This is achieved by 
providing the cells with a value .u0that represents the area of the nearest triangle. The value .u0could 
be used to pass information about local curvature or any other parameter that we wish to use to change 
the cell behavior. S Example of reaction-diffusion in discrete cells. The full deriva­tion [7] of this 
set of equations is beyond the scope of this paper, however we will describe the equations brie.y. The 
.rst equation in this set de.nes a genetic switch [18] that tends to drive Sc0to­wards one of two values, 
depending on the in.uence of the term .a0d.a2. In terms of Meinhardt s activator-inhibitor models [16], 
c0is the activator and Sa1is the inhibitor, which is propagated by the activity of membrane chemicals. 
The other equations deter­mine interactions of membrane chemicals that lead to an effective diffusion 
of the value of Sa1among the cells. The value of Sc0can then be used to determine the .nal rendered shape 
of the cell, as illustrated in to Figures 2 and 7. 4.4 Surface Constraints We have applied surface constraints 
to a variety of surface classes: polygonal mesh,  implicit function, and  isosurfaces of volume data. 
 The surface constraint cell program evaluates an implicit func­tion to enable the cell to .nd and stay 
on the surface (Table 1). Several of the surfaces used to create the .gures are de.ned by triangular 
meshes. We create a rough approximation to an implicit function for these meshes. Any implicitization 
method will work, and in fact it doesn t have to be very exact. We implement this function by constructing 
an approximate kd-tree for the triangular mesh. In constructing a true kd-tree, each additional vertex 
may add several new partitions. Our approximation adds only the one necessary partition for each added 
vertex. This makes the tree smaller and faster to precompute, but no longer actually gives the closest 
point. To evaluate the function, we look up the triangle center in the kd-tree supposedly nearest to 
a given point. We then check adjacent triangles to see if they are closer, to improve the characteristics 
of the approximation. We then compute the direction and distance to that triangle, and use it as we would 
the gradient of an implicit function. We .nd the approximation, in conjunction with the local search, 
to be satisfactory for this application.  5 Particle Converter The particle converter converts information 
about the particles and their environment into geometry and appearance parameters for rendering. It receives 
all of the results of the simulation, includ­ing the position, orientation and size of each cell, concentration 
of reaction-diffusion chemicals, and other arbitrary user-de.ned parameters, such as type or color. It 
also may have access to infor­mation about how far each cell is from the surface and properties of the 
surface near the cell (e.g, curvature) The converter also knows which cells contact each other. The particle 
converter concept has proven to be extremely con­venient. It enables us to do a variety of useful operations, 
includ­ing: choosing an appropriate representation for each cell based on its screen size (Figure 4); 
 smoothly changing the appearance of a cell based on a con­tinuously varying parameter (Figure 7); 
 using the cell positions to generate a spatial subdivision (sim­ilar to [33, 37, 41]);  using the cell 
orientations to compute a .ow .eld on the surface (useful for displacement maps [21]); and  experimenting 
with various colorations and geometries using the same simulation dataset.  The output of the particle 
converter is a collection of geometry and appearance information suitable for a particular renderer. 
This collection will generally include one or more geometric primitives for each cell, and the local 
texture, transparency, or bump informa­tion. The geometry can be simple, as in Figure 5, where each cell 
is rendered as a few polygons with a mottled-green texture map. Or it can be more complicated, as in 
the parameterized 3D thorn shape that curls based on cell state information (Figure 7 (c)). The particle 
converter can also use contact information to calcu­late the size or shape of geometric primitives based 
on neighboring cell proximity, or to interpolate parameters such as orientation be­tween cell centers. 
We have implemented two particle converters. One provides options for choosing a particular geometry 
and texture for each cell. In addition, it considers information associated with the underlying polygons, 
such as which body part it represents in an anatomical model (lips, eyes, etc.) This can be used to change 
the rendering of cells in certain areas, as can be seen on the lips of the man in Figure 1. Our second 
particle converter was implemented using a general purpose modeler [31]. Taking advantage of the .exibility 
of this modeler, we can create parameterized objects such as the bump-to­thorn shape shown in Figure 
2. The modeler is used to create the Figure 4: In the top image, the thorny spheres at further distances 
are rendered with fewer polygons. The bottom image shows a closeup of the nearest and furthest objects, 
so we can see the re­duced number of polygons. thorny spheres in Figure 7. Rendering an appropriately 
scaled representation One of the drawbacks of data ampli.cation techniques [30, 26] such as ours is their 
ability to generate a ridiculous amount of geometry to render. To ameliorate this, we use the particle 
converterto choosegeometric primitives appropriate to the size of the object in the .nal image (Figure4). 
Thisapproachcouldbecarriedevenfurther,forinstance, by creating texture maps based on the cell positions. 
 6Results In this section we list a series of examples that highlight features of our system. Scales 
Figure 5 shows four views of a spherical object with a uniform covering of similarly-oriented cells. 
The cell programs used here incorporate terms to divide until the surface is covered, to stay on the 
surface, and to die if pushed too far off of the surface. Initially, several cells were placed near the 
surface, and allowed to divide and wander. The cells were also given a soft constraint to align their 
y-axes with the gradient of the surface implicit function, and to align their x-and z-axes with their 
neighbors. Note that two singularities in the orientations of cells arise nat­urally on the sphere, due 
to its topology. One is visible on the near Figure 5: Scales: Four views of a spherical object uniformly 
cov­ered with similarly oriented cells. Each cell is rendered as a group of four polygons with a texture 
and transparency map. The polygons are tilted slightly to give a layered appearance.  Figure 6: Cellular 
textures can handle unusual topologies. sideoftheupperrightsphere. Unlikestandardtexturemapping,this 
method introduces no particular parameterization problems, such as stretching or shrinking of the texture. 
A Knotty Problem Figure 6 shows that the cellular texture ap­proach is capable of creating textures for 
surfaces with unusual topologies. It is not necessary to have a parameterization for the surface. This 
surface was designed by John Hughes and John Sny­der. Thorny Head: Changing cell size to match surface 
features The examples described so far have used cells that are relatively uniform in size. Figure 1 
shows an example where the cell size is related to the detail level in the underlying polygonal model. 
We achieve this by providing the cells with another environmental variable: the area of the nearest triangle 
in the underlying polygon mesh. Note the .ner texture and geometry around the eyes and mouth. Different 
rendering parameters were chosen according to prop­erties of the underlying polygonal model. Each polygon 
in the underlying database is associated with a region of the body. The particle converter assigned different 
shading properties to the cells in the head and neck regions. At the eyes, the underlying polygonal representation 
shows through the cell texture. Thorny Spheres: Differentiated cellular textures This example shows several 
important capabilities of the system. It shows the creation of simple reaction-diffusion patterns on 
a surface,  the use of the concentrations of cell chemicals to change parameters of the rendered geometry, 
and  the ability to restart simulations from an previous state with new cell programs, causing new behaviors 
to occur.  These cells are using reaction-diffusion equations similar to those in Table 1 to create 
patterns of chemicals in the cells. The diffusion of chemicals occurs by contact between cell membranes, 
thus it can only occur between adjacent cells. Using a geometric modeler, we created a parameterized 
geo­metric object that changes from a bump to a thorn based on a single parameter [31]. The particle 
converter sets this parameter to the value of a state variable representing the concentration of one 
of the reaction-diffusion chemicals. We can see that there is a patch of cells on the front of the sphere 
with very little of the chemical (rendered as bumps), and a larger patch on the back with more of the 
chemical (rendered as thorns). In addition to the sharp boundary between the patches, note that the height 
of the thorns on the back patch varies continuously as they sweep around the sphere. These simulations 
began where the earlier sphere simulation of Figure 5 left off, with new rules to cause the cells to 
differentiate. This is a common motif of user interaction with the system: halt a simulation, modify 
cell programs and parameters, and then continue simulating. A Bear of a Surface In Figure 8, we show 
a fur-covered model of a bear de.ned as an isosurface of sampled volume data. We would like for the bear 
s fur to have a natural-looking orientation [15]. The bear on the left, with the fully combed fur, started 
from a single cell and used a set of rules similar to those used for Figure 5 to distribute and orient 
the cells. Each cell on the bear is rendered with a group of geometric objects meant to roughly represent 
a hunk of thick hair. The bear on the right, with the patchy fur, was the result of a serendipitous combination 
of unintentional cell programs. Rather than having each cell align with all of its neighbors, each cell 
chooses one neighbor to align with. Also, cells do not attempt to align with neighbors that are oriented 
in the directly opposing direction. This bear started from about 2000 arbitrarily chosen cells on the 
surface. Additional, more speci.c, orientation constraints could cause the fur to run more naturally 
down the limbs. Other cell programs could be added to cause the fur to be shorter in the region of the 
face and longer on the haunches, or to change the coloration based on the orientation or curvature of 
the bear s features.  7 Discussion The combination of particle constraint techniques with developmen­tal 
models enables the generation of a variety of cellular textures, as shown in the .gures. We have found 
the approach to be a pow­erful method of creating attractive computer graphics models of organic objects. 
In our experience with making cellular textures, we encountered some dif.culties, which we describe below. 
Some of these limitations are associated with our current implementation, and can be remedied without 
changing the basic framework. The problems with simulation speed and data explosion are less easily .nessed, 
and will require further research to address fully. Some commercially produced computer graphics .lms 
and vi­deos contain models that have textures that appear similar to ours. The techniques used to generate 
them are generally proprietary and unpublished, hence we cannot de.nitively compare them with our work. 
Software for orienting fur on a CG character has been developed at Industrial Light &#38; Magic [5]. 
It is interesting to note that their discussion of the dif.culties encountered closely parallels our 
own experience. Shapes The spherical shapes of cells in a simulation generally are not the shapes we 
want to render, and so the particle converter might make objects with undesirable intersections. This 
can be minimized by a careful choice of cell geometry, but a more robust solution is to use the desired 
geometric shape directly in the cellular particle simulation. This would allow cell programs to calculate 
collisions based on more accurate geometry. Experience with Writing Cell Programs Writing cell programs 
can be dif.cult. Programming independently moving cells by spec­ifying differential equations has many 
desirable properties, but re­quires a different intuition than other types of programming, and often 
takes a while to get right. As with many tasks, it gets easier with practice. Here are some suggestions 
for using this program­ming paradigm: Copy and combine known cell programs from other research­ers, 
such as surface or orientation constraints [41, 33].  Think about the constraints in the energy formulation 
(Sec­tion 4, and [39]).  Satisfy one constraint at a time; e.g., .rst get cell positions right, then 
modify other attributes.  Force certain problem cells to be a certain way (through direct interaction, 
Section 4.1).  Kill problem cells and regrow (Section 4.1).  Apply arti.cial evolution [29], and be 
patient.  Simulation Speed Simulations can be slow for some kinds of cell programs. We have some that 
run in seconds, and others, like the large datasets, that take many hours (e.g., the bear in Figure 8, 
and the head in Figure 1). Generally, performance degrades as the differential equations get stiff [11]. 
For some behaviors, clever cell programs like those described in [41] avoid creating stiff differential 
equations, and so run faster. Data Explosion The data produced both by the simulation and by the particle 
converter can get very large. We have partially addressed this by parameterizing the particle converter 
output by viewing distance (Figure 4). However, the simulation still has to compute enough cells to cover 
the surfaces, independent of viewing distance.  Figure 7: Varying Thorns. Reaction-diffusion-like equations 
determine the pattern of bumps and thorns on these spheres. Note the continuously varying thorn height 
and thorn curvature on the center and rightmost spheres. Figure 8: The bear on the left is fully combed, 
with all cells oriented like their neighbors. The bear on the right has patches of similarly­oriented 
cells. Future Work There are several directions in which we would like to extend this work. First, we 
plan to continue extending and re.ning the cell programs to generate more complex cellular textures. 
We also are interested in running simulations on objects as they move and change shape. Modeling the 
motion of feathers on the wings of a .ying bird, or hair on a running animal would be exciting. Initial 
experiments (not discussed in this paper) indicate that this will be feasible. Implementing more sophisticated 
cell geometries in the particle simulator will give us more realistic placement of detail, and avoid 
self-intersections in the rendering. Finally, we would like to explore the possibilities of creating 
shapes directly from the fundamental interactions of the cells, without the surface constraint. Acknowledgments 
Many thanks to Erik Winfree for designing and implementing the kd-tree approximation, as well as for 
providing many helpful sug­gestions. We are grateful to Allen Corcorran, Matt Avalos, Cindy Ball, Dan 
Fain, Louise Foucher, Marcel Gavriliu, Barbara Meier, Mark Montague, Alf Mikula, Preston Pfarner, Ravi 
Ramamoorthi, Dian De Sha, and Denis Zorin for valuable discussions, support, code, and proofreading. 
MRI data was taken at the Huntington MRI center in Pasadena, CA. This work was supported in part by grants 
from Apple, DEC, Hewlett Packard, and IBM. Additional support was provided by NSF (ASC-89-20219) as part 
of the NSF/ARPA STC for Com­puter Graphics and Scienti.c Visualization, by the DOE (DE-FG03­92ER25134) 
as part of the Center for Research in Computational Biology, the Beckman Foundation, and by the National 
Institute on Drug Abuse and the National Institute of Mental Health as part of the Human Brain Project. 
All opinions, .ndings, conclusions, or recommendations expressed in this document are those of the authors 
and do not necessarily re.ect the views of the sponsoring agencies. REFERENCES [1] James Arvo and David 
Kirk. Modeling plants with environment­sensitive automata. In Proceedings of Ausgraph 88, pages 27 33, 
1988. [2] Ronen Barzel. Physically-Based Modeling: A Structured Approach. Academic Press, Cambridge, 
MA, 1992. [3] Robert L. Cook. Shade trees. In Computer Graphics (SIGGRAPH 84 Proceedings), volume 18, 
pages 223 231, July 1984. [4] F. C. Crow. A more .exible image generation environment. In Com­puter Graphics 
(SIGGRAPH 82 Proceedings), volume 16, pages 9 18, July 1982. [5] Jody Duncan. The making of a rockbuster. 
Cinefex: the Journal of Cinematic Illusions, 58:34 65, June 1994. [6] Kurt Fleischer. Cells: Simulations 
of multicellular development. In Siggraph Video Review, 1994. A video presented at Siggraph 94. [7] Kurt 
W. Fleischer. A Multiple-Mechanism Developmental Model for De.ning Self-Organizing Structures. PhD dissertation, 
Caltech, De­partment of Computation and Neural Systems, June 1995. [8] Kurt W. Fleischer and Alan H. 
Barr. A simulation testbed for the study of multicellular development: The multiple mechanisms of morpho­genesis. 
In Arti.cial Life III. Addison-Wesley, 1994. [9] Deborah R. Fowler, Hans Meinhardt, and Przemyslaw Prusinkiewicz. 
Modeling seashells. In Computer Graphics (SIGGRAPH 92 Proceed­ings), volume 26, pages 379 388, July 1992. 
[10] Deborah R. Fowler, Przemyslaw Prusinkiewicz, and Johannes Battjes. A collision-based model of spiral 
phyllotaxis. In Computer Graphics (SIGGRAPH 92 Proceedings),volume 26,pages361 368,July 1992. [11] C. 
W. Gear. Numerical Initial Value Problems in Ordinary Differential Equations. Prentice-Hall, Englewood 
Cliffs, NJ, 1971. [12] Goldstein. Classical Mechanics. Addison-Wesley, 1980. [13] Ned Greene. Voxel space 
automata: Modeling with stochastic growth processes in voxel space. In Computer Graphics (SIGGRAPH 89 
Proceedings), volume 23, pages 175 184, July 1989. [14] James T. Kajiya. Anisotropic re.ection models. 
In ComputerGraphics (SIGGRAPH 85 Proceedings), volume 19, pages 15 21, July 1985. [15] James T. Kajiya 
and Timothy L. Kay. Rendering fur with three dimen­sional textures. In ComputerGraphics(SIGGRAPH 89 Proceedings), 
volume 23, pages 271 280, July 1989. [16] Hans Meinhardt. Models of Biological Pattern Formation. Academic 
Press, London, 1982. [17] Gavin Miller and Andrew Pearce. Globular dynamics: A connected particle system 
for animating viscous.uids. Computersand Graphics, 13(3):305 309, 1989. [18] J. D. Murray. Mathematical 
Biology. Springer-Verlag, New York, 2nd edition, 1993. [19] B. N. Nagorcka, V. S. Manoranjan, and J. 
D. Murray. Complex spatial patterns from tissue interactions an illustrative model. Journal of Theoretical 
Biology, 128:359 374, 1987. [20] Garrett M. Odell, George Oster, P. Alberch, and B. Burnside. The mechanical 
basis of morphogenesis.Developmental Biology,85, 1981. [21] Hans Køhling Pedersen. Displacement mapping 
using .ow .elds. In Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), pages 279 286. ACM 
Press, July 1994. [22] John Platt. Constraint Methods for Neural Networks and Computer Graphics. PhD 
dissertation, Caltech, Department of Computer Sci­ence, Pasadena, CA, 91125, 1989. [23] Przemyslaw Prusinkiewicz, 
Mark James, and Radomi´rM.ech. Syn­thetic topiary. In Proceedings of SIGGRAPH 94 (Orlando, Florida, July 
24 29, 1994), pages 351 358. ACM Press, July 1994. [24] Przemyslaw Prusinkiewicz and Aristid Lindenmayer. 
The Algorithmic Beauty of Plants. Springer-Verlag, New York, 1990. [25] W. T. Reeves. Particle systems 
 a technique for modeling a class of fuzzy objects. ACM Trans. Graphics, 2:91 108, April 1983. [26] William 
T. Reeves and Ricki Blau. Approximate and probabilistic algorithms for shading and rendering structured 
particle systems. In Computer Graphics (SIGGRAPH 85 Proceedings), volume 19, pages 313 322, July 1985. 
[27] Craig W. Reynolds. Flocks, herds, and schools: A distributed behav­ioral model. In Computer Graphics 
(SIGGRAPH 87 Proceedings), volume 21, pages 25 34, July 1987. [28] Karl Sims. Particle animation and 
rendering using data parallel com­putation. In Computer Graphics (SIGGRAPH 90 Proceedings),vol­ume 24, 
pages 405 413, August 1990. [29] Karl Sims. Arti.cial evolution for computer graphics. In Computer Graphics 
(SIGGRAPH 91 Proceedings), volume 25, pages 319 328, July 1991. [30] Alvy Ray Smith. Plants, fractals 
and formal languages. In Computer Graphics (SIGGRAPH 84 Proceedings),volume 18, pages 1 10, July 1984. 
[31] John Snyder. Generative Modeling for Computer Graphics and CAD: Symbolic Shape Design using Interval 
Analysis. Academic Press, 1992. [32] John M. Snyder and Alan H. Barr. Ray tracing complex models containing 
surface tessellations. In Computer Graphics (SIGGRAPH 87 Proceedings), volume 21, pages 119 128, July 
1987. [33] Richard Szeliski and David Tonnesen. Surface modeling with oriented particle systems. In Computer 
Graphics (SIGGRAPH 92 Proceed­ings), volume 26, pages 185 194, July 1992. [34] Demetri Terzopoulos, John 
Platt, and Kurt Fleischer. From goop to glop: Heating and melting deformable models. In Graphics Interface 
89, 1989. [35] Alan Turing. The chemical basis of morphogenesis. Phil. Trans. B., 237, 1952. [36] Greg 
Turk. Generating textures for arbitrary surfaces using reaction­diffusion. In Computer Graphics (SIGGRAPH 
91 Proceedings),vol­ume 25, pages 289 298, July 1991. [37] Greg Turk. Re-tiling polygonal surfaces. In 
Computer Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 55 64, July 1992. [38] Stephen H. Westin, 
James R. Arvo, and Kenneth E. Torrance. Pre­dicting re.ectance functions from complex surfaces. In Computer 
Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 255 264, July 1992. [39] Andrew Witkin, Kurt Fleischer, 
and Alan Barr. Energy constraints on parameterized models. In Computer Graphics (SIGGRAPH 87 Proceedings), 
volume 21, pages 225 232, July 1987. [40] Andrew Witkin and Michael Kass. Reaction-diffusion textures. 
In Computer Graphics (SIGGRAPH 91 Proceedings), volume 25, pages 299 308, July 1991. [41] Andrew P. Witkin 
and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Proceedings of SIGGRAPH 
94 (Orlando, Florida, July 24 29, 1994), pages 269 278. ACM Press, July 1994.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218448</article_id>
		<sort_key>249</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[Fast and resolution independent line integral convolution]]></title>
		<page_from>249</page_from>
		<page_to>256</page_to>
		<doi_number>10.1145/218380.218448</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218448</url>
		<keywords>
			<kw><![CDATA[periodic motion filtering]]></kw>
			<kw><![CDATA[texture synthesis]]></kw>
			<kw><![CDATA[vector field visualization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P66279</person_id>
				<author_profile_id><![CDATA[81100330715]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Detlev]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stalling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Konrad-Zuse-Zentrum f&#252;r Informationstechnik Berlin (ZIB), Heilbronner Str. 10, D-10711 Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39066929</person_id>
				<author_profile_id><![CDATA[81100352601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hans-Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hege]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Konrad-Zuse-Zentrum f&#252;r Informationstechnik Berlin (ZIB), Heilbronner Str. 10, D-10711 Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166151</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brian Cabral and Leith (Casey) Leedom. Imaging vector fields using line integral convolution. Proc. of SIGGRAPH '93 (Anaheim, California, August 1-6,1993). In Computer Graphics 27, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 263-272.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>79507</ref_obj_id>
				<ref_obj_pid>79505</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. R. Cash and Alan H. Karp. A variable order Runge-Kutta method for initial value problems with rapidly varying right-hand sides. ACM transactions on Mathematical Software 16, pp. 201-222,1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Peter Deuflhard and Folkmar Bornemann. Numerische MathematikH: Integration gewghnlicher Differentialgleichungen. Verlag de Gruyter, Berlin, 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J.R. Dormand and E J. Prince. Higher order embedded Runge-Kutta formulae. J. Comp. Appl. Math., 7, pp. 67-75, 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951132</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lisa K. Forssell. Visualizing flow over curvilinear grid surfaces using line integral convolution. In Proc. of Visualization '94, (Washington, D.C., Oct. 17-21, 1994), pp. 240-247,IEEE Computer Society, 1994.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97902</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Paul E. Haeberli. Paint by numbers: Abstract image representations. Proc. of SIGGRAPH '90 (Dallas, Texas, August 6-10, 1990). In Computer Graphics, 24(4) (August 1991), pp. 207-214.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>153158</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ernst Hairer, Syvert Paul NOrsett, and Gerhard Wanner. Solving Ordinary Differential Equations I, Nonstiff Problems. Springer Verlag, Berlin, Heidelberg, New York, Tokyo, 1987.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617655</ref_obj_id>
				<ref_obj_pid>616017</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[James L. Helman and Lambertus Hesselink. Visualizing vector field topology in fluid flows. IEEE Computer Graphics and Applications, 11(3), pp. 36-46, May 1991.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949859</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Andrea J. S. Hin and Frits H. Post. Visualization of turbulent flow with particles. In Proc. of Visualization '93 (San Jose, California, October 25-29, 1993), pp. 46-52, IEEE Computer Society, 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Jeff E M. Hultquist. Interactive numerical flow visualization using stream Surfaces. Computing Systems in Engineering, 1 (2-4), pp. 349- 353, 1990.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949698</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kwan-Liu Ma and Philip J. Smith. Virtual smoke: An interactive 3d flow visualization technique. In Proc. of Visualization '92 (Boston, MA, October 19-23,1992), pp. 46-52, IEEE Computer Society, 1992.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gordon D. Mallinson. The calculation of the lines of a threedimensional vector field. In Proc. of the International Symposium on Computational Fluid Dynamics (Sydney, Australia, August 1987), pp. 525-534, North-Holland, August 1988.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Jerrold E. Marsden and Anthony J. Tromba. Vector Calculus. W.H. Freeman, New York, 3rd edition, 1988.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949854</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Nelson Max, Barry Becker, and Roger Crawfis. Flow volumes for interactive vector field visualization. In Proc. of Visualization '93 (San Jose, CA, Oct. 25-29,1993), pp. 19-24, IEEE Computer Society, 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Nancy J. Nersessian. Faraday's field concept. In D. Gooding and E A. J. L. James, eds., Faraday Rediscovered: Essays on the Live and Work of Michael Faraday, pp. 175-187. Stockton Press, New York, 1985.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. An image synthesizer. Proc. of SIGGRAPH '85 (San Francisco, California, July 22-26, 1985). In Computer Graphics 19, pp. 287-296,1985.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>148286</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian E Flannery. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, 2nd edition, 1992.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122751</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Jarke J. van Wijk. Spot noise-texture synthesis for data visualization. Proc. of SIGGRAPH '91 (Las Vegas, Nevada, 28 July - 2 August, 1991). In Computer Graphics, 25, pp. 309-318,1991.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949699</ref_obj_id>
				<ref_obj_pid>949685</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Jarke J. van Wijk. Rendering surface-particles. In Proc. of Visualization '92 (Boston, Massachusetts, October 19-23,1992), pp. 54-61, IEEE Computer Society, 1992.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast and Resolution Independent Line Integral Convolution Detlev Stalling Hans-Christian Hege Konrad-Zuse-Zentrum 
f¨ur Informationstechnik Berlin (ZIB)1 Abstract Line Integral Convolution (LIC) is a powerful technique 
for gener­ating striking images and animations from vector data. Introduced in 1993, the method has rapidly 
found many application areas, rang­ing from computer arts to scienti.c visualization. Based upon lo­cally 
.ltering an input texture along a curved stream line segment in a vector .eld, it is able to depict directional 
information at high spatial resolutions. We present a new method for computing LIC images. It em­ploys 
simple box .lter kernels only and minimizes the total num­ber of stream lines to be computed. Thereby 
it reduces computa­tional costs by an order of magnitude compared to the original algo­rithm. Our method 
utilizes fast, error-controlled numerical integra­tors. Decoupling the characteristic lengths in vector 
.eld grid, input texture and output image, it allows computation of .ltered images at arbitrary resolution. 
This feature is of signi.cance in computer animation as well as in scienti.c visualization, where it 
can be used to explore vector data by smoothly enlarging structure of details. We also present methods 
for improved texture animation, again employing box .lter kernels only. To obtain an optimal motion ef­fect, 
spatial decay of correlation between intensities of distant pixels in the output image has to be controlled. 
This is achieved by blend­ing different phase-shifted box .lter animations and by adaptively rescaling 
the contrast of the output frames. CR Categories: I.3.3 [Computer Graphics]: Picture/Image gener­ation; 
I.3.6 [Computer Graphics]: Methodology and Techniques; I.4.3 [Image Processing]: Enhancement Additional 
Keywords: vector .eld visualization, texture synthesis, periodic motion .ltering  1 Introduction Generation 
of textured images from various kinds of vector .elds has become an important issue in scienti.c visualization 
as well as in animation and special effects. In 1993 Cabral and Leedom pre­sented a powerful technique 
for imaging vector data called line in­tegral convolution [1]. Their algorithm has been used as a general 
tool for visualizing vector .elds. Additionally it has broad applica­tions for image enhancement. A major 
drawback of the original al­gorithm, however, is its high computational expense and its restric­tion 
to a .xed spatial resolution. In this paper we present an improved algorithm for line inte­gral convolution, 
in which computation of streamlines is algorith­ 1Heilbronner Str. 10, D-10711 Berlin, Germany E-mail: 
fstalling,hegeg@zib-berlin.de Permission to make digital/hard copy of part or all of this work for personal 
or classroom use is granted without fee provided that copies are not made or distributed for profit or 
commercial advantage, the copyright notice, the title of the publication and its date appear, and notice 
is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, 
or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 mically separated from that of convolution. This allows us to ex­ploit economies and to provide 
wider functionalism in each of the computational steps. The new algorithm .is about an order of magnitude 
faster than original line integral convolution, making interactive data exploration possible .is more 
accurate by employing an adaptive, error-controlled streamline integration technique .is resolution 
independent, enabling the user to investigate im­age details by smooth detail enlargement (zooming) 
 .improves texture animation using shifted box .lter kernels to­gether with a simple blending technique. 
In recent years a number of methods for arti.cially generating textures have been suggested. These methods 
cover a variety of ap­plications. In the .eld of scienti.c visualization texture-based meth­ods are of 
special interest because they allow the display of vec­tor .elds in an unrivaled spatial resolution. 
Traditionally, vector data has been represented by small arrows or other symbols indi­cating vector magnitude 
and direction. This approach is restricted to a rather coarse spatial resolution. More sophisticated 
methods include the display of stream lines [8], stream surfaces [10], .ow volumes [14], as well as various 
particle tracing techniques [19, 9, 11]. These methods are well suited for revealing characteristic fea­tures 
of vector .elds. However, they strongly depend on the proper choice of seed points. Experience shows 
that interesting details of the .eld may easily be missed. Texture-based methods are not affected by 
such problems. They depict all parts of the vector .eld and thus are not susceptible to missing characteristic 
data features. In addition they achieve a much higher spatial resolution, which in some sense can be 
viewed as the maximum possible resolution since the minimum possible feature size of a textured image 
is a single pixel. In an early method in­troduced by van Wijk [18] a random texture is convolved along 
a straight line segment oriented parallel to the local vector direction. Line integral convolution (LIC) 
[1] modi.es this method, so that convolution takes place along curved stream line segments. In this way 
.eld structure can be represented much more clearly. Forssell [5] describes another extension that allows 
her to map .at LIC im­ages onto curvilinear surfaces in three dimensions. Vector .elds are not only of 
relevance in science and engineer­ing. Many objects of our natural environment exhibit characteristic 
directional features which are naturally represented by vector data. Consequently algorithms for turning 
such data into pictorial infor­mation are of great importance for synthetic image generation, im­age 
post-processing, and computer arts [6, 16]. The variety of di­rectional .lters offered by commercial 
image processing software is just one evidence for this. The remainder of the paper is organized as follows. 
Section 2 provides mathematical background and .xes notation. The basic ideas of the new algorithm are 
outlined in section 3. In the following three sections we present algorithms for fast and accurate streamline 
integration, discuss some optimization issues, and sketch strategies for fast texture map sampling. We 
then discuss periodic motion .l­tering and smooth detail enlargement. Finally we present some re­sults 
and give an outlook concerning various aspects of LIC meth­ods. Figure 1: LIC image of a vector .eld 
(electrical .eld) containing disconti­nuities. Field strength jvjis indicated by color.  2 Background 
Before looking at line integral convolution, let us introduce vector .elds formally, de.ne some characteristic 
features and .x notation. For more detailed expositions on vector .elds see standard texts on vector 
analysis, e.g. [13]. Restricting ourselves to the simplest case, we consider a stationary vector .eld 
de.ned by a map v:R2­R2,x7-v(x). The directional structure of vcan be graphically depicted by its integral 
curves, also denoted .ow lines or stream lines1.An integral curve is a path a(u)whose tangent vectors 
coincide with the vector .eld: d a(u)=v(a(u)) (1) du Like any path, a(u)can be reparametrized by a continous, 
strictly increasing function without changing its shape and orientation. For our purpose it is convenient 
to use arc-length s. Noting that ds/du=jv(a(u))jwe have ddaduv a(s)= ==f(a(s)).(2) dsdudsjvj Of course, 
this reparametrization is only valid in regions of non­vanishing jvj, i.e. for non-degenerate curves 
a. To .nd a stream line through xthe ordinary differential equation (2) has to be solved with the initial 
condition a(0)=x. It can be proved that there is a unique solution if the right hand side flocally obeys 
a Lipschitz­condition. In particular this condition is ful.lled for any function with continuous .rst 
derivative. Otherwise, there may exist multi­ple solutions at a single point x, i.e. multiple stream 
lines may start at that point. A typical example are point sources in an electrostatic .eld. Numerical 
integrators used in LIC have to be robust enough to handle such cases. Beside isolated singularities 
also discontinu­ities occur quite often in vector .elds. Usually these are encoun­tered across the boundaries 
of distinctly characterized .eld regions, e.g. regions with different electromagnetic properties. An 
example of this is shown in Fig. 1. 1The image of integral curves ( lines of force ) and their graphical 
repre­sentation played a crucial role in Faradays development of the .eld concept during 1820-1850 [15]. 
Given a stream line a, line integral convolution consists in cal­culating the intensity for a pixel located 
at x0=a(s0)by Z s+L 0 I(x0)= k(s-s0)T(a(s))ds.(3) s.L 0 Here Tdenotes an input texture, usually some 
sort of random image like white noise. The .lter kernel kis assumed to be normalized to unity. The convolution 
operation (3) causes pixel intensities to be highly correlated along individual stream lines, but independent 
in directions perpendicular to them. In the resulting images the direc­tional structure of the vector 
.eld is clearly visible. Usually good results are obtained by choosing .lter length 2Lto be 1/10th of 
the image width. It is possible to simultaneously visualize .eld strength jvjby coloring or animating 
LIC images. 3 Making Line Integral Convolution Fast In traditional LIC for each pixel in the output 
image a separate stream line segment and a separate convolution integral are com­puted. There are two 
types of redundancies in this approach. First, a single stream line usually covers lots of image pixels. 
Therefore in traditional LIC large parts of a stream line are recomputed very frequently. Second, for 
a constant .lter kernel kvery similar con­volution integrals occur for pixels covered by the same stream 
line. This is not exploited by traditional LIC. Consider two points located on the same stream line, 
x1=a(s1)and x2=a(s2). Assume, both points are separated by a small distance 6s=s2-s1.Then for a constant 
.lter kernel kobviously s.L+ ss1+L+ s 1 ZZ I(x2)=I(x1)-kT(a(s))ds+kT(a(s))ds. (4) s1.Ls1+L The intensities 
differ by only two small correction terms that are rapidly computed by a numerical integrator. By calculating 
long stream line segments that cover many pixels and by restricting to a constant .lter kernel we avoid 
both types of redundancies being present in traditional LIC. To design a fast LIC algorithm, we have 
taken an approach which relies on computing the convolution integral by sampling the in­put texture Tat 
evenly spaced locations xialong a pre-computed stream line a(s). For the moment we assume that input 
texture and output image are of the same size, like in traditional LIC. The dis­tance between different 
sample points is denoted by ht. We initiate stream line computation for some location x0=a(s0)(see Fig. 
2). The convolution integral for this location is approximated as n X I(x0)=kT(xi),with xi=a(s0+iht).(5) 
i .n To ensure normalization we take k=1/(2n+1). The resulting intensity is added to the output image 
pixel containing x0. Calcula­tion of more accurate trapezoidal sums instead of Riemann sums is nearly 
as fast, but does not pay in terms of the visual effect. After having computed I(x0), we step in both 
directions along the current stream line, thereby updating the convolution integrals as follows I(xm+1)=I(xm)+k 
T(xm+1+L)-T(xm.L)] (6) I(xm.1)=I(xm)+k T(xm.1.L)-T(xm+L)]. For each sample point the corresponding output 
image pixel is de­termined and the current intensity is added to that pixel. In this way we ef.ciently 
obtain intensities for many pixels covered by the same Figure 2: The input texture is sampled at evenly 
spaced locations xialong a stream line .. For each location the convolution integral I(xi)is added to 
the pixel containing xi. A new stream line is computed only for those pixels where the number of samples 
does not already exceed a user-de.ned limit. stream line. The probability for an output image pixel to 
be hit by a sample point is proportional to the length of the line segment cover­ing that pixel. This 
can be used to set up some sort of quality con­trol. Running through all output image pixels, we require 
the total number of hits already occurred in each pixel to be larger than some minimum. If the number 
of hits is smaller than the minimum, a new stream line computation is initiated. Otherwise that pixel 
is skipped. At the end accumulated intensities for all pixels have to be normal­ized against the number 
of individual hits. Basically our algorithm (referenced as fast-LIC hereafter) can be described by the 
follow­ing pseudocode: for each pixel p if (numHits(p)<minNumHits)then initiate stream line computation 
with x0=center of p compute convolution I(x0) add result to pixel p set m=1 while m<some limit M update 
convolution to obtain I(xm)and I(x.m) add results to pixels containing xmand x.m set m=m1 for each pixel 
p normalize intensity according to numHits(p) There are a number of remarks necessary at this point. 
First, if stream line segments were computed for each pixel separately, the discrete sampling approach 
would be tainted with major aliasing problems, unless htis chosen much smaller than the width of a tex­ture 
cell. However, if a single stream line is used for many pixels, correlation of pixel intensities along 
the stream line is guaranteed because exactly the same sampling points are used for convolution. We found 
a step size of ht =0.5times the width of a texture cell to be completely suf.cient. Although we have 
assumed input tex­ture and output image to be of the same size, the fast-LIC algorithm can easily be 
generalized to set these sizes independently. This is necessary for smooth detail enlargement as discussed 
in Sect. 8. The order in which all the output image pixels are processed is of some importance for the 
ef.ciency of the algorithm. The goal is to hit as many uncovered pixels with each new stream line as 
possible. Some optimization strategies are discussed in Sect. 5. In our algorithm the computation of 
stream line segments can be performed without referencing input texture or output image. This allows 
us to utilize powerful, adaptive numerical integration meth­ods. We have implemented several different 
integrators, which are discussed in Sect. 4. These methods not only accelerate stream line tracking signi.cantly 
in homogeneous regions, but also ensure high accuracy necessary for resolving small details. Accuracy 
is espe­cially importantin fast-LICbecausemultiple streamlines determine the intensity of a single pixel. 
If these lines are incorrectly com­puted, the LIC pattern gets disturbed. This is most evident near the 
center of a vortex in the vector .eld. Accurate stream line integration also offers new opportunities 
for texture animation using shifted .lter kernels, cf. Sect. 7. For an­imation we need a full sized convolution 
range. Therefore, when a stream line leaves the domain of v, we continue the path in the current direction. 
For texture sampling all points are remapped to fall somewhere into the input texture. We continue stream 
lines in a similar way if jvjvanishes or if a singularity was encountered. Of course, arti.cially continued 
stream line segments can not be used to determine intensities of the underlying pixels. 4 Streamline 
Integration Usually the vector .eld will not be available in functional form. For sake of simplicity 
we assume vto be given at discrete locations on an uniform grid. Vector values at intermediate locations 
have to be computed by interpolation. We use bilinear interpolation. Of course, better interpolation 
schemes can be employed if more infor­mation is available about the .eld. Sometimes global .eld proper­ties 
are known, e.g. the existance of closed stream lines. In general these properties are not retained, when 
a local interpolation scheme like bilinear interpolation is used. In particular closed stream lines in 
the true vector .eld may no longer be closed in the interpolated .eld [12]. However, in practice errors 
due to interpolation are usu­ally much smaller than errors caused by a poor numerical integrator, unless 
vis given on a very coarse grid. Bilinear interpolation results in a representation of the .eld that 
is not differentiable across the boundaries of grid cells. Therefore, to integrate Eq. (2) in general 
we can t rely on sophisticated algo­rithms like extrapolation methods or predictor-corrector schemes, 
which require a very smooth right hand side. Instead we have em­ployed traditional Runge-Kutta methods. 
Accompanied with mod­ern error monitoring and adaptive step size control these methods are quite competitive 
[17, 7, 3]. We also have to take into account that in many applications vector .elds arise that are very 
rough or even discontinuous. In such cases stream line integration is con­fronted with the potential 
risk of missing small details embedded in homogeneous regions. This problem can be tackled by delimiting 
the maximum allowed step size of an adaptive numerical integrator. At the extreme, a really safe method 
would require stepping from cell to cell in the v-grid. A fast and accurate general-purpose stream line 
integrator can be built up from the well-known classical fourth-order Runge-Kutta formula. This formula 
requires four evaluations of the right hand side to proceed from some point xto some other point h xlocated 
a step size hahead on the same stream line: 1 k1 =hf(x) k3 =hf(x+k2) 2 1 k2 =hf(x+k1)k4 =hf(x+k3) 2 h 
k1 k2 k3 k45 x=x+++++O(h) (7) 6336 The equation is called fourth-order because it resembles the true 
so­lutionuptoapower of h4. However, an integration method is rather useless without any means for estimating 
the actual value of the er­ror term. It turns out that an independent third-order approximation h xcan 
be computed by reusing some of the intermediate steps in (7), namely h k1 k2 k3 hf( h x) 4 x=x++++ +O(h).(8) 
6336 The difference between both methods simply equals to hh h 6=x-x=1(k4-hf(x)).(9) 6 This term is an 
estimate of the error of the less accurate formula. However, it can be shown [3] that in many cases this 
estimate can be safely used to control the step size of the more accurate method, too. The idea of adaptive 
step size control is to choose has large as possible while observing a user-de.ned error tolerance TOL. 
For p­th order integration methods the error term scales as hp+1.There­fore if a step size hresults in 
some error 6, an optimized step size h.can be obtained by p . p 1 h=h TOL/6, (10) with a safety factor1. 
With this equation a control mechanism can be set up as follows. We ask the integrator to step forward 
by hand compute 6from Eq. (9). If 6is bigger than TOL, we re­peat the current step with h=h.. Otherwise, 
we proceed and take h=min(h. ,hmax)for the next iteration, where hmax is the max­imum allowed step size. 
If hbecomes much smaller than the grid spacing, we assume that a singularity was encountered and termi­nate 
stream line integration. The resulting adaptive numerical inte­grator, denoted as RK4(3) hereafter, turns 
out to be very robust and well suited for our application. We have also implemented two .fth order methods 
with fourth order error estimation. The .rst method due to Dormand and Prince [4] requires .ve f-evaluations 
per iteration. The other due to Cash and Karp [2] requires six. In our case, where the right hand side 
f is obtained by bilinearly interpolating between discrete grid points, the higher order methods usually 
will not be signi.cantly superior to RK4(3), except for smooth vector .elds sampled at high resolution. 
However, experience shows that they will never be signi.cantly in­ferior either. 5 Selecting Streamlines 
For the fast-LIC algorithm it is not only important to quickly com­pute single stream lines, but also 
to process the output image pixels in such an order that the total number of stream line computations 
is minimized. For instance it is not a good idea to process pixels in scanline order, because it would 
be quite probable that new stream lines hit pixels already being covered by other lines. Instead of look­ing 
for the optimal pixel to be processed next, we simply subdivide the image into smaller blocks, taking 
the .rst pixel of each block, then the second, and so on. With this method the number of com­puted stream 
lines is typically about 2% of the number of image pix­els. It is possible to incorporate some more sophisticated 
schemes here like Sobol quasi-random sequences [17], which may be com­bined with methods for .nding areas 
in the image not covered by stream lines so far. To obtain an approximately equal stream line density 
in the im­age, we stop following an individual line after some distance Mht (cf. pseudocode in Sect. 
3). Ideally, this length should be adjusted automatically. If lots of previously covered pixels are encountered, 
computation should be terminated. However, currently we are us­ing a much simpler scheme which nevertheless 
works reasonably well. We use a .xed Muntil a certain percentage of pixels is hit. For the remaining 
pixels we simply compute a short stream line seg­mentandthecorrespondingconvolutionintegral, butdonottraverse 
the stream line further. Usually a covering limit of 90% and a value Mhtof about 50-100 pixel widths 
yield optimal run times, but these values are not that critical for overall performance. A simple way 
to compensate for a non-optimal stream line selec­tion strategy is to decrease the minimum number of 
hits required for Figure 3: Distances between stream line points as returned by the adaptive numerical 
integrators are usually so large that cubic interpolation is neces­sary to track the path for texture 
sampling. a pixel. Even with a low limit the total number of hits for each pixel may be large due to 
stream lines which are computed later. In fact, for all images in this paper we have taken a limit of 
only a single hit. Despite this low value, each pixel usually will be covered by several stream lines, 
as may be seen from Fig. 2. 6 Texture Map Convolution The ODE solvers discussed in Sect. 4 are able 
to quickly compute long stream lines at guaranteed high accuracy. However, the actual step sizes used 
by these integrators are usually much bigger than the distance htneeded for texture sampling. Therefore 
we have to in­terpolate between every two neighbouring locations returned by the ODE solver. The distance 
between these locations and the curvature of the stream line may easily take values that prohibit the 
use of a simple linear interpolation scheme. This is illustrated in Fig. 3. Av­erage increments from 
10 to 30 times the spacing of the v-grid are quite common in practice. A much better approximation of 
stream lines can be obtained us­ing cubic Hermite-interpolation. This ensures that the .rst deriva­tive 
at the boundaries of the interpolation interval is represented cor­rectly. We need to evaluate the cubic 
interpolation polynomial at evenly spaced sample points only. Therefore forward differences can be employed 
for stream line tracking. After initialization, for­ward differences require just three additions per 
component to eval­uate the polynomial, instead of three additions and three multiplica­tions required 
by Horner s rule. It should be noted that we do not necessarily need to keep interpo­lation separate 
from stream line integration. As an interesting alter­native, so-called continous integration methods 
might be considered [7]. These methods use information gathered during integration to constitute an optimized 
interpolation polynomial, thereby providing dense output directly.  7 Periodic Motion Filters LIC images 
can be animated by changing the shape and location of the .lter kernel kover time. The apparent motion 
is well suited to envision vector .eld direction in addition to the pure tangential in­formation contained 
in static images. In previous work [1] specially designed periodic .lter kernels have been used to achieve 
a motion effect. On .rst sight it might appear dif.cult to combine texture an­imation with the fast-LIC 
algorithm, since the latter is restricted to constant .lter kernels, i.e. box .lters. However, this is 
not the case. In the following we will .rst introduce the notion of intensity corre­lation. We will then 
present a simple blending technique that keeps intensity correlation constant over time and thereby achieves 
high quality animations. a) b) dd tt Figure 4: Intensity correlation between two points on a single stream 
line for different motion .lter kernels: box .lter (a) and Hanning .lter (b). Two periods are shown in 
t-direction. 7.1 Intensity Correlation Using box .lters, an obvious method to animate LIC images is 
to cycle the boxes through some interval along the stream lines. If this is done with equal velocity 
for all pixels, a periodic sequence arises. Cycling a box .lter can be easily accomplished with the fast-LIC 
al­gorithm. Essentially we just have to add some periodic offset func­tion to the limits of the convolution 
sum in Eq. (5). It turns out that this naive approach is not well suited for an­imation since noticeable 
artifacts are introduced when the boxes reenter the interval. To see this, consider two points p1and 
p2on a single stream line that are half a .lter length apart. The corre­sponding pixel intensities initially 
have a 50% correlation because half of the texture cells being convolved are covered by both .lter boxes. 
When the .lter boxes reenter the interval, correlation sud­denly drops to zero, as demonstrated in the 
following .gure: 2d Correlation the beginning. Obviously, such a sequence is not periodic anymore, but 
it will exhibit a constant intensity correlation over time. We have simply discarded all frames associated 
with the peaks in Fig. 4a. A periodic sequence Aof length N/2may be obtained by smoothly blending between 
phase-shifted B-frames, namely 1 2 An =w1(n)Bnmod N+w2(n)B(n+NJmod N (12) with the weights w1and w2chosen 
as follows: 1 w1 w2 0 N/2 N 3N/2 n This means that frames get less and less weighted as their .lter boxes 
get closer to the extreme positions. Whenever wiequals one, the middle frame of Bwill be visible. For 
each pixel both inten­sity contributions are completely independent, provided that .lter boxes do not 
overlap. In this case averaging multiple LIC images is statistically equivalent to computing the convolution 
integral from a modi.ed input texture given as the weighted average of two tex­tures distributed in the 
original way. While effective .lter length L remains the same, averaging multiple frames causes the contrast 
of the resulting image to be reduced. This has to be compensated. In raw LIC images intensity Iof a single 
pixel usually is given by convolving a large number of independent texture cells. Therefore the central 
limit theorem of statistics applies and Ican assumed to be gaussian distributed, that is 50% t0 . . 
pp2 12 (I- ) !(I)=). (13) const. exp(­ 2(2 25% t1 t2 . . . . Here and (2denote average and variance of 
the intensity distribu­ 0% tion !, respectively. Any linear combination of independent gaus­ sian distributed 
quantities will again be distributed gaussian. The d resulting variance is given by (2 = P w 2(2. Consequently, 
after res ii averaging multiple LIC images of equal and (2, the original in- An intensity correlation 
function .measuring the amount of overlap tensity distribution and therefore also contrast can be restored 
by a between .lter kernels kfor two points separated by a distance dmay be de.ned as simple linear scaling, 
R min(k(s,t),k(s+d,t))ds .(d,t)= R (11) k(s,t)ds for each frame t. For a cycled box .lter a plot of this 
function is shown in Fig. 4a. The length of the .lter box was chosen to be 0.5 times the length of the 
interval. Reduced correlation results in a smaller feature size in the resulting LIC images. This is 
perceived as a disturbing artifact in animation. Note, that at the same time distant points temporarily 
become correlated. To achieve a smoother motion Cabral and Leedom [1] suggested to employ a weighted 
.lter kernel made up of two so-called Hanning .lters. Correlation for their n=2.lter is depicted in Fig. 
4b. This function varies signi.cantly less over time than the correlation for thecycledbox.lter. However,itcannotbeusedinconjunctionwith 
the fast-LIC algorithm. Fortunately, there is a simple method capa­ble of generating periodic animation 
sequences that can be used in fast-LIC. With this method no artifacts at all occur due to reentering 
.lter boxes.  7.2 Frame Blending Consider an image sequence Bn, n=0,1,...,N-1, with a .lter box running 
along some stream line segment, but not reentering at I- I p+ . (14) 22 w+w 12 Figure 5 summarizes the 
process of frame blending and intensity rescaling. Note, that for Eq. (14) to be valid intensities need 
to be statistically independent. This is guaranteed if the .lter boxes in the frames being averaged do 
not overlap, i.e. if .lter length does not ex­ceed 0.5times the length of the interval. As an alternative 
we may also use two image sequences computed from completely different input textures. In this case a 
periodic sequence of length Nwould be obtained.  7.3 Variable Velocities The simple blending technique 
described above comes to its real value when the texture is to be animated with variable velocities for 
each pixel. Such animations are useful to display not only vector direction and orientation, but also 
to give an impression of vector magnitude jvj. For variable velocities the standard .lter cycling approach 
will not yield periodic sequences anymore. Forsell [5] describes a tech­nique for endlessly playing back 
a variable motion movie from a .xed number of pre-computed constant speed images. The .nal in­tensity 
for a pixel is computed by interpolating the pixel intensities  . .  . . .    Figure 5: Snapshots 
from a periodic LIC animation obtained by frame blending. The .rst and the last image are identical. 
The .gure contains a schematic view of the differently weighted .lter boxes moving along the stream line. 
In the lower part intensity histograms of the blended images are shown. To keep contrast constant, intensity 
has to be rescaled to .t the original gaussian distribution. from those two images, where the .lter kernel 
phase approximately resembles the actual value. However, there still remains a major problem. With ongoing 
time, .lter kernel phases for neighbouring pixels will lose any correlation. Drastic spatio-temporal 
aliasing ef­fects are introduced. For example the texture may appear to move in the opposite direction 
in some areas. To avoid these effects we build up a variable speed animation from only such frames, where 
the .lter kernel phases are corre­lated. Correlated frames can be produced by letting .lter boxes move 
some variable distance proportional to their velocity, as de­picted in the following picture: .. 2 22 
222 (=(w+w2)(1-u)+u(.(15) res 1 With this equation we are able to rescale intensity of every pixel so 
that the original (2is restored. In this way a high quality animation sequence is obtained. It should 
be noted that building up animation sequences from shifted box .lter convolutions requires accurate stream 
line com­putation, because highly unsymmetric convolution ranges can oc­cur. These will emphasize errors 
due to poor numerical integration. For example, circular stream lines may be falsely depicted as spirals. 
Artifacts of this kind are usually disguised by a symmetric .lter ker­nel [18, 1]. They do not occur 
if stream line integration is accurate.  8 Smooth Detail Enlargement For many applications it is useful 
to adjust the size of a LIC input texture, so that a single texture cell is covered by lots of output 
im­age pixels. This can be easily accomplished with the fast-LIC algo­rithm. As before we are using Eq. 
(5) to compute the convolution integral for some initial point x0. It is suf.cient to sample the in­ 
v1 t0 put texture at increments ht =0.5times the width of a texture cell. However, when stepping along 
the stream line and updating the in­tegral according to Eq. (6), we use a smaller step size in order 
to ensure that we hit as many pixels covered by the stream line as be­fore. Of course, using a smaller 
step size means that the value of kin Eq. (6) has to be adjusted, too. The ability to choose the sizes 
. . . v2 < v1 t1 . . . t2 of input texture and output image independently can be exploited in several 
ways. To generate a periodic sequence we would like to use the blending First, in LIC images created 
from high frequency input textures, such as white noise, these high frequencies are retained in direc­technique 
described above again. However, in general the intensi­ ties being averaged are not independent because 
.lter boxes overlap tions perpendicular to the .eld direction. This is caused by the one­ dimensional 
nature of the .lter kernel. The resulting images often in regions of low velocity. Therefore Eq. (14) 
is not valid anymore. It is also not a good idea to use two image sequences computed from look quite 
busy. Problems arise if the images are to be processed by lower bandwidth .lters like video tape recorders 
or image com­different input textures. This would cause the LIC pattern to change over time in regions 
of low velocity. Although no .ow would be pression algorithms. The usual remedy is to use a low-pass 
.ltered input texture or to blur the .nal LIC images afterwards. With our al­perceived, blending between 
different patterns is somehow irritat­ ing. Instead, we have to rescale intensity locally according to 
the ac­gorithm convolutions over long distancesLcan easily be computed. Therefore a better approach is 
to simply scale up the size of a texture tual amount of .lter box overlap. Overlap is inversely proportional 
to velocity and may be described by a number u20,1].An ex­cell as well as convolution length Lin terms 
of pixel width. pression for the resulting local variance can be derived by splitting With traditional 
LIC it is hard to generate exactly the same im­blended intensity into three independent contributions, 
one due to age at different resolutions. It would require to use both a resampled the overlapping part 
and two due to the non-overlapping parts of the input texture as well as a resampled vector .eld. This 
approach is individual boxes: tedious and will unnecessarily introduce errors. However, often it is important 
to create several versions of a single image at different resolutions, e.g. adopted to various output 
devices, or for use in ani­mationsthatrequiredistancedependenttextureresolution. Thiscan beeasilyaccomplishedwith 
fast-LIC sincethe sizeofthe outputim­age can be chosen independently of vector .eld resolution and the 
input texture. A slightly different utilization of this feature is the computation of smooth zooms into 
the vector data .eld to enlarge interesting de­tails. As an example some close-ups of details in a vector 
.eld are Figure 6: Details of a vector .eld displayed at different magni.cation fac­tors (1, 3, 15, 
100). For each frame a completely new LIC image has been computed. The data set had a resolution of 5002. 
At the .nest level only a few grid points are covered. shown in Fig. 6, where linear magni.cation extends 
up to a factor 100. If the zoom is to be played back in a sequence, care has to be taken for low magni.cation 
factors. If stream line integration is unconditionally started at the center of output image pixels, 
then in each frame slightly different stream lines are computed. This causes annoying variations in texture 
to occur from frame to frame. One solution would be to increase the minimal number of hits required for 
a pixel. Another more robust method is to try exactly the same stream lines used in the previous frame 
.rst. For these lines the start­ing point will not correspond to the center of an output image pixel 
anymore. Remaining pixels are treated as usual afterwards. This method yields smooth animation sequences, 
allowing one to com­pute striking trips into details. 9 Results We have implemented the fast-LIC algorithm 
in the C++ program­ming language within the framework of the modular visualization environment IRIS Explorer. 
Within this system it is possible to pre­process the vector .eld as well as to post-process the resulting 
LIC images in various ways. We have found it especially useful to apply a directional gradient .lter 
to the raw LIC images to further empha­size directional information. Another useful method is to multiply 
color into the images to simultaneously visualize a scalar quantity in addition to vector .eld orientation. 
The images in Fig. 1, 7, and 8 were post-processed in this way. The data shown in Fig. 1 comes from so-called 
hyperthermia sim­ulation, a form of cancer therapy based upon radiating radio waves into the human hip 
region. In Fig. 7 electrical .eld lines irraditated by a dipole antenna are depicted. This image has 
to be compared with Fig. 6a. In both cases the same vector .eld is shown. However, after gradient .ltering 
and coloring, the image looks much more at­tractive. In Fig. 8 a snapshot from the simulation of an instationary 
.uid .ow around a cylinder is shown. Finally, Fig. 9 presents an application of LIC in modern art. L 
LIC RK CK DP Hyperthermia 400.600 10 12.26 3.36 3.65 3.55 20 21.93 3.75 4.15 3.99 40 41.36 4.60 5.20 
4.88 Dipole 500.500 10 18.35 4.35 4.41 4.31 20 34.29 4.78 4.81 4.60 40 71.14 5.61 5.61 5.39 Cylinder 
600.200 10 7.76 1.49 1.54 1.57 20 14.44 1.62 1.65 1.70 40 27.01 1.92 1.99 2.00 Table 1: Performance 
of the original LIC algorithm compared to the new algorithm equipped with different numerical integrators: 
RK = adaptive Runge-Kutta scheme RK4(3), CK = Cash and Karp, DP = Dormand and Prince (cf. Sect. 4). The 
boldface entry gives the shortest time in each row. Table 1 summarizes some execution times of fast-LIC 
compared to the original LIC algorithm of Cabral and Leedom. The numbers, obtained on a SGI Indigo2with 
150 MHz MIPS R4400, are in sec­onds. They refer to the vector .elds shown in Fig. 1, 7 and 8, but do 
nottakeinto accountcomputingtime forgradient.lteringandcolor­ing. For better comparision with the original 
algorithm the dimen­sions of input texture, vector .eld, and resulting image were cho­sen to be equal. 
The actual sizes are indicated in the table. Lis the extent of the convolution integral in one direction. 
The table con­tains different columns for various numerical integrators we have implemented. These integrators 
do not differ much in performance. Usually only about 25% of the time is spent in stream line integra­tion. 
Most time is spent in texture sampling. For the hyperthermia data set, fast-LIC performs somewhat worse 
than in the other exam­ples. This is caused by the discontinuities in the vector .eld, forc­ing the adaptive 
integrators to choose very small step sizes across the boundaries. The higher order methods are more 
affected by this than RK4(3). 10 Conclusion We have introduced a new line integral convolution algorithm 
that performs an order of magnitude faster than previous methods. A feature of our method is the ability 
to compute images at arbitrary resolution. We presented methods for producing high quality tex­ture animation 
sequences, employing constant .lter kernels only. The new techniques have particular signi.cance for 
computer graphics. They are useful for fast procedural generation of textures with directional features 
and of texture sequences with continously variable spatial resolutions. The production of such sequences 
is of growing interest in computer animation, where several versions of a texture with different spatial 
resolutions are often needed for dif­ferent views or output media. There are a number of directions for 
future research. We intend to investigate the visualization of time varying and three-dimensional vector 
.elds. The inclusion of visual representations of global and local vector .eld characteristics other 
than .ow lines is also an in­teresting topic that deserves further investigation. Finally there is room 
for considerable further research work with respect to computer animation, e.g. concerning the production 
of hi­erarchies of directional textures with different spatial resolutions, or new methods for synthesizing 
vector .elds from images to auto­convolve them. This may lead to a new class of directional .lters for 
image processing.  Acknowledgements We would like to thank Charlie Gunn, Roland Wunderling, and Ger­hard 
Zumbusch for reviewing the manuscript and for various help­ful discussions. We are also grateful to the 
anonymous reviewers of this paper for their valueable remarks, and to Brian Cabral and Casey Leedom for 
making their code available on the net. References [1] Brian Cabral and Leith (Casey) Leedom. Imaging 
vector .elds using line integral convolution.Proc. ofSIGGRAPH 93 (Anaheim,Califor­nia, August 1-6, 1993). 
In Computer Graphics 27, Annual Conference Series, 1993, ACM SIGGRAPH, pp. 263 272. [2] J. R. Cash and 
Alan H. Karp. A variable order Runge-Kutta method for initial value problems with rapidly varying right-hand 
sides. ACM transactions on Mathematical Software 16, pp. 201 222, 1990. [3] Peter Deu.hard and Folkmar 
Bornemann. Numerische Mathematik II: Integration gew¨ohnlicherDifferentialgleichungen. Verlag de Gruyter, 
Berlin, 1994. [4] J. R. Dormand and P. J. Prince. Higher order embedded Runge-Kutta formulae. J. Comp. 
Appl. Math., 7, pp. 67 75, 1981. [5] Lisa K. Forssell. Visualizing .ow over curvilinear grid surfaces 
using line integral convolution. In Proc. of Visualization 94, (Washington, D.C., Oct. 17-21, 1994), 
pp. 240 247, IEEE Computer Society, 1994. [6] Paul E. Haeberli. Paint by numbers: Abstract image representations. 
Proc. of SIGGRAPH 90 (Dallas, Texas, August 6-10, 1990). In Com­puter Graphics, 24(4) (August 1991), 
pp. 207 214. [7] Ernst Hairer, Syvert Paul Nørsett, and Gerhard Wanner. Solving Or­dinary Differential 
Equations I, Nonstiff Problems. Springer Verlag, Berlin, Heidelberg, New York, Tokyo, 1987. [8] James 
L. Helman and Lambertus Hesselink. Visualizing vector .eld topology in .uid .ows. IEEE Computer Graphics 
and Applications, 11(3), pp. 36 46, May 1991. [9] Andrea J. S. Hin and Frits H. Post. Visualization of 
turbulent .ow with particles. In Proc. of Visualization 93 (San Jose, California, October 25-29, 1993), 
pp. 46 52, IEEE Computer Society, 1993. [10] Jeff P. M. Hultquist. Interactive numerical .ow visualization 
using stream Surfaces. Computing Systems in Engineering, 1(2-4), pp. 349 353, 1990. [11] Kwan-Liu Ma 
and Philip J. Smith. Virtual smoke: An interactive 3d .ow visualization technique. In Proc. of Visualization 
92 (Boston, MA, October 19-23, 1992), pp. 46 52, IEEE Computer Society, 1992. [12] Gordon D. Mallinson. 
The calculation of the lines of a three­dimensional vector .eld. In Proc. of the International Symposium 
on Computational Fluid Dynamics (Sydney, Australia, August 1987), pp. 525 534, North-Holland, August 
1988. [13] Jerrold E. Marsden and Anthony J. Tromba. Vector Calculus.W. H. Freeman, New York, 3rd edition, 
1988. [14] Nelson Max, Barry Becker, and Roger Craw.s. Flow volumes for in­teractive vector .eld visualization. 
In Proc. of Visualization 93 (San Jose, CA, Oct. 25-29, 1993), pp. 19 24, IEEE Computer Society, 1993. 
[15] Nancy J. Nersessian. Faraday s .eld concept. In D. Gooding and F. A. J. L. James, eds., Faraday 
Rediscovered: Essays on the Live and Work of Michael Faraday, pp. 175 187. Stockton Press, New York, 
1985. [16] Ken Perlin. An image synthesizer. Proc. of SIGGRAPH 85 (San Francisco, California, July 22 
26, 1985). In Computer Graphics 19, pp. 287 296, 1985. [17] William H. Press, Saul A. Teukolsky, William 
T. Vetterling, and Brian P. Flannery. Numerical Recipes in C: The Art of Scienti.c Com­puting. Cambridge 
University Press, Cambridge, 2nd edition, 1992. [18] Jarke J. van Wijk. Spot noise-texture synthesis 
for data visualiza­tion. Proc. of SIGGRAPH 91 (Las Vegas, Nevada, 28 July -2 August, 1991). In Computer 
Graphics, 25, pp. 309 318, 1991. [19] Jarke J. van Wijk. Rendering surface-particles. In Proc. of Visual­ization 
92 (Boston, Massachusetts, October 19-23, 1992), pp. 54 61, IEEE Computer Society, 1992.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218449</article_id>
		<sort_key>257</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[Correction of geometric perceptual distortions in pictures]]></title>
		<page_from>257</page_from>
		<page_to>264</page_to>
		<doi_number>10.1145/218380.218449</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218449</url>
		<keywords>
			<kw><![CDATA[distortion]]></kw>
			<kw><![CDATA[perception]]></kw>
			<kw><![CDATA[perspective]]></kw>
			<kw><![CDATA[viewing transformations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor>Transform methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003717</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Computation of transforms</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39037916</person_id>
				<author_profile_id><![CDATA[81100328351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Denis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zorin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14034821</person_id>
				<author_profile_id><![CDATA[81100070192]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Barr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Leonardo da Vinci. Notebooks I, H. Dover, New York, 1970.]]></ref_text>
				<ref_id>dV70</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180886</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[G. Glaeser. Fast Algorithms for 3-D Graphics. Springer-Verlag, New York, 1994.]]></ref_text>
				<ref_id>Gla94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Margaret A. Hagen. Influence of picture surface and station point on the ability to compensate for oblique view in pictorial perception. Developmental Psychology, 12(1):57-63, January 1976.]]></ref_text>
				<ref_id>Hag76</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Margaret A. Hagen, editor. The Perception of pictures. Academic Press series in cognition and perception. Academic Press, New York, 1980.]]></ref_text>
				<ref_id>Hag80</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Margaret A. Hagen, Harry B. Elliott, and Rebecca K. Jones. A distinctive characteristic of pictorial perception: The zoom effect. Perception, 7(6):625-633, 1978.]]></ref_text>
				<ref_id>HEJ78</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Michael. Kubovy. The psychology of perspective and Renaissance art. Cambridge University Press, Cambridge &lt;Cambridgeshire&#62; ; New York, 1986.]]></ref_text>
				<ref_id>Kub86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42249</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Brian E Flannery, Saul A. Teukolsky, and William T. Vetterling. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press, Cambridge, UK, 1988.]]></ref_text>
				<ref_id>PFTV88</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M.H. Pirenne. Optics, painting and photography. Cambridge University Press, New York, 1970.]]></ref_text>
				<ref_id>Pir70</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[L.S. Pontriagin. The mathematical theory of optimal processes. Interscience Publishers, New York, 1962.]]></ref_text>
				<ref_id>Pon62</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[B.V. Raushenbakh. Sistemy perspektivy v izobrazitel'nom iskusstve : obshchaia teoriia perspektivy. Nauka, Moskva, 1986. in Russian.]]></ref_text>
				<ref_id>Rau86</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Jack Tumblin and Holly E. Rushmeier. Tone reproduction for realistic images. IEEE Computer Graphics and Applications, 13(6):42-48, November 1993. also appeared as Tech. Report GIT-GVU-91-13, Graphics, Visualization &amp; Usability Center, Coll. of Computing, Georgia Institute of Tech.]]></ref_text>
				<ref_id>TR93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Denis Zorin. Perceptual distortions in images. Master's thesis, Caltech, 1995.]]></ref_text>
				<ref_id>Zor95</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Correction of Geometric Perceptual Distortions in Pictures. Denis Zorin, Alan H. Barr California Institute 
of Technology, Pasadena, CA 91125 Abstract We suggestan approachfor correcting several types of perceived 
geometric distortions in computer-generated and photographic im­ages. The approach is based on a mathematical 
formalization of desirable properties of pictures. From a small set of simple assumptions we obtain perceptually 
preferable viewing transformations and show that these transforma­tions can be decomposed into a perspective 
or parallel projection followed by a planar transformation. The decomposition is eas­ily implemented 
and provides a convenient framework for further analysis of the image mapping. We prove that two perceptually 
important properties are incom­patible and cannot be satis.ed simultaneously. It is impossible to construct 
a viewing transformation such that the images of all lines are straight and the images of all spheres 
are exact circles. Percep­tually preferable tradeoffs between these two types of distortions can depend 
on the content of the picture. We construct parametric families of transformations with parameters representing 
the rela­tive importance of the perceptual characteristics. By adjusting the settings of the parameters 
we can minimize the overall distortion of the picture. It turns out that a simple family of transformations 
produces results that are suf.ciently close to optimal. We implement the pro­posed transformations and 
apply them to computer-generated and photographic perspective projection images. Our transformations 
can considerably reduce distortion in wide-angle motion pictures and computer-generated animations. Keywords: 
Perception, distortion, viewing transformations, perspective. Introduction The process of realistic image 
synthesis can be subdivided into two stages: modeling the physics of light propagation in three­dimensional 
environments and projecting the geometry of three­dimensional space into the picture plane (the viewing 
transfor­mation. ) While the .rst stage is relatively independent of our understanding of visual perception, 
the viewing transformations are based on the fact that we are able to perceive two-dimensional patterns 
-pictures -as reasonably accurate representations of three­dimensional objects. We can evaluate the quality 
of modeling the propagation of light objectively, by comparing calculated photomet­ric values with experimental 
measurements. For viewing transfor­mations the quality is much more subjective. Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 a. b.  Figure 1. a. Wide-angle pinhole 
photograph taken on the roof of the Church of St. Ignatzio in Rome, classical example of perspective 
distortions from [Pir70]; reprinted with the permission of Cambridge University Press. b. Corrected version 
of the picture with transformation applied. The perspective projection 1 is the viewing transformation 
that has been primarily used for producing realistic images, in art, pho­tography and in computer graphics. 
One motivation for using perspective projection in computer 1By perspective projection or linear perspective 
we mean either central projection or parallel projection into a plane. graphics is the idea of photorealism: 
since photographic images have one of the highest degrees of realism that we can achieve, perhaps realistic 
rendering should model the photographic process. But the intuitive concept of realism in many cases differs 
from photorealism: photographic images, are often perceived as distorted (note the shape of the sphere 
in Fig. 1a.) On the other hand, some paintings, while using perspective projection, contain considerable 
deviations from it (Fig. 2). These paintings, however, are perceptu­ally correct and realistic. In this 
paper we derive viewing transformations from some basic principles of perception of pictures rather than 
by modeling a partic­ular physical process of picture generation. Our approach is based on formalizations 
of desirable perceptual properties of pictures as mathematical restrictions on viewing transformations. 
The main result (Section 5.1) allows us to construct usable fam­ilies of transformations; it is a decomposition 
theorem which states that under some assumptions, any perceptually acceptable picture­independent viewing 
transformation can be decomposed into a per­spective or parallel projection and a two-dimensional transforma­tion. 
 Figure 2. School of Athens by Rafael. (c @1994-95 Christus Rex, repro­duced by permission) It is possible 
to reconstruct the center of projection from the architectural details. The calculated image of the sphere 
in the right part of the picture is an ellipse with aspect ratio 1:1.2, while the painting is a perfect 
circle. Our approach allows us to achieve several goals: We construct new viewing transformations that 
reduce distor­tions that appear in perspective projection images. It turns out that some of these transformations 
can be implemented as a postprocessing stage (Equation 1 , pseudocode in Section 7) after perspective 
projection and can be applied to existing images and photos (Figs. 1,7,8,9,10.)  We provide a basis 
for understanding limitations of two­dimensional images of three-dimensional space; certain per­ceptual 
distortions can be eliminated only at the expense of increasing other distortions.  Our transformation 
works well in animations and movies.  Our families of transformations can be modi.ed or extended by 
adding or removing auxiliary perceptual requirements; this provides a general basis for constructing 
pictures with desirable perceptual properties.  The transformations that we propose may have a number 
of applications: creation of computer-generated wide-angle pictures and wide-angle animations with reduced 
distortion, and correction of photographic images and movies. Related Work Considerable data on picture 
perception have been accumulated by experimental psychologists; overviews can be found in [Kub86], [Hag80]. 
Computer graphics was in.uenced by the study of human vision in many ways: for example, RGB color representation 
is based on the trichromatic theory of color perception and anti-aliasing is based on various observations 
in visual perception. Principles of the perception of color have been applied to com­puter graphics [TR93]. 
A curvilinear perspective system based on experimental data is described in [Rau86]. Limitations of perspective 
projection are well known in art and photography. [Gla94] mentions the limitations of linear perspective. 
As far as we know, this paper is the .rst attempt to apply per­ceptual principles to the analysis and 
construction of viewing trans­formations in computer graphics. Outline of the Paper. The paper is organized 
as follows:2 In Section 2 we discuss the properties of linear perspective, in Section 3 we formulate 
our assumptionsaboutperception of pictures and formulate some desirable properties. Section 4 describes 
some restrictions that we have to impose on the viewing transformation to make it practical. In Section 
5.1 we discuss the decomposition theorem for viewing transformations. In Section 5.3 we discuss construction 
of the 2D component of decomposition, Section 6 describes a perceptual basis for the choice of the pro­jection 
component of the decomposition of viewing transformation. In Section 7 we discuss the implementation 
issues and we pro­pose some applications of our methods. Sketches of mathematical proofs can be found 
in appendices in the CD-ROM version of the paper and in [Zor95]. 2Analysisoflinearperspective. The theory 
of linear perspective is based on the following construc­tion (Fig. 3). Suppose the eye of an observer 
is located at the point O. Then, the image on the retina of his eye is created by the rays re­.ected 
from the objects in the direction of point O. If we put a plane between the observer and the scene, and 
paint each point on the plane with the color of the ray of light going into O and crossing the plane 
at this point, the image on the retina will be indistinguishable from the real scene. eye picture object 
Figure 3. Pictures can produce the same retinal projection as a real object The above argument contains 
some important assumptions: the observer looks at the scene with one eye, (or is located far enough 
from the image plane to consider the images in both eyes identical);  when we look at the picture, the 
position of the eye coincides with the position of the eye or camera when the picture was made.  In 
fact, both assumptions for linear perspective are almost never true. We can look at a picture from various 
distances and directions 2The reader who is interested primarily in the implementation can go directly 
to Equation 1 in Section 5.3 and pseudocode in Section 7 with both eyes, but our perception of the picture 
doesn t change much in most cases [Hag76]. This property of pictures makes them different from illusions: 
while stereograms of all kinds should be observed from a particular point, traditional pictures are relatively 
insensitive to the changes in the viewing point. As the assumptions are not always true, it is not clear 
why perspective projection should be the preferred method of mapping the three-dimensional space into 
the plane. In many cases we observe that perspective projection produces pictures with apparent distortions 
of shape and size of objects, such as distortions of shape in the margins (Figs. 1,7,8,9,10). These distortions 
are signi.cantly ampli.ed in animations and movies, resulting in shape changes of rigid bodies. Leonardo 
s rule. The fact that linear perspective doesn t al­ways produce pictures that look right was known to 
painters a long time ago. Leonardo da Vinci [dV70] formulated a rule which said that the best distance 
for depicting an object is about twenty times the size of the object. It is well-known in photography 
that in order to make a good portrait the camera should be placed far enough away from the object. In 
many paintings we can observe consider­able deviations from linear perspective which in fact improve 
their appearance (Fig. 2.) We conclude that there are a number of reasons to believe that linear perspective 
is not the only way to create realistic pictures. 3Propertiesofpictures. In this section we will describe 
our main assumptions about the nature of picture perception and specify the requirements that we will 
use in our constructions. A more detailed exposition can be found in [Zor95]. Structural features. We 
believe the that the features of images that are most essential for good representation are the structural 
features such as dimension (whether the image of an object is a point, a curve or an area) and presence 
or absence of holes or self­intersectionsintheimage. Thepresenceorabsenceofthesefeatures can be determined 
unambigously. Most of the visual information that is available to the brain is contained in the images 
formed on the retina. We will postulate the following general requirement, which will de.ne our concept 
of realistic pictures: The retinal projections of a two-dimensional image of an object should not contain 
any structural features that are not present in any retinal projection of the object itself. Structural 
properties of retinal images are identical to the properties of the projections into an arbitrarily chosen 
plane [Zor95]; our requirement can be restated in more intuitive form: A two-dimensional image of an 
object should have only structural features that are present in some planar projection of the object. 
We can identify many examples of structural requirements: the image of a convex object without holes 
should not have holes in it, the image of a connected object cannot be disconnected, images of two intersecting 
objects should intersect etc. We choose a set of three structural requirements that we will use to prove 
the decomposition theorem in Section 5.1. Figure 4. Mappings forbidden by structural conditions 2 and 
3 1. The image of a surface should not be a point. 2. The image of a part of a straight line either 
shouldn t have self-intersections ( loops ) or else should be a point (Fig. 4).  3. The image of a plane 
shouldn t have twists in it. This means that either each point of the plane is projected to a different 
point in the image, or the whole plane is projected onto a curve (Fig. 4). We will call these conditions 
structural conditions 1, 2 and 3. Note that these requirements are quite weak: we don t require that 
features of some particular planar projection are represented; we just don t want to see the features 
that are not present in any projection. Desirable properties. Next, we formulate some requirements that 
are not as essential as the structural ones; the corresponding fea­tures of the images can be varied 
continuously and can be changed within some intervals of tolerance. Examples of such features in­clude 
relative sizes of objects, angles between lines, verticality. We will refer to these properties as desirable 
properties. We will use two of them which we consider to be the most important. One of the most restrictive 
desirable properties is the following one: Zero-curvature condition. Images of straight lines should 
be straight. Note that this is different from the structural requirement 2 above, which is weaker. However, 
as we can judge straightness of lines only with some .nite precision, some deviations from this property 
can be tolerated. Another requirement is based on the following observation: the images of objects in 
the center of the picture never look distorted, given that the distance to the center of projection is 
large compared to the size of the object (Leonardo s rule). We will call perspective projections into 
the plane perpendicular to the line connecting the center of projection with the object direct view projections.Then 
the requirement eliminating distortions of shape can be stated as follows: Direct view condition Objects 
in the image should look as if they are viewed directly as they appear in the middle of a photograph. 
Unfortunately, as we will see later, the two properties formulated above cannot be satis.ed exactly at 
the same time. We found several other requirements (foreshortening of objects, relative size of objects, 
verticality) to be of importance, but having much larger tolerance intervals. We will discuss their signi.cance 
in Section 7. 4Technicalrequirements To narrow down the area of search for perceptually acceptable view­ing 
transformations we are going to specify several additional tech­nical requirements. They don t have any 
perceptual basis and are quite restrictive; however, they make the task of constructing view­ing transformations 
manageable and the resulting transformation can be applied to a wide class of images. 1. We need a parametric 
family of viewing transformations so that an appropriate one can be chosen for each image. 2. The number 
of parameters should be small, and they must have a clear intuitive meaning. 3. The mapping must be 
suf.ciently universal and should not depend on the details of the scene too much.  5Derivationofviewingtransformations 
In the following sections we formalize the perceptual and techni­cal conditions that were stated above 
and use them to prove that any viewing transformation that satis.es the structural conditions and technical 
conditions for any image can be implemented as a perspective projection and subsequent transformation 
of the pic­ture plane. We show that direct view and zero curvature properties cannot be exactly satis.ed 
simultaneously. We introduce quanti­tative measures of corresponding distortions and describe a simple 
parametrized family of transformations (Equation 1) where values of parameters correspond to the tradeoff 
between the two types of distortion. This family of transformation is close to optimal in a sense described 
in Section 5.3 and is easy to implement (Section 7). 5.1Generalstructureofviewingtransformations picture 
plane p Figure 5. Coordinate systems In this section we present a decomposition theorem derived from 
structural conditions 1 -3 (Section 3) and technical requirements of the previous section. The seemingly 
weak structural conditions 1-3 turn out to be quite restrictive if we want to construct image mappings 
that don t depend on the details of any particular picture, speci.cally, on the presence of lines or 
planes in any particular point of the depicted scene. Applicaton of these requirements to all possible 
lines and planes allows us to prove that the viewing transformation with no twists in the images of planes 
and no loops or folds in the images of lines should be a composition of perspective or parallel projection 
and a one-to-one transformation of the plane. In order to formulate the precise result let s introduce 
some de.nitions and notation: We will use x,y,..for the points in the domain of a viewing transformation 
(a volume in 3D space), and O,1...for the points in the range (a point in the picture plane). By a line 
segment we mean any connected subset of a line. A viewing transformation maps many points in space to 
the same point in the picture plane: De.nition 1 The set of all points of the domain of a mapping that 
map to a .xed point Ois called the .ber of the mapping at the point O. In our case, .bers typically are 
curves in space that are mapped into single points in the picture plane. Consider a viewing transformation 
which is a continuous map­ping P of a region of space to the picture plane, more particularly, of an 
open path-connected domain V CR3 to an open subset of R2, satisfying the following formalizations of 
structural conditions: 1. The mapping of any line l to its image P(l) is one-to-one everywhere or it 
is a point. This condition prevents loops in the images of lines. It is more restrictive: it doesn t 
allow not only loops but also folds , that is, situations when almost any point inP(l)isthe image of 
at least two points of l. 2. The dimension of the .ber dim P-1(O)= 1forall Oin the range of P, and all 
the .bers are path-connected. This condition prevents mapping of regions of space to single points and 
continuous regions to discontinuous images. 3. the mapping of a subset of a plane m to the image P(m)is 
one-to-one everywhere or nowhere. This condition prevents twists in the images of planes.  Then our 
theorem can be stated as follows: Theorem 1 For any viewing transformation P, satisfying the con­ditionsabove,thereis 
aperspectiveprojection Isuch that the .bers of P are subsets of .bers of I. An outline of the proof is 
given in Appendix A. Our practical applications are based on the following corollary: Corollary 1 Let 
a viewing transformation P satisfy the assumptions of Theorem 1 and Ibe the corresponding perspective 
projection. If Iis a central projection, assume, in addition, that the region V lies entirely in one 
half-space with respect to a plane going through the center of I. Then P can be decomposed in two ways: 
asa compositionofa perspectiveprojection Iplane into a plane followed by a transformation Tplane of the 
plane , as a projection into a sphere Isphere followed by one-to-one mapping Tsphere of the sphere into 
the picture plane . It is not true that any picture satisfying only structural conditions (without additional 
technical requirements) should be generated with an image mapping which has this particular decomposition, 
because for a particular scene the structural conditions have to be satis.ed only by the objects that 
are actually present in it. It also should be noted that our theorem is an example of a large num­ber 
of statements that can be proven given some speci.c choice of structural conditions. We believe that 
our choice is reasonable for many situations, but it is quite possible that there are cases when least 
restrictive requirements are suf.cient and larger families of transformations can be considered. While 
the choice of possible viewing transformations is consid­erably restricted by this theorem, there are 
still several degrees of freedom left: 2D mappings Tplane and Tsphere can be any continuous map­pings. 
 We can choose the center of projection for the .rst part of the decomposition; it is important to note 
that the theorem places no restrictions on the position of this center. For example, in an of.ce scene 
it can be located outside the room, which is impossible for physical cameras.  If we can split our scene 
into several disconnected domains (for example, foreground and background), the viewing trans­formation 
can be chosen independently for each connected part of the scene. However, separation of the space into 
sev­eral path connected domains introduces dependence of the transformation on the scene.  In the next 
sections we will consider how we can use these degrees of freedom to minimize the perceptual distortions. 
5.2Formalizationofdesirableconditions In this section we formalize the conditions listed in Section 5.1, 
to apply them to the construction of viewing transformations. We will .nd error functions for both conditions 
that can be used as a local measure of distortion and error functionals that measure the global distortion 
for the whole picture. Let s establish some notation for the viewing transformations that satisfy the 
conditions of the theorem. Iplane ' V CR3 Wplane CR2= p H H P H Isphere Tplane H H H j Tsphere 'R2 Wsphere 
CS2 = 7 We will consider viewing transformations P : V CR3 -R2, from an open connected domain V into 
the picture plane 7,which are compositions of the projection Iplane from the center O into the intermediate 
plane p and a mapping Tplane : Wplane CR2 -R2, Wplane = Iplane (V), We can choose the plane p so that 
that the distance from O to the plane is L. P can be also represented as a composition of central projection 
from O into the sphere of radius L with center at O (intermediate sphere ) Isphere : V CR3 -S2 and some 
mapping Tsphere : Wsphere CS2 -R2, Wsphere = Isphere(V), We will assume that the image of V in the sphere 
belongs to a hemisphere. Let s introduce rectangular coordinates (x,y) and polar coordi­nates (r,<) on 
the plane p; rectangular coordinates (r,O) and polar coordinates (p,1) in the plane 7. On the sphere 
we will choose angular coordinate system (e,(), and local coordinates in the neigh­borhood of a .xed 
point (e0,(0): u = L(e-e0),v = L((-(0)sin e0 (Fig. 5). The correspondence between Tplane and Tsphere 
is given by the mapping S2 -R2: <= (, r =tan e. Curvature error function Formally the restriction on 
images of line segments from section 3 can be expressed as follows: Images of line segments should have 
zero curvature at each point. We will call this requirement the zero-curvature condition.Cur­vature of 
the image of a line at a .xed point gives us a measure of how well the viewing transformation satis.es 
the zero-curvature condition. If we consider the decomposition of P = Tplane 0Iplane , we can observe 
that Iplane satis.es the zero-curvature condition. There­fore, we have to consider only the mapping Tplane 
. We will denote components of Tplane (x,y), which is a point in the picture plane, by (r(x,y),O(x,y) 
The curvature depends not only on the point but also on the direction of the line, whose image we are 
considering. As an error function for the zero-curvature condition at a point x we will use an estimate 
of the maximum curvature of the image of a line going through x. It can be shown (see Appendix B on CD-ROM, 
[Zor95]) that the curvature p 2 222 j,jjrxx j+ jryyj 2+2 jrpxy j+ jOxxj+ jOyyj 2+2 jOxyj 1 2 ((A + C) 
-(A -C)2 +4B2) 22 22 where A = (rx)+ (Ox), B = rxry + OxOy, C = (ry)+ (Oy). We use a characteristic size 
of Tplane (W) (corresponding to the size of the picture for perspective projection) R0 as a scaling coef.cient 
to obtain a dimensionless error-function. We also use the square of the curvature estimate to make the 
expression simpler: 2 222 jrxxj+ jryy j 2+2 jrxyj+ jOxxj+ jOyyj 2+2 jOxyj K(Tplane ,x,y)= R02 (p)2 1 
4(A + C) -(A -C)2 +4B2 If we set K(Tplane ,x,y) = 0, we can see that the all the second derivatives of 
rand Oshould be equal to zero,therefore, Tplane should be a linear transformation. This coincides with 
the fundamental theorem of af.ne geometry which says that the only transformations of the plane which 
map lines into lines are linear transformations. Direct view error function. In order to formalize the 
direct view condition we consider mappings which are locally equivalent to direct projection as de.ned 
in Section 3. We can observe that the projection onto the sphere is locally a direct projection. Therefore, 
if we use the decomposition P = Tsphere 0Isphere we have to con­struct the mapping Tsphere which is locally 
is as close to a similarity mapping as possible. Formally, it means that the differential of the mapping 
Tsphere , which maps the tangent plane of the sphere at each point x to the plane Tf (x)R2= R2 coinciding 
with the picture plane 7at the point f (x), should be close to a similarity mapping. The differential 
DTf (x) can be represented by the Jacobian matrix J of the mapping Tsphere at the point x. A nondegenerate 
linear transforma­tion J is a similarity transformation if and only if jJwjIjwjdoesn t depend on w. If 
this ratio depends on w, then we de.ne the direct view error function to be 22 jJwjjJwj D(Tsphere ,e,()= 
max2Imin2-1 jwjjwjcan be used as the measure of non-directness of the transformation at the point (for 
more detailed discussion see [Zor95].). It can be shown (see Appendix B on CD-ROM, [Zor95]) that p (E 
+ G) -(E -G)2 +4F2 D(Tsphere ,e,()= p-1 (E + G)+ (E -G)2 +4F2 2 where E = (ru)2+ (Ou)2, F = rurv + OuOv, 
G = (rv)2+ (Ov). Using the correspondence between intermediate sphere and plane we can write D as a function 
of Tplane , x and y. Global Error functionals We want to be able to characterize the global error for 
each of the two perceptual requirements. We can use a norm of the error functions D and K as a measure 
of the global error. The choice of the norm can be different: the L1­norm corresponds to the average 
error, the sup-norm corresponds to the maximal local error. In the .rst case, the error functionals are 
de.ned as ZZ K[Tplane ]= K(Tplane ,x,y)dxdy, W ZZ D[Tplane ]= D(Tplane ,x,y)dxdy. W In the second case, 
K[Tplane ]= max K(Tplane ,x,y) D[Tplane ]= max D(Tplane ,x,y)(x,y)2W (x,y)2W . 5.32DTransformation We 
are going to use the error functionals de.ned in Section 5.2 to construct families of transformations 
by solving an optimization problem. We consider only the 2D part of the decomposition of viewing transformation 
assuming that the perspective projection is .xed. Structural conditions suggest only that it be continuous 
and one-to-one. Optimization problem. The error functionals de.ned above depend on the domain Wplane 
and the planar transformation Tplane . The .rst parameter is de.ned by the projection part of the viewing 
transformation, so we will assume it to be .xed now. In this case, functions satisfying K[Tplane ] = 
0 are linear functions and the only functions satisfying D[Tplane ] = 0 are those derived from conformal 
mappings of the sphere onto the plane. Unfortunately, these two classes of functions don t intersect. 
In this case for a given level Jfor either error functional Kor D we minimize the level of the other. 
The corresponding optimization problems are K[Tplane ]= min,D[Tplane ]= Jor K[Tplane ]= J,D[Tplane ]= 
min These optimization problems are equivalent and can be reduced [Zor95]. to an unconstrained optimization 
problem for the func­tional F[Tplane]= JK[Tplane ]+ (1 -J)D[Tplane ]), where Jrepre­sents the desired 
tradeoff between two functionals: for J=0we completely ignore the zero-curvature condition, for J= 1 
the direct view condition. We also have to specify the boundary conditions in order to make the problem 
well-de.ned. This can be done by .xing the frame of the picture, that is, the values of Tplane on the 
boundary of Wplane . We will consider solutions of this optimization problem in the next section. Error 
functions for transformations with central symmetry From now on we will restrict our attention to transformations 
that also have central symmetry. This assumption allows distribution of the error evenly in all directions 
in the picture. The advantage of this additional restriction is a considerable simpli.cation of the problem. 
The disadvantage is that real pictures seldom have this type of symmetry and, therefore, non-symmetric 
transformation might result in better images. We will discuss a way to create nonsymmetric transformations 
in Section 5.4. In polar coordinates transformation Tplane can be written as p= p(r) 1= <. In this case 
we get the following simpli.ed expressions for the error functions ()2 p 0002 3 -p + pK(p,r)= R20 r2 
r () 2 min p2 ,p02 r2 max(p 02(1 + r2)2 ,p 2(1 + r12 ))D(p,r)= -1 min(p02(1 + r2)2 ,p2(1 + r12 )) We 
note that in both cases the dependence on the angular coordinate completely disappeared, so now the problem 
is one­dimensional. We did not use symmetry in our derivation for the general expression for K; absence 
of dependence on the angle in the formulae above suggests that our bounds are quite tight. In order for 
the problem to have a solution, the boundary condi­tions should have the same type of symmetry. We can 
take V to be the cone with angle at the apex e0. In this case W = I(V) will be a circle of radius R = 
L tan e0. The corresponding boundary condi­tions are p(R)= 1, p(0) = 0 (from continuity). Here we assume 
that the radius of the picture P(V), corresponding to R0 in Section 5.2, is 1. Now there are unique functions 
psatisfying K =0or D =0. For K it is obvious: pK = rIR.For D it is pD(r)= p1D(r)Ip1D(R), p where p1D(r)= 
r2+1 -1Ir The solutions of the optimization problem will form a parametric family p(J,r)and p(0,r)= pD(r), 
p(1,r)= pK(r). We consider solutions for the sup norm, which is more appro­priate from perceptual point 
of view: we are guaranteed that the distortion doesn t exceed a speci.ed amount. Now we can state the 
optimization problem that we have to solve: Minimize the functional F[p]= maxF(p,r) [0 R] subject to 
boundary conditions p(0) = 0, p(R)=1, p 00(0) = 0, where F(p,r)= JK(p,r)+ (1 -J)D(p,r) Solving a minimization 
problem of this type (Chebyshev min­imax functional) is in general quite dif.cult. We found the lower 
estimate for the values of F, and numerically approximated the optimal solution. It turns out that the 
values of Ffor linear inter­polation between solutions for K=0 and D= 0 are close to the optimal values. 
 1 K D 4 0.6 2 0.2  0 0.2 0.6 1 0.2 0.6 1 .. Figure 6. Functionals Kand Dfor pgiven by Equation 
1 as functions of Afor the .eld of view 90°. Note that when A=1,there is no curvature distortion. When 
A=0,then there is no distortion of shape. For a given i, we can .nd Athat will approximately minimize 
the functional F. It appears that for practical purposes linear interpolation can be used. The resulting 
transformations have the following form: pR(r2+1 -1) p(r)= .rIR +(1 -.)p; 1= <(1) r(R2+1 -1) where the 
original image is represented in polar coordinate system (r,<), the transformed image in polar coordinate 
sys­tem (p,1). 5.4Generalizationtonon-symmetriccases We can use Equation 1 to construct more general 
transformations by replacing a constant .with a .depending on the angle. In this case we can choose the 
balance between direct view and zero curvature conditions to be different for different directions. First, 
an initial constant value of .is chosen for the whole image. Then .is speci.ed for a set of important 
directions and then interpolated for the rest of the directions. (Figure 7c). Making .dependent on the 
radius and angle is more dif.cult, but possible; we leave this as future work. 6Choiceofviewingtransformation 
In the previous section we obtained an analytical expression for a family of viewing transformations 
parameterized by L and ..The distance L from the center of projection to the intermediate plane p determines 
Iplane,and .determines the tradeoff between the zero­curvature and direct-view conditions. We need to 
choose both parameters for a particular scene or image. As we have mentioned before, in our approach 
the center of projection need not be the position of a hypothetical camera or observer; we are free to 
choose it using perceptual considerations. However, we are restricted in our choice by the content of 
the picture that we want to obtain. In many cases, the most important constraint is the amount of foreshortening 
that we want to have across the scene. By the amount of foreshortening we mean the desired ratio of sizes 
of identical objects placed in the closest and most remote part of the scene (for example, human .gures 
in the foreground and background of Fig. 9a). This ratio can be small for scenes which contain only objects 
of comparable size placed close to each other, such as the of.ce scene (Fig. 9), and should be large 
for scenes with landscape background (Fig. 10). According to [HEJ78] people typically prefer pictures 
with a small amount of foreshortening in individual objects. The behavior of the error functionals is 
in agreement with this fact: as we move the center of projection away from the intermediate plane (L 
-1), the size of the intermediate image Wplane goes to 0 and it is possible to show that both direct 
view and zero-curvature error functionals decrease. However, a total absence of foreshortening produces 
distortion (Fig. 9b). The best choice of the center of projection typically corresponds to the .eld of 
view in the range 10 to 50 degrees. When such a choice is possible, we can achieve reasonably good results 
simply by choosing a small .eld of view and taking . to be equal to 1 (Fig. 9c). There are some types 
of scenes, however, that don t allow us to choose small .elds of view. If we try to decrease the .eld 
of view in some scenes, either parts of the scene are lost, or the amount foreshortening becomes too 
close to 1 and objects in the foreground become too small. (Fig. 10c,d). In this case, we can choose 
the 2D transformation by varying . to achieve the appropriate balance between two types of distortion 
that we described. We choose a global .for the whole image; if parts of the image still look distorted, 
we can make additional corrections in various parts of the image by varying .as described in Section 
5.4. (Fig 10b, Fig. 7c).  Figure 7. A wide-angle photograph of a room. a. Original image (approximately 
100°angle. b.Transformation 1 applied with A= 0. c. Generalization of transformation 1, (Section 5.4) 
applied, Avaries from 0 to 1.0 across the image. Note the correct shape of the head and straightness 
of the walls. Figure 8. Photo from the article Navigating Close to Shore by Dave Dooling ( IEEE Spectrum 
, Dec. 1994),c @1994 IEEE, photo by Intergraph Corp. 92° viewing angle. a. Original image. b. Transformation 
1 applied with A=0.  Figure 9. Shallow scene: model of an of.ce (frames from the video), standard projections. 
a. 92°viewing angle. b. 3°viewing angle. c. 36°viewing angle, close to perceptually optimal for most 
people.  a b c d c. 60°viewing angle, keeping pyramids in the same position in the picture. d. 60°viewing 
angle, keeping people in the center in the same position. 7ImplementationandApplications Implementation.The 
implementation of our viewing transfor­mations is straightforward. The Iplane projection practically 
coin­cides with the standard perspective/parallel projection. There is, however, an important implementation 
detail that is absent in some systems. As we mentioned before, our center of projection need not coincide 
with the position of the camera or the eye. It is chosen according to perceptual requirements. For instance, 
it can happen that the most appropriate center of projection for an of.ce scene is outside the room. 
In these cases it is necessary to have a mechanism for making parts of the model invisible (these parts 
of the model should participate in lighting calculations but should be ignored by the viewing transformation). 
This can be done using clipping planes. The 2D part of the viewing transformation Tplane can be imple­mented 
as a separate postprocessing stage. The advantage of such an implementation is that it allows us to apply 
it to any perspective image, computer-generated or photographic. The only additional information required 
is the position of the center of projection rela­tive to the image. The basic structure of the implementation 
is very simple: for all output pixels (i,j) do p r := i2+ j2 setpixel( i,j, interpolated color( p -1(r)iIr, 
p -1(r)jIr)) end Theinversefunction p -1 can be computed numerically with any of the standard root-.nding 
methods, such as those found in Numerical Recipes [PFTV88]. The interpolated color( x,y) function computes 
the color for any point (x,y) with real coordinates in the original image by interpolating the colors 
of the integer pixels. The position of the center of projection is usually known for computer-generated 
images, but is more dif.cult to obtain for pho­tos. For photos it can be calculated if we know the size 
of the .lm and focal distance of the lens used in the camera. Alternatively, it can be computed directly 
from the image if there is a rectangular object of known aspect ratio present in the picture [Zor95]. 
Applications.Examples of applications of our viewing transfor­mations were mentioned throughout the paper. 
We can identify the following most important applications: Creation of wide-angle pictures with minimal 
distortions. (Figs. 9,10).  Reduction of distortions in photographic images. (Figs 7,8).  Creation 
of wide-angle animation with reduced distortion of shape.  A better alternative to .sheye views. Fisheye 
views are used for making images with extremely wide angle (up to 180 for hemispherical .sheye), when 
the distortions in linear projec­tion make it impossible to produce any reasonable picture. However, 
.sheye pictures have considerable distortions of their own. The pictures that we obtain using our transforma­tions 
look signi.cantly less distorted than .sheye views.  Zooming of parts of a wide-angle picture: For example, 
we can cut out a portrait of one of the authors from the trans­formed image in Fig. 7b, while it would 
look quite distorted if we had used the original photo (Fig. 7a)  8ConclusionandFutureWork We developed 
an approachfor constructing viewing transformations on perceptual basis. We demonstrate that two important 
perceptu­ally desirable requirements are incompatible and there is no unique viewing transformation producing 
perceptually correct images for any scene. We described a simple family of viewing transformations suitable 
for reducing distortions in wide-angle images. These trans­formations are straightforward to implement 
as a postprocessing stage in a rendering system or for photographs and motion pictures. As we have mentioned 
in Section 5.1, Theorem 1 applies only in cases when we consider all possible lines and planes in the 
scene, not only the ones present in it. Better results can be achieved by introducing direct dependence 
of the viewing transformation on the objects of the scene. Possible extensions of this work include considering 
the depen­dence of .in the Equation 1 on r, and conformal transformations of the plane preserving the 
direct-view property. We also can introduce new perceptually desirable properties and .nding new families 
of transformations that produce optimal images with respect to these properties. 9Acknowledgements We 
wish to thank Bena Currin for her help with writing the software and Allen Concorran for the help with 
preparing the images. Many thanks to Greg Ward for his RADIANCE system that was used to render the image 
in the paper. We also thank the members of Caltech Computer Graphics Group for many useful discussions 
and suggestions. This work was supported in part by grants from Apple, DEC, Hewlett Packard, and IBM. 
Additional support was provided by NSF (ASC-89-20219), as part of the NSF/DARPA STC for Com­puter Graphics 
and Scienti.c Visualization. All opinions, .ndings, conclusions, or recommendations expressed in this 
document are those of the author and do not necessarily re.ect the views of the sponsoring agencies. 
References [dV70] Leonardo da Vinci. Notebooks I, II. Dover, New York, 1970. [Gla94] G. Glaeser. Fast 
Algorithms for 3-D Graphics. Springer-Verlag, New York, 1994. [Hag76] Margaret A. Hagen. In.uence of 
picture surface and station point on the ability to compensate for oblique view in pictorial perception. 
Developmental Psychology, 12(1):57 63, January 1976. [Hag80] Margaret A. Hagen, editor. The Perception 
of pictures. Academic Press series in cognition and perception. Academic Press, New York, 1980. [HEJ78] 
Margaret A. Hagen, Harry B. Elliott, and Rebecca K. Jones. A distinctive characteristic of pictorial 
perception: The zoom effect. Perception, 7(6):625 633, 1978. [Kub86] Michael. Kubovy. The psychology 
of perspective and Renais­ sance art. Cambridge University Press, Cambridge <Cam­ bridgeshire> ; New 
York, 1986. [PFTV88] William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. 
Numerical Recipes in C: The Art of Sci­enti.c Computing. Cambridge University Press, Cambridge, UK, 1988. 
[Pir70] M. H. Pirenne. Optics, painting and photography. Cambridge University Press, New York, 1970. 
[Pon62] L. S. Pontriagin. The mathematical theory of optimal processes. Interscience Publishers, New 
York, 1962. [Rau86] B. V. Raushenbakh. Sistemy perspektivy v izobrazitel nom iskusstve: obshchaiateoriiaperspektivy. 
Nauka, Moskva,1986. in Russian. [TR93] Jack Tumblin and Holly E. Rushmeier. Tone reproduction for realistic 
images. IEEE Computer Graphics and Applications, 13(6):42 48, November 1993. also appeared as Tech. Report 
GIT-GVU-91-13, Graphics, Visualization &#38; Usability Center, Coll. of Computing, Georgia Institute 
of Tech. [Zor95] Denis Zorin. Perceptual distortions in images. Master s thesis, Caltech, 1995.  Appendices 
AProofofthedecompositiontheorem The theorem is proved as a sequence of three lemmas. The details of the 
proofs can be found in [Zor95]. We will use (a,b) to denote the open line segment between a and b and 
[a,b] the closed line segment. Lemma 1 All .bers of the mapping P are line segments. x 3 x x2 1 -1 y 
P (.) 1 x 2 y x 2 1 -1 P ( .1) -1P (.2) Figure A.1. Constructions for lemmas 1 and 2 Proof. Consider 
a .ber P.1(6). Suppose it is not a line segment. Let x1,x2 be two points of the .ber. Then P.1(6) 6c(x1,x2). 
Let S = P.1(6) n(x1,x2), S0= P.1(6) n(x1,x2). It is possible to show that there is a point x 2P.1(6) 
such that in any neighborhood of this point there are points of S and S0.Let B(x) be a ball contained 
in V (the domain of the mapping). We can choose x1, x2, x3 2P.1(6) to be three non-collinear points in 
B(x). Consider the image of the triangle 4x1 x2 x3. As the whole triangle cannot map into 6(this would 
imply that dimP.1(6) >1, contradicting condition 2 from Section 5.1), there is a point y in the triangle 
such that P(y)= 606= 6. Consider the segment [x1,y0] 3y, where y02[x2,x3]. If P(y0)= 6then the image 
of [x1,y0] is not one­to-one but contains at least two points. If P(y0) 6= 6, then the same is true for 
[x2,x3]. Thus the assumption that a .ber can contain non­collinear points contradicts the conditions 
that we have imposed on the mapping. Note that this lemma doesn t rely on the condition 3, Section 5.1. 
Lemma 2 Any two .bers are coplanar. Proof. Suppose two .bers are not coplanar. Choose two points on these 
.bers. Consider a continuous path connecting these two points that is contained in V. Such a path exists 
because V is assumed to be path-connected. As in the previous lemma, we can separate all the points on 
the path into two sets: S, the set of all points x for which the .ber P.1(P(x)) is coplanar with P.1P(x1), 
and S0,the rest of the path. In the same manner we can .nd a point x such that in any neighborhood of 
this point there are points from S and S0.Again, let B(x) be a ball contained in V.Let P.1(61)and P.1(62)be 
two non-coplanar .bers corresponding to points in B(x). Consider two points x1,x2 on P.1(61)and y1,y2 
on P.1(62). The image of [x1,x2] is not one-to-one; it follows from condition 3, Section 5.1, that the 
image of 4x1 x2 y1 is not one-to-one anywhere. Let 60be a point in the image. As the intersection of 
the .ber P.1(60) with the plane of the triangle consists of more than one point, and the .ber is a line 
segment, the whole .ber belongs to the plane of the triangle. But the same thing will be true for 4y1 
y2 x1. Choose 602P([x1,y1]). As two triangles share the edge [x1y1], 602P(4y1 y2 x1)and 0 62P(4x1 x2 
y1), and the .ber P.1(60) should be in the planes of both triangles, i.e. in the intersection of these 
planes (x1,y1)and as it follows from continuity, should contain [x1,y1]. This is impossible, because 
x1 and y1 map to different points. Therefore, there can be no .bers that are not coplanar. Each .ber 
is a line segment, therefore it de.nes a line uniquely. We will call these lines .ber lines. Finally 
we prove the following Lemma 3 If any two .ber lines intersect but do not coincide , all .ber lines intersect 
at the same point. Otherwise, all .ber lines are parallel. Proof. Let f1 and f2 be two intersecting but 
not coinciding .ber lines. Let x 2V be a point outside the plane p de.ned by f1 and f2, and f3 be the 
.ber line of x.As f3 should be coplanar to f1 (Lemma 2) it can be either parallel to it or intersect 
it. But if it is parallel to f1, it is parallel to the plane p, and therefore doesn t intersect f2. Butit 
isn t parallelto f2 either; therefore, f3 and f2 are not coplanar. Thus, f3 should intersect f1 and f2. 
But it can intersect the plane p only in one point, so it should be the common point of f1 and f2. Consider 
a point x0in the plane p and the .ber line f4 corre­sponding to it. The same reasoning applies to f4 
and the pair f1, f3. Suppose f1 and f2 do not intersect. Suppose some .ber f3 inter­sects f1. Ifitisnotin 
p, then it cannot be parallel to f2 and it doesn t intersect it, which is impossible. If it is in p, 
then a .ber line f4 which is not in the plane should intersect it in two points: f1 nf3 and f2 nf3, which 
is impossible. BDerivationoferrorfunctions First, we derive the zero-curvature error function. By de.nition, 
= daIds,where dais the change in the direction of the tangent vector of the curve along the segment ds.Let 
w be the tangent of the curve ,= ,(t): w = d,Idt.Then dw dw dw w jwj dtdtdt j j=< = jwj3 jwj3 jwj2 
In our case the curve is de.ned by the equation ,(t)= g(x + vt). To estimate the curvature from above,we 
will estimate the maximum of ddtw Ijvj2 and minimum of jwj2Ijvj2 for a .xed point x. Using dtd F(x+vt)=(rF,v) 
for a scalar function F, we can write: 2 dw(t) 2 dw' dv, 2 = + =(rw',v)2 +(rw,,v)2 < dtdtdt 16 222 (jrw'+ 
jrw,j2)jvj2=(jrj+ jrj2)jvj= dt dt 2 22 (jr(r1,v)j+ jr(r6,v)j2)jvj2=(j(v,r)r1j+ j(v,r)r6j2)jvj< 24 (j(ex,r)r1j+ 
j(ey,r)r1j2+(ex,r)r6j2+(ey,r)r6j2)jvj= (n 2 22 22 j1xxj+ j1yyj2+2 j1xyj+ j6xxj+ j6yyj2+2 j6xyjjvj Now 
let s estimate jwj2 from below. jwj2=(r1,v)2 +(r6,v)2 = 222 222 ((1x + 6x )vx +2(1x1y + 6x6y)vxvy +(1y 
+ 6y )vy)= Av2 x +2Bvyvx + Cv2 y : p 1 ((A + C) -(A -jvj2 C)2 +4B2) 2 Finally, we get the estimate for 
the curvature: p 2 222 j1xxj+ j1yyj2+2 j1xyj+ j6xxj+ j6yyj2+2 j6xyj j j<p 1((A + C) -(A -C)2 +4B2) 2 
The direct view error function is derived in the following way. Consider Jf x =1u 1v 6u 6v 11 1. = 1u 
1v = 1. 6. = 6u,6v = 6. sin . sin . Then jJwj2= Ew2 u +2Fwv + Gw2 v (E, F and G were de.ned previously). 
jJwj1 jwj22 p jJwj21 max =((E + G)+ (E -G)2 +4F2) jwj22 CAbetterinterpolation After the .nal version 
of the paper was submitted, we have realized that another interpolation between the mapping P(r)= rjR 
and PD(r) (Section 5.3) is more convenient for implementation. PD(r) can be written as tan arctan r PD(r)= 
2 tan arctan R 2 If we replace 2 by 1 in this formula, we get P(r)= rjR.There­fore, the following family 
of functions is an interpolation between r and PD(r): tan arctan r P(),r)= 2.. tan arctan R 2.. The most 
important advantage of this interpolation is that the inverse of all functions in this family can be 
computed explicitly, even if )depends on the angle as in Section 5.4. It also has slightly lower values 
of Kfor given D, but the difference from the values for the linear interpolation (Equation 1) is insigni.cant. 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218450</article_id>
		<sort_key>265</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[AutoKey]]></title>
		<subtitle><![CDATA[human assisted key extraction]]></subtitle>
		<page_from>265</page_from>
		<page_to>272</page_to>
		<doi_number>10.1145/218380.218450</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218450</url>
		<keywords>
			<kw><![CDATA[alpha value]]></kw>
			<kw><![CDATA[block matching]]></kw>
			<kw><![CDATA[edge detection]]></kw>
			<kw><![CDATA[image composition]]></kw>
			<kw><![CDATA[key]]></kw>
			<kw><![CDATA[object extraction]]></kw>
			<kw><![CDATA[spline]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Edge and feature detection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.2</cat_node>
				<descriptor>Feature evaluation and selection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010321.10010336</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning algorithms->Feature selection</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P284098</person_id>
				<author_profile_id><![CDATA[81100380187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitsunaga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation. 6-7-35 Kitashinagawa, Shinagawa, Tokyo 141, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P278082</person_id>
				<author_profile_id><![CDATA[81332537220]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Taku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yokoyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation, 6-7-35 Kitashinagawa, Shinagawa, Tokyo 141, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31099455</person_id>
				<author_profile_id><![CDATA[81332532185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Totsuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Corporation, 6-7-35 Kitashinagawa, Shinagawa, Tokyo 141, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anderson, T. W., Introduction to Multivariate Statistical Analysis, John Wiley &amp; Sons, 1958.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Attneave, F., "Some Informational Aspects of Visual Perception", Psychological Review, Vol.61, pp.183-193, 1954.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dadourian, A., "Compositor for Video Images", patent in U.S.A., US5343252-A, 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Etoh, M. and Shirai, Y., "Automatic Extraction of Complex Objects Using Region Segmentation", NICOGRAPH'92, pp.8-17, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Inui, T., "A Model of Human Visual Memory", 6th Scandinavian Conference on Image Analysis, pp.325-332, 1989.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Inoue, S., "An Object Extraction Method for Image Synthesis", IEICE Trans., Vol.J74-D-II, No.10, pp.1411-1418, 1991.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Inoue, S. and Koyama, H., "An Extraction and Composing Method for Moving Image Synthesis", Journal of lTEJ, Vol.47, No.7, pp999.-1005, 1993.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>59921</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Jain, A. K., Fundamentals of Digital Image Processing, Prentice-Hall, 1989.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Kass, M., Witikin, A. and Terzopoulos, D., "Snakes : Active Contour Models",International Journal of Computer Vision, Vol.1, No.3, pp.321-331, 1988.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Klinker, G. J., Shafer, S. A. and Kanade, T., "The Measurement of Highlight in Color Images", International Journal of Computer Vision, Vol.2, pp.7-32, 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Mishima, Y., "A Software Chromakeyer Using Polyhedric Slice", NICOGRAPH'92, pp.44-52, 1992.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808606</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Porter, T. and Duff, T., "Compositing Digital Images", Computer Graphics, Vo1.18, No.3, pp.253-259, 1984.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Robinson, G. S., "Color Edge Detection", Optical Engineering, Vo1.16, No.5, pp.479-484, 1977.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sezan, M. I. and Lagendijk, R. L., Motion Analysis and Image Sequence Processing, Kluwer Academic Publishers, 1993.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Trahanias, E E. and Venetsanopoulos, A. N., "Color Edge Detection Using Vector Order S tatistics",IEEE Trans. Image Processing, Vol.2, No.2, pp.259-264, 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ueda, N., Mase, K. and Suenaga, Y., "A Contour Tracking Method Using Elastic Contour Model and Energy Minimization Approach", IEICE Trans., Vol.J75-D-II, No.l, pp.111- 120, 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Vlahos, E, "Combining Method for Colour Video Signals", patent in U.S.A., US4625231-A, 1986.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Wong, R. Y. and Hall, E. L., "Sequential Hierarchical Scene Matching", IEEE Trans. Comput., Vol.C-27, No.4, pp.359- 366, 1978.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>7059</ref_obj_id>
				<ref_obj_pid>7053</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Zenzo, S. D., "A Note on the Gradient of a Multi-Image", Computer Vision, Graphics, and Image Processing, Vol.33, pp.116-125, 1986.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[MPEG Video Committee, "Information Technology- Generic Coding of Moving Pictures and Associated Audio", ISO/IEC IS 13818-2, Nov. 1994.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218452</article_id>
		<sort_key>273</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[Stochastic screening dithering with adaptive clustering]]></title>
		<page_from>273</page_from>
		<page_to>276</page_to>
		<doi_number>10.1145/218380.218452</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218452</url>
		<keywords>
			<kw><![CDATA[adaptive clustering]]></kw>
			<kw><![CDATA[digital halftoning]]></kw>
			<kw><![CDATA[dithering algorithms]]></kw>
			<kw><![CDATA[space filling curves]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>B.4.2</cat_node>
				<descriptor>Image display</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP15025597</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA-Instituto de Matem&#225;tica Pura e Aplicada, Estrada Dona Castorina, 110 22460-320 Rio de Janeiro, RJ, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31077714</person_id>
				<author_profile_id><![CDATA[81452615467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gomes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA-Instituto de Matem&#225;tica Pura e Aplicada, Estrada Dona Castorina, 110 22460-320 Rio de Janeiro, RJ, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>578095</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Rosenfeld and A. C. Kak. Digital Picture Processing. Academic Press, 1976.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>27674</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Ulichney. Digital Halftoning. MIT Press, Cambridge, Ma, 1987.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122727</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[L. Velho and J. de M. Gomes. Digital halftoning with space filling curves. Computer Graphics (Proceedings SIGGRAPH '91), 25(4):81-90, 1991.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[L. Velho and J. de M. Gomes. Space filling curve dithering with adaptive clustering. In Proceedings of SIBGRAPI'92, V Brazilian Simposium of Computer Graphics and Image Processing, pages 1-9. SBC- Sociedade Brasileira de Computagfio, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[L. Velho and J. de M. Gomes. Color halftoning with stochastic screening and adaptive clustering. Preprint, 1995. IMPA- Instituto de Matem~itica Pura e Aplicada.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stochastic Screening Dithering with Adaptive Clustering Luiz Velho Jonas Gomes IMPA Instituto de Matem´ 
atica Pura e Aplicada 1 Abstract We develop a clustered dithering method that uses stochastic screen­ing 
and is able to perform an adaptive variation of the cluster size. This makes it possible to achieve optimal 
rendition of gray shades while preserving image details. The algorithm is an improvement to the dithering 
with space .lling curves method, published in [3]. CR Descriptors: B.4.2 [Input/Output and Data Communica­tions]: 
Input-Output Devices -Image Display; I.3.3 [Computer Graphics]: Picture/image generation -Display algorithms; 
I.3.6 [Computer Graphics]: Methodology and Techniques; I.4.3 [Im­age Processing]: Enhancement. Additional 
Keywords: Digital Halftoning, Dithering Algorithms, Space Filling Curves, Adaptive Clustering. 1 INTRODUCTION 
The reproduction of gray scale images in bilevel graphic display devices is achieved through a process 
called halftoning.Given a gray scale image it consists in generating a binary image which perceptually, 
approximates the original image. This process can be analog or digital. Digital halftoning is done using 
a technique called dithering to determine the binary state (black or white) of the elements of the output 
image. Dithering algorithms distribute the black and white pixels in such a way that the input and output 
images are perceptually as close as possible, within the physical limitations of the display device. 
Dithering methods can be subdivided into two main groups according to the type of images they produce 
[2]. Dispersed dot dithering methods generate images in which black and white pixels are evenly distributed 
throughout the image area. Clustered dot dithering methods generate images in which black and white pixels 
are concentrated together forming clusters. These two approaches serve different purposes and are suitable 
for different classes of display devices. Classical clustering dithering algorithms use regular, .xed 
size cluster, and the black and white dot patterns inside the cluster vary 1IMPA Instituto de Matem´atica 
Pura e Aplicada, Estrada Dona Castorina, 110 22460-320Rio de Janeiro, RJ, Brazil, lvelho | jonas@visgraf.impa.br 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
according to the image intensity values. These clusters are dis­tributed periodically over the image 
domain. For this reason, these algorithms are known by the name of amplitude modulated (AM) dithering. 
Stochastic dithering algorithms use a .xed size dot pattern and vary their spacing according to the image 
intensity values. For this reason, these algorithms are called frequency modulated (FM) dithering. Frequencymodulatedditheringtechniqueshavebeenim­plemented 
recently on the raster image processorsof high resolution phototypesetters. The space .lling curve dithering 
algorithm introduced in [3], contains characteristics of both AM and FM dithering: it is a clus­tered 
dithering technique, but the clusters are distributed stochasti­cally over the image domain. Therefore 
the algorithm adapts well to a wide range of display devices. Figure 1 describes the space of possibilities 
covered by the ex­isting digital halftoning techniques: from periodic clustered dither, to periodic dispersed 
dot dither, and from this to aperiodic dispersed dot dithering techniques. Figure 1: Range of existing 
dithering techniques. In this paper we present an improvement to the cluster con­struction of the algorithm 
in [3] in order to get a better rendition of image details, without loosing tonal resolution. The major 
result though, is an extension of the algorithm to obtain a dispersed clus­tered dithering technique 
whose cluster size may vary adaptively according to image intensity characteristics. We should point 
out that this extension was suggested in [3]. An earlier report of the method presented in this paper 
contain­ing some preliminary results and simple experiments appeared in [4]. Here, we formulate the problem 
from the point of view of AM/FM dithering techniques and investigate the application of the algorithm 
to image reproduction on high resolution bilevel devices. The remaining of the paper is organized as 
follows: Section 2 reviews the space .lling curve dither algorithm. Section 3 describes the dither with 
adaptive clustering. Section 3.1 discusses implemen­tation related issues. Section 4 shows results of 
experiments with the method. 2 SPACE FILLING CURVE DITHER In this section we brie.y review the dithering 
with space .lling curves (SFC) introduced in [3]. The trace of a space .lling curve approximation is 
used to scan the image, generating a parametriza­tion of the image elements with many desirable properties. 
The method consists of the following steps: Subdivision of the image into cells;  Computation of the 
average intensities of each cell;  Generation of corresponding black and white dot patterns for each 
cell;  positioning of the dot pattern within the cell to generate the cluster.  The subdivision of 
the image is performed by following the path of the space .lling curve until the number of elements visited 
is equal to the cluster size. This is illustrated in Figure 2(a), using a Hilbert space .lling curve. 
For each cell, the computation of the accumulated intensity is performed as each one of its elements 
is visited. Then, the corre­sponding dot pattern is generated by selecting a group of contiguous elements 
proportional in number to the total intensity. In this way, the cell is subdivided into subcells of black 
and white pixels such that its average intensity approximates the image average intensity within the 
cell. Figure 2(b), from [3], shows the con.guration of dots corresponding to intensity levels 15/16 to 
0 for a cluster of 4x4pixels. Figure 2: 4x4cluster and dot patterns from (Velho and Gomes, 1991). There 
is a quantization error associated with each cell de.ned by the difference between the average intensity 
of the grayscale image and the intensity of the dot patterns. The quantization error in a region is diffused 
by the algorithm, propagating it to neighbor regions along the path of the space .lling curve. 2.1 Cluster 
Generation The last step of the algorithm creates the cluster by positioning the dot pattern within the 
cell. Here we introduce an improvement over the algorithm published in [3]. We position the central pixel 
of the generated dot pattern at the pixel inside the cell which has the lowest intensity level (i.e. 
corresponding to the highest percentage of black ink). This is illustrated in Figure 3: (a) shows the 
image and a 4x4cell; (b) shows (in gray) the lowest intensity pixel within the cell; (c) shows a dot 
pattern of 5pixels; (d) shows the translation of the dot pattern center to the pixel of lowest intensity. 
This positioning method results in a much better rendition of the image details, without compromising 
tonal resolution. Figure 3: Dot pattern position. We should observe that besides the non-directionality 
implied by the space .lling curve traversal of the image pixels, the cluster generation method described 
introduces some randomness to the distribution of the dot patterns within the cell. In brief, the dithering 
algorithm with space .lling curves uses clustering similar to the traditional amplitude modulated (AM) 
algo­rithms, but at the same time it performs error diffusion. Therefore, it also incorporates characteristics 
of techniques that use frequency modulation (FM dithering).  3 SFC DITHER with ADAPTIVE CLUSTERING From 
the results of the previous section we know that the dithering algorithm with space .lling curves distributes 
stochastically the clusters and performs an error diffusion between neighbor cells. In this section we 
will show how to extend the method in order to have an adaptive control over the cluster size. This control 
will enable us to incorporate a variable size clustering, which along with the above mentioned properties, 
creates a dithering texture similar to the .lm grain found in photographic textures. The space .lling 
curve dithering algorithm subdivides the im­age domain into cells, and at each cell it approximates the 
image function f(x,y)by some bi-level image function f(x,y).The ap­proximation criteria is a perceptual 
one, based on pixel intensities. The adaptive clustering dithering consists of changing the size of each 
cell, based on some adaptive criteria, in order to get a better binary approximation fof the image function 
f. The adaptiveness criteria to compute the cluster size depends on the desired effect to be obtained 
by the halftone method. In our case, the goal is to achieve the best rendition of image detail without 
compromising tonal reproduction. Therefore, we should use an adaptive criteria that varies the cluster 
size according to the rate of change of the image intensity. In order to accomplish for this, we need 
to measure the variation of image intensities as we scan the image. Since we are scanning the image along 
the path of the space .lling curve, the directional derivative along the curve provides a good measure 
ot the rate of change of the image intensities along the scanning direction. It is computed by modulating 
the gradient vector gradf(f,f), xy of the image function f, by the unit vector ualong the scanning direction 
de.ned by the space .lling curve. That is, f hu,gradfi, u where h,iis the usual euclidean inner product. 
After deciding that the directional derivative will take care of the adaptiveness criteria, it remains 
to obtain the correct relation­ship between the cluster size and the derivative intensity. As the derivative 
magnitude gets bigger, image intensities change faster and, therefore, the cluster size should get smaller. 
We need to .nd the correct relationship between the cluster size and the derivative values. For this, 
we .rst observe that the in­tensities distribution in a dithered image must follow a perceptual criteria. 
Also, the eye response to intensity changes obeys a log­arithmic law (see [1]). Based on these two remarks, 
we conclude that we should vary the cluster size exponentially with the gradient magnitude. This rule 
maintains a linear relationship between the perceptual intensity inside each cluster and the directional 
slope of the image intensity. 3.1 Implementation Issues It is possible to implement the space .lling 
curve dithering with adaptive cluster in two stages: 1. Estimate the cluster size based on image characteristics; 
 2. Change the cluster size according to some function of (1).  The separation of these two mechanisms 
makes the algorithm more .exible and allows for experimentation with different sets of criteria in the 
speci.cation of the cluster size. The latter procedure gives input to the former establishing a clean 
interface between them. Although in this work we have experimented mainly with an adaptive criterium 
based on the variation of the directional deriva­tive of the image intensity function, as explained in 
the previous section; there are other types of criteria that we believe are worth exploring. One example 
is the cluster adaptation based on the physical reproduction function of the imaging system in order 
to compensate for its de.ciencies. Other example is an adaptation cri­terium based on a function of the 
image domain in order to create graphical effects. This opens up many other possibilities of use for 
the method. In our implementation the cluster size control is done in the .rst pass through the cluster 
cells. At each point of the image, the maxi­mum allowable cluster size is determined by the adaptation 
criteria. While image elements in the region are processed (to accumulate intensity), the current cluster 
size is compared with the maximum allowable cluster size at that point. If this maximum allowable size 
is smaller than the current size, it becomes the current cluster size. The algorithm terminates the .rst 
pass when the number of elements in the region exceeds the cluster size. Then, it moves on to generate 
the dot pattern con.guration, as explained in the previous section. To make the algorithm more versatile, 
the adaptation criteria can be passed as a parameter to the program. The default criterium is the rule 
described above using an exponential of base 2. The other options provided include: linear variation 
with the value of the directional derivative of the image function, or using a table supplied by the 
user.  4 RESULTS In this section we show the results of applying the algorithm to various types of images. 
These test images re.ect the main charac­teristics of images encountered in digital printing situations. 
The .gures below compare the output of the space .lling curve dithering algorithm with and without adaptive 
clustering. All input images are gray-scale with 8 bits of intensity resolution and spatial resolution 
varying from 80 to 400 ppi (pixels per inch). The output images are halftoned and printed at 1200 dpi 
(dots per inch). The images in Figures 4 and 5 are scaled using pixel replication in order to show more 
clearly the halftone screen dots. Figure 4 is a test pattern of intensity gradations using linear ramps. 
The gradient increases in steps such that the rate of change in intensity is almost constant for each 
rectangle. As the slope of the ramp doubles, the cluster size also decreases by a factor of two. The 
maximum cluster size is 31 pixels. (a) (b) Figure 4: Linear intensity gradations (a) constant cluster 
size of 31 pixels (b) variable cluster size. Figure 5 is a cartoon image consisting of line drawings 
and areas of constant gray level. The shadow on the wall in the background is rendered as regular pattern 
of dots simulating a standard halftone screen. The effect of the adaptive method is striking. The improve­ment 
obtained is mainly due to the high frequencies of the image. It needs dithering only in the areas of 
intermediate intensity. In this case, the algorithm was capable of matching exactly the edges of the 
drawings and at the same time reproducing with uniform dot patterns the different gray shades. We should 
remark that printing this image with the traditional AM clustering algorithm would cer­tainly result 
in moir´ee patterns on the gray regions of the original image, because of the dot patterns used to produce 
the gray shade. Figure 6 shows a comparison of the adaptive dithering (right) with the space .lling curve 
dithering algorithm using .xed cluster size (left), as we increase the cluster size. The .xed cluster 
sizes in this .gure are, from top to bottom, 30, 15and 3pixels. Also, these are the maximum cluster sizes 
used to process the corresponding image with the adaptive algorithm. Finally, we show the application 
of the algorithm for high res­olution printing. In order to do this, we made several tests to .nd the 
optimal cluster size for the phototypesetter used to print this paper. The image in Figure 7(b) is printed 
at 1200 dpi with a max­imum cluster size of 7pixels, as determined by the experiments. The image in Figure 
7(a) is printed at the same resolution with a dispersed-dot dither (cluster size of 1 pixel). This comparison 
demonstrates the effectiveness of the method to match the charac­teristics of the output device.  5 
CONCLUSIONS An adaptive digital halftoning method with variable size clustering was presented. It extends 
the space .lling curve dithering algorithm Figure 5: Line drawing cartoon (a) constant cluster size of 
27 pixels  (b) variable cluster size to change the size of pixel clusters according to local characteristics 
of the image. This makes possible to achieve an optimal rendition of gray shades while preserving image 
detail. The dithering techniques described in this paper can also be applied with great effectiveness 
to color images. We developed a color halftoning algorithm that employs adaptive dot clustering with 
error diffusion on multiple image channels [5]. These features make the algorithm particularly suited 
for color reproduction using any number of process colors.  ACKNOWLEDGEMENTS This work was done during 
the period the authors visited the Hewlett-Packard Labs in Palo Alto. We wish to thank Ricardo Motta 
who provided us with the working environment necessary for doing this research. The visit to HP was sponsored 
by the RHAE project from MCT/CNPq in Brazil. REFERENCES [1] A. Rosenfeld and A. C. Kak. Digital Picture 
Processing. Aca­demic Press, 1976. [2] R. Ulichney. Digital Halftoning. MIT Press, Cambridge, Ma, 1987. 
[3] L. Velho and J. de M. Gomes. Digital halftoning with space .lling curves. Computer Graphics (Proceedings 
SIGGRAPH 91), 25(4):81 90, 1991. [4] L. Velho and J. de M. Gomes. Space .lling curve dithering with adaptive 
clustering. In Proceedings of SIBGRAPI 92, V Brazil­ian Simposium of Computer Graphics and Image Processing, 
pages 1 9. SBC Sociedade Brasileira de Computac¸ ao, 1992. [5] L. Velho and J. de M. Gomes. Color halftoning 
with stochastic screening and adaptive clustering. Preprint, 1995. IMPA Instituto de Matem´atica Pura 
e Aplicada. Figure 6: Comparison of adaptive (right) and non-adaptive (left) clustering. From top to 
bottom, cluster sizes of 30, 15, and 3 pixels.  (b) Figure 7: Images printed at a resolution of 1200dpi: 
(a) dispersed­dot dither, (b) adaptive dither with maximum cluster size of7pixels.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218454</article_id>
		<sort_key>277</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[Fast multiresolution image querying]]></title>
		<page_from>277</page_from>
		<page_to>286</page_to>
		<doi_number>10.1145/218380.218454</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218454</url>
		<keywords>
			<kw><![CDATA[content-based retrieval]]></kw>
			<kw><![CDATA[image databases]]></kw>
			<kw><![CDATA[image indexing]]></kw>
			<kw><![CDATA[image metrics]]></kw>
			<kw><![CDATA[query by content]]></kw>
			<kw><![CDATA[query by example]]></kw>
			<kw><![CDATA[similarity retrieval]]></kw>
			<kw><![CDATA[sketch retrieval]]></kw>
			<kw><![CDATA[wavelets]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.0</cat_node>
				<descriptor>Image processing software</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.3.1</cat_node>
				<descriptor>Indexing methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor>Retrieval models</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor>Search process</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.2.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003317.10003325</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Information retrieval query processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003365.10003366</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Search engine architectures and scalability->Search engine indexing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003338</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Retrieval models and ranking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003318</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Document representation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010389</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Graphics processors</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P43568</person_id>
				<author_profile_id><![CDATA[81100068731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Jacobs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P10635</person_id>
				<author_profile_id><![CDATA[81100576882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Finkelstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P63622</person_id>
				<author_profile_id><![CDATA[81100188207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Salesin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science and Engineering, University of Washington, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Barber, W. Equitz, W. Niblack, D. Petkovic, and R Yanker. Efficient query by image content for very large image databases. In Digest of Papers. COMPCON Spring '93, pages 17-19, San Francisco, CA, USA, 1993.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[G. Beylkin, R. Coifman, and V. Rokhlin. Fast wavelet transforms and numerical algorithms I. Communications on Pure and Applied Mathematics, 44:141-183, 1991.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[R. DeVore, B. Jawerth, and B. Lucier. Image compression through wavelet transform coding. IEEE Transactions on Information Theory, 38(2):719-746, March 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>188728</ref_obj_id>
				<ref_obj_pid>188724</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Faloutsos, R. Barber, M. Flickner, J. Hatner, W. Niblack, D. Petkovic, and W. Equitz. Efficient and effective querying by image content. Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, 3(3-4):231-262, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics: Principles and Practice. Prentice-Hall, 1990.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>756128</ref_obj_id>
				<ref_obj_pid>648307</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[T. Gevers and A. W. M. Smuelders. An approach to image retrieval for image databases. In V. Marik, J. Lazansky, and R. R. Wagner, editors, Database and Expert Systems Applicatons (DEXA ' 93), pages 615-626, Prague, Czechoslovakia, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Yihong Gong, Hongjiang Zhang, H. C. Chuan, and M. Sakauchi. An image database system with content capturing and fast image indexing abilities. In P1vceedings of the International Conference on Multimedia Computing and Systems, pages 121-130. IEEE, 1994.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>654633</ref_obj_id>
				<ref_obj_pid>645477</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[William I. Grosky, Rajiv Mehrotra, F. Golshani, H. V. Jagadish, Ramesh Jain, and Wayne Niblack. Research directions in image database management. In Eighth International Conference on Data Engineering, pages 146-148. IEEE, 1992.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6251</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Donald Hearn and M. Pauline Baker. Computer Graphics. Addison-Wesley Publishing Company, Inc., 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>649863</ref_obj_id>
				<ref_obj_pid>645336</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[K. Hirata and T. Kato. Query by visual example -- content based image retrieval. In A. Pirotte, C. Delobel, and G. Gottlob, editors, Advances in Database Technology (EDBT '92), pages 56-71, Vienna, Austria, 1992.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[N. Jayant, J. Johnston, and R. Safranek. Perceptual coding of images. In P1vceedings of the SHE -- The International Society for Optical Engineering, volume 1913, pages 168-178, 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Atreyi Kankanhalli, Hong Jiang Zhang, and Chien Yong Low. Using texture for image retrieval. In International Conference on Automation, Robotics and Computer Vision. IEE, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[T. Kato. Database architecture for content-based image retrieval. In P~vceedings of the SPIE -- The International Society for Optical Engineering, volume 1662, pages 112-123, San Jose, CA, USA, 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[T. Kato, T. Kurita, N. Otsu, and K. Hirata. A sketch retrieval method for lull color image database -- query by visual example. In Proceedings of the llth IAPR International Conference on Pattern Recognition, pages 530-533, Los Alamitos, CA, USA, 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>695298</ref_obj_id>
				<ref_obj_pid>646494</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Patrick M. Kelly and T. Michael Cannon. CANDID: Comparison Algorithm for Navigating Digital Image Databases. In Proceedings of the Seventh International Working Conference on Scientific and Statistical Database Management Storage and Retrieval for Image and Video Databases. IEEE, 1994.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[A. Kitamoto, C. Zhou, and M. Takagi. Similarity retrieval of NOAA satellite imagery by graph matching. In Storage and Retrieval for Image and Video Databases, pages 60-73, San Jose, CA, USA, 1993.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>702827</ref_obj_id>
				<ref_obj_pid>646709</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[J. Liang and C. C. Chang. Similarity retrieval on pictorial databases based upon module operation. In S. Moon and H. Ikeda, editors, Database Systems for AdvancedApplications, pages 19-26, Taejon, South Korea, 1993.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[G.S. Maddala. Intlvduction to Econometrics. Macmillan Publishing Company, second edition, 1992.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Stephane Mallat and Sifen Zhong. Wavelet transform maxima and multiscale edges. In Ruskai, et al, editor, Wavelets and Their Applications, pages 67-104. Jones and Bartlett Publishers, Inc., Boston, 1992.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[W. Niblack, R. Barber, W. Equitz, M. Flickner, E. Glasman, D. Petkovic, R Yanker, C. Faloutsos, and G. Taubin. The QBIC project: Querying images by content using color, texture, and shape. In Storage and Retrieval for Image and Video Databases, pages 173-187. SPIE, 1993.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[G. Petraglia, M. Sebillo, M. Tucci, and G. Tortora. Rotation invariant iconic indexing for image database retrieval. In S. Impedovo, editor, P~vceedings of the 7th International Conference on Image Analysis and P1vcessing, pages 271-278, Monopoli, Italy, 1993.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Brian Pinkerton. Finding what people want: Experiences with the WebCrawler. In The Second International WWW Conference '94: Mosaic and the Web, October 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[William H. Press, Brian R Flannery, Saul A. Teukolsky, and William T. Fetterling. Numerical Recipes. Cambridge University Press, second edition, 1992.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T.R. Reed, V. R. Algazi, G. E. Forrd, and I. Hussain. Perceptually-based coding of monochrome and color still images. In DCC '92- Data Compression Conference, pages 142-51, 1992.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>521260</ref_obj_id>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[SAS Institute Inc. SAS/STAT User's Guide, Version 6, Fourth Edition, Volume 2. SAS Institute Inc., 1989.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R. Shann, D. Davis, J. Oakley, and F. White. Detection and characterisation of Carboniferous Foraminifera for content-based retrieval from an image database. In Storage and Retrieval for Image and Video Databases, volume 1908, pages 188-197. SPIE, 1993.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M. Shibata and S. Inoue. Associative retrieval method for image database. Transactions of the Institute of Electlvnics, Information and Communication Engineers D-H, J73D-II:526-34, 1990.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[E. R Simoncelli, W. T. Freeman, E. H. Adelson, and D. J. Heeger. Shiftable multiscale transforms. IEEE Transactions on Information Theory, 38:587-607, 1992.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>184519</ref_obj_id>
				<ref_obj_pid>184514</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Stephen W. Smoliar and Hong Jiang Zhang. Content-based video indexing and retrieval. IEEE Multimedia, 1(2):62-72, 1994.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[R L. Stanchev, A. W. M. Smeulders, and F. C. A. Groen. An approach to image indexing of documents. IFIP Transactions A (Computer Science and Technology), A-7:63-77, 1992.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618273</ref_obj_id>
				<ref_obj_pid>616036</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Eric J. Stollnitz, Tony D. DeRose, and David H. Salesin. Wavelets for computer graphics: A primer, Part I. IEEE Computer Graphics and Applications, 15(3):76- 84, May 1995.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618291</ref_obj_id>
				<ref_obj_pid>616037</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Eric J. Stollnitz, Tony D. DeRose, and David H. Salesin. Wavelets for computer graphics: A primer, Part II. IEEE Computer Graphics and Applications, 15(4), July 1995. In press.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Michael J. Swain. Interactive indexing into image databases. In Storage and Retrieval for Image and Video Databases, volume 1908, pages 95-103. SPIE, 1993.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Patrick C. Teo and David J. Heeger. Perceptual image distortion. In Human Vision, Visual P1vcessing and Digital Display V, IS&amp;T/SPIE's Symposium on Electronic Imaging: Science &amp; Technology, 1994. In press.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>189702</ref_obj_id>
				<ref_obj_pid>189693</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Chen Wu Tzong and Chin Chen Chang. Application of geometric hashing to iconic database retrieval. Pattern Recognition Letters, 15(9):871-876, 1994.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>159617</ref_obj_id>
				<ref_obj_pid>159544</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[M. Weiser. Some computer science issues in ubiquitous computing. Communications of the ACM, 36(7):74-84, 1993.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Multiresolution Image Querying Charles E. Jacobs Adam Finkelstein David H. Salesin Department of 
Computer Science and Engineering University of Washington Seattle, Washington 98195 Abstract We present 
a method for searching in an image database using a query image that is similar to the intended target. 
The query im­age may be a hand-drawn sketch or a (potentially low-quality) scan of the image to be retrieved. 
Our searching algorithm makes use of multiresolution wavelet decompositions of the query and database 
images. The coef.cients of these decompositions are distilled into small signatures for each image. We 
introduce an image query­ing metric that operates on these signatures. This metric essentially compares 
how many signi.cant wavelet coef.cients the query has in common with potential targets. The metric includes 
parameters that can be tuned, using a statistical analysis, to accommodate the kinds of image distortions 
found in different types of image queries. The resulting algorithm is simple, requires very little storage 
overhead for the database of signatures, and is fast enough to be performed on a database of 20,000 images 
at interactive rates (on standard desktop machines) as a query is sketched. Our experiments with hundreds 
of queries in databases of 1000 and 20,000 images show dramatic improvement, in both speed and success 
rate, over using a conven­tional L1, L2, or color histogram norm. CR Categories and Subject Descriptors: 
I.4.0 [Image Process­ing]: General Image processing software; I.3.6 [Computer Graphics]: Methodology 
and Techniques Interaction Techniques. Additional Key Words: content-based retrieval, image databases, 
image indexing, image metrics, query by content, query by example, similarity retrieval, sketch retrieval, 
wavelets. 1 Introduction With the explosion of desktop publishing, the ubiquity of color scan­ners and 
digital media, and the advent of the World Wide Web, peo­ple now have easy access to tens of thousands 
of digital images. This trend is likely to continue, providing more and more people with ac­cess to increasingly 
large image databases. As the size of these databases grows, traditional methods of interac­tion break 
down. For example, while it is relatively easy for a per­son to quickly look over a few hundred thumbnail 
images to .nd a speci.c image query, it is much harder to locate that query among several thousand. Exhaustive 
search quickly breaks down as an ef­fective strategy when the database becomes suf.ciently large. One 
commonly-employed searching strategy is to index the image database with keywords. However, such an approach 
is also fraught Permission to make digital/hard copy of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for profit or commercial 
advantage, the copyright notice, the title of the publication and its date appear, and notice is given 
that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to 
redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 with dif.culties. First, it requires a person to manually tag all the images with keys, a time-consuming 
task. Second, as Niblack et al. point out [20], this keyword approach has the problem that some visual 
aspects are inherently dif.cult to describe, while others are equally well described in many different 
ways. In addition, it may be dif.cult for the user to guess which visual aspects have been in­dexed. 
In this paper, we explore an alternative strategy for searching an image database, in which the query 
is expressed either as a low­resolution image from a scanner or video camera, or as a rough sketch of 
the image painted by the user. This basic approach to im­age querying has been referred to in a variety 
of ways, including query by content [1, 4, 20], query by example [10, 13, 14], similarity retrieval [6, 
16, 17, 21, 35] and sketch retrieval [14]. Note that this type of content-based querying can also be 
applied in conjunction with keyword-based querying or any other existing ap­proach. Several factors make 
this problem dif.cult to solve. The query image is typically very different from the target image, so 
the retrieval method must allow for some distortions. If the query is scanned, it may suffer artifacts 
such as color shift, poor resolu­tion, dithering effects, and misregistration. If the query is painted, 
it is limited by perceptual error in both shape and color, as well as by the artistic prowess and patience 
of the user. For these reasons, straightforward approaches such as L1 or L2 image metrics are not very 
effective in discriminating the target image from the rest of the database. In order to match such imperfect 
queries more effec­tively, a kind of image querying metric must be developed that accommodates these 
distortions and yet distinguishes the target im­age from the rest of the database. In addition, the retrieval 
should ideally be fast enough to handle databases with tens of thousands of images at interactive rates. 
In this paper, we describe how a Haar wavelet decomposition of the query and database images can be used 
to match a content-based query both quickly and effectively. The input to our retrieval method is a sketched 
or scanned image, intended to be an approximation to the image being retrieved. Since the input is only 
approximate, the approach we have taken is to present the user with a small set of the most promising 
target images as output, rather than with a single correct match. We have found that 20 images (the number 
of slides on a slide sheet) are about the most that can be scanned quickly and reliably by a user in 
search of the target. In order to perform this ranking, we de.ne an image querying met­ric that makes 
use of truncated, quantized versions of the wavelet decompositions, which we call signatures. The signatures 
contain only the most signi.cant information about each image. The image querying metric essentially 
compares how many signi.cant wavelet coef.cients the query has in common with potential targets. We show 
how the metric can be tuned, using statistical techniques, to discriminate most effectively for different 
types of content-based image querying, such as scanned or hand-painted images. We also present a novel 
database organization for computing this metric ex­ tremely fast. (Our system processes a 128 128 image 
query on 1.2 Overview of paper . a database of 20,000 images in under 1/2 second on an SGI Indy R4400; 
by contrast, searching the same database using an L1 met­ric takes over 14 minutes.) Finally, we evaluate 
the results of ap­plying our tuned image querying metric on hundreds of queries in databases of 1000 
and 20,000 images. The content-based querying method we describe has applications in many different domains, 
including graphic design, architecture [30], TV production [27], multimedia [29], ubiquitous computing 
[36], art history [13], geology [26], satellite image databases [16], and medical imaging [15]. For example, 
a graphic designer may want to .nd an image that is stored on her own system using a painted query. She 
may also want to .nd out if a supplier of ultra-high­resolution digital images has a particular image 
in its database, using a low-resolution scanned query. In the realm of ubiquitous comput­ing, a computer 
may need to .nd a given document in its database, given a video image of a page of that document, scanned 
in from the real-world environment. In all of these applications, improving the technology for content-based 
querying is an important and ac­knowledged challenge [8]. 1.1 Related work Previous approaches to content-based 
image querying have applied such properties as color histograms [33], texture analysis [12], and shape 
features like circularity and major-axis orientation of regions in the image [7], as well as combinations 
of these techniques. One of the most notable systems for querying by image content, called QBIC, was 
developed at IBM [20] and is now available commercially. The emphasis in QBIC is in allowing a user to 
com­pose a query based on a variety of different visual attributes; for ex­ample, the user might specify 
a particular color composition (x% of color 1, y% of color 2, etc.), a particular texture, some shape 
fea­tures, and a rough sketch of dominant edges in the target image, along with relative weights for 
all of these attributes. The QBIC sys­tem also allows users to annotate database images by outlining 
key features to search on. By contrast, the emphasis in our work is in searching directly from a query 
image, without any further speci.­cations from the user either about the database images or about the 
particulars of the search itself. The work of Hirata and Kato [10] is perhaps the most like our own in 
its style of user interaction. In their system, called query by vi­sual example (QVE), edge extraction 
is performed on user queries. These edges are matched against those of the database images in a fairly 
complex process that allows for corresponding edges to be shifted or deformed with respect to each other. 
It is dif.cult to directly compare our results with these previous methods, since running times are rarely 
reported, and since the num­ber of tests reported and the size of the databases being searched have generally 
been quite small. From the little information that has been provided, it appears that the success rate 
of our method is at least as good as that of other systems that work from a simple user sketch. To our 
knowledge, we are the .rst to use a multiresolution approach for solving this problem. Among other advantages, 
our approach al­lows queries to be speci.ed at any resolution (potentially different from that of the 
target); moreover, the running time and storage of our method are independent of the resolutions of the 
database im­ages. In addition, the signature information required by our algo­rithm can be extracted 
from a wavelet-compressed version of the image directly, allowing the signature database to be created 
con­veniently from a set of compressed images. Finally, our algorithm is much simpler to implement and 
to use than most previous ap­proaches. In the next section, we discuss our approach to image querying 
in more detail and de.ne an image querying metric that can be used for searching with imprecise queries. 
Section 3 describes the image querying algorithm in detail; the algorithm is simple enough that al­most 
all of the code is included here. Section 4 describes the appli­cation we have built on top of this algorithm, 
and gives some exam­ples of its use. Section 5 describes the results of our tests, and Sec­tion 6 outlines 
some areas for future research. Finally, the appendix discusses the statistical technique, logit, that 
we used to tune the weights of our metric.  2 Developing a metric for image querying Consider the problem 
of computing the distance between a query image Q and a potential target image T. The most obvious metrics 
to consider are the L1 or L2 norms:   jj jj I j  j QT1. QijTij(1) ij 12 jj jj I QT2.(QijTij)2(2) 
ij However, these metrics are not only expensive to compute, but they are also fairly ineffective when 
it comes to matching an inexact query image in a large database of potential targets. For example, in 
our experience with scanned queries (described in Section 5), the L1 and L2 error metrics rank their 
intended target image in the highest 1% of the database only 3% of the time. (This rank is com­puted 
by sorting the database according to its L1 or L2 distance from the query, and evaluating the intended 
target s position in the sorted list.) On the other hand, the target of the query image is almost always 
readily discernible to the human eye, despite such potential arti­facts as color shifts, misregistration, 
dithering effects, and distortion (which, taken together, account for the relatively poor performance 
of the L1 and L2 metrics). The solution, it would seem, is to try to .nd an image metric that is tuned 
for the kind of errors present in image querying; that is, we would like a metric that counts primarily 
those types of differences that a human would use for discriminat­ing images, but that gives much less 
weight to the types of errors that a human would ignore for this task. This problem is related to that 
of .nding a good perceptual error metric for images, although, to our knowledge, most previous work in 
this area has been devoted primarily to minimizing image artifacts, for example, in image com­pression 
[11, 24, 34]. Since there is no obvious correct metric to use for image querying, we are faced with the 
problem of constructing one from scratch, us­ing (informed) trial and error. The rest of this section 
describes the issues we addressed in developing our image querying metric. 2.1 A multiresolution approach 
Our goal was to construct an image metric that is fast to compute, that requires little storage for each 
database image, and that im­proves signi.cantly upon the L1 or L2 metrics in discriminating the targets 
of inexact queries. For several reasons, we hypothesized that a two-dimensional wavelet decomposition 
of the images [31, 32] would provide a good foundation on which to build such a metric: a Wavelet decompositions 
allow for very good image approxima­ tion with just a few coef.cients. This property has been exploited 
a for lossy image compression [3]. Typically, in these schemes, just the wavelet coef.cients with the 
largest magnitude are used. a Wavelet decompositions can be used to extract and encode edge information 
[19]. Edges are likely to be among the key features of a user-painted query. The coef.cients of a wavelet 
decomposition provide informa­ a tion that is independent of the original image resolution. Thus, a wavelet-based 
scheme allows the resolutions of the query and the target to be effectively decoupled. Wavelet decompositions 
are fast and easy to compute, requiring linear time in the size of the image and very little code.  
2.2 Components of the metric Given that we wish to use a wavelet approach, there are a number of issues 
that still need to be addressed: 1. Color space. We need to choose a color space in which to repre­sent 
the images and perform the decomposition. (The same issue arises for L1 and L2 image metrics.) We decided 
to try a number of different color spaces: RGB, HSV, and YIQ. Ultimately, YIQ turned out to be the most 
effective of the three for our data, as reported in Figure 4 of Section 5. 2. Wavelet type. We chose 
Haar wavelets, both because they are fastest to compute and simplest to implement. In addition, user­painted 
queries (at least with our simple interface) tend to have large constant-colored regions, which are well 
represented by this basis. One drawback of the Haar basis for lossy compres­sion is that it tends to 
produce blocky image artifacts for high compression rates. In our application, however, the results of 
the decomposition are never viewed, so these artifacts are of no con­cern. We have not experimented with 
other wavelet bases; others may work as well as or better than Haar (although will undoubt­edly be slower). 
 3. Decomposition type. We need to choose either a standard or non-standard type of two-dimensional wavelet 
decomposition [2, 31]. In the Haar basis the non-standard basis functions are square, whereas the standard 
basis functions are rectangular. We would therefore expect the non-standard basis to be better at identifying 
features that are about as wide as they are high, and the standard basis to work best for images containing 
lines and other rectangular features. As reported in Figure 4 of Section 5, we tried both types of decomposition 
with all three color spaces, and found that the standard basis works best on our data, for both scanned 
and painted queries. 4. Truncation. For a 128 128 image, there are 1282 16 384 different wavelet coef.cients 
for each color channel. Rather than using all of these coef.cients in the metric, it is preferable to 
truncate the sequence, keeping only the coef.cients with largest magnitude. This truncation both accelerates 
the search for a query and reduces storage for the database. Surprisingly, trun­cating the coef.cients 
also appears to improve the discrimina­tory power of the metric, probably because it allows the met­ric 
to consider only the most signi.cant features which are the ones most likely to match a user s painted 
query and to ignore any mismatches in the .ne detail, which the user, most likely, would have been unable 
to accurately re-create. We exper­imented with different levels of truncation and found that stor­ing 
the 60 largest-magnitude coef.cients in each channel worked best for our painted queries, while 40 coef.cients 
worked best for our scanned queries.  5. Quantization. Like truncation, the quantization of each wavelet 
coef.cient can serve several purposes: speeding the search, re­ducing the storage, and actually improving 
the discriminatory power of the metric. The quantized coef.cients retain little or no data about the 
precise magnitudes of major features in the im­ages; however, the mere presence or absence of such features 
appears to have more discriminatory power for image querying than the features precise magnitudes. We 
found that quantizing each signi.cant coef.cient to just two levels 1, represent­ f ing large positive 
coef.cients; or 1, representing large negative coef.cients works remarkably well. This simple classi.cation 
scheme also allows for a very fast comparison algorithm, as dis­cussed in Section 3. 6. Normalization. 
The normalization of the wavelet basis func­tions is related to the magnitude of the computed wavelet 
co­ef.cients: as the amplitude of each basis function increases, the size of that basis function s corresponding 
coef.cient de­creases accordingly. We chose a normalization factor that makes all wavelets orthonormal 
to each other. This normalization fac­tor has the effect of emphasizing differences mostly at coarser 
scales. Because changing the normalization factor requires re­building the entire database of signatures, 
we have not experi­mented further with this degree of freedom. 2.3 The image querying metric In order 
to write down the resulting metric, we must introduce some notation. First, let us now think of Q and 
T as representing just a single color channel of the wavelet decomposition of the query and target images. 
Let Q 0 0 and T 0 0 be the scaling function coef­.cients corresponding to the overall average intensity 
of that color channel. Further, let Qij Tij represent the ij -th trun­ and cated, quantized wavelet coef.cients 
of Q and T; these values are f either 1, 0, or1. For convenience, we will de.ne Q 00 and T 0 0 , which 
do not correspond to any wavelet coef.cient, to be 0. A suitable metric for image querying can then be 
written as QTw00 Q 00 T 00 .wijQij Tij  jjjj I jj f II We can simplify this metric in a number of 
ways. First, we have found the metric to be just as effective if the differ­ by( b) is interpreted as 
1 6 j ij 6j ence between the wavelet coef.cients Qi j Ti j is replaced Qij Tij ), where the expression 
(a if ab, and 0 otherwise. This expression will be faster to compute 6 in our algorithm. Second, we would 
like to group terms together into buckets so that only a small number of weights wij need to be determined 
experi- I mentally. We group the terms according to the scale of the wavelet functions to which they 
correspond, using a simple bucketing func­tion bin(ij), described in detail in Section 3. Finally, in 
order to make the metric even faster to evaluate over many different target images, we only consider 
terms in which the query has a non-zero wavelet coef.cient Qi j . A potential bene.t of this approach 
is that it allows for a query without much detail to match a very detailed target image quite closely; 
however, it does not allow a detailed query to match a target that does not contain that same detail. 
We felt that this asymmetry might better capture the form of most painted image queries. (Note that this 
last mod­i.cation technically disquali.es our metric from being a metric at all, since metrics, by de.nition, 
are symmetric. Nevertheless, for lack of a better term, we will continue to use the word metric in the 
rest of this paper.) Thus, the .nal Lq image querying metric Q Tq is given by jj jj w0 Q 00 T 00 . 
wbin (ij)Qij Tij(3) ij :Q ij0 jj f I L I 6I 6 The weights wb in equation (3) provide a convenient mechanism 
for tuning the metric to different databases and styles of image query­ing. The actual weights we use 
are given in Section 3, while the method we use for their computation is described in the appendix. 2.4 
Fast computation of the image querying metric To actually compute the Lq metric over a database of images, 
it is generally quicker to count the number of matching Q and T coef­.cients, rather than mismatching 
coef.cients, since we expect the vast majority of database images not to match the query image well at 
all. It is therefore convenient to rewrite the summation in (3) in terms of an equality operator (ab), 
which evaluates to 1 when ab, and 0 otherwise. Using this operator, the summation 6 . wkQ T ij : Q ij0 
 I L I 6 in equation (3) can be rewritten as . wk . wkQ T ij : Q ij0 ij :Q ij0  I L I 6I L I 6 Since 
the .rst part of this expression .wk is independent of T , we can ignore it for the purposes of ranking 
the different target images in Lq. It therefore suf.ces to compute the expression w0 Q 00 T 00 . wbin 
(ij)Qi j Ti j(4) jj I L I 6I ij :Q ij0 This expression is just a weighted sum of the difference in 
the aver­age color between Q and T, and the number of stored wavelet coef­.cients of T whose indices 
and signs match those of Q. 3 The algorithm The .nal algorithm is a straightforward embodiment of the 
Lq met­ric as given in equation (4), applied to the problem of .nding a given query in a large database 
of images. The complexity of the algorithm is linear in the number of database images. The constant factor 
in front of this linear term is small, as discussed in Section 5. At a high level, the algorithm can 
be described as follows: In a preprocessing step, we perform a standard two-dimensional Haar wavelet 
decomposition [2, 31] of every image in the database, and store just the overall average color and the 
indices and signs of the m largest-magnitude wavelet coef.cients. The indices for all of the database 
images are then organized into a single data structure in the program that optimizes searching. Then, 
for each query image, we perform the same wavelet decomposition, and again throw away all but the average 
color and the largest m coef.cients. The score for each target image T is then computed by evaluating 
expression (4). The rest of the section describes this algorithm in more detail. 3.1 Preprocessing step 
A standard two-dimensional Haar wavelet decomposition of an im­age is very simple to code. It involves 
a one-dimensional decom­position on each row of the image, followed by a one-dimensional decomposition 
on each column of the result. The following pseudocode performs this one-dimensional decom­position 
on an array A of h elements, with h a power of two: proc DecomposeArray(A : array 0h 1 of color): A Ah 
 while AA hm p 1 do: hh2   for i 0 to h 1 do: A 0 f AA fff p p Ai (A 2iA 2i 1)2 Ah i (A 2iA 2i 1)2 
 end for AA A 0  end while end proc  In the pseudocode above, the entries of A are assumed to be 3­dimensional 
color components, each in the range 0 1 . The various arithmetic operations are performed on the separate 
color compo­nents individually. An entire rr image T can thus be decomposed as follows: A proc DecomposeImage(T 
: array 0r 10r 1 of color): for row 1 to r do: DecomposeArray(T row 0r 1) end for  for col 1 to r do: 
A DecomposeArray(T 0r 1 col )  end for end proc  (In practice, the DecomposeImage routine is best implemented 
by decomposing each row, then transposing the matrix, decomposing each row again, and transposing back.) 
After the decomposition process, the entry T 0 0 is proportional to the average color of the overall 
image, while the other entries of T contain the wavelet coef.cients. (These coef.cients are suf.cient 
for reconstructing the original image T, although we will have no need to do so in this application.) 
Finally, we store only T 0 0 and the indices and signs of the largest m wavelet coef.cients of T. To 
optimize the search process, the re­maining m wavelet coef.cients for all of the database images are 
or­ganized into a set of six arrays, called the search arrays, with one ar­ray for every combination 
of sign (or ) and color channel (such f  as R, G, and B). For example, let Dcdenote the positive search 
array for the color bb channel c. Each element Dcij of this array contains a list of all images T having 
a large positive wavelet coef.cient Ti j in color channel c. Similarly, each element Dcij of the negative 
search t array points to a list of images with large negative coef.cients in c. These six arrays are 
used to speed the search for a particular query, as described in the next section. In our implementation, 
the search arrays are created as a preprocess for a given database and stored on disk. We use a small 
stand-alone program to add new images to the database incrementally. This program performs the wavelet 
decom­position for each new image, .nds the largest m coef.cients, and augments the database search arrays 
accordingly. 3.2 Querying The querying step is straightforward. For a given query image Q, we perform 
the same wavelet decomposition described in the pre­vious section. Again, we keep just the overall average 
color and the indices and signs of the largest m coef.cients in each color channel. To compute a score, 
we loop through each color channel c. We .rst Figure 1: The image querying application. The user paints 
a query in the large rectan­gular window, and the 20 highest-ranked targets appear in the small windows 
on the right. To avoid copyright infringements, the database for this example contains only 96 images 
(all created by artists who have been dead more than 75 years). Because the database is so limited, only 
the intended target (in the upper-left small window) appears to match the query very closely. compute 
the differences between the query s average intensity in that channel Qc 0 0 and those of the database 
images. Next, for each of the m non-zero, truncated wavelet coef.cients Q ci j , we search through the 
list corresponding to those database images con­taining the same large-magnitude coef.cient and sign, 
and update each of those image s scores accordingly: func ScoreQuery(Q : array 0 r 10 r 1 of color; m 
: int): DecomposeImage(Q) Initialize scores i 0 for all i for each color channel c do: for each database 
image T do:   A f fjj scores index(T) wc 0 Qc 00 Tc 00 end for Q TruncateCoef.cients(Qm) A for each 
non-zero coef.cient Q ci j do if Q ci j 0 then  A m t list Dcij b else . list Dcij end if for each 
element of list do scores index() wc bin(ij) end for end for end for return scores end func The function 
bin(ij) provides a way of grouping different coef.­cients into a small number of bins, with each bin 
weighted by some constant wb . For a given set of bins, the best weights wb can be found experimentally, 
as discussed in the appendix. The larger the training set, the more weights that can be used. The size 
of our train­ing set was suf.cient for 18 weights: 6 per color channel. In our implementation, we use 
the function bin(ij) : minmaxij5  ffgg For our database of images, a good set of weights, using the 
YIQ color space and standard decomposition, was found to be: Painted Scanned b wY b wI b wQ b wY b wI 
b wQ b 0 4.04 15.14 22.62 5.00 19.21 34.37 1 0.78 0.92 0.40 0.83 1.26 0.36 2 0.46 0.53 0.63 1.01 0.44 
0.45 3 0.42 0.26 0.25 0.52 0.53 0.14 4 0.41 0.14 0.15 0.47 0.28 0.18 5 0.32 0.07 0.38 0.30 0.14 0.27 
 (All scaling function coef.cients in our implementation are reals in the range 0 1 , so their differences 
tend to be smaller than the dif­ferences of the truncated, quantized wavelet coef.cients. Thus, the weights 
on the scaling functions w 0 have relatively large magni­tudes because they generally multiply smaller 
quantities.) As a .nal step, our algorithm examines the list of scores, which may be positive or negative. 
The smallest (typically, the most negative) scores are considered to be the closest matches. We use a 
Heap-Select algorithm [23] to .nd the 20 closest matches in linear time. 4 The application We have built 
a simple interactive application that incorporates our bb image querying algorithm. The program is written 
in C, using OpenGL and Motif. It runs on SGI workstations. A screen dump of the running application is 
shown in Figure 1. The user paints an image query in the large rectangular area on the left side of the 
application window. When the query is complete, the user presses the Match button. The system then tests 
the query against all the images in the database and displays the 20 top-ranked targets in the small 
windows on the right. (The highest-ranked target is dis­played in the upper left, the second-highest 
target to its right, and so on, in row-major order.) For convenience, the user may paint on a canvas 
of any aspect ra­tio. However, our application does not currently use this informa­tion in performing 
the match. Instead, the painted query is internally rescaled to a square aspect ratio and searched against 
a database in which all images have been similarly rescaled as a preprocess. We discuss how a user-speci.ed 
aspect ratio might also be used to im­prove the match in Section 6. Figure 2(a) shows an example of a 
painted query, along with the Lq rank of its intended target (c) in databases of 1093 and 20,558 im­ages. 
(a) Painted (b) Scanned (c) Target 12 11 jj Figure 2: Queries and their target: (a) a query painted from 
memory; (b) a scanned query; and (c) their intended target. Below the queries, the Lq ranks of the intended 
target are shown for two databases of sizes 1093 20558. j I :11 :24 :42 :51 :59  25945 7171 129 
116 11 Target Figure 3: Progression of an interactive query. Above each partially-formed query is the 
actual time (in seconds) at which the snapshot was taken. Below each query are the Lq ranks of the intended 
target for databases of sizes 1093 20558. j I 5.1 Rather than painting a query, the user may also click 
on any of the displayed target images to serve as a subsequent query, or use any stored image .le as 
a query. Figure 2(b) shows an example of using a low-quality scanned image as a query, again with its 
Lq rank in the two databases. Because the retrieval time is so fast (under 1/2 second in a database of 
20,000 images), we have also implemented an interactive mode, in which the 20 top-ranked target images 
are updated when­ever the user pauses for a half-second or more. Figure 3 shows the progression of an 
interactive query, along with the actual time at which each snapshot was taken and the Lq rank of the 
intended tar­get at that moment in the two different databases.  5 Results To evaluate our image querying 
algorithm, we collected three types of query data. Training Each training set was subdivided into 2 
equal sets. The .rst training set of 85 queries was used to determine the weights of the image querying 
metric, as described in the appendix. The second training set of 85 queries was used to .nd the optimal 
color space, decom­position type, and number m of coef.cients to store for each image. We performed an 
exhaustive search over all three dimensions, us­ing color spaces RGB, HSV, and YIQ; standard and non-standard 
wavelet decompositions; and m 10 20 30 100. For each com­bination, we found weights using the .rst set 
of images, and then tested these weights on the second set of images, using the percent­age of intended 
targets that were ranked among the top 1% in our database of 1093 images as the evaluation function. 
The results of these tests for scanned and painted queries are re­ported in Figure 4. For scanned queries, 
40 coef.cients with a stan­dard decomposition and YIQ worked best. The same con.gura­tion, except with 
60 coef.cients, worked best for painted queries. This latter con.guration was used for testing the success 
of mem­ory queries as well. Painted queries Scanned queries Successful queries (%) 100 100 YIQ, Std 
80 80 RGB, Std The .rst set, called scanned queries, were obtained by printing out small 1 2 1 2 thumbnails 
of our database images, using a full­ color Tektronix Phaser IISDX printer at 300dpi, and then scanning 
them back into the system using a Hewlett-Packard ScanJet IIc scan­ ner. As a result of these steps, 
the scanned images became somewhat HSV, Std YIQ, Non 60 60 40 40 RGB, Non HSV, Non 20 20 0 0 altered; 
in our case, the scanned images generally appeared fuzzier, darker, and slightly misregistered from the 
originals. An example of a scanned query is shown in Figure 2(b). We gathered 270 such queries, of which 
100 were reserved for evaluating our metric, and the other 170 were used as a training set. The second 
set, called painted queries, were obtained by asking 20 20 40 60 80100 20 40 60 80100 Number of coefficients 
Number of coefficients Figure 4: Choosing among color spaces (RGB, HSV, or YIQ), wavelet decomposition 
type (standard or non-standard), and number of coef.cients. 5.2 Performance on actual queries subjects, 
most of whom were .rst-time users of the system, to paint complete image queries, in the non-interactive 
mode, while looking at thumbnail-sized versions of the images they were attempting to retrieve. We also 
gathered 270 of these queries and divided them into evaluation and training sets in the same fashion. 
The third set, called memory queries, were gathered in order to see how well this style of querying might 
work if users were not looking at small versions of the images they wanted to retrieve, but instead were 
attempting to retrieve images from memory. To obtain these queries, we asked each subject to initially 
examine two targets T1 and T2, and paint a query for T1, which we threw away. The sub­ject was then asked 
to iteratively examine a targetTi1 (starting with b 2) and paint query Ti, which had not been viewed 
since before query Ti1 was painted. In this way, we hoped to get a more accurate t idea of how well a 
user might do if attempting to retrieve a familiar image from memory, without being able to see the image 
directly. Using the weights obtained from the training set, we then evaluated the performance using the 
remaining 100 queries of each type. The graphs in Figure 5 compare our Lq metric to the L1 and L2 metrics 
and to a color histogram metric Lc, for a database of 1093 images. The three graphs show, from left to 
right: scanned queries (using 40 coef.cients), painted queries (using 60 coef.cients), and memory queries 
(using 60 coef.cients). The L1 and L2 metrics in these graphs were computed on both the full-resolution 
images (128 128 pixels) and on averaged-down versions (8 8 pixels), which have roughly the same amount 
of data as the 60-coef.cient Lq metric. The color histogram metric Lc was computed by quantizing the 
pixel colors into a set of 6 6 6 bins in RGB space, and then computing an L1 metric over the number of 
pixels falling in each bin for the query versus the target. An example of a memory query is shown in 
Figure 2(a). We gath-Results for all six methods are reported by giving the percentage of ered 100 of 
these queries, which were used for evaluation only. queries y that were ranked among the top x% of the 
database images, (a) Scanned queries (b) Painted queries (c) Memory queries 0 1 2 3 4 50 1 2 3 4 50 
1 2 3 4 5 Threshold for success Threshold for success Threshold for success Figure 5: Comparison of Lq 
metric against L1,L2, and a color histogram metric Lc. The percentage of queries y ranked in the top 
x% of the database are plotted on the x and y axes. with x and y plotted on the x-and y-axes. For example, 
the leftmost data point of each curve, at x 1 1093 0 09%, reports the per- F centage of queries whose 
intended targets were ranked in .rst place for each of the six methods; the data points at x 1% report 
the per­centage of queries whose intended targets were ranked among the top 0 01109310 images; and so 
on. bfc Note that the scanned queries perform remarkably poorly under the L1, L2 and Lc metrics. These 
poor scores are probably due to the fact that the scanned queries were generally darker than their in­tended 
targets, and so matched many incorrect (darker) images in the database more closely.  5.3 Robustness 
with respect to distortions In order to test more precisely how robust the different metrics are with 
respect to some of the distortions one might .nd in image querying, we devised the following suite of 
tests. In the .rst test, 100 randomly chosen color images from the database were scaled by a factor s 
ranging from 1 to 2 and used as a query. In the second test, ff the same images were rotated by a factor 
r between 0and 45. In the third test, the same images were translated in a random direction by a distance 
t between 0 and 0.5 times the width of the query. In the fourth test, the colors of these images were 
uniformly shifted in nor­malized RGB space in a random direction by a distance c between 0 and 1. In 
the .nal test, all four of these transformations were applied for each test, in the order scale/rotate/translate/color-shift, 
with s, r, t, and c ranging as in the other tests. For all .ve tests, in cases where a border of the 
distorted image was unde.ned by the transformation (which occurs for rotations and translations), the 
image was padded with its overall average color. In cases where the color shift would lie outside the 
RGB cube, the color was clamped to 0 1 3. The top row of Figure 6 shows the results of these .ve tests. 
The curves in these graphs report the percentage of queries whose intended targets were ranked in the 
top 1% of the 1093-image database. Note that the Lq metric performs as well as or better than all other 
methods, except for Lc. However, as expected, the Lc met­ric does very poorly for color shifts, severely 
reducing this metric s utility in situations where a query s color is not always true. The bot­tom row 
shows the same .ve tests, but applied to each of our 100 scanned, painted, and memory queries all with 
the Lq metric. 5.4 Effect of database size We also wanted to test how well our method would perform 
as the size of the database was increased. We therefore gathered 19,465 images from the World Wide Web, 
using the WebCrawler [22] to .nd .les on the Web with a .gif extension. We computed a signa­ture and 
thumbnail for each image and stored the resulting database locally, along with a URL for each image 
a pointer back to the original Web site. The resulting application is a kind of graphical Web browser, 
in which a user can paint a query and very quickly see the images on the Web that match it most closely. 
Clicking on one of these thumbnail images calls up the full-resolution original from the Web. In order 
to check how well our metric performed, we created a set of 20 nested databases, with each database containing 
our original 1093 images plus increasingly large subsets of the Web database. The largest such database 
had 20,558 images. For each of the three sets of 100 queries, we then evaluated how many of those queries 
would .nd their intended target in the top 1% of the different nested databases. We found that the number 
of queries matching their cor­rect targets by this criterion remained almost perfectly constant in all 
three cases, with the number of correctly matching queries varying by at most 2% across the different 
database sizes.  5.5 Speed of evaluation We measured the speed of our program by running 68 queries 
100 times each, with databases ranging in size from n 1093 to n 20 558, and with the number of coef.cients 
ranging from m 20 to m 1000. A regression analysis indicates that the running time is linear in both 
m and n, with each query requiring approx­imately 190 0 11m 0 012n milliseconds to process on an SGI 
ff Indy R4400. This running time includes the time to decompose a 128 128-pixel query, score all n images 
in the database according to the Lq metric, and .nd the 20 top-ranked targets. As two points of comparison, 
Table 1 reports the average running time of our algorithm to that of the other methods surveyed for .nd­ing 
a query using m 20 coef.cients per channel in databases of size n 1093 and n 20 558 images. In all cases, 
the times re­ported do not include any preprocessing that can be performed on the database images alone. 
Metric Time n 1093 n 20 558 Lq 0.19 0.44 L1 (8 8) 0.66 7.46 L2 (8 8) 0.68 6.39 L1 (128 128) 47.46 892.60 
(est.) L2 (128 128) 42.04 790.80 (est.) Lc 0.47 5.03 Table 1: Average times (in seconds) to match a single 
query in databases of 1093 and 20,558 images under different metrics. (a) Scale changes (b) Rotations 
(c) Translations (d) Color shift (e) Combined changes 100  100 100 80 80 80 60 60 60 40 40 40 20 20 
20 Successful queries (%) Successful queries (%)  0 0 0 1 1.2 1.4 1.6 1.8 2 0 5 1015202530354045 0 
0.1 0.2 0.3 0.4 0.5 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 100 100 100 100 100    80 80 80 80 80 
60 60 60 60 60 40 40 40 40 40 20 20 20 20 20 0 0 0 0 0 Scale factor Figure 6: Robustness of various 
querying metrics with respect to different types of image distortions: (a) Scale changes; (b) Rotations; 
(c) Translations; (d) Color shifts; (e) All four effects combined. The top row (legend at upper right) 
compares the Lq metric to L1 ,L2 and Lc, using the target itself as the original undistorted query. The 
bottom row (legend at lower right) shows the same .ve tests applied to each of our 100 scanned, painted, 
and memory queries, using the Lq metric. 5.6 Interactive queries To test the speed of interactive queries, 
we asked users to paint in the interactive mode, and we kept track of how long it took for the intended 
target to appear among the top 20 images in the database. For these tests, we used just m 20 signi.cant 
coef.cients. For the .rst such test, we had 5 users paint a total of 106 interactive queries, allowing 
them to look at thumbnails of the intended targets. The overall median time to retrieve the target images 
was 20 sec­onds. Next, in order to see how this median query time might vary with database size, we asked 
2 users to paint a total of 21 interactive queries in our database of 20,558 images. For each query, 
the ap­plication kept a log of each paint stroke and the time at which it was drawn. We then used these 
logs to simulate how quickly the same query would bring up the intended target among the top 20 images 
in databases of various sizes. The results are shown in Figure 7. Median retrieval time (s) some random 
details, which sometimes help in bringing up the im­age. If this tactic fails, users will simply give 
up and, in a real system, would presumably fall back on some other method of searching for the image. 
(In this case, we report an in.nite query time.) We have observed two bene.ts of painting queries interactively. 
First, the time to retrieve an image is generally reduced because the user simply paints until the target 
image appears, rather than paint­ing until the query image seems .nished. Second, the interactive mode 
subtly helps train the user to .nd images more ef.ciently, because the application is always providing 
feedback about the rel­ative effectiveness of an un.nished query while it is being painted.  6 Discussion 
and future work The algorithm we have described is extremely fast, requires only a small amount of data 
to be stored for each target image, and is remarkably effective. It is also fairly easy to understand 
and im­plement. Finally, it has parameters that can be tuned for a given database or type of query image. 
Although this new image searching method has substantial advan­tages over previous approaches, its ultimate 
utility may depend to a large extent on the size of the image database being searched. Our tests suggest 
that, for the majority of non-interactive queries, our method will be able to pinpoint the correct target 
to within a 1%-sized subset of the overall database, regardless of the database s size. Thus, for a database 
of 100 images, it is easy to pull up the 50 40 30 20 10 0 Database size (x 1000) Figure 7: The effect 
of database size on median interactive query time. To see if painting from memory would affect retrieval 
time, we se­lected 20 target images and, for each subject, we randomly divided these targets into two 
equal sets. Each subject was then asked to paint the 10 images from the .rst set while looking at a thumbnail 
of the image, and the 10 images from the second set from memory, in the style described for memory queries 
above. We used 3 subjects in this experiment. We found that the median query time increased from 18 seconds 
when the subjects were looking at the thumbnails, to 22 seconds when the queries were painted from memory. 
In our experience with interactive querying, we have observed that users will typically be able to sketch 
all the information they know about an image in a minute or less, whether they are looking at a thumbnail 
or painting from memory. In most cases, the query suc­ceeds within this short time. If the query fails 
to bring up the in­tended target within a minute or so, users will typically try adding correct image 
precisely. However, for a database of 20,000 images, the user is still left with a list of 200 potential 
matches that must be searched visually, or by some other means. On the other hand, with interactive querying, 
even for a 20,000-image database it is still pos­sible to place the target into the top 20 images the 
majority of the time. Nonetheless, creating a good query becomes increasingly dif­.cult as the database 
grows. For a large enough database, even this interactive style of querying would begin to require more 
precision than most users can provide. We have tried to perform a number of different tests to measure 
the success and robustness of our image querying metric. However, it is easy to envision many more tests 
that would be interesting to per­form. One interesting test would be to try to quantify the degree to 
which different training sets affect our metric s sensitivity to vari­ous image distortions. For example, 
in querying images from mem­ory, colors are less likely to be accurate. Presumably, a training set of 
memory queries would therefore reduce the metric s sensitiv­ity to color accuracy. How signi.cant is 
this effect? In addition, it would be interesting to examine whether providing separate train­ing sets 
for individual users or for particular databases would make a signi.cant difference in the metric s discriminatory 
power. Our method also has some limitations, which we hope to address in future work. For example, while 
it is fairly robust with respect to a large degree of distortion in the query image, our metric does 
not currently allow for general pattern matching of a small query, such as an icon or company logo, inside 
some larger database image. Here are some other areas for future research: Aspect ratio. Currently, we 
allow users to choose an aspect ratio for their query; however, this aspect ratio is not used in the 
search itself. It would be straightforward to add an extra term to our image querying metric for the 
similarity of aspect ratio. The weight for this term could be found experimentally at the same time as 
the other weights are computed. Perceptually-based spaces. It would be interesting to try using a perceptually 
uniform color space, such as CIE LUV or TekHVC [5], to see if it improves the effectiveness of our metric. 
In the same vein, it may help to compute differences on logarithmically-scaled inten­sities, which is 
closer to the way intensity is perceived [9]. Image clusters. Images in a large database appear to be 
clustered in terms of their proximity under our image querying metric. For ex­ample, using a portrait 
as a query image in our Web database selects portraits almost exclusively as targets. By contrast, using 
a planet image pulls up other planets. It would be interesting to perform some statistical clustering 
on the database and then show the user some representative images from the center of each cluster. These 
could be used either as querying keys, or merely as a way of providing an overview of the contents of 
the database. Multiple metrics. In our experience with the system, we have no­ticed that a good query 
will bring up the target image, no mat­ter which color space and decomposition method (standard or non­standard) 
is used; however, the false matches found in these differ­ent spaces all tend to be very different. This 
observation leads us to wonder whether it is possible to develop a more effective method by combining 
the results of searching in different color spaces and decomposition types, perhaps taking the average 
of the ranks in the different spaces (or, alternately, the worst of the ranks), as the rank chosen by 
the overall metric. Af.ne transformation and partial queries. As discussed above, a very interesting 
(and more dif.cult) direction for future research is to begin exploring methods for handling general 
af.ne transfor­mations of the query image or for searching on partial queries. The shiftable transforms, 
described by Simoncelliet al. [28], which allow for multiresolution transforms with translational, rotational, 
and scale invariance, may be helpful in these respects. Another idea for specifying partial queries would 
be to make use of the alpha channel of the query for specifying the portions of the query and tar­get 
images over which the Lq metric should be computed. Video querying. We would like to extend our method 
to the prob­lem of searching for a given frame in a video sequence. The sim­plest solution would be to 
consider each frame of the video as a sep­arate image in the database and to apply our method directly. 
A more interesting solution would be to explore using a three-dimensional multiresolution decomposition 
of the video sequence, combined perhaps with some form of motion compensation, in order to take better 
advantage of the extra coherence among the video frames. Acknowledgements We would like to thank Leo 
Guibas, Dan Huttenlocher, and Eric Saund for many useful discussions during the initial phase of this 
project; Jutta Joesch and Werner Stuetzle for guiding us to the logit statistical model; Jack Tumblin 
and Matthew Turk for informative background on perception and matching; Karin Scholz for advice on the 
user interface; Ronen Barzel and Maureen Stone for advice on the paper; and Michael Cohen for helpful 
discussions along the way. We would also like to thank Anh Phan for help in scanning images; Brian Pinkerton 
for help in using the WebCrawler to .nd still more; and Piotr Brzezicki, Dennis Dean, Stuart Denman, 
Glenn Fleishman, George Forman, James Gunterman, Li-Wei He, Greg Heil, David Keppel, Diane King, Andrea 
Lingenfelter, Gene Mor­gan, Jonathan Shakes, Taweewan Siwadune, Christina Tonitto, Con­nie Wellnitz, 
Erin Wilson, Michael Wong, and Mikala Woodward for trying out their artistic talents as subjects in our 
tests. This work was supported by an Alfred P. Sloan Research Fellowship (BR-3495), an NSF Young Investigator 
award (CCR-9357790), an ONR Young Investigator award (N00014-95-1-0728), a grant from the University 
of Washington Royalty Research Fund (65-9731), and industrial gifts from Adobe, Microsoft, and Xerox. 
 References [1] R. Barber, W. Equitz, W. Niblack, D. Petkovic, and P. Yanker. Ef.cient query by image 
content for very large image databases. In Digest of Papers. COMPCON Spring 93, pages 17 19, San Francisco, 
CA, USA, 1993. [2] G. Beylkin, R. Coifman, and V. Rokhlin. Fast wavelet transforms and numerical algorithms 
I. Communications on Pure and Applied Mathematics, 44:141 183, 1991. [3] R. DeVore, B. Jawerth, and B. 
Lucier. Image compression through wavelet trans­form coding. IEEE Transactions on Information Theory, 
38(2):719 746, March 1992. [4] C. Faloutsos, R. Barber, M. Flickner, J. Hafner, W. Niblack, D. Petkovic, 
and W. Equitz. Ef.cient and effective querying by image content. Journal of Intelli­gent Information 
Systems: Integrating Arti.cial Intelligence and Database Tech­nologies, 3(3-4):231 262, 1994. [5] James 
D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Com­puter Graphics: Principles and Practice. 
Prentice-Hall, 1990. [6] T. Gevers and A. W. M. Smuelders. An approach to image retrieval for image databases. 
In V. Marik, J. Lazansky, and R. R. Wagner, editors, Database and Ex­pert Systems Applicatons (DEXA 93), 
pages 615 626, Prague, Czechoslovakia, 1993. [7] Yihong Gong, Hongjiang Zhang, H. C. Chuan, and M. Sakauchi. 
An image database system with content capturing and fast image indexing abilities. In Pro­ceedings of 
the International Conference on Multimedia Computing and Systems, pages 121 130. IEEE, 1994. [8] William 
I. Grosky, Rajiv Mehrotra, F. Golshani, H. V. Jagadish, Ramesh Jain, and Wayne Niblack. Research directions 
in image database management. In Eighth International Conference on Data Engineering, pages 146 148. 
IEEE, 1992. [9] Donald Hearn and M. Pauline Baker. Computer Graphics. Addison-Wesley Pub­lishing Company, 
Inc., 1994. [10] K. Hirata and T. Kato. Query by visual example content based image retrieval. In A. 
Pirotte, C. Delobel, and G. Gottlob, editors, Advances in Database Technol­ogy (EDBT 92), pages 56 71, 
Vienna, Austria, 1992. [11] N. Jayant, J. Johnston, and R. Safranek. Perceptual coding of images. In 
Proceed­ings of the SPIE The International Society for Optical Engineering , volume 1913, pages 168 
178, 1993. [12] Atreyi Kankanhalli, Hong Jiang Zhang, and Chien Yong Low. Using texture for image retrieval. 
In International Conference on Automation, Robotics and Com­puter Vision. IEE, 1994. [13] T. Kato. Database 
architecture for content-based image retrieval. In Proceedings of the SPIE The International Society 
for Optical Engineering , volume 1662, pages 112 123, San Jose, CA, USA, 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218456</article_id>
		<sort_key>287</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Animating soft substances with implicit surfaces]]></title>
		<page_from>287</page_from>
		<page_to>290</page_to>
		<doi_number>10.1145/218380.218456</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218456</url>
		<keywords>
			<kw><![CDATA[implicit surface]]></kw>
			<kw><![CDATA[inelasticity]]></kw>
			<kw><![CDATA[physics-based animation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14026403</person_id>
				<author_profile_id><![CDATA[81100041821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Desbrun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS/IMAG BP 53, F-38041 Grenoble cedex 09, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P189240</person_id>
				<author_profile_id><![CDATA[81100519117]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gascuel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[iMAGIS/IMAG BP 53, F-38041 Grenoble cedex 09, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>91427</ref_obj_id>
				<ref_obj_pid>91394</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jules Bloomenthal and Brian Wyvill. Interactive techniques for implicit modeling. Computer Graphics, 24(2):109-116, March 1990.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mathieu Desbrun and Marie-Paule Gascuel. Highly deformable material for animation and collision processing. In 5th Eurographics Workshop on Animation and Simulation, Oslo, Norway, September 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mathieu Desbrun, Nicolas Tsingos, and Marie-Paule Gascuel. Adaptive sampling of implicit surfaces for interactive modeling and animation. In First International Workshop on Implicit Surfaces, Grenoble, France, April 1995.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166157</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Marie-Paule Gascuel. An implicit formulation for precise contact modeling between flexible solids. Computer Graphics, pages 313- 320, August 1993. Proceedings of SIGGRAPH'93 (Anaheim, CA).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Annie Luciani, St6phane Jimenez, Olivier Raoult, Claude Cadoz, and Jean-Loup Florens. An unified view of multitude behaviour, flexibility, plasticity, and fractures: balls, bubbles and agglomerates. InlFIP WG 5.10 Working Conference, Tokyo, Japan, April 1991.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Agata Opalach and Steve Maddock. Implicit surfaces: Appearance, blending and consistency. In Fourth Eurographics Workshop on Animation and Simulation, Barcelona, Spain, September 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378524</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[John Platt and Alan Bart. Constraint methods for flexible models. Computer Graphics, 22(4):279-288, August 1988. Proceedings of SIGGRAPH'88 (Atlanta, Georgia).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>218086</ref_obj_id>
				<ref_obj_pid>218013</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Ari Rappoport, Alla Sheffer, Daniel Youlus, and Michel Bercovier. Volume-preserving free-form deformations. In ACM Solid Modeling'95, Salt Lake City, Utah, May 1995.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Jean-Paul Smets-Solanes. Surfacic textures for animated implicit surfaces: the 2d case. In Fourth Eurographics Workshop on Animation and Simulation, Barcelona, Spain, September 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378522</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos and Kurt Fleischer. Modeling inelastic deformations: Viscoelasticity, plasticity, fracture. Computer Graphics, 22(4):269-278, August 1988. Proceedings of SIGGRAPH'88 (Atlanta, Georgia).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Demetri Terzopoulos, John Platt, and Kurt Fleisher. Heating and melting deformable models (from goop to glop). In Graphics Interface'89, pages 219-226, London, Ontario, June 1989.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>127997</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[David Tonnesen. Modeling liquids and solids using thermal particles. In Graphics Interface'91, pages 255-262, Calgary, AL, June 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Brian Wyvill, Craig McPheeters, and Geoff Wyvill. Animating soft objects. The Visual Computer, 2(4):235-242, August 1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Brian Wyvill and Geoff Wyvill. Field functions for implicit surfaces. The Visual Computer, 5:75-82, December 1989.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Animating Soft Substances with Implicit Surfaces Mathieu Desbrun Marie-Paule Gascuel iMAGIS*/ IMAG Abstract 
This paper presents a hybrid model for animation of soft inelastic substance which undergo topological 
changes, e.g. separation and fusion and which .t with the objects they are in contact with. The model 
uses a particle system coated with a smooth iso-surface that is used for performing collision detection, 
precise contact modeling and integration of response forces. The animation technique solves three problems 
inherent in implicit modeling. Firstly, local volume controllers are de.ned to insure constant volume 
deformation, even during highly inelastic processes such as splitting or fusion. Sec­ondly, we avoid 
unwanted distance blending between disconnected pieces of the same substance. Finally, we simulate both 
collisions and progressive merging under compression between implicit sur­faces that do not blend together. 
Parameter tuning is facilitated by the layered model and animation is generated at interactive rates. 
Keywords: implicit surface, physics-based animation, inelasticity. 1 Introduction Most deformable models 
in Computer Graphics are dedicated to visco-elastic deformation: objects deform under an external force 
.eld and then progressively come back to their rest shape. Animat­ing highly deformable inelastic substances, 
such as clay, dough or mud, is a more challenging problem. These substances are charac­terized by a smooth 
surface that .ts with the objects it is in con­tact with and can undergo any topological change. One 
can make a hole in it, split a block of substance into several pieces and even merge two pieces together 
by compressing them strongly against each other. During all these deformations the total volume remains 
approximately constant. This paper presents an integrated set of methods for simulating these behaviors. 
 1.1 Previous inelastic models Contrary to elastic objects, the shape of inelastic bodies depends on 
the entire history of applied forces. Terzopoulos et al. [10] use two layers to simulate this behavior: 
an inelastic reference component, that computes motion and absorbs large scale deformations, and an elastic 
layer that represents the difference between the current and reference shapes. The model handles visco-elasticity, 
plasticity and fractures. However, since the lattice used for discretizing equations has a .xed topology, 
the model is restricted to very structured in­elastic objects. Other models [5, 11, 12] use physically-based 
particle systems for modeling a wide range of behaviors, including visco-elasticity, *iMAGIS is a joint 
project of CNRS, INRIA, Institut National Polytech­nique de Grenoble and Universit´e Joseph Fourier. 
Address: BP 53, F-38041 Grenoble cedex 09, France Email: [Mathieu.Desbrun|Marie-Paule.Gascuel]@imag.fr. 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
 plasticity, collisions, separation and fusion. The change from stiff material to the quasi-liquid state 
is achieved by adapting the in­teraction laws between particles. However, visualizing the sur­face of 
a substance during animation is dif.cult since particles can change their positions during deformations. 
Therefore, a .xed set of boundary particles cannot be used for surface representation. One solution is 
to display an iso-surface generated by the set of par­ticles [11, 12]. Nevertheless, since this surface 
is only introduced for rendering and is not considered for collision detection, visual anomalies such 
as local inter-penetrations with obstacles or bounc­ing before contact may occur. 1.2 Overview This 
paper presents a new model for interactive animation of smooth soft substances which .t with other objects 
during contact, can be split into pieces and may merge when disconnected components are compressed against 
each other. The model ensures volume preser­vation, performs collision detection and models precise contact 
sur­face and local deformation during collisions. A precise description of the surface of an object is 
maintained throughout the animation and can be used for .nal high quality rendering. Implicit surfaces 
seem to be the best surface representation for smooth bodies that deform over time and may change their 
topol­ogy [13]. Our basic idea, introduced in [2], is to combine particle systems and implicit surfaces 
during the animation. Controlled by the particles as in [11, 12], the implicit surface is animated according 
to the implicit elastic model of [4] that gives it the ability to detect collisions, to deform locally 
for exact contact modeling, and to com­pute precise integration of response forces. These forces are 
trans­mitted to the particles to be subsequently integrated. However, the direct use of this model generates 
a number of anomalies. This paper presents novel and general methods for con­trolling volume variation, 
avoiding unwanted blending effects, and simulation of both collisions and progressive fusion under compres­sion 
of disconnected pieces of the same substance. Section 2 reviews the hybrid model for soft substances 
intro­duced in [2], and discusses its limitations. Section 3 presents our new method for generating constant-volume 
deformations. An algorithm for performing separation without subsequent distance blending is detailed 
in Section 4. Section 5 explains how to process either collision or progressive fusion between surfaces 
that do not blend, according to the amount of compression forces.  2 A Hybrid Model for Soft Substances 
2.1 Combining particles and implicit surfaces The hybrid model we use for modeling soft inelastic bodies 
is com­posed of two layers (see [2]). Motion and large scale deformations are governed by an inelastic 
reference component made of particles, while an elastic implicit layer gives the current shape of an 
object and is used to compute local deformation during collisions. Reference component: As in [5, 12], 
we model inelasticity with a particle system, i.e. a set of mass points Pi subject to both attrac­tion/repulsion 
forces Fint and .uid friction forces Ffr depending on local particle density. In our system, the forces 
applied by particle P1on particle P2are: 8 4 r0 r0 P2-P1 P1. P2)= .- (1) Fint( 2 rrr P1. P2)= µ(r)||P.1-P.2|| 
(P.1-P.2) (2) Ffr( where r =||P2-P1||, .is a parameter for regulating the stiffness of a material, P.i 
is the speed vector of particle Pi,and µis a decreasing continuous function with a restricted scope of 
in.uence. External elastic layer: Implicit surfaces such as distance sur­faces [1, 13] are particularly 
suitable for animating deformable bod­ies capable of splitting and fusion. We use them as a coating over 
a particle system: each particle generates a .eld fi, a smooth de­creasing function of the distance with 
a restricted scope of in.uence, and the surface of an object is de.ned as the set of points P such as 
s f(P)= fi(P)=s, s being a given isovalue. The implicit elastic model of [4] is used to animate the implicit 
surface and for collision detection and response. This model de.nes a simple correspondence between applied 
forces and deformation, the force at a particular point being given by the local variation of the .eld 
value. Exact contact modeling is performed during colli­sions by adding deformation terms to the .elds 
de.ning objects: for objects de.ned by f1=s and f2=s, the respective values of these terms are s - f2and 
s - f1. This generates an exact contact sur­face of equation f1=f2where opposite normal compression forces 
F1.2=-F2.1=(s -f1)N2are applied. At each time step, animation is computed as follows: 1. Compute the 
new position of each particle by integrating the associated equation of motion from the set of applied 
forces. 2. After a pre-detection with bounding boxes, use the implicit surface generated by the particles 
for detecting collisions (test the sample points of an object against the .eld of another one). 3. Avoid 
inter-penetration by generating exact contact surfaces between colliding objects, and compute response 
forces. 4. Distribute response forces between particles that contribute to surface generation in contact 
area. These forces will be used at the next time step.   2.2 Problems to be solved Despite its capability 
of de.ning smooth substances that .t with other objects during contact, this hybrid model generates several 
anomalies. Due to the implicit coating, a piece of substance may undergo very signi.cant volume variation 
during deformations, especially during separation and fusion. The partial solution proposed in [2] is 
far from suf.cient. Firstly, it is based on the choice of a speci.c .eld function2, which is very restrictive 
since both the shape of an object and its stiffness are controlled by this function. Moreover, it gives 
good results near equilibrium states only, so it is of no use for animating large scale deformations 
and topological changes. The second problem concerns the irreversibility of soft substance splitting. 
Two pieces coming back close to each other should not produce the same intermediate shapes than when 
they were discon­nected, i.e. they should not blend before contact [9]. Unfortunately, since they are 
components of the same implicit body, their surfaces locally in.ate and merge. This artifact is related 
to the well-known unwanted blending problem [6, 14], but the dif.culty is intensi­.ed in this case, since 
the desired blending properties for a soft sub­stance are changing with its topology. 3 2This function 
is fi(P)=(r0/r),where r is the distance d(P,Pi)and where r0, introduced in equation (1), is the rest 
distance between particles. Finally, both collision and progressive fusion between soft bodies are to 
be produced, depending on their physical properties and on the amount of compression forces that press 
them against each other. The following sections present solutions to these three problems.  3 Constant 
Volume Deformation Constant volume deformation of .exible models have already drawn some attention [7, 
8]. In the case of objects discretized into lattices of .xed topology, the problem can be solved by using 
con­strained optimization techniques based on Lagrange multipliers. To the authors knowledge, no solution 
has been proposed in the case of soft substances, although volume variations may increase due to topological 
changes such as separation or fusion. This section presents a general method for controlling volume of 
objects de.ned using implicit surfaces. This method is well adapted to the soft substances we are modeling, 
but can also be applied to any other way of animating implicit surfaces. 3.1 Basic ideas Our aim was 
to develop a general method for controlling the volume of an implicit object that does not set any restriction 
on the choice of the .eld function, thus allowing for a wide range of shapes and stiffness to be modeled. 
First of all, a good way of detecting volume variation must be de­ aaa .ned. An implicit volume de.ned 
by V = dxdydz f(P)>=s where P =(x,y,z), cannot be computed analytically for arbitrary .eld functions. 
Discretizing space into voxels can be used for .nd­ing an approximate value. However, this way of detecting 
volume variation would not give us any chance of solving the problem. As illustrated by the example in 
Figure 1, reducing the strength of .eld contributions at step 2 in order to avoid volume variation should 
only be done in the area where the object has been deformed. Thus, volume should be controlled locally. 
step 1 step 2  (a) (b) Figure 1: (a) Volume variation. (b) Local volume control in step 2. Our basic 
idea is to detect the area where the volume is changing, and then adjust the strength of local .elds 
in this area. Thus, a notion of local volume needs to be de.ned. We call the territory Ti of a particle 
Pi the part of the implicit object where its .eld contribution is the highest. Territories form a partition 
of the implicit volume (f(P)= s). Ti ={P . IR/ (f(P)= s) and (.jfi(P)= fj(P))} The local volume Vi associated 
with Pi is the volume of Ti.  3.2 Detecting local volume variation We are looking for an ef.cient way 
of approximating local volume. Since deformation is continuous over time, we can take advantage of temporal 
coherence. This is achieved by maintaining a sampling of territory boundaries throughout the animation. 
Each particle sends a .xed number of points called seeds to sample its territory boundary, in a set of 
distributed directions called seed-axes that are de.ned in the particle local coordinates system (see 
Figure 2). At each animation step, seeds migrate to the surface from their previous position along their 
axis. They stop either when Ti Particle Territories Sampling Ti with seeds Vi approximation Figure 2: 
Particle territories and seeds used for volume approximation. they have reached the isosurface or when 
the preponderant .eld fi becomes smaller than another one. We approximate Vi by the sum of the volumes 
of small pyramids de.ned by seeds (see Figure 2): Vi = Ki d(s,Pi)3 s.Si where Si is the set of seeds 
sent by Pi, and where the factor Ki only depends on the seed repartition chosen for Pi. In practice, 
Ki can be left out in volume computations, since it is suf.cient to maintain s s.Si d(s,Pi)3 at a constant 
level.  3.3 Local volume control We control local volume variation by associating a proportional­derivative 
controller with each particle. Given the current local vol­ume Vi and the initial value Vi,0 to maintain, 
this controller outputs an adequate adjustment of the .eld function fi. For our application, the way 
of modifying fi must be chosen carefully since the norm of its gradient gives the local stiffness of 
an object [4]. In order to adjust the volume of particle territories with­out modifying the physical 
properties of the object, we combine the original .eld function with a translation .i,t. At each time 
step, the .eld, originally de.ned by the decreasing function of the distance fi(P)=hi(d(P,Pi)), is replaced 
by: fi,t =hi (d(P,Pi)-.i,t ). In order to produce steady shape variation, we control the time derivative 
..i,t of the translation parameter rather than its value. The input of the proportional-derivative controller 
consists of the nor­malized volume variation .i,t and its time derivative ..i,t:  Vi,t -Vi,0 .Vi,t -Vi,t-dt 
.i,t =.i,t = Vi,0 Vi,0 dt and its output is: ..i,t = a.i,t +ß..i,t A simple example of volume control 
is given in Figure 3. Figures 5 and 6 show the results obtained during fusion, where volume con­trol 
is essential. Otherwise, very signi.cant and sudden increases of volume would be produced when two soft 
bodies merge. (a) Initial positions (b) Without control: 41% (c) Constant volume of volume increase 
(error under 3%) Figure 3: Volume control during a blending process (a =10.0and ß =1.0).  4 Avoiding 
Unwanted Blending One of the main dif.culties raised by the animation of implicit sur­faces is the avoidance 
of undesirable blending effects between ob­jects. This problem is well known in the case of character 
anima­tion: arms and legs of a character should not blend together, al­though both blend with the body. 
The solution suggested in [14] pre-de.nes a blending graph where a connection between two skele­tons 
indicates that their .eld contributions are to be added. Then, the .eld value at a point P is computed 
by .rst .nding the skeleton with the highest .eld contribution at P, and then adding the contributions 
of neighboring skeletons in the graph3 . The unwanted blending problem is more dif.cult in the case 
of a soft substance splitting into pieces. If two disconnected pieces come back close to each other, 
they will blend at some distance as in Fig­ure 3 rather than colliding because they are considered to 
be parts of the same object. Moreover, since the topology of the substance varies over time, a pre-de.ned 
blending graph cannot be used: the blending properties of the particles change according to the surface 
decomposition into connected components.  Figure 4: The in.uence graph and its connected components. 
Particles Aand B lie in the same component, so their .elds will blend if they come close to each other. 
Therefore, we compute a time-varying blending graph, repre­sented by lists of neighbors called a blending 
list associated with each particle. Processing unwanted blending is achieved by reduc­ing blending lists 
each time the implicit surface breaks into discon­nected components that no longer blend. Before the 
animation, the blending graph is initialized as a complete graph, where each parti­cle is connected to 
all others. Then, at each animation step: 1. For each pair of particles, look if their spheres of in.uence, 
de­.ned by the radius of in.uence of their .eld, intersect. Transi­tive closure of this relation, that 
de.nes an in.uence graph , gives the in.uence connected components (see Figure 4). 2. For each particle, 
remove from its blending list those of the neighbors that are no longer in the same in.uence component. 
 3. Use the blending graph and the .eld function it de.nes for computing seeds migration. Since seeds 
sample the territory boundary of a particle, .elds can be evaluated very ef.ciently: we already know 
which .eld contribution is the highest, so we just have to add the contributions from the neighbors. 
  The soft substances animated with this method split into compo­nents that no longer blend, as shown 
in the three .rst frames in Fig­ure 5. The next section explains how to handle collisions between these 
components, and to enable fusions according to the amount of compression forces and to the properties 
of the substance. Figure 5: Soft substance made of 9 particles grabbed away by pliers and released. 
 3The blending graph must be de.ned with care, otherwise surface discon­tinuities may appear between 
areas where a given skeleton s contribution is still considered, and zones where it is not a neighbor 
anymore. Figure 6: Progressive fusion under compression of two colliding bodies.  5 From Collision 
to Progressive Fusion 5.1 Collision processing Instead of processing collisions between pairs of objects 
as in [2, 4], the collision processing algorithm computes collisions between each pair of particle territories 
such that the .elds of the particles do not blend together. As a result, if a soft substance splits into 
pieces, subsequent collisions will be detected between the pieces. The main problem is to enable collision 
detection between particle territories. The methods for modeling contact and computing colli­sion response, 
reviewed in Section 2, do not need to be modi.ed. The data needed for detecting collisions between implicit 
sur­faces are a bounding box around each of these surfaces and a set of points that sample them. These 
sample points will be tested against the .eld function of another object. In our implementation, we use 
the seeds introduced in Section 3 for both sampling the portion of the surface associated with a particle 
territory Ti and computing a local bounding box around it: A seed is said to be valid if it reaches 
the iso-surface during migration. At each time step, the set of valid seeds associated with Ti gives 
the sample points needed for collision detection.  The axis-parallel bounding box associated with Ti 
is computed from the positions of the valid seeds, and enlarged by the max­imum distance between neighboring 
seeds.  Local bounding boxes are grouped into connected component  boxes to optimize collision detection. 
Valid seeds can also be used for the interactive opaque display of the substance during the animation 
[3]. 5.2 Fusion under compression Blocks of soft substance such as clay or dough merge under com­pression 
forces that exceed a speci.ed threshold. This behavior can be easily simulated with our model. The fusion 
threshold is added as a new parameter in a substance description. Then, each time a collision is computed 
between two bodies made of the same substance, compression forces computed along the contact surface 
between two particle territories are com­pared with the fusion threshold. If the compression exceeds 
it, each particle adds the other one to its blending list. At the next time step, .elds from the two 
bodies will locally blend in this area, while col­lisions will still be computed elsewhere (see Figure 
6). As mentioned in Section 4, the use of a blending graph that is not fully connected may produce discontinuities 
in the implicit sur­face. A solution, introduced in [6], de.nes the .eld value at a point P as the maximum 
.eld from groups of particles that blend together. In our current implementation we obtain the same effect 
by render­ing a substance as a set of implicit components representing these groups. Each component includes 
the area where the groups merge and a union of all components gives the .nal isosurface. However, neither 
of these methods avoids tangent discontinuities in the .nal shape.  6 Conclusions We have presented 
a hybrid model for animation of smooth soft sub­stances that maintain their volume, collide and can undergo 
separa­tion and fusion during animation. Control is facilitated by the lay­ered nature of the model. 
It combines a particle system that mod­els large scale inelastic deformations, an implicit surface that 
gener­ates local deformations during contact and a control module that per­forms local volume preservation. 
Animation is computed and visu­alized at interactive rates. The implicit surface parameters are stored 
at each animation step providing a compact storage of the animation and enabling the direct use of surface/ray 
intersection algorithms for computing .nal high-quality images. The solution developed for local control 
of implicit volumes of­fers a general contribution to the .eld of animation using implicit surfaces. 
For instance, it can be used in character animation, for set­ting user-de.ned volume variation that imitate 
the contraction and dilatation of muscles. We are currently experimenting with a solu­tion to avoid tangent 
discontinuities in the implicit surface when an object is modeled using a blending graph. Acknowledgements 
Many thanks to Jean-Dominique Gascuel for his ef.cient ray-tracing soft­ware, to the reviewers for their 
helpful comments, and to Agata Opalach for carefully re-reading this paper.  References [1] Jules Bloomenthal 
and Brian Wyvill. Interactive techniques for im­plicit modeling. Computer Graphics, 24(2):109 116, March 
1990. [2] Mathieu Desbrun and Marie-Paule Gascuel. Highly deformable mate­rial for animation and collision 
processing. In 5th Eurographics Work­shop on Animation and Simulation, Oslo, Norway, September 1994. 
[3] Mathieu Desbrun, Nicolas Tsingos, and Marie-Paule Gascuel. Adap­tive sampling of implicit surfaces 
for interactive modeling and anima­tion. In First International Workshop on Implicit Surfaces, Grenoble, 
France, April 1995. [4] Marie-Paule Gascuel. An implicit formulation for precise contact modeling between 
.exible solids. Computer Graphics, pages 313 320, August 1993. Proceedings of SIGGRAPH 93 (Anaheim, CA). 
[5] Annie Luciani, St´ephane Jimenez, Olivier Raoult, Claude Cadoz, and Jean-Loup Florens. An uni.ed 
view of multitude behaviour, .exibility, plasticity, and fractures: balls, bubbles and agglomerates. 
In IFIP WG 5.10 Working Conference, Tokyo, Japan, April 1991. [6] Agata Opalach and Steve Maddock. Implicit 
surfaces: Appearance, blending and consistency. In Fourth Eurographics Workshop on Ani­mation and Simulation, 
Barcelona, Spain, September 1993. [7] John Platt and Alan Barr. Constraint methods for .exible models. 
Computer Graphics, 22(4):279 288, August 1988. Proceedings of SIGGRAPH 88 (Atlanta, Georgia). [8] Ari 
Rappoport, Alla Sheffer, Daniel Youlus, and Michel Bercovier. Volume-preserving free-form deformations. 
In ACM Solid Model­ing 95, Salt Lake City, Utah, May 1995. [9] Jean-Paul Smets-Solanes. Surfacic textures 
for animated implicit sur­faces: the 2d case. In Fourth Eurographics Workshop on Animation and Simulation, 
Barcelona, Spain, September 1993. [10] Demetri Terzopoulos and Kurt Fleischer. Modeling inelastic defor­mations: 
Viscoelasticity, plasticity, fracture. Computer Graphics, 22(4):269 278, August 1988. Proceedings of 
SIGGRAPH 88 (At­lanta, Georgia). [11] Demetri Terzopoulos, John Platt, and Kurt Fleisher. Heating and 
melt­ing deformable models (from goop to glop). In Graphics Interface 89, pages 219 226, London, Ontario, 
June 1989. [12] David Tonnesen. Modeling liquids and solids using thermal particles. In Graphics Interface 
91, pages 255 262, Calgary, AL, June 1991. [13] Brian Wyvill, Craig McPheeters, and Geoff Wyvill. Animating 
soft objects. The Visual Computer, 2(4):235 242, August 1986. [14] Brian Wyvill and Geoff Wyvill. Field 
functions for implicit surfaces. The Visual Computer, 5:75 82, December 1989.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218458</article_id>
		<sort_key>291</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Decorating implicit surfaces]]></title>
		<page_from>291</page_from>
		<page_to>300</page_to>
		<doi_number>10.1145/218380.218458</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218458</url>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Texture</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010243</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Appearance and texture representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Reliability</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31091965</person_id>
				<author_profile_id><![CDATA[81332520351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hans]]></first_name>
				<middle_name><![CDATA[K&#248;hling]]></middle_name>
				<last_name><![CDATA[Pedersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of North Carolina at Chapel Hill and Integrated Systems, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E.A. Bier and K. R. Sloan, Jr. Two part texture mappings. IEEE Computer Graphics and Applications, 6(9):40-53, September 1986.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139889</ref_obj_id>
				<ref_obj_pid>139834</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max and Geoff Wyvill. Shapes and textures for rendering coral. In N. M. Patrikalakis, editor, Scientific Visualization of Physical Phenomena (P1vc eedin g s of CG International' 91), pages 333-343. Springer-Verlag, 1991.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30309</ref_obj_id>
				<ref_obj_pid>30300</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Geoff Wyvill, Brian Wyvill, and Craig McPheeters. Solid texturing of soft objects. In CG International'87. Tokyo, May 1987.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Darwyn R. Peachey. Solid texturing of complex surfaces. In B. A. Barsky, editor, Computer Graphics (SIGGRAPH'85 P1vceedings), volume 19, pages 279-286, July 1985.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ken Perlin. An image synthesizer. In B. A. Barsky, editor, Computer Graphics (SIGGRAPH '85 P1vceedings), volume 19, pages 287-296, July 1985.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192227</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Andrew R Witkin and Paul S. Heckbert. Using particles to sample and control implicit surfaces. In Andrew Glassner, editor, Proceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 269-278. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97903</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. In Forest Baskett, editor, Computer Graphics (SIGGRAPH '90 P1vceedings), volume 24, pages 215-223, August 1990.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Barbara Robertson. Fresh paint. Computer Graphics World, 17(12):28-37, December 1994.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192187</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Peter Litwinowicz and Gavin Miller. Efficient techniques for interactive texture placement. In Andrew Glassner, editor, Proceedings of SIGGRAPH '94 (01- lando, Florida, July 24-29, i994), Computer Graphics Proceedings, Annual Conference Series, pages 119-122. ACM SIGGRAPH, ACM Press, July 1994.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199429</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Maneesh Agrawala, Andrew C. Beers, and Marc Levoy. 3 d painting on scanned surfaces. In P1vceedings 1995 Symposium on Interactive 3D Graphics (Monterey, California, April 9-12,1995), pages 145-152.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122744</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Chakib Bennis, Jean-Marc V6zien, G6rard Igl6sias, and Andr6 Gagalowicz. Piecewise surface flattening for non-distorted texture mapping. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 P1vceedings), volume 25, pages 237-246, July 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J6r6me Maillot, Hussein Yahia, and Anne Verroust. Interactive texture mapping. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH '93 P1vceedings), volume 27, pages 27-34, August 1993.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[M. Samek. Texture mapping and distortion in digital graphics. Visual Computer, 2(5):313-20,1986.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35072</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Richard Bartels, John Beatty, and Brian Barsky. An Intlvduction to Splines for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann Publishers, Palo Alto, CA, 1987.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[S. Gabriel and James T. Kajiya. Spline interpolation in curved space. State of the Art Image Synthesis, Course notes for SIGGRAPH '85, 1985.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[William Welch and Andrew Witkin. Variational surface modeling. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 P1vceedings), volume 26, pages 157-166, July 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>172376</ref_obj_id>
				<ref_obj_pid>172372</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[R. Dietz, J. Hoschek, and B. Jtittler. An algebraic approach to curves and surfaces on the sphere and on other quadrics. Computer Aided Geometric Design, 10(3):211-230, August 1993.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>190249</ref_obj_id>
				<ref_obj_pid>190070</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Ron Kimmel, A. Amir, and A. M. Bruckstein. Finding shortest paths on surfaces. In Pierre-Jean Laurent, editor, Curves and Sulfaces in Geometric Design, pages 259-268. A. K. Peters, Wellesley, Massachusetts, August 1994.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Nicholas M. Patrikalakis and George A. Kriezis. Representation of piecewise continuous algebraic surfaces in terms of B-splines. The Visual Computer, 5 (6) :360-374, December 1989.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[L. L. Schumaker and C. Traas. Fitting scattered data on spherelike surfaces using tensor products of trigonometric and polynomial splines. Numerische Matematik, 60(1):129-139,1991.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>33372</ref_obj_id>
				<ref_obj_pid>33367</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[J.S.B. Mitchell, D. M. Mount, and C. H. Papadimitriou. The discrete geodesic problem. SlAM J. Comput., 16(4):647-668,1987.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Karsten Opitz and Helmut Pottmann. Computing shortest paths on polyhedra: Applications in geometric modeling and scientific visualization. Intl. Journal of Computational Geometry and Applications, 4(2):165-178, June 1994.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134086</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Alan H. Barr, Bena Currin, Steven Gabriel, and John F. Hughes. Smooth interpolation of orientations with angular velocity constraints using quaternions. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH ' 92 P1vceedings), volume 26, pages 313-320, July 1992.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>55285</ref_obj_id>
				<ref_obj_pid>55279</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal. Polygonization of implicit surfaces. Computer Aided Geometric Design, 5(4):341-356,1988.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Brian Wyvill, Craig McPheeters, and Geoff Wyvill. Data structure for soft objects. The Visual Computer, 2(4):227-234,1986.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Manfredo R do Carmo. Differential Geometry of Culwes and Sulfaces. Prentice-Hall Inc., 1976. ISBN 0-13-212589-7.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Thomas H. Cohen, Charles E. Leiserson, and Ronald L. Rivest. Intlvduction to Algorithms. MIT Press, Cambridge, Massachusetts, 1990.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134011</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Surface reconstruction from unorganized points. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 P1vceedings), volume 26, pages 71-78, July 1992.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Generating textures for arbitrary surfaces using reaction-diffusion. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH '91 P1vceedings), volume 25, pages 289-298, July 1991.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Ron Kimmel and Nahum Kiryati. Finding shortest paths on surfaces by fast global approximation and precise local refinement. In SPIE Vision and Geometry III, pages 198-209, November 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Richard E. Williamson, Richard H. Crowell, and Hale F. Trotter. Calculus of Vector Functions. Prentice-Hall Inc., 1962 (first edition). ISBN 0-13-112367.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>186755</ref_obj_id>
				<ref_obj_pid>186741</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Charles Loop. A G} triangular spline surface of arbitrary topological type. Computer Aided Geometric Design, (11):303-330,1994.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>207497</ref_obj_id>
				<ref_obj_pid>207475</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[J/3rg Peters. C } surface splines. SIAM Journal on NumericalAnalysis, October 1993.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>161150</ref_obj_id>
				<ref_obj_pid>160985</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Paul Chew. Guaranteed quality mesh generation for curved surfaces. In ACM Symposium on Computational Geometry, 1993.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Greg Turk. Re-tiling polygonal surfaces. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH '92 P1vceedings), volume 26, pages 55-64, July 1992.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[William Welch and Andrew Witkin. Free-Form shape design using triangulated surfaces. In Andrew Glassner, editor, P1vceedings of SIGGRAPH '94 (Orlando, Florida, July 24-29, 1994), Computer Graphics Proceedings, Annual Conference Series, pages 247-256. ACM SIGGRAPH, ACM Press, July 1994. ISBN 0-89791-667-0.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180909</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Andrew J. Hanson. Geometry for N-dimensional graphics. In Paul Heckbert, editor, Graphics Gems IV, pages 149-170. Academic Press, Boston, 1994.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83821</ref_obj_id>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes. Computer Graphics, Principles and Practice, Second Edition. Addison-Wesley, Reading, Massachusetts, 1990. Overview of research to date.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357310</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. A generalization of algebraic surface drawing. ACM Transactions on Graphics, 1 (3):235-256, July 1982.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[F. Sebastian Grassia. Using particles to texture implicit surfaces. Assignment for Paul Heckbert's Rendering course at CMU, unpublished, December 1993.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Decorating Implicit Surfaces Hans Køhling Pedersen Department of Computer Science University of North 
Carolina at Chapel Hill. Abstract This work presents a new general approach for applying textures onto 
implicit surfaces. Its main contributions are:  An improved set of interactive tools for subdividing 
implicit surfaces into convenient patches.  An ef.cient and reliable algorithm for deriving parameteri­zations 
for these patches.  A new set of useful texturing operations.  These results provide a uni.ed representation 
scheme for a variety of texturing techniques that were previously incompatible, allow­ing more ef.cient 
manipulation, storage, and rendering of textured objects and a fuller use of current texture mapping 
hardware.  1 Introduction. While the class of implicit surfaces often referred to as soft ob­ jects 
, blobs or meta-balls historically has played a prominent role in the computer graphics literature, these 
models have only experienced limited practical use in the entertainment industry to date. This unful.lled 
promise can largely be attributed to two weaknesses that have plagued implicits compared to the more 
widely used spline-based models. First, dif.culties in visualizing changing implicit surfaces in real-time 
have made interactive modeling awkward, and second, because implicits have no natural material coordinates,they 
are not as .exible with respect to texture mapping as parametric surfaces. Existing techniques for texturing 
implicit surfaces [1, 2, 3] are predominantly based on solid textures [4, 5], as these do not require 
material coordinates; solid textures are excellent for simulating objects carved out of wood, marble, 
and similar materials, but not a natural choice for patterns related to the intrinsic surface geometry. 
While Witkin &#38; Heckbert s recent progress in implicit surface modeling [6], based on visualizing 
implicits as a set of particles (see .gure 4), provides a promising new approach to the .rst prob­ lem, 
the texturing problems continue to compromise the practical applicability of implicit surfaces in computer 
graphics imaging. This paper addresses that lack. 1.1 Related work. For the purposes of our analysis, 
it will be convenient to divide existing work on interactive texturing of 3D surfaces into two categories: 
painting methods, in which texture detail is applied or manipulated locally on the object, and patch 
based methods, where the surface is covered with an atlas of charts, each of which de.nes a separate 
texture mapping. *The author can be reached at Center for Integrated Systems, Stanford University, Stanford, 
CA 94305-4070 (e-mail: hkp@daimi.aau.dk).  Permission to make digital/hard copy of part or all of this 
work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, 
and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 
ACM-0-89791-701-4/95/008 $3.50 1.1.1 Painting methods. Hanrahan and Haeberli presented the .rst systematic 
approach for painting 3D surfaces in [7], sparking the evolution of a new and thriving sub.eld in computer 
graphics software. Their method, which assumes that a suitable uv-parameterization for the surface is 
available, uses the parameter space to store texture information, and forms the basis for all currently 
available commercial 3D texturing applications [8]. The main shortcoming of this approach is that it 
relies entirely on user-intensive brush painting operations that require a substantial amount of patience 
and artistic skills in order to obtain good results in practice. Motivated by the need for supplementary 
high-level texturing tools, Litwinowicz and Miller [9] presented an interactive strategy for compensating 
for the distortion of texture mappings, allowing the original parameterization to be replaced with a 
more suitable mapping of the texture. However, they did not develop a general mechanism for specifying 
the boundaries of the texture on the destination surface. To eliminate the need for a parameterization 
of the surface, Agrawala et al. [10] developed an interactive system for painting scanned polyhedral 
meshes using a 3D input device. This alterna­tive approach could also be applied in the case of implicit 
surfaces, but requires very densely sampled meshes and is not well suited for detailed high-frequency 
textures. 1.1.2 Patch based methods. In this category, the common thread in the references ([11, 12, 
13]) is to subdivide the surface into a set of charts and then introduce a bivariate uv-mapping for each 
chart while trying to minimize distortion within the parameterized regions. Bennis et al. [11] presented 
an incremental algorithm for re­ducing distortion of C2 continuous texture mappings in which the user 
.rst partitions the surface into suitable patches interac­tively, and these patches are then reparameterized 
( .attened )in a relaxation process. In [12], a different optimization scheme for improving an existing 
texture mapping was presented by Maillot etal. Inaddition,thispaperdiscusseddifferent unfolding methods 
for deriving parameterizations for general polygonal meshes. A comparison between an automatic and an 
interactive parameteri­zation scheme produced the conclusion that the best results were obtained by letting 
the user specify the charts interactively and us­ing the automated technique as a supplementary tool 
for complex surfaces. This observation is supported by the fact that anima­tors like control over patch 
placement, so the model s surface and textures look correct when motion is added. Unfortunately, these 
unfolding/.attening techniques suffer from several limitations. First, minimizing distortion within each 
chart is performed at the expense of introducing seams at the bor­ders between the parameterized regions,and 
this type of zero-order discontinuities is a severe detraction for applications depending on visual appeal, 
no matter how much distortion is reduced (as an example of these artifacts, there would be half dots 
at the edges of a dotted region). Second, the methods offer little control over boundary conditions, 
resulting in dif.culties in matching texels at borders between charts (see .gure 1). Most importantly, 
how­ever, the textures have to be aligned with the borders between the charts, and if, for example, a 
photo-texture is to be applied across a border, it is necessary to make changes in the underlying parameterization. 
Combined with the fact that existing tools for de.ning the charts are user-intensive, these problems 
make un­folding/.attening methods dif.cult to apply in practice and limit the range of texturing operations 
that can be applied. Figure 1: Matching texels at borders between charts: existing parameterization 
techniques offer little control over boundary conditions, making the application of many texturing operations 
unnecessarily complex.  1.2 Synthesis and outline of contents. The historical distinction between painting 
and patch based textur­ing techniques is arti.cial and unfortunate: The patches produced by existing 
parameterization methods are not well suited for in­teractive painting, and existing painting methods 
could bene.t from new high-level texturing operations based on .exible param­eterization techniques. 
Furthermore, because existing techniques have focused on parametric and polyhedral surfaces, they have 
not taken advantage of the attractive mathematical representation offered by implicit models. Motivated 
by these two observations, the remainder of this doc­ument will describe a new approach for parameterizing 
implicit surfaces that is speci.cally designed for 3D painting. Although several of the quoted patch 
based references advo­cated the use of user speci.ed curves in the surface subdivision step, none of 
them elaborated on how this interaction was per­formed, and we will therefore start with a coherent solution 
to this problem in section 2. Section 3 and 4 will deal with the problem of .nding material coordinates 
for the patches, and these results will then lead to the design of new texturing algorithms in section 
5. Finally, section 6 will discuss ideas for future work.  2 Curves on implicit surfaces. While the 
theory of interpolating curves in the plane is well stud­ied, its generalization to 3D surfaces remains 
an active area of research, and existing work can conveniently be categorized into parametric [14, 15, 
16] and implicit/algebraic approaches. Our interest is in the latter category, which unfortunately is 
the least explored in the computer graphics literature. Dietz et al. [17] derived and implemented analytical 
tools for .tting Bezier curves onto general quadratic surfaces, and related work is described in [18, 
19, 20]. However, the non-polynomial nature of soft objects lends itself poorly to existing mathematical 
tools and has left their theory of interpolating curves largely un­explored. One solution is to polygonize 
the surface and thereby reduce the problem into the discrete geodesics problem, which is currently an 
active area of research in computational geometry [21, 22]. Unfortunately, this theory currently does 
not at allow such curves to be computed and manipulated interactively on gen­eral 3D surfaces, and its 
generalization to spline curves is yet to be explored. Instead, we will use an optimization approach 
similar to the one employed in by Barr et al. in [23] for smoothly interpolating quaternions in 4-space. 
However, as the robustness of any opti­mization technique relies on the quality of the initial guess 
to the solution, and as the linear interpolation between the key nodes employed in [23] does not work 
in the case of soft objects, section 2.1 will derive an ef.cient algorithm for computing a better guess 
for the shortest geodesic between two points on the implicit sur­face. This initial approximation will 
subsequently be optimized to a geodesic curve in section 2.2. Section 2.3 describes a general­ization 
of this curve drawing algorithm from geodesics to smooth interpolating curves, and section 2.4 concludes 
with a discussion of implementation issues. 2.1 Estimating geodesics. Given two arbitrary points on the 
implicit surface, the problem is to compute the shortest path between them that is embedded in the surface. 
Furthermore, these curves should be computed at interactive speed, and the interface for drawing them 
should be convenient (in our system, the user selects the two endpoints by pointing at arbitrary locations 
on the surface, and the algorithm described below then creates the geodesic on-line). As this is an overwhelming 
task for non-polynomial surfaces, we choose to make the following simplifying assumptions: 1. The surface 
does not deviate excessively from its tangent plane within a given distance 40of any point1. 2. If, 
for two arbitrary points pand qon the surface, jp-qj 40,then hnpnqi0, where npand nqdenote the local 
normal, n,at pand q. 3. The curve will be approximated by a piecewise polynomial interpolant.  The 
.rst two assumptions effectively set an upper bound for the Nyquist limit of the surface, allowing it 
to be sampled with­out missing any sharp spikes or violating the topology. These assumptions are reasonable 
as soft objects are characterized by their smoothness and rarely contain sharp corners or discontinu­ities, 
and are made in some formulation by virtually all existing algorithms involving implicit surfaces (for 
example [6, 24, 25] ). The last assumption is necessary as no practical mathematical tools for representing 
this kind of curves exist, and it makes sense as the implicit surface itself will eventually be replaced 
with a piecewise linear approximation in order to facilitate ef.cient scan­line rendering. 2.1.1 Surface 
discretization. The .rst task is to replace the continuous surface representation with a suitable discretization 
that is easier to work with. Fortu­nately, recent results [6] present an ef.cient interactive solution 
to this problem: the implicit surface is sampled by a uniformly distributed collection of particles that 
repel each other subject to the constraint that they lie on the surface. Although this algorithm does 
not automatically determine an appropriate sampling fre­quency (cf. assumption 1 above), it is so intuitive 
and easy to use that we will assume that the user sets the parameter 0manually. The sampling distribution 
arising from [6] has the attractive property that the samples form a locally hexagonal pattern almost 
everywhere that maximizes the minimum distance between each sample and its closest neighbor. This coherence 
can be exploited for computing a guess for the shortest path between two points on the surface. 2.1.2 
Approximating geodesics. Given an appropriate set of samples, the idea is to use the obser­vation that 
geodesics locally minimize the distance between two points on a surface [26] (prop. 4, p. 292) (the surface 
equivalent of the shortest curve between two points in a plane being a straight line). More speci.cally, 
the shortest path between the points in a graph faithfully representing the surface geometry is likely 
to be a good approximation to a geodesic, and computing such paths is a standard problem in graph theory. 
In particular, Dijkstra s algorithm for the single-source-all-nearest-neighbors problem has complexity 
O(N·log N)if the graph is sparse [27] (pp. 527-532). We therefore proceed by constructing a sparse graph 
that repre­sents the surface with suf.cient accuracy. This graph then provides a rough approximation 
to a distance metric for the surface. 1Consistent with [6], .denotes half the average distance between 
each sample and its closest neighbor, and 4.is thus a reasonable estimate of the diameter of largest 
disk extending from a sample that does not contain other samples. A possible solution could be to compute 
a suitable triangulation of the sampled surface [28], but as this problem is dif.cult to solve robustly, 
and as an exact triangulation is not needed for our purpose, we choose a simpler approach that produces 
an equally acceptable result: The graph is constructed by computing the Mnearest neighbors to each sample 
with the restriction that all edges longer than 40 are discarded. Due to the aforementioned hexagonal 
sampling pattern, M= 8 works well, as all important edges are included while those violating assumption 
1 are ignored. Using a simple spatial subdivision [29] (p. 294), the graph can be constructed in a time 
which is theoretically O(N2), but essentially linear for realistic problems, up to at least 10,000 points. 
As the surface is assumed to be modeled in advance, the graph can be computed in a pre-processing step 
and subsequently reused for computing arbitrary geodesics. The resulting path will be a good guess to 
a short geodesic curve between the two points, and although this may not necessarily be theshortestsolutionglobally,itisfully 
adequateforourinteractive  2.3 Smooth interpolating curves. In addition to geodesics, it will be convenient 
to operate with smooth interpolating curves on the surfaces, similar to splines in the plane. Fortunately, 
this is easily accomplished: Given N points on the surface, a piecewise geodesic interpolating curve 
is .rst computed as already described, and then subjected to op­timization. Again, this approach works 
because the piecewise geodesic serves as a good initial guess for optimization. Just as traditional splines 
minimize curvature subject to speci.ed con­straints, our problem is to .nd an interpolating curve with 
minimal curvature as seen from the surface ,or more formally to minimize D the covariant derivative, 
0 , along the curve. dt Motivated by Da0 00 00 (t)=a(t)-ha(t)n(t)in(t) dtwhere ndenotes the local surface 
normal (the normalized gradient at a(t)), and by . drawing application. It is important to note that 
this strategy relies 001 a(t):a(t+h)-a(t)+a(t-h)-a(t) on the regularity of the sampling, that characterizes 
the point h2 repulsion technique [29], and does not necessarily generalize to the following discretization 
was chosen empirically: more random sampling populations. .  2.2 Optimizing geodesics. E= X jpi+1 -pi 
-hpi+1 -piniinij 2 + i 2 The above algorithm produces a good guess for the geodesic, jpi 1 -pi -hpi 
1 -piniinijwhich is then re.ned using constrained optimization. For ef.­ ciency, we choose to minimize 
the length of the curve: This functional can be minimized in the Npi-s subject to the N ..Z a 0 (t) .. 
 tangent plane constraints: dt hpi -piold nii= 0 T where a: TCR7>R3 denotes some parameterization of 
the curve. As the curve has to be embedded in the implicit surface, this optimization must be performed 
subject to the following con­straint: F(a(t))=0 t2T where Fis the scalar function de.ning the implicit 
surface. Simple discretization approximating by the difference quotient: 0pi+1 -pi a(ti): h with hbeing 
the average distance between the samples, works well, and the constraints are enforced using the feedback 
term, F(p)rF(p) c2(cis some constant and rFdenotes the gradient of jrF(p)jF), derived in [6] (p. 271). 
The complete algorithm for computing an approximation to the shortest geodesic between two points pand 
qlooks like this: class Curve integer size point samples[MAX SIZE] procedure Surface: Attract Point(p) 
F(p)rF(p) return p-c jrF(p)j2 procedure Surface: Compute Geodesic(pq) Curve geodesic=graph.Estimate Geodesic(pq) 
while (geodesic.Stable() == FALSE) for pi2geodesic.samplesdo pi.1+pi+1 ptmp=2 ptmp0=pi.Project To Tangent 
Plane(ptmp) p 0=Attract Point(ptmp0) i return geodesic What actually happens in the optimization process 
is quite simple: for each sample, pi, along the curve, the midpoint between the two adjacentsamplesis 
.rstprojectedtothe tangentplanethrough piand then pulled back towards to the implicit surface using the 
feedback formula. (See Kimmel and Kiryati s recent vision paper [30] for an alternative and more mathematical 
two-step approach to computing geodesics). using the Lagrange multiplier method [31] (pp. 355-356) in 
each iteration. The complete algorithm for computing a smooth curve, connecting the .xed points p1 ...pNon 
the surface, looks like this: procedure Surface: Compute Closed Spline(p1 ...pN) Curve geodesics[NJ, 
spline for i=1 to Ndo geodesics[iJ=Compute Geodesic(pipi+1) spline=Concatenate Curves(geodesics N) while 
(spline.Stable() == FALSE) spline.Optimize Lagrange() for pi2spline.samplesdo p 0=Attract Point(pi) i 
 return spline This optimization algorithm is a compromise between speed and mathematical accuracy that 
makes several simplifying assump­tions in order to be applicable in our interactive system. However, 
it has proven to be ef.cient and reliable in practical use and pro­duces smooth and visually attractive 
curves (see .gure 6 and 11). (An alternative and more elaborate approach to constrained mini­mization 
of the geodesic curvature along a curve can be found in [23] (pp. 316-317)).   2.4 Implementation. 
The algorithms presented in this section can be applied interac­tively: for surfaces represented by a 
few thousand samples, the shortest path is typically computed in the order of 5-15 seconds on an SGI 
Indy, and the subsequent optimization takes a similar amount of time. The practical performance is dramatically 
im­proved using a divide and conquer approach, allowing the user to interactively double or halve the 
sampling rate along the curve: re­ducing the number of samples and optimizing the curve at a lower resolution 
yields a much more rapid convergence to a coarse solu­tion, which can then be re.ned at a higher sampling 
frequency (i.e. a multigrid method). For further ef.ciency, the curve optimizer is continuously running 
on-line, allowing the user to draw and ma­nipulate curves while previously de.ned ones are still converging. 
In conclusion, the new approach to interactive curve-drawing presented in this section provides a high-level 
alternative to pre­vious brute-force methods. Instead of having to specify every sample point along the 
curve explicitly, the user simply clicks on a number of arbitrary key points on the implicit surface 
and selects the geodesic or spline operation. As the curves are sampled on a continuous surface and thus 
not limited to follow the edges of a polygonal mesh, they are more .exible and can, for example, be dragged 
continuously across the surface. Finally, due to the good initial approximation and the simplicity of 
the optimization procedure, the algorithms are reliable and robust.  3 Patch design. The interactive 
tools described in the previous section provide a practical solution to the problem of subdividing an 
implicit surface. However, as the resulting charts are to be used for storing textures, a good design 
strategy should support simple and ef.cient manipulation of texture maps. For this reason, it will be 
useful to impose certain restrictions on the charts. For many texturing and image processing algorithms 
(.ood .lls, anti-aliasing, etc.), it is desirable to avoid continuity prob­lems at borders between charts 
(see .gure 1), and this can be accomplished by imposing suitable boundary conditions. Fur­thermore, as 
current texture mapping hardware allow only a re­stricted amount of memory to be devoted to texture maps, 
the texture representation should be compact. Finally, parametric dis­tortion (stretching/compression 
and shear) should be minimized inside each patch to avoid wasting resolution to an inhomogeneous sampling 
rate. Aside from these purely technical aspects, perceptual concerns regarding the ease-of-use of the 
interface should be considered. It is a well know result in differential geometry that any regular surface 
can be decomposed into triangular or rectangular patches [26] (p. 272), but we have found that the interactive 
workload is greatly reduced if both types of patches are supported. Further improvement was gained by 
supplementing the geodesic curve tool with the smooth interpolating curves described in section 2.3. 
Based on these observations, the following rules were adopted for de.ning the charts: 1. The charts are 
represented as polynomial tensor product patches with parameter space corresponding exactly to the unit 
square (for triangular patches: the triangle uv 0 u+v1). 2. Apatchisde.nedbythreeorfournon-intersectingboundary 
curves (any combination of geodesics or splines) connected in a cycle. 3. For any interior angle, 0, 
between two adjoining boundary curves of a patch, 0 0. 4. The interior of a rectangular patch is homeomorphic 
to the unit square such that the four edges of the square map onto to the boundaries of the patch (a 
similar condition holds for triangular patches).  To understand the signi.cance of the third condition, 
imagine that the user has drawn four curves outlining a small rectangular patch on a large sphere and 
now wants to parameterize it before drawing additional patches. However, the small rectangle actually 
de.nes two patches on the surface: a small one, and a very large one. To be able to discard uninteresting 
patches automatically, our system enforces the third restriction so that only the small patch is considered 
when the parameterize patches operation is selected. As the idea of adding curves and patches incrementally 
is essential in our approach (see .gure 5), this feature has proven to be very convenient. The last condition 
rules out certain contrived cases, assuring that the interior of a patch does not have loops etc. As 
it is of marginal practical concern, the details are beyond the scope of this paper.  4 Deriving material 
coordinates. Once the surface has been partitioned according to the guidelines in section 3, the next 
step is to construct a uv-parameterization for each patch. The general idea in the new parameterization 
algorithm, which will be presented in this section, is to start by identifying two families of iso-parametric 
curves, corresponding to .xed uand vparameters, and subsequently reconstruct a pa­rameterization from 
these. More speci.cally, given 3 or 4 piecewise linear curves, Cui, Cvi, bounding some region Rof the 
surface (see .gure 2), the problem is to construct a mesh of iso-parametric curves that faith­fully represents 
the geometry of Rand minimizes parametric dis­tortion. As the uand vcurves are .rst estimated independently, 
and as the algorithm for rectangular patches can also be applied for the triangular case, it will be 
convenient to start by consider­ing the subproblem of deriving the set of curves corresponding to .xed 
uvalues for a rectangular patch in section 4.1. Section 4.2 will describe how to combine the two sets 
of curves into a mesh and how to derive convenient (uv)coordinates at each sample of this mesh. Section 
4.3 discusses the modi.cations needed in the case of triangular patches, and section 4.4 how a continuous 
parameterization can be reconstructed from the discrete mesh. Fi­nally, section 4.5 describes how the 
parameterizations can be used to store textures. 4.1 Computing iso-parametric (u)curves. First, the 
bounding curves Cv0 and Cv1 (see .gure 2) are resam­pled using the tools described in section 2, until 
a stable con.gu­ration with the average sample distance equal to 20(within some tolerance) is reached. 
v u Figure 2: A patch outlined by bounding curves. As the distances between adjacent iso-parametrics 
are to be less than 40everywhere to sample the surface properly, the initial guess for the required number 
of iso-parametrics is set to Length(Cv0 )Length(Cv1 ) dimu =max(). 40 40 Next, dimusamples are positioned 
uniformly along Cv0 and Cv1 respectively, and the idea is now to .nd a curve connecting each pair of 
samples. 4.1.1 Inside-outside test for ef.ciency. Although the geodesic curve algorithm could be used 
directly for this purpose, it is convenient to consider only the subset of surface samples interior to 
the speci.c patch in the discrete shortest-path algorithm from section 2.1.2. Therefore, an equivalence 
relation is imposed on the set of samples, assigning each sample to the patch to which it belongs. We 
use the following algorithm: stack=0 for p2Pdo p.state = UNDETERMINED .nd closest point qto pon bounding 
curves if (jq-pj40) if (pis on interior side of q) stack.Push(p,INSIDE)  else stack.Push(p,OUTSIDE) 
while (stack60) = p= stack.Pop() for q2p.neighbors do if (q.state == UNDETERMINED) stack.Push(q,p.state) 
 First, all samples within a distance of 40of any curve are marked. As no edge in the graph is longer 
than 40, this assures that no edge with an UNDETERMINED end point crosses a patch boundary, and the equivalence 
classes can then be computed using the last traversal. In the actual implementation, it is necessary 
to com­pensate for curvature of the surface and replace 0in the above algorithm by a slightly smaller 
number, depending on the approx­imation error introduced by assumption 1 in section 2.1. The algorithm 
has proven to work well in practice, on average reduc­ing the number of samples to be considered in the 
shortest path Figure 3: a) Boundary conditions of the .ows. b) Initial state. c) N algorithm to ,where 
Nand Mdenote the number of samples M and patches respectively, yielding a signi.cant speed-up. When the 
initial approximations for the uand vgeodesics are computed, these are optimized to give a quali.ed guess 
for the iso-parametric curves.  4.1.2 Estimating uand vcurves using 2D .ows. Using geodesics as the 
initial guess for the iso-parametrics works well if the boundary curves are geodesics and the patch is 
not too bumpy . However, in order to be able to work with more irregular patches, it is sometimes desirable 
to compute a more accurate initial approximation (as an example, irregular patches are useful for warping 
photo-textures onto patches outlined by non-geodesic boundaries, such as the spline curves from section 
2.3, see .gure 8a). To get an intuitive understanding of the problem, consider a patch with a big bump 
in the middle: because the geodesic curves used to compute the iso-parametrics try to minimize distance, 
they will cluster together near the boundary curves and never reach the top of the bump. Therefore, a 
supplementary technique, capable of producing better approximations at the tradeoff of increased After 
optimization. As it is necessary to evaluate the vector .elds at each sample point along the interior 
curves, an ef.cient mechanism for recon­ structing a continuous vector function from the discretely sampled 
.elds is needed. We use a Gaussian .lter and a spatial subdivision provided by the surface graph: each 
sample, p, along the interior curves, continuously tracks the closest vertex, w, in the graph, and computing 
time, was developed. The idea is to use two homogeneous .ows (one for each para­ metric direction), parallel 
with each set of boundary curves, to align the approximated iso-parametrics with the boundaries and thus 
distribute the curves more evenly within the patch. This can be accomplished using simple vector .eld 
theory to .nd an optimal .ow subject to appropriate boundary conditions and subsequently cuand cvat pare 
then reconstructed as a weighted average of the corresponding angles at wand its neighbors (in practice, 
it is only necessary to consider the immediately adjacent vertices in order to reconstruct the .elds 
with suf.cient accuracy). Ourexperienceis thatthesimple geodesicapproximationofthe iso-parametric curves 
is fully adequate for almost all the parame­ terization problems that occur in practice (.gure 4a illustrates 
the aligning the curves with the .ow using optimization. In order to work ef.ciently with continuous 
.ows on a com­ puter, a suitable discretization is needed. Fortunately, the discrete robustness of this 
method), as the optimization procedures to be described below have proved to be reliable and forgiving. 
The vector .eld technique is more general, but slower and should only graph representing the surface 
(section 2.1.2) can be reused for be applied in pathological cases (see .gure 4b). this purpose. We use 
the following algorithm for computing the .ows: 1. Each sample inside the patch and along the boundary 
curves is assigned an arbitrary unit reference vector in the tangent plane and two random angles, cuand 
cv. The reference vector induces a local polar coordinate system in a neigh­borhood around each sample, 
in which the angles, one for each parametric direction, represent the orientations of the .ows (as we 
are only interested in directions, it is most con­venient to perform the optimization in polar coordinates). 
 2. Impose boundary conditions: For each sample along the curves, cuand cvare initialized so each .eld 
is tangential to the corresponding pair of curves, and orthogonal to the other .eld along the remaining 
two curves (see .gure 3a). 3. Align the orientations of the .elds using simple optimization based on 
the energy function   .. 2 i jneari where cuiand cvidenote the angles at the i thvertexin the graph 
(relative to the .xed reference vector at this vertex), and cuijis the angle cuat the j th neighbor to 
the i th vertex measured relative to the reference vector at the i th vertex (similar for cvij). 4. When 
the system is stable, the set of boundary constraints requiring the .elds to be orthogonal is removed, 
while the tangent constraints are still enforced. Repeat the optimiza­tion until the system converges 
again. Given the .ows, the idea is now to optimize the curves pro­duced by the geodesic curve algorithm 
so that the work done by the .elds along each curve is minimal. This make the interior curves align themselves 
with the .ows and thus with the boundary curves, producing a better guess to the iso-parametric curves. 
We minimize the following functional: ..XX .. .. E = cui ­ cuij cvi ­ cvij Z discrete geodesic algorithm 
was too inaccurate to exploit coher­hG(a(t))a 0(t)idt ence between neighboring curves in the optimization. 
In order to T take advantage of this coherence to further improve the approxima­ where Gdenotes the vector 
.eld, using a straightforward dis-tions, it is necessary to assure that no two adjacent u-or v-curves 
cretization. are farther apart than approximately 40, as this is the requirement Figure 5: Parameterizing 
an implicit surface (106 patches). Total time for entire session: 75 minutes using an SGI Onyx. for locally 
approximating distances on the surface by their Eu­clidean counterpart. Therefore, the number of iso-parametrics 
in each direction is increased if necessary: Length(Cvi) dim0 u =max(dimumax ()) 0 .i.dimv 40 Length(Cui) 
dimv 0 =max(dimvmax ()) 0 .i.dimu 40 If additional curves are needed in order to ful.ll the sampling 
requirements, they are computed and optimized as described ear­lier in this section and added incrementally 
to the existing curves. Figure 6: Parameterizedimplicitsurface. Each mandrill image corresponds to the 
texture space of one patch. To recap, we now have a set of dimuindependent curves, run­ning from one 
side of the patch to the opposite, and from these we want to construct a regular grid in order to be 
able to apply standard .nite difference methods for minimizing parametric distortion (in order to improve 
the modularization of our system, the distor­tion minimizing techniques have been implemented as a separate 
black box ). However, although the curves were constructed to represent the patch faithfully, there is 
no guarantee that they do so. In order to supply the best possible mesh to the distortion reducing algorithms, 
we therefore choose to apply an additional optimiza­tion step dedicated to reducing irregularities in 
the distribution of the curves that might cause more general optimization techniques to fail. This intermediate 
optimization step explores the fact that although some segments of adjacent curves may be more distant 
than 40, the curves are generally homogeneously distributed, and, moreover, the samples along each curve 
are uniformly spaced. To explore this coherence, each curve is sampled uniformly so that the distance 
between any two samples is less than 20and all curves have the same number of samples (at this point, 
the set of vcurves is discarded, as only one set of curves will be needed in the remaining steps; the 
vcurves were only used to assure that a suf.cient number of ucurves was selected). The re­sulting samples 
de.ne a regular grid, which is then re.ned using simple optimization in order to maximize the distance 
between adjacent curves subject to the (stronger) condition that the struc­tural integrity of each curve 
is preserved. The implementation boils down to updating each interior point in the grid with an ap­propriately 
weighted average of its eight neighbors until a stable con.guration is reached. The resulting mesh can 
now be passed on to the minimize distortion -module. Figure 7: Matching texels at patch boundaries. 
4.2 Minimizing distortion. All our efforts have been devoted to ensuring that the mesh is regular and 
represents the patch accurately. Given such a mesh, standard optimization techniques for reducing parametric 
distor­tion can be applied, for example using the Green-Lagrange defor­mation tensor [12]. Because the 
mesh is represented as a regular grid rather than a more general graph, this .nal optimization step is 
particularly simple and robust (.gure 6 shows an example of texture coordinates computed in this process). 
4.3 Triangular patches. The algorithm for parameterizing rectangular patches can also be applied for 
triangular patches, the only difference being slightly different boundary conditions when the .ow technique 
from sec­tion 4.1.2 is utilized. The initial boundary conditions along the side corresponding to the 
hypotenuse can conveniently be set so both .elds are orthogonal to this curve. It is convenient to orient 
the parameter space of a triangular patch so its origin coincides with the vertex corresponding to the 
widest dihedral angle in order to minimize distortion. 4.4 Reconstruction. The parameterization algorithm 
produces a sampling of the sur­face with corresponding material coordinates. As each patch is represented 
by a regular grid of samples, the surface could now be reconstructed as spline patches [32, 33], but 
as existing hard­ware favors polyhedral representations, and as implicit surfaces typically are scan-line 
rendered as a set of polygons, we choose to reconstruct the surface as a triangulation (see .gure 5). 
The patches were constructed so the maximum distance between any two adjacent samples was less than 40, 
but the sampling may be unnecessarily high in some low-curvature regions. Recent progress in polygonal 
and algebraic surface modeling [34, 35, 36] provides ef.cient tools for retiling polygonal representations 
of implicit surfaces. At present, only edge .ipping [34] and adaptive sampling has been implemented in 
our experimental system, as this has produced satisfactory results, but we plan to enhance the system 
with more elaborate mesh optimization features in the near future. c d Figure 8: Summary of the patch 
parameterization step: a) User supplies boundary curves, and discrete geodesics are used as an initial 
guess for iso-parametric curves. b) The approximations are re.ned using either simple geodesics or vector 
.elds. Optimiza­tion basedoncoherencebetweenneighboringcurvesimprovesthe approximation further. c) One 
set of curves is sampled uniformly to de.ne a mesh. d) The mesh is optimized to reduce parametric distortion 
and a continuous parameterization is extrapolated. 4.5 Storing textures. The patch parameterizations 
provide natural material coordinates for the surface, but we still need an effecient strategy for repre­senting 
textures. It is emphasized that our goal is not to map a separate image onto each patch, but in lieu 
of developing more sophisticated texturing techniques in section 5, we need a way of storing the texture 
information. Eventually, our goal will be to make the network of patches and their corresponding texture 
mappings completely invisible to the artist. There are several factors to consider regarding the texture 
rep­resentation. First, the sampling rate should be suf.ciently high to avoid loss of high-frequency 
detail or visible rastering artifacts. Second, the sampling rate should be no higher than necessary to 
reduce overhead in memory consumption. Third, reconstruction should be simple and hardware supported, 
and fourth, matching texels at patch borders should be straightforward. Based on these observations and 
trade-offs, the following strategy was chosen: The textures are represented as 2D tables for easy reconstruc­tion 
and for compatibility with existing renderers and graphics hardware. The sampling rate in each parametric 
direction is cho­sen to be a power of 2 to make texel matching trivial at patch borders (see .gure 7 
and compare with .gure 1): . 2log2bdimu'4c resu = 4 . 2log2bdimv'c resv = where Erepresents the desired 
maximum sample separation, and dimuand dimvare the number of iso-parametrics for the patch. For triangular 
patches, resuand resvare chosen to be equal for simplicity. This design supports both ef.cient texture 
representa­tion and ef.cient texturing operations. Another nice property is that the resolution need 
not be the same across the surface, but can easily be adapted if certain regions require a higher sampling 
frequency. Given such a set of disjoint texture mappings covering the entire implicit surface(orwhateverpartofithasbeenparameterized),we 
can now move on to our original goal of designing new texturing algorithms. 5 Decorating Implicit Surfaces. 
Our analysis of existing 3D painting techniques in section 1.1.1 produced the following results: 1. New 
high-level texturing operations with less dependence on the users artistic talent and patience would 
be useful. 2. These operations should be performed at interactive speed, while the user is free to view 
the texture from any angle and distance. 3. A general mechanism for specifying the global position and 
extent of a texture on a surface would be convenient. 4. Texture distortion should be minimal, but not 
at the expense of introducing discontinuities, which should be be avoided at all costs.  The remainder 
of this section will describe how these objectives can be met. We propose four supplementary methods 
for decorating ob­jects easily and without introducing seams: curve drawing (see .gure 6), .ood .ll operations 
(.gure 11), tiling (.gure 9), and positioning of smaller images onto the object (.gure 10). Com­mon to 
these operations is that the resulting textures are not being mapped in the traditional sense, but are 
applied directly onto the surface while the texture mappings only serve to store the texture signal. 
This is a subtle but important difference from the tradi­tional texture mapping paradigm, as the artist 
does not have to worry about the underlying mapping functions. We believe that texture mapping is an 
unfortunate term when it comes to dec­orating complex 3D shapes, as the mathematical details of how the 
textures are represented are irrelevant for the artist. Instead, we use the term decoration , as it is 
a less technocratic and more friendly metaphor for the artistic process, our tools are aimed at. Flood-and 
pattern-.ll operations are useful for quickly and ac­curately covering larger regions of a surface with 
a color or high frequency texture. Tiling is useful for covering a surface with a more complex pattern, 
and positioning of smaller images is use­ful for unique local patterns. The last two features require 
the derivation of a change of coordinates from the source image to the destination texture coordinates, 
and these tools will be developed in section 5.2, while section 5.3 and 5.4 will present our most recent 
results and discuss implementation issues. To motivate these ideas, we note that none of the currently 
avail­able high-end 3D brush-painting applications (i.e. in the price range $15,000+ [8]) offer a consistent 
solution to even basic geodesic curve-drawing operations, that simple .ood .lls there­fore have to be 
accomplished through user-intensive brush paint­ing operations, and that these systems do not provide 
any of the aforementioned change-of-coordinates based texture placement features. 5.1 Lines and curves. 
The curve drawing algorithms from section 2 can be applied im­mediately for drawing geodesics and splines 
onto the surface. The remaining problem is how to convert these curves into texels in the texture representation. 
As the curves are sampled on the actual implicit surface and not on the corresponding triangulation, 
it is convenientto startby projecting eachcurveonto the triangulation. This approximation makes sense 
if the triangulation is a good ap­proximation to the surface, and is valid as the polygonization was 
speci.cally designed to meet this criterion (cf. section 2.1). First, the sample points along the curve 
are projected onto the triangulation by computing its intersection with the line de.ned by each sample 
and its corresponding normal vector (the exact surface normal at the sample computed from the gradient 
to the implicit function).  These points form a series of line segments, not necessarily on the triangulation. 
Each segment is then projected onto the triangulation along the plane de.ned by the two end points and 
the average of the corresponding normal vectors, forming a connected chain of points completely embedded 
in the triangulation while preserving the structural integrity of the curve. 5.2 Patches. As in the case 
of lines and curves, the previously described patch parameterization algorithm can be reused for positioning 
images onto the surface. Similar to the surface subdivision process, the user speci.es the extent of 
the image using the curve-drawing tools. When the patch is de.ned and parameterized, the remaining problem 
is here to compute a change of coordinates from the sourcepatchtexture coordinatesto thetexture spaceofthesurface. 
Again, this can be accomplished by a projection, transforming the source texture patch into a set of 
polygons aligned with the surface triangulation (see .gure 8). First, the bounding curves are projected 
onto the surface as described in section 5.1, and the destination coordinates of each Figure 9: Tiling. 
sample along the projected curve are computed using, for exam­ple, barycentric coordinates, [37] (remember 
that the destination texture coordinates are stored at each vertex of the surface trian­gulation). The 
resulting closed projected curve de.nes a set of interior polygons on the triangulation, which can conveniently 
be categorized into border polygons and interior triangles. Border polygons (highlighted in .gure 8) 
denote polygons directly adja­cent to a border segment, and can theoretically have any number of vertices. 
However, border polygons are always planar, as they are each contained in a single triangle of the surface 
triangulation. Per de.nition, interior triangles are not divided by a border and thus always remain triangles. 
5.2.1 Texture transformation. Given these two sets of polygons and the destination texture co­ordinates 
at each of their vertices, the .nal step is to compute the corresponding source coordinates at each vertex. 
Just as the destination coordinates for vertices along the boundary curves of the source patch were derived 
by projecting these points onto the destination triangulation, the source coordinates for the interior 
vertices can be computed by projecting these onto the triangu­lation of the source patch. Because the 
normal vectors at each vertex on both source and destination triangulations are those of the actual implicit 
surface rather than estimates computed from the polyhedral approximations, the projections can be performed 
safely both ways with minimal loss of precision. The .nal representation of the change of coordinates 
is thus a set of polygons embedded in the surface, where both source and destination coordinates are 
known at each vertex. Under the reasonable assumption that the coordinate change is linear within each 
polygon, an assumption shared by all existing scan-line ren­dering algorithms for polygonal surfaces, 
the actual transforma­tion of the source texture information into the surface texture representation 
can now be accomplished through a series of linear transformations, one for each polygon. However, if 
Silicon Graph­ics hardware is available, this step can be performed particularly elegantly: Because the 
polygons were designed to match the boundaries of the texture representation patches, no polygon crosses 
a patch border. Therefore, it is convenient to introduce an equivalence relation on the polygons with 
classes corresponding to the desti­nation patches. The actual transformation algorithm now looks like 
this (see .gure 8b): set texture pointer to source texture for each destination patch Mdo draw one rectangular 
polygon covering the entire texture space of Minto a frame buffer for p2M.polygons do for w2p.vertices 
do set texture parameters to (w.sourceuw.sourcev) set geometric parameters to (w.destuw.destv) draw p 
read frame buffer back into texture space of M All instructions used in this algorithm are provided by 
the Re­ality Engine texture mapping hardware, allowing it to be imple­mented ef.ciently in less than 
20 lines of code. For anti-aliasing, we use 16 times super-sampling by drawing into a frame buffer 4 
times the number of pixels in the texture representation in each parametric direction and subsequently 
reconstructing with an ap­propriate .lter [38] (chapter 14). Figure 8 shows an application of the change-of-coordinates 
al­gorithm. Destination texture coordinates are indicated by a man­drill image for every patch, and the 
source texture coordinates by a stone tiling. Notice the perfect match at the curved tile borders. Curves 
and images can be positioned arbitrarily on the surface and the underlying texture representation is 
completely invisible to the user. The total computing time for the four coordinate changes and subsequent 
transformation of the texture information was 18 seconds on an SGI Onyx. 5.3 Epilogue: dragging textures. 
As a further extension of the interactive curve drawing and image positioning techniques described in 
section 5.1 and 5.2, a feature allowing textures to be selected,dragged, rotated, and scaled freely across 
the surface has been implemented. When the texture is .rst positioned on the surface, a mesh of springs 
representing its geometry is computed, and as the element deforms as it moves across the surface, an 
optimization process is used to restore it to its rest shape subject to the constraint that the entire 
texture remains embedded in the implicit surface. At any time, the user can paste the texture onto its 
current position on the surface. These facilities have proven to be useful for .ne-tuning the position 
and extent of a texture. Unfortunately, space does not permit a detailed description of these algorithms.2 
5.4 Implementation. The new texturing operations presented in this section have been implemented on a 
Silicon Graphics Reality Engine, which is widely used in the industry that would bene.t from this work. 
2The work described in section 5.3 was done after the provisional acceptance of this paper. Our experimental 
implementation runs at comfortably interactive speed for all operations, the most complex patch parameteriza­tion/change 
of coordinates step typically taking in the order of 5 to 30 seconds per patch, depending on the sampling 
density within the patch. As all operations are performed on-line, the user is free to, for example, 
rotate the objects and perform new oper­ations while previously speci.ed ones are still being computed. 
Although the texturing algorithms were developed speci.cally for the Reality Engine, the curve drawing 
and parameterization algo­rithms do not require the use of sophisticated graphics hardware. These techniques 
have performed well on a Silicon Graphics Indy and would probably also be applicable on a fast personal 
computer. Figure 11: Pattern .ll, curve drawing, anti-aliasing. 6 Future work. The biggest limitation 
of our original implementation was that the parameter 0was speci.ed globally, causing the sampling fre­quency 
to be unnecessarily high in low curvature regions. There­fore, a simple generalization of [6] to adaptive 
sampling has been implemented (see .gure 6). Space does not permit a detailed account for these preliminary 
results, and although adaptive sam­pling has proven to be convenient, we emphasize that all algo­rithms 
presented in this paper are self-contained and have proven to perform well in practical use. Generalization 
to splines and polyhedral surfaces. Although the algorithms in this paper lend themselves especially 
elegantly to implicit surfaces, we believe that they can be applied with spline patches and polyhedral 
meshes as well. [6] outlined how the point repulsion mechanism can be generalized to parametric surfaces, 
and an algorithm for curvature adaptive point repulsion on polyhedral meshes has already been published 
by Turk [35]. As these sampling results form the theoretical foundation for our curve-drawing and patch 
parameterization algorithms, the gener­alization to other models should be straightforward, and would, 
for example, allow the texture representation for spline surfaces to be speci.ed independently of the 
patch parameterizations, and the new texturing algorithms to be applied directly in current spline based 
3D painting applications. Animation. Due to their .exibility, implicit surfaces have been used to simulate 
interesting visco-elastic effects in anima­tions since the dawn of time [39], and this application of 
implicits is widely considered more important than their use in model­ing. With simple extensions, it 
appears that the ideas in this paper could be generalized to animated skeletal models: The problem of 
making the textured patches follow the moving surface is straight­forward and to some extent covered 
in [6], and feature alignment can be enforced by specifying anchor points at key positions on the textured 
surface. The anchors could, for example, be speci.ed as the intersections between the implicit surface 
and line segments de.ned relative to speci.c limbs of the skeleton (imagine small needles sticking out 
from the skeleton). Given a skeletal model, an atlas of textured patches covering the surface, and a 
set of an­chor points, the texture could be made to track the object using Witkin &#38; Heckbert s ideas 
to keep the patches on the surface, and a generalization of Maillot et al. [12] to minimize parametric 
dis­tortion subject to the constraint that the texture coordinates remain constant at each anchor point. 
Texturing operations. There is a need for better high-level texturing operations for 3D objects; not 
only generalizations of existing 2D painting algorithms, but entirely new concepts specif­ically designed 
to meet the added challenges presented by complex shapes. As an example, we believe that the tiling operations 
out­lined in section 5 could be useful in present texturing applications: in contrast to brush painting, 
tiling does not require particular artis­tic skills, and the interactive texture positioning tools in 
section 5.2 allows the borders of the tiles to be matched perfectly on curved surfaces (see .gure 8). 
The mesh optimization algorithm (section 4.2) can easily be modi.ed so parametric distortion is not only 
minimized within each patch, but also across patch borders. This is useful for avoiding .rst order discontinuities 
between adjacent tiles. 7 Conclusion. The algorithms in this paper allow a new range of useful operations 
to be performed on implicit surfaces. In particular, they present a practical solution to the texturing 
problem, which has been one of the biggest limitations of soft objects. Paradoxally, as most of the texturing 
operations are new and currently have no counterparts in the case of parametric or polyhedral surfaces, 
it appears that implicits could have unexpected advantages in this respect. In conclusion, abandoning 
the editorial we on a last note, I hope that this work will eventually provide artists and graphics programmers 
with better tools and that it will be used for the positive and creative purposes for which it was intended. 
8 Acknowledgements. Sebastian Grassia, Carnegie Mellon University, gave me the orig­inal inspiration 
for this work [40], but unfortunately was too busy taking classes to become co-author of a paper that 
would have greatly bene.tted from his talent. I will always be indebted to Fred Brooks for a wonderful 
year in Chapel Hill and for giving me the freedom to pursue these ideas. Special thanks to Dinesh Manocha, 
Greg Turk and Peggy Wetzel for their hospitality and much appreciated help, to Paul Heckbert, Will Welch 
and Andy Witkin for immense amounts of inspiration while I was at CMU, to Pat Hanrahan, Stanford University, 
for an experience that I am very much looking forward to, and, not least, to the anonymous reviewers 
(#4: stop by any time for a free lunch!). Finally, thanks to the Fulbright Commission and USIA for sponsoring 
my studies in the United States. References [1] E. A. Bier and K. R. Sloan, Jr. Two part texture mappings. 
IEEE Computer Graphics and Applications, 6(9):40 53, September 1986. [2] Nelson L. Max and Geoff Wyvill. 
Shapes and textures for rendering coral. In N. M. Patrikalakis, editor, Scienti.c Visualization of Physical 
Phenomena (ProceedingsofCG International 91), pages 333 343.Springer-Verlag, 1991. [3] Geoff Wyvill, 
Brian Wyvill, and Craig McPheeters. Solid texturing of soft objects. In CG International 87. Tokyo, May 
1987. [4] Darwyn R. Peachey. Solid texturing of complex surfaces. In B. A. Barsky, editor, Computer Graphics 
(SIGGRAPH 85 Proceedings), volume 19, pages 279 286, July 1985. [5] Ken Perlin. An image synthesizer. 
In B. A. Barsky, editor, Computer Graphics (SIGGRAPH 85 Proceedings), volume 19, pages 287 296, July 
1985. [6] Andrew P. Witkin and Paul S. Heckbert. Using particles to sample and control implicit surfaces. 
In Andrew Glassner, editor, Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), Computer 
Graphics Proceedings, Annual Conference Series, pages 269 278. ACM SIGGRAPH, ACM Press, July 1994. ISBN 
0-89791-667-0. [7] Pat Hanrahan and Paul E. Haeberli. Direct WYSIWYG painting and texturing on 3D shapes. 
In Forest Baskett, editor, Computer Graphics (SIGGRAPH 90 Proceedings),volume 24, pages215 223,August 
1990. [8] Barbara Robertson. Fresh paint. Computer Graphics World, 17(12):28 37, December 1994. [9] PeterLitwinowiczandGavinMiller.Ef.cienttechniquesforinteractivetexture 
placement. In Andrew Glassner, editor, Proceedings of SIGGRAPH 94 (Or­lando, Florida, July 24 29, 1994), 
Computer Graphics Proceedings, Annual ConferenceSeries,pages119 122.ACMSIGGRAPH, ACMPress, July1994. 
[10] ManeeshAgrawala,AndrewC.Beers,andMarcLevoy.3dpaintingonscanned surfaces. In Proceedings 1995 Symposium 
on Interactive 3D Graphics (Mon­terey, California, April 9 12, 1995), pages 145 152. [11] Chakib Bennis, 
Jean-Marc V´ezien, G´erard Igl´esias, and Andr´e Gagalowicz. Piecewise surface .attening for non-distorted 
texture mapping. In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH 91 Proceedings),vol­ume 
25, pages 237 246, July 1991. [12] J´er ome Maillot, Hussein Yahia, and Anne Verroust. Interactive texture 
map­ping. In James T. Kajiya, editor, Computer Graphics (SIGGRAPH 93 Pro­ceedings), volume 27, pages 
27 34, August 1993. [13] M. Samek. Texture mapping and distortion in digital graphics. Visual Com­puter, 
2(5):313 20, 1986. [14] Richard Bartels, John Beatty, and Brian Barsky. An Introduction to Splines for 
Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann Publishers, Palo Alto, CA, 1987. [15] 
S. Gabriel and James T. Kajiya. Spline interpolation in curved space. State of the Art Image Synthesis, 
Course notes for SIGGRAPH 85, 1985. [16] William Welch and Andrew Witkin. Variational surface modeling. 
In Ed­win E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), volume26,pages157 166,July 
1992. [17] R. Dietz, J. Hoschek, and B. J¨uttler. An algebraic approach to curves and surfaces on the 
sphere and on other quadrics. Computer Aided Geometric Design, 10(3):211 230, August 1993. [18] Ron Kimmel, 
A. Amir, and A. M. Bruckstein. Finding shortest paths on surfaces. In Pierre-Jean Laurent, editor, Curves 
and Surfaces in Geometric Design,pages259 268.A.K.Peters, Wellesley,Massachusetts,August1994. [19] Nicholas 
M. Patrikalakis and George A. Kriezis. Representation of piecewise continuous algebraic surfaces in terms 
of B-splines. The Visual Computer, 5(6):360 374, December 1989. [20] L. L. Schumaker and C. Traas. Fitting 
scattered data on spherelike surfaces using tensor products of trigonometric and polynomial splines. 
Numerische Matematik, 60(1):129 139, 1991. [21] J. S. B. Mitchell, D. M. Mount, and C. H. Papadimitriou. 
The discrete geodesic problem. SIAM J. Comput., 16(4):647 668, 1987. [22] Karsten Opitz and Helmut Pottmann. 
Computing shortest paths on polyhedra: Applications in geometric modeling and scienti.c visualization. 
Intl. Journal of Computational Geometry and Applications, 4(2):165 178, June 1994. [23] Alan H. Barr, 
Bena Currin, Steven Gabriel, and John F. Hughes. Smooth inter­polation of orientations with angular velocity 
constraints using quaternions. In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), 
volume26,pages313 320,July 1992. [24] J. Bloomenthal. Polygonization of implicit surfaces. Computer Aided 
Geo­metric Design, 5(4):341 356,1988. [25] Brian Wyvill, Craig McPheeters, and Geoff Wyvill. Data structure 
for soft objects. The Visual Computer, 2(4):227 234, 1986. [26] Manfredo P. do Carmo. Differential Geometry 
of Curves and Surfaces. Prentice-Hall Inc., 1976. ISBN 0-13-212589-7. [27] Thomas H. Cohen, Charles E. 
Leiserson, and Ronald L. Rivest. Introduction to Algorithms. MIT Press, Cambridge, Massachusetts, 1990. 
[28] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Surface reconstruction 
from unorganized points.In Edwin E. Catmull, editor, Computer Graphics (SIGGRAPH 92 Proceedings), volume 
26, pages 71 78, July 1992. [29] Greg Turk. Generating textures for arbitrary surfaces using reaction-diffusion. 
In Thomas W. Sederberg, editor, Computer Graphics (SIGGRAPH 91 Pro­ceedings),volume25,pages289 298,July 
1991. [30] Ron Kimmel and Nahum Kiryati. Finding shortest paths on surfaces by fast global approximationand 
precise local re.nement. In SPIE Vision and Geom­etry III, pages 198 209, November 1994. [31] Richard 
E. Williamson, Richard H. Crowell, and Hale F. Trotter. Calculus of Vector Functions. Prentice-Hall Inc., 
1962 (.rst edition). ISBN 0-13-112367. [32] Charles Loop. A G1 triangular spline surface of arbitrary 
topological type. Computer Aided Geometric Design, (11):303 330,1994. [33] J¨orgPeters. C1 surface splines. 
SIAMJournal on Numerical Analysis, October 1993. [34] Paul Chew. Guaranteed quality mesh generation for 
curved surfaces. In ACM Symposium on Computational Geometry, 1993. [35] Greg Turk. Re-tiling polygonal 
surfaces. In Edwin E. Catmull, editor, Com­puter Graphics (SIGGRAPH 92 Proceedings), volume 26, pages 
55 64, July 1992. [36] William Welch and Andrew Witkin. Free Form shape design using triangu­lated surfaces. 
In Andrew Glassner, editor, Proceedings of SIGGRAPH 94 (Orlando, Florida, July 24 29, 1994), Computer 
Graphics Proceedings, An­nual Conference Series, pages 247 256. ACM SIGGRAPH, ACM Press, July 1994. ISBN 
0-89791-667-0. [37] Andrew J. Hanson. Geometry for N-dimensional graphics. In Paul Heckbert, editor, 
Graphics Gems IV, pages 149 170. Academic Press, Boston, 1994. [38] JamesD.Foley,AndriesvanDam,StevenK.Feiner,andJohnF.Hughes. 
Com­puter Graphics, Principles and Practice, Second Edition. Addison-Wesley, Reading, Massachusetts, 
1990. Overview of research to date. [39] James F. Blinn. A generalization of algebraic surface drawing. 
ACM Transac­tions on Graphics, 1(3):235 256, July 1982. [40] F. Sebastian Grassia. Using particles to 
texture implicit surfaces. Assignment for Paul Heckbert s Rendering course at CMU, unpublished, December 
1993.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218460</article_id>
		<sort_key>301</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[Implicitization using moving curves and surfaces]]></title>
		<page_from>301</page_from>
		<page_to>308</page_to>
		<doi_number>10.1145/218380.218460</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218460</url>
		<keywords>
			<kw><![CDATA[Be&acute;zier patches]]></kw>
			<kw><![CDATA[base points]]></kw>
			<kw><![CDATA[implicitization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P281616</person_id>
				<author_profile_id><![CDATA[81100400673]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Sederberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brigham Young University, 368 Clyde Building, Brigham Young University, Provo UT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP33023459</person_id>
				<author_profile_id><![CDATA[81100107527]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Falai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dept. of Mathematics, University of Science and Technology of China, Hefei 230026, Anhui, P. R. China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>94289</ref_obj_id>
				<ref_obj_pid>94252</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Buchberger, Bruno. Applications of Gr6bner Bases in Nonlinear Computational Geometry. In D. Kapur and J. L. Mundy, editors, Geometric Reasoning, pages 413-446. Elsevier Science Publisher, MIT Press, 1989.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>99914</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chionh, Eng Wee. Base Points, Resultants, and the Implicit Representation of Rational Surfaces. PhD thesis, University of Waterloo, 1990.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>139648</ref_obj_id>
				<ref_obj_pid>139643</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chionh, Eng Wee and Ronald N. Goldman. Degree, Multiplicity, and Inversion Formulas for Rational Surfaces using U-resultants. Computer Aided Geometric Design, 9:93-108, 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>151573</ref_obj_id>
				<ref_obj_pid>151570</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chionh, Eng Wee and Ronald N. Goldman. Using Multivariate Resultants to Find the Implicit Equation of a Rational Surface. The Visual Computer, 8(3):171-180, 1992.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Coolidge, Julian. A History of the Conic Sections and Quadric Surfaces. Oxford, 1945.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cox, David, John Little and Donal O'Shea. Ideals, Varieties, and Algorithms. Undergraduate Texts in Mathematics. Springer-Verlag, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Crow, Frank. The Origins of the Teapot. IEEE Computer Graphics and Applications, 7(1 ):8-19, 1987.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dixon, A. L. The Eliminant of Three Quantics in Two Independent Variables. Proceedings of London Mathematical Society, 6:46-49,473-492, 1908.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74803</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hoffmann, Christoph. Geometric and Solid Modeling: An Introduction. Morgan Kaufmann, 1989.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>160685</ref_obj_id>
				<ref_obj_pid>160673</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Hoffmann, Christoph. Implicit Curves and Surfaces in CAGD. IEEE Computer Graphics &amp; Applications, 13(1 ):79-88, 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801287</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kajiya, James. Ray Tracing Parametric Patches. Proceedings of SIGGRAPH 82 (Boston, July 26-30, 1982). In Computer Graphics, 16,3 (July 1982), 245-254.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kalkbrener, Michael. Three Contributions to Elimination Theory. PhD thesis, Johannes Kepler Universitat, Linz, Austria, 1991.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>54601</ref_obj_id>
				<ref_obj_pid>54595</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Katz, Sheldon and Thomas W. Sederberg. Genus of the Intersection Curve of Two Parametric Surface Patches. Computer Aided Geometric Design, 5:253-258, 1988.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>897800</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Krishnan, S. and Dinesh Manocha. An Efficient Surface Intersection Algorithm Based on the Lower Dimensional Formulation. Technical Report TR94-062, Department of Computer Science, University of North Carolina, 1994.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>150957</ref_obj_id>
				<ref_obj_pid>147632</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Manocha, Dinesh and John F. Canny. Algorithms for Implicitizing Rational Parametric Surfaces. Computer-Aided Geometric Design, 9:25-50, 1992.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>141587</ref_obj_id>
				<ref_obj_pid>141594</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Manocha, Dinesh and John F. Canny. The Implicit Representation of Rational Parametric Surfaces. Journal of Symbolic Computation, 13:485-510, 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Salmon, George. Modern Higher Algebra. Chelsea, New York, 5th edition, pp. 83-86, 1885.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Salmon, George. A Treatise on the Analytic Geometry of Three Dimensions. Longmans, Green and Co., London, 5th edition, p. 264, 1915.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>911263</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W. Implicit and Parametric Curves and Surfaces for Computer Aided Geometric Design. PhD thesis, Purdue University, 1983.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>19106</ref_obj_id>
				<ref_obj_pid>19101</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W. Improperly Parametrized Rational Curves. Computer Aided Geometric Design, 3:67-75, 1986.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617571</ref_obj_id>
				<ref_obj_pid>616012</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W. Techniques for Cubic Algebraic Surfaces. IEEE Computer Graphics and Applications, 10(4): 14- 26, 1990.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W., David C. Anderson, and Ronald N. Goldman. Implicit Representation of Parametric Curves and Surfaces. Computer Vision, Graphics and Image Processing, 28:72-84, 1984.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>18559</ref_obj_id>
				<ref_obj_pid>18548</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W. and Scott R. Parry. A Comparison of Curve-Curve Intersection Algorithms. Computer-Aided Design, 18:58-63, 1986.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>205721</ref_obj_id>
				<ref_obj_pid>205715</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W., Takafumi Saito, Dongxu Qi, and Krzysztof S. Klimaszewski. Curve Implicitization using Moving Lines. Computer Aided Geometric Design, 11:687-706, 1994.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Implicitization using Moving Curves and Surfaces Thomas W. Sederberg1 and Falai Chen2 Brigham Young 
University This paper presents a radically new approach to the century old problem of computing the implicit 
equation of a parametric surface. For surfaces without base points, the new method expresses the implicit 
equation in a determinant which is one fourth the size of the conventional expression based on Dixon 
s resultant. If base points do exist, previous implicitization methods either fail or become much more 
complicated, while the new method actually simpli.es. The new method is illustrated using the bicubic 
patches from Newell s teapot model. Dixon s method can successfully implicitize only 8 of those 32 patches, 
ex­pressing the implicit equation as an 18X18determinant. Thenewmethodsuccessfully implicitizes all 32ofthepatches. 
Fouroftheimplicitequationscanbewrittenas3 X3 determinants, eight can be written as 4 X4 determinants, 
and the remaining 20 implicit equations can be written using 9 X9 determinants. Categories and Subject 
Descriptors: I.3.5 [Computer Graphics]: Computational Ge­ometry and Object Modeling. General Terms: Algorithms 
Additional Key Words and Phrases: B´ezier patches, implicitization, base points. 1 INTRODUCTION a(t) 
b(t) For any 2 D parametric curve x== where a, b, d(t), yd(t)and dare polynomials, there exists an implicit 
equation f(x,y= 0, where fis also a polynomial, which de.nes exactly the same curve. For example, a circle 
can be de.ned by the parametric 1_t2t equation x=1+t22 , y=1+t2 or by the implicit equation x 2 + y 2 
_1 =0. The process of .nding the implicit equation given the parametric equations is known as implicitization. 
Implicitization of 2 D curves leads to many practical algorithms. For example, a very fast algorithm 
for computing the intersection of two 2 D curves of low degree is based on implicitization [23]. Implicitization 
reduces the problem of curve intersection to one of .nding the roots of a single polynomial. (as:t)(bs:t) 
Similarly, for any parametric surface x== d(s:t), yd(s:t), c(s:t) z= where a, b, c,and dare polynomials 
in s,t, there exists d(s:t)a polynomial implicit equation f(x,y,z=0 which de.nes the same surface. 1368 
Clyde Building, Brigham Young University, Provo UT 84602-4014. tom@byu.edu 2Permanent Address: Dept. 
of Mathematics, University of Science and Technology of China, Hefei 230026, Anhui, P. R. China Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 This version of 
the paper contains three pages of appendices that are not in the version printed in the SIGGRAPH 95 conference 
proceedings. When referencing this paper, please be careful to distinguish it from the printed paper 
if you refer to results in the appendix. The general problem of surface implicitization has been studied 
for well over a century. In 1862, Salmon [18] noted that surface im­plicitization can be performed by 
eliminating the parameters from the parametric surface equations. Presumably he had in mind us­ing Sylvester 
s dialytic method with which one could eliminate two variables from three polynomials, though the result 
generally needed to be expressed as the quotient of one determinant divided by another" [17]. In 1908, 
Dixon published a more compact re­sultant for eliminating two variables from three polynomials which 
has become the standard method for surface implicitization, at least in the absence of base points. In 
1983, Sederberg [19] resurrected Dixon s and Salmon s work in addressing the problem of how to implicitize 
surface patches. Other implicitization methods are sur­veyedin [10], andinclude onesbasedon Gr¨obnerbases[1], 
numer­ical techniques [16], and multivariate resultants [4]. To implicitize a tensor product surface 
of degree mxn, Dixon s resultant produces a 2mnx2mnmatrix whose elements are linear in x,y,z. The determinant 
of that matrix is the implicit equation. For a biquadratic surface, the matrix is 8 x8, and for a bicubic 
patch, the matrix is 18 x18. (In this paper, any statement that a determinant is the implicit equation 
of a curve or surface should be taken as shorthand for setting that determinant to zero gives the implicit 
equation".) Surface implicitization has seen limited practical use partly be­cause of the huge expressions 
involved, but also because in the event of base points (see section 4.1), things can get even more complicated. 
For example, if base points exist, Dixon s resultant is identically zero and hence fails to produce the 
implicit equa­tion. Manocha has shown that in many cases the largest non-zero minor of Dixon s determinant 
is the implicit equation, but often it includes an extraneous factor [14]. Substantial further work is 
then needed to remove the unwanted factor. Implicitization using Gr¨obner bases [6] also usually fails 
when base points occur, since the implicit equation does not belong to the ideal generated by the parametric 
equations [15], although this problem can sometimes be circumvented by introducing some auxiliary polynomials 
into the Gr¨obner system [12, 10]. Nonetheless, Gr¨obner bases are known to be very slow in implicitizing 
bicubic patches. Several other procedures have been devised to implicitize sur­faces with base points 
[2, 9, 15, 16]. We don t review those methods here, but observe that those methods are generally more 
compli­cated than Dixon s method. Furthermore, base points are not arare occurrence; most of the teapot 
patches have numerous base points. This paper presents a fundamentally new procedure for implic­itizing 
curves and surfaces in which the implicit equation can be written in much more compact form than before. 
Furthermore, in the presence of base points, the expressions actually simplify.In particular, the new 
method allows the implicit equation for a general bicubic patch to be written as a 9 x9 determinant whose 
elements are all degree two in x,y,z. If a base point exists, one row of that determinant can in general 
be replaced by degree one elements. Section 2 introduces the new strategy as it applies to curve implic­itization. 
We refer to this method as the moving curve method. Section 3 introduces the moving surface method for 
surface implic­itization. Sections 4 and 5 discuss how to implicitize general tensor product and triangular 
patches using moving surfaces. Section 6 reports on what happens when those methods are used to implicitize 
the 32 bicubic patches of the teapot. It turns out that those patches show surprising diversity in the 
number of base points. 2 CURVE IMPLICITIZATION The standard method for implicitizing a 2 D curve is 
to use Bezout s resultant [22]. For a degree nrational curve, Bezout s resultant is the determinant of 
an nxnmatrix whose elements are linear in x,y. For example, the implicit equation of the curve 22 2t+4t+53t+t+4 
x= ; y=(1 t2 +2t+3 t2 +2t+3 can be found by taking the resultant of 2 t(x_2 +t(2x_4 +(3x_5 and 2 t(y_3 
+t(2y_1 +(3y_4 Bezout s resultant for these two polynomials in tis: x_22x_y_32y_ 4 1  x_23x_5 y_33y_4 
 x_23x_ 5  2x_43x_5 y_33y_ 4 2y_13y_4  5x_10 5x_y_7 = (2 5x_y_7 _5x_2y+11 Setting (2) to zero gives 
the implicit equation for (1). Bezout s resultant is purely an algebraic device. By studying the following 
geometric interpretation of Bezout s resultant, we are led to the new implicitization algorithm. A pencil 
of lines can be described by the equation (a0x+b0y+c0 (1 _t+(a1x+b1y+c1 t=0 (3 where the equations a0x+b0y+c0 
=0and a1x+b1y+c1 =0 de.ne any two distinct lines. Given two distinct pencils, (a00x+b00y+c00 (1_t+(a10x+ 
b10 y+c10 t=0and (a01x+b01 y+c01 (1 _t+(a11 x+b11y+ c11 t=0, one line from each pencil corresponds to 
each value of t, and those two lines intersect in a point. The locus of points thus created for _1:t:1is 
a conic section, as illustrated in Figure 1. This observation is attributed to Steiner and Chasles in 
the 1830 s, though a roughly equivalent method for generating conic sections dates back to Newton [5]. 
It is easily shown that the implicit equation of this curve is  a00 x+b00 y+c00 a10x+b10y+c10 =0 (4 
a01 x+b01 y+c01 a11x+b11y+c11 This same curve can actually be de.ned using many different pairs of pencils 
of lines, and it turns out that Bezout s resultant for degree two curves is nothing more than one manifestation 
of this Figure 1: Intersection of Two Pencils of Lines fact. For example, the two rows of the Bezout 
resultant (2) can be shown to de.ne pencils of lines (5x_10 t+(5x_y_7 =0 (5 and (5x_y_7 t+(_5x_2y+11 
=0 (6 which intersect in the parametric curve (1). The idea of de.ning a conic section as the intersection 
of two pencils of lines can be generalized to curves of any degree. One such generalization is given 
in [24]. This section presents some results similar to those in [24], but using a different approach. 
The approach taken here lays the foundation for the surface implicitization method presented in section 
3. Given a degree vrational curve Q(t=(x(t,y(t,w(t, de.ne g(tto be the GCD of x(t, y(t,and w(t. We can 
write n X i Q(t=g(tP(t=g(tXit, (7 i=0 where Xi=(xi,yi,wi.If jis the degree of g(t, n=v_j,we will say 
that Q(thas jbase points. A moving line m X X.L(t:=X.Ljtj =0; Lj=(aj,bj,cj,X=(x,y,w j=0 (8 is a parametric 
family of implicitly de.ned lines, with one line corresponding to each time t.For m=1, the moving line 
is a pencil. A moving line is said to follow a rational curve if Q(t.L(t=0, (9 which means that at any 
time t, point Q(tlies on line L(t. A set of moving lines Li(t, i=0,... ,is linearly independent if there 
do not exist constants ci, i=0,... ,(not all zero) such P. that i=0 ciLi(t=0. Theorem 1. For a degree 
vcurve Q(twith jbase points, there exist at least 2m+2 +j_vlinearly independent moving lines of degree 
mwhich follow the curve. Proof: Q(t.L(t=g(tP(t.L(t=0 (10 implies P(t.L(t=0 (11 since g(t6=0. We de.ne 
n X P(t=(xi,yi,wit i , (12 i=0 m X L(t=(ai,bi,cit i , (13 i=0 n+m XX P(t.L(t=(aixj+biyj+ciwjt k (14 
k=0i+j=k The condition P(t.L(t=0 can be expressed Mb=0where M is the (n+m+1 x(3m+3 matrix 23 x0 y0 w00 
...000 6 x1 y1 w1 x0 ...000 7 67 x2 y2 w2 x1 ...000 67 6...... . . 7 6 ...... . . 7 ...... . . 67(15 
6xnynwnxn_1 ... 7 67 6 ...... . . 7 ...... . . 6...... . . 7 45 000 0 ... xn_1 yn_1 wn_1 000 0 ... xn 
yn wn and [ T b=a0 b0 c0 ...ambmcm(16 The dimension of the solution set is 3m+3 _rank(M.But rank(M:n+m+1, 
so at least 2m+2 _n=2m+2 +j_v linearly independent moving lines follow Q(t. If m=n_1, we know from theorem 
1 that there exist m+1 linearly independent moving lines Li(t.X=0where m X Li(t= Lijtj; Lij=(aij,bij,cij; 
i=0,... m(17 j=0 such that Q(t.Li(t=0,i=0,... ,m (18 Theorem 2. Select any set of m+1 linearly independent 
moving lines that follow Q(tand de.ne L00 .X ... L0m.X .... . f(X = .... . (19 .... . Lm0 .X ... Lmm.X 
Then f(X =0 is the implicit equation of Q(t. (In the case of an improperly parametrized curve [20], f(X 
=0 will actually be some power of the implicit equation, since we generally take the implicit equation 
to be an irreducible polynomial). Proof: f(X=0 is the implicit equation of Q(tif the following conditions 
are met: 1. f(Q(t=0. 2. For all Xfor which f(X=0, there exists a value of tsuch that X=lQ(twhere lis 
a scalar constant. Since we are dealing with homogeneous (projective) coordinates, this simply means 
that Xand Q(tmap to the same point in 2 D Cartesian space.  As for requirement 1, (17) and (18) can 
be written 8989 23 L00 ... L0m 1 0 6 .... . 7 t 0 6 .... . 7 . .Q(t=. (20 45 . . . .... . . Lm0 ... 
Lmm tm 0 Specializing t=Tand letting X=Q(T, (20) can be expressed 2 6 6 4 L00 .X.. . L0m.X ... . . ... 
. . ... . . Lm0 .X.. . Lmm.X 3 7 7 5 8  1 T . . . Tm 9  = 8  0 0 . . . 0 9  (21 For (21) to be 
valid, either 8  1 T . . . Tm 9  = 8  0 0 . . . 0 9  (22 or L00 .X... L0m.X .... . .... .=0 (23 
.... . Lm0 .X... Lmm.X Equation 22 never holds; hence (23) must be true, and condition 1 is satis.ed. 
Condition 2 requires that f(X6=0. We can prove that this holds because the moving lines are linearly 
independent (see the appendix in the electronic version of this paper). Therefore f(X must be a non-zero 
polynomial of degree at most n=m+1. Now consider a point X*for which f(X*=0. Take an arbitrary line ax+by+cw=0 
that contains X*.There are vroots of the equation (a,b,c.Q(t=0, but jof those roots map to the unde.ned 
point (0,0,0 , leaving nvalues of twhich map to actual intersection points between the line and Q(t. 
Each of those values of tsatisfy condition 1, that is, they map to npoints for which f(Q(t=0, and each 
of those points lies on the line (a,b,c.X=0. The equation f(X=0 de.nes an algebraic curve of degree n 
which intersects a general line in npoints. Suppose f(X*=0, but there does not exist a value of tfor 
which Q(t=lX*.This means that we have found n+1 points at which a degree nalgebraic curve intersects 
a line, a violation of Bezout s theorem. So far, we have merely examined well known curve impliciti­zation 
methods from a different angle, but yielding no signi.cant computational advantage. We are now prepared 
to venture into pro.table new territory. We de.ne a 2 D moving curve as m X j C(X; t:=fj(Xt=0 (24 j=0 
 where X=(x,y,wand fj(Xis a polynomial of degree d. Thus C(X; t=0 is a family of algebraic curves that 
vary with t. A moving curve is said to follow" a rational curve P(t= (x(t,y(t,w(tif for all values of 
t, the point P(tlies on the moving curve: m X C(P(t; t= fj(x(t,y(t,w(ttj =0 (25 j=0 For a degree ncurve 
P(t, there are at least d(d+3)(m+1) _nd 2 linearly independent moving curves of degree din Xand degree 
min tthat follow the curve P(t. This can be shown as follows. Since C(P(t; tis a degree nd+mpolynomial 
in tand the total number of coef.cients in the polynomials fj(X(j=0,1,... ,m (d+1)(d+2)(m+1) is 2 , 
condition (25) is equivalent to a system of (d+1)(d+2)(m+1) nd+m+1 linear equations with 2 unknowns. 
(d+1)(d+2)(m+1) Hence there are at least 2 _(nd+m+1 = d(d+3)(m+1) 2 _ndlinearly independent moving curves 
that fol­low P(t.If (m+1 (d2 +3d_2 :2nd, there exist at least m+1 linearly independent moving curves 
that follow P(t.For example, d=1and m=n_1 is the case in theorems 1 and 2. Theorem 3. Given m+1 moving 
curves m X Ci(X; t:=fij(X tj =0,i=0,... ,m(26 j=0 which follow P(t,we de.ne f00(X ... f0m(X .... . f(X 
= .... . (27 .... . fm0(X ... fmm(X If the degree of f(X is n_j(which implies that f(X 6=0), then f(X 
=0 is the implicit equation of P(t. Proof: Essentially the same as the proof for theorem 2. We now explore 
the possible degrees for moving curves which follow P(t. Letting d=2and m=n_21 ], we .nd that there exist 
at least m+1 linearly independent curves of degree 2 in X and degree min tthat follow the curve P(t. 
In the case where nis odd, theorem 1 assures that there will also be one moving line of degree mwhich 
follows P(t. Thus, from theorem 3, a rational curve with no base points, and of even degree, can generally 
be implicitized as the determinant of a n 2 x n 2 matrix whose elements are degree 2 in x,y,w. Likewise, 
a rational curve of odd degree and no base points can generally be implicitized as the determinant n+1 
n+1 ofan 2 x2 matrix with one linear row, and the remaining rows quadratic. We emphasize the word generally 
because theorem 3 requires f(X6=0. Under certain conditions, the determinant in (27) will vanish, even 
though the rows are linearly independent. The reason is that the rows might be linearly independent, 
but polynomially de­pendent. The following theorem shows that high order singularities can create such 
a condition. A thorough description of the geomet­ric properties of curves for which this condition occurs 
remains an open question, though it appears at present that singularities are not the complete answer. 
Theorem 4. The implicit equation of a quartic curve with no base points can be written as a 2x2 determinant. 
If the curve doesn t have a triple point, then each element of the determinant is a quadratic; otherwise 
one row is linear and one row is cubic. The rather tedious proof includes showing that if f(Xis formed 
bya 2 x2 determinant with quadratic elements, the following four statements are equivalent: 1. f(X=0. 
 2. There exists a degree one moving line that follows P(t.  3. P(t has a triple point. 4. x0 y0 w0 
0 0 0 x 1 y 1 w 1 x 0 y 0 w 0 xx 2 3 yy 2 3 ww 2 3 xx 1 2 yy 1 2 ww 1 2 = 0 (28 x 4 y 4 w 4 x 3 y 3 w 
3 0 0 0 x 4 y 4 w 4 The details of the proof are omitted here (see the appendix of the electronic version 
of the paper). However, this discussion is important because similar phenomena can occur with surfaces. 
 3 SURFACES It is convenient to de.ne a rational surface in homogeneous form: X(s,t=(X(s,t,Y(s,t,Z(s,t,W(s,t(29 
 where X(s,t,Y(s,t,Z(s,t,W(s,tare polynomials in s,t. The Cartesian coordinates of points on the surface 
are given by X(s,t Y(s,t Z(s,t x= ,y= ,z=(30 W(s,t W(s,t W(s,t Among the most common rational surfaces 
used in computer graph­ics are the tensor product patches for which, in power basis, dd 12 XX X(s,t= 
Xijs i tj ,Xij=(xij,yij,zij,wij(31 i=0 j=0 3.1 Base Points A base point is a value of (s,tfor which 
X(s,t=(0,0,0,0 .In the absence of base points, the implicit equation of a tensor product surface can 
be expressed using Dixon s resultant [8], which is the determinant of a 2d1d2 x2d1d2 matrix whose elements 
are linear in (x,y,z. For example, a biquadratic surface requires an 8 x8 determinant, and a bicubic 
surface an 18 x18 determinant. The method presented in this section expressed the implicit equation of 
a bicubic patch with no base points as a 9 x9 determinant whose elements are degree two in (x,y,z. (It 
is well known that all tensor product surfaces have multiple base points at s=1and t=1. Here we mean 
no additional base points.) Base points are of interest for two reasons. First, each simple base point 
decreases the degree of the implicit equation of the ratio­nal surface by one. The full story on the 
relationship between base points and degree becomes more complicated when considering base points with 
higher multiplicity [3, 13]. Second, if base points exist, Dixon s resultant vanishes iden­tically. To 
implicitize surfaces which contain base points, more complicated methods have been devised [2, 9, 15, 
16] such as the method of undetermined coef.cients, successive elimination, per­turbations, and customized 
resultants. In general, these methods are much more complicated in the presence of base points. By contrast, 
the implicitization approach in this paper simpli.es in the case of base points. In general, for each 
base point on a bicubic patch, one of the rows of the 9 x9 determinant can be converted from degree two 
to degree one in (x,y,z. 3.2 Moving Surfaces We de.ne a moving surface as X g(X,s,t:=hi(X/i(s,t=0 (32 
i=1 where the equations hi(X=0, i=1,... ,de.ne a collection of implicit surfaces and where the /i(s,t, 
i=1,... ,are a collection of polynomials in sand t. We will refer to the /i(s,t as the blending functions 
for the moving surface. We require the blending functions to be linearly independent and to have no non­constant 
factor common to all of them. A moving surface is said to follow a rational surface X(s,t(29) if g(X(s,t,s,t=0 
(33 When we make a statement such as so many moving surfaces exist", it is implied that those moving 
surfaces follow the parametric surface under discussion, even though we may not explicitly say so. Theorem 
5. Given a set of moving surfaces X gj(X,s,t= hji(X /i(s,t=0,j=1,... ,,(34 i=1 each of which follows 
a given rational surface X(s,t(29). De.ne h11(X ... h1 (X ... f(X = ... (35 ... h1(X ... h(X If the degree 
dof f(X is equal to the degree of the implicit equa­tion of the rational surface X(s,t,then f(X =0 is 
the implicit equation of X(s,t. Proof: f(X=0 is the implicit equation of X(s,tif the following conditions 
are met: 1. f(X6=0. 2. f(X(s,t=0. 3. For all Xfor which f(X=0, there exists a parameter pair s,t such 
that X=lX(s,twhere lis a scalar constant. Condition 1 is satis.ed by the requirement in the theorem that 
the degree of f(Xis equal to the degree of the implicit equation.  The fact that each of the moving 
surfaces follow X(s,t means that the set of equations 8923 h11(X... h1 (X /1(s,t ... . 4... 5. =0 (36 
... . h1(X... h(X /(s,t is satis.ed for X=X(s,t.If for some (s,tf(X(s,t60, =then /1(s,t=... =/(s,t=0. 
But since /1(s,t,... ,/(s,t have no common factor, there are at most a .nite number of (s,t values for 
which /1(s,t=... =/(s,t=0. Consequently if there are any (s,tpairs such that f(X(s,t6=0, the number of 
such pairs is .nite. But since f(X(s,tis a polynomial in (s,t, it is therefore identically zero and condition 
2 is met. Base points map ( blow up") to entire curves on the surface known as seam curves [15]. Some 
authors haved argued that these curves can be interpreted as lying on the implicit surface but not on 
the parametric surface since there is not a parameter value for which X=lX(s,tif Xlies on a seam curve 
[3]. We avoid that debate here, and are content to prove that condition 3 holds at least for points not 
on seam curves. Suppose then that there exists a point X *such that f(X=0 but X * =6kX(s,tfor any (s,t. 
Choose a line through X *which does not intersect any seam curves and which is not tangent to the surface. 
Take any two planes containing that line and compute their intersection with the parametric surface X(s,t, 
yielding two curves in parameter space g1(s,t=0and g2(s,t=0. Those two curves will intersect at all base 
points of the surface, and at dother (s,tparameter pairs where dis the degree of the implicit equation 
of the surface [13]. But from condition 2, those dparameter pairs map to points for which f(X=0, making 
a total of d+1 points lying on a line for which f(X=0, a contradiction of Bezout s theorem. Sections 
4 and 5 will prove that it is always possible to .nd a square matrix of moving surfaces that follow any 
given tensor product or triangular surface patch, and they present a systematic way of .nding such matrices. 
It is very dif.cult to give a rigorous proof that for any given X(s,ta matrix (35) can always be found 
so that the degree of f(Xis equal to the degree of the implicit equation of the rational surface X(s,t(and 
hence the determinant is not identically zero). However, in scores of example cases, we have never failed 
to .nd such a matrix.  3.3 Examples Theorem 5 proposes a method for implicitizing rational surfaces 
by .nding sets of moving surfaces which follow it. We here illustrate that concept with a few simple 
cases. These may seem somewhat ad hoc, but they have the advantage of being concrete numerical examples 
which are small enough to verify by hand. No explanation is given in these examples of how to .nd the 
moving surfaces; sections 4 and 5 outline a procedure for that. 3.3.1 Explicit Surface The simplest parametric 
surface to implicitize is the explicit surface x=s; y=t; z=q(s,t (37 for which the implicit equation 
is merely q(x,y_z=0. This case is so trivial, that it actually becomes a little more complicated to implicitize 
it using moving surfaces than to merely write q(x,y_ z=0. However, it serves as a simple introduction 
to moving surfaces. In this case, we can take as blending functions /1(s,t= s, /2(s,t=t, /3(s,t=1. The 
three moving surfaces, in matrix form, are 10 _xs 01 _yt=0 (38 a(x,yb(x,y_z+q(0,01 where a(x,yand b(x,yare 
chosen to satisfy a(x,yx+b(x,yy+ q(0,0 =q(x,y. These three moving surfaces clearly follow the parametric 
surface, and the determinant of the 3 x3matrix in(38) is clearly the implicit equation. 3.3.2 Cubic 
Surface As far as the authors are aware, the closest hint in the literature to anything like moving surfaces 
is the observation, dating back at least to Salmon in 1862 [18], that a degree three algebraic surface 
can be de.ned as the intersection of three bundles of planes (the classical term for what we here would 
call a moving plane with blending functions 1, s,and t). Salmon began with those three bundles of planes, 
and computed the parametric and implicit equations of the surface from them (see [21] for a more recent 
presentation). For implicitization, we work in reverse, .nding the moving planes given the parametric 
equations. Here are a set of parametric equations for a surface that we know in advance to have a degree 
three implicit equation. The parametric equations have six base points. 322 2 x=2 +2t+s+4t+4ts+2t+ts+3s 
2 23 y=_2ts_ts_2s+s_s_2t+2 222 32 z=_3ts+2t_2ts_3ts_2t_s_3s_2s 22 332 w=_t+ts+s_s+s+t_1 +t Once again, 
the moving surface blending functions are 1, s,and t. The three moving surfaces, in matrix form, are 
 xyzs y+w2y_zy+2wt=0 (39 z_y_x+2wx_y1 and the implicit equation is the determinant of the matrix. 3.3.3 
Steiner Surface 4 TENSOR PRODUCT PATCHES The canonical Steiner surface is given by parametric equations 
x=2st; y=2t; z=2s; w=s 2 +t 2 +1 This is a special case of a triangular surface patch, a general implic­itization 
procedure for which is given in section 5. We can again take /1(s,t=s, /2(s,t=t, /3(s,t=1. The three 
moving surfaces, in matrix form, are y_2zxs y_z 0 t=0 (40 xzxy_xz_2zwx 2 +yz1 22 2222 The determinant 
of the matrix, xy+xz+yz_2xyzwis indeed the implicit equation of the canonical Steiner surface.  3.3.4 
Surface of Revolution When Newell reverse-engineered his teapot in 1975 [7], rational B´ezier patches 
were not in wide use and so the surfaces of revolu­ tion were approximated using polynomial patches. 
While that ap­ proximation is well within graphical tolerance, there are advantages to reformulating 
the teapot using rational B´ezier patches. First, the rational case can exactly represent surfaces of 
revolution. Second, a rational bicubic patch can model 1800of a surface of revolution, thus cutting in 
half the number of patches used to model the rim, body, lid, and bottom. Third, and most importantly 
for current needs, an exact surface of revolution can be implicitized in a much more compact form than 
can the polynomial approximation in [7]. In this section, we implicitize a patch from the lower body 
of the teapot which has been modi.ed to exactly represent a surface of revolution. As discussed in section 
6, without that modi.cation, the implicit equation takes the form of a 9 x9 determinant whereas the implicit 
equation of the modi.ed patch can be expressed in a 2 x2 determinant. The teapot lower body is de.ned 
by rotating around the zaxis the cubic polynomial B´ezier curve with control points (2,0,9, (2,0,45 , 
(15,0,225 ,and (15,0,15 . The exact surface of revolution can be represented in rational B´ezier form 
by 83 9 (1 _s 2 3 22 33s(1 _s X(s,t=(1 _t3t(1 _t3t(1 _tt]M 3s 2(1 _s 3 s where M= 23 2:4:49:1 _2:4:49:1 
(2,0,9,1 (3 (3 (_2,0,9,1 2:4:445:1 _2:4:445:1 6(2,0,45,1 (3 (3 (_2,0,45,1 7 4145:3:4225:1 _145:3:4225:1 
5 (15,0,225,1 (3 (3 (_15,0,225,1 145:3:415:1 _145:3:415:1 (15,0,15,1 (3 (3 (_15,0,15,1 The implicit equation 
of this surface can be expressed in a 2 x2 determinant. In this case, we can take /1(s,t=1, /2(s,t=s 
and the two moving surfaces are (x+yC_B_2xC1 =0 (41 _yC(x+yC+Bs where B=618 +6720z+17000z 2 _64000/9z 
3 22 22 +1305/2y_120yz+1305/2x_120xz 22 2 C=3x+3y+1600z+6900z+1197 The determinant then gives the implicit 
equation of the surface of revolution a degree 6 polynomial. We now present a systematic method for 
computing moving sur­faces which follow a tensor product patch (31). It is most convenient to work in 
power basis. As we will see, a good choice for the /i(s,t in (32) is simply the tensor product basis: 
/i(s,t=sjt k; (42) j=0,... ,b1; k=0,... ,b2; i=k(b1 +1 +j+1, so =(b1 +1 (b2 +1. Adegree npolynomial 
in three variables has (n+1 (n+2 (n+ 3 /6 coef.cients. Thus, if all polynomials hi(Xare degree n, thereare 
atotal of (n+1 (n+2 (n+3 /6 coef.cients. We can determine T, a lower bound on the number of linearly 
indepen­dent families of moving surfaces g(X,s,tthat follow X(s,tby generating a set of linear equations 
as we did for the curve case in (15). For the surface case, the identity in (33) can be satis.ed by solving 
a set of (nd1 +b1 +1 x(nd2 +b2 +1 linear homogeneous equations in (n+1 (n+2 (n+3 /6 unknowns. Thus, (b1 
+1 (b2 +1 (n+1 (n+2 (n+3 T= 6 _(nd1 +b1 +1 (nd2 +b2 +1 (43) If we can .nd values of n, b1,and b2 so 
that T:, there will be enough hji(Xto .ll a square matrix and, if the conditions in theorem 5 are met, 
the determinant of that matrix will be the implicit equation of P(s,t. Two cases turn out to exactly 
give T=: moving planes, and moving quadrics. Choosing n=1 (moving planes), b1 =2d1 _1and b2 =d2 _1 yields 
T==2d1d2. With this choice, the implicit equation of a bicubic patch with no base points occurs as the 
determinant of an 18 x18 matrix. It can be shown that Dixon s resultant is a special case of this implicitization 
method using moving planes. If we choose n=2 (the moving algebraic surfaces are quadrics), b1 =d1 _1and 
b2 =d2 _1, we then have at least T=/=d1d2 linearly independent moving surfaces. This means, for example, 
that a bicubic patch with no base points can generally be implicit­ized in the form of a 9x9 determinant 
whose elements are quadratic in x,y,z. A biquadratic patch with no base points can generally be implicitized 
using a 4 x4 matrix with quadratic elements. We stress the word generally. For arbitrarily chosen control 
points, experience has shown that the conditions in theorem 5 are always satis.ed. As will be seen in 
the teapot patches, however, when control points are placed in some coherent fashion, the likeli­hood 
of singularities increases and the conditions in theorem 5 may no longer be met. Section 7 comments on 
this in more detail. 4.1 Base Points For n=1, b1 =d1 _1and b2 =d2 _1, we .nd that T=0. Note that Tin 
(43) is a lower bound on the number of linearly independent moving surfaces; the actual number might 
be higher, depending on the rank of the (nd1 +b1 +1 x(nd2 +b2 +1 matrix. Here we show that in the presence 
of base points, the rank does indeed drop. Theorem 6. If P(s,thas pdistinct base points in general position 
(as de.ned in the proof found in the appendix of the electronic version), there exist at least plinearly 
independent moving planes whose blending functions are given by (42) with b1 =d1 _1 and b2 =d2 _1. Proof: 
This is a brief sketch of the proof. The complete proof can be found in the appendix of the electronic 
version of the paper. As noted, the identity in (33) can be satis.ed by solving a set of 4d1d2 homogeneous 
linear equations in 4d1d2 unknowns. The way we generated those equations before was to simply expand 
(33), producing a polynomial of degree 2d1 _1in sand degree 2d2 _1 in t. This polynomial has 4d1d2 terms, 
and the 4d1d2 equations are created by setting the coef.cient of each of those terms equal to zero. We 
could create an equally valid set of 4d1d2 equations by choosing 4d1d2 different parameter pairs (sj,tj, 
j=1,... ,4d1d2 and taking the equations to be d1d2 X hi(X((sj,tj/i(sj,tj=0,j=1,... ,4d1d2 i=1 However, 
if we take pof those (sj,tjto be base points, those p equations will be identically zero and the rank 
of the matrix will diminish by p. Hence, it is possible to .nd at least pmoving planes. This means that 
for b1 =d1 _1and b2 =d2 _1, we can .nd pmoving planes and (at least) d1d2 _pmoving quadrics that follow 
P(s,t. If the determinant of the d1d2 xd1d2 matrix containing those pmoving planes and d1d2 _pmoving 
quadrics is not identically zero, it is the implicit equation. This discussion on base points has dealt 
with distinct base points. One might be tempted to postulate that base points with multiplicity greater 
than one [13] would always free up moving planes equal in number to the total base point multiplicity. 
While this happens in many cases (such as with the teapot), it is not always so. A more detailed analysis 
of this question must await a future paper. What if more than d1d2 base points occur, since then there 
are no more quadratic rows left to convert to linear rows? Again, a complete answer to this question 
will be left for later. However, preliminary tests suggest that the size of the matrix can continue to 
shrink, usually allowing blending functions with b1 :d1 _2, and/or b2 :d2 _2. The example in section 
3.3.4 is such a case, involving 12 base points. We note that the surface of revolution can also be implicitized 
as a 4 x4 determinant with two linear rows and two quadratic rows. In that case, b1 =b2 =1. 5 TRIANGULAR 
PATCHES By a triangular surface patch, we mean one whose parametric equa­tions are of pure degree d: 
X X(s,t= Xijs i tj (44 i+j.d In this case, the right choice for moving surface blending functions are 
the =d(d+1 /2 monomials in s,tof total degree .d: /i(s,t=sjt k ,j=0,... ,d_1; k=0,... ,d_j_1; (45 j_1 
with i=j(d_2 +k+1. As in the tensor product case, if all polynomials hi(Xare degree n, there are a total 
of (n+1 (n+2 (n+3 /6 coef.cients. The identity in (33) can be satis.ed by solving a set of(nd+d(nd+ d+1 
/2 linear homogeneousequations in (n+1 (n+2 (n+3 /6 unknowns. Thus, the number of degree nmoving surfaces 
is at least T=d(d+1 (n+1 (n+2 (n+3 /12 _(nd+d(nd+d+1 /2 (46) Letting n=1, we .nd that there are at least 
dmoving planes. Letting n=2, there are at least (d2 +7d/2 moving quadrics. However, 4dof those moving 
quadrics can be created from the moving planes as follows. Let a moving plane be given by (32). Then 
 X (c1x+c2y+c3z+c4whi(X/i(s,t=0 i=1 gives a moving quadric that also follows the surface. Hence, for 
each moving plane, there exists four moving quadrics, and there are only (d2 +7d/2_4d=(d2 _d/2 moving 
quadrics which cannot be created from moving planes. Hence, the matrix in (35) can just exactly be .lled 
with dlinear rows and (d2 _d/2 quadratic rows. If the determinant does not vanish, f(Xis degree d2 as 
expected. The Steiner surface example in section 3.3.3 is an application of this method, for d=2. The 
base point discussion in section 4.1 applies also to triangular patches; each simple base point will 
generally allow a quadratic row to be converted to a linear row. The cubic surface example in section 
3.3.2 illustrates what can happen when several (6 in this case) base points occur. 6 THE TEAPOT The 
32 bicubic patches de.ning Newells teapot [7] provide a sur­prisingly diverse set of tests for moving 
surface implicitization. The teapot patches fall into ten groups: rim, upper body, lower body, upper 
handle, lower handle, upper spout, lower spout, upper lid, lower lid, and bottom. All patches in a given 
group are simple linear transformations of one another, so their implicit equations are similar. In every 
case, it is possible to implicitize these patches using 9 x9 determinants with tensor product blending 
functions (42) for which b1 =b2 =2. The 16 patches in the rim, lower lid, upper body and lower body all 
have degree nine implicit equations (all having nine base points), and their implicit equations can each 
be expressed as 9 x9 determinants with nine linear rows. Those nine moving planes can be found by solving 
a set of linear equations as discussed in section 4. The four patches in the upper lid have .ve base 
points (degree 13 implicit equations). The determinant has .ve moving planes and four moving quadratics. 
The four bottom patches have three base points (degree 15 implicit equations), and their determinant 
has three moving planes and six moving quadratics. All four spout patches have no base points (degree 
18 implicit equations), and their determinant has nine moving quadratics. The four handle patches provided 
a surprise. These patches have no base points (degree 18 implicit equations). However, the determinant 
has three moving planes, three moving quadratics, and threemovingcubics! Thisphenomenonisreminiscentofthedegree 
four planar curve with a triple point. It turns out that the rim s implicit equation can also be expressed 
as a 3 x3 determinant using 1 =1, 2 =s,3 =s 2.In this case, there are two moving quartics and one moving 
plane. This discovery was made purely by trial and error, but the search was motivated by the fact that 
for this surface, zis a quadratic function of sonly, so we immediately knew there exists a moving plane 
with 1 =1, 2 =s,3 =s 2. The two moving quartics were pure serendipity. Similarly, the upper lid s implicit 
equation can be written as a 4x4 determinant with three moving quartics and one moving plane. The blending 
functions are 1 =1, 2 =s,3 =s 2, 4 =s 3. Finally, the four bottom patches can be implicitized using a 
4 x4 determinant with one moving plane, one moving quartic, and two moving quintics. The blending functions 
are 1 =1, 2 =s, 23 3 =s,4 =s.  7 DISCUSSION This paper has proven several theorems and provided empirical 
support which suggests that the method of moving surfaces is a comprehensive solution to the problem 
of surface implicitization, and the resulting expressions for the implicit equation are much more compact 
than those obtained with previous methods. This work has largely been an adventure in experimental math­ematics. 
The basic notion of moving surfaces was arrived at using pencil and paper, as was the realization that 
moving surfaces can always be found for tensor product and triangular patches such that, if the determinant 
formed by them (35) does not vanish, it must be the implicit equation. While we have not yet succeeded 
in theoretically showing that a non-vanishing determinant always exists, empirical substantiation has 
been provided using computer algebra. We have tried numerous examples with randomly chosen control points 
(assuring that the surface has no base points or high order singularities), and the blending functions 
in sections 4.1 and 5 have always worked. Thus, we conjecture that for randomly chosen control points, 
the methods in sections 4.1 and 5 are robust. We have also run numerous test cases in which we generated 
surfaces with randomly chosen simple base points, and have found no counterexample to the conjecture 
that in randomly chosen cases it always works to trade one quadratic row for a linear row (until there 
are more base points than quadratic rows), thereby reducing the degree of the surface by one. Based on 
our experience, an automatic algorithm for implicitiz­ing bicubic patches (for example) would begin by 
determining the degree of the implicit equation. This can be done very quickly by .ring two skew rays 
into it [11] and checking at how many unique (s,tpairs they each intersect the surface. This is the degree. 
If the degree is 18, compute how many moving quadrics exist. If there are exactly nine, they will .ll 
a 9 x9 determinant which de.nes the implicit equation. If there are more than nine, search for moving 
planes and moving cubics to .ll the determinant. If the degree is be­tween 9 and 18, search for rmoving 
planes and smoving quadrics such that r+2s=degree. If the degree is less than 9, try blending functions 
(42) with b1 =b2 =1 Whether or not a non-zero determinant can always be found which satis.es the conditions 
in theorem 5 remains an open ques­tion. All of the hundreds of cases we have studied thus far have yielded 
a non-zero determinant, though at present we have no proof that this is always so. We have observed that 
base points with multiplicity greater than one can produce surprising results. In one example, a case 
was studied involving a base point that was a common double point on the curves x(s,t=y(s,t=z(s,t= w(s,t=0 
(thus a base point of multiplicity four [13]). The antic­ipation was that four of the rows on the standard 
9 x9 determinant could convert from quadratic to linear. In fact, only three moving planes exist! Nonetheless, 
we succeeded in .nding a solution in­volving an 8 x8 determinant with six moving quadrics and two moving 
planes. We note that moving surfaces can also solve the inversion prob­lem (given a point on the surface, 
compute the corresponding pa­rameter values). Acknowledgements The .rst author formulated the central 
idea in this paper while driving to SIGGRAPH 91. The following year, Takafumi Saito and Wang Guojin provided 
helpful discus­sions while working as visiting scholars at Brigham Young Uni­versity. Ron Goldman contributed 
much valuable feedback, and painstakingly edited several drafts of the manuscript. Thanks also to Malcolm 
Sabin, Dinesh Manocha and David Cox, along with .ve conscientious reviewers, for their very careful readings 
and helpful suggestions.   REFERENCES [1] Buchberger, Bruno. Applications of Gr¨obner Bases in Non­linear 
Computational Geometry. In D. Kapur and J. L. Mundy, editors, Geometric Reasoning, pages 413 446. Elsevier 
Sci­ ence Publisher, MIT Press, 1989. [2] Chionh, Eng Wee. Base Points, Resultants, and the Implicit 
Representation of Rational Surfaces. PhD thesis, University of Waterloo, 1990. [3] Chionh, Eng Wee and 
Ronald N. Goldman. Degree, Multi­plicity, and Inversion Formulas for Rational Surfaces using U-resultants. 
Computer Aided Geometric Design, 9:93 108, 1992. [4] Chionh, Eng Wee and Ronald N. Goldman. Using Multivariate 
Resultants to Find the Implicit Equation of a Rational Surface. The Visual Computer, 8(3):171 180, 1992. 
[5] Coolidge, Julian. A History of the Conic Sections and Quadric Surfaces. Oxford, 1945. [6] Cox, David, 
John Little and Donal O Shea. Ideals, Vari­eties, and Algorithms. Undergraduate Texts in Mathematics. 
Springer-Verlag, 1992. [7] Crow, Frank. The Origins of the Teapot. IEEE Computer Graphics and Applications, 
7(1):8 19, 1987. [8] Dixon, A. L. The Eliminant of Three Quantics in Two In­dependent Variables. Proceedings 
of London Mathematical Society, 6:46 49, 473 492, 1908. [9] Hoffmann, Christoph. Geometric and Solid 
Modeling: An Introduction. Morgan Kaufmann, 1989. [10] Hoffmann, Christoph. Implicit Curves and Surfaces 
in CAGD. IEEE Computer Graphics &#38; Applications,13(1):79 88, 1993. [11] Kajiya, James. Ray Tracing 
Parametric Patches. Proceedings of SIGGRAPH 82 (Boston, July 26 30, 1982). In Computer Graphics, 16,3 
(July 1982), 245 254. [12] Kalkbrener, Michael. ThreeContributionstoElimination The­ory. PhD thesis, 
Johannes Kepler Universitat, Linz, Austria, 1991. [13] Katz, Sheldon and Thomas W. Sederberg. Genus of 
the Inter­section Curve of Two Parametric Surface Patches. Computer Aided Geometric Design, 5:253 258, 
1988. [14] Krishnan, S. and Dinesh Manocha. An Ef.cient Surface Inter­section Algorithm Based on the 
Lower Dimensional Formu­lation. Technical Report TR94-062, Department of Computer Science, University 
of North Carolina, 1994. [15] Manocha, Dinesh and John F. Canny. Algorithms for Implic­itizing Rational 
Parametric Surfaces. Computer-Aided Geo­metric Design, 9:25 50, 1992. [16] Manocha, Dinesh and John F. 
Canny. The Implicit Represen­tation of Rational Parametric Surfaces. Journal of Symbolic Computation, 
13:485 510, 1992. [17] Salmon, George. Modern Higher Algebra. Chelsea, New York, 5th edition, pp. 83 
86, 1885. [18] Salmon, George. A Treatise on the Analytic Geometry of Three Dimensions. Longmans, Green 
and Co., London, 5th edition, p. 264, 1915. [19] Sederberg, Thomas W. Implicit and Parametric Curves 
and Surfaces for Computer Aided Geometric Design. PhD thesis, Purdue University, 1983. [20] Sederberg, 
Thomas W. Improperly Parametrized Rational Curves. Computer Aided Geometric Design, 3:67 75, 1986. [21] 
Sederberg, Thomas W. Techniques for Cubic Algebraic Sur­faces. IEEE Computer Graphics and Applications, 
10(4):14 26, 1990. [22] Sederberg, Thomas W., David C. Anderson, and Ronald N. Goldman. Implicit Representation 
of Parametric Curves and Surfaces. Computer Vision, Graphics and Image Processing, 28:72 84, 1984. [23] 
Sederberg, Thomas W. and Scott R. Parry. A Comparison of Curve-Curve Intersection Algorithms. Computer-Aided 
De­sign, 18:58 63, 1986. [24] Sederberg, Thomas W., Takafumi Saito, Dongxu Qi, and Krzysztof S. Klimaszewski. 
Curve Implicitization using Mov­ing Lines. Computer Aided Geometric Design, 11:687 706, 1994.  Appendix 
This appendix contains additional details for the proofs of theorems 2, 4 and6. Proof of Theorem 4. In 
order to show that f(X in (19) is not identically zero, we prove the following facts: 1. rank(M=2n,where 
Mis the matrix de.ned in (15). 2. Let L0 i(t.X, i=0,1,... ,mbe another set of linearly independent moving 
lines and de.ne  L0 .X ... L0 .X00 0m .... . 0 f(X = .... . .... . 0 0 L0m.X ... Lmm.X then there exists 
a non-zero constant csuch that 0 f(X =c.f(X 3. f(X 6=0. 1. We .rst assume x(tand w(tdo not have a common 
factor. Denote by M1 the matrix obtained by deleting from Mcolumns 2,5,... ,3n_1. The determinant of 
M1 is none other than the resultant of x(tand w(t, hence is nonzero. Therefore rank(M = rank(M1 =2n. 
If x(tand w(tdo have a common factor, there exist constants a(a60 and jsuch that ax(t+jy(tand w(t =are 
coprime, since curve P(t=(x(t,y(t,w(tdoesn t have a base point. Now consider curve P0(t=(ax(t+jy(t,y(t,w(t.In 
this case, the matrix Min (15) is the 2nx3nmatrix M0 = 00 1 xy0 w0 0 00 B xy1 w1 xy0 w0 C 10 B C B .. 
0 .. C B . x1 y1 w1. C 0 B C xy0 w0 0 B C B . C B 0 . 0 C xynwn . xy1 w1 n 1 B C B C 0 .  B . C xnynwn 
. A . . 0 . xnynwn where x 0 i =axi+jyi, i=0,1,... ,n. Since ax(t+jy(tand w(tdo not have common factors, 
rank(M0 =2n. By taking column reductions to matrix M0we have rank(M=rank(M0 =2n. 2. Let m X 00j0 000 
Li =Lijt; Lij =(aij,bij,cij; i=0,1,... ,m j=0 From statement 1 we know there are exactly nindependent 
moving lines which follow P(t. Hence there exist constants dijsuch that m X (Li0 0,... ,Lim0 = dij(Lj0,... 
,Ljm,i=0,1,... ,m j=0 or, inmatrixform, L0 =D.L, where L0 =(L ij0 , L=(Lij, D=(dij. Since Li(t.X, i=0,1,... 
,mand L0 i(t.X, i=0,1,... ,m are two sets of independent moving lines, we have rank(L0= rank(L= m+1, 
hence rank(D=m+1, i.e. Dis an invertible matrix. On the other hand, we also have f0 (X=det(D.f(X Since 
det(D60, statement 2 is proved. = 3. This statement follows immediately from statement 2 and the fact 
that Bezout s resultant does not vanish. But we can also prove it without using that result. First we 
assume x(tand w(tdo not have a common factor. We can solve equations Mb=0 in the following way: Take 
bi, i=0,1,... ,n_1 as free parameters and solve for ai, i=0,1,... ,n_1and ci, i=0,1,... ,n_1. Each given 
set of parameters bigives one unique solution since rank(M1 =2n.If we choose parameters (b0,... ,bn_1 
to be ei(creating an ndimen­sional vector with all components being 0 except ithelement being 1), i=1,2,... 
,n, respectively, then we get nlinearly independent moving lines. Furthermore, if we form determinant 
(19) from these moving lines, the coef.cient for monomial term y nof f(Xis 1, hence f(Xis not identically 
zero. Now suppose x(tand w(tdo have a common factor,then there exist constants a(a60 and jsuch that ax(t+jy(t 
=and w(t do not have a common factor. Similarly as in the proof of statement 1 we consider curve P0(t:=(ax(t+jy(t,y(t,w(t.To 
.nd moving lines which follow this curve, we need to solve the linear equation M0 .b0 =0, where M0is 
de.ned as before and 0000 000 T b=(a0,b0,c0,...,an_1,bn_1,cn_1 ,i=0,1,... ,n_1 Since ax(t+jy(tand w(tdo 
not have common factor, the determinant corresponding to (19) formed by the moving lines of curve P0(t(denote 
it by g(x,y) doesn t vanish, thus is the implicit equation of curve P0(t. On the other hand, noticing 
matrices Mand M0have the rela­tionship M0 =M.B, where B=diag(C,C,... ,C, and Cis a 3 x3matrix: a00 C=j10 
, 001 we know the solutions of M.b=0and M0 .b0 =0have the relationship b=B.b0 , from which we can easily 
obtain the determinant formed by the moving lines of curve P(tis just g(ax+jy,y, hence is non­zero. This 
complete the proof of theorem 2.  Proof of Theorem 4: 1 .2.  Suppose the fij(X(i,j=0,1 in ( 27) are 
all quadratics. If f(X=0, then f00(Xf11(X =f10(Xf01(X. We show that fij(X(i,j=0,1 all can be factored 
into linears. The proof is by contradiction. Suppose for example f00(Xis prime, then there exists a con­stant 
csuch that f10(X=cf00(Xor f01(X=cf00(X.If f10(X=cf00(X,then f11(X=cf01(X, this contradicts the fact that 
the coef.cients of two rows are linearly indepen­dent. If f01(X=cf00(X,then f11(X=cf10(X.But fi0(P(t+tfi1(P(t=0, 
i=0,1, so fi0(P(t=0, means that P(tis in fact a degree two curve and this contradicts the fact that P(tdoesn 
t have base point. Now let f00(X=l0(Xl1(Xand f11(X=l2(Xl3(X, so f10(X=l0(Xl2(Xand f01(X=l1(Xl3(X. Hence 
we have l0(P(t+tl3(P(t=0. that is, there exists a moving line which follows P(t. Conversely, if there 
exists a moving line L(X; twhich follows P(t, then every moving quadratic which follows P(tmust be L(X; 
tscaled by a linear function in xand y. This is so because if the moving line and the moving quadratic 
were to form a 2 x2 determinant which is not identically zero, it would be the implicit equation according 
to theorem 3. But that expression would be cubic in x, yand the true implicit equation is a quartic because 
there is no base point. Hence f(X=0. 24 Suppose moving line L(X; t=(a0x+b0y+c0w+t(a1x+ b1y+c1wfollows 
P(t,so L(P(t; t=0. This is equivalent to a system of homogeneous linear equations with a0,b0,c0,a1,b1 
and c1 being unknowns. It is easy to know that the coef.cient matrix is none other than the matrix in 
(28). Hence the equations have non-zero solutions if and only if (28) holds. 23 Suppose P(thas a triple 
point T.Let L0(X=0 be the line which passes through Tand P(0 ,and L1(X=0bethe linewhich passes through 
Tand P(1 .De.ne L(X; t=(1 _tcL0(X+ tL1(X. We prove that there exists a scalar cfor which L(X; t follows 
P(t. For this, the intersection of line L(X; twith curve P(tcan be expressed as the degree .ve polynomial 
L(P(t; t. This polynomial has zeros at the three triple point parameters, as well as zeros at t=0and 
t=1. We can choose cso that L(P(t; t=0 for one other value of t, which makes it identically zero, and 
hence L(X; tfollows P(t. Conversely, if there exists a pencil of lines which follows a quartic curve, 
the pencil axis must lie at a triple point on the curve, because as tsweeps monotonically from _1to +1, 
L(X; t rotates monotonically 1800about a .xed axis A. However, if L(X; twere to intersect P(tat more 
than one point other than A, then as tsweeps monotonically from _1to +1, the line A P(t would not rotate 
monotonically. The equivalence of the above four statments shows that if the quartic curve doesn t have 
a triple point then the implicit equation can be formed by two quadratic rows, and if the quartic curve 
does have a triple point we can .nd a moving line following it. If we multiply the moving line by a degree 
two polynomial in x,y,w(that has six coef.cients), we see that there is a six parameter family of moving 
cubics that are degree one in twhich also follow the quartic curve. In the following we prove we can 
also .nd a moving cubic which follows the curve and which is not just a quadratic scale of the moving 
line. To do this, we only need to show there exists at least seven moving cubics. Suppose the quartic 
curve P(thas a triple point T=P(t1 = Hence we only have thirteen equations, but there are twenty un­knowns, 
so there exist at least seven moving cubics which follow the quartic curve.  Proof of Theorem 6: In 
the theorem, we impose the require­ment that the pbasepointsarein generalposition". Herewede.ne what 
that means. Given a set of points (si,ti, i=1,... ,k,de.ne 2d_12d_1 vi=(1,... ,si 1,ti,... ,tisi 1 ,... 
2d_12d_12d_1 ... ,t2,... ,t2 s1,i=1,... ,k i ii The points are in general position if the vectors vi, 
i=1,... ,k, are linearly independent. We prove the following facts: 1. If (si,ti, i=1,... ,k(k:d:=4d1d2) 
are in general position, then there exist additional points (si,ti, i=k+ 1,... ,dsuch that (si,ti, i=1,... 
,dare also in general position, that is, TT TT M=(v1 ,v2 ,... ,vd has full rank. 2. Let d1d2 X h(s,t:=hi(X(s,t/i(s,t, 
i=1 where hi(Xare linear functions in x,y,zand /i(s,tare blending functions de.ned in (42). If (si,ti, 
i=1,... ,d are in general position, then h(s,t=0iff h(si,ti=0,i=1,... ,d 3. If P(s,thas pdistict base 
points and these base points are in general position, then there are at least pmoving planes which follow 
P(s,t. 1. We only need to prove that there exists (sk+1,tk+1 such that (si,ti, i=1,... ,k+1 are in general 
position, the general case follows from induction. By assumption, (si,ti, i=1,... ,kare in general position, 
that is v1,... ,vkare linearly independent. Therefore there exists .. 1 2 ... k a kxksubmatrix of M,say 
M1 =M such j1 j2 ... jk that det(M1 60. Now de.ne = 0 T TTT M=(v1 ,... ,vk,v, 2d_12d_12d_12d_1 1 122 
 where v=(1,... ,s,t,... ,ts,...... ,t,... ,t 1 s 2d_1 , and consider a (k+1 x(k+1 submatrix of M0 , 
.. 00 1 2 ... kk+1 M=M, 1 j1 j2 ... jk l here l6ji, i=1,... ,k. Because det(M1 =0, det(M1 0is a non­ 
=6 Without loss of generality, we assume that t1 P(t2 =P(t3. zero polynomial in sand t(denoted by f(s,t). 
Hence there exists =6 t3 (if some of them are equal the discussion will involve deriva-some (s,t=(sk+1,tk+1 
such that det(M10 t2 =6 tives). To .nd the moving cubics C(X; t:=f0(X+tf1(X which follows the curve 
P(t, we just substitute the equation of P(tinto C(X; tand get a degree thirteen polynomial g(t,then choose 
fourteen different parameters s1,s2,... ,s14 and obtain four­teen equations g(si=0, i=1,2,... ,14. Speci.cally 
we can choose three of the parameters be t1, t2 and t3 and get three equations f0(T+tif1(T=0, i=0,1,2. 
These three equations are linearly dependent because they are equivalent to f0(T=f1(T=0. = f(sk+1,tk+1 
=6 0. Thus M1 0has full rank, i.e., (si,ti, i=1,... ,k+1are in general position. 2. Noticing h(s,tis 
a tensor product polynomial of degree 2d1 _1in sand degree 2d2 _1in t, we can rewrite h(s,tas 2d_12d_1 
12 XX h(s,t= hijs i tj i=0 j=0 If h(s,t=0, we certainly have h(si,ti=0, i=1,... ,d. Conversely, if h(si,ti=0, 
i=1,... ,d,we get M.b=0, where Mis de.ne as before and b=(h00,... ,h2d1 _1:0,h01 ,... ,h2d_1:1 ,... ,h0:2d_1,... 
,h2d_1:2d_1 .Since Mis an invert­ 1 212 ible matrix we must have b=0, thus h(s,t=0. 3. Let (si,ti, i=1,... 
,pbe the pbase points of P(s,tand these base points are in general position. According to statement 1, 
we can .nd pairs (si,ti, i=p+1,... ,dsuch that (si,ti, i=1,... ,dare in general position. Therefore, 
h(s,t=0is equivalent to (si,ti=0, i=1,... ,dby statement 2. But since (si,ti, i=1,... ,pare base points 
of P(s,t, equa­tions h(si,ti=0, i=1,... ,pare identities. Hence h(s,t=0 has at least psolutions, i.e. 
there are at least pmoving planes which follow P(s,t.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218462</article_id>
		<sort_key>309</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Polygonization of non-manifold implicit surfaces]]></title>
		<page_from>309</page_from>
		<page_to>316</page_to>
		<doi_number>10.1145/218380.218462</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218462</url>
		<keywords>
			<kw><![CDATA[implicit surface]]></kw>
			<kw><![CDATA[non-manifold]]></kw>
			<kw><![CDATA[polygonization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P150698</person_id>
				<author_profile_id><![CDATA[81100193431]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jules]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bloomenthal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, The University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39076164</person_id>
				<author_profile_id><![CDATA[81332498230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Keith]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferguson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, The University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>89603</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Allgower and K. Georg, Numerical Continuation Methods, an Introduction, Springer-Verlag, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>133689</ref_obj_id>
				<ref_obj_pid>133687</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. Bajaj, Smface Fitting with Implicit Algebraic Sulface Patches, in Topics in Surface Modeling, H. Hagen. ed., SIAM Publications, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>55285</ref_obj_id>
				<ref_obj_pid>55279</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[J. Bloomenthal, Polygonization of Implicit Sulfaces, Computer Aided Geometric Design, Nov. 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. B loomenthal and K. Ferguson, Polygonization of Non- Manifold Sulfaces, Research Rep. 94-541-10, Dept. of Computer Science, The University of Calgary, June 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>61954</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G. Farin, Curves and Sulfaces for Computer Aided Geometric Design, a Practical Guide, Academic Press, New York 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>21270</ref_obj_id>
				<ref_obj_pid>21266</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A. Koide, A. Doi, and K. Kajioka, Polyhedral Approximation Approach to Molecular Orbital Graphics, Journal of Molecular Graphics 4, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>60949</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. M~intyl~i, An Introduction to Solid Modeling, Computer Science Press, Md., 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Miller, Sculptured Swfaces in Solid Models: Issues and Alternative Approaches, IEEE Computer Graphics and Applications, Dec. 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[D. Moore and J. Warren, Mesh Displacement: An Improved Contouring Method for Trivariate Data, Rice University Technical Rep. TR91-166, Sept. 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>4159</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M.E. Mortensen, Geometric Modeling. Wiley and Sons, New York, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>135797</ref_obj_id>
				<ref_obj_pid>135786</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. Muuss and L. Butler, Combinatorial Solid Geometry, B- Reps, and n-Manifold Geometry, in Computer Graphics Techniques: Theory and Practice, D. Rogers and R. Earnshaw, eds., Springer Verlag, New York, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617872</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[P. Ning and J. Bloomenthal, An Evaluation of Implicit Smface Tilers, IEEE Computer Graphics and Applications, Nov. 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>169719</ref_obj_id>
				<ref_obj_pid>169728</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[A. Paoluzzi, F. Bernardini, C. Cattani, and V. Ferrucci, Dimension-Independent Modeling with Simplicial Complexes, ACM Trans. on Graphics 12, Jan. 1993.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[A. Rockwood and J.C. Owen, Blending Smfaces in Solid Modeling, Proc. of SIAM Conf. on Geometric Modeling and Robotics, G. Farin, ed., Albany New York, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and M. O'Connor, SGC: a Dimension- Independent Model for Pointsets with Internal Structures and Incomplete Boundaries, Geometric Modeling for Product Engineering, Elsevier Science, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>115606</ref_obj_id>
				<ref_obj_pid>115604</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Rossignac and A. Requicha, Constructive Non-Regularized Geometry, in Beyond Solid Modeling, special ed. of Computer Aided Design, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[K. Weiler, Topological Structures for Geometric Modeling, Ph.D. dissertation, Dept. of Computer and Systems Engineering, Rensselaer Polytechnic Institute, Aug. 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[G. Wyvill, C. McPheeters, and B. Wyvill, Data Structure for Soft Objects. Visual Computer 2, 4, Aug. 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218463</article_id>
		<sort_key>317</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[A realistic camera model for computer graphics]]></title>
		<page_from>317</page_from>
		<page_to>324</page_to>
		<doi_number>10.1145/218380.218463</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218463</url>
		<keywords>
			<kw><![CDATA[camera modeling]]></kw>
			<kw><![CDATA[lens simulation]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[sampling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P50383</person_id>
				<author_profile_id><![CDATA[81100640935]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kolb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Princeton University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31038868</person_id>
				<author_profile_id><![CDATA[81100360165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Don]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitchell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Technology Division, Microsoft]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15033698</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Max Born and Emil Wolf. Principles of Optics. MacMillan, New York, second edition, 1964.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Robert L. Cook, Thomas Porter, and Loren Carpenter. Distributed ray tracing. In Computer Graphics (SIGGRAPH '84 P1vceedings), volume 18, pages 137-145, July 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Status report of the graphics standards planning committee of the ACM/SIGGRAPH. Computer Graphics, 11:19+117, Fall 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. Foote. Scientific paper 263. Bulletin of the Bureau of Standards, 12, 1915.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6023</ref_obj_id>
				<ref_obj_pid>6020</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Ned Greene and Paul S. Heckbert. Creating raster Omnimax images from multiple perspective views using the elliptical weighted average filter. IEEE Computer Graphics and Applications, 6(6):21-27, June 1986.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rudolph Kingslake. Optics in Photography. SPIE Optical Engineering Press, Bellingham, Washington, 1992.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max. Computer graphics distortion for IMAX and OMNIMAX projection. In Nicograph '83 Proceedings, pages 137-159, December 1983.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Parry Moon and Domina Eberle Spencer. The Photic Field. The MIT Press, Cambridge, Massachusetts, 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[F. E. Nicodemus, J. C. Richmond, J. J. Hsia, I. W. Ginsberg, and T. Limperis. Geometric considerations and nomenclature for reflectance. Monograph 161, National Bureau of Standards (US), October 1977.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806818</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[M. Potmesil and I. Chakravarty. A lens and aperture camera model for synthetic image generation. Computer Graphics (SIGGRAPH '81 P1vceedings), 15(3):297-305, August 1981.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63448</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[David F. Rogers and J. Alan Adams. Mathematical Elements for Computer Graphics. McGraw Hill, New York, second edition, 1990.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Mikio Shinya. Post-filter for depth of field simulation with ray distribution buffer. In P1vceedings of Graphics Intelface '94, pages 59-66. Candian Human- Computer Communications Society, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Peter Shirley. Discrepancy as a quality measure for sample distributions. Emvgraphics '91 P~vceedings, pages 183-193, June 1991.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Warren J. Smith. Modern Lens Design. McGraw Hill, New York, 1992.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[W.T. Weltbrd. Aberrations of the Symmetrical Optical System. Academic Press, London, 1974.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Turner Whitted. An improved illumination model for shaded display. Communications of the ACM, 23(6):343-349, June 1980.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Charles S. Williams and Orville A. Becklund. Optics: A Short Course for Engineers and Scientists. Wiley-Interscience, New York, 1972.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Realistic Camera Model for Computer Graphics Craig Kolb Don Mitchell Pat Hanrahan Computer Science 
Department Advanced Technology Division Computer Science Department Princeton University Microsoft Stanford 
University Abstract Most recent rendering research has concentrated on two subprob­lems: modeling the 
re.ection of light from materials, and calculat­ing the direct and indirect illumination from light sources 
and other surfaces. Another key component of a rendering system is the cam­era model. Unfortunately, 
current camera models are not geometri­cally or radiometrically correct and thus are not suf.cient for 
syn­thesizing images from physically-based rendering programs. In this paper we describe a physically-based 
camera model for computer graphics. More precisely, a physically-based camera model accurately computes 
the irradiance on the .lm given the in­coming radiance from the scene. In our model a camera is described 
as a lens system and .lm backplane. The lens system consists of a sequence of simple lens elements, stops 
and apertures. The camera simulation module computes the irradiance on the backplane from the scene radiances 
using distributed ray tracing. This is accom­plished by a detailed simulation of the geometry of ray 
paths through the lens system, and by sampling the lens system such that the ra­diometry is computed 
accurately and ef.ciently. Because even the most complicated lenses have a relatively small number of 
elements, the simulation only increases the total rendering time slightly. CR Categories and Subject 
Descriptors: I.3.3 [Computer Graph­ics]: Picture/Image Generation; I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism. Additional Key Words and Phrases: ray tracing, camera model­ing, lens simulation, 
sampling. 1 Introduction The challenge of producing realistic images of 3d scenes is often broken into 
three subproblems: modeling re.ection to account for the interaction of light with different materials, 
deriving illumina­tion algorithms to simulate the transport of light throughout the en­vironment, and 
modeling a camera that simulates the process of im­age formation and recording. In the last several years 
the majority of the research in image synthesis has been concentrated on re.ec­tion models and illumination 
algorithms. Since the pioneering work by Cook et al.[2] on simulating depth of .eld and motion blur, 
there has been very little work on camera simulation. Permission to make digital/hard copy of part or 
all of this work for personal or classroom use is granted without fee provided that copies are not made 
or distributed for profit or commercial advantage, the copyright notice, the title of the publication 
and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy otherwise, 
to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or 
a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Although current camera models are usually adequate 
for pro­ducing an image containing photographic-like effects, in general they are not suitable for approximating 
the behavior of a particular physical camera and lens system. For instance, current models usu­ally do 
not correctly simulate the geometry of image formation, do not properly model the changes in geometry 
that occur during fo­cusing, use an improper aperture in depth of .eld calculations, and assume ideal 
lens behavior. Current techniques also do not compute exposure correctly; in particular, exposure levels 
and variation of ir­radiance across the backplane are not accounted for. There are many situations where 
accurate camera models are im­portant: One trend in realistic computer graphics is towards physically­based 
rendering algorithms that quantitatively model the transport of light. The output of these programs is 
typically the radiance on each surface. A physically-based camera model is needed to simulate the process 
of image formation if accurate comparisons with empirical data are to be made.  In many applications 
(special effects, augmented reality) it is necessary to seamlessly merge acquired imagery with syn­thetic 
imagery. In these situations it is important that the syn­thetic imagery be computed using a camera model 
similar to the real camera.  In some machine vision and scienti.c applications it is neces­sary to simulate 
cameras and sensors accurately. For example, a vision system may want to test whether its internal model 
of the world matches what is being observed.  Many users of 3d graphics systems are very familiar with 
cam­eras and how to use them. By using a camera metaphor the graphics system may be easier to use. Also, 
pedagogically it is helpful when explaining the principles of 3d graphics to be able to relate them to 
real cameras.  Perhaps the earliest introduction of a camera model in computer graphics was the synthetic 
camera model proposed in the CORE system[3]. This and later work used a camera metaphor to describe the 
process of synthesizing an image, but did not intend to repro­duce photographic effects or provide photographic-like 
control over image formation. The next major breakthrough in camera model­ing was the simulation of depth 
of .eld and motion blur[10][2][12]. Current methods for simulating these effects use idealized lens sys­tems 
and thus cannot be used to simulate the behavior of a partic­ular physical system. A number of researchers 
have shown how to perform non-linear camera projections, such as those for .sheye or OMNIMAX lenses[7][5]. 
These methods derive a transformation that maps image points to directions in 3D, and have the disadvan­tage 
that effects such as depth of .eld cannot be combined with these special-purpose projections.  This 
paper describes a physically-based camera model for com­puter graphics. The model is capable of simulating 
the image for­mation of a particular physical lens system described by the arrange­ment of simple lenses 
as speci.ed by the manufacturer. Image for­mation is simulated by a modi.ed distributed ray tracing algorithm 
that traces rays through the lens system in order to compute the exposure on the .lm plane. This algorithm 
is a hybrid of render­ing techniques used by the computer graphics community and tech­niques used by 
lens makers to design camera lenses. Tracing rays through the lens system has the advantage that both 
the geometry and the radiometry of image formation can be accurately modeled. Moreover, we show that 
this simulation costs little more than previ­ous algorithms. For the purposes of this paper, our emphasis 
is on simulating the lens system, and as such the important effects caused by .lm re­sponse, shutter 
shape and movement, .lters, and other parts of the camera will not be addressed here. We will further 
assume that the system is aberration-limited, and so the effects of diffraction can be ignored. The paper 
begins with a discussion of the construction of lenses and how they are modeled in our system. We then 
consider the var­ious geometrical factors that effect image formation and how those factors can be accurately 
accounted for. The radiometry of image formation and its computation are then presented. Finally, results 
of an implementation of our model are shown and discussed.  2 Lens Systems Lens systems are typically 
constructed from a series of individual spherical glass or plastic lenses and stops centered on a common 
axis. A stop is an opaque element with a roughly circular opening to permit the passage of light. The 
element that most limits the angular spread of the bundle of rays that will pass unobstructed through 
the system from the axial point on the image plane is termed the aper­ture stop. The size of the aperture 
stop in a camera is typically set by the photographer through the use of an adjustable diaphragm, and 
serves to provide control over the quantity of light striking the .lm plane and the depth of .eld in 
the image. As shown in Figure 1, the construction of a lens is traditionally presented in a tabular format1. 
Our system reads tables like these and uses the information to model the behavior of the lenses they 
de­scribe. Lens manufacturers are reluctant to release lens design data, but it is possible to .nd tables 
in patents that might cover a particu­lar lens, or in collections of lens designs such as those given 
in the book by Smith[14]. There are two challenges to simulating a real lens system: The geometry of 
image formation must be correctly computed. Ideally, a lens will cause a point in object space to be 
imaged as a single point in image space, and will have constant magni.­cation over the entire .eld of 
view. This is the assumption that is made in most rendering systems that use the pin-hole camera model 
or projective transformations. Unfortunately, no physi­cal system is capable of ideal image formation. 
Real lenses ex­hibit deviations from the ideal in the form of aberrations such as coma or pin-cushion 
distortion[15]. The radiometry of image formation must be correctly com­puted. The correct exposure must 
be computed given the lighting in the scene. In most rendering systems this computation is ar­bitrary, 
with little attention paid to units and their physical magnitudes. In a real camera, the exposure is 
controlled by a variety of factors and these must be correctly simulated if a physically-based rendering 
system is to produce realistic out­put. Moreover, while ideal lenses focus light energy evenly at 1In 
our .gures, we follow the convention of drawing object space to the left of the lens system, image space 
to the right, with coordinates along the axis increasing from left to right. Distances in the lens system 
are signed quantities, with a distance measured from left to right being positive, and right to left 
negative. Unprimed variables are in object space, primed are in image space. all points on the image 
plane, real lenses suffer from an un­even exposure across the backplane. Accurate computation is therefore 
more than a matter of simply computing a correct overall scale factor. Abstractly, the purpose of our 
camera module is to transform the scene radiances computed by the lighting and shading modules into the 
response at a pixel. This may be modeled by the measurement equation[9] (in computer graphics sometimes 
called the pixel equa­tion) ZZZZ 0000 R=L(T(xw));))S(xt)P(x))dx0 .dw0dtd) (1) 0 In this equation, xrepresents 
a position vector on the backplane, w0 is a direction vector towards the lens system, tis time and )is 
wavelength. Lis the scene spectral radiance de.ned in object space. The function Tmodels the geometry 
of image formation, in effect transforming from image space to object space (for generality, we assume 
this is a function of wavelength). Smodels the behavior of the shutter and is a function of time (more 
generally, the response of real shutters may also depend on position). Pdescribes the sensor response 
characteristics and is a function of position within a pixel and wavelength. The measurement equation 
provides the basis for quantifying the effects of the lens and other camera components on image forma­tion. 
The rest of the paper discusses how we model the lens and evaluate the measurement equation. 3 Lens 
Geometry and Image Formation In this section, we discuss the geometrical properties of lens sys­tems. 
We describe how to trace rays through a lens system, how to derive a projective transformation that approximates 
the action of the lens system, how to accurately model the geometry of focusing, and .nally how to derive 
the effective size of the aperture. These techniques allow us to use actual lens descriptions in rendering 
sys­tems that use ray tracing, as well as those that use linear viewing transformations. They also allow 
us to model the depth of .eld and exposure due to real lens systems. 3.1 Tracing Rays Through Lens Systems 
One robust and accurate method to predict how a lens will form an image is to trace rays of light through 
the system. Lens and optical system designers have employed ray tracing techniques to evaluate designs 
for more than a century, and thus the process is now quite well-understood. Typically, a random set of 
rays are traced from ob­ject space to image space and their positions on the .lm plane are recorded to 
form a spot diagram. Various statistics are derived from these diagrams to evaluate the quality of the 
lens. Surprisingly, to our knowledge, ray tracing is not used by lens designers to create synthetic imagery 
because of the perceived high cost of doing these calculations. R= Ray(point on .lm plane, point on rear-most 
element) For each lens element Ei, from rear to front, p= intersection of Rand Ei If pis outside clear 
aperture of Ei ray is blocked Else if medium on far side of Ei6 =medium on near side compute new direction 
for Rusing Snell s law Figure 2: Basic algorithm for tracing a ray through a lens system. P' f' F' 
F fP Figure 3: Finding a thick approximation to the lens in Figure 1. The actual path of an axis-parallel 
ray from object space is drawn as a solid line, and its idealized path is drawn as a dashed line. The 
standard algorithm for tracing a ray through the lens is given in Figure 2. The propagation of a ray 
through a lens surface involves both .nding the point of intersection between the ray and the surface 
and the refraction of the ray as it crosses the interface between the two media. The vast majority of 
lenses have spherical or planar sur­faces, and therefore these computations are quite simple [16][17]. 
Although spherical surfaces are by far the most common, an object­oriented design of the lens software 
makes it possible to include el­ements of any shape for which intersection and normal-.nding rou­tines 
can be written. Tracing rays through a lens system described in the tabular for­mat is considerably faster 
than it would be if the lens were modeled as a collection of general objects for the ray tracer to render. 
This is because the exact visibility ordering of the surfaces is known a priori, and thus there is no 
search required to .nd the closest sur­face in a given direction. The main computational cost of tracing 
rays through spherical systems is two square roots per surface. This cost is .xed relative to scene complexity, 
and is usually small com­pared to the total cost of object intersection tests and other lighting calculations. 
  3.2 Thick Lens Approximation In some situations the geometry of image formation may be approx­imated 
by treating the lens as an ideal thick lens. A thick lens forms perfect images; that is, each point in 
object space is imaged onto a single point in image space and all points in the plane of focus map onto 
the image plane with uniform magni.cation. We use thick lenses in our model to determine the exit pupil, 
as discussed in Sec­tion 3.4. The behavior of a thick lens can be characterized by its focal points and 
principal planes, which are illustrated in Figure 3. Axis­parallel rays from a point at in.nity in object 
space will enter the lens, be refracted through it, and emerge with a new direction and in­tersect the 
axis at the secondary focal point, F0. The point at which the incident ray and the emergent ray would 
intersect de.nes the 00 secondary principal plane P. Pis an imaginary surface normal to the axis at which 
we assume refraction to have occurred. Similarly, axis-parallel rays from image space intersect the axis 
at F, the pri­mary focal point, and the intersection of the original and refracted rays de.ne P, the 
primary principal plane. The signed distance from 00 Pto Fis the effective focal length of the lens, 
f0, and is equal to -fwhen both object and image space are in the same medium. The thick lens derives 
its name from the fact that, unlike the thin lens model usually used in computer graphics, the principal 
planes 0 are not assumed to coincide. The distance from Pto Pis the the lens effective thickness, and 
may be negative, as for the lens in Fig­ure 3. This additional parameter allows for a more general model 
of image formation. Although a thin lens approximation can be valid if the thickness is negligible, the 
thickness of photographic lenses is usually signi.cant. The utility of both approximations is that their 
imaging properties can be modeled by a simple transformation. To .nd a thick approximation to a given 
lens system, we apply the above de.nitions of focal points and principal planes directly. We trace rays 
through the lens system from each side and .nd the appro­ 0 priate points of intersection to de.ne P, 
F, P0,and F. An alterna­tive way to .nd these values is by using the various thick lens formu­las, which 
provide an analytical means for deriving a thick lens from a collection of simple lenses. The advantage 
of the .rst method is that it yields a more accurate approximation to the lens because typ­ical lens 
systems are designed to exhibit ideal image formation even though the individual elements are less than 
ideal. The geometry of image formation by a thick lens may be realized by a projective transformation 
de.ned by the focal points and prin­cipal planes[1]. Given a point in object space at a signed distance 
z along the axis from P, the conjugate equation holds that 111 -= (2) zf0z 0 00 where zis the axial distance 
from Pto the point s image in image space. This equation and some simple geometry can be used to .nd 
the image of a point on either side of the lens. However, the result­ 0 ing equations are inconvenient 
in that zand zare measured from different origins. If the origin is assumed to be at Pand both dis­tances 
are measured from it, the same equations apply, except that 00 zmust then be translated by t=P-P, the 
thickness of the lens. The total transformation can be written as a 4x4 matrix: 232323 1000 X 0 x 6Y0 
76 0100 76y7 4Z05 = 40 t 0 t54 z 015 f 0 1 W000 11 f Thus the thick lens approximation may be used in 
conventional ren­dering systems that use 4x4 projective transformations to model the camera. Note that 
when tis zero, the above transformation is identical to the usual thin lens transformation used in computer 
graphics[11]. 3.3 Focusing In order to make the camera model easy to control, it should be pos­sible 
to specify the distance from the .lm plane at which the cam­era is focused. Focusing physical systems 
involves moving one or more lens elements along the axis in order to change the distance at which points 
are focused. For simple lenses, the housing and all of the elements are moved together, while in more 
complicated internal focusing lenses, only a few elements move while the lens housing itself remains 
stationary. Given a point located at an axial distance zfromthe .lmplane, we can use (2) to determine 
how far the lens must be moved in order to bring the point into focus. If the lens is focused at in.nity, 
refo­cusing at zcan be done by moving the lens a distance Taway from the .lm plane, where Tsatis.es: 
T2002 T(2f t-z) f=0 (3) One solution to (3) corresponds to the lens being near the .lm and far from the 
object, the other to the lens being near the object and far from the .lm. In most situations, physical 
constraints on the dis­tance the lens can move will make the latter solution unrealizable. Moving the 
lens relative to the .lm plane has the additional effect of changing the .eld of view. As the distance 
at which the camera is focused is decreased, the distance of the lens from the .lm plane is increased 
and the .eld of view shrinks. This effect is not modeled in the standard camera model, which assumes 
that the .lm plane is always located at the focal point and that the lens can be focused at any arbitrary 
distance without any change of con.guration. Figure 4: Illustration of the exit pupil for the double-Gauss 
lens of Figure 1. The diaphragm, drawn in solid black, acts as the aper­ 0 ture stop for the point xon 
the axis at the .lm plane. The extent of 0 the bundle of rays from xthat pass unobstructed through the 
lens is represented by the pair of solid lines on either side of the axis. The exit pupil, the image 
of the aperture stop through the rear-most two groups of elements, is drawn in outline. The exit pupil 
de.nes the cone of rays from xthat pass unobstructed through the lens, as shown by the dashed lines. 
 3.4 The Exit Pupil Recall that when looking through a lens system from a point on the backplane, there 
is a cone of rays within which the environment is visible, and that the aperture stop is the element 
limiting the extent of this cone. The exit pupil is de.ned to be the image of the aperture stop as viewed 
from image space (see Figure 4). Only rays directed from the .lm plane at the interior of the exit pupil 
will pass through the physical aperture stop, and so it is only these rays that we need consider when 
tracing rays through the system. Note the difference between this and directing rays at the aperture 
stop itself; this can produce incorrect results, because the image of the aperture may be larger than 
the aperture itself (as shown in Figure 4), and some rays that would pass through the system would not 
be generated. Note also the difference between this and .ring rays at the lens element closest to the 
.lm plane. While this will produce correct results in the limit, it is wasteful because some of these 
rays may be blocked by the aperture stop. Using the correct exit pupil is critical if the depth of .eld 
and the exposure are to be computed consistently. We .nd the exit pupil as follows: For each potential 
stop, we de­termine its apparent size and position from the axial point on the image plane. This is done 
by imaging the stop through those lens elements that fall between the stop and image space. We then deter­mine 
which image disk subtends the smallest angle from the axial point on the image plane. This image is the 
exit pupil, and the stop corresponding to it is the aperture stop. If we assume that each group of lens 
elements exhibits ideal im­age formation, the image of a given stop can be computed using a thick lens 
approximation to the appropriate subsystem of elements. In physical lenses, this is accurate only to 
the extent that the circu­lar exit pupil is a reasonable approximation to the actual image of the aperture 
stop as viewed from off-axis points. In particular, some lenses distort the shape and position of the 
exit pupil when viewed from off-axis in order to increase or decrease exposure at points near the edge 
of the .lm[6]. We cannot validly use a thick approxima­tion to .nd the exit pupil for such lenses in 
these cases. However, we can always ensure correct simulation by using the rear-most lens element as 
the exit pupil, at the cost of some loss of ef.ciency. The exit pupil, rather than the aperture, should 
also be considered when using a thick lens in a ray tracer. Cook et al. described an al­gorithm for tracing 
rays through a thin lens by selecting a point on the aperture stop and tracing a ray from that point 
through the image of the current image plane point. As noted above, using the aperture stop rather than 
the exit pupil can lead to errors. The process of trac­ing a ray through a thick lens and exit pupil 
is shown in Figure 5. x P P 0 Figure 5: To trace a ray from xthrough a thick lens, a point son 0 the 
exit pupil is chosen. The point of intersection of the ray from x 0 to swith Pis found, and is then translated 
parallel to the axis to P. 0 The ray from this point through x,the image of x,isthen usedto Figure 6: 
Geometry for computing the irradiance at a point on the sample the scene. .lm plane and the exact form 
factor.   4 Radiometry and Sampling In this section we describe how we compute exposure on the .lm 
plane. 4.1 Exposure Sensor response is a function of exposure, the integral of the irra­ 0 diance at 
a point xon the .lm plane over the time that the shutter is open. If we assume that irradiance is constant 
over the exposure period, and that exposure time is .xed, 00 H(x)=E(x)T(4) 00 where E(x)is the irradiance 
at x, Tis the exposure duration, and 00 H(x)is the exposure at x. This model is a simpli.cation of the 
exposure process in physical systems, where the exposure at a point is dependent upon the shape and movement 
of the shutter. 00 In order to compute E(x), we integrate the radiance at xover the solid angle subtended 
by the exit pupil, which is represented as a disk, as shown in Figure 6. Z O 0O00 0 000 coscos x dA 00 
kx00 00 2D -x0k2 E(x)=L(xx) (5) If the .lm plane is parallel to the disk, this can be rewritten as Z 
1 0 0004 O0dA00 E(x)= L(xx)cos(6) 2 Z 00 x2D where Zis the axial distance from the .lm plane to the disk. 
This formula differs from that described by Cook et al., which assumed where Zis the axial distance from 
the .lm plane to the disk, and Ais the area of the disk. If Zis assumed to be the focal length, (7) can 
be written .cos4O0 0 E(x)=L(8) n2 4 where nis the f-number of the lens. Equation (7) is the one most 
often found in optics texts, while (8) appears in many photographic texts. Note that both assume a small 
solid angle. 2. For larger solid angles, a more accurate way to estimate the variation in irradiance 
is to compute the differential form fac­tor from a point on the .lm plane to a disk. This correctly ac­counts 
for the .nite size of the disk, and the variation in angle as we integrate over the disk. This integral 
may be computed analytically[4] (an elegant derivation may be found in [8]). In real lens systems these 
analytical formulas overestimate the F = 1 2 1­ p(a2 a-r22 Z2 Z2 r2)2-4r2a2 (9) exposure. This is due 
to vignetting, the blocking of light by lens el­ements other than the aperture stop when a ray passes 
through the system at a large angle to the axis. Vignetting can be a signi.cant effect in wide-angle 
lenses and when using a lens at full aperture. Fortunately, the ray tracing algorithm described in the 
last section accounts for this blockage, and hence computes the exposure cor­rectly. Figure 7 compares 
the irradiance computed by tracing rays through the lens system pointed at a uniform radiance .eld with 
0.20 each ray has the same weight. It is also important to perform the in­tegral using a disc-shaped 
exit pupil, rather than a rectangular one. Using a rectangular pupil causes the depth of .eld to be computed 
incorrectly, since points not in focus will then have rectangular cir­ cles of confusion on the .lm plane. 
The weighting in the irradiance integral leads to variation in irra­ diance across the .lm plane due 
to the lens system. There are two 4 simple analytical ways to estimate this effect: the coslaw and the 
differential form factor to a disk. Irradiance (W/m^2) Standard 0.15 Form factor cos^4 0.10 Vignetted 
0.05 0 1. If the exit pupil subtends a small solid angle from x, O0can be 0Distance from center (mm) 
assumed to be constant and equal to the angle between xand the center of the disk. This allows us to 
simplify (5) to: Figure 7: Irradiance on the .lm plane resulting from a uniform unit O A radiance .eld 
imaged through the double-Gauss lens at full aperture, 0 E(x 0 )=Lcos4(7) as a function of distance from 
the center of the .lm. 2 Z  Figure 8: Four views of the same scene taken with a 16mm .sheye lens (bottom 
left), 35mm wide-angle lens (top left), 50mm double-Gauss lens (top right), and a 200mm telephoto lens 
(bottom right). A pro.le view of the lens system used to take each image is shown on the left. As with 
physical lenses, perspective is compressed with long focal lengths and expanded with short focal lengths. 
The .sheye image shows the lens signature barrel distortion. values computed using the usual computer 
graphics camera model 4 (no weighting), the form factor, the cosapproximation, and the 4 full lens simulation. 
For this particular lens, the coslaw and the form factor approximations do not differ signi.cantly. However, 
vi­gnetting reduces the true exposure near the edge of the .lm to nearly one third of its approximated 
value.  4.2 Sampling In our model, a pixel s value is proportional to the radiant power falling on a 
hypothetical pixel-sized sensor in the image plane. The radiant power is given by the radiance integrated 
over the four­dimensional domain of pixel area and solid angle of incoming di­rections. This is estimated 
by sampling radiance over this domain (i.e., by casting rays from the pixel area toward the lens). There 
are several ways to improve the ef.ciency of this calcula­tion. First, we sample within the solid angle 
subtended by the exit pupil rather than sampling radiance over the entire hemisphere. Ad­ditional noise 
reduction might also be obtained by importance sam­ cose00 pling, folding the factor of cose0 into the 
distribution of rays 00 0 2 kx_xk over solid angle. Finally, ef.ciency can be improved by the use of 
good sampling patterns, which can reduce the amount of error in a pixel as well as affecting the overall 
distribution of noise in the .nal image. We have used strati.ed and quasirandom sampling patterns in 
our experiments. Sampling without importance distribution is straightforward. Rays are cast from points 
in the pixel area toward points on the disk of the exit pupil, and the resulting values are weighted 
by cose0 cose00 . We can think of these pairs of points on the pixel area 00 0 2 kx_xk and exit pupil 
as being single points in the four-dimensional domain of integration. Rather than generating uniformly 
distributed points on the lens and weighting them, we can perform importance sampling by gen­erating 
rays with a cosine-weighted distribution in solid angle and averaging the unweighted radiance values. 
We implemented an im­portance version of the third square-to-disk mapping described be­low. In the 35mm 
camera lenses that we tested, importance sam­ 4 pling reduced noise by only about one percent, because 
the cosO0 weighting factor only varied by approximately twenty percent (see Figure 7). Since importance 
sampling adds a great deal of complex­ity and expense to the sampling operation, we believe it is not 
worth the effort in this particular application. To generate these sample locations, it is usually necessary 
to start with some pattern of points de.ned in the hypercube [01]4.Two of the dimensions are translated 
and scaled to the pixel area, and the other two dimensions are mapped to the disk of the exit pupil. 
The mapping from unit square to disk must be measure preserving (have a constant Jacobian) in order to 
avoid introducing a sampling bias. Thus, uniformly distributed points in the square map to uniformly 
distributed points on the disk. There are a number of such mappings. However, when mapping special sampling 
patterns such as strati.ed patterns it is good to choose a mapping that does not severely distort the 
shape of the strata. The obvious mapping, p r=uO=2.v (10) is actually rather poor in this respect. A 
better mapping, used by Shirley[13], takes concentric squares to concentric circles. For ex­ample, in 
one wedge of the square, we have: 00 x=2x-1y=2y-1 x 0 0 r=yO= (11) y 0 A third mapping we have implemented 
takes subrectangles [0x] [01]to a chord with area proportional to x, as illustrated below. We have used 
two schemes to generate good sampling patterns in the hypercube. One is strati.ed sampling, dividing 
the dimensions of the hypercube into blocks and placing a sample randomly within each block. Given Nsamples, 
we could divide the hypercube into 1 4 Nstrata along each dimension. For typical values of N(it is un­ 
usual for a distributed ray tracer to cast more than a few hundred rays per pixel), this does not amount 
to many divisions of each dimen­sion, and the bene.ts of strati.cation would be small. Instead, the pixel-area 
dimensions and the aperture-disk dimensions are strati­Figure 9: Images synthesized with a 35mm wide-angle 
lens using, in order of decreasing accuracy, the full lens simulation (left), thick approx­imation (center), 
and the standard model (right). The top left arrow indicates the location of the scanline used in Figure 
11.  pp .ed separately as Nby Ngrids on subsquares. To avoid sys­tematic noise, the correlation of strata 
between pixel area and disk are randomly permuted. We have found that the choice of square-to-disk mapping 
and strati.cation scheme strongly affect sampling ef.ciency and image quality. Using Shirley s mapping 
(11) yielded signi.cantly lower RMS error (15 percent lower than (10) in typical experiments) as well 
as visibly improved image quality. Using the strati.cation method described above gives reasonable pixel 
antialiasing where edges are in sharp focus, and good distribution of noise in regions where depth of 
.eld causes blur. Pixel Number   5Results We have implemented our camera model as part of a ray tracer. 
The system supports rendering scenes using cameras constructed with different lenses and .lm formats. 
Figure 8 shows four images gen­erated by the renderer and the lenses used in taking each of them. For 
each image, the camera was positioned in the scene so that the bust was imaged at approximately the same 
place and magni.cation on the .lm. As the focal length of the lens is increased, the rela­tive size of 
the picture frame in the background grows, as expected. Darkening near the edge of the image due to vignetting 
is also ap­parent when using the .sheye and double-Gauss lens. These images typically required 90 minutes 
of CPU time to compute on a Silicon Graphics Indigo2 workstation at 16 rays per pixel. Approximately 
10% of that time was spent tracing rays through the lens system, and thus the use of the full lens simulation 
is quite practical. Figure 9 illustrates the differences in image geometry resulting from the use of 
different camera models. The standard camera model produces an image with too large a .eld of view, with 
both the bust and picture frame appearing smaller than in the full sim­ulation image. The similarity 
of the full simulation and thick lens images illustrates the fact that using a thick lens can result 
in a good approximation if the actual lens forms nearly ideal images. Figure 10 illustrates the change 
in .eld of view that occurs when the focus of a lens is changed. The .gure shows the same scene as that 
in Figure 9, but with the lens focused on the picture frame in the background. Note that more of the 
front of the bust can be seen in the lower-left corner compared to the full simulation image in Figure 
9. This increased .eld of view is caused by the movement of the lens towards the .lm plane when the lens 
is focused on a more distant object. The new camera model also produces signi.cant differences in exposure 
compared to the standard model. The exposure computed for a typical scene by the full simulation and 
the two approximations is shown in Figure 11. Exposure is generally overestimated in the standard model, 
and, as expected, the error tends to grow near the edge of the .lm. An image taken with the .sheye lens 
is shown in Figure 12. Again, barrel distortion and darkening caused by vignetting are ev­ident. 6 Summary 
and Discussion The physically-based camera model that we have described draws upon techniques from both 
the lens design and computer graphics literature in order to simulate the geometry and radiometry of 
image formation. The lens system is described using standard lens con­struction information, and its 
behavior is characterized by tracing light rays through its various elements and weighting them properly. 
The primary added cost of using the model is .nding the intersec­tion with and refraction caused by each 
lens surface; for reasonably complex scenes the increase in rendering time is small, and the full  Figure 
12: simulation is very practical. Further, we show how the behavior of well-corrected lens systems can 
be approximated using a projective transformation derived from a thick lens approximation. The new model 
is an improvement over standard models in a number of ways: The geometric relationships between the 
lens, object, and .lm plane are modeled properly by precise placement and move­ment of lens elements. 
This is necessary for accurate .eld of view and depth of .eld calculations.  Image geometry is computed 
correctly by tracing the path of light through the system. The model is capable of simulating non-linear 
geometric transformations such as those produced by .sheye and anamorphic lenses, while simultaneously 
com­puting the correct exposure and depth of .eld.  The image irradiance, or exposure, is computed properly 
be­cause the model applies the correct weighting to rays traced through the lens system, and derives 
the correct exit pupil to control the limits of the integration. The model also correctly accounts for 
vignetting due to the blockage of rays.  Although our model is more accurate than previous camera mod­els, 
there are many aspects of cameras and lens systems that we have not simulated. For example, our model 
assumes the shutter opens and closes instantaneously, which is not true. Our model also as­sumes that 
the lens transmittance is perfect, and that the properties of lens surfaces do not vary with position 
or time. We have also ig­nored many wavelength-dependent effects, in particular sensor sen­sitivity and 
response, and chromatic aberration due to the variation of index of refraction with wavelength. In the 
future we intend to experimentally verify our model by simulating particular lens sys­tems and comparing 
the results with captured images. The goal of these experiments will be to .nd what level of detail of 
the camera must be simulated to match computer-generated and photographic images.  7 Acknowledgements 
Thanks to Matt Pharr and Reid Gershbein for help with the .gures and the rendering system, Viewpoint 
DataLabs for the Beethoven and Pentax models, and to the anonymous reviewers for their sug­gestions. 
This research was supported by equipment grants from Apple and Silicon Graphics Computer Systems, and 
a research grant from the National Science Foundation (CCR-9207966).  References [1] Max Born and Emil 
Wolf. Principles of Optics. MacMillan, New York, second edition, 1964. [2] Robert L. Cook, Thomas Porter, 
and Loren Carpenter. Distributed ray tracing. In Computer Graphics (SIGGRAPH 84 Proceedings), volume 
18, pages 137 145, July 1984. [3] Status report of the graphics standards planning committee of the ACM/SIGGRAPH. 
Computer Graphics, 11:19+117, Fall 1977. [4] P. Foote. Scienti.c paper 263. Bulletin of the Bureau of 
Standards, 12, 1915. [5] Ned Greene and Paul S. Heckbert. Creating raster Omnimax images from multi­ple 
perspective views using the elliptical weighted average .lter. IEEE Computer Graphics and Applications, 
6(6):21 27, June 1986. [6] Rudolph Kingslake. Optics in Photography. SPIE Optical Engineering Press, 
Bellingham, Washington, 1992. [7] Nelson L. Max. Computer graphics distortion for IMAX and OMNIMAX pro­jection. 
In Nicograph 83 Proceedings, pages 137 159, December 1983. [8] Parry Moon and Domina Eberle Spencer. 
The Photic Field. The MIT Press, Cam­bridge, Massachusetts, 1981. [9] F. E. Nicodemus, J. C. Richmond, 
J. J. Hsia, I. W. Ginsberg, and T. Limperis. Geometric considerations and nomenclature for re.ectance. 
Monograph 161, Na­tional Bureau of Standards (US), October 1977. [10] M. Potmesil and I. Chakravarty. 
A lens and aperture camera model for syn­thetic image generation. Computer Graphics (SIGGRAPH 81 Proceedings), 
15(3):297 305, August 1981. [11] David F. Rogers and J. Alan Adams. Mathematical Elements for Computer 
Graphics. McGraw Hill, New York, second edition, 1990. [12] Mikio Shinya. Post-.lter for depth of .eld 
simulation with ray distribution buffer. In Proceedings of Graphics Interface 94, pages 59 66. Candian 
Human-Computer Communications Society, 1994. [13] Peter Shirley. Discrepancy as a quality measure for 
sample distributions. Euro­graphics 91 Proceedings, pages 183 193, June 1991. [14] Warren J. Smith. Modern 
Lens Design. McGraw Hill, New York, 1992. [15] W. T. Welford. Aberrations of the Symmetrical Optical 
System. Academic Press, London, 1974. [16] Turner Whitted. An improved illumination model for shaded 
display. Communi­cations of the ACM, 23(6):343 349, June 1980. [17] Charles S. Williams and Orville A. 
Becklund. Optics: A Short Course for Engi­neers and Scientists. Wiley-Interscience, New York, 1972. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218466</article_id>
		<sort_key>325</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Physically-based glare effects for digital images]]></title>
		<page_from>325</page_from>
		<page_to>334</page_to>
		<doi_number>10.1145/218380.218466</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218466</url>
		<keywords>
			<kw><![CDATA[bloom]]></kw>
			<kw><![CDATA[flare]]></kw>
			<kw><![CDATA[glare]]></kw>
			<kw><![CDATA[lenticular halo]]></kw>
			<kw><![CDATA[vision]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P99524</person_id>
				<author_profile_id><![CDATA[81100166204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spencer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Taligent, Inc., 10201 N. De Anza Blvd., Cupertino, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083338</person_id>
				<author_profile_id><![CDATA[81100449948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University, 580 ETC, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39086156</person_id>
				<author_profile_id><![CDATA[81332538048]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zimmerman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Indiana University, Lindley Hall, Bloomington, IN]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University, 580 ETC, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BECKMAN, C., NILSSON, O., AND PAULSSON, L.-E. Intraocular scatterinng in vision, artistic painting, and photography. Applied Optics 33, 21 (1994), 4749-4753.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BETTELHEIM, F. A., AND PAUNOVIC, M. Light scattering of normal human lens 1. Biophysical Journal 26, 3 (April 1979), 85-99.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLACKWELL, O. M., AND BLACKWELL, H. P~. Individual responses to lighting parameters for a population of 235 observers of varying ages. Journal of the IES 2 (July 1980), 205-232.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BOETTNER, E., AND WOLTER, J. Transmission of the ocular media. Investigative Ophthalmology 1 (1962), 776.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CAMPBELL, F., AND GUBISCH, P~. Optical quality of the human eye. Journal of Physiology 186 (1966), 558-578.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[CARTER, J. H. The effects of aging on selected visual functions: Color vision, glare sensitivity, field of vision, and accomodation. In Aging and Human Visual Function, R. Sekular, D. Kline, and K. Dismukes, Eds., vol. 2 of Modern Aging Research. Alan R. Liss, Inc., 1982, pp. 121-130.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[CHIU, K., HERF, M., SHIRLEY, P., SWAMY, S., WANG, C., AND ZIMMERMAN, K. Spatially nonuniform scaling functions for high contrast images. In Graphics Interface '93 (May 1993), pp. 245- 244.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[COREN, S., AND GIRGUS, J. Density of human lens pigmentation in vivo measures over an extended age range. Vision Research 12, 2 (1972), 343-346.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[CORNSWEET, T. N., AND TELLER, D. Y. Relation of increment thresholds to brightness and luminance. Journal of the Optical Society of America 55 (1975), 1303-1308.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[DAVSON, H., Ed. The Eye, third ed., vol. la. Academic Press, inc. ltd., London, 1984.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[FRY, G. A re-evaluation of the scattering theory of glare. Illuminating Engineering 49, 2 (1954), 98-102.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[HEMENGER, R. P. Intraocular light scatter in normal vision loss with age. Applied Optics 23, 12 (1984), 1972-1974.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[HEMENGER, R. P. Small-angle intraocular light scatter: a hypothesis concerning its source. Journal of the Optical Society of America A 5, 4 (1987), 577-582.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[HEMENGER, R. P. Light scatter in cataractous lenses. Opthalmological Phyisiological Optics 10 (October 1990), 394-397.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[HEMENGER, 1:~. P. Sources of intraocular light scatter from inversion of an empirical glare function. Applied Optics 31, 19 (1992), 3687-3693.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[HOLLADAY, L. Action of a light source in the field of view on lowering visibility. Journal of the Optical Society of America 14, 1 (1927), 1-15.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[IJSPEERT, J. K., DE WAARD, P. W. T., VAN DEN BERG, T., AND DE JONG, P. The intraocular straylight function in 129 healthy volunteers; dependence on angle, age, and pigmentation. Vision Research 30, 5 (1990), 699-707.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[MELLERIO, J., AND PALMER, D. A. Entopic halos and glare. Vision Research 12 (1972), 141-143.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>45601</ref_obj_id>
				<ref_obj_pid>45596</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[MEYER, G. W. Wavelength selection for synthetic image generation. Computer Vision, Graphics, and Image Processing 41 (1988), 57-79.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97922</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[NAKAMAE, E., KANEDA, K., OKAMOTO, T., AND NISHITA, T. A lighting model aiming at drive simulators. Computer Graphics 24, 3 (August 1990), 395-404. ACM Siggraph '90 Conference Proceedings.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[ORDY, J. M., BRIZZEE, K. P~., AND JOHNSON, H. A. Cellular alterations in visual pathways and the limbic system: Implications for vision and short-term memory. In Aging and Human Visual Function, R. Sekular, D. Kline, and K. Dismukes, Eds., vol. 2 of Modern Aging Research. Alan R. Liss, Inc., 1982, pp. 79-114.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[OWSLEY, C., SEKULER, P~., AND SIEMSEN, D. Contrast sensitivity throughout adulthood. Vision Research 23, 7 (1983), 689-699.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[P~EA, M. S., Ed. The Illumination Engineering Society Lighting Handbook, 8th ed. Illumination Engineering Society, New York, NY, 1993.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[P~ONCI, C., AND STEFANACCI, S. An annotated bibliography on some aspected of glare. Art. Fond. Ronci 30 (1975), 277-317.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Ross, J. E., CLARKE, D. D., AND BRON, A. J. Effect of age on contrast sensitivity function: uniocular and binocular findings. British Journal of Opthalmology 69 (1985), 51-56.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[SAID, F., AND WEALE, R. The variation with age of the spectral transmissivity of the living human crystaline lens. Gerontologia 3, 4 (1959), 213.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[SCHLICK, C. Quantization techniques for visualization of high dynamic range imagess. In Proceedings of the Fifth Eurographics Workshop on Rendering (June 1994), pp. 7-18.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[SEKULER, R., AND BLAKE, R. Perception, second ed. McGraw- Hill, New York, 1990.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[SIMPSON, G. Ocular halos and coronas. British Journal of Opthalmology 37 (1953), 450-486.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[SMITH, G., VINGRYS, A. J., MADDOCKS, J. D., AND HELY, C. P. Color recognition and discrimination under full-moon light. Applied Optics 33, 21 (1994), 4741-4748.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[STILES, W. The effect of glare on the brightness difference threshold. Proceedings of the Royal Society of London 104 (1929), 322-351.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[TUMBLIN, J., AND P~USHMEIER, H. Tone reproduction for realistic computer generated images. IEEE Computer Graphics Applications 13, 7 (1993).]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Vos, J. Disability glare- a state of the art report. C.I.E. Journal 3, 2 (1984), 39-53.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180934</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[WARD, G. A contrast-based scalefactor for luminance display. In Graphics Gems IV, P. Heckbert, Ed. Academic Press, Boston, 1994, pp. 415-421.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. The radiance lighting simulation and rendering system. Computer Graphics 28, 2 (July 1994), 459-472. ACM Siggraph '94 Conference Proceedings.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[WYSZECKI, G., AND STILES, W. Color Science: Concepts and Methods, Quantitative Data and Formulae, second ed. Wiley, New York, N.Y., 1992.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Physically-Based Glare E.ects for Digital Images Greg Spencer* Peter Shirleyy Kurt Zimmermanz Donald 
P. Greenbergx Taligent, Inc. Cornell University Indiana University Cornell University Abstract The physical 
mechanisms and physiological causes of glare in human vision are reviewed. These mechanisms are scattering 
in the cornea, lens, and retina, and di.raction in the coherent cell structures on the outer radial areas 
of the lens. This scat­tering and di.raction are responsible for the bloom and .are lines seen around 
very bright objects. The di.rac­tion e.ects cause the lenticular halo . The quantitative models of these 
glare e.ects are reviewed, and an algorithm for using these models to add glare e.ects to digital images 
is presented. The resulting digital point-spread function is thus psychophysically based and can substantially 
increase the perceived dynamic range of computer simulations con­taining light sources. Finally, a perceptual 
test is presented that indicates these added glare e.ects increase the apparent brightness of light sources 
in digital images. CR Categories and Subject Descriptors: I.3.0 [Computer Graphics]: General; I.3.6 [Computer 
Graphics]: Methodology and Techniques. Additional Key Words and Phrases: bloom, .are, glare, lenticular 
halo, vision. 1 Introduction There is a continual quest for photorealistic simulations, not only by accurately 
modeling the physical behavior of light re.ection, propagation and transport, but by the creation of 
images that are perceived to be realistic. Unfortunately, a digital image can only be as realistic as 
the limited color gamut, dynamic range, spatial resolution, .eld-of-view, and stereo-capacity that the 
display medium will allow. If we had a display medium which could produce the high luminances of real 
scenes, we would calculate the radiometric quanti­ties for each pixel in the two dimensional image lattice, 
and send the resulting lattice to the display. However, digital images are displayed on devices with 
from 256 to 1024 lu­minance levels and a maximum luminance of approximately 50 cd/m2 . To illustrate 
why this lack of intensity can hamper realism, consider the di.erence between the perception of a displayed 
*10201 N. De Anza Blvd., Cupertino, CA 95014. Greg Spencer@Taligent.COM. y580 ETC, Ithaca, NY 14850. 
shirley@graphics.cornell.edu. zLindley Hall, Bloomington, IN 47405. kuzimmer@cs.indiana.edu. x580 ETC, 
Ithaca, NY 14850. dpg@graphics.cornell.edu. Permission to make digital/hard copy of part or all of this 
work for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, 
and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 
ACM-0-89791-701-4/95/008 $3.50 Figure 1: Carl Saltzmann, Erste elektrische Straßenbeleuchtung in Berlin, 
Potsdamer Platz, 1884. digital image of a single white pixel on a black background, and the real experience 
of looking at a small incandescent bulb. The real bulb di.ers from the digital image in two important 
ways. The .rst di.erence is a qualitative bright­ness that the bulb possesses. The second di.erence is 
the hazy glow that can be seen around the bulb. This glow not only gives an impression of greater brightness, 
but it can also interfere with the visibility of objects near the bulb. We can improve the realism of 
simulated images by adding e.ects which perceptually expand and enhance the perceived dynamic range. 
These e.ects are most pronounced where bright light sources are visible within the scene. Perceptual 
e.ects which exaggerate the brightness of objects in an image have long been used in artistic expression. 
The impression­ists, in the late 19th century depicted the brightness of illu­minating sources by adding 
tell-tale radial lines (Figure 1). Cinematographers often add etched lenses to create special e.ects 
around lights, starbursts, or explosions to make them appear brighter than otherwise. Although these 
techniques are not psychophysically accurate, each produces the desired impression by exaggerating the 
luminance of the sources. The idea of adding glare e.ects to a digital image is not new. Nakamae et al. 
[20] pointed out that the limited dy­namic range of CRTs prevents the display of luminaires at their 
actual luminance values, and that adding streaking and blooming around the luminaires helps give the 
appearance of glare. While Nakamae et al. s glare algorithm is extremely e.ective in conveying an impression 
of luminaire intensity, it does not account for the visual masking e.ects of glare, which is needed for 
object-visibility prediction. 3.8° 3.5° 3.2° Lenticular2.9° Halo 2.6° 2.3° 2.0° Ciliary Corona Figure 
2: The ciliary corona and lenticular halo for a small white light source (after [29]). Our approach has 
been to model the physical e.ects, pri­marily caused by the interaction of light rays and the phys­iology 
of the human eye. For many years, researchers in optics, psychophysics, and illumination engineering 
have at­tempted to determine the mechanisms behind glare, and to quantify the e.ects of glare on viewers. 
A camera lens .lter that mimics the underlying mechanisms of glare in human vision has recently been 
developed, and had better results than conventional glare .lters for some e.ects [1]. Glare e.ects can 
be subdivided into two major compo­nents: .are and bloom. Flare is composed of a lenticular halo and 
a ciliary corona (Figure 2), and is primarily caused by the lens [29]. Bloom is caused by scattering 
from three parts of the visual system: the cornea, lens and retina (Fig­ure 3). The lenticular halo, 
ciliary corona, and bloom are the dominant contributing factors to glare e.ects and greatly a.ect our 
perception of the brightnesses of light sources. Rays of the ciliary corona appear as radial streaks 
ema­nating from the center of the source. Similar ray patterns associated with other coronas have been 
studied by physi­cists and are caused by random .uctuations in refractive index of the ocular media [15]. 
The lenticular halo is observed as a set of colored, con­centric rings, surrounding the light source 
and distal to the ciliary corona. The somewhat irregular rings are composed of radial segments, where 
the color of each segment of the ray varies with its distance from the source. The apparent size of the 
halo is constant and independent of the distance between the observer and the source. This phenomenon 
is caused by the radial .bers of the crystalline structure of the lens [29, 15]. Bloom, frequently referred 
to as veiling luminance is the glow around bright objects. Bloom causes a reduction in contrast that 
interferes with the visibility of nearby ob­jects, such as the night-time view of the grill between two 
car headlamps. Bloom is caused by stray light attributed to scatter from three portions of the eye: the 
cornea, the crys­talline lens, and the retina, all with approximately equal contributions [33]. The physiology 
of the eye and the resultant physical ef­fects are explained in greater detail in the following section. 
In Section 3 we develop the quantitative aspects of this glare in terms of the point-spread function 
of the human eye and present an algorithm for generating the digital .are .lter that approximates the 
point spread function. In Section 4 we describe a brief perceptual experiment which veri.ed Aqueous Humor 
Cornea Crystalline Lens Retina Incoming ray Scattered ray Photoreceptors Collector Cells Direction of 
photoreceptor's maximum sensitivity Magnification of Retina Figure 3: Scattering in the eye (after [28]). 
A small beam of light entering the eye is partially scattered in the cornea, the lens, and in the .rst 
layer of the retina. A B Figure 4: Glare around a nearby source A and distant source B. Since the halo 
subtends the same angle for each source, the halo around B has the illusion of being larger than the 
halo around A. the increase in perceived brightness. We conclude the pa­per showing several vivid examples 
and recommendations for future work.  2 Physiology and Physical E.ects The physical mechanisms behind 
glare have been studied since the late 19th century [29] and have been a matter of debate until quite 
recently [15]. In this section we present the physical origins of glare, drawing mainly on Simpson s 
work on lenticular haloes [29], and Vos [33] and Hemenger s [15] work on scattering in the eye. 2.1 Lenticular 
Halo When one observes a point source of light in a dark sur­round, there appears to be a series of concentric 
colored rings around the source. This is known as the lenticular halo (Figure 2). No matter how far away 
the source is from the observer, the haloes always subtend the same angle at the eye. As shown in Figure 
4, this creates an illusion that haloes around distant light sources appear larger than haloes around 
nearby sources. The intensity of the halo decreases with distance, and streaks are seen if the source 
subtends a su.ciently small solid angle. The lenticular halo is caused by the circular optical grat­ing 
formed by the radial .bers at the periphery of the crys­talline lens. This was .rst explained by Drualt 
in 1897 [29], and experimentally veri.ed by the Emsley-Fincham tests in 1922 [29]. A clear explanation, 
.rst presented by Simpson in 1953 is illustrated below. Figure 5(a) shows a biconvex lens with a circular 
grating etched into the outer portion of the lens. The axis of the lens is through the center, perpendicular 
to the plane of the paper, and meets the focal plane at point F.If we consider a small segment of the 
circular grating at G, where the lines of the segment are nearly parallel, we have a typical parallel 
di.raction grating. Light is refracted according to the following equation: . sine=, e where eis the 
angular deviation of the light path, .is the wavelength, and eis the distance between adjacent grating 
spaces. Thus, when white light is passed through the region G, and focused on the focal plane, the violet 
components appear at V, F,and V0, and the red components appear at R, F,and R0 . Thus two lines are formed, 
each one radiating outward and containing the full range of spectral colors. As we circumferentially 
traverse the circular grating, two over­lapping haloes are produced. The biconvex lens with the circular 
optical grating is ac­tually a simpli.ed model of the crystalline lens of the hu­man eye (Figure 5(b)). 
This is composed of .bers which are relatively large strips of transparent material having a cross-section 
of roughly hexagonal shape [10]. Although the central part of the lens is optically homogeneous, the 
exte­rior portions act as an optical grating with a spacing of e, the width of the .bers. A beam of light 
which is less than 3mmin diameter can pass through the clear portion of the lens, but subtending larger 
angles will always pass through the grating, thus cre­ating the lenticular halo. This means that haloes 
are not seen in daylight levels of illumination (when the pupil is 2mmacross) but is seen in darker conditions. 
 Figure 5: a) Diagram of the etched biconvex lens (after [29]). b) Cell structure of the Crystalline 
Lens (cell size exaggerated). Figure 6: A reduction in contrast that results from scattered light in 
the eye causes a reduction in contrast that depends on f, the angle of separation. 2.2 The Ciliary Corona 
The ciliary corona is depicted in Figure 2 and consists of rays emanating from a point light source. 
These radial rays may extend beyond the lenticular halo, and are brighter and more pronounced as the 
angle subtended by the source decreases (Figure 4). The ciliary corona is caused by semi-random density 
.uctuation in the nucleus of the lens which causes forward scattering that is independent of wavelength 
[15]. As the size of the source increases, it appears that the ciliary corona blurs and contributes to 
the bloom e.ect. This is because superimposing the .ne .are lines coming from each part of an areal source 
eliminates the crisp pattern of any given set of radial .are lines. Simpson observed that sources much 
larger than 20 minutes of arc did not have signi.cant .are lines. 2.3 Bloom Bloom, frequently referred 
to as disability glare or veiling luminance is best illustrated by the reduced visibility which occurs 
in the presence of bright light sources. This e.ect is attributed to the scattering of light in the ocular 
media, where the scatter contributions from the cornea, crystalline lens, and retina occur in roughly 
equal portions. This e.ect is illustrated in Figure 6 where light from source A scatters inside the eye 
and is added to light coming from object B. This scattered light adds an e.ective lumi­nance sthat does 
not originate at B. Because light is added to both the light and dark parts of object B,the contrast 
ratio L2/L1is reduced. In addition, since sensitivity to ab­solute luminance di.erence decreases as the 
base luminance increases, the di.erence between L1and L2might be dis­cernible, while the di.erence between 
s+L1and s+L2is not. The magnitude of sdepends on the angle of separation e, and the luminance and solid 
angle of the source. The quantitative details of this dependence will be discussed in Section 3. Veiling 
luminance has been the subject of investigations for almost two centuries, and there is still some controversy 
surrounding some of the details of the mechanisms for glare. It is evident that the stray or scattered 
light plays a dom­inant role [15], but neural inhibitory e.ects may also be present at very small angles 
of incidence [33]. It is not feasible to document the large number of psy­chophysical studies performed 
on this subject, and the reader is referred to the annotated bibliography of Ronci and Stefanacci [24], 
as well as the more recent studies of Owsley et al. [22], Ross et al. [25], and Ijspeert et al. [17]. 
Inves­tigations by Stiles [31], Cornsweet and Teller [9], Blackwell and Blackwell [3], Hemenger [15] 
and others all corroborate that the masking e.ect of glare is caused primarily by stray light. Direct 
evidence has also been obtained by observing the interior of the eye, revealing that the light scattering 
comes primarily from the cornea, crystalline lens, and retina (Figure 2). These cellular structures, 
many microns in di­ameter, scatter light independent of wavelength, much like a rough re.ecting surface. 
For the cornea and the lens, the light is scattered in a narrow forward cone with approximately a Gaussian 
dis­tribution [5]. The corneal scattering can be di.erentiated from the lenticular scattering since it 
casts a shadow of the iris on the retina. The retina to retina scattering, although physically in all 
directions is only important in the same forward directions due to the drastically reduced directional 
sensitivity of the cone system to obliquely incident light (the Crawford-Stiles e.ect). Because the rod 
sensitivity does not have as high a directional sensitivity as the cones, the magnitude of glare is greater 
in dark (scotopic) conditions. For these reasons, the light scattering is somewhat like a blurring or 
blooming e.ect with a sharp drop-o., and can be approximated with empirical formulae to match the experimental 
results.  3 Algorithm Although at a high level we understand the physical mech­anisms behind scattering 
in the eye, the exact structure of the cells in the eye is not known to the extent that we can simulate 
the scattering from .rst principles. In fact, current knowledge about the cell structure in the eye comes 
from inversion of observed scattering behavior [2, 15]. For this reason, we use psychophysical and phenomenological 
results in addition to physical modeling. If the eye is focused on a point source , then ideally a small 
discrete area of non-zero irradiance would fall on the retina. Because the eye is a real optical system, 
there will be some blurring of this signal on the retina. This blurring can be described by a point source 
function (PSF) for the eye. In Section 3.1 we describe quantitative glare models in terms of the PSF 
of the eye, and in Section 3.2 we show how this model can be simulated by convolving a radiometric image 
with a particular digital .lter kernel. 3.1 Quantitative Model of Glare There is approximate agreement 
on the exact perceptual contribution of the bloom for a normal viewer. Several researchers [16, 31, 11] 
have studied the magnitude of the glare e.ect by examining the threshold of visibility of an ob­ject 
near a source that produces illuminance Eat the front 0 of the eye. By turning the source o., and adding 
a back­ground luminance Lvthat makes the object barely visible, the equivalent veiling luminance Lvcan 
be found. This has led to empirical equations taking the general form: kE Lv(e)= 0 , (1) f(e) where Lvis 
the equivalent veiling luminance in cd/m2 , E0is the illuminance from the glare source at the eye in 
lx, kis a constant depending on the experimental conditions, eis the angle between the primary object 
and the glare source in degrees, and fis an experimentally determined function. Various values for kbetween 
3 and 50 have been used, and N f(e)is usually set to be eor (e+e0)Nwith Nranging from 1.5 to 3. Since 
the bloom is viewer-dependent, all of these values for kand fcan beconsidered to bein somesense reasonable, 
but recently an approximate consensus has been reached on the details of these parameters. The form of 
Equation 1 is somewhat confusing because it involves both luminance and illuminance. Vos has presented 
the equation in a less intimidating form by rewriting it as a point spread function (PSF). A PSF is a 
density (unit vol­ume) function de.ned on the visual .eld that describes how a unit volume point source 
(a delta function) is spread onto other points of the visual .eld. If we assume that the unscattered 
component of Equation 1 is unchanged (appears as an exact point source), then the PSF P(e)is: P(e)=a5(e)+ 
k, (2) f(e) where 5(e)is an ideal PSF and ais the fraction of light that is not scattered. The form of 
Equation 2 assumes that there is no energy loss in the system. This is not the case, and has been the 
cause of some debate in the glare literature. The perceived R fraction of light scattered in Equation 
2 (i.e. k/f(e))is roughly 10% for normal viewers. However, physical experi­ments suggest that as much 
as 40% of the light is actually scattered [26, 4]. Researchers have investigated this appar­ent contradiction. 
The most common explanation is that an­gular dependence of the sensitivity of the cones in the retina 
(the Crawford-Stiles e.ect [36]) e.ectively absorbs some of the stray light, particularly for emore than 
a few degrees. This same e.ect causes light transmitted by the outer edge of a fully dilated pupil to 
be 5-10 times less e.ective than light through the center of the pupil [33]. This implies that we should 
trust the ten percent .gure for our purposes be­cause it is the perceptual quality of the light that 
we need to account for. Thus, we should realize that Equation 2 rep­resents a normalized perceptual PSF 
and does not measure the spread of retinal illuminance. Recently, Vos has attempted to unify the large 
number of PSF models for the eye [33]. In this section we review Vos work, and add two e.ects studied 
by Hemenger [15] not accounted for in Vos model. If the point spread function is de.ned on the hemisphere 
of directions entering the eye, where Cis the angle from the gaze direction and cis the angle around 
the gaze direction, then Z27Z. 2 P(e)sinededc=1, (3) 00 where the angles are measured in radians. This 
normaliza­tion condition asserts that the PSF Predistributes energy, for the central Gaussian, 20.91 
f1(e)= , (e+0.02)3 for the e .3component, and 72.37 f2(e)= ,(e+0.02)2 for the e .2component. These functions 
are shown in Fig­ure 7. The function f0represents the unscattered component of the light, It shows the 
typical Gaussian shape expected for an real-world imaging system. This term should vary slightly with 
pupil size [5] but for our purposes could be re­ placed by a delta function because the angular size 
of the Visual Angle [degrees] Gaussian will be much smaller than a pixel width. Figure 7: The PSF components 
(a) f0,(b) f1,(c) f2, and (d) f3. Finally, Vos suggests the following combination for the PSF of a normal 
viewer: Pp(e)=0.384f0(e)+ 0.478f1(e)+ 0.138f2(e). (5) This PSF is subscripted with a p because it is 
appropriate for observers in a photopic (light adapted) state. The light adaptation of a viewer is described 
by one of three basic states [23]: less than 0.01 cd/m2is the scotopic region (night vision); the range 
0.01-3 cd/m2is the mesopic region (mixed night and day vision); more than 3 cd/m2is the photopic region 
(day vision) The graph of Equation 5 is shown in Figure 8. 108 6 4 2 0 2 4 6 810 Visual Angle [degrees] 
Figure 8: (a) The photopic PSF Pp. (b) The scotopic PSF Ps. but does not emit or absorb energy. If the 
optical system does absorb energy, this is accounted for by a constant sep­arate from the PSF. Because 
the glare literature reports its results in degrees, we can rewrite the PSF normalization condition for 
a P(e), where eis in degrees: . Z 90 .2 P(e)sinede=1. (4) 90 0 Any non-negative function of ethat satis.es 
Equation 4 is a candidate for a point spread function. This means that any weighted average of functions 
that each satisfy Equation 4 is also a candidate. Vos [33] has reviewed the various models for glare 
and noted that there are three di.erent empirical components in the PSF for an eye. The .rst is a narrow 
Gaussian that represents the unscattered component. The second is a function that is roughly proportional 
to e .3that is dominant for non-zero eless than one or two degrees. The third is a term proportional 
to e .2for emore than a degree. Because both the e .3and e .2terms would blow up near e=0, Vos replaces 
them with (e+a).2and (e+a).3for some empirical constant a, and suggests the following three normalized 
components: 2 6.() f0(e)=2.61 10e0 02, 3.1.1 Adding the lenticular halo For pupil diameters less than 
three millimeters, Simpson re­ports that the coherent .bers in the lens are blocked by the iris. The 
pupil diameter is in.uenced by many factors such as age, mood, and the spectral distribution of incoming 
light, but it is primarily related to the .eld luminance of the scene. Moon and Spencer (1944) [36] relate 
average pupil diameter D(in mm)to .eld luminance L(in cd/m2): D=4.9 3tanh(0.4(log10L+1)). This yields 
a pupil diameter of about 3mmfor L=10cd/m2 , which is the .eld luminance of a dimly lit interior. We 
should expect no lenticular halo in daylight conditions, a mild lenticular halo in dimly lit rooms, and 
prominent lentic­ular haloes for dark scenes. It was observed by Mallero and Palmer [18] that light at 
568nmcaused a lenticular halo of approximately 3.radius with an angular width of 0.35. . Based on this 
observation, Hemenger [15] used the following empirical formula to model the lenticular halo with these 
properties produced by light at 568nm: .19 75(e.e0)2 C(t)=Be , (6) where Bis a constant and e0 =3. . 
Since the angle of a di.raction pattern peak is propor­tional to wavelength, we can establish the formula: 
. e0(.)= . 568 Since we expect the same fraction of incident energy to be di.racted for each wavelength, 
we can construct a unit vol­ume PSF for the lenticular halo: 568.(e.3.)2 f3(e,.)=436.9e568. (7) . Mallero 
and Palmer also observed that in dark conditions the halo at 568nm had about ten times the luminance 
of the ciliary corona. From this fact, and from Equation 5 and Equation 7 we see that Pp(3.)=1.462and 
f3(3. ,568)= 436.9, so a reasonable coe.cient for f3will make the ring appear ten times as bright as 
PP, so the coe.cient we use is 101.462/436.9=0.033. However, this assumes a fully dilated pupil, which 
is only true for scotopic conditions. So we assume that about half of the radial .bers of the lens are 
exposed, calling for a coe.cient for f3of 0.016, resulting in the equation: Pm(e,.)=0.368f0(e)+ 0.478f1(e)+ 
0.138f2(e)+ 0.016f3(e,.). (8) This assumes the lenticular halo is diverted from the central peak. The 
subscript mrefers to a mesopic observer, whose pupil is large, but whose cones are still active. For 
darker conditions the amount of glare may be higher. At 0.15cd/m2there is 50% more straylight than at 
100cd/m2 [22]. This suggests an alternative form for the darkest point­spread function: Ps(e,.)=0.282f0(e)+ 
0.478f1(e)+ 0.207f2(e)+ 0.033f3(e,.). (9) The graph of Psis shown in Figure 8. 3.1.2 Viewer-Speci.c 
Variation in PSF There is an increase in glare with age, although the shape of the PSF stays the same 
[13]. If there are no cataracts, Vos [33] has established a rough age relation: Pp(e)=(0.3846.910.9A4)f0(e)+ 
0.478f1(e)+ (0.138+6.910.9A4)f2(e),(10) where Ais the age of the viewer in years. This change in vision 
is caused primarily by optical changes in the eye [12], but there is some loss due to neural changes 
as well [21]. Equation 10 implies that the fraction of light scattered more than 0.05.increases from 
0.36 at age 20 to 0.45 at age 60. The ratio of this light to unscattered light approximately doubles 
between the ages of 20 and 70. The situation can be even worse for viewers with cataracts, where the 
fraction of light that is scattered can be close to one [14]. There are no sex di.erences in the PSF 
[22], and only minor di.erences for viewers with di.erent pigmentation [17]. The formulas for Pmand Pswill 
have similar behavior to Pp, but the coe.cient for f3may remain relatively constant because the pupil 
diameter becomes more static with age. This may result in mild lenticular haloes in photopic conditions 
for old viewers. There is also an age-related color shift toward the yellow in the light transmitted 
by the lens. This, as well as increased .uorescence of the lens, can cause an additional reduction in 
visibility for elderly viewers in some viewing conditions such as twilight [6] and scotopic conditions 
[21]. The yellowing of the lens also causes metamerism to vary with age [8]. Since age-related changes 
vary widely from viewer to viewer, these studies are primarily useful for establishing guidelines for 
the design of environments that are safe for typical elderly viewers.  3.2 Digital Glare Filter Generation 
The glare formulae of Section 3.1 can be applied directly to digital images by using a digital point 
spread function to spread energy in high-intensity pixels to nearby pixels. This basic strategy has been 
used by Nakamae et al. [20] and Chiu et al. [7]. Unlike these previous approaches, we use di.erent .are 
.lters based on the adaptation state of the viewer. To develop a .lter for a particular image, we .rst 
construct digital versions of f0(e), f1(e), f2(e),and f3(e).Since each of these .lters must have unit 
volume, we can calculate each .lter proportional to a given function, and then renormalize the .lter 
so that all pixels sum to one. These .lters and the images they are applied to are stored in Ward s .oating 
point .le format [35], so that the small values in the o.­center .lter pixels are not lost. We compute 
the .lter for an NNimage, where N=2n+1so there is guaranteed to be a single central pixel. We number 
these pixels (0,0)through (N1,N1)and we calculate the angle tesubtended by the pixels of the image that 
we will add glare to. This assumes a relatively small .eld of view so that tecan be approximated by a 
constant for all pixels. To calculate the value d(i,j)for a particular .are component f(e)we evaluate 
the integral ZZ i+05 j+05 (p p(i,j)= fte(un)2+(vn)2dvdu. i.05 j.05 We use the trapezoidal rule to evaluate 
this integral. For f0, f1,and f2we use 10000 sample points in the central part of the .lter where there 
is rapid change in the function, and 100 sample points elsewhere. To construct the colors in the lenticular 
halo, f3(e,.), we process Equation 7 for 50 wavelengths from 400nmto 700nmandthenconvert to a trichromatic 
transform as described by Meyer [19]. This halo is not a classic pure spectrum because each wavelength 
bleeds into its neighbors, so it is not on the boundary of the visible part of the CIE diagram and is 
thus easier to display on a monitor gamut. The resulting pattern is shown in Figure 9 and is consistent 
with Simpson s observations (Figure 2). Unfortunately, the .lters calculated in this manner will lack 
the .are lines we expect (Figure 2), because they are spatial averages over areas that cover many .are 
lines. We need to add these .are lines without disturbing the macro­scopic structure suggested by Equations 
5, 8, and 9. We add .are detail to our digital .lter by drawing random an­tialiased radial lines of random 
intensity in the range [0,1] on a digital image the same size as our .lter. We draw a number of lines 
to visually match Simpson s observations (Figure 2). We assume that the ciliary corona (represented by 
f1(e)and f2(e)) is composed of one set of .are lines, and that the lenticular halo (represented by f3(e))is 
composed of a di.erent set of .are lines. This separation is consistent with the fact that there is a 
di.erent physical mechanism for the two components, as was discussed in Section 2. We take the random 
pattern of .are lines and adjust it so that it has an average pixel value of 1.0, and that each small 
neighborhood also has an average value of 1.0. This has the e.ect of increasing the pixel intensity radially 
because the fraction of pixels in a streak decreases radially from the Figure 9: Algorithmically generated 
lenticular halo. Compare to Simpson s observed values in Figure 2. center. This new pattern is then multiplied 
by the origi­nal .are functions, which gives them the appropriate detail without changing their carefully 
calculated macroscopic be­havior. This process is shown in Figure 10, for the particular case of Pm(e)(Equation 
8), where the .lter is built up in stages. The .lter is independent of a particular image, but must be 
recomputed for a new .eld-of-view because the angular size of a pixel changes. Thus only one .lter is 
computed for an animation sequence that uses one set of camera pa­rameters. The width and height of the 
.lter is double the width and height of the target image. Because the values in the .lter decrease away 
from the center (except for the lenticular halo which is approximately a factor of ten larger than its 
nearby interior neighbors), we can use only a central portion of the .lter when processing dim pixels. 
This enor­mously decreases the execution time (approximately a factor of 100 in our implementation on 
the images in Section 5). Because the viewer will experience actual glare for the dis­played pixels, 
we only need add glare to pixels whose full intensity is not displayed. So if the maximum displayable 
intensity is Im, and the computed intensity for a given pixel is I,where I.Im, the .lter is applied to 
the value IIm. Note that the .lter is applied at each of these bright pixels in the source image, which 
is spread to the appropriate regions of the destination image.  4 Perceptual Tests Once the techniques 
are developed to simulate .are and bloom, simple experiments can be conducted to determine the perceptual 
e.ects. In one simple experiment, two stimuli, one with a ciliary corona, and one without, were compared 
to see which one was perceived to be brighter. Each greyscale image was pre­sented in a window with a 
short presentation time, 400ms or 700ms. Colormap manipulation was implemented to con­trol the presentation 
time to within .10ms. Each image window was 300 by 300 pixels on the 1280 by 1024 display monitor. The 
presentation window was the only item visible on a screen with a black background. The basic staircase 
method was utilized in the exper­iment with a three-way forced choice (choices were Im­age A Brighter 
, Image B Brighter and Neither Image Brighter ). The staircase method refers to a method of decreasing 
the adjustment to a stimulus to converge on a threshold while virtually eliminating predictive bias [36]. 
Images were presented on a Sony Trinitron 19 inch display connected to a Hewlett-Packard 9000/750 with 
a VGRX Graphics card. One window contained an image with the ciliary corona .are .lter applied, and 
the other contained an identical image, except that the light source was replaced with a hardware-drawn 
disc surrounded by an annulus of one-third the intensity of the central disc (Figure 11(a)). The maxi­mum 
intensity of the source with the corona .are was 75% of the maximum value displayable by the display 
device. The intensity of the disc varied from 0 to 100% of the maximum value. Trials were arranged into 
two groups, with and with­out context for the source. In one group, the light source was presented by 
itself, in the center of a black .eld (Fig­ure 11(b)). In the other group, the light source was placed 
into the context of a light bulb at the end of a desk lamp on a desk (Figure 11(c)). The order in which 
these groups of trials was run was randomized half of the subjects per­formed the experiment with the 
context set .rst, and the other half observed the context free environments .rst. The glare images were 
also randomly swapped with the disc im­ages so that one type of image did not always appear in the Comparison 
of With and Without Context Perceived Intensity as Percentage of Maximum Displayable 100% 95% 90% 85% 
80% 75% 70% 65% 60% 55% 50% With Context Without Context Overall Average Actual Level 1 2 3 4 5 6 7 
8 910 11 12 Amount of Flare Figure 13: Overall results of experiment. Figure 11: Sample stimuli showing 
the lowest and highest .are intensity. Results of Trials With Context 100% Subject 1 95% Perceived Intensity 
as Percentage ofMaximum Displayable 30% The results indicate a fairly strong e.ect due to glare. This 
observation implies that applying a glare .lter improves the apparent dynamic range of an image.  5Examples 
The digital .lters of Section 4 were applied to several digital images. By contracting the .lter radius 
for dim pixels, we were able to run the .lters in approximately one to three minutes per image on a HP9000/755. 
All of the images are shown before and after application of the .are .lter. The scotopic PSF Pswas applied 
to a night scene (Fig­ure 14). Note that the haloes stay the same size for sources at di.erent distances. 
The cars in two .ltered images are at di.erent angles, so only the car on the bottom shows appre­ciable 
glare. Also notice that the brightness of the headlights at di.erent angles can only be detected once 
the glare is added. The images in Figure 14 we desaturated so that the saturation in HSVspace was reduced 
by 70% to simulate scotopic (rod) vision. The image was not completely desat­urated because color vision 
degrades gradually and is still partially active even under moonlight (about 0.03 cd/m2) 90% 85% 80% 
75% 70% 65% 60% 55% 50% 45% 40% 35%  Subject 2 Subject 3 Subject 4 Subject 5 Subject 6 Subject 7 Average 
   123456789101112 Actual Level Flare Intensity Figure 12: Stimulus with context. same side of the 
display. In both groups of trials, there were 12 glare images, ranging from a simple slightly blurry 
dot to a light source which had too much glare to be believable. The entire experiment took place in 
a darkened room, so that the intensity of the test images would appear brighter overall, and enhance 
the glare e.ect. 4.1 Experimental Results The experiment was conducted with a group of seven sub­jects, 
generating a statistically signi.cant sample, given the small deviation usually present in brightness 
perception ex­periments. The results, shown in Figures 12, and 13, show the ex­pected response to the 
glare increase. If the glare had no e.ect, then the perceived intensity would be constant, as shown by 
the horizontal line at the 75% level. The presence of a context (a lamp on a table) had no sig­ni.cant 
in.uence on the user s perception, but subjects did report having more con.dence about the absolute brightness 
of the light source when the context was present. [30]. Figure 15 shows an application of the mesopic 
PSF Pmto a rendered image. Note that the lenticular halo is prominent. Figure 16 shows an application 
of the photopic PSF Pp to a digital photo of a tree composited over a sky with lu­minance 4.0103cd/m2andasundiskof 
7.5108cd/m2 . The sun pokes through just a few holes in the leaves of the tree. Note that, as expected 
when viewing a bright scene, the lenticular halo is missing from Figure 16. All of these images have 
some burn-out, where the value of the pixel goes above one. Ultimately, a more sophisticated tone mapping 
algorithm should be used [32, 7, 34, 27], so that the images will have the appropriate degree of object 
visibility, and qualitative lightness or darkness. This issue is not addressed in this work.  6 Conclusion 
We have presented the mechanisms of glare in the human visual system, and have provided quantitative 
formulae used by the vision community that describe its magnitude. These mechanisms are scattering in 
the cornea, lens, and retina, and di.raction in the coherent cell structures on the outer radial areas 
of the lens. The scattering and di.raction are responsible for the bloom and .are lines seen around 
 Figure 15: An indoor simulation before and after the mesopic glare algorithm.. very bright objects. 
The di.raction e.ects are responsible for the lenticular halo . We have used these glare formulae to 
develop a digital point-spread function to add glare e.ects to digital images, and have run a perceptual 
experiment that indicates that the added glare increases the e.ective dynamic range in a digital image. 
Because the physically-based glare e.ects are expensive to compute, future work should focus on develop­ing 
e.cient methods that yield the same perceptual e.ects as the physically-based glare. Acknowledgements 
Thanks to Dan Kartch for help with the content and format of the paper, Andrew Kunz for help on image 
of the tree, Brian Smits for providing the theater image, Ben Trumbore and Bruce Walter for de­tailed 
comments on the paper, Alireza Esmailpour and Georgios Sakas for help obtaining a good reproduction of 
the Saltzmann painting, to Ken Torrance for proving important pointers into the optics litera­ture, and 
to Jim Ferwerda for help understanding the methodology of psychophysics. This work was supported by the 
NSF/ARPA Science and Technology Center for Computer Graphics and Scienti.c Visual­ization (ASC-8920219) 
and by NSF CCR-9401961 and performed on workstations generously provided by the Hewlett-Packard Corpora­tion. 
 References [1] Beckman, C., Nilsson, O., and Paulsson, L.-E. Intraocular scatterinng in vision, artistic 
painting, and photography. Applied Optics 33, 21 (1994), 4749 4753. [2] Bettelheim, F. A., and Paunovic, 
M. Light scattering of normal human lens 1. Biophysical Journal 26, 3 (April 1979), 85 99. [3] Blackwell, 
O. M., and Blackwell, H. R. Individual responses to lighting parameters for a population of 235 observers 
of vary­ing ages. Journal of the IES 2 (July 1980), 205 232. [4] Boettner, E., and Wolter, J. Transmission 
of the ocular me­dia. Investigative Ophthalmology 1 (1962), 776. [5] Campbell, F., and Gubisch, R. Optical 
quality of the human eye. Journal of Physiology 186 (1966), 558 578. [6] Carter, J. H. The e.ects of 
aging on selected visual functions: Color vision, glare sensitivity, .eld of vision, and accomodation. 
In Aging and Human Visual Function, R. Sekular, D. Kline, and K. Dismukes, Eds., vol. 2 of Modern Aging 
Research.Alan R. Liss, Inc., 1982, pp. 121 130. [7] Chiu, K., Herf, M., Shirley, P., Swamy, S., Wang, 
C., and Zimmerman, K. Spatially nonuniform scaling functions for high contrast images. In Graphics Interface 
93 (May 1993), pp. 245 244. [8] Coren, S., and Girgus, J. Density of human lens pigmentation in vivo 
measures over an extended age range. Vision Research 12, 2 (1972), 343 346. [9] Cornsweet, T. N., and 
Teller, D. Y. Relation of increment thresholds to brightness and luminance. Journal of the Optical Society 
of America 55 (1975), 1303 1308. [10] Davson, H.,Ed. The Eye, third ed., vol. 1a. Academic Press, inc. 
ltd., London, 1984. [11] Fry, G. A re-evaluation of the scattering theory of glare. Illu­minating Engineering 
49, 2 (1954), 98 102. [12] Hemenger, R. P. Intraocular light scatter in normal vision loss with age. 
Applied Optics 23, 12 (1984), 1972 1974. [13] Hemenger, R. P. Small-angle intraocular light scatter: 
a hy­pothesis concerning its source. Journal of the Optical Society of America A 5, 4 (1987), 577 582. 
[14] Hemenger, R. P. Light scatter in cataractous lenses. Opthal­mological Phyisiological Optics 10 (October 
1990), 394 397. [15] Hemenger, R. P. Sources of intraocular light scatter from in­version of an empirical 
glare function. Applied Optics 31,19 (1992), 3687 3693. [16] Holladay, L. Action of a light source in 
the .eld of view on lowering visibility. Journal of the Optical Society of America 14, 1 (1927), 1 15. 
[17] Ijspeert, J. K., de Waard, P. W. T., van den Berg, T., and de Jong, P. The intraocular straylight 
function in 129 healthy volunteers; dependence on angle, age, and pigmentation. Vision Research 30, 5 
(1990), 699 707. [18] Mellerio, J., and Palmer, D. A. Entopic halos and glare. Vi­sion Research 12 (1972), 
141 143. [19] Meyer, G. W. Wavelength selection for synthetic image gener­ation. Computer Vision, Graphics, 
and Image Processing 41 (1988), 57 79. [20] Nakamae, E., Kaneda, K., Okamoto, T., and Nishita, T. A lighting 
model aiming at drive simulators. Computer Graphics 24, 3 (August 1990), 395 404. ACM Siggraph 90 Conference 
Proceedings. [21] Ordy, J. M., Brizzee, K. R., and Johnson, H. A. Cellular al­terations in visual pathways 
and the limbic system: Implications for vision and short-term memory. In Aging and Human Visual Function, 
R. Sekular, D. Kline, and K. Dismukes, Eds., vol. 2 of Modern Aging Research. Alan R. Liss, Inc., 1982, 
pp. 79 114. [22] Owsley, C., Sekuler, R., and Siemsen, D. Contrast sensitivity throughout adulthood. 
Vision Research 23, 7 (1983), 689 699. [23] Rea, M. S.,Ed. The Illumination Engineering Society Lighting 
Handbook, 8th ed. Illumination Engineering Society, New York, NY, 1993. [24] Ronci, C., and Stefanacci, 
S. An annotated bibliography on some aspected of glare. Att. Fond. Ronci 30 (1975), 277 317. [25] Ross, 
J. E., Clarke, D. D., and Bron, A. J. E.ect of age on contrast sensitivity function: uniocular and binocular 
.ndings. British Journal of Opthalmology 69 (1985), 51 56. [26] Said, F., and Weale, R. The variation 
with age of the spectral transmissivity of the living human crystaline lens. Gerontologia 3, 4 (1959), 
213. [27] Schlick, C. Quantization techniques for visualization of high dy­namic range imagess. In Proceedings 
of the Fifth Eurographics Workshop on Rendering (June 1994), pp. 7 18. [28] Sekuler, R., and Blake, R. 
Perception, second ed. McGraw-Hill, New York, 1990. [29] Simpson, G. Ocular halos and coronas. British 
Journal of Opthalmology 37 (1953), 450 486. [30] Smith, G., Vingrys, A. J., Maddocks, J. D., and Hely, 
C. P. Color recognition and discrimination under full-moon light. Ap­plied Optics 33, 21 (1994), 4741 
4748. [31] Stiles, W. The e.ect of glare on the brightness di.erence thresh­old. Proceedings of the Royal 
Society of London 104 (1929), 322 351. [32] Tumblin, J., and Rushmeier, H. Tone reproduction for realis­tic 
computer generated images. IEEE Computer Graphics &#38; Applications 13, 7 (1993). [33] Vos, J. Disability 
glare-a state of the art report. C.I.E. Journal 3, 2 (1984), 39 53. [34] Ward, G. A contrast-based scalefactor 
for luminance display. In Graphics Gems IV,P.Heckbert, Ed.Academic Press, Boston, 1994, pp. 415 421. 
[35] Ward, G. J. The radiance lighting simulation and rendering system. Computer Graphics 28, 2 (July 
1994), 459 472. ACM Siggraph 94 Conference Proceedings. [36] Wyszecki, G., and Stiles, W. Color Science: 
Concepts and Methods, Quantitative Data and Formulae, second ed. Wiley, New York, N.Y., 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218467</article_id>
		<sort_key>335</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[Applications of irradiance tensors to the simulation of non-Lambertian phenomena]]></title>
		<page_from>335</page_from>
		<page_to>342</page_to>
		<doi_number>10.1145/218380.218467</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218467</url>
		<keywords>
			<kw><![CDATA[angular moment]]></kw>
			<kw><![CDATA[axial moment]]></kw>
			<kw><![CDATA[directional luminaire]]></kw>
			<kw><![CDATA[double-axis moment]]></kw>
			<kw><![CDATA[glossy reflection]]></kw>
			<kw><![CDATA[glossy transmission]]></kw>
			<kw><![CDATA[irradiance tensor]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Modeling and recovery of physical attributes</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14183802</person_id>
				<author_profile_id><![CDATA[81100529394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arvo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Engineering and Theory Center Building, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808589</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AMANATIDES, J. Ray tracing with cones. Computer Graphics 18, 3 (July 1984), 129-135.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>922822</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ARvo, J. Analytic Methods for Simulated Light Transport. PhD thesis, Yale University, 1995.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166137</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[AUPPERLE, L., AND HANRAHAN, P. A hierarchical illumination algorithm for surfaces with glossy reflection. In Computer Graphics Proceedings (1993), Annual Conference Series, ACM SIG- GRAPH, pp. 155-162.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BERGER, M. Geometry, Volume II. Springer-Verlag, New York, 1987. Translated by M. Cole and S. Levy.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CHRISTENSEN, P. H., STOLLNITZ, E. J., SALESIN, D. U., AND DEROSE, T. D. Wavelet radiance. In Proceedings of the Fifth Eurographics Workshop on Rendering, Darmstadt, Germany (1994), pp. 287-302.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[COOK, R. L. Distributed ray tracing. Computer Graphics 18, 3 (July 1984), 137-145.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[HOWELL, J. R. A Catalog of Radiation Configuration Factors. McGraw-Hill, New York, 1982.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J. T. The rendering equation. Computer Graphics 20, 4 (August 1986), 143-150.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[KROOK, M. On the solution of equations of transfer, I. Astrophysical Journal 122, 3 (November 1955), 488-497.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[MOON, P. The Scientific Basis of Illuminating Engineering. McGraw-Hill, New York, 1936.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[NIMROFF, J. S., SIMONCELLI, E., AND DORSEY, J. Efficient rerendering of naturally illuminated environments. In Proceedings of the Fifth Eurographics Workshop on Rendering, Darmstadt, Germany (1994), pp. 359-373.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325169</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[NISHITA, T., AND NAKAMAE, E. Continuous tone representation of 3-D objects taking account of shadows and interreflection. Compurer Graphics 19, 3 (July 1985), 23-30.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360839</ref_obj_id>
				<ref_obj_pid>360825</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[PHONG, B. T. Illumination for computer generated pictures. Communications of the ACM 18, 6 (June 1975), 311-317.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[POMRANING, G. C. The Equations of Radiation Hydrodynamics. Pergamon Press, New York, 1973.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[PREISENDORFER, R. W. Radiative Transfer on Discrete Spaces. Pergamon Press, New York, 1965.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SCHMID, E. Beholding as in a Glass. Herder and Herder, New York, 1969.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166138</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[SCHR6DER, P., AND HANRAHAN, P. On the form factor between two polygons. In Computer Graphics Proceedings (1993), Annual Conference Series, ACM SIGGRAPH, pp. 163-164.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SCHR6DER, P., AND HANRAHAN, P. Wavelet methods for radiance computations. In Proceedings of the Fifth Eurographics Workshop on Rendering, Darmstadt, Germany (1994), pp. 303-311.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[SHERMAN, M. P. Moment methods in radiative transfer problems. Journal of Quantitative Spectroscopy and Radiative Transfer 7, 89-109 (1967).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[SHIRLEY, P., AND WANG, C. Distribution ray tracing: Theory and practice. In Proceedings of the Third Eurographics Workshop on Rendering, Bristol, United Kingdom (1992), pp. 33-43.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[SPARROW, E. M. A new and simpler formulation for radiative angle factors. ASME Journal of Heat Transfer 85, 2 (May 1963), 81-88.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[SPIVAK, M. Calculus on Manifolds. Benjamin/Cummings, Reading, Massachusetts, 1965.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[VERBECK, C. P., AND GREENBERG, D. P. A comprehensive light-source description for computer graphics. IEEE Computer Graphics and Applications 4, 7 (July 1984), 66-75.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37438</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[WALLACE, J., COHEN, M. F., AND GREENBERG, D. P. A two-pass solution to the rendering equation: A synthesis of ray tracing and radiosity methods. Computer Graphics 21, 3 (July 1987), 311-320.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134078</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. Measuring and modeling anisotropic reflection. Computer Graphics 26, 2 (July 1992), 265-272.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[WARD, G. J. The RADIANCE lighting simulation and rendering system. In Computer Graphics Proceedings (1994), Annual Conference Series, ACM SIGGRAPH, pp. 459-472.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Applications of Irradiance Tensors to the Simulation of Non-Lambertian Phenomena James Arvo Program 
of Computer Graphics. Cornell University Abstract We present new techniques for computing illumination 
from non-di.use luminaires and scattering from non-di.use sur­faces. The methods are based on new closed-form 
expres­sions derived using a generalization of irradiance known as irradiance tensors. The elements of 
these tensors are angu­lar moments, weighted integrals of the radiation .eld that are useful in simulating 
a variety of non-di.use phenom­ena. Applications include the computation of irradiance due to directionally-varying 
area light sources, re.ections from glossy surfaces, and transmission through glossy surfaces. The principles 
apply to any emission, re.ection, or trans­mission distribution expressed as a polynomial over the unit 
sphere. We derive expressions for a simple but versatile sub­class of these functions, called axial moments, 
and present complete algorithms their exact evaluation in polyhedral en­vironments. The algorithms are 
demonstrated by simulating Phong-like emission and scattering e.ects. CR Categories and Subject Descriptors: 
I.3.7 [Com­puter Graphics]: Three-Dimensional Graphics and Realism. Additional Key Words and Phrases: 
angular moment, axial moment, directional luminaire, double-axis moment, glossy re.ection, glossy transmission, 
irradiance tensor. 1 Introduction Rendering algorithms are frequently quite limited in the sur­face re.ectance 
functions and luminaires they can accommo­date, particularly when they are based on purely determin­istic 
methods. To a large extent, this limitation stems from the di.culty of computing multi-dimensional integrals 
asso­ciated with non-di.use phenomena, such as re.ections from surfaces with directional scattering. 
While numerous closed­form expressions exist for computing the radiative exchange among uniform Lambertian 
(di.use) surfaces with simple ge­ometries [7, 12, 17], these expressions rarely apply in more general 
settings. Currently, the only approaches capable of simulating non-di.use phenomena are those based on 
Monte Carlo [6, 20, 25, 26], hierarchical subdivision [3], or numeri­cal quadrature [5, 18]. .580 Engineering 
and Theory Center Building, Ithaca, New York 14853, http://www.graphics.cornell.edu Permission to make 
digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage, the copyright notice, the 
title of the publication and its date appear, and notice is given that copying is by permission of ACM, 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Currently, few methods 
exist for computing semi-coherent re.ections of a scene in a nearly-specular or glossy surface. The earliest 
examples of glossy re.ection in computer graph­ics are due to Amanatides [1] and Cook [6]. Amanatides 
used cone tracing to simulate glossy re.ections for simple scene geometries and re.ectance functions. 
Cook introduced a general Monte Carlo method for simulating such e.ects that was later extended to path 
tracing by Kajiya [8] and applied to realistic surfaces by Ward [25]. Wallace et al. [24] approximated 
Phong-like directional scattering by rendering through a narrow viewing aperture using a z-bu.er. Aup­perle 
et al. [3] devised the .rst general deterministic method using three-point transfers coupled with view-dependent 
hi­erarchical subdivision. This paper o.ers the .rst analytic method for comput­ing direct lighting e.ects 
involving area light sources and a wide range of surfaces from di.use to highly directional: such e.ects 
include illumination from directional luminaires and view-dependent glossy re.ection and transmission. 
The method greatly extends the repertoire of e.ects that can be computed in closed form. The present 
work begins with a tensor representation of irradiance comprised of angular moments,or weighted in­tegrals 
of radiance with respect to direction [14]. Meth­ods based on angular moments have a long history in 
the .eld of radiative transfer [9, 19], but are applied here in a fundamentally di.erent way. In classical 
radiative transfer problems only low-order moments are relevant since detailed surface re.ections can 
generally be ignored [19]. For image synthesis, where surface re.ection is paramount, high-order moments 
can be used to capture the appearance of a non­di.use surface or the distribution of a directional luminaire. 
The idea of using high-order angular moments is ex­tremely general and applies to all emission and re.ectance 
functions that are polynomials over the sphere. The spe­ci.c algorithms that we present, however, address 
only a limited class of polynomial functions; essentially the Phong distributions [13]. This class of 
polynomials has a represen­tation that is simply related to irradiance tensors, and this leads to convenient 
closed-form expressions in polyhedral en­vironments. The expressions are not much more di.cult to evaluate 
than Lamabert s formula for irradiance [12, 17]; however, this does entail .nding the visible contours 
of lu­minaires when occlusions are present. The remainder of the paper is organized as follows. Sec­tion 
2 introduces basic de.nitions and motivates the concept of irradiance tensors, which is further developed 
in section 3. These tensors apply to a much larger class of functions than those explored in this paper. 
Using irradiance tensors, we derive expressions for axial moments in section 4; these are a simple but 
convenient form of moment with immediate applications. In section 5 we focus on polygonal luminaires 
and derive several closed-form expressions which are then applied to three related non-di.use simulations 
in section 6. 2 Radiometric Concepts In this section we introduce the fundamental radiometric concepts 
used in the following sections as well as several concepts that motivate the notion of irradiance tensors. 
Let f(r,u)denote a monochromatic radiance function [watts/m2sr] de.ned at all points r 2IR3 and directions 
u 2S2 ,where S2 is the set of all unit vectors in IR3.The function fcompletely speci.es the radiation 
.eld at large scales for a single wavelength. For .xed r 2IR3, the function f(r,.)is known as the directional 
distribution function1[15, p. 29]. The goals of this paper are simply to obtain a useful characterization 
of f(r,.)and apply it to the simulation of various direct lighting e.ects. Most radiometric quantities 
can be de.ned in terms of weighted integrals of radiance. We shall examine three such quantities that 
lead naturally to irradiance tensors. First, the monochromatic radiation energy density [15] at the point 
r 2IR3is de.ned by Z u(r)= 1 f(r,u)dI(u), (1) c S2 where cis the speed of light in the medium and Idenotes 
the canonical measure on the sphere [4, p. 276]; that is, I(A) is the surface area of any measurable 
subset ACS2 .The function u(r)is then the radiant energy per unit volume at r, with units [joules/m3]. 
Similarly, the vector irradiance [15] at r is de.ned by the vector integral Z <(r)=u f(r,u)dI(u), (2) 
S2 which has the units [watts/m2]. The scalar quantity <(r).v is the net .ux of radiant energy through 
a surface at r with normal v [15]. Finally, the radiation pressure tensor [14] at r is a symmetric 3x3matrix 
de.ned by Z 1 T (r)= uu f(r,u)dI(u), (3) c S2 where uu T denotes an outer product; this function has 
the units [joules/m3]. The bilinear form w T (r)v is the rate at which photon momentum in the direction 
w .ows across asurface at r with normal v.Thus, each of the above in­tegrals has a distinct interpretation 
and provides di.erent information about the radiance distribution function at the point r. Note that 
in equations (2) and (3), the integral is in ef­fect distributed across the elements of the vector or 
matrix. In equation (2) each element of the vector is a weighted integral of f(r,.)where the weighting 
functions are .rst­order monomials on the sphere; that is, the direction cosines x, y,and zwhere (x,y,z)=u 
2S2 . Similarly, in equa­tion (3) the weighting functions are the second-order mono­mials, x 2,y 2 ,z 
2 ,xy,xz,and yz. These scalar-valued inte­grals respectively correspond to .rst-and second-order an­gular 
moments of the radiance distribution function. 3 Irradiance Tensors The ideas of the previous section 
can be extended to higher orders using the formalism of tensors. Aside from the con­ 1Note that this 
function is distinct from a radiant intensity distribution, with the units of [watts/sr], which can be 
used to characterize a point light source. Figure 1: (a) The surface Pand its spherical projection .(P) 
onto the unit sphere S 2 . (b) For any region AS 2 , the unit vector n is normal to the boundary .Aand 
tangent to the sphere. stant 1ic, all of the integrals described in section 2 are mul­tilinear functionals 
of the form Z u ®...®u f(r,u)dI(u), (4) S2 where ®denotes a tensor product. In this view, radiation energy 
density, vector irradiance, and the radiation pres­sure tensor are tensors of order 0, 1, and 2 respectively, 
with equation (4) providing the natural extension to tensors of all higher orders. Given that fis a radiance 
function, this fam­ily of tensors generalizes the notion of vector irradiance, with each member possessing 
the units of irradiance [watts/m2]. Consequently, these new expressions are called irradiance tensors 
[2]. Although high-order irradiance tensors lack di­rect physical interpretations [14, p. 5], they are 
nevertheless useful vehicles for integrating polynomial functions over the sphere, as we demonstrate 
in this paper. In this work we restrict f(r,.)to be piecewise constant or piecewise polynomial over the 
sphere. Angular moments of fthen reduce to polynomials integrated over regions of the sphere. To concisely 
represent these integrals we introduce a simpli.ed form of irradiance tensor given by Z Tn(A)=u ®...®u 
dI(u), (5) . z A nfactors where ACS2 and n.0is an integer. The integrand of such a tensor contains all 
monomials of the form x i yjz k where (x,y,z)2S2 ,and i+j+k=n;thus, Tn(A)consists of nth-order monomials 
integrated over A. In what follows, the region ACS2 will represent the spherical projection of a luminaire 
PCIR3, which we denote by .(P). Without loss of generality, we may assume that the sphere is centered 
at the origin, as shown in Figure 1a. The tensors Tn(A)allow us to perform several useful com­putations; 
for example, we may compute angular moments of the illumination due to uniform Lambertian luminaires, 
or irradiance due to directionally varying luminaires. Irradiance tensors of all orders are de.ned by 
surface inte­grals, and in some cases may be reduced to one-dimensional integrals by means of the generalized 
Stokes theorem [22]. The resulting boundary integrals can be evaluated analyt­ically for certain patch 
geometries, such as polygons. The foregoing approach extends the classical derivation of Lam­bert s formula 
for irradiance [10, 21], yielding more complex boundary integrals in the case of higher-order tensors. 
To express the fundamental formula for Tn(A),we require some special notation. To conveniently index 
tensors of all orders, we shall use the n-index I =(i1,i2,,in),where ik2f1,2,3gfor 1.k.n. We de.ne Ikto 
be the kth sub­index, and Ilkto be the (n-1)-index obtained by deleting the kth sub-index. That is, Ilk=(i1,i2,,ik.1,ik+1,,in) 
Using this notation, we may concisely express the formula for irradiance tensors that will be our starting 
point. For all integers n.0and AS2, the tensor Tn(A)satis.es the recurrence relation n.1 X nn.2 (n1)T(A)= 
5(Ik,j)T IjI?k(A) k=1 Z -(u u)I njds,(6) 1A where I is an (n-1)-index, and Ijis the n-index formed by 
appending jto I. Here n denotes the outward normal to the boundary curveAas shown in Figure 1b, dsde­notes 
integration with respect to arclength, and 5(i,j)is the Kronecker delta, which is one if i=jand zero 
otherwise. Equation (6) follows from the generalized Stoke s theo­rem [2] and states that each tensor 
of the form de.ned in equation (5) can be reduced to a boundary integral and a term constructed from 
the tensor of two orders lower. The base cases T.1(A)=0and T0(A)= (A)complete the recurrence relation. 
It follows from equation (6) that Tn(A)can be computed analytically whenever the boundary integrals and 
base case can be. In particular, when Ais the spherical projection of a k-sided polygon and n=1, equation 
(6) yields Z k X 11 i T1 j(A)=-njds=-0inj, (7)22 1A i=1 where 0iis the length of the arc corresponding 
to the ith edge of the polygon, and n iis its outward normal. This is a well-known formula with numerous 
applications in com­puter graphics [12] originally derived by Lambert more than two centuries ago [17]. 
Although equation (6) is impractical computationally for moments of high order, it succinctly ex­presses 
the relationship among all the tensors. For instance, it is apparent that all even-order tensors incorporate 
solid angle T0(A), while the odd-order tensors do not. We now derive several useful formulas from this 
equation. 4 Axial Moments From equation (6) we may obtain expressions for individual moments or sums 
of moments without explicitly construct­ing the tensors. This is of great practical importance since 
the size of Tn(A)grows exponentially with n,yet only O(n 2) of its elements are distinct. We .rst consider 
the special case of moments about an axis, which de.nes a simple class of polynomials over the sphere. 
Given an arbitrary subset AS2and a unit vector w, we de.ne the nth axial moment of Aabout w by Z n T(A,w)=(w 
u)nd (u) (8) A As a cosine to a power, the polynomial weighting function within Tis essentially a Phong 
distribution centered around n Figure 2: Cross-sections of the weighting functions (w . u)nand (w . 
u)n(v . u) where v is the vertical axis; moment orders are 2, 4, 10, and 100, starting from the outer 
curves. the direction w, as shown in Figure 2a. These functions are steerable in the sense that they 
can be re-oriented without raising their order [11]. The ease of controlling the shape and direction 
of the lobe makes this polynomial function useful in approximating re.ectance functions, as Ward did 
with Gaussians [25], or an exact representation of simple Phong­like functions. To obtain a closed-form 
expression for T, n we begin by expressing the integrand of equation (8) as a composition of tensors: 
n (w u)=(u u)(w w) (9) II Here we have adopted the summation convention, where summation is implicit 
over all repeated pairs of indices; in equation (9) this means all sub-indices of I. It follows that 
nn T(A,w)=T(A)(w w), (10) II which associates the nth axial moment with the nth-order tensor. Using equation 
(6) to expand equation (10), and noting that 5(i,j)wiwj=w w,we obtain nn.2 (n1) T=(n-1)(w w) T Z -(w 
u)n.1 w n ds,(11) 1A where the function arguments have been omitted for brevity. Equation (11) is a recurrence 
relation for Twith base cases n T.1(A)=0and T0(A)= (A).When n0the recurrence relation reduces to a single 
boundary integral involving a polynomial in w u.Since w is a unit vector, we have Z 1A nqn.1n.3 (n1) 
T= T-(w u)(w u) q+1] (w u)w n ds,(12) where q=0if nis even, and q=-1if nis odd. This expres­sion is useful 
as a component of more general expressions, such as double-axis moments. 4.1 Double-Axis Moments An important 
generalization of equation (8) is to allow for moments with respect to multiple axes simultaneously; 
this will prove useful for handling radiant exchanges involving pairs of surfaces. We de.ne the double-axis 
moment of A with respect to w and v by Z n m(A,w,v)n T =(w u)(v u)md (u) (13) A n m A recurrence relation 
for T can also be obtained from equation (6) by expressing the integrand as a tensor compo­sition with 
ncopies of w and mcopies of the vector v.We Figure 3: (a) The solid angle of a spherical triangle is 
easily ob­tained from the internal angles. (b) Non-convex polygons can be handled by spoking into triangles 
from an arbitrary point Q. shall only consider the case where m=1, which corresponds to Tn+1(A)(w wv)and 
yields the formula n1n.1 (n2)TT(A,w,v)=n(wv)T(A,w) Z -(wu)n vn ds(14) 1A Figure 2a shows how an additional 
axis can change the shape of the weighting function. Note that when v =w,equa­tion (14) reduces to the 
(n1)-order axial moment given by equation (11). Recurrence relations for T n mwith m1 can be obtained 
in a similar manner, although the resulting boundary integrals are more di.cult to evaluate. Evaluating 
equations (12) and (14) in closed form is the topic of the next section. In section 6 we show how these 
moments apply to non-di.use phenomena.  5 Exact Evaluation Equations (6) and (12) reduce tensors and 
moments to one­dimensional integrals and (in the case of even orders) solid angle. This section describes 
how both of these components can be evaluated in closed form. Thus far no restrictions have been placed 
on the region AS2; however, we shall now assume that Ais the spherical projection of a polygon PIR3, 
which may be non-convex. The resulting projection is a geodesic or spherical polygon, whose edges are 
great arcs; that is, segments of great circles. When Pis a polygon, the computation of solid angles and 
boundary integrals are both greatly simpli.ed. Because the outward normal n is constant along each edge 
of a spherical polygon, the factors of wn and vn can be moved outside the integrals in equations (12) 
and (14) respectively. A sec­ond simpli.cation emerges in parametrizing the boundary integrals, as shown 
below. 5.1 Solid Angle The surface integral de.ning the solid angle subtended by P does not reduced to 
a boundary integral; this is because the corresponding di.erential 2-form is not exact [22, p. 131]. 
Fortunately, the solid angle subtended by a polygon can be computed directly in another way. If Pis a 
triangle in IR3 its projection .(P)is a spherical triangle 4ABC.Girard s formula for spherical triangles 
[4, p. 278] states that (4ABC)=af,- , (15) where a, f,and ,are the three internal angles, as shown in 
Figure 3a. The internal angles are the dihedral angles between the planes containing the edges. For instance, 
the angle ain Figure 3a is given by (BxA)(AxC) a=cos .1(16) jjBxAjjjjAxCjj Equation (15) generalizes 
immediately to arbitrary convex polygons [4, p. 279]. One way to compute the solid angle subtended by 
an arbitrary polygon is to cover its spherical projection with ntriangles, all sharing an arbitrary vertex 
Q2S2 . See Figure 3b. The solid angle is then the sum of the triangle areas signed according to orientation. 
In the .gure, 4QAB, 4QCD,and 4QDEare all positive, while 4QBCis negative, according to the clock-sense 
of the ver­tices. This method does not require .(P)to be decomposed into triangles. 5.2 Boundary Integrals 
The boundary integrals in equations (12) and (14) can be approximated by numerical quadrature, or evaluated 
ana­lytically in terms of O(n)elementary functions per edge. We shall only describe analytic evaluation 
and a related ap­proximation, both based upon a sum of integrals of the form Z y n Fn(x,y)=cosBdB (17) 
x These integrals may be evaluated exactly using a recur­rence relation given below. To express the integral 
in equa­tion (12) in terms of Fn(x,y)when Ais a spherical polygon, we parametrize the great arc corresponding 
to each edge by u(B)=s cosBt sinB, where s and t are orthonormal vectors in the plane of the edge, with 
s directed toward the .rst vertex. To simplify the line integral over a great arc (S2,let ibe the length 
of p the arc, and let a=ws, b=wt,and c=a2 b2.Then by the above parametrization, we have   ZZ£ n (wu)nds=[acosBbsinBrdB 
0 Z £ nn =ccos(B-c)dB, 0 n =cFn(-c,i-c), (18) where cis the angle satisfying cosc=alcand sinc=blc. To 
evaluate equation (18), we integrate equation (17) by parts to obtain the recurrence relation 1 n.1 n.1 
Fn(x,y)= cosysiny-cosxsinx n ] (n-1)Fn.2(x,y),(19) where F0(x,y)=y-xand F1(x,y)=siny-sinx.By means of 
this recurrence relation the function Fn(x,y)may be evaluated in b(n1)l2csteps. The complete integral 
in equation (12) is a weighted sum of these integrals for a sequence of di.erent exponents, which are 
all even or all odd. The corresponding sequence of integrals can be computed incrementally, as demonstrated 
in the following section. 5.3 Algorithms for E.cient Evaluation We now show that nth-order axial moments 
of k-sided poly­gons may be computed exactly (in the absence of roundo. error) in O(nk)time. The algorithm 
evaluates equation (12) in O(n)time for each of the kedges, using recurrence re­lation (19). The steps 
are broken into three procedures: CosSumIntegral, LineIntegral,and BoundaryIntegral .The .rst of these 
is the key to e.cient evaluation; procedure CosSumIntegral computes the sum kk+2n cFk(xPy)+cFk+2(xPy)+...+cFn(xPy)P(20) 
where k=mif m+nis even, and k=m+1otherwise, for a given integer m>0. The parameter mis included to accommodate 
both single-and double-axis moments. Be­cause recurrence relation (19) generates integrals of cosine 
with increasing powers, all integrals in expression (20) may be generated as easily as the last term 
c nFn(xPy).This strategy is embodied in the following procedure. CosSumIntegral( real xPyPc; int mPn) 
i+if even(n)then 0else 1; F+if even(n)then y-xelse siny-sinx; S+0; while i<ndo if i>mthen S+S+c i*F; 
i+1i+1 T+cosy*siny-cosx*sinx; F+[T+(i+1)*FJj(i+2); i+i+2; endwhile return S; end The next procedure 
computes the line integral corresponding to a polygon edge; the steps correspond to equation (18), summed 
over a sequence of exponents from mto n. LineIntegral ( vec APBPw; int mPn) if (n<0)or (w.Aand w.B) then 
return 0; s +Normalize [A J; t +Normalize [(I -ss T)B J; a+w .s; b+w .t; pc+a2+b2; g+angle between A 
and B; c+sign( b) *cos .1(ajc); return CosSumIntegral(-cPg-cPcPmPn); end The next procedure, BoundaryIntegral 
, computes the com­plete boundary integral for a given k-sided polygon Pby forming a weighted sum of 
kline integrals. The weight asso­ciated with each edge is the cosine of the angle between its outward 
normal and the second vector v, which may coincide with w. BoundaryIntegral ( pgon P; vec w, v; int mPn) 
b+0; for each edge AB in Pdo n +Normalize [A xB J; b+b+(n .v)*LineIntegral(APBPwPmPn); endfor return 
b; end With these three basic procedures we may now de.ne the procedure AxialMoment, which computes 
the nth axial mo­ment of a polygon Pwith respect to the axis w. Even-order moments also require the computation 
of a signed solid angle, which is handled by this procedure. Equation (12) then corresponds to the following 
function. AxialMoment( pgon P; vec w; int n) a+-BoundaryIntegral (PPwPwP0Pn-1); if even(n) then a+a+SolidAngle(P); 
return aj(n+1); end The function SolidAngle returns the solid angle subtended by the polygon Pusing 
the method described in section 5.1. Because the sign of the boundary integral depends on the orientation 
of the polygon, the solid angle must be simi­larly signed. Thus, SolidAngle is positive ifPis oriented 
counter-clockwise as seen from the origin, and negative oth­erwise. Finally, the procedure DoubleAxisMoment 
computes the nth-order moment of a polygon Pwith respect to the w axis and the 1st-order moment with 
respect to the v axis. Equation (14) then corresponds to the procedure DoubleAxisMoment ( pgon P; vec 
wPv; int n) if n=0then return AxialMoment(PPvPn); a+AxialMoment(PPwPn-1); b+BoundaryIntegral (PPwPvPnPn); 
return (n*a*w .v -b)j(n+2); end If we assume that trigonometric and other elementary func­tions are evaluated 
in constant time, then it is easy to see that procedures AxialMoment and DoubleAxisMoment both require 
O(nk)time. 5.4 Optimizations For clarity, the pseudo-code in the previous section does not depict a 
number of simple optimizations. For instance, the powers in procedure CosSumIntegral may be computed 
in­crementally. Also, in computing double-axis moments, a great deal of redundant computation may be 
avoided by al­lowing procedure CosSumIntegral to return one additional term in the series. These optimizations 
do not change the time complexity of the algorithms, but can signi.cantly re­duce the constant. Another 
means of speeding the computation is to settle for an approximation. Note that the terms in equation 
(12) decrease in magnitude monotonically because jFk+2j<jFkjfor all k,and 0<c<1. When the terms approach 
zero rapidly we may obtain an accurate approximation with little work. Moreover, by bounding the tail 
of the series it is possible to guarantee any given tolerance. For example, to compute a double-axis 
moment to a relative accuracy of E, the loop in CosSumIntegral may be terminated immediately upon updating 
Sif the condition  (v .n)c nFc kF +<EjSj (21) 2 (u .v)(u .n)1-c is met. In this case the tail of the 
series and the .nal integral in equation (14) may be dropped. Early termination of the loop is particularly 
useful with high orders; however, the test is costly and therefore should not be performed at every iteration 
of the loop. Figure 4: (a) Computing the irradiance at the point r due to a directionally-varying area 
light source Pis equivalent to (b) com­puting a double-axis moment of a uniform Lambertian source of 
the same shape.  6 Applications For ideal di.use surfaces irradiance is su.cient to compute re.ected 
radiance. The situation is dramatically di.erent for non-di.use surfaces, however. For surfaces that 
are nei­ther ideal di.use nor ideal specular, high-order moments can be used to quantify additional features 
of the incident illu­mination, in analogy with a power series expansion. For polygonal environments with 
emission and re.ection distri­butions de.ned in terms of moments, the procedures given in the previous 
section may be used to simulate directional luminaires, glossy re.ections, and glossy transmissions. 
6.1 Directional Luminaires Methods for simulating the illumination due to di.use area sources [12] and 
directional point sources [23] are well known; however, directional area sources are problematic for 
deter­ministic algorithms. In this section we shall see how a class of directional luminaires can be 
handled using double-axis moments. Let Pbe a polygonal luminaire whose emission distribu­tion is spatially 
uniform but varies directionally according to a Phong distribution; that is, as a cosine to a power [13]. 
For instance, the direction of maximum radiance may be normal to the plane of the luminaire, falling 
o. rapidly in other directions, as shown by the distribution in Figure 4a. The irradiance at the point 
r is then given by Z n ju .wjcosBd (u)P (22) . P0 where P0is the visible portion of the luminaire Ptranslated 
by -r,and Bis the angle of incidence of u;thus cosB=v.u, where v is the surface normal. Observe that 
this computa­tion is equivalent to a double-axis moment of a Lambertian source P,where the nth moment 
is taken with respect to -w and the factor of cosBis accounted for by the second axis v. See Figure 4b. 
Therefore, procedure DoubleAxisMoment can be used to compute the irradiance due to directional luminaires 
of this type in closed form. Figure 5 shows a simple scene illuminated by an area source with three di.erent 
directional distributions. Note that the areas directly beneath the luminaire get brighter with higher 
orders, while the surrounding areas get darker. Polygonal occlusions are handled by clipping the luminaire 
against all blockers and computing the contribution from each remaining portion, precisely as Nishita 
and Naka­mae [12] handled Lambertian sources.  Figure 5: (Left column) Irradiance from a directional 
luminaire computed analytically at each pixel; moment orders are 1, 10, and 20. (Right column) Analytic 
glossy re.ection of a window design by Elsa Schmid [16]; moment orders are 10, 45, and 400.  6.2 Glossy 
Re.ection A similar strategy can be used to compute glossy re.ections of polygonal Lambertian luminaires. 
Let r bea pointon a re.ective surface. Then the re.ected radiance at r in the direction u due to luminaire 
Pis given by Z 000 f(rPu)= p(u u)f(rPu )cosBd (u )P(23) . P where pis the bidirectional re.ectance distribution 
function (BRDF) and Bis the angle of incidence of u 0.Now consider a simple BRDF de.ned in terms a Phong 
exponent. Let n0 T T 0 p(u u) cuI -2vv u P (24) where cis a constant and v is the surface normal. Note 
that the Householder matrix I -2vv T performs a re.ection through the tangent plane at r. This BRDF de.nes 
a cosine lobe about anaxis inthe directionof mirror re.ection, as shown in Figure 6a. Because pobeys 
the reciprocity relation p(u 0 u)=p(u u 0), the radiance re.ected in the direction -u 0is found by integrating 
over the distribution shown in the .gure. To obey energy conservation the constant cmust be bounded by 
2 j(n+2).  Figure 6: (a) A simple BRDF de.ned by an axial moment around 0 the mirror re.ection w of 
u . By reciprocity, the radiance in the direction .u 0 due to Preduces to a double-axis moment of Pwith 
respect to w and v. (b) The boundaries used for normalization. For a uniform Lambertian luminaire P, 
the function f(rP.) is constant. In this case the integral in equation (23) reduces to a double-axis 
moment of Pwith respect to the vector wI -2vv T u 0P and the surface normal v. Procedure DoubleAxisMoment 
may therefore be used in this context as well, to compute the glossy re.ection of a di.use luminaire. 
The technique is demonstrated in Figure 5 using a variety of moment orders to simulate surfaces with 
varying .nishes. More complex BRDFs may be formed by superposing lobes of di.erent orders and/or di.erent 
axes. Other e.ects such as anisotropic re.ection and specular re.ection near grazing can be simulated 
by allowing cand nto vary with the incident direction u 0; doing so does not alter the moment computations, 
however reciprocity is generally violated.  Figure 7: A simple test scene with a glossy surface of order 
300. The enlargemetns of the re.ection were computed analytically (middle) and by Monte Carlo with 49 
samples per pixel (right). In Figure 7 an exact solution is compared with a Monte Carlo estimate based 
on 49 samples per pixel; the samples were distributed according to the BRDF and strati.ed to reduce variance. 
Both methods operated directly on non­convex polygons in the scene. The run times were compara­ble: 6.5 
seconds and 14.7 seconds respectively on an HP-755 workstation (100 MIPS). The analytic method eliminates 
the grainyness due to statistical noise and, in this instance, is also faster. The disparity in both 
time and quality is greater for lower moment orders and also for scenes with higher contrast.  6.3 Glossy 
Transmission As a .nal example, we note that glossy transmission can be handled in much the same way 
as glossy re.ection; the di.er­ence is in the choice of the axes w and v, which must now exit from the 
far side of the transparent material. Figure 8 shows two images depicting frosted glass , with di.erent 
.nishes Figure 8: A frosted glass simulation demonstrating glossy trans­mission. From left to right the 
moment orders are 10 and 65. corresponding to moments of di.erent orders. This e.ect was .rst demonstrated 
by Wallace et al. [24] using a form of stochastic sampling; in Figure 8 a similar e.ect has been computed 
analytically using procedure DoubleAxisMoment. 6.4 Normalization In applying the above methods, it is 
frequently useful to normalize the resulting distributions while ignoring negative lobes. We now show 
how this can be done for distributions de.ned in terms of double-axis moments. The two planes orthogonal 
to the axes of a double-axis moment partition the sphere into four spherical lunes (also known as spherical 
digons); let L1and L2be the two lunes in the positive half-space de.ned by v, as shown in Figure 6b. 
To normalize a BRDF, for example, we compute Z N(wPvPn)(w .u)n(v .u)d(u)P (25) L 2 where v is the surface 
normal; thus, the integrand of equa­tion (25) is positive for all exponents non the lune L2.Typ­ically, 
luminaires must be clipped by the planes de.ning this lune before computing the moments. Integral (25) 
can be evaluated analytically using equa­tion (14), which results in two boundary integrals corre­sponding 
to the arcs Cw and Cv. The special nature of the boundaries greatly simpli.es the computation. For instance, 
w .u is zero on Cw while v .n is one on Cv.Incorporat­ing these facts, and accounting for the minor di.erences 
due to parity, we obtain a very simple means of evaluating inte­gral (25) for arbitrary unit vectors 
w and v, and any integer n>0. The optimized code, which runs in O(n)time, is shown below. N ( vec wPv; 
int n) S+0; d+w .v; p c+1-d2; t+if even(n)then j2else c; A+if even(n)then j2else -cos .1d; i+if even(n)then 
0else 1; while i<n-2do S+S+T; T+T*c 2 *(i+1)j(i+2); i+i+2; endwhile return 2*(T+d*A+*S)j(n+2); d2 end 
  7 Conclusions We have presented a number of new closed-form expressions for computing the illumination 
from luminaires with direc­tional distributions as well as re.ections from and trans­missions through 
surfaces with a wide range of non-di.use .nishes. The expressions can be evaluated exactly in O.nk. time 
for arbitrary non-convex polygons, where nis related to the directionality of the luminaire or glossiness 
of the sur­face, and kis the number of edges in the polygon. To derive the new expressions we examined 
several well­known physical quantities as well as a tensor generalization of irradiance. Irradiance tensors 
satisfy a recurrence relation that subsumes Lambert s formula for irradiance and leads to expressions 
for axial moments and double-axis moments, which are quantities with direct applications in simulating 
non-di.use phenomena. The resulting formulas give rise to easily implemented algorithms. 8 Future Work 
An important extension of this work is to accommodate more realistic emission and re.ectance distributions 
by expand­ing the class of polynomials over the sphere; these may be constructed by superposing cosine 
lobes, by introducing ad­ditional axes, or by forming linear combinations from some appropriate set of 
polynomial basis functions. Useful extensions of equation (12) include non-integer or­ders and non-planar 
luminaires. Since equation (6) places no restriction on the boundaries, surfaces such as spheres also 
admit closed-form solutions; perhaps more general sur­faces do as well. Equation (6) can be extended 
to handle luminaires with spatially varying distributions, although the resulting expressions involve 
special functions such as diloga­rithms or Clausen integrals [2]. These expressions may prove useful 
in collocation-based methods for global illumination. Axial moments of with three or more axes and arbitrary 
orders are a more immediate extension of the present work. With this type of moment it is possible to 
combine the ef­fects demonstrated here; for example, glossy re.ections of directional luminaires. Another 
application would be the simulation of non-di.use surfaces illuminated by skylight us­ing the polynomial 
approximation of a skylight distribution proposed by Nimro. et al. [11]. It can be shown that all mo­ments 
of this form admit closed-form expressions, however e.cient algorithms for their evaluation do not yet 
exist.  Acknowledgments The author wishes to thank Peter Shirley for many valuable discussions, Albert 
Dicruttalo and Ben Trumbore for their assistance in modeling the stained glass window, and the anonymous 
reviewers for helpful comments. This research was supported by the NSF/ARPA Science and Technology Center 
for Computer Graphics and Scienti.c Visualization (ASC-8920219) and performed on workstations generously 
provided by the Hewlett-Packard Corporation. References [1] Amanatides, J. Ray tracing with cones. Computer 
Graphics 18, 3 (July 1984), 129 135. [2] Arvo, J. Analytic Methods for Simulated Light Transport.PhD 
thesis, Yale University, 1995. [3] Aupperle, L., and Hanrahan, P. A hierarchical illumination al­gorithm 
for surfaces with glossy re.ection. In Computer Graph­ics Proceedings (1993), Annual Conference Series, 
ACM SIG-GRAPH, pp. 155 162. [4] Berger, M. Geometry, Volume II. Springer-Verlag, New York, 1987. Translated 
by M. Cole and S. Levy. [5] Christensen, P. H., Stollnitz, E. J., Salesin, D. H., and DeRose, T. D. Wavelet 
radiance. In Proceedings of the Fifth Eurographics Workshop on Rendering, Darmstadt, Germany (1994), 
pp. 287 302. [6] Cook, R. L. Distributed ray tracing. Computer Graphics 18,3 (July 1984), 137 145. [7] 
Howell, J. R. A Catalog of Radiation Con.guration Factors. McGraw-Hill, New York, 1982. [8] Kajiya, J. 
T. The rendering equation. Computer Graphics 20, 4 (August 1986), 143 150. [9] Krook, M. On the solution 
of equations of transfer, I. Astro­physical Journal 122, 3 (November 1955), 488 497. [10] Moon, P. The 
Scienti.c Basis of Illuminating Engineering. McGraw-Hill, New York, 1936. [11] Nimroff, J. S., Simoncelli, 
E., and Dorsey, J. E.cient re­rendering of naturally illuminated environments. In Proceedings of the 
Fifth Eurographics Workshop on Rendering, Darmstadt, Germany (1994), pp. 359 373. [12] Nishita, T., and 
Nakamae, E. Continuous tone representation of 3-D objects taking account of shadows and interre.ection. 
Com­puter Graphics 19, 3 (July 1985), 23 30. [13] Phong, B. T. Illumination for computer generated pictures. 
Communications of the ACM 18, 6 (June 1975), 311 317. [14] Pomraning, G. C. The Equations of Radiation 
Hydrodynamics. Pergamon Press, New York, 1973. [15] Preisendorfer, R. W. Radiative Transfer on Discrete 
Spaces. Pergamon Press, New York, 1965. [16] Schmid, E. Beholding as in a Glass. Herder and Herder, New 
York, 1969. [17] Schr¨On the form factor between oder, P., and Hanrahan, P. two polygons. In Computer 
Graphics Proceedings (1993), An­nual Conference Series, ACM SIGGRAPH, pp. 163 164. [18] Schr¨oder, P., 
and Hanrahan, P. Wavelet methods for radiance computations. In Proceedings of the Fifth Eurographics 
Work­shop on Rendering, Darmstadt, Germany (1994), pp. 303 311. [19] Sherman, M. P. Moment methods in 
radiative transfer problems. Journal of Quantitative Spectroscopy and Radiative Transfer 7, 89 109 (1967). 
[20] Shirley, P., and Wang, C. Distribution ray tracing: Theory and practice. In Proceedings of the Third 
Eurographics Workshop on Rendering, Bristol, United Kingdom (1992), pp. 33 43. [21] Sparrow, E. M. A 
new and simpler formulation for radiative an­gle factors. ASME Journal of Heat Transfer 85, 2 (May 1963), 
81 88. [22] Spivak, M. Calculus on Manifolds. Benjamin/Cummings, Read­ing, Massachusetts, 1965. [23] 
Verbeck, C. P., and Greenberg, D. P. A comprehensive light-source description for computer graphics. 
IEEE Computer Graphics and Applications 4, 7 (July 1984), 66 75. [24] Wallace, J., Cohen, M. F., and 
Greenberg, D. P. Atwo-pass solution to the rendering equation: A synthesis of ray tracing and radiosity 
methods. Computer Graphics 21, 3 (July 1987), 311 320. [25] Ward, G. J. Measuring and modeling anisotropic 
re.ection. Computer Graphics 26, 2 (July 1992), 265 272. [26] Ward, G. J. The RADIANCE lighting simulation 
and render­ing system. In Computer Graphics Proceedings (1994), Annual Conference Series, ACM SIGGRAPH, 
pp. 459 472.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218470</article_id>
		<sort_key>343</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Specializing shaders]]></title>
		<page_from>343</page_from>
		<page_to>350</page_to>
		<doi_number>10.1145/218380.218470</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218470</url>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P32646</person_id>
				<author_profile_id><![CDATA[81100130209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guenter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P283183</person_id>
				<author_profile_id><![CDATA[81100628296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Knoblock]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P78858</person_id>
				<author_profile_id><![CDATA[81100400397]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ruf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Research, One Microsoft Way, Redmond, WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andersen, Lars Ole. Self-applicable C Program Specialization. Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation (San Francisco, California, June 12- 20, 1992). Yale University technical report YALEU/DCS/RR-909, 1992, 54-61.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Andersen, Peter Holst. Partial Evaluation Applied to Ray Tracing. Unpublished manuscript, October 1994.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baier, Romana, Robert Gltick, and Robert Z6chling. Partial Evaluation of Numerical Programs in Fortran. Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation (Orlando, Florida, June 25, 1994). University of Melbourne technical report 94/9, 1994, 119-132]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>567544</ref_obj_id>
				<ref_obj_pid>567532</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Demers, Alan J., Thomas Reps, and Tim Teitelbaum. Incremental Evaluation for Attribute Grammars with Application to Syntax-directed Editors. Proceedings of the Eighth Annual ACM Symposium on Principles of Programming Languages (Williamsburg, Virginia, January 1981), 105-116.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801277</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Goad, Chris. Special Purpose Automatic Programming for Hidden Surface Elimination. Proceedings of SIGGRAPH 82 (Boston, Massachusetts, July 26-30, 1982). In Computer Graphics 16, 3 (July 1982), 167-178.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801136</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, Pat. Ray Tracing Algebraic Surfaces. Proceedings of SIGGRAPH 83 (Detroit, Michigan, July 25- 29, 1983). In Computer Graphics 17, 3 (July 1983), 83-90.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97911</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, Pat and Jim Lawson. A Language for Shading and Lighting Calculations. Computer Graphics 24, 4 (August 1990), 289-298.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>143139</ref_obj_id>
				<ref_obj_pid>143095</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hoover, Roger. Alphonse: Incremental Computation as a Programming Abstraction. Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation (San Francisco, California, June 1992), 261-272.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>153676</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Jones, Neil D., Carsten K. Gomard, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice-Hall, 1993.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Mogensen, Torben. The Application of Partial Evaluation to Ray-Tracing. Master's thesis, DIKU, University of Copenhagen, Denmark, 1986.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Osgood, Nathaniel David. PARTICLE: an Automatic Program Specialization System for Imperative and Lowlevel Languages. Master's thesis, MIT, September 1993.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>75305</ref_obj_id>
				<ref_obj_pid>75277</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Pugh, William and Tim Teitelbaum. Incremental Computation Via Function Caching. Proceedings of the Sixteenth Annual ACM Symposium on Principles of Programming Languages (Austin, Texas, January 1989), 315-328.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>165439</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ruf, Erik. Topics in Online Partial Evaluation. Ph.D. thesis, Stanford University, April 1993. Published as Stanford Computer Systems Laboratory technical report CSL-TR-93-563, March 1993.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74365</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sequin, Carlo H. and Eliot K. Smyrl. Parameterized Ray Tracing. Proceedings of SIGGRAPH 89 (Boston, Massachusetts, July 31-August 4, 1989). In Computer Graphics 23, 3 (July 1989), 307-314.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>99587</ref_obj_id>
				<ref_obj_pid>99583</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sundaresh, R.S. and Paul Hudak. A Theory of Incremental Computation and Its Application. Proceedings of the Eighteenth Annual ACM Symposium on Principles of Programming Languages (Orlando, Florida, January 1991), 1-13.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Upstill, Steve. The RenderMan Companion. Addison- Wesley, 1989.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>573381</ref_obj_id>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Watkins, Christopher D., Stephen B. Coy, and Mark Finlay. Photorealism and Ray Tracing in C. M&amp;T Books, 1992]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>107217</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Watt, Alan and Mark Watt. Advanced Animation and Rendering Techniques. ACM Press, 1992.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Specializing Shaders Brian Guenter, Todd B. Knoblock, Erik Ruf * Microsoft Research Abstract We have 
developed a system for interactive manipulation of shading parameters for three dimensional rendering. 
The system takes as input user-defined shaders, written in a subset of C, which are then specialized 
for interactive use. Since users typically experiment with different values of a single shader parameter 
while leaving the others constant, we can benefit by automatically generating a specialized shader that 
performs only those computations depending on the parameter being varied; all other values needed by 
the shader can be precomputed and cached. The specialized shaders are as much as 95 times faster than 
the original user defined shader. This dramatic improvement in speed makes it possible to interactively 
view parameter changes for relatively complex shading models, such as procedural solid texturing. 1. 
Introduction During the process of interactively producing a computer­rendered image, a user will typically 
experiment with various image parameters, such as the positions of objects and light sources, the optical 
characteristics of surfaces, and surface textures. We would like to display the results of such parameter 
changes as rapidly as possible. Some of these changes, such as moving an object, are likely to require 
significant recomputation, while others, such as changing the color of one object, should not. This paper 
describes a mechanism for rapidly reacting to changes to shading parameters that control the local shading 
of objects in the scene. The local shading of an object is dependent only on object surface properties, 
such as the surface normal and surface position, and the geometric relationship of those properties with 
the illumination sources and the viewpoint. Global illumination effects, such as specular and diffuse 
interreflectance among objects, and shadows, are not part of the local shading model. Consider the simple 
Phong-like shader in Figure 1. If the user changes control parameter ks, only the multiplication ks*specular.component 
and the enclosing addition and vector multiplication operations need be reperformed; all other computations 
are guaranteed to return the same values as before. If, on the other hand, the parameter specular is 
changed, we may either have to recompute specular.component(and all expressions referring to it), or 
nothing at all, depending on the value of n.dot.h for that particular pixel. In either case, *Authors 
address: Microsoft Research, One Microsoft Way, Redmond, WA 98052. Email: {briangu, toddk, erikruf}@microsoft.com. 
Paper to appear in Proceedings of SIGGRAPH 95 (Computer Graphics Proceedings, Annual Conference Series, 
1995). Permission to make digital/hard copy of part or all of this work for personal or classroom use 
is granted without fee provided that copies are not made or distributed for profit or commercial advantage, 
the copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. 1 &#38;#169;1995 ACM-0-89791-701-4/95/008 
$3.50 changing a parameter value invalidates only a small portion of the work already performed. We take 
advantage of this observation as follows. Given an arbitrary user shading routine written in a subset 
of C and an indication of which parameter is to be altered, we perform an analysis to determine which 
of the shader s computations are invariant across changing values of the distinguished parameter. We 
then automatically generate 1 . code to perform the non-varying computations and save an appropriate 
subset of the results into a per-pixel cache area, and 2 . code to efficiently perform shading given 
the new parameter value and the cached information.  After executing (1) once, we need only execute 
the more efficient code in (2) each time the user alters the parameter. When the user selects a different 
parameter, we simply install new versions of (1) and (2). Figures 2 and 3 show the code resulting from 
the specialization of Phongon the parameter specular. The remainder of this paper is divided into five 
sections. Section 2 outlines the general architecture of our system, while Section 3 describes the specialization 
process in more detail. In Section 4, we analyze the costs and benefits of our approach on several shaders. 
We conclude, in Sections 5 and 6, with descriptions of related and future work.  2. System Architecture 
Our system consists of three main components: the geometric renderer, the specializer, and the user interface 
driver loop. 2.1 Geometric Renderer We render geometric image information offline before the interactive 
manipulation stage. Each pixel of the geometric image contains an object identifier associated with the 
object visible at that pixel, and the depth and surface normal of the point on the object surface which 
is at the center of the pixel. Our prototype uses a modified version of the Vivid ray tracer [17] to 
compute this information. During the interactive stage, the user loads a precomputed geometric image 
and then adjusts shading parameters on objects in the scene. Because the geometric image is rendered 
prior to the interactive stage, some image properties cannot be varied interactively. For example, the 
eyepoint is fixed, as are the position and orientation of all the objects in the scene. However, many 
scene characteristics, such as the position of lights, and the color and reflectance characteristics 
of objects, can be varied interactively. void Phong(int i, int j, Vec eye.position, Vec surface.normal, 
Flt depth, Matrix view.inverse, Matrix model.inverse, Matrix shade.inverse, PImageStruct dib, /* the 
following are the control parameters for this shader */ double red, double green, double blue, double 
specular, double ks, double kd, double ambient, double lx, double ly, double lz) { Vec light = {lx, ly, 
lz}; Vec world.position, light.vector, h.vector, eye.vector; double specular.component, diffuse.component, 
n.dot.h; Vec rgb = {red, green, blue};  world.position = trans.vector(view.inverse, vec(i, j, depth)); 
eye.vector = VecNormalize(VecSub(eye.position, world.position)); light.vector = VecNormalize(VecSub(light, 
world.position)); h.vector = VecNormalize(VecAdd(light.vector, eye.vector)); n.dot.h = VecDot(surface.normal, 
h.vector); if (n.dot.h > 0) specular.component = pow(n.dot.h, specular); else specular.component = 0; 
 diffuse.component = VecDot(surface.normal, light.vector); if (diffuse.component < 0) diffuse.component 
= 0;  rgb = VecScalarMul(rgb, (kd*diffuse.component + ambient) + ks*specular.component); SetPixelColor(j, 
i, rgb, dib); }  Figure 1: Source for a simple Phong shader. The parts of the shader that are dependent 
upon the parameter specular are underlined. 2.2 Specializer The specializer takes a user shader function 
and a designation of the control parameter to be varied and produces shading code specialized for that 
parameter. The user writes the shaders in a subset of C augmented with a library of useful graphics and 
mathematical functions. The framework is similar to that provided for writing shaders in RenderMan [7, 
16]. The restricted subset of C excludes the following C features: struct, union, pointer, and array 
types other than limited support for primitive data types such as vectors and points,  continue, break, 
and goto statements, and  recursion in the shader functions (although subroutines may employ recursion). 
 In some cases (e.g., struct types and structured loop exits), these restrictions simplify the implementation; 
in other cases (e.g., pointers, goto), they serve to make the necessary analyses tractable. The shader 
is responsible for calculating rgb values for each pixel based on image model information common to all 
shaders, and on shader-specific control parameters. Consider the sample shader, Phong, from Figure 1. 
The first 9 parameters of Phong, up through dib, are standard; they provide rendering data about each 
point and an output context. The remainder are the control parameters, and are associated with sliders 
in the user interface. Given a shader and a designated parameter, the specializer produces C++ code for 
two new procedures: a cache loader that precomputes and caches intermediate values that are independent 
of the designated parameter, and a cache reader that performs shading using the cached information. Figures 
2 and 3 show the generated loader and reader functions, respectively. We rely upon the compiler s optimizer 
to further improve the code by, e.g., eliminating dead code and hoisting loop-invariant computations. 
We install a new shader into the system by calling the specializer on the shader and each of the shader 
s control parameters, generating a loader/reader pair for each parameter. These procedures are then compiled 
and dynamically linked into the system.  2.3 Driver Loop The user interface driver loop allows the user 
to perform housekeeping tasks such as loading images, selecting shaders, and designating particular objects 
within the image for shading. It also allows the user to interactively modify the current shader s control 
parameters, and performs shading and image redisplay after each modification. Pseudocode for this portion 
of the driver is shown in Figure 4. After the user has selected a parameter to modify, the driver executes 
the corresponding cache loader to precompute the shader typedef struct { int c0; double c1; double c2; 
 } Cache;  world.position = trans.vector(view.inverse, vec(i, j, depth)); eye.vector = VecNormalize(VecSub(eye.position, 
world.position)); light.vector = VecNormalize(VecSub(light, world.position)); h.vector = VecNormalize(VecAdd(light.vector, 
eye.vector)); n.dot.h = VecDot(surface.normal, h.vector); if (pcache->c0 = n.dot.h>0) pcache->c1 = n.dot.h; 
else specular.component = 0; diffuse.component = VecDot(surface.normal, light.vector); if (diffuse.component<0) 
diffuse.component = 0; pcache->c2 = kd*diffuse.component+ambient; Figure 2: Body of the cache loader 
for the specialization with respect to specular. if (pcache->c0)  specular.component = pow(pcache->c1, 
specular); else specular.component = 0; rgb = VecScalarMul(rgb, pcache->c2+ks*specular.component); SetPixelColor(j, 
i, rgb, dib); Figure 3: Body of the cache reader for the specialization with respect to specular. repeat 
until new image/object/shader user selects a parameter P for each pixel in current object invoke loader[P] 
 repeat until new parameter P (*) user supplies a value V for P for each pixel in current object invoke 
reader[P](V) redisplay all pixels in object Figure 4: User interface driver loop. values that will be 
needed by the cache reader. Then, each time the user supplies a value for the chosen parameter, the cache 
reader is executed to perform the actual shading and the image is redisplayed. This runs more quickly 
because we have hoisted the parameter-independent computations out of the inner repeat loop (marked with 
*) in Figure 4.  3. Specialization The specializer begins by parsing and typechecking the shader function. 
The result is an abstract syntax tree (AST) annotated with type information. The core of the specialization 
process then proceeds in three steps. First, the dependence analysis determines which parts of the shader 
function are independent of the value of the designated control parameter.  Second, the caching analysis 
determines which of the independent computations should be cached.  Third, the splitting transformation 
separates the original shader function into cache loader and cache reader functions. Space limitations 
prevent us from presenting the specializer in detail. The next three sections present a simplified description 
of the three specialization steps. 3.1 Dependence Analysis Dependence analysis determines which parts 
of the computation are dependent or independent of the designated control parameter. The basic rule for 
dependence is that an expression is dependent on the designated parameter if one or more of its operands 
is. Although this basic rule would handle pure functional programs, the imperative assignment of variables 
in C requires a slightly more complicated analysis. A reference to a variable is dependent (on the designated 
control parameter) if the value assigned to the variable is dependent. It is also dependent if the choice 
of which (independent) value to assign is dependent. For example, if an assignment statement is guarded 
by an ifstatement whose predicate is dependent, then a downstream reference to the variable is dependent. 
In Figure 1, the parts of the function that are dependent upon the parameter specular are underlined. 
In this function, the first assignment to specular.component is dependent because it is assigned a value 
computed from the designated parameter, specular. This in turn forces the part of the calculation of 
rgb involving specular_component to be dependent. Finally, the call to SetPixelColor is dependent because 
it contains a reference to rgb,which is dependent because of the previous assignment. Our solution to 
dependence analysis is a straightforward application of standard dataflow analysis/abstract interpretation 
techniques. It works by flowing a model of the dependence of every variable though the shader function 
along the control paths. The final result of the dependence analysis is an annotation on every node of 
the AST as to whether or not its value may be dependent upon the designated control parameter. These 
annotations are necessarily conservative; assuming that a computation is dependent upon the control parameter 
when in fact it is not may make the specializer miss an optimization opportunity, but will not make it 
produce the wrong result.  3.2 Caching Analysis The caching analysis determines which of the independent 
values in the shader should be cached and which parts of the shader only need to occur in the cache reader. 
The results of the dependence analysis help to provide an upper bound on what might be cached. A priori, 
any independent computation is a candidate to be cached. However, independence is not sufficient to determine 
what should be cached. Consider the cache for the specialization of Phong on specular, represented in 
Figures 2 and 3. Since the value for kd*diffuse.component+ambientis cached, caching the either subexpression 
kd*diffuse.component, or the individual values of kd, diffuse.component, or ambient, would be inefficient. 
The caching analysis classifies each shader expression as either static, cache, or dynamic. Static expressions 
are those that need only be evaluated at load time. For example, in the assignment h_vector = VecNormalize(VecAdd(lightVector, 
eye_vector)); even though the right hand side is independent of the designated parameter, it does not 
need to be cached because the only reference to h_vectoris itself cached. Cache expressions are also 
evaluated at load time, but are consumed by dependent computations, so their results must be stored into 
the cache for later use by the cache reader. Dynamic expressions need to be computed during cache reading. 
This classification is performed using simple heuristics. An expression is marked as cache if it is independent, 
is the maximal such expression, is governed only by independent control predicates, and is not within 
a dependent while statement. Independent, non-cached expressions that do not directly contribute to dependent 
computations are marked as static. The remaining expressions are marked as dynamic. In the Phongshader, 
three expressions are selected for caching:1 n.dot.h>0,  n.dot.h, and  kd*diffuse.component+ambient. 
 All of the assignment statements other than the assignments to specular.component and rgb are marked 
as static. The 1The reason that n_dot_h>0 and n_dot_h are both cached is that there are distinct (dependent) 
consumers of these values in the cache reader. One could choose to cache just n_dot_h and recalculate 
n_dot_h>0in the cache reader. AST MakeLoader(AST e) { switch on mark(e) case Static: return e case Cache: 
allocate a cache slot s for e return MakeAssignment(s,e) case Dynamic: switch on form of e case e is 
an expression e0.e1: return MakeExp(., MakeLoader(e0), MakeLoader(e1)) case e is if (e0) e1 else e2: 
if (e0 is independent) return MakeIf(MakeLoader(e0), MakeLoader(e1), MakeLoader(e2)) else return MakeLoader(e0) 
 case e is while(e0) e1: if (e0 and e1 are independent) return MakeWhile(MakeLoader(e0), MakeLoader(e1)) 
else return Ø case ... } Figure 5: The cache loader transformation. AST MakeReader(AST e) { switch mark(e) 
case Static: return Ø case Cache: find cache slot s allocated for e return s case Dynamic: switch on 
form of e case e is an expression e0.e1: return MakeExp(., MakeReader(e0), MakeReader(e1)) case e is 
if (e0) e1 else e2: return MakeIf(MakeReader(e0), MakeReader(e1), MakeReader(e2)) case e is while(e0) 
e1: return MakeWhile(MakeReader(e0), MakeReader(e1)) case ... } Figure 6: The cache reader transformation. 
 first ifstatement is dynamic because its then-part is dependent, but the second ifis static.  3.3 Splitting 
Transformation In its third and final phase, the specializer derives cache loader and cache reader functions 
from the AST, which has been annotated with the results of the dependence and cache analyses. Figures 
5 and 6 present partial pseudo-code for the transformations that produce the cache loader and cache reader. 
The transformations are based solely upon the annotations and a case analysis on the form of the AST 
node e: If the node eis marked as static, then by definition it only needs to be included in the cache 
loader.  If the node eis marked as cache, then the loader allocates a cache slot for it and generates 
an assignment statement in the cache loader function that assigns the slot the value of the expression. 
The corresponding piece of the reader simply reads the cache slot.  If the node eis a dynamic ifstatement 
and it has a test that is independent of the designated parameters, then the loader may load cache values 
in the branches of the if. Such loading cannot be performed if the test is dependent, since the cached 
expressions will no longer be guarded by the test, possibly inducing a run-time exception that would 
not have occurred in the original program.  If the node e is a dynamic while statement, we conservatively 
choose not to cache anything within it. This implementation misses some cases where an independent value 
could be cached: where (1) the independent value is loop invariant or (2) where the only dependent consumers 
are outside the loop.  Figures 2 and 3 contain the results of running the splitting transformation on 
the specialization of Phongfor specular. The first five assignments of the shader are static, and are 
reproduced only in the loader. The first ifstatement is dynamic with an independent test; thus a version 
of the ifappears in both the loader and the reader. Both the predicate and the reference to n.dot.h are 
cached, and are thus assigned in the loader and refernced in the cache reader. The assignment to identifier 
specular.component is actually dead in the loader--we would expect the compiler optimizer to remove it. 
The second if is static, and so is included only in the loader. The third and final cache element represents 
the maximally independent expression kd*diffuse.component+ambient. 3.4 Pragmatics Cache space is a valuable 
resource. The pseudo-code in figure 5 implies that new space is allocated for each value that is cached, 
which would be correct but inefficient. For example, there are situations where two cached values may 
be correctly allocated to overlapping space in the cache, e.g., when only one of the values will be live 
on any given invocation of the cache loader. Our specializer employs some simple rules to more efficiently 
allocate cache slots. Cache space is also a finite resource, as performance will suffer if the total 
space allocated for the cache exceeds the available physical memory. For the moderate-complexity shaders 
we have specialized thus far, cache size has not been an issue. In the specialization of more complex 
shaders, it will be necessary to explicitly limit the amount of cache space that is allocated. To do 
this effectively, we plan to introduce a cost metric that approximates the cost of each computation that 
is a candidate for caching. This would then be used to guide what should be cached and what should be 
recomputed in the cache reader by caching only the top-scoring candidates. We can simultaneously improve 
cache size and reader efficiency by rewriting computations in the shader. One such opportunity is the 
associative rearrangement of arithmetic expressions.2 For example, the expression (x+y)+z contains two 
computations that are independent of x, namely y and z. But if we reassociate it as x+(y+z), this contains 
just one independent computation, y+z. The transformed code requires one less cache slot, and eliminates 
one + operation in the cache reader (Hanrahan and Lawson [7] describe a similar optimization). Thus far, 
we have described specializing shaders on a single control parameter. The specializer, in fact, supports 
specializations on multiple parameters. So, for example, one could produce a specialization that would 
allow the fast manipulation of any of the red, green, or blueparameters without having to call the loader 
again. For combinations of control parameters, there are too many possible specializations for them all 
to be generated in advance. We plan to extend the system to dynamically call the specializer at run-time 
when a subset of the parameters is designated by the user.  4. Results The specialization algorithm 
described above has been implemented in a prototype system consisting of approximately 20K lines of C++, 
Flex, and Bison code. The user interface driver loop is implemented in a small amount of Visual Basic. 
This section presents timing results for specialized shaders. 4.1 Timings We show results of the specializations 
of two shaders: the Phong shader of Figure 1, which has 10 control parameters, and a procedural solid 
texture shader, Texture, having 18 control parameters. These shaders were tested on the 320 by 243 pixel 
image shown in Plate 1. This image consists of five objects, including the background plane. We chose 
to time our shaders on the background plane because it is the largest object in the image. The timings 
were performed on a Gateway 2000 P5-90 Pentium processor with 64 megabytes of physical memory. The performance 
of the specializer is not an issue; specialization is performed offline and typically requires less than 
one second per specialization generated. Thus, we present timings only for the shaders themselves. Four 
statistics are of interest: 1 . the time to execute the original shader, 2 . the time to precompute 
the cached values,  2While floating point arithmetic does not obey the mathematical associativity laws, 
as long as the rewritten expression does not overflow, the effects are unlikely to be observable in a 
shading function. lz ly lx ambient kd ks specular blue green red 02468 Figure 7: Relative speedup due 
to specialization for Phong for each control parameter. scale_z scale_y scale_x r2 r1 ambient kd ks specular 
blur amplitude flt_terms blue1 blue0 green1 green0 red1 red0  Figure 8: Relative speedup due to specialization 
for Texture for each control parameter 3 . the time to perform shading given the cached and the control 
parameter values, and 4 . the ratio of (1) and (3). Figures 7 and 8 show the speedup ratio (4) for both 
shaders. These ratios vary from 1.4 through 7.4 for Phong, and 1.0 through 97.0 for Texture. Table 1 
gives the absolute data for Texture; the table of absolute data for Phong is omitted to save space. 
 4.2 Discussion The overhead of our two-phase shading approach is the time taken to run the cache loader. 
In our test cases, the sum of the loader and reader times is approximately equal to the time taken for 
a single invocation of the unspecialized shader. The cost of the additional memory operations to load 
the cache is balanced by the removal of statements that do not contribute to the cached values. When 
the user first selects a new control parameter, the system must execute both the cache loader and reader; 
thus, the first shading may be slower than the original, unspecialized shader. Subsequent parameter changes 
only execute the cache reader; thus, our interactive speedup is the ratio of the original and reader 
times. The user will experience this speedup most of the time when interacting with the system. The speedup 
ratio is highly dependent upon the particular control parameter chosen because we may only cache the 
results of computations that are independent of this parameter. In some cases, virtually all of the computations 
depend upon the chosen parameter, e.g., lx in Phong and flt.terms in Texture. Speedups for such cases 
are low, approaching 1, but never less than 1. In the majority of cases, however, the cache reader is 
much faster than the unspecialized shader, meaning that we achieve a net gain in processing speed when 
the user changes a parameter twice or more successively. Figures 7 and 8 demonstrate that the majority 
of parameters are computed significantly faster in specialized form. For example, 7 of the 10 parameters 
in Phongwere at least 3 times faster, and 6 parameters were at least 7 times faster, in specialized form. 
The speedup ratios are even more dramatic for the procedural solid texture shader. Of 18 parameters, 
14 were at least 25 times faster and 9 were at least 90 times faster. The Texture shader contains several 
expensive computations that depend on only a few control parameters, allowing significant speedups in 
specializations on other parameters. Parameter User Shader Cache Loader Cache Reader Ratio scale_z 31.14 
2.24 30.54 1.02 scale_y 31.15 2.43 30.46 1.02 scale_x 31.14 2.24 30.22 1.03 r2 31.27 32.25 0.70 44.95 
r1 30.95 32.11 0.72 42.82 ambient 31.15 31.33 0.33 93.03 kd 31.15 31.22 0.34 92.69 ks 31.15 31.02 0.33 
93.26 specular 31.11 30.70 0.80 38.80 blur 31.15 31.32 0.48 65.45 amplitude 31.15 32.43 1.50 20.83 flt_terms 
44.81 2.23 43.08 1.04 blue1 31.15 31.25 0.32 95.87 blue0 31.15 31.47 0.33 95.17 green1 31.15 31.06 0.32 
96.94 green0 31.15 31.03 0.33 95.64 red1 31.15 31.07 0.33 95.20 red0 31.15 31.24 0.32 97.05 Table 1: 
Execution time in seconds for the user shader, cache loader, and cache reader, and ratio of user shader 
time to reader time. Color Plates 1 and 2 show a typical incremental control parameter change. In Plate 
1, the amplitude parameter of the procedural texture shader for the tiled floor object is set to 12. 
In plate 2, it has been changed to 6. The time to reshade the image following this change was approximately 
12 seconds for the original (unspecialized) shader and 0.5 seconds for the specialized shader.  5. 
Related Work Relevant work includes the specialization of programs on partial input and the automatic 
construction of incremental versions of programs. 5.1 Partial Evaluation The specialization of programs 
on partial input specifications is known as partial evaluation [9,13]. A partial evaluator takes a program 
and values for some of its inputs, and produces a new, specialized, program parameterized by the remaining 
inputs, while a partial evaluator generator takes a program and produces a transformer which, given some 
input values, generates the appropriate specialized program, eliminating the need to analyze the original 
program each time a specialization is needed. General-purpose partial evaluators for imperative languages 
include Andersen s C-Mix [1] and the system of Baier et al. [3], which process programs written in subsets 
of C and Fortran, respectively. Osgood [11] addresses the full C language, but the large time and space 
costs of his algorithm limit its applicability. Our system can be viewed as a partial evaluator generator 
because it processes the original program only with respect to particular argument positions, generating 
a program (the cache loader) that later makes use of particular argument values. However, our approach 
differs from traditional partial evaluation strategies in that we are not limited to constructing specialized 
program text; we also construct specialized data values (such strategies are referred to as mixed computation 
in the literature). Applying a partial evaluator to our driver loop would yield a specialized shading 
procedure for each pixel in the image, whereas we limit ourselves to constructing specialized intermediate 
data values for each pixel, which are then processed by a single specialized shading procedure common 
to all pixels. The traditional approach might achieve better performance because of per-pixel code customization 
but is impractical due to the huge amount of specialized code that would be generated. Several researchers 
have applied partial evaluation to ray-tracing by using a general-purpose partial evaluator to specialize 
a general intersection routine with respect to a scene, producing a specialized ray-tracer parameterized 
only by the viewpoint. Mogensen [10] partially evaluated a simple ray-tracer coded in Lisp and got a 
two-fold speedup; when shading was added to the ray-tracer, the speedup increased to 8 times. Andersen 
[2] performed a similar experiment in C, and achieved speedups of 1.8 with a code size blowup of 15-90 
times. Other researchers used somewhat more application-specific specialization techniques. Hanrahan 
s surface compiler [6] symbolically simplifies input equations defining a general algebraic surface, 
then outputs a C program that uses numerical methods to find roots of the resulting polynomial. He reported 
a speedup of 1.3. Goad [5] proposed specializing a hidden surface elimination routine with respect to 
a fixed scene and variable viewpoint. In the area of mixed computation, Sequin [14] used ray tree caching 
to significantly speed up shading computations for a ray traced image. 5.2 Incremental program execution 
We know of two main approaches to efficiently handling incremental changes. One strategy encodes the 
program as a set of constraints, and explicitly reestablishes the constraints after each input change. 
Attribute grammars [4] are one example of a constraint system with an efficient incremental solver. Constraint 
systems are common in graphics; however, they are primarily used for their convenience and generality 
rather than as a foundation for efficient incremental execution. Most other approaches are similar to 
ours in that they cache intermediate results of an existing batch program. Recent research on memoization 
for functional programs includes improved caching strategies [12] and methods for memoizing specialized 
functions instead of data values [15]. There is also research on adding dependence information to existing 
imperative code [8]. Incremental execution techniques are general, but suffer in performance due to the 
need to dynamically process arbitrary input changes. Our restricted application domain, in which the 
user performs a series of repeated modifications to individual scalar inputs, makes it practical for 
us to perform the dependence analysis and caching transformations statically, and to rebuild the intermediate 
value cache using batch rather than incremental methods.  6. Conclusion We have described an automatic 
method for improving the performance of user-written shading routines by specializing them with respect 
to individual control parameters. By precomputing, caching, and reusing intermediate results that will 
remain constant as the user experiments with a single control parameter, the specialized shaders achieve 
up to a factor of 95 improvement in performance, allowing even complex shaders to be used in an interactive 
environment. We plan to extend this work in several ways. The analyses and transformations will be extended 
to handle a larger subset of the C language, and to perform additional optimizations such as associative 
rearrangement of expressions and cost-based allocation of cache storage. Specializing the mathematical 
library routines (as opposed to only their user-level clients) may also achieve a significant gain in 
performance.  7. Acknowledgements The authors would like to thank Stephen Coy for the use of his ray 
tracer and Greg Kusnick, Daniel Ling, Ellen Spertus, and the reviewers for helpful comments on previous 
drafts of this paper.  Plate 1: Test image.  Bibliography [1] Andersen, Lars Ole. Self-applicable 
C Program Specialization. Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based 
Program Manipulation (San Francisco, California, June 12­20, 1992). Yale University technical report 
YALEU/DCS/RR-909, 1992, 54-61. [2] Andersen, Peter Holst. Partial Evaluation Applied to Ray Tracing. 
Unpublished manuscript, October 1994. [3] Baier, Romana, Robert Glück, and Robert Zöchling. Partial Evaluation 
of Numerical Programs in Fortran. Proceedings of the ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based 
Program Manipulation (Orlando, Florida, June 25, 1994). University of Melbourne technical report 94/9, 
1994, 119-132 [4] Demers, Alan J., Thomas Reps, and Tim Teitelbaum. Incremental Evaluation for Attribute 
Grammars with Application to Syntax-directed Editors. Proceedings of the Eighth Annual ACM Symposium 
on Principles of Programming Languages (Williamsburg, Virginia, January 1981), 105-116. [5] Goad, Chris. 
Special Purpose Automatic Programming for Hidden Surface Elimination. Proceedings of SIGGRAPH 82 (Boston, 
Massachusetts, July 26-30, 1982). In Computer Graphics 16, 3 (July 1982), 167-178. [6] Hanrahan, Pat. 
Ray Tracing Algebraic Surfaces. Proceedings of SIGGRAPH 83 (Detroit, Michigan, July 25­29, 1983). In 
Computer Graphics 17, 3 (July 1983), 83-90. [7] Hanrahan, Pat and Jim Lawson. A Language for Shading 
and Lighting Calculations. Computer Graphics 24, 4 (August 1990), 289-298. [8] Hoover, Roger. Alphonse: 
Incremental Computation as a Programming Abstraction. Proceedings of the SIGPLAN 92 Conference on Programming 
Language Design and Implementation (San Francisco, California, June 1992), 261-272. Plate 2: Image obtained 
by reshading image of Plate 1. This took 12 seconds unspecialized, 0.5 seconds specialized. [9] Jones, 
Neil D., Carsten K. Gomard, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice-Hall, 
1993. [10] Mogensen, Torben. The Application of Partial Evaluation to Ray-Tracing. Master's thesis, DIKU, 
University of Copenhagen, Denmark, 1986. [11] Osgood, Nathaniel David. PARTICLE: an Automatic Program 
Specialization System for Imperative and Low­level Languages. Master's thesis, MIT, September 1993. [12] 
Pugh, William and Tim Teitelbaum. Incremental Computation Via Function Caching. Proceedings of the Sixteenth 
Annual ACM Symposium on Principles of Programming Languages (Austin, Texas, January 1989), 315-328. [13] 
Ruf, Erik. Topics in Online Partial Evaluation. Ph.D. thesis, Stanford University, April 1993. Published 
as Stanford Computer Systems Laboratory technical report CSL-TR-93-563, March 1993. [14] Sequin, Carlo 
H. and Eliot K. Smyrl. Parameterized Ray Tracing. Proceedings of SIGGRAPH 89 (Boston, Massachusetts, 
July 31-August 4, 1989). In Computer Graphics 23, 3 (July 1989), 307-314. [15] Sundaresh, R.S. and Paul 
Hudak. A Theory of Incremental Computation and Its Application. Proceedings of the Eighteenth Annual 
ACM Symposium on Principles of Programming Languages (Orlando, Florida, January 1991), 1-13. [16] Upstill, 
Steve. The RenderMan Companion. Addison-Wesley, 1989. [17] Watkins, Christopher D., Stephen B. Coy, and 
Mark Finlay. Photorealism and Ray Tracing in C. M&#38;T Books, 1992 [18] Watt, Alan and Mark Watt. Advanced 
Animation and Rendering Techniques. ACM Press, 1992.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218473</article_id>
		<sort_key>351</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[A signal processing approach to fair surface design]]></title>
		<page_from>351</page_from>
		<page_to>358</page_to>
		<doi_number>10.1145/218380.218473</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218473</url>
		<keywords>
			<kw><![CDATA[graphics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39036867</person_id>
				<author_profile_id><![CDATA[81100305012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taubin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T.J. Watson Research Center, P.O.Box 704, Yorktown Heights, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134014</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[C.L. Bajaj and Ibm. Smoothing polyhedra using implicit algebraic splines. Computer Graphics, pages 79-88, July 1992. (Proceedings SIGGRAPH'92).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. Baker. Building surfaces of evolution: The weaving wall. International Journal of Computer Vision, 3:51-71,1989.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>176581</ref_obj_id>
				<ref_obj_pid>176579</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[E Borrel. Simple constrained deformations for geometric modeling and interactive design. ACM Transactions on Graphics, 13(2):137- 155, April 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[E. Catmull and J. Clark. Recursively generated B-spline surfaces on arbitrary topological meshes. Computer Aided Design, 10:350-355, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122746</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[G. Celniker and D. Gossard. Deformable curve and surface finiteelements for free-form shape design. Computer Graphics, pages 257- 266, July 1991. (Proceedings SIGGRAPH'91).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[T.D. DeRose, M. Lounsbery, and J. Warren. Multiresolution analysis for surfaces of arbitrary topological type. Technical Report 93-10- 05, Department of Computer Science and Enginnering, University of Washington, Seattle, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M. Do Carmo. Differential Geometry of Curves and Surfaces. Prentice Hall, 1976.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. Doo and M. Sabin. Behaviour of recursive division surfaces near extraordinary points. Computer Aided Design, 10:356-360, 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[G. Golub and C.E Van Loan. Matrix Computations. John Hopkins University Press, 2nd. edition, 1989.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617896</ref_obj_id>
				<ref_obj_pid>616031</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[G. Greiner and H.E Seidel. Modeling with triangular b-splines. IEEE Computer Graphics and Applications, 14(2):56-60, March 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Gu6ziec and R. Hummel. The wrapper algorithm: Surface extraction and simplification. In IEEE Workshop on Biomedical Image Analysis, pages 204-213, Seattle, WA, June 24-25 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. Halstead, M. Kass, and T. DeRose. Efficient, fair interpolation using catmull-clark surface. Computer Graphics,pages 35-44,August 1993. (Proceedings SIGGRAPH'93).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166119</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. Mesh optimization. Computer Graphics, pages 19-26, August 1993. (Proceedings SIGGRAPH'93).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134036</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[W.M. Hsu, J.E Hughes, and H. Kaufman. Direct manipulation of freeform deformations. Computer Graphics, pages 177-184, July 1992. (Proceedings SIGGRAPH'92).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>145007</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[A.D. Kalvin. Segmentation and Surface-Based Modeling of Objects in Three-Dimensional Biomedical Images. PhD thesis, New York University, New York, March 1991.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[M. Kass, A. Witkin, and D. Terzopoulos. Snakes: active contour models. International Journal of Computer Vision, 1(4):321-331, 1988.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>81069</ref_obj_id>
				<ref_obj_pid>81065</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[T. Lindeberg. Scale-space for discrete signals. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(3):234-254, March 1990.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[C. Loop. Smooth subdivision surfaces based on triangles. Master's thesis, Dept. of Mathematics, University of Utah, August 1987.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>186755</ref_obj_id>
				<ref_obj_pid>186741</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[C. Loop. A G1 triangular spline surface of arbitrary topological type. Computer Aided Geometric Design, 11:303-330, 1994.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192238</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[C. Loop. Smooth spline surfaces over irregular meshes. Computer Graphics, pages 303-310, July 1994. (Proceedings SIGGRAPH'94).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[W. Lorenson and H. Cline. Marching cubes: A high resolution 3d surface construction algorithm. Computer Graphics, pages 163-169, July 1987. (Proceedings SIGGRAPH).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617779</ref_obj_id>
				<ref_obj_pid>616025</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[M. Lounsbery, S. Mann, and T. DeRose. Parametric surface interpolation. IEEE Computer Graphics and Applications, 12(5):45-52, September 1992.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617893</ref_obj_id>
				<ref_obj_pid>616031</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[J. Menon. Constructive shell representations for freeform surfaces and solids. IEEE Computer Graphics and Applications, 14(2):24-36, March 1994.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[H.E Moreton and C.H. S6quin. Functional optimization for fair surface design. Computer Graphics, pages 167-176, July 1992. (Proceedings SIGGRAPH'92).]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>628456</ref_obj_id>
				<ref_obj_pid>628299</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[J. Oliensis. Local reproducible smoothing without shrinkage. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(3):307-312, March 1993.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>117761</ref_obj_id>
				<ref_obj_pid>117754</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[A. Pentland and S. Sclaroff. Closed-form solutions for physically based shape modeling and recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(7):715-729, July 1991.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[E. Seneta. Non-Negative Matrices, An Introduction to Theory and Applications. John Wiley &amp; Sons, New York, 1973.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>119393</ref_obj_id>
				<ref_obj_pid>119388</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[L.A. Shirman and C.H. S6quin. Local surface interpolation with bezier patches. Computer Aided Geometric Design, 4:279-295, 1987.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134010</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[W.J. Shroeder, A. Zarge, and W.E. Lorensen. Decimation of triangle meshes. Computer Graphics, pages 65-70, 1992. (Proceedings SIGGRAPH'92).]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[R.S. Szeliski, D. Tonnesen, and D. Terzopoulos. Modeling surfaces of arbitrary topology with dynamic particles. In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition, pages 82- 87, New York, NY, June 15-17 1993.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840029</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. Curve and surface smoothing without shrinkage. Technical Report RC-19536, IBM Research, April 1994. (also in Proceedings ICCV'95).]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>840020</ref_obj_id>
				<ref_obj_pid>839277</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[G. Taubin. Estimating the tensor of curvature of a surface from a polyhedral approximation. Technical Report RC-19860, IBM Research, December 1994. (also in Proceedings ICCV'95).]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[G. Taubin, T. Zhang, and G. Golub. Optimal polyhedral surface smoothing as filter design. (in preparation).]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[D. Terzopoulos and K. Fleischer. Deformable models. The Visual Computer, 4:306-311,1988.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134008</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[G. Turk. Re-filing polygonal surfaces. Computer Graphics, pages 55-64, July 1992. (Proceedings SIGGRAPH'92).]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192241</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[G. Turk and M. Levoy. Zippered polygon meshes from range data. Computer Graphics, pages 311-318, July 1994. (Proceedings SIG- GRAPH'94).]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[W. Welch and A. Witkin. Variational surface modeling. Computer Graphics, pages 157-166, July 1992. (Proceedings SIGGRAPH'92).]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[W. Welch and A. Witkin. Free-form shape design using triangulated surfaces. Computer Graphics, pages 247-256, July 1994. (Proceedings SIGGRAPH'94).]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[A.E Witkin. Scale-space filtering. In Proceedings, 8th. International Joint Conference on Artificial Intelligence (IJCAI), pages 1019-1022, Karlsruhe, Germany, August 1983.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[C.T. Zahn and R.Z. Roskies. Fourier descriptors for plane closed curves. IEEE Transactions on Computers, 21(3):269-281, March 1972.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218475</article_id>
		<sort_key>359</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[Modeling surfaces of arbitrary topology using manifolds]]></title>
		<page_from>359</page_from>
		<page_to>368</page_to>
		<doi_number>10.1145/218380.218475</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218475</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39048488</person_id>
				<author_profile_id><![CDATA[81100553787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cindy]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Grimm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Science and Technology Center for Computer Graphics and Scientific Visualization]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024462</person_id>
				<author_profile_id><![CDATA[81100166298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hughes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Science and Technology Center for Computer Graphics and Scientific Visualization]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>35072</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Bartels, J. Beatty, and B. Barsky. An Introduction to Splines for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann, 1987.]]></ref_text>
				<ref_id>BBB87</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Catmull andJ. Clark. Recursively Generated B-spline Surfaces on Arbitrary Topological Meshes. Computer Aided Design, 10(6):350-355, November 1978.]]></ref_text>
				<ref_id>CC78</ref_id>
			</ref>
			<ref>
				<ref_obj_id>61954</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Farin. Curves and Sulfaces for Computer Aided Geometric Design. Academic Press, 1988.]]></ref_text>
				<ref_id>Far88</ref_id>
			</ref>
			<ref>
				<ref_obj_id>924482</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Grimm. Sulfaces of Arbitrary Topology using Manifolds. PhD thesis, Brown University (in progress).]]></ref_text>
				<ref_id>Gri</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166121</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Halstead, T. DeRose, and M. Kass. Efficient, Fair Interpolation Using Catmull-Clark Surfaces. Computer Graphics, 27(2):35-44, July 1993. Proceedings of SIG- GRAPH '93 (Los Angeles).]]></ref_text>
				<ref_id>HDK93</ref_id>
			</ref>
			<ref>
				<ref_obj_id>87557</ref_obj_id>
				<ref_obj_pid>87526</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[K. Hollig and H. Mogerle. G-splines. Computer Aided Geometric Design, 7:197-207, 1990.]]></ref_text>
				<ref_id>HM90</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[E King. On Local Combinatorial Pontrjagin Numbers (I). Topology, 16:99-106, 1977.]]></ref_text>
				<ref_id>Kin77</ref_id>
			</ref>
			<ref>
				<ref_obj_id>77059</ref_obj_id>
				<ref_obj_pid>77055</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[C. Loop and T. DeRose. A Multisided Generalization of B6zier Surfaces. ACM TOG, 8(3):204-234, July 1989.]]></ref_text>
				<ref_id>LD89</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[C. Loop. Smooth Subdivision Surfaces Based on Triangles. Master's thesis, University of Utah, 1987.]]></ref_text>
				<ref_id>Loo87</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192238</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[C. Loop. Smooth Spline Surfaces Over Irregular Meshes. Computer Graphics, 28(2):303-310, July 1994. Proceedings of SIGGRAPH '94.]]></ref_text>
				<ref_id>Loo94</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. Milnor and J. Stasheff. Annals of Mathematical Studies. Princeton University Press, Princeton, New Jersey, 1974.]]></ref_text>
				<ref_id>MS74</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166120</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J. Maillot, H. Yahia, and A. Verroust. Interactive Texture Mapping. Computer Graphics, 27(4):27-35, July 1993. Proceedings of SIGGRAPH '93.]]></ref_text>
				<ref_id>MYV93</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[E.H. Spanier. Algebraic Topology. McGraw-Hill Inc., New York, New York, 1966.]]></ref_text>
				<ref_id>Spa66</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[M. Spivak. Differential Geomen7 Volume 1. Publish or Perish Inc., 1970.]]></ref_text>
				<ref_id>Spi70</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[I.M. Singer and J. A. Thorpe. Lecture Notes on Elemental7 Topology and Geomet~7. Scott, Foresman and Company, Glenview, Illinois, 1967.]]></ref_text>
				<ref_id>ST67</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[N. Steenrod. The Topology of Fibre Bundles. Princeton University Press, Princeton, New Jersey, 1974.]]></ref_text>
				<ref_id>Ste74</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122749</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[G. Turk. Generating Textures on Arbitrary Surfaces Using Reaction-Diffusion. Computer Graphics, 25(2):289-298, July 1991. Proceedings of SIGGRAPH '91 (Las Vegas).]]></ref_text>
				<ref_id>Tur91</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134033</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[W. Welch and A. Witkin. Variational Surface Modeling. Computer Graphics, 22(2):157-166, July 1992. Proceedings of SIGGRAPH '92.]]></ref_text>
				<ref_id>WW92</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192216</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[W. Welch and A. Witkin. Free Form Shape Design Using Triangulated Surfaces. Computer Graphics, 28(2):247-256, July 1994. Proceedings of SIGGRAPH '94.]]></ref_text>
				<ref_id>WW94</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling Surfaces of Arbitrary Topology using Manifolds1 Cindy M. Grimm John F. Hughes cmg@cs.brown.edu 
(401) 863-7693 jfh@cs.brown.edu (401) 863-7638 The Science and Technology Center for Computer Graphics 
and Scienti.c Visualization ABSTRACT We describe an extension of B-splines to surfaces of arbitrary 
topol­ogy, including arbitrary boundaries. The technique inherits many of the properties of B-splines: 
local control, a compact representa­tion, and guaranteed continuity of arbitrary degree. The surface 
is speci.ed using a polyhedral control mesh instead of a rectangular one; the resulting surface approximates 
the polyhedral mesh much as a B-spline approximates its rectangular control mesh. Like a B­spline,thesurfaceisasingle,continuousobject. 
Thisisachievedby modeling the domain of the surface with a manifold whose topology matches that of the 
polyhedral mesh, then embedding this domain into 3-space using a basis-function/control-point formulation. 
We provide a constructive approach to building a manifold. CR Categories: I.3.5 [Computer Graphics]: 
Computational Geometry and Object Modeling, Curve, Surface, Solid, and Object Representations, Splines 
Introduction Surfaces of arbitrary topology are currently attracting a good deal of attention. While 
spline surfaces have proven a powerful mod­eling tool [BBB87] [Far88], modeling topologically arbitrary 
sur­faces with them is hard, because they require a rectangular param­eterization. To create complex 
surfaces, especially free-form ones such as Figure 17, we need a surface model which is computation­ally 
inexpensive and yet capable of modeling arbitrary topologies. Ideally, this surface model retains the 
power of spline surfaces: a compact representation, guaranteed continuity, and .exibility. This paper 
presents such a method of surface modeling. Our approach differs from previous techniques in that the 
sur­face is constructed from pieces of surface which overlap substan­tially instead of abutting only 
along their edges. Mathematicians have studied such surfaces for many years [MS74] [ST67] using the technology 
of manifolds. Although the underlying mathematics is somewhat complicated, manifolds have advantages 
that make this complexity worthwhile. 1This work was supported in part by grants from NSF, ARPA, IBM, 
NCR, Sun Mi­crosystems, DEC, HP, and ONR grant N00014-91-J-4052, ARPA order 8225. Permission to make 
digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage, the copyright notice, the 
title of the publication and its date appear, and notice is given that copying is by permission of ACM, 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Consider, for example, 
the problem of mapping textures onto two adjacent patches in a conventional spline surface. Matching 
the texture along the boundary between the patches may be dif.cult, since there is no common parameterization 
between the patches. With a manifold, however, the edge of one texture region is well withinthe adjacenttextureregion,sothatthetwo 
texturescaneasily be blended. Similarly, if one tries to make a smooth path on a surface, and the path 
crosses a patch boundary, maintaining smoothness of the path and its derivatives may be dif.cult. But 
there are no bound­aries on a manifold because as a path gets near the edge of a patch, it has already 
entered the adjacent patch, and its derivatives can be computed in that new patch s coordinate system. 
This can be used to make various forms of user-interaction (sliding one object along another, for example) 
much smoother. Similarly, differential equa­tions such as those used to generate reaction-diffusion textures 
can be solved by blending partial solutions across patch overlaps. This paper begins with a survey of 
previous and related work, then sketchesa high-level view of the surface construction technique. This 
is followed by a discussion of manifolds in general and how to build a manifold for a given surface. 
Next we discuss adding geom­etry to the manifold. Finally, we conclude with results and future work. 
2 Previous work Several different approaches to arbitrary-topology surface models have been suggested. 
Subdivision [CC78] [Loo87] produces a smooth surface by repeatedly subdividing a polyhedral mesh and 
in the limit yields a G1surface. This method is very general, but does not admit an analytical form (although 
recent work [HDK93] has made subdivision more tractable). Another approach is to .ll in any non-rectangular 
parts of a mesh with n-sided patches [HM90] [LD89]. This is analogous to B´ezier surfaces, in that it 
ensures continuity across the boundaries of patches by main­taining constraints on control points. A 
similar technique is to pro­duce a collection of triangular (and possibly rectangular) elements from 
an initial mesh and stitch them together into a surface using the geometric information in the original 
mesh [Loo94]. In [WW94], the initial sketch is a set of contours over which a triangulated sur­face is 
stretched, using variational modeling techniques [WW92] to control the shape of the surface. Unlike the 
previous methods, our approach produces a surface whichisonecontinuouspieceandhencedoesnotrequire constraints 
to maintain continuity. Adding to (or removing from) the surface is similar to adding or removing a row 
of control points from a B­spline surface continuity is automatically guaranteed. Figure 1: Left: Gluing 
two patches together along their thin edge then bending the patches along the crease. Right: Gluing two 
patches together along a region then bending the patches together. Overview Spline patches are a powerful 
modeling tool but stitching them to­gether into complex surfaces has proven dif.cult. As an analogy, 
consider building surfaces out of stretchy pieces of fabric that can be glued toeachother. Thepiecesoffabricaresplinepatches,and 
the glue consists of mathematical operations such as control point constraints. Previous methods have 
focused on gluing these fabric pieces together by applying glue thinly along the abutting edges of the 
fabric pieces (see left of Figure 1). The problem with this tech­nique is that a change to one of the 
fabric pieces is not re.ected in theadjoiningpatchexceptalongthegluededge. Thesmoothnessof the joint 
is maintained by adjusting the adjoining patch afterwards. Our approach is to apply glue to the top of 
one fabric piece and the bottom of another piece and then glue them together by over­lapping the two 
pieces. Now, when the .rst piece is stretched or moved, the second, overlapping piece follows naturally 
with it (see right of Figure 1). This eliminates the need to re-establish the con­tinuity of the join 
after every change to the surface. In the curve domain, this is the difference between B´ezier curves 
and B-spline curves; B´ezier curves are joined together into larger curves by con­straining the control 
points at the end of one curve and at the be­ginning of the following curve. B-splines, on the other 
hand, are extended by adding in another overlapping curve segment. We pre­fer the B-spline approach because 
the domain is continuous and no constraints are required; to extend B-splines to arbitrary topologies, 
however, we .rst need a mechanism for adding overlapping pieces to a surface. We begin by taking several 
pieces of fabric and gluing them to­gether into a larger object by overlapping them. To describe the 
ob­ject, we need to describe the pieces of fabric and how they over­lap. This is very similar to the 
familiar concept of an atlas of the world; each page of the atlas is rectangular (i.e., a piece of fabric) 
but the collection of pages describes a spherical object, the world. The pages of the atlas overlap enough 
to get from one page to the next. For example, the page for France contains part of Spain, and the page 
for Spain contains part of France. When traveling from France to Spain there is a time when one is located 
on both pages; the two maps may not be identical where they overlap but there is enough information to 
establish a correspondence between the two pages. With an atlas we begin with an object, the world, and 
create a set of pages that cover the world, with each page overlapping with its neighbors. Suppose we 
did not have the world, but instead had just the pages of an atlas. We could put the pages onto stretchy 
pieces of fabric and glue them together using the information on the overlap­ping parts. This glued-together 
object is then a world . This is a constructive approach to building a world as opposed to an analyti­cal 
one. Because we do not have a world (i.e., the surface) apriori, we use this constructive approach to 
building a surface out of pieces of surface. There is one more consideration; when we build our world 
from the pages of the atlas, how do we know what the world looks like? Vertex pages Figure 2: Stretching 
the pages of the atlas out to approximate the polygon, then gluing them together Edge pages Vertex pages 
Glued pages Figure 3: Glue the pages of the atlas together, then stretch them out to approximate the 
polygon. The pages and their overlaps provide information on the topology of the object but no information 
on the geometry of it. With a real atlas we have some implicit knowledge of what the world looks like, 
but this knowledge is external to the atlas. There are two possible ways to add geometrical information 
into the atlas; consider the case of making an atlas for a curve. The rough shape of the desired curve 
is given by a control polygon. For each vertex and edge of the polygon we create have a page in the atlas. 
A page corresponding to an edge of the polygon overlaps with the two vertex pages corresponding to the 
vertices of the edge. The curve is built by gluing the vertex and edge pages together and adding geometrical 
information to describe what the curve looks like. This can be accomplished in two ways: .First describe 
what each page looks like, then glue the pages together. This corresponds to taking the pages of the 
atlas, stretching them out to approximate the control polygon, then gluing them together (see Figure 
2). .First glue the pages together, then describe where they go. This correspondsto gluing the pages 
of the atlas together, then stretching them out to approximate the control polygon (see Figure 3).  
We take the second approach because it is simpler. In the .rst approach, the gluing stage must be repeated 
every time the geom­etry of the surface changes, i.e., when the control polygon is moved. In the second 
approach, the gluing process is performed exactly once, and is independent of the particular geometry 
(but not topol­ogy) of the object. Although this construction process is excessive for de.ning a curve, 
imagine constructing a surface from a polyhedron. Building an atlas provides us with a local description 
of the surface that is continuous and upon which we can navigate, i.e., perform opera­tions such as calculating 
geodesics. The surface is relatively sim­ple to describe locally but we can still perform global operations 
be­cause the atlas pages overlap, allowing us to easily move from one area of the surface to another. 
In contrast, the traditional method of stitching patches together is to abut them, resulting in joins 
between the patches that must be dealt with separately. 4 Outline of the construction process To build 
our surface we begin with a polyhedral mesh (created by the user) that describes the basic shape and 
topology of the surface. Thisisinanalogywith aB-splinecontrolmesh,exceptthatthepoly­hedral mesh is not 
limited to a rectangular topology. We formally de.ne this mesh in Section 5. Next we de.ne the pages 
of the atlas and how those pages over­lap. We create one page for each element in the polyhedron, i.e., 
one page for each vertex, edge, and face. How the pages overlap is determined by the adjacency relationships 
in the polyhedron. For example, a face page overlaps with the pages for the vertices and edges of the 
face. Figure 20 shows a sample polyhedron and a sur­face colored by page type. In Section 6 we formally 
de.ne an atlas and show how to construct an atlas using the polyhedron as a guide. Finally, we add geometry 
( shape ) to the atlas. We do this in a manner similar to the one used for B-splines. On each page we 
build several basis functions and associate a control point with each function. This tells us where the 
middle part of each page goes; be­cause the pages overlap, the location of the edges of the page will 
be in.uenced by the control points of the overlapping pages. This is covered in Section 8. 5 The polyhedron 
Construction of a surface starts with a polyhedral sketch. To sim­plify later steps, we require that 
every interior vertex have exactly four faces adjacent to it, i.e., vertices are of valence four. A polyhe­dron 
of this form can be constructed from an arbitrary polyhedron by taking the dual of the .rst subdivision 
[CC78][Kin77] (see Ap­pendix D). The polyhedral sketch must satisfy some technical restrictions that 
essentially say it looks like a surface: every vertex must be an end of some edge, every edge must be 
an edge of some face, at most two faces can meet at an edge, each interior vertex must have 4 edgesand4facesadjacentto 
it, andeach boundaryvertex must have nedges and n-1faces adjacent to it. Furthermore, the polyhedron 
must be orientable, i.e., it must contain no embedded Mobius strips. These technical restrictions make 
the polyhedron an oriented sur­face in the sense of algebraic topology [Spa66]. Finally, we require that 
each face have 3, 4, 5, or 6 edges. The polyhedral sketch contains three types of information: the geometric 
information given by the locations of the polyhedron ver­tices; the local topological information as 
given by the incidence relations which vertices lie on which edges, which edges are in which faces; and 
the global topological information that can be de­rived from this local data: the number of components 
or pieces of the sketch, the number of boundary components, and the genus. 6 The atlas, or manifold We 
have informally described an object consisting of pieces of fab­ric glued together . This concept is 
called amanifold. Manifolds were introduced in the 1890s and formalized in the 1920s in order to describe 
objects whose topology was more complex than that of Euclidean space. The notion was that an object locally 
like Eu­clidean space could be studied in much the same way as Euclidean space. In one view, a manifold 
is a structure imposed on a set a division of the set into overlapping regions, each of which is in 
correspondence with a portion of the Euclidean plane. Consider a world atlas. Every point on the world 
can be found in at least one page in the atlas and sometimes in several. A path from one point to another 
can be found by tracing a line through the pages. Where the path must cross from one page to another, 
the two pages overlap enough that one can locate oneself on the second page. The indi­vidual pages are 
regions of .2but taken together they represent a sphere [MYV93]. There are also implicitly de.ned transition 
func­tions from one page to another. These are the glue we use to glue the pages together; they establish 
a correspondence between the re­gion of one page and a region of another. Thus Brussels and its envi­rons 
may appear on two different atlas pages: the page for Benelux countries, and also in the upper right 
corner of the page for France. The labels for Brussels and the surrounding towns, etc., establish a correspondence 
between the upper right corner of the France page and the lower left corner of the Benelux page. 6.1 
Formal de.nition In the traditional de.nition of a manifold the object exists and the manifold consists 
of charts, or mappings from the object to pieces of .n . (The images of the charts are our atlas pages. 
From now on, we will refer to the atlas pages as charts.) This is an analytical view; because we do not 
have an object but are building it we de­part from this view and de.ne a constructiveview of manifolds. 
Our constructive de.nition of a manifold starts with charts and informa­tion on how they overlap (the 
charts and the transition functions). We call this a proto-manifold. From the proto-manifold we build 
a manifold using an equivalence relation, i.e., we glue the charts to­gether using this place on this 
chart is the same as that place on that other chart. In [Gri] we show that this de.nition is equivalent 
to the traditional one. De.nition 1 A Ckdifferentiable proto-manifold Kof dimension nconsists of: 1. 
A .nite set Aof connected open sets in .n . Ais called a proto-atlas. Each element c2Ais called a chart. 
 2. A set of subsets UijCci,where ciand cjare charts in A and where Uii=ci. 3. A set of functions \called 
transition functions. A transition function lij2\is a map lij:Uij-Ujiwhere UijCci and UjiCcj. Note that 
Uijand Ujimaywell beempty. The following conditions on lijmust hold: (a) lijis 1-1, onto, and Ck-differentiable 
 (b) li1=lji   ij (c) lii(x)=x, x2ci (d) The cocycle condition :(lijoljk)(x)=lx)for Tik(x2UikUij(see 
Figure 4) The charts c2Aare the pages of the atlas. The subsets Uij describe what part of chart ioverlaps 
with chart j. The function lij de.nes the exact correspondence between points in Uijand points in Uji. 
Next we build the manifold. If K=(A,\)is a proto-manifold then there is a relation .de.ned on tc2Ac(where 
tdenotes dis­joint union) such that if x2ci, y2cj,then x.yiff lij(x)=y. Conditions (1) (3) in De.nition 
1 ensure that .is an equivalence relation [Gri]. Continuing the analogy, each chart cis apageof the world 
at­las, each transition function lijis a correspondence between parts of two charts, and the equivalence 
relation .says that the place la­beled Brussels on page 93 is the same as the place labeled Brussels 
on page 24. ik jk  Figure 4: Three overlapping charts. The cocycle condition requires T that (lijoljk)(x)=lik(x)for 
x2UikUij. The relation lets us build a single object from the charts. If a point xin one chart is taken 
via lijto a point yin another chart (lij(x)=y) then those two points are a single point on the .nal object. 
De.nition 2 Let betheequivalencerelationdescribedaboveand Ka proto-manifold as de.ned above. De.ne Mas 
the quotient of tc2Acby .Let Ibe the map taking x2tc2Acto [xl2M, where [xlis the equivalence class of 
x. In [Gri] we prove under weak conditions satis.ed in the con­struction below that Mis a Hausdorff space 
upon which we can construct a traditional manifold structure. The correspondence be­tween our de.nition 
and the standard one is relatively simple: the image of a chart cunder the map Iis a subset I(c)=DcCM; 
the restriction of Ito cde.nes a one-to-one correspondence lc:c-DcCM, lc(x)=I(x) between cand Dc. The 
inverse of lcisa map Ccfrom a subset Dc of Mto cwhich is a subset of 2, i.e., Cc:Dc -cC.2 , Cc(x)=lci1 
(x) The maps Ccare the coordinate charts in the standard de.nition of a manifold structure on the set 
M[Spi70]. The map lcis called a local parameterizationin [MS74] and a coordinatesystem in [Ste74]. Note 
thatthename chart refers to oneofoursubsetsof 2and that Cand Ci1denotethe correspondencebetweenthesechartsand 
subsets of the manifold.  6.2 Building a manifold from a polyhedron In the following discussion we construct 
a manifold without bound­ary, i.e., the polyhedron has no boundary vertices. Extending this construction 
to a manifold with boundary is straightforward, as ex­plained in Section 7. We use the topological structure 
of the polyhedron as a guide for building the manifold. We construct one chart for each element in the 
polyhedron and de.ne the overlaps of the charts by the adja­cency relationships in the polyhedron. This 
produces three sets of charts: the vertex charts those charts corresponding to the ver­tices of the 
polyhedron the edge charts,and the face charts.We denote these by V=fchart for vgv2V, E,and F, respectively. 
SS The entire proto-atlas Ais then VEF. Figure 20 shows a polyhedron and the resulting surface, which 
has been colored according to chart type. The vertex charts (V)are in red. Each vertex chart has eight 
other charts overlapping it; four edge charts (in green) and four face charts (in purple). To keep the 
notation simple, henceforth Vindicates the chart associated with a vertex vand V0the chart for a vertex 
named v 0 , and similarly for edges and faces. A chart in one set never overlaps with a different chart 
in that same set. A chart does overlap with those charts that are nearby in the polyhedron. For example, 
a face chart only overlaps with the vertex and edge charts corresponding to the vertices and edges of 
the face. In summary: 0 1. UVV= , V6=V0(and similarly for UEE0 and UFF0 ). 2. UVE=6 iff v2e. 3. UVF=6 
iff v2f. 4. UEF=6 iff eis an edge of f.  If we perform a similar construction in 2 dimensions then 
we have one chart for each vertex and one for each edge (see Figure 3). An edge chart Eoverlaps with 
two vertex charts Vand V0,where e=fv,v 0 g. In this case the charts are all segments of the real line; 
note how each edge chart is nearly covered by the two neighbor­ing vertex charts. By nearly we mean the 
chartEis covered by the closure of UEVand UEV0 , i.e., . E=UEVUEV0 To duplicate this in 3D we ensure 
that the edge and face charts are nearly covered by the overlapping vertex charts (see Figures 6 and 
7). In the 2D example, the interior vertex charts are also covered by the two overlapping edge charts, 
i.e., . 0 V=UVEUVE We would also like to cover the vertex and face charts in this way but this turned 
out to be too restrictive. We do, however, require that the charts overlap as much as possible. 6.2.1 
The charts A chart is a connected, open subset of 2. An overlap region is an open subset of the chart. 
The vertex charts are unit squares centered at the origin. A ver­tex chart overlaps with four faces (the 
four quadrants) and four edges (Figure 5 shows the vertex chart and the overlap regions). A vertex­edge 
overlap region UVEoverlaps with the two regions UVF0 and UVF1 where f0and f1are the two faces adjacent 
to e. The edge charts are diamonds with the left and right ends chopped off (see Figure 6). An edge chart 
overlaps with two vertex charts (the left and right half of the diamond) and two face charts (the upper 
and lower half of the diamond). The face chart for an n-sided face is an n-sided regular polygon centered 
at the origin (slightly smaller then a unit polygon). The edge charts overlap the edges of the polygon, 
while the vertex charts overlap the corners. The region UFEoverlaps the two regions UFV0 and UFV1 ,where 
v0and v1are the endpoints of e. The details of the charts and their overlaps are given in Appendix B. 
 6.2.2 The transition functions The transition functions are the glue that holds the charts together. 
We have described what parts of the charts to glue together but not the exact correspondence between 
them. Transition functions must meet two requirements: they must be Ck, and the cocycle condition must 
hold (see Figure 4). Addition­ally, we would like the functions to be as close to the identity func­tion 
as possible. By this we mean that a transformed (via lcc0)im­age on Uc0cshould look as much like the 
original image on Ucc0 as possible. U FE 1  .5 UVF UVF 3 2 .5 -.5 UVF UVF 0 1 -.5  Chart 
for edge e Chart for face f Figure 8: Mapping from an edge chart to a face chart by translating then 
rotating. P1 Face overlaps Edge overlaps Figure 5: A vertex chart. Left: the four regions UVFiRight: 
the q q 2  four regions UVEi 3 . Pq0 10 q Face Chart Unit Polygon Figure 9: Mapping from a face chart 
to a vertex chart using a pro­jective transformation. .5 Charts within the set V(and similarly Eand 
F) do not overlap. Thus the transition function lVVis either the empty function (if 0  V 0) or the identity 
(if V=V0), and similarly for edge-edge V =6 and face-face transition functions. This means we need only 
consider the transition functions be­tween charts in distinct chart sets, e.g. vertex-to-face transitions. 
If we de.ne the function lCCthen the function lC0Cis de.ned as its 0 inverse. Thus the transition functions 
we de.ne can be divided into three categories: edge-to-face, vertex-to-face, and vertex-to-edge. Satisfying 
the cocycle condition is a matter of ensuring that the Face overlaps Vertex overlaps function taking 
the edge chart to the vertex chart directly is the same as the combination of functions taking an edge 
chart to a face chart toavertexchart(seeFigure10). We.rstde.netheedge-to-faceand Figure 6: An edge chart. 
Left: the two regions UEFi(Fis 3-sided, 0 F1is 4-sided). Right: the two regions UEV i vertex-to-face 
functions, and then de.ne the edge-to-vertex func­ . tions as compositions of the other two. The edge-to-face 
transition function is a plane isometry; the re­ gion UEFis simply translated and rotated (see Figure 
8). The vertex­ to-face transition function takes the quadrant UVFand stretches it using a projective 
transformation to .t it into the corner of the face chart (see Figure 9). The edge-to-vertex transition 
function is built as a blend of two functions which are compositions. Examining Figure 10, on the top 
 half of the diamond the function must be the composition of the edge-to-face and face-to-vertex functions 
for the top face, while on the bottom half of the diamond the function must be the correspond­ing composition 
of the bottom face functions. These two composed functions will not, in general, agree along the x-axis. 
To.xthis, we have left a gap between the two composed functions; this lets us blend between the two composed 
functions in the gap. (This is the reason the face charts are slightly smaller than the unit polygon 
to give us room to do the blend.) Care must be taken to ensure that this function is 1-1, onto, and Ck; 
details of this (and the other functions) are given in Appendix C. Vertex overlaps Edge overlaps  6.3 
The manifold Figure 7: A face chart. Left: the nregions UFViRight: the nre-A formal proof that these 
charts and transition functions form a proto­gions UFEi manifold (De.nition 1) appears in [Gri]. Informally, 
the charts are de.ned to be open sets in 2and the transition functions are de.ned to be 1 1, onto, and 
Ck. The cocycle condition is satis.ed because in the only non-trivial case, where three charts overlap 
(one each of 1 h hh h Blend function Blend region  a vertex, edge, and face chart), we have de.ned 
the functions to be compositions of the others. 7 Manifolds with boundary To extend our de.nition to 
a manifold with boundary we allow the charts of the proto-manifold to be half-balls in n. Weuseamani­fold 
with boundary when the polyhedral sketch has a boundary, i.e., when an edge has a single adjacent face. 
Since a boundary never occurs in the middle of a face chart, we need only alter our descrip­tions of 
vertex and edge charts. An edge chart Ecorresponding to a boundary edge is simply a triangle joined with 
the x-axis. A vertex chart Vcorresponding to a boundary vertex is a contiguous subset of the quadrants 
of the unit square, with some part of the axes in­cluded (e.g., a vertex chart corresponding to a corner 
vertex would consist of a single quadrant). Thede.nitionsofthe transition functionsremainunchangedex­cept 
for the edge-vertex functions, since the overlap regions remain unchanged. The edge-vertex function becomes 
just the single com­posed function one one-half of the edge chart. The Ccfunctions also remain the same. 
8 Adding geometry to the manifold We now have a collection of stretchy fabric pieces glued together but 
with no geometric structure (they are just collapsed on the .oor in a pile ). Rather than describe what 
the entire object looks like all at once, we just describe what the middle of each chart looks like. 
Because of the way the charts overlap, this will determine the ge­ometry of the entire manifold. To de.ne 
the geometry we use a basis-function control-point for­mulation. The basis functions fBs:M-.gsare a .nite 
col­ 2Slection Sof local, Ckfunctions that sum to one everywhere; they areanalogoustotraditionalB-splinebasisfunctions. 
Thebasisfunc­tion Bsdetermines how much the control point Gs2.3 in.uences the surface Qat a given point: 
X Q(p)=GsBs(p) s2S We next describe how to build the basis functions. 8.1 Basis functions To build the 
basis functions we .rst de.ne a set of proto-basis func­tions on the chart and associate a control point 
with each proto-basis function. Speci.cally, we have a set f bsgs2Sof Ckfunctions  bs:c­ where s=fc,ig, 
c2Aand i21nc. The number ncis de­pendent upon the desired continuity kand the type of chart (vertex, 
edge, or face). We de.ne these functions in Section 8.1.1; for now, suf.ce it to say they are Ckandgoto 
0by the boundary of cand are similar to a tensor-product basis function. The bfunctions are de.ned on 
the individual charts and hence do not interact with functions on other charts. We next extend the b 
functions to the manifold where they will interact. Imagine building piles of sand on each page of the 
atlas. When the pages of the atlas are glued together the piles of sand are no longer on a single page 
but possibly on several pages glued together, each page of which may have its own piles of sand. Formally 
these piles of sand are built by extending the proto-basis functions to the manifold by using the Cc 
functions where they are de.ned and 0where they are not. De.ne Bs:M-.by . 2Ci1 bs(Cc(p))if p(c) c Bs(p)= 
0 otherwise These functions are Ckand local. For the surface Qto behave like a spline surface, we require 
basis functions Bsthat meet three prop­erties that traditional basis functions satisfy: .Bsis Ckfor some 
k. .Bsis local (its support lies in a single chart).  P .Bs(p)=1for all p2M. s2S The last step is to 
normalize the B sfunctions to ensure that they P sum to one. If for every point p2Mwe have B s(p)6=0 
s2S then the de.nition Bs:M-., Bs(p) Bs(p)= P B0 (p) 0 s s2S is valid. Figure 11 shows how the center 
parts of edge and face charts overlap a vertex chart. Recall that the vertex charts nearly cover every 
chart and hence the manifold; to ensure that the above de.nition is valid, we make certain that the supports 
of the proto­basis functions cover the center area of the chart in which they are de.ned. As shown in 
Figure 11, this ensures that the vertex charts are covered by the supports of the fB sgs2Sfunctions and 
hence that the manifold is covered by them as well. 8.1.1 The proto-basis functions We now show how to 
build the proto-basis functions using a tensor­product B-spline and a projective transformation. For 
each proto­basis function bc.i(r)we start with a quadrilateral Qiin its chart c -.25 .25  Vertex Chart 
Edge Chart .25 -.25  l0 l_0l_0l_0l_0 dd dd Face Charts Figure 12: The quadrilaterals Qifor the proto-basis 
functions with grid division 4. (see Figure 12) and construct a projective transformation cQi(see Appendix 
A) from the quadrilateral to the unit square. Let f: 2 -.be a Cktensor-product B-spline whose support 
is from 0-(k/2)to 1k/2. Then the ithproto-basis function is bi(r)=f(cQi(r)) The quadrilaterals for the 
face chart are formed by mapping a subdivided unit square into UFVvia a projective transform. If cFV 
is a projective transform from UFVto the unit square then the four corners of Q0are ci1 of (0,0), (0,1/d), 
(1/d,1/d),and (1/d,0). FV The choiceof ddependson k. Thesizeandnumberofthequadri­laterals (and the size 
of the support of the tensor-product B-spline) are chosen so that the supports of the proto-basis functions 
cover as much of the chart as possible without falling out of it. For the C2 pictures we used a grid 
division of 4.  8.2 The control points The location of the control points is completely unconstrained; 
how­ever, the user has already provided a rough sketch of the shape of the surface (the polyhedron). 
We provide an initial placement of the control points based on the subdivision surface of the polyhedron. 
We describe how to assign values to the control points using the Catmull-Clark subdivision surface (L) 
of the polyhedron. This pro­duces a surface with the feel of a B-spline surface (note, though, that the 
choice of values does not affect the continuity). We de.ne a mapping from the manifold to the subdivision 
surface H:M-L and assign the function Gsthe value H(ps),where psis the center of support for the basis 
function Bs. To de.ne H, we .rst note that after one level of subdivision we have one subdivision point 
lcfor each chart cin M. We relate the origins of the charts to the subdivision points by H(Cc(0,0))=lc. 
This places the subdivision points in a chart Vas shown on the left of Figure 13. After one level of 
subdivision every face in the subdivision sur­face is 4-sided; these faces are mapped to the quadrants 
of the ver­tex charts. Further subdivisions grids the vertex chart as shown in the right of Figure 13. 
We de.ne Hby assigning the grid points (Ci1(grid point)) to their corresponding points in the subdivision 
V surface. Eventually, this relates a set of points that are dense in M to the subdivision points.1The 
function for Hon this dense set can be extended to Min a natural way. 1We assign the points along the 
boundary of the vertex charts .V(V)to their ad­jacent points in M. f e f e v e f e f  Figure 14: Left: 
Triangulating the interior of a vertex chart. Middle: Adjoining edges. Right: Filling in the corners. 
 8.3 Triangulating the embedding There is a tradeoff between the number of triangles in the triangula­tion 
and how closely it approximates the surface. The triangulation presented here produces approximately 
(2r)(2r)Nvtriangles for a given resolution r. If the control points are evenly spaced then the resulting 
triangulation is also evenly spaced. We triangulate the domain by .rst triangulating the vertex charts 
as shown on the left of Figure 14, where rdetermines the number of squares. To .ll in the remaining gaps, 
we adjoin the triangles along the boundaries to the triangles of neighboring vertex charts using a strip 
of triangles. The corners are .lled in with an appropriate n­sided triangulation (see Figure 14). 9 Remarks 
and future work Images 17 22 show some example surfaces, most of which were created using an interactive 
editor. The coloring of Image 18 was determined by running a reaction-diffusion simulation on the man­ifold 
and using the resulting chemical concentrations to create the stripes[Tur91]. Becausetheembeddingde.nesametricontheman­ifold, 
we can use either that global metric or the local (chart) metric with the reaction-diffusion equations. 
This surface technique produces aesthetically pleasing models fairly ef.ciently and easily. It is also 
suitable for data .tting be­cause the topology of the surface can be made to .t the topology of the data, 
bringing the surface fairly close to the data initially. Addi­tionally, continuity constraints need not 
be maintained while .tting the surface to the data. Although this technique shares many of the properties 
of tradi­tional splines, some techniques have yet to be developed, such as the equivalent of knot insertion 
and the Oslo algorithm. Although we can use subdivision to produce a more .nely controllable sur­face 
similar in shape to the original surface, this surface is not nec­essarily identical to the original. 
Note that if the polyhedral mesh is rectangular then the resulting surface reduces to a B-spline surface. 
10 Implementation We have implemented a simple interactive polyhedral editor to cre­atethesurfacesshownhere. 
Theuserbuildsanarbitrarypolyhedron Pby creating vertices and connecting them together into faces. The 
system automatically creates a second polyhedral model Cwhich is the dual of the .rst subdivision surface 
of P. (Figure 16 shows both Pand Cfor the .ower model.) The user is free to move the ver­tices of C. 
The editor runs in real time on an HP-735 with surfaces of continuity C2and a triangulation level of 
4. 11 Acknowledgments We would like to thank our sponsors for providing the support for this research, 
Dan Robbins for help with the video, Michael Kowal­ski for the model of Alexander s two-holed torus, 
and Tony DeRose, Jean Schweitzer, Marc Levoy, and Jules Bloomenthal for their help­ful comments. A Projective 
Transformations Let B1 =(1,0,0)t ,B=(0,1,0)t ,B=(0,0,1)t,and B= 23 4 (1,1,1)tand let P1 Pbe the (column 
vectors of) homogeneous 4 coordinates of four points of the plane, no three colinear. There is amatrix 
TPsuch that TP(Bi)is a non-zero multiple of Pi.Here is the construction of TP:Let MPbe the 3x3matrix 
whose columns are P1,P2 ,and P3.Let A=MPi1 P4 . Letting .i(i=1,2,3) denote the entries of A, TPis the 
matrix whose columns are .1P1, .2P2and .3P3. To .nd a projective transformation on the plane taking any 
set of four points fPig, no three colinear, to any other such set fQig, Ti1 compute the matrix K=TQ. 
Multiplying Kby the vector P (x,y,1)tgives a vector (X(x,y),Y(x,y),W(x,y)). The projec­ Xlx.y) Ylx.y) 
tive transformation we seek is just (x,y)7-( Wlx.y),Wlx.y)). BCharts We describe the exact shape of each 
chart (a subset of 2)and the overlap regions Ucc 0. Each vertex chart is a unit square centered at the 
origin (see Fig­ure 5). A vertex chart overlaps four face charts (corresponding to the four faces having 
vas a vertex) and four edge charts (corre­sponding to the four edges having vas a vertex). The UVcare 
de­.ned to be 'cV(UcV).If UVFiand UVEjoverlap then emust be an edge of f. The face chart for an n-sided 
face is an n-sided regular polygon centered at the origin. The size of the polygon is chosen so that 
the perpendicular distance from the edge of the face chart to the edge of the unit polygon containing 
it is a constant h(see Figure 7). Typi­cally, his small. For the .gures in this paper a value of 1was 
used. A wedge of a polygon is the triangle whose vertices are the center of the polygon and the two ends 
of one side of the polygon. An n-sided face chart overlaps with nvertex charts (the ncor­ners of F)and 
nedge charts (the nedges of F). The UFViare bounded by lines drawn from the centroid of the polygon to 
the mid­points of the polygon edges. The UFEiare the parts of the chopped­off wedges mentioned above 
that actually lie in the chartF.If T UFVUFE6=then vis a vertex of e. The edge chart is a diamond with 
its left and right ends chopped off. Thediamondconsistsoftwotriangles,onecongruenttoawedge of a unit 
polygon with the same number of sides as each of the over­lapping face charts. The triangles are joined 
along the sides that cor­respond to the edge, and that side is placed along the x-axis. An edge chart 
overlaps two face charts and two vertex charts (the left and right sides of E). If f0 =f,v,v 0 ,gand 
f1 = f,v 0 ,v,gthen UEF0 is in the upper half plane, UEVis on the left, and UEV0 is on the right. C Transition 
functions The face-to-edge function is a rigid motion (see Figure 8). Let dn be the height of the wedge 
and ebe the amount of rotation: 'FE(s,t)=fscose-tsine,tcosessinedng Theface-to-vertextransition functionusesthe 
uniqueprojective transformation taking any four points in the plane, no three of which are collinear, 
to any other four such points (see Appendix A). Let cPQdenote the projective map that takes a corner 
of a unit polygon Pcontaining a face chart Fto a quadrant Qof a vertex chart V (see Figure 9). The function 
'FVis simply the restriction of cPQ to UFVand 'VFis c i1 restricted to UVF. PQ The edge-to-vertex transition 
is de.ned in terms of the other tran­sition functions. We de.ne 'EVto be 'F0Vo'EF0 on UEF0 and 'F1Vo'EF1 
on UEF1 so that the cocycle condition is satis.ed (see Figure 10). To complete our de.nition, we need 
only de.ne 'EV on the no-man s land, i.e., we must produce a smooth blend between S the two composite 
functions on the region UEV -(UEF0 UEF1 ). Recall that the regions UEFiare each at a distance hfrom the 
x-axis (this displacement by hwas included precisely to permit us do this blend). The choice of hwill 
affect the embedding function de.ned in Section 8, but does not affect the discussion here. To de.ne 
'EVwe .rst extend the domains of the functions 'EFi and 'FiV. 'EF0 is linear, so it extends to all of 
2; similarly for 'EF1 . Now we extend the domains of the functions 'FiVto the region 'EF0 (UEV -UEF1 
), i.e., the domain of the 'EF0 plus the no-man s land. The singularities of 'VF0 lie on a line that 
does not intersect 'EF0 (UEV -UEF1 )[Gri]. Therefore the composite func­tion canbeextendedto theregion 
UEV -UEF1 . A similar argument holds for 'EF1 . Using these extended functions, we de.ne 'EVby 'EV(x,y)=1(y)'EF0 
o'F0V(x,y) (1-1(y))'EF1 o'FV(x,y) 1 where 1:.-.is a blend function which is Ck,and is 1to the right of 
hand 0to the left of -h. In [Gri] we show that 'EVis invertible and 1 1 on UVE. D Dual of .rst subdivision 
surface Given a polyhedron P, the .rst subdivision surface P0of Pcon­tains a vertex for every vertex, 
edge, and face of P(see Figure 15). All of the faces of P0have exactly 4 sides [Kin77]. Taking the dual 
of this produces a polyhedron Cwith vertices of valence 4. Fig­ure 16 shows the original polyhedron Pin 
light blue, and the dual surface Cin green. The vertices of Care initially placed at the cen­troids of 
the faces of P0 . REFERENCES [BBB87] R. Bartels, J. Beatty, and B. Barsky. An Introduction to Splines 
for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann, 1987. [CC78] E. Catmull and J. 
Clark. Recursively Generated B-spline Surfaces on Arbitrary Topological Meshes. Computer Aided Design, 
10(6):350 355, November 1978. [Far88] G. Farin. Curves and Surfaces for Computer Aided Ge­ometric Design. 
Academic Press, 1988. [Gri] C. Grimm. Surfaces of Arbitrary Topology using Mani­folds. PhD thesis, Brown 
University (in progress). [HDK93] M. Halstead, T. DeRose, and M. Kass. Ef.cient, Fair Interpolation Using 
Catmull-Clark Surfaces. Computer Graphics, 27(2):35 44, July 1993. Proceedings of SIG-GRAPH 93 (Los Angeles). 
[HM90] K. Hollig and H. Mogerle. G-splines. Computer Aided Geometric Design, 7:197 207, 1990. [Kin77] 
P. King. On Local Combinatorial Pontrjagin Numbers (I). Topology, 16:99 106, 1977. [LD89] C. Loop and 
T. DeRose. A Multisided Generalization of B´ezier Surfaces. ACM TOG, 8(3):204 234, July 1989. [Loo87] 
C. Loop. Smooth Subdivision Surfaces Based on Trian­gles. Master s thesis, University of Utah, 1987. 
[Loo94] C. Loop. Smooth Spline Surfaces Over Irregular Meshes. Computer Graphics, 28(2):303 310, July 
1994. Proceedings of SIGGRAPH 94. [MS74] J. Milnor and J. Stasheff. Annals of Mathematical Stud­ies. 
Princeton University Press, Princeton, New Jersey, 1974. [MYV93] J. Maillot, H. Yahia, and A. Verroust. 
Interactive Texture Mapping. Computer Graphics, 27(4):27 35, July 1993. Proceedings of SIGGRAPH 93. [Spa66] 
E. H. Spanier. Algebraic Topology. McGraw-Hill Inc., New York, New York, 1966. [Spi70] M. Spivak. Differential 
Geometry Volume 1. Publish or Perish Inc., 1970. [ST67] I.M. Singer and J. A. Thorpe. Lecture Notes on 
Ele­mentary Topology and Geometry. Scott, Foresman and Company, Glenview, Illinois, 1967. [Ste74] N. 
Steenrod. The Topology of Fibre Bundles. Princeton University Press, Princeton, New Jersey, 1974. [Tur91] 
G. Turk. Generating Textures on Arbitrary Sur­faces Using Reaction-Diffusion. Computer Graphics, 25(2):289 
298, July 1991. Proceedings of SIGGRAPH 91 (Las Vegas). [WW92] W. Welch and A. Witkin. Variational Surface 
Modeling. Computer Graphics, 22(2):157 166, July 1992. Pro­ceedings of SIGGRAPH 92. [WW94] W. Welch and 
A. Witkin. Free Form Shape Design Using Triangulated Surfaces. Computer Graphics, 28(2):247 256, July 
1994. Proceedings of SIGGRAPH 94.   Figure 21: A laser-scanned image of a ceramic bunny, courtesy 
of Stanford University. Figure 22: An approximation of the laser-scanned bunny.  Figure 20: A two-holed 
torus colored by atlas page type (vertex, edge, or face).   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218486</article_id>
		<sort_key>369</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[A general construction scheme for unit quaternion curves with simple high order derivatives]]></title>
		<page_from>369</page_from>
		<page_to>376</page_to>
		<doi_number>10.1145/218380.218486</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218486</url>
		<keywords>
			<kw><![CDATA[B-spline]]></kw>
			<kw><![CDATA[Be&acute;zier]]></kw>
			<kw><![CDATA[hermite]]></kw>
			<kw><![CDATA[orientation]]></kw>
			<kw><![CDATA[quaternion]]></kw>
			<kw><![CDATA[rotation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P203741</person_id>
				<author_profile_id><![CDATA[81414595731]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Myoung-Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pohang University of Science and Technology (POSTECH), Pohang 790-784, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP79028604</person_id>
				<author_profile_id><![CDATA[81545573856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Myung-Soo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP79028593</person_id>
				<author_profile_id><![CDATA[81406593243]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sung]]></first_name>
				<middle_name><![CDATA[Yong]]></middle_name>
				<last_name><![CDATA[Shin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology (KAIST), Taejeon 305-701, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134086</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARe, A., CURRIN, B., GABRIEL, S., AND HUGHES, J. Smooth interpolation of orientations with angular velocity constraints using quaternions. Computer Graphics (Proc. of SIGGRAPH '92) (1992), pp. 313-320.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARRY, P., AND GOLDMANN, R. A recursive evaluation algorithm for a class of Catmull-Rom splines. Computer Graphics (Proc. of SIGGRAPH '88) (1988), pp. 199-204.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CURTIS, M. Matrix Groups, Springer-Verlag, 1972.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[DE BOOR, C. A Practical Guide to Splines, Springer-Verlag, 1978.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[HAMILTON, W. Elements of Quaternions (Volume I, II), Chelsea Publishing Company, 1969.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[JUNKINS, J., AND TURNER, J. Optimal continuous torque attitude maneuvers. J. Guidance and Control 3, 3 (1980), pp. 210-217.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[JUNKIES, J., AND TURNER, J. Optimal Spacecraft Rotational Maneuvers, Elsevier, 1986.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[JUTTLER, B. Visualization of moving objects using dual quaternion curves. Computers &amp; Graphics 18, 3 (1994), pp. 315-326.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[KIM, M.-J., KIM, M.-S., AND SHIN, S. A compact differential formula for the first derivative of a unit quaternion curve. To appear in J. of Visualization and Computer Animation (1995).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>791475</ref_obj_id>
				<ref_obj_pid>791214</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[KIM, M.-J., KIM, M.-S., AND SHIN, S. A C2-continuous B- spline quaternion curve interpolating a given sequence of solid orientations. Proc. of Computer Animation '95 (1995), pp. 72- 81.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[KIM, M.-S., AND RAM, K.-W. Interpolating solid orientations with circular blending quaternion curves. To appear in Computer-Aided Design (1995).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G. Smooth interpolation of orientation. Models and Techniques in Computer Animation (Proc. of Computer Animation '93) (1993), N. Thalmann and D. T. (Eds.), Eds., Springer-Verlag, pp. 75-93.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[NIELSON, G., AND HEILAND, R. Animated rotations using quaternions and splines on a 4D sphere. Programmirovanie(Russia) (July-August 1992), Springer-Verlag, pp. 17-27. English edition, Programming and Computer Software, Plenum Pub., New York.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[PLETINCKS, D. Quaternion calculus as a basic tool in computer graphics. The Visual Computer 5, 1 (1989), pp. 2-13.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[POBEGAILO, A. Modeling of C~ spherical and orientation splines. To appear in Proc. of Pacific Graphics ' 95 (1995).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[SCHLAG, J. Using geometric constructions to interpolate orientation with quaternions. Graphics GEMS H, Academic Press, 1992, pp. 377-380.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325242</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[SHOEMAKE, K. Animating rotation with quaternion curves. Computer Graphics (Proc. of SIGGRAPH '85) (1985), pp. 245-254.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[SHOEMAKE, K. Quaternion calculus for animation. Math for SIGGRAPH (ACM SIGGRAPH '91 Course Notes #2) (1991).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[WANG, W. Rational spherical curves. Presented at Int'l. Conf. on CAGD, Penang, Malaysia (July 4-8, 1994).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[WANG, W., AND JOE, B. Orientation interpolation in quaternion space using spherical biarcs. Proc. of Graphics Interface '93 (1993), pp. 24-32.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A General Construction Scheme for Unit Quaternion Curves with Simple High Order Derivatives1 Myoung-Jun 
Kim2, Myung-Soo Kim, and Sung Yong Shin32 2Korea Advanced Institute of Science and Technology (KAIST) 
3Pohang University of Science and Technology (POSTECH) ABSTRACT This paper proposes a new class of unit 
quaternion curves in SO 3 . A general method is developed that transforms a curve in R3 (de­.ned as a 
weighted sum of basis functions) into its unit quaternion analogue in SO 3 . Applying the method to well-known 
spline curves (such as B´ ezier, Hermite, and B-spline curves), we are able to construct various unit 
quaternion curves which share many im­portant differential properties with their original curves. Many 
of our naive common beliefs in geometry break down even in the simple non-Euclidean space S3 or SO 3 
. For exam­ple, the de Casteljau type construction of cubic B-spline quaternion curves does not preserve 
C2-continuity [10]. Through the use of decomposition into simple primitive quaternion curves, our quater­nion 
curves preserve most of the algebraic and differential properties of the original spline curves. CR Descriptors: 
I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling Curve, surface, solid, and object 
representation, Geometric algorithms. Keywords: Quaternion, rotation, orientation, SO 3 ezier, Hermite, 
B-spline ,B´ 1 INTRODUCTION In computer animation, it is very important to have appropriate tools to 
produce smooth and natural motions of a rigid body. A rigidmotionin R3 can be represented by a Cartesian 
product of two component curves: one in the Euclidean space R3 and the other in the rotation group SO 
3 [3, 12]. The curve in R3 represents the center position of the rigid body, and the other curve in SO 
3 represents its orientation. Often, techniques for specifying a rigid motion construct the two curves 
independently. It is relatively easy to produce smooth curves in the Euclidean space. B-spline, Hermite, 
and B´ ezier curves exemplify well-known techniques for constructing position curves in R3. However, 
smooth orientation curves in SO 3 are much more dif.cult to construct. 1This research was partially supported 
by the Korean Ministry of Science and Technology under the contract 94-S-05-A-03 of STEP 2000 and TGRC-KOSEF. 
2Computer Science Department, KAIST, Taejeon 305-701, Korea. 3Dept. of Computer Science, POSTECH, Pohang 
790-784, Korea. E-mail: fmjkim, syshing@jupiter.kaist.ac.kr and mskim@vision.postech.ac.kr. Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 The spline curves 
in R3 are usually de.ned in two different but equivalent ways: i.e., either as an algebraic construction 
using basis functions or as a geometric construction based on recursive linear interpolations [2]. This 
paper proposes a general framework which extends the algebraic construction methods to SO 3 . Most of 
the previous methods are based on extending the linear interpolation in R3 to the slerp (spherical linear 
interpolation) in SO 3 [14, 16, 17, 18]. The two (i.e., algebraic and geometric) construction schemes 
generate identical curves in R3; however, this is not the case in the non-Euclidean space SO 3 [10]. 
Many of our commonplace beliefs in geometry break down in the non-Euclidean space SO 3 . When the recursive 
curve construction is not based on a simple closed form algebraic equation, it becomes extremely dif.cult 
to do any extensive analysis on the constructed curve. For example, consider the problem of computing 
the .rst derivative of the cubic B´ ezier quaternion curve or the squad curve generated by a recursive 
slerp construction [17, 18]. Though Shoemake [17, 18] postulates correct .rst derivatives, the quaternion 
calculus employed there is incorrect (see [9] for more details). Kim, Kim, and Shin [9] developed a correct 
quaternion calculus for the .rst derivatives of unit quaternion curves; however, an extension to the 
second derivatives becomes much more complex (also see [11] for a related quaternion calculus on blending-type 
quaternion curves). More seriously, the C2-continuity of a spline curve in R3 may not carry over to SO 
3 . Furthermore, the curve conversion be­tween two different spline curves, e.g., from a cubic B´ ezier 
curve to the corresponding cubic B-spline curve, may not work out in SO 3 [10]. Kim, Kim, and Shin [10] 
show that there is no C2-continuity guaranteed for the cubic B-spline quaternion curves which are gen­erated 
by the recursive slerp construction of Schlag [16]. Similarly, the B-spline quaternion curve of Nielson 
and Heiland [13] also fails to have C2-continuity (see [10] for more details). In this paper, we take 
an important step toward generalizing the algebraic formulation of spline curves in R3 to similar ones 
in SO 3 . In the new algebraic formulation, the computation of high order derivatives becomes almost 
as easy as that of the spline curves in R3. We show that the quaternion curves generated in this way 
preserve many important differential properties of their original curves in R3. They are de.ned in simple 
closed algebraic forms of quaternion equations, and they have the same degrees of geometric continuity 
as their counterparts in R3. As a demonstration of the feasibility of our proposed method, we .rst construct 
a B´control unit ezier quaternion curve with nquaternions. Then, the cubic B´ ezier quaternion curve 
is used to construct a Hermite quaternion curve. We also construct a k-th order B-spline quaternion curve 
which is Ck-2-continuous and lo­cally controllable. There are many other spline curves which can be de.ned 
by weighted sums of control points; our construction method is general in that all these spline curves 
are extendible to their corresponding quaternion curves. Since it is possible to ma­nipulate the position 
curve as well as the corresponding orientation curve in a uni.ed manner, the modeling and manipulation 
of rigid motions can be managed more conveniently. The key to the success of our construction method 
is that the quaternion curve is formulated as a product of simple primitive quaternion curves: qit=exp 
wifit, i=1,,n,for some .xed angular velocity wi2R3 and real valued function fit, where exp : R3 -SO3 
is an exponential map [3]. Each primitive curve, qit, represents a rotation about a .xed axis, wi2R3, 
for which the derivative formula has an extremely 000 simple form: qit=exp wifitwifit=qitwifit. Since 
the chain rule can be applied to the quaternion product: qt=q1 tqnt, the resulting differential formula 
has the simplest expression that one can expect for unit quaternion curves. Furthermore, a similar technique 
can be used to obtain high order derivatives of our quaternion curves. There have been many other methods 
suggested for the construction of quaternion curves, in­cluding the CAGD approaches based on constructing 
rational curves on the 3-sphere S3 [8, 20]. Nevertheless, no method has provided such a simple derivative 
formula. The only exception is the method of Pobegailo [15] which constructs a Ck-continuous spherical 
curve as a product of k+1 rotation matrices: Rk+1RkR1. The gen­erated curve is, however, essentially 
restricted to the B´ ezier type quaternion curve of our method. The development of closed form equations 
for the second order derivatives provides an important step toward solving the important problem of torque 
minimization for 3D rotations [1, 6, 7]. How­ever, the torque minimization problem requires much more. 
The optimal solution is given as a critical path in the problem of cal­culus of variations among a set 
of quaternion paths which satisfy the given conditions. For this purpose, we may need to extend the quaternion 
curve construction scheme of this paper to that of quaternion surfaces and volumes. This is currently 
a dif.cult open problem. Therefore, the important problem of torque minimization for 3D rotations has 
not been solved in this paper. Nevertheless, the basic algebraic approach taken in this paper provides 
an important conceptual framework for future research toward this goal. The rest of this paper is organized 
as follows. In Section 2, we brie.y review some useful concepts and de.nitions related to unit quaternions. 
Section 3 describes the main motivation of this work. Section 4 outlines our basic idea for constructing 
unit quater­nion curves. Section 5 constructs the B´ ezier, Hermite, and B-spline quaternion curves and 
discusses their differential properties. Sec­tion 6 shows some experimental results with discussions 
on possible further extensions. Finally, in Section 7, we conclude this paper. 2 PRELIMINARIES 2.1 Quaternion 
and Rotation Given a unit quaternion q2S3, a 3D rotation Rq2SO3 is de.ned as follows: -13 Rqp=qpq,for 
p2R, 1 where p=x,y,zis interpreted as a quaternion 0,x,y,zand the quaternion multiplication is assumed 
for the products [3, 17, 18]. Let q=cos 0,v sin 02S3, for some angle 0and unit vector v 2S2,then Rqis 
the rotation by angle 20about the axis v.The multiplicative constant, 2, in the angle of rotation, 20, 
is due to the fact that qappears twice in Equation (1). Also note that Rq=R-q; that is, two antipodal 
points, qand -qin S3, represent the same rotation in SO3. The two spaces S3 and SO3 have the same local 
topology and geometry. The major difference is in the distance measures of the two spaces SO3 and S3. 
A rotation curve Rq(t)2SO3 is twice as long as the corresponding unit quaternion curve qt2S3.When a smooth 
rotation curve Rq(t)has an angular velocity 2wt2R3, the unit quaternion curve qt2S3 satis.es: 0 qt=qtwt 
2 In this paper, we construct the unit quaternion curves in S3, instead of SO3 ; therefore, we interpret 
the velocity component wtof 0 qtas the angular velocity, instead of 2wt. The unit quaternion multiplication 
is not commutative; the or­der of multiplication is thus very important. Let q1,q2,,qnbe a sequence of 
successive rotations. When each qiis measured in the global frame, the product qnqn-1q1 is the net rotation 
of successive rotations. However, when each qiis measured in the local frame, the .nal product q1q2qnrepresents 
the net rota­tion. Note that the latter is the same as the successive rotations of qn,qn-1,,q2,q1 in 
the global frame. Here, we assume each rotation is speci.ed in the local frame. This is simply for notational 
convenience; in the local frame, we can write the multiplications in the same order as the rotations. 
By reversing the order of quaternion multiplications, the same construction schemes can be applied to 
the quaternion curves de.ned in the global frame. 2.2 Exponential and Logarithmic Maps Given a vector 
v=0v 2R3, with v2S2, the exponential: X 1 exp v=v i=cos 0,v sin 02S 3 , i 0 becomes the unit quaternion 
which represent the rotation by angle 20about the axis v,where v iis computed using the quaternion mul­tiplication 
[3, 11, 18]. The exponential map exp can be interpreted as a mapping from the angular velocity vector 
(measured in S3)into the unit quaternion which represents the rotation. When we limit the domain of exp 
so that j0j, the map exp becomes 1-to-1 and its inverse map log can be de.ned for unit quaternions. (See 
[9] for more details on exp and log.) The power of a unit quaternion qwith respect to a real valued exponent 
ais de.ned to be a unit a quaternion: q=exp alog q. Given two unit quaternions q1 and q2, the geodesic 
curve ,q1 ,q2 2 S3 (which connects q1 and q2) has constant tangential velocity -1 log q1 q2 . The geodesic 
curve equation is given by: -1 -1 t ,q1,q2 t=q1 exp t log q1 q2 =q1 q1 q2 3 The geodesic ,q1,q2 is also 
called the slerp (spherical linear inter­polation) between q1 and q2.  3 MOTIVATION For a translational 
motion in R3, the position curve ptis repre­sented by: Z pt=vtdt+p0, 4 where vtis the velocity and p0 
is the initial position. This relation can also be represented by the following linear differential equation: 
0 pt=vt 5 However, as shown in Equation (1), the relation between qtand 0 wtis non-linear: qt=qtwt. One 
of the most important problems in aero-space engineering is how to .nd a torque optimal rotational path 
which connects the start and target orientations [7]. Many numerical methods have been suggested to .nd 
the optimal path, in which Equation (1) plays an important role as governing equation [6]. Barr et al. 
[1] also raised torque optimization as a signi.cant problem in computer animation, and they constructed 
a torque optimal piecewise slerp quaternion path (i.e., as a sequence of discrete unit quaternions) by 
using a non-linear optimization technique. There still remains the important open problem of how to construct 
such a torque optimal path with piecewise spline quaternion curves which have simple closed form equations. 
An immediate open problem concerns the computation of high order derivatives of a unit quaternion curve. 
In this paper, we resolve the crucial problem of how to formulate the quaternion curves and their high 
order derivatives as closed form equations. For a unit quaternion curve qt2S3, we may reformulate the 
curve in the following equivalent form: qt=exp log qt By applying the chain rule to this equation, we 
obtain the .rst derivative formula: d 0 qt=dexp log(q(t))log qt, dt where dexp log(q(t))is the differential 
(i.e., Jacobian matrix) of exp. Kim, Kim, and Shin [9] used this formula to provide simple C1-continuity 
proofs for various previous methods [11, 17, 18]. The second and high order derivative formulas, however, 
become extremely complex even with this representation. Moreover, for general quaternion curves qt, their 
logarithmic curves, log qt, also have very complex formulas. In this paper, we exploit simple primitive 
quaternion curves qtwhich allow simple formulas for both the high order derivatives and the logarithmic 
curves. Shoemake [18] used the formula: aaa-1 dq=qlog qda+aqdq 6 In general, this formula is incorrect. 
For example, Equation (6) claims that: dq2 =2qdq. However, we have: 2 dq=dqq=dqq+qdq=62qdq since the 
quaternion multiplication is not commutative (also see [9] for more details). Nevertheless, there is 
a special case where this formula works. When the quaternion curve qtis restricted to the rotation around 
a .xed axis w2R3: i.e., qt=exp wat, 0 for a real-valued function at, qthas a simple form: 0 00 qt=exp 
watwat=qtwat, which is equivalent to the formula of Equation (6) in this special case. Higher order derivatives 
of qtare also easy to compute by the chain rule. To make good use of this simple differential property, 
all the unit quaternion curves of this paper will be constructed as the products of primitive quaternion 
curves: qit=exp wiait, i=1,,n, for a .xed angular velocity wi2R3 and a real valued function ait. Since 
the chain rule can be applied to the quaternion prod­uct: qt=q1 tqnt, the quaternion curve derivative 
can be obtained in an extremely simple form. Furthermore, applying a similar technique recursively, high 
order derivatives can also be obtained in simple forms. In this paper, we construct each com­ponent quaternion 
curve qitto be Ck-continuous by choosing Ck-continuous basis function ait. Therefore, their quaternion 
product qt=q1 tqntbecomes Ck-continuous. q 1 .1 q . n 2 q 0 . n q 2 Figure 1: Piecewise Slerp Interpolation 
of fwig.  4 BASIC IDEA 4.1 Cumulative Form Given a sequence of points p0,p1,,pn2R3, the simplest C0­continuous 
curve pt2R3, which interpolates each point piat t=i, is given by the following linear interpolation: 
pt=p0 +a1 t.p1 +a2 t.p2 ++ant.pn P n = p0 +ait.pi, i1 where .pi =pi-pi-1 i 0 if ti ait=t-iif i:ti+1 1 
if t i+1 Similarly, given a sequence of unit quaternions q0 ,,q2S3, we can construct a C0-continuous 
unit quaternion curve qtn2S3, which interpolates each unit quaternion qiat t=i, as follows: qt=q0 exp 
w1a1 texp w2a2 texp wnant Q n =q0 exp wiait, i1 where -1 wi =log qi-1qi This is a piecewise slerp (spherical 
linear interpolation) of fqig(see Figure 1). In the rotational space, a slerp implies a rotation with 
a constant angular velocity (around a .xed rotation axis). The slerp curve segment joining qiand qi+1 
is the geodesic interpolation, which is given by: qiexp wiait, based on Euler s theorem of principal 
rotation. At this point, we may generalize faigto other functions, rather than a piecewise linear function, 
so that the resulting quaternion curve becomes Ck-continuous while interpolating the given se­quence 
of unit quaternions. The two sequences faigand fwigcan be viewed as basis functions and their coef.cients, 
respectively. Q We call q0 exp wiaithe cumulative form of qtwith their co­ef.cients fwig. The cumulative 
form has several advantages over other quaternion curve representations: 1. It has a simple closed form 
equation, which simpli.es the evaluation of curve points and reduces the numerical errors.  2. It facilitates 
straight-forward computations of high order deriva­tives. 3. Using Ck-continuous basis functions faig, 
we can easily construct Ck-continuous quaternion curves, which are further controlled by the coef.cients 
fwig. 4. A well-chosen set of basis functions makes the constructed quaternion curves locally controllable. 
 5. Since kexp awk=1, the quaternion curves are in S3.   4.2 Cumulative Basis In the Euclidean space 
R3, there are many well-known spline curve construction schemes such as B´Most ezier and B-spline curves. 
of the spline curves are represented as the sums of basis functions with their control points as the 
coef.cients. Let fBigbe the basis functions and fpigbe the control points. Then, the spline curve pt, 
determined by fpig, is given by: X n pt=piBit, i0 which is called the basis form of pt. We present a 
general scheme that converts the basis form to the cumulative form of the quaternion curve. Using this 
method, we can easily construct various unit quaternion curves from their analogues in the Euclidean 
space R3. The basis form can be .rst converted into the following form: X n pt=p0Be0 t+.piBeit, i1 where 
.pi =piP -pi-1 , n Beit= ji Bit Then, the corresponding quaternion curve is obtained as follows: Y n 
Be(t) 0 qt=qexp wiBeit, 7 0 i1 -1 by converting ptto qt, p0 to q0, .pito wi =log qi-1qi, and vector addition 
to quaternion multiplication. Equation (7) is given in a cumulative form. One needs to be extremely careful 
in -1 the order of multiplication: qi-1qi. If the angular velocities are -1 given in the global frame, 
the quaternion multiplication: qiqi-1 -1 should be used, instead of qi-1qi. The new basis fBeigis called 
the cumulative basis of fBig. The cumulative form is the basic tool for our quaternion curve construction 
in SO3 .Simply by deriving a cumulative basis fBeig, we can easily obtain a quaternion curve which shares 
many important differential properties with its counterpart in the Euclidean space R3.  5 A NEW CLASS 
OF QUATERNION CURVES 5.1 B´ezier Quaternion Curve We can represent an n-th order B´ezier curve with Bernstein 
basis pe n-ii !i,nt= ni 1 -ttas follows: X n pt=pi!i,nt, 8 i0 where pi s are the control points. For 
the B´ ezier curve given in a basis form, we can apply our quaternion curve construction method. We .rst 
reformulate Equation (8) as follows: X n pt=p0! 0,nt+.pi! i,nt, i1 where the cumulative basis functions 
are given by: X n ! i,nt=!i,nt 9 ji Then, by converting it to the cumulative form, we can obtain the 
n-th order B´as follows: ezier quaternion curve with control points fqig Y n qt=q0 exp wi! i,nt, 10 i1 
where -1 wi =log qi-1qi Note that ! 0,nt1. This B´ =ezier quaternion curve has a different shape from 
the B´ ezier quaternion curve of Shoemake [17]. 5.2 Hermite Quaternion Curve A cubic Hermite curve is 
de.ned by two end points, paand pb,and two end velocities, vaand vb. Alternatively, the Hermite curve 
can be represented by a cubic B´ ezier curve: 3 X pt=pi!it, 11 i0 with the condition: p0 =pa,p1 =pa+va/3,p2 
=pb-vb/3,p3 =pb,12 where !it=!i,3 tfor i=0,1,2,3. Similarly, a cubic B´ ezier quaternion curve can be 
used to de.ne a Hermite quaternion curve which interpolates two end unit quater­nions, qaand qb, and 
two end angular velocities, waand wb. From Equation (10), the cubic B´ ezier quaternion curve is given 
by: 3 Y qt=q0 exp wi! it, 13 i1 where ! it=! i,3 tfor i=1,2,3. The quaternion counterpart of Equations 
(12) is given by: -1 q0 =qa,q1 =qaexp wa/3 ,q2 =qbexp wb/3 ,q3 =qb These four identities determine the 
three coef.cients wiof the cubic B´ ezier quaternion curve in Equation (13) as follows: -1 -1 w1 =log 
q0 q1 =log qaqaexp wa/3 =wa/3 -1 -1 -1 -1 w2 =log q1 q2 =log exp wa/3 qaqbexp wb/3 -1 -1 w3 =log q2 q3 
=log exp wb/3 qbqb =wb/3 Using these three angular velocities, we can construct a cubic Her­mite quaternion 
curve from Equation (13). Note that we can assign arbitrarily large angular velocities at the curve end 
points. The angular velocity w2 provides an extra degree of freedom in choos­ing the number nof revolutions 
while not losing the end point interpolation property. That is, instead of w2,we may use w2 +wb2 nfor 
an integer n, where wb2 =w2/kw2k. The curve shape changes, depending on the number of revolutions. (Note 
that the angular velocity is measured here in S3; therefore, the magnitude is half the rotation in the 
physical space.) Figure 2 shows the graphs of basis functions ! i s. Figure 3 shows Hermite quaternion 
curves with the same control points, but with different angular velocities wb s. Since it is dif.cult 
to visualize the quaternion curves in S3, they are projected onto a unit sphere in R3. Now, we will show 
that the cubic Hermite quaternion curve interpolates the two orientations, qaand qb, and the two angular 
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 Figure 2: Graphs of !i. 1 qa  Figure 3: Examples of Hermite Quaternion 
Curves. velocities, waand wb, at the curve end points. It is easy to show that: q0 =q0exp0 exp0 exp0 
=q0 =qa, q1 =q0 exp w1 exp w2 exp w3 -1 -1 -1 =q0 qq1 qq2 qq3 012 =q3 =qb The .rst derivative of qtis 
given by: 0 . . qt=q0 exp w1!1 tw1!1 texp w2!2 texp w3!3 t +q0 exp w1!1 texp w2!2 tw2!2 texp w3!3 t 
 . +q0 exp w1!1 texp w2!2 texp w3!3 tw3!3 t where 3 23 3 !1 t=1 -1 -t,!2 t=3t-2t,!3 t=t, . 2. . 2 !t=31 
-t,!t=6t1 -t,!t=3t 123 Thus, we have: 0 q0 =q0 exp 0 w13 exp0 exp0 +q0 exp w1!1 texp 0 w20 exp w3!3 t 
+q0 exp w1!1 texp w2!2 texp 0 w30 =q0wa =qawa 0 q1 =q0 exp w1 w10 exp w2 exp w3 +q0 exp w1 exp w2 w20 
exp w3 +q0 exp w1 exp w2 exp w3 w33 -1 -1 -1 =q0qq1qq2qq33w3 012 =q3wb =qbwb, which means that the quaternion 
curve qthas its angular velocities -1 0-1 0-1 0-1 0 wa =qq0 =q0 q0 and wb =qq1 =q1 q1 ab at the curve 
end points. 5.3 B-spline Quaternion Curve The B-spline curve is very popular in computer graphics because 
of its extreme smoothness and local controllability. By moving a control point, we can selectively modify 
the B-spline curve without losing its geometric continuity. In this section, we consider how to convert 
a B-spline curve in R3 into its quaternion analogue with a cumulative form. Then, we investigate the 
properties such as Ck-continuity and local controllability. A B-spline curve is de.ned by a weighted 
sum of B-spline basis functions Bi,kt, which are Ck-2-continuous k-1 -th order piecewise polynomials. 
Given a set of control points fpig,a B-spline curve ptis given by: X n pt=piBi,kt i0 The B-spline basis 
functions Bi,ktare de.ned by the following recurrence relation [4]: 1if titti+1 Bi,1 t= 0 otherwise 
and t-ti ti+k-t Bi,kt= Bi,k-1 t+ Bi+1,k-1 t ti+k-1 -ti ti+k-ti+1 It is easy to show that Bi,k s are 
Ck-2-continuous piecewise poly­nomials of degree (k-1). They are Ck-2-continuous every­where, and may 
not be Ck-1-continuous only at the knots ftig. Each Bi,kthas a non-zero support on the interval [ti,ti+k], 
i.e., Bi,kt=0for ttior tti+k. The B-spline curve may be reformulated in the following cu­mulative form: 
X n pt=p0Be0,kt+.piBei,kt, i1 where .pi =pi-pi-1 and P n Bei,kt=Bj,kt 8 ji P i+k ji Bj,ktif titti+k-1 
=1 if tti+k-1 0 if t:ti By converting ptto qt, p0 to q0, .pito wi, and summation to quaternion multiplication, 
the corresponding quaternion curve is obtained in a cumulative form as follows: n Y Be 0 k(t) qt=q0 exp 
wiBei,kt, i1 -1 where wi =log qi-1qi. This gives our B-spline quaternion curve, which is Ck-2-continuous 
and locally controllable with the control points fqigand the angular velocities fwig. The B-spline quater­nion 
curve also allows arbitrarily large angular velocities between two consecutive control points fqig. Figures 
4 and 5 show the graphs of basis functions Bi,4 tand Bei,4 t, respectively. Note that Bei,ktis non-constant 
only in the interval [ti,ti+k-1], whereas Bi,ktis non-constant in [ti,ti+k]. Figure 6 shows examples 
of B­spline quaternion curves. The local shape controllability is shown with dashed curves. 0.7 B0 4 
t 0.6 B1 4 t 0.5 B2 4 t 0.4 0.3 0.2 0.1 0 0123456 Figure 4: B-spline Basis Functions. 1.2 1 e B0 4 t 
0.8 e B1 4 t 0.6 e B2 4 t 0.4 0.2 0 0123456 Figure 5: Cumulative B-spline Basis Functions. Now, we can 
investigate some interesting differential and ge­ometric properties of the B-spline quaternion curve: 
i.e., Ck­continuity and local controllability. It is easy to show that Bei,kt is Ck-2-continuous since 
it is a weighted sum of Ck-2-continuous functions fBi,kg. Therefore, the k-th order B-spline quaternion 
curve is Ck-2-continuous as well. Since the cumulative basis func­tion Bei,ktis non-constant in the interval 
[ti,ti+k-1], the quater­nion curve qton the interval tl:ttl+1 can be represented by: QQ l-k+1 l e qt=q0expwi1 
expwiBi,kt i1 il-k+2 pQe n expwi0 il+1 Q l =ql-k+1 expwiBei,kt il-k+2 This equation shows that the quaternion 
curve qtdepends only on the quantities: ql-k+1, wl-k+1,,wl, that is, only on the k control points: ql-k+1,ql-k,,ql. 
In other words, by moving the control point ql, the curve shape is in.uenced only on the interval [tl,tl+k]. 
0 Furthermore, the derivation of qtis extremely simple due to the B-spline differentiation formula [4]: 
XX dai-ai-1 aiBi,kt=k-1 Bi,k-1t, dt ti+k-1 -ti where ai s are constants. Therefore, we have: e 0 k-1B= 
Bi,k-1t i,kt-t t i.k.1i In the case of a uniform B-spline, there is a further reduction to: 0 Bei,kt=Bi,k-1t. 
q4 q6 q5 q3 q1 q2 Figure 6: B-spline Quaternion Curve with Six Control Points.   6 EXPERIMENTAL RESULTS 
6.1 Torque Computation The spline interpolation in R3 produces a path pwhich minimizes the energy: Z 
2 kp¨kdt, while satisfying given constraints. Thus, the spline interpolation path does not generate unnecessary 
force (or acceleration). The energy minimization property makes the spline interpolation very useful. 
As far as rotational motion is concerned, it is important to minimize torque (or angular acceleration). 
Most of the previous results on quaternion interpolation have concentrated on improving the computational 
ef.ciency rather than attacking the more chal­lenging problem of energy minimization. Barr et al. [1] 
took an important step toward the energy min­imization. The quaternion path is approximated by discrete 
unit quaternions and a time-consuming non-linear optimization is em­ployed in the algorithm. There still 
remains an important open prob­lem concerning how to construct an energy minimization quaternion path, 
ideally with a closed form equation and without using a time­consuming optimization technique. In this 
paper, we have provided a useful step toward the resolution of this problem by introducing a new class 
of spline quaternion curves for which high order deriva­tives can be obtained in simple closed form equations. 
Although the problem of how to deal with these closed form equations is still unsolved, our quaternion 
curves exhibit promising behaviors by generating relatively small torque compared with those generated 
by the previous methods. Our cubic quaternion curves generate a similar amount of torque (in the range 
of ±5%) to that of Shoemake [17] when the control unit quaternions have small angular variations. However, 
our curves perform much better when the variations are large. The meth­ods based on recursive slerp constructions 
may generate twisted curves when the spherical control polygon on S3 has edges of rel­atively large lengths. 
This effect can be explained as follows. The slerp-based methods generate a sequence of intermediate 
quaternion curves qi,kt s, and blend each pair of two consecutive curves qi,ktand qi+1,ktto generate 
another sequence of quaternion curves qi,k+1t s: -1 qi,k+1t=qi,ktexpfitlogqi,ktqi+1,kt, for some blending 
function fit. When the variations of con­trol quaternions are large, the two quaternion curves qi,ktand 
qi+1,kthave large curve lengths. They may have complex wind­ing shapes in the compact space S3 . The 
difference quaternion curve -1 qi,ktqi+1,ktwould also wind many times. However, the -1 measure, log qi,ktqi+1,kt, 
is always bounded by ,which totally ignores the large amount of winding. As a result, the blend­ing curve 
qi,k+1 texperiences large bending, which produces large torque. As the intermediate curves with bending 
shapes are blended recursively, the resulting quaternion curve has a twisting shape. Our quaternion curves 
do not suffer from such a degener­acy. This is because our primitive quaternion curve qit= exp wiaitmay 
accommodate arbitrarily large angular velocity wi. The resulting curve has a large number of winding; 
however, the curve does not produce extraordinary bending and/or twisting. Therefore, our curves perform 
much better when the angular varia­tions are large. Furthermore, our curves have C2-continuity, which 
means that there is no torque jump at the curve joint. The piecewise cubic quaternion curves (based on 
the de Casteljau type construc­tion) are not C2-continuous; they have torque jump at each curve joint. 
High degree rational spherical curves have C2-continuity [19]; however, their speeds are less uniform 
than the curves based on the de Casteljau type construction. Therefore, the rational curves generate 
redundant tangential accelerations, which has undesirable effect.  6.2 Animation Examples We present 
some examples to demonstrate the feasibility of our quaternion curves. Figure 7 shows an animation of 
a .ying boomerang. In this example, the motion path is composed of a Hermite curve for the translation 
and a Hermite quaternion curve for the rotation. Using the Hermite quaternion curve, we can specify arbitrary 
ori­entations and angular velocities of the boomerang at the start and end of the animated motion. Note 
that the boomerang experiences many revolutions. This effect is obtained by assigning large angular velocities 
at both ends. Our cubic B-spline quaternion curve produces extremely smooth motions. Figure 8 shows a 
motion path, which is speci.ed by a B-spline curve with six control points. A rigid motion path can be 
used to specify the sweeping of a 2D cross section. Figure 9 shows a sweep object generated from the 
same motion path given in Figure 8. Figure 10 shows an example of B-spline quaternion interpolation for 
a rigid body, where the position and orientation interpolation curves are constructed by the B-spline 
interpolation curves in R3 and SO3 , respectively. Six keyframes are used in this example, and they are 
shown in dark tints.  7 CONCLUSIONS A general construction method is proposed for unit quaternion curves. 
Given a spline curve in R3, the spline curve is reformulated in a cumulative basis form and the corresponding 
quaternion curve is constructed by converting each vector addition into the quater­nion multiplication. 
The quaternion curve is formulated as a .nite product of simple quaternion curves, which makes the evaluation 
of high order derivatives quite straightforward. The constructed quaternion curves preserve many important 
differential properties of their counterparts in R3. Furthermore, the quaternion curves and their high 
order derivatives are given by simple closed form equations. Experimental results are quite promising 
in that our quaternion curves use small torque compared with the previous quaternion curves, especially 
when the control quaternions have large varia­tions. Although the important torque minimization problem 
has not been solved in this paper, our approach provides an important initial step toward this goal. 
Figure 7: Boomerang Animation using one Hermite Quaternion Curve. Figure 8: Rigid Motion by a Cubic B-spline 
Quaternion Curve. Figure 9: Sweeping with Cubic B-spline Quaternion Curves. Figure 10: Example of B-spline 
Quaternion Interpolation.  REFERENCES [1] BARR,A., CURRIN,B., GABRIEL,S., AND HUGHES, J. Smooth interpolation 
of orientations with angular velocity constraints using quaternions. Computer Graphics (Proc. of SIGGRAPH 
92) (1992), pp. 313 320. [2] BARRY,P., AND GOLDMANN, R. A recursive evaluation algo­rithm for a class 
of Catmull-Rom splines. Computer Graphics (Proc. of SIGGRAPH 88) (1988), pp. 199 204. [3] CURTIS,M. Matrix 
Groups, Springer-Verlag, 1972. [4] DE BOOR,C. A Practical Guide to Splines, Springer-Verlag, 1978. [5] 
HAMILTON,W. Elements of Quaternions (Volume I, II), Chelsea Publishing Company, 1969. [6] JUNKINS,J., 
AND TURNER, J. Optimal continuous torque attitude maneuvers. J. Guidance and Control 3, 3 (1980), pp. 
210 217. [7] JUNKINS,J., AND TURNER,J. Optimal Spacecraft Rotational Maneuvers, Elsevier, 1986. [8] JUTTLER, 
B. Visualization of moving objects using dual quaternion curves. Computers &#38; Graphics 18, 3 (1994), 
pp. 315 326. [9] KIM,M.-J., KIM,M.-S., AND SHIN, S. A compact differential formula for the .rst derivative 
of a unit quaternion curve. To appear in J. of Visualization and Computer Animation (1995). [10] KIM,M.-J., 
KIM,M.-S., AND SHIN,S. A C2-continuous B­spline quaternion curve interpolating a given sequence of solid 
orientations. Proc. of Computer Animation 95 (1995), pp. 72 81. [11] KIM,M.-S., AND NAM, K.-W. Interpolating 
solid orienta­tions with circular blending quaternion curves. To appear in Computer-Aided Design (1995). 
[12] NIELSON, G. Smooth interpolation of orientation. Models and Techniques in Computer Animation (Proc. 
of Computer Animation 93) (1993), N. Thalmann and D. T. (Eds.), Eds., Springer-Verlag, pp. 75 93. [13] 
NIELSON,G., AND HEILAND, R. Animated rotations using quaternions and splines on a 4D sphere. Pro­grammirovanie(Russia) 
(July-August 1992), Springer-Verlag, pp. 17 27. English edition, Programming and Computer Soft­ware, 
Plenum Pub., New York. [14] PLETINCKS, D. Quaternion calculus as a basic tool in computer graphics. The 
Visual Computer 5, 1 (1989), pp. 2 13. [15] POBEGAILO, A. Modeling of Cnspherical and orientation splines. 
To appear in Proc. of Paci.c Graphics 95 (1995). [16] SCHLAG, J. Using geometric constructions to interpolate 
orien­tation with quaternions. Graphics GEMS II, Academic Press, 1992, pp. 377 380. [17] SHOEMAKE, K. 
Animating rotation with quaternion curves. Computer Graphics (Proc. of SIGGRAPH 85) (1985), pp. 245 254. 
[18] SHOEMAKE, K. Quaternion calculus for animation. Math for SIGGRAPH (ACM SIGGRAPH 91 Course Notes 
#2) (1991). [19] WANG, W. Rational spherical curves. Presented at Int l. Conf. on CAGD, Penang, Malaysia 
(July 4-8, 1994). [20] WANG,W., AND JOE, B. Orientation interpolation in quater­nion space using spherical 
biarcs. Proc. of Graphics Interface 93 (1993), pp. 24 32. APPENDIX In this appendix, we provide the 
pseudo codes for the construction of quaternion curves presented in Section 5. ezier Quaternion Curve 
B´ // q0,,qn: control points Ppe nnn-ii double ! i,nt= jii1 -tt quaternion Bezier q0,q1,qnt Q n -1 return 
(q0 i1 exp log qi-1qi!i,nt; end Hermite Quaternion Curve // qa,qb: the start and end orientations // 
wa,wb: the start and end angular velocities 3 23 3 double ! 1 t=1 -1 -t, !2 t=3t-2t, !3 t=t quaternion 
Hermite qa,qb,wa,wbt q0 =qa; w1 =wa/3; -1 -1 w2 =log exp w1 qaqbexp w3; w3 =wb/3; return (q0 exp w1! 
1 texp w2! 2 texp w3! 3 t); end B-spline Quaternion Curve // ti: knot sequence double Bi,kt= B-spline 
basis of order k double Bei,kt= Pi+k Bj,kt ji quaternion B-spline q0,q1,qnt l=maxfijti+k-1 :tg; if (l0) 
then e B0 k(t) fl=0; q=qg 0 else q=ql; for (i=l+1; i:n&#38;&#38; tit; i++) -1 q=qexp log qi-1qiBei,kt; 
return (q); end  Uniform B-spline Quaternion Curve // ti =i-2: uniform knots // 0 :t:n // uniform-B-spline 
0 =q0 // uniform-B-spline n=qn // uniform-B-spline i.qi quaternion uniform-B-spline q0,q1,qnt -1 q-1 
=q0q1 q0; //phantom -1 qn+1 =qnqn-1qn;// control points l=bt-1c; q=ql; for (i=l+1; it+2; i++) -1 q=qexp 
log qi-1qiBei,kt; return (q); end  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218488</article_id>
		<sort_key>377</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[X-splines]]></title>
		<subtitle><![CDATA[a spline model designed for the end-user]]></subtitle>
		<page_from>377</page_from>
		<page_to>386</page_to>
		<doi_number>10.1145/218380.218488</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218488</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P41621</person_id>
				<author_profile_id><![CDATA[81100118797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carole]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Blanc]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratoire Bordelais de Recherche en Informatique (Universit&#233; Bordeaux I and Centre National de la Recherche Scientifique), 351 cours de la lib&#233;ration, 33405 Talence (France)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109976</person_id>
				<author_profile_id><![CDATA[81100297901]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schlick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laboratoire Bordelais de Recherche en Informatique (Universit&#233; Bordeaux I and Centre National de la Recherche Scientifique), 351 cours de la lib&#233;ration, 33405 Talence (France)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>910231</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. Barsky, The Beta-Spline: a Local Representation based on Shape Parameters and Fundamental Geometric Measures, PhD Thesis, University of Utah, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>35072</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[R. Bartels, J. Beatty, B. Barsky, An Introduction to Splines for Computer Graphics and Geometric Modeling, Morgan Kaufmann, 1987.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. Blanc, Techniques de Moddlisation et de Ddformation de Surfaces pour la SynthOse d'Images, PhD Thesis, Universit6 Bordeaux I, 1994 (in french).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[C. Blanc, C. Schlick, More Accurate Representation of Conics by NURBS, Technical Report, LaBRI, 1995 (submitted for publication).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[C. Blanc, C. Schlick, X-Splines: Some Additional Results, Technical Report, LaBRI, 1995 (available by HTTP at www. labri, u-bordeaux, fr/LaBRl/People/schlick).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E. Catmull, R. Rom, A Class of Interpolating Splines, in Computer Aided Geometric Design, p317-326, Academic Press. 1974]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[E. Cohen, T. Lyche, R. Riesenfeld, Discrete B-Splines and Subdivision Techniques, Computer Graphics &amp; Image Processing, v 14, p87-111, 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[T. Duff, Splines in Animation and Modelling, SIGGRAPH Course Notes, 1986.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>83600</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[G. Farin, Curves and Smfaces for Computer Aided Geometric Design, Academic Press, 1990.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378512</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[D. Forsey, R. Bartels, Hierarchical B-Spline Refinement, Computer Graphics, v22, n4, p205-212, 1988.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617625</ref_obj_id>
				<ref_obj_pid>616015</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L. Piegl, On NURBS: a Survey, Computer Graphics &amp; Applications, v11, nl, p55-71, 1991.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>906872</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[R. Riesenfeld, Applications of B-Spline Approximation to Geometric Problems of Computer Aided Design, PhD Thesis, University Syracuse, 1973.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 X-Splines : A Spline Model Designed for the End-User Carole Blanc Christophe Schlick LaBRI . 351 cours 
de la libération, 33405 Talence (France) [blancjschlick]@labri.u-bordeaux.fr Abstract This paper presents 
a new model of spline curves and surfaces. The main characteristic of this model is that it has been 
created from scratch by using a kind of mathematical engineering process. In a .rst step, a list of speci.cations 
was established. This list groups all the properties that a spline model should contain in order to appear 
intuitive to a non-mathematician end-user. In a second step, a new family of blending functions was derived, 
trying to ful.ll as many items as possible of the previous list. Finally, the degrees of freedom offered 
by the model have been reduced to provide only shape parameters that have a visual interpretation on 
the screen. The resulting model includes many classical properties such as af.ne and perspective invariance, 
convex hull, variation diminution, local control and C2/G2or C2/G0continuity. Butitalsoincludesoriginal 
features such as a continuum between B-splines and Catmull-Rom splines, or the ability to de.ne approximation 
zones and interpolation zones in the same curve or surface. 1 Introduction Since the ground work in CAD 
during the late sixties, many different models of splines have been introduced. One speci.c characteristic 
of CAD is that the mathematical models developped by researchers are later manipulated by non-mathematician 
end users (designers, archi­tects, animators). Therefore, rather than its complete mathematical properties, 
a major criterion for the evaluation of a spline model may be the ability to understand intuitively the 
degrees of freedom that it provides. A full study of existing spline models on that particular point 
lies not within the scope of this short introduction, but let us just take one or two examples. The popular 
NURBS model is a good example in which the user has to be familiar with the mathematical structure to 
obtain best results. For instance, the manipulation of the knot vector is really complex: .rst the geometrical 
effects generated by these manipulations can hardly be predicted, second these effects are not robust 
because further knot manipulations may move them along the curve, and third the effects are propagated 
along the whole isoparametric curves in the case of surfaces. Even the manipulation of the weights may 
sometimes be confusing: for instance, the modi.cations of two adjacent weights are mutually cancelled 
[11]. The model that accounts the most for the ergonomics of the manipu­lation is undoubtedly the ,-spline 
model [1] which includes intuitive shape parameters (tension and bias). Yet, if the behaviour of the 
.Laboratoire Bordelais de Recherche en Informatique (Université Bordeaux I and Centre National de la 
Recherche Scienti.que). The present work is also granted by the Conseil Régional d Aquitaine. Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 model is really 
natural when using global tension and bias, the ex­tended model [2] with local parameters is less convincing, 
mainly because these parameters are not directly related to the control points. Moreover, the C0/G2continuity 
of the ,-splines is lost by interpola­tion, this makes them inadequate for many applications [9]. This 
paper proposes a new spline model that has been designed to make user manipulations as intuitive as possible. 
Its formulation is presented in four steps: Section 2 presents the list of speci.cations for the new 
model, Section 3 explains the principle and the basic formulation, Section 4 derives a more complete 
expression including an original shape parameter, .nally, Section 5 details the general formulation. 
 2 Background 2.1 De.nition In their most general de.nition, splines can be considered as a math­ematical 
model that associates a continuous representation (curve or surface) with a discrete set of points of 
an af.ne space (usually IR 2 or IR 3). In the case of curves, this de.nition can be expressed as follows: 
let Pk2IR 3with (k0..n) be a set of points called control points,and let Fk[0,1]-IR (with k0..n) be a 
set of functions called blending functions, the spline curve generated by the couples (Pk,F)is the curve 
Cde.ned by the parametric equation: kn X 8t2[0,1]C(t) Fk(t)Pk (1) k=0 According to the shape of the blending 
functions, the resulting curve may either approximate the control points or interpolate them. Fig­ure 
1 and Figure 2 illustrate this distinction by showing two classical examples of spline curves (cubic 
uniform B-splines [12] in Figure 1, cubic Catmull-Rom [6] in Figure 2). Each .gure is divided in two 
parts, the top shows the control lattice and the curve, the bottom shows the plots of the blending functions. 
The same graphical framework will be used throughout the paper. 2.2 Properties The family of curves 
that obeys Equation 1 is extremely vast and thus many of its members are likely to be of little interest. 
In fact, the work done over the years in the literature has exhibited many properties that a spline model 
should include to become useful for geometric modelling. In a recent survey, we have shown that all these 
properties can be obtained by imposing speci.c constraints on the blending functions [3]. PP 12 PP 45 
 FFFFF 123 45 FF 06 Figure 1: Uniform B-spline curve P 4 P5 Figure 2: Catmull-Rom spline curve Using 
that result, we are going to list now all the properties (as well as the corresponding constraints on 
the blending functions) that we have found vital or simply desirable to include in our user-oriented 
spline model: Af.ne invariance: The af.ne transformation of a spline should be obtained by applying the 
transformation to its control points. This is provided by the normality constraint: n X 8t2[0,1] Fk(t)1 
(2) k=0 Convex hull: The spline should be entirely contained in the convex hull of its control lattice. 
This is provided by the normality constraint combined with the positivity constraint: 8k0..n8t2[0,1]Fk(t):0 
(3) Variation diminution: The number of intersections between the spline and a plane (or a line, for 
2D splines) should be at most equal to the number of intersections between the plane and the control 
lattice, which means that the spline should have less oscillations than its control lattice. This property 
is provided by combining the normality, the positivity with the regularity constraint: 8k0..n9Tk2[0,1]/ 
(4) 0 8k0 8tTk Fk(t):0andk1..nFk0(t) Fk(t) 8tTk Fk0(t) 0and8k0 0..k-1Fk0(t) Fk(t) This constraint may 
appear complex at a .rst glance, but it simply says that the blending functions are bell-shaped and that 
two functions cannot cross each other in the zone where they are simultaneously increasing or decreasing. 
Local control: Each control point should only in.uence the shape of the spline in a restricted zone. 
This property is provided by the locality constraint: .:2 8k0..n9(Tk,Tk)2[0,1]/ (5) . : 8tTk Fk(t)0and8tTk 
Fk(t) 0 A spline may offer more or less local control according to the extent of the in.uence of a given 
control point. To quantify this aspect, the notion of Lplocality [3] can be used: a spline curve (resp. 
surface) has got Lplocality when each control point in.uences psegments (resp. patches) at most. Smooth 
shapes/Sharp shapes: The spline model should allow both smooth shapes and sharp shapes and more precisely 
mixing smooth zones and sharp ones in the same curve. It is well known that parametric continuity does 
not provided any information on the shape of the curve; therefore one has to use geometric continuity: 
smooth shapes are G2at least, sharp shapes are G0at most. On the other hand, parametric continuity is 
needed to provide smooth motion in animation; therefore the model should also provide C2continuity. 
 Intuitive shape parameters: In addition to the control points, the spline should also provide other 
degrees of freedom, usually called shape parameters. But to be usable by a non-mathematician end user, 
the role of these parameters should be as intuitive as possible. Among all the shape parameters (knots, 
weights, tension, bias, curvature) that we can .nd in existing spline models, only the local tension 
effect (which allows the user to pull the curve locally toward one or several control points) appears 
totally intuitive.  Existence of re.nement algorithms: The spline model should allow the use of re.nement 
or subdivision techniques which are pow­erful tools that increase the number of degrees of freedom for 
a spline (control points or shape parameters) without modifying its shape.  Representation of conics: 
The spline model should be able to represent conic sections, and consequently a large set of curves and 
surfaces (circles, ellipses, spheres, cylinders, surfaces of revolution, etc) that are intensively used 
in CAD. The exact representation of conics is one reason for the popularity of the NURBS model [3]. Nevertheless, 
having only a close approximation (up to the resolution of the display, for instance) is suf.cient for 
most applications.  Approximation/Interpolation: For some applications or some users, approximation 
splines are preferable, whereas for others, in­terpolation splines are imperative. For that reason, the 
model should provide approximation splines and interpolation splines in a uni.ed formulation. Among the 
existing models, only the general Catmull-Rom model [6] includes such a feature; but we would like to 
get a step further by allowing the creation of approximation zones and interpolation zones in the same 
curve.  In the following sections, we describe a new spline model which was designed to ful.ll as many 
items as possible of the previous list. At the current stage in this development, all items but one (the 
existence of re.nement techniques) are ful.lled by the model. The possibility of including the last item 
will be discussed in the conclusion.  3 Basic X-Splines 3.1 Principle Building a new spline model from 
scratch implies de.ning a new fam­ily of blending functions. Among the constraints that have been listed 
in Section 2, the most dif.cult to ful.ll is the normality constraint. Indeed, .nding a family of functions 
Fk(t)that sum to one whatever the value of tis a tricky task. For that reason, we have chosen to build 
our blending functions independently of the normality constraint, and then to apply in a .nal step, a 
normalization process which replaces Fk(t)by Fk(t): Fk(t) 8t2[0,1]Fk(t)P n (6)Fk(t) k=0 Thus, the actual 
blending functions Fk(t)will be normalized rational polynomials which, as a side-effect, adds the projective 
invariance property to the resulting curves. By combining the different properties recalled in Section 
2, we can establish that for a normal, positive, regular and local spline, each blending function Fk(t)is 
bell-shaped, starts to grow at a given value . Tk, reaches its unique maximum at a second value Tkand 
drops to zero at a third value Tk:(see Figure 3). In classical spline models, Fk(t)is de.ned by a piecewise 
polynomial or a rational piecewise polynomial, composed of as many segments as consecutive intervals 
. between Tkand Tk:(e.g. four segments with cubic B-splines). tttttt 012345 T0 T0 T3T0+T3 T3 --+ T 
T1 T4 T1+ T4 T4 1 -T-+ T+T2 T2 5T2T5 5 Figure 3: Con.guration of the blending functions The driving idea 
of the new model that we propose here is the follow­ing: the non-null part of the blending function should 
be composed . of only two segments1 . The .rst, called Fk(t), is de.ned between . Tkand Tk; the second, 
called Fk:(t), is de.ned between Tkand Tk: . In order to make this idea clearer, let us take the case 
of a spline in which each control point Pkin.uences four segments of the curve (i.e. L4locality). This 
is a usual case (shared by every classical model of cubic splines, for instance) and is often considered 
[2] as the best trade-off between low degree splines on one hand (which are closely related to the control 
lattice and thus can hardly provide very smooth shapes) and high degree splines on the other hand (which 
can hardly provide very sharp shapes). By de.nition, for an L4spline, each blending function is non-null 
over four consecutive intervals of the knot vector: Fk(t)becomes non-null at knot tk.2, is maximal at 
knot tkand becomes null again at knot tk:2(the knots are shown on the top of Figure 3). As Fk(t) is composed 
only of two segments, it depends only on tk.2, tkand tk:2. Thus there is a kind of alternation in the 
way the knots are taken into account (even points use even knots and odd points use odd knots). Moreover, 
as we will see, the blending functions Fk.2and Fk:2cross each other at knot tkand all the derivation 
of the model is based on this crossing. For that reason, we have called this new model, cross-splines 
or X-splines, for short. 3.2 Formulation In fact, once the general principle has been established, the 
basic for­mulation of the new model can be derived quite naturally. Let us .rst 1In fact, we have also 
tried the case where the non-null part is composed of only one segment. Butthismakesthemodelmuchmoreexpensive(degree8rationalpolynomials) 
with no additional features. take the case of a uniform knot vector: 8k1..ntk -tk.1 If we apply the following 
reparametrization to the curve, t-tk.2 t-tk.2 u(t) (7)tk -tk.2 2 we are assured that u0at knot tk.2where 
Fk(t)starts to grow and u1at knot tkwhere Fk(t)reaches its maximum. Therefore, we have to .nd a polynomial 
f(u)de.ned on the range [0,1]which can be linked to the left part of Fk(t)by: ) . t-tk.2 Fk(t)f(8)2 Because 
we want a C2continuous curve, the following constraints for the function f(u)can be immediately derived: 
f(0)0f0(0)0f00(0)0 (9) As the maximum of the blending function is reached at u1, its .rst derivative 
is necessarily null. Moreover, we can set f(1)1 because the normalization step will reduce the maximum 
to its exact value anyway. Finally, the second derivative at u1canbe setto a givenconstant(we callthisconstant 
-2pto simplify the formulation): f(1)1f0(1)0f00(1)-2p (10) Thus we have derived a system of six constraints. 
As we search for a polynomial solution, it will necessarily be quintic, in order to get six degrees of 
freedom. By matching the constraints and the coef.cients of the polynomial, we obtain: .. fp(u)u 3 10-p(2p-15)u(6-p)u 
2 (11) Moreover, the property of regularity requires an increasing function on the range [0,1]and thus 
a positive derivative. Therefore there is an additional condition on p: 0p10 The function fp(u)(see Figure 
4) provides the left part of Fk(t) according to Equation 8. By reversing the direction and the origin 
of the reparametrization, the right part of Fk(t)is obtained similarly: ) : tk:2 -t Fk(t)fp(12)2 . The 
two functions Fkand Fk:join at knot tkwith C2continuity 0 00 (Fk(tk)0and Fk(tk)-p/2 2)which means that 
the global blending function Fk(t), and therefore the whole curve C(t),are C2 continuous.  Figure 4: 
Function fp(u)for p0,2,4,6,8,10 Finally, we get the formulation for a segment of the curve C(t)on the 
parameter range [tk:1,t:2], de.ned by the four control points k Pk,Pk:1,Pk:2,Pk:3: A0(t)Pk A1(t)Pk:1 
A2(t)Pk:2 A3(t)Pk:3 C(t) A0(t)A1(t)A2(t)A3(t) (13) )) tk:2 -ttk:3 -t A0(t)fp A1(t)fp2 2 )) t-tk t-tk:1 
A2(t)fp A3(t)fp2 2 The process de.ned above has provided a quintic rational approxima­tion spline model 
that includes the properties of normality, positivity, regularity, locality and C2continuity. Moreover, 
the curves contain a degree of freedom p2[0,10]which allows a (slight) modi.cation of their shapes. It 
should be noticed that a very interesting case is obtained for p8. Indeed, after the normalization step, 
the blending functions are very close to the cubic uniform B-splines basis func­tions (see Figure 6). 
It means that the resulting curves call them basic X-splines are almost identical to the uniform cubic 
B-splines (compare Figure 5 and Figure 1). PP 12 PP 45 FFFFF 123 45 FF 06 Figure 5: Basic X-spline curve 
Figure 6: Similarity of the cubic uniform B-splines and the basic X-splines blending functions (for p8) 
  4 Extended X-Splines 4.1 Formulation Thedegreeoffreedom pin Equation 11 does not offer enough variety 
in the shapes of the blending functions (see Figure 4) to provide inter­esting effects on the resulting 
curves. Therefore, it appears somewhat useless in the formulation of the new model. In fact, the existence 
of this degree of freedom will be hidden to the end user. As we will see below, this parameter pis needed 
to manage another parameter s, that we are going to introduce now and which is the actual degree of freedom 
accessible by the end user. Among the items of our list of speci.cations, tension and angular shapes 
(G0continuity) can be included in our model by the same derivation. Indeed, the basic idea which has 
led to the concept of tension in the spline literature is to be able to strain the curve (or the surface) 
in order to pull it toward the control lattice. At its limit, this process forces the curve to interpolate 
one or several control points, and due to the convex hull property, this interpolation will create sharp 
edges. To bring the curve closer to a given part of the control lattice, one has to increase the in.uence 
of the corresponding control points. A straightforward idea to realize this process is to add a speci.c 
weighting coef.cient to each control points. But, as we have recalled in Section 1, this solution (which 
is used in every classical rational spline) does not work in a satisfying way, because the in.uences 
of neighbouring weights are mutually cancelled. Therefore, we propose here an original solution to include 
the concept of tension, which does not contain the drawback of the existing models. To illustrate this 
new solution, let us take the blending functionsF2,F3 and F4in Figure 3. We know that F3reaches its maximum 
at t3.But, as F2and F4are not null at t3, the normalization process has set the actual maximum to F3/(F2 
F3 F4). Therefore, a way to increase this maximum, in order to bring the curve closer to the control 
point P3, is to decrease F2(t3)and F4(t3). We know that in the area of interest, F2(respectively F4) 
decreases (respectively increases) monotonically in the range [t2,t]. Thus, to 4obtain smaller values 
for these functions at t3, one has to speed up the decrease of the former and to slow down the increase 
of the latter. To realize these two operations symmetrically, we actually push the crossing point of 
F2and F4down toward the horizontal axis. For that, we introduce a new degree of freedom s32[0,1]at point 
P3. This parameter will be used, .rst to compute the value T2:(where F2 becomes null) by interpolation 
between t4and t3: T: -)t3 s3 2 t3 s3(t4 t3 . and second, to compute the value T4(where F4becomes non 
null) by interpolation between t3and t2: T4 . t3 s3(t2 -t3)t3 -s3 In other words, it means that F2(respectively 
F4) is null all over the . range [T2: ,t](respectively [t2,T4]). The same operation can be 4 done for 
each k. The resulting values (Tk. ,Tk:)have to be replaced in the reparametrization equations (Equation 
8 and Equation 12) as follows:   ()() t-T. t-T: F(t)fp F(t)fp (14) . k : k k. k: tk -Ttk -T kk The 
two parts of Fk(t)still join at tk, their .rst derivatives are still null but their second derivatives 
are different: 00. -2p 00: -2p Fk(t) Fk(t) (15) k. k: (tk -T)2 (tk -T)2 kk Here is the point where our 
parameter pwill .nally be used. Indeed, in order to equal the left and right expressions, the only thing 
to do . is to use a speci.c value for p(noted pk.1)in Fkand another one (noted pk:1)in Fk:.Taking .2 
:2 2(tk -T)2(tk -T)pk.1 kandpk:1 k(16) 22 gives 4 Fk(tk)Fk(tk)­ 00. 00: 2 which provides C2continuity 
but assures also that the parameters pk are in the range [0,8]as needed to get the property of regularity 
and to obtain the cubic B-splines as a limit case. Therefore we can derive a new formulation2for the 
segment of the curve C(t)on the range [tk:1,t:2]de.ned by the four control points k Pk,Pk:1,Pk:2,Pk:3: 
C(t) A 0(t) P k A1(t)A0(t) Pk:1 A1(t) A2(t)Pk:2 A3(t)Pk:3 A2(t)A3(t) (17) ( ) A0(t) t T : k ? 0 fpk.1 
t-T: k tk -T: k A1(t) A2(t) A3(t) t t t TTT : k:1 . k:2 . k:3 ??? 000 fpk (t-T: k:1 tk:1 -T: k:1) fpk+1 
( t-T. k:2 tk:2 -T. k:2 ) fpk+2(t-T. k:3 tk:3 -T. k:3) 2 T:)2 2 T:)2 pk.1 (tk -pk (tk:1 ­ 2k2k:1 2 .)2 
2 .)2 2k:22k:3 pk:1 (tk:2 -Tpk:2 (tk:3 -T The expression of C(t)seems complex but in fact it can be imple­mented 
very compactly and ef.ciently (12 lines of source code in C language). So for the end user, an extended 
X-spline is totally de.ned by a set of quadruples (xk,y,zk,s)with k0...n. All these de­ kk grees of freedom 
have a very simple interpretation. The parameters (xk,yk,zk)2IR 3are the coordinates of the control points 
Pk.The parameter sk2[0,1]symbolizes the distance between the curve and the control lattice:when sk 1, 
the curve passes relatively far away from point Pk;when sdecreases, the curve comes closer and closer 
k to Pk; .nally when sk 0, the curve passes through Pk. It should be noticed that the curve is always 
C2(due to the construction process that has been used), even when it interpolates a control point Pk. 
But in that case, the .rst and second derivatives drop to zero at 2We use here the (test ? a :b) operator 
borrowed from the C programming language which allows one to write multiple expressions in a compact 
way. tkand therefore the curve is usually (when Pk.1,Pand Pk:1are not aligned) only G0at Pk. In other 
words, it means that, even if it is always C2, the model enables the creation of angular points or sharp 
edges. k 4.2 Examples This section demonstrates the role of the parameter skby showing its in.uence on 
the resulting shapes. The basic formulation de.ned in Section3 is aparticularcaseofthe extendedone,whereallparameters 
skaresettoone. Aswehaveseen,basicX-splinesarealmostidentical to uniform cubic B-splines. A .rst variant 
consists in setting s0and snto zero in order to inter­polate the end points of the control lattice and 
thus to enable better control of the curve boundaries. The resulting curves call them extremal X-splines 
(see Figure 7) are very close to the classical extremal cubic B-splines (also called non-periodic cubic 
B-splines). PP 12 PP P0 36 PP 45  F FFFF 1 234 5 F 0 F6 Figure 7: Extremal X-spline curve s0 0,s1 1,s2 
1,s3 1,s4 1,s5 1,s6 0 Let us now decrease the value of one parameter sk(say s3). By comparing Figure 
7 and Figure 8, one can see that the crossing point of F2and F4at knot t3has been pushed down. P1 P2 
 P0 PP 36 PP 45  FFFF 12 45 F0F3 F6 Figure 8: Augmentation of the in.uence of point P3 s0 0,s1 1,s2 
1,s3 0.5,s4 1,s5 1,s6 0 Therefore, after the normalization step, the maximum of F3has been increased 
and the curve has been pulled toward P3. Moreover, neither the maximum of F2nor the maximum of F4has 
been modi.ed. This means that the curve has not changed near P2or P4: all the modi.cations are localized 
in the neighbourhood of point P3.More precisely, one can show that a shape parameter skin.uences only 
two segments of the curves which is half the extent of the other three coordinates (xk,y,z)of point Pk(i.e. 
L2locality rather than L4).   kk While s3decreases, the maximum of F3increases. Finally, for s3 0, 
this maximum is set to one, which provides a sharp (G0 continuous) interpolation of point P3. P 1P PP 
45 Figure 9: Sharp interpolation of point P3 s0 0,s1 1,s2 1,s3 0,s4 1,s5 1,s6 0 The L2locality of the 
in.uence of the parameters skallows the same kind of action on several adjacent control points. For instance, 
if we decrease s2,sand s4, the curve is pulled simultaneously toward 3 P2,P3and P4, P P2 P 4 P5 and 
if we set the three parameters to zero, we obtain a sharp interpo­lation of P2,Pand P4(see Figure 11). 
Finally, for the limit case 3 where all the parameters skare set to zero, the curve merges with the control 
lattice (see Figure 12). But notice that the curve is not a linear spline because the parametrization 
is C2here, whereas it is only C0 for linear splines. PP PP 45 s0 0,s1 1,s2 0,s3 0,s4 0,s5 1,s6 PP 12 
PP 45 s0 0,s1 0,s2 0,s3 0,s4 0,s5 0,s6 0 This ability to mix smooth curves and sharp edges in an unrestricted 
way makes the extended X-spline model a candidate of choice for many applications. In vectorial font 
design, for instance, one switches frequenty between smoothness and sharpness. Therefore, the use of 
X-splines enables the design of characters with one single spline for the outline (plus eventually one 
spline for each hole) de.ned by a small number of control points (see left part of Figure 13) To conclude 
this section, note that a very useful case is obtained when the control lattice forms a regular polygon 
and all the skare set to one: the resulting curve is a circle (see right part of Figure 13. In fact, 
this circle is only an approximated one but this approximation is so close (for 8 control points, the 
amplitude of the oscillations of the curve around the true circle represents less than a factor 10.3of 
the radius, and for 12 control points, this variation is less than 10.6) that it is suf.cient for most 
of the applications3 . Starting from that kernel case, other conics can be approximated as well with 
a similar accuracy [5]. 3A similar result is obtained with B-splines [4], therefore it is not surprizing 
that it holds also for X-splines which approximate B-splines in that particular con.guration.  5 General 
X-Splines 5.1 Formulation As they have been formulated above, extended X-splines ful.ll many of the properties 
listed in Section 2. Nevertheless, even if they al­low interpolating one or several control points, extended 
X-splines are still approximation splines, because only sharp interpolations are provided. The last feature 
of our list was the ability to manipulate the same model either as an approximation spline or as an interpolation 
spline. The goal of this section is to show how this characteristic can be included in the X-spline model. 
But, as recalled in Section 2, using interpolation splines implies for­saking the positivity of the blending 
functions and therefore the con­vex hull property. For some applications (and for some users), this is 
inconceivable. For that reason, we have purposely separated this extension from the previous section. 
So, the reader may choose be­tween the formulation that ful.lls the convex hull property and the formulation 
that provides the approximation/interpolation duality. In Section 4, we saw that when the value of the 
parameter skis decreased, the blending function F.(respectively F:) becomes k:1k.1 null between tk.1and 
T.(respectively T: and tk:1). At the k:1k.1 limit case, when sk 0, F. (respectively F:) is null over 
the k:1k.1 whole range [tk.1,t](respectively [tk,tk:1]). Starting from that k con.gurationofsharpinterpolation, 
togeta smooth (G2continuity) . interpolation of point Pk, we must allow Fand F: to become k:1k.1 negative 
over these ranges. Moreover, in the same manner as we have sought to approximate cubic B-splines with 
the basic formulation, we will try to approximate cubic Catmull-Rom splines with this general formulation. 
If we apply the following reparametrization to the curve, t-tk t-tk u(t) (18)tk:1 -tk . we are assured 
that u-1at knot tk.1where Fk:1gets negative, . u0at knot tkwhere Fk:1gets positive, and u1at knot tk:1 
. where Fk:1reaches its maximum. Therefore, we have to .nd two polynomials: g(u)de.ned on [0,1]which 
represents the positive part . of Fk:1and h(u)de.ned on [-1,0]which represents its negative part. These 
two functions must join up at u0with C2continuity. As in Section 3, we can derive a system of constraints 
but this time there are two functions, which means 12 constraints: g(0) 0 g 0(0) q g 00(0) 4q g(1) 1 
g 0(1) 0 g 00(1) -2p h(0) 0 h0(0) q h00(0) 4q (19) h(-1) 0 h0(-1) 0 h00(-1) 0 where qis a degree of 
freedom that controls the value of the .rst derivative at u0(the same degree of freedom has been used 
by Duff in his tensed interpolation spline model [8]. All these constraints can be ful.lled by two quintic 
polynomials: g(u)qu2qu 2 (10-12q-p)u 3 (2p14q-15)u 4 (6-5q-p)u 5 (20) 245 h(u)qu2qu-2ququ Starting from 
these equations, the same construction process detailed in Section 3 provides a rational quintic interpolation 
spline model that includes the properties of normality, locality and C2continuity. Moreover, the curves 
contain a degree of freedom qwhich allows modi.cation of their shapes. Two important remarks should 
be made about this model. First, as in every interpolation spline model, the regularity property is lost, 
thus the curve may have unwanted oscillations. We have observed experimentally that these oscillations 
can usually be avoided by lim­iting qto the range [0,1/2]. Second, an interesting case is obtained for 
q1/2because the blending functions are very close to the Catmull-Rom functions (see Figure 14). But it 
should be noticed that the new functions are C2continuous instead of C1 . The .nal step of the construction 
of our new spline model will be to merge the parameter sof the approximation model and the parameter 
qof the interpolation one. Here again, the goal is to simplify the degrees of freedom manipulated by 
the end user. Practically, only one shape parameter skper control point Pkwill be used. This is done 
with the following convention: When the user sets all skin the range [0,1], it means that he wants to 
manipulate approximation splines. In that case, skis the curve/lattice distance parameter de.ned in Section 
4 (in particular, uniform cubic B-splines are approximated for sk 1).  When the user sets all skin the 
range [-1,0], it means that he wants to manipulate interpolation splines. In that case, qk is obtained 
from skby qk -sk/2(so, sk -1provides q1/2which approximates cubic Catmull-Rom splines).  The positive/negative 
distinction for skindicates clearly that there is a breaking point: for positive sk, the convex hull 
property is ful.lled, for negative sk, it is not the case anymore. On the other hand the intuitive notion 
of curve/lattice distance is preserved even for negative sk. Indeed, as we will see below, the more skdeparts 
from zero, the more the curve departs from the control lattice.  5.2 Examples We already know that a 
sharp (G0continuity) interpolation of the control lattice can be obtained by setting all skto zero (see 
Figure 12). If we want to realize a smooth (G2continuity) interpolation, the only thing to do is to set 
these parameters to negative values. For instance, by setting all skto -1, an interpolation spline almost 
identical to the Catmull-Rom spline is obtained (compare Figure 15 and Figure 2). As expected, the blending 
functions become partly negative, and thus the convex hull property is lost. P 4 P5 s0 s1 s2 s3 s4 
s5 s6 -1 P 4P 5 s0 0,s1 s2 s3 -1,s4 s5 -0.5,s6 0 By providing different values for the parameter sk, 
the shape of the interpolation curve can be controlled precisely. For instance, one can enable very slack 
interpolation for a speci.c zone of the lattice and a much tighter interpolation for another zone (see 
Figure 16). And .nally, what is perhaps the most interesting feature of the X­spline model, one can combine 
without any restriction, positive and negative shape parameters skin order to create approximation zones 
and interpolation ones in the same curve (see Figure 17). PP PP 45 s0 0,s1 s2 s3 -1,s4 s5 1,s6 0  
6 Surfaces The extension of the new model from curves to surfaces is straight­forward. The only thing 
to do is to compute the tensor product of two non-normalized X-spline curves and then to apply the normaliza­tion 
step4 . The characteristic of the X-splines to create all possible geometric effects by using only uniform 
knot vectors is vital here because, as we have recalled in Section 1, effects due to knot ma­nipulations 
(e.g. sharp edges for B-splines) are propagated along the whole isoparametric curves. On the contrary, 
the shape parameters of the X-spline model are directly related to the control points and thus can be 
localized precisely on a given zone of the surface. Because of the tensor product, two shape parameters 
rkand skare provided for each control point Pkwhere rkacts in the udirection of the surface and skacts 
in the vdirection. A nice consequence is that non-isotropic manipulations are allowed (for instance, 
creating sharpness in one direction and smoothness in the other one). As a counterpart, the behaviour 
of these parameters is a bit more subtle than previously: rk 0, sk 0: Pkis a C2/G2approximation point 
 0, s0: Pis a C2/G0interpolation point  rkkk rk 0, sk 0: Pkis a C2/G2interpolation point  rk 0, sk 
0: Pk is an approximation point providing C2/G0continuity in uand C2/G2continuity in v  rk 0, sk 0: 
Pkis an interpolation point providing C2/G0 continuity in uand C2/G2continuity in v  Figure 19 and Figure 
18 shows some examples of X-spline surfaces. You should notice the ability to create interpolation of 
adjacent control points, localized sharp edges as well as soft transitions between sharp and smooth zones; 
three features that are impossible (or at best, only possible in speci.c cases) with any existing spline 
model. 4This process is sometimes called generalized tensor product [11]  Note that the star-shaped 
.at face on the top of the object is composed of two sides with straight edges (left and bottom) and 
two sides with rounded edges (top and right). Straight sides create sharp edges that are propagated all 
along the extrusion whereas the sharp edges smoothly vanish when they come near the rounded sides of 
the top face.  7 Conclusion In this paper, we have presented a new model of spline curves and surfaces. 
This model includes many classical properties such as af.ne and perpective invariance, convex hull, variation 
diminution, local control and C2/G2or C2/G0continuity, as well as some original features such as a continum 
between (an approximation of) B-splines and (an approximattion of) Catmull-Rom splines, or the ability 
to de.ne approximation zones and interpolation zones in the same curve or surface. These properties have 
been obtained by de.ning a new family of blending functions that are quintic rational polynomials and 
introducing an original shape parameterthat provides, for each control point, a smooth transition between 
approximation, sharp interpolation and smooth interpolation. This paper is only intended as an initial 
presentation of X-splines. For space limitations, several topics could not be included here. We propose 
some additional results in [5] which should be considered as the companion paper of this one. More precisely, 
the following topics are discussed in it: Some precisions on ef.cient implementation of X-splines: For 
instance, one can show that even if they are quintic, rational and provide more geometrical effects, 
uniform X-splines are less expensive to compute than non-uniform cubic B-splines).  Lower order and 
higher order X-splines: Quintic polynomials have been chosen here because we sought for C2/G2continu­ity, 
but in fact a similar construction process can be used for any polynomial of degree 2k1providing splines 
with Ck/Gk continuity.  Extension to non-uniform knot vectors: Geometrical effects generated by non-uniformity 
in classical splines can be created by the shape parameters, so this extension is not that vital. Nevertheless, 
non-uniform knots may be useful for key-frame animation or data-.tting.  Re.nement algorithms: This 
is clearly a much harder task. For the moment, we propose only some preliminary results on a kind of 
De Casteljau subdivision algorithm. Acknowledgements We wish to thank all the anonymous reviewers for 
many helpful com­ments and suggestions. We also thank Brian Smith (from Lawrence Berkeley Laboratory), 
the maintainer of the xfig package, who gave us the permission to include the X-splines model in his 
software and to use it for public demonstration. Finally, special thanks to C. Feuille, S. Grobois, L. 
Mazière and L. Minihot who modi.ed xfig for us and discovered the L2(rather than L4) locality of the 
shape parameters.  8 References [1] B. Barsky, The Beta-Spline: a Local Representation based on Shape 
Parameters and Fundamental Geometric Measures,PhD Thesis, University of Utah, 1981. [2] R.Bartels,J.Beatty,B.Barsky, 
An Introduction to Splines for Computer Graphics and Geometric Modeling, Morgan Kauf­mann, 1987. [3] 
C. Blanc, Techniques de Modélisation et de Déformation de Sur­faces pour la Synthèse d Images, PhD Thesis, 
Université Bor­deaux I, 1994 (in french). [4] C. Blanc, C. Schlick, More Accurate Representation of Conics 
by NURBS, Technical Report, LaBRI, 1995 (submitted for pub­lication). [5] C. Blanc, C. Schlick, X-Splines: 
Some Additional Results, Tech­nical Report, LaBRI, 1995 (available by HTTP at www.labri.u-bordeaux.fr/LaBRI/People/schlick). 
[6] E. Catmull, R. Rom, A Class of Interpolating Splines,in Com­puterAidedGeometricDesign,p317-326,AcademicPress. 
1974 [7] E. Cohen, T. Lyche, R. Riesenfeld, Discrete B-Splines and Subdi­vision Techniques, Computer 
Graphics &#38; Image Processing, v14, p87-111, 1980. [8] T. Duff, Splines in Animation and Modelling,SIGGRAPH 
Course Notes, 1986. [9] G. Farin, Curves and Surfaces for Computer Aided Geometric Design, Academic Press, 
1990. [10] D. Forsey, R. Bartels, Hierarchical B-Spline Re.nement,Com­puter Graphics, v22, n4, p205-212, 
1988. [11] L. Piegl, On NURBS: a Survey, Computer Graphics &#38; Applica­tions, v11, n1, p55-71, 1991. 
[12] R. Riesenfeld, Applications of B-Spline Approximation to Geo­metric Problems of Computer Aided Design,PhD 
Thesis,Uni­versity Syracuse, 1973.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218490</article_id>
		<sort_key>387</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[Rendering interactive holographic images]]></title>
		<page_from>387</page_from>
		<page_to>394</page_to>
		<doi_number>10.1145/218380.218490</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218490</url>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39035215</person_id>
				<author_profile_id><![CDATA[81100043706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lucente]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T. J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY and Massachusetts Institute of Technology, Media Laboratory, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P282976</person_id>
				<author_profile_id><![CDATA[81100291883]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tinsley]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Galyean]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology, Media Laboratory, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>196549</ref_obj_id>
				<ref_obj_pid>195966</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Michael McKenna and David Zeltzer. Three Dimensional Visual Display Systems For Virtual Environments. Presence: Teleoperators and Virtual Environments. Vol. 1 #4, 1992, pp. 421-458.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[P. Hariharan. Optical Holography. Cambridge: Cambridge University Press. 1984.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pierre St. Hilaire, Stephen A. Benton and Mark Lucente. Synthetic Aperture Holography: A Novel Approach To Three Dimensional Displays. Journal of the Optical Society of America A, Vol. 9, #11, Nov. 1992, pp. 1969 - 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[William J. Dallas. Computer-Generated Holograms. Chapter 6 in The Computer in Optical Research, Topics in Applied Physics, Vol. 41, ed. B.R. Frieden. Berlin: Springer-Verlag. 1980, pp. 291-366.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mark Lucente. Interactive Computation Of Holograms Using A Look-up Table. Journal of Electronic Imaging, Vol. 2, #1, Jan 1993, pp. 28-34. Also: Mark Lucente. Optimization of hologram computation for real-time display, in SPIE Proc. #1667 Practical Holography VI, 1667-04 (SHE, Bellingham, WA, 1992), pp. 32-43.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[E Mok, J. Diep, H.-K. Liu and D. Psaltis. Real-time Computer-generated Hologram By Means Of Liquid-Crystal Television Spatial Light Modulator. Optics Letters, Vol. 11 # 11, Nov. 1986, pp. 748-750.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[S. Fukushima, T. Kurokawa and M. Ohno. Real-time Hologram Construction And Reconstruction Using A High-resolution Spatial Light Modulator. Applied Physics Letters, Vol. 58 #8, Aug. 1991, pp. 787-789.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Stephen A. Benton. Survey of Holographic Stereograms. In Processing and Display of Three-Dimensional Data. Proceedings ofthe SPIE 367, 1983, pp. 15-19.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Michael Halle. The Generalized Holographic Stereogram. Master's thesis, Massachusetts Institute of Technology, 1991.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Joseph W. Goodman. Introduction to Fourier Optics. New York: McGraw-Hill. 1968.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[J. R. Fienup. Iterative Method Applied to Image Reconstruction And to Computer-Generated Holograms. Optical Engineering, Vol. 19 #3, May/June 1980, pp.297-305.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>222938</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Mark Lucente. Diffraction-Specific Fringe Computation for Electro-Holography. Ph.D. Thesis, Massachusetts Institute of Technology, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering Interactive Holographic Images Mark Lucente and Tinsley A. Galyean Massachusetts Institute 
of Technology Media Laboratory* ABSTRACT We present a method for computing holographic patterns for 
the generation of three-dimensional (3-D) holographic images at inter­active speeds. We used this method 
to render holograms on a con­ventional computer graphics workstation. The framebuffer system supplied 
signals directly to a real-time holographic ( holovideo ) display. We developed an ef.cient algorithm 
for computing an im­age-plane stereogram, a type of hologram that allowed for several computational simpli.cations. 
The rendering algorithm generated the holographic pattern by compositing a sequence of view images that 
were rendered using a recentering shear-camera geometry. Computational ef.ciencies of our rendering method 
allowed the workstation to calculate a 6-megabyte holographic pattern in under 2 seconds, over 100 times 
faster than traditional computing meth­ods. Data-transfer time was negligible. Holovideo displays are 
ideal for numerous 3-D visualization applications, and promise to provide 3-D images with extreme realism. 
Although the focus of this work was on fast computation for holovideo, the computed ho­lograms can be 
displayed using other holographic media. We present our method for generating holographic patterns, preceded 
by a background section containing an introduction to optical and computational holography and holographic 
displays. Keywords: Electro-Holography, Holovideo, Computer-Generated Holography, Accumulation Buffer. 
CR Categories: I.3.1 [Computer Graphics]: Hardware Architec­ture - three-dimensional displays, graphics 
processors; I.3.7 [Com­puter Graphics]: Three-Dimensional Graphics and Realism. 1. INTRODUCTION The practical 
use of three-dimensional (3-D) displays has long been a goal in computer graphics. Three-dimensional 
displays are generally electronic devices that provide binocular depth cues, par­ticularly binocular 
disparity and convergence. (See the Glossary on the next page.) Some 3-D displays provide additional 
depth cues such as motion parallax and ocular accommodation. The reference *MIT Media Laboratory, Cambridge, 
MA 02139. (lucente | tag)@media.mit.edu Mark Lucente is now at: IBM T. J. Watson Research Center, P.O. 
Box 218, Yorktown Heights, NY 10598 Permission to make digital/hard copy of part or all of this work 
for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, 
and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 
ACM-0-89791-701-4/95/008 $3.50 by McKenna[1] contains a good discussion of the visual depth cues and 
a detailed evaluation of 3-D display techniques. Recently, some 3-D displays have been used interactively. 
A 3-D display allows the viewer to more ef.ciently and accurately sense both the 3-D shapes of objects 
and their relative spatial locations, particularly when monocular depth cues are not prevalent in a scene. 
When viewing complex or unfamiliar object scenes, the viewer can more quickly and accurately identify 
the scene con­tents. Therefore, 3-D displays are important in any application in­volving the visualization 
of 3-D data, including telepresence, education, medical imaging, computer-aided design, scienti.c vi­sualization, 
and entertainment. The merit of a 3-D display depends primarily on its ability to provide depth cues 
and high resolutions. The inclusion of depth cues - particularly binocular disparity, motion parallax, 
and occlu­sion - increases the realism of an image. Holography[2] is the only imaging technique that 
can provide all the depth cues[1]. All other 3-D display devices lack one or more of the visual depth 
cues. For example, stereoscopic displays do not provide ocular accommoda­tion, and volume displays cannot 
provide occlusion. Image resolu­tion and parallax resolution are also important considerations when displaying 
3-D images. While most 3-D displays fail to provide ac­ceptable image and parallax resolutions, holography 
can produce images with virtually unlimited resolutions. In optical holography, a recorded interference 
pattern recon­structs an image with an extremely high degree of accuracy. A ho­lographic pattern - called 
fringes - can be computed and used to generate a 3-D image, most recently in real time[3]. Both the com­puting 
process and the displaying process are signi.cantly more dif.cult than in other 3-D display systems. 
Nevertheless, a real-­time electro-holographic ( holovideo ) display can produce dy­namic 3-D images 
with all of the depth cues and image realism found in optical holography. Therefore, holovideo has the 
potential to produce the highest quality 3-D images. Also, to view holo­graphic images, the viewer is 
unencumbered by equipment such as glasses or sensors. Figure 1 illustrates the basic functionality of 
holovideo. output aperture input signals  red viewer green blue image volume Figure 1: A typical 
real-time 3-D holographic (holovideo) display. When positioned in the viewing zone, viewers see a 3-D 
image in the vicinity of the output aperture. The input signals are generated by a computer. The size 
and complexity of holographic fringe patterns often precludes their computation at interactive rates. 
In the .eld of com­putational holography[4], a discretized holographic fringe pattern is generated by 
numerically simulating the propagation and inter­ference of light. Typical sampling sizes are smaller 
than the wave­length of visible light. Therefore, a computer-generated hologram (CGH) must contain a 
huge number of samples. Furthermore, the cost of calculating each sample is high if a conventional approach 
is taken. Even with the power currently available in scienti.c workstations, researchers in the .eld 
of computational holography commonly report computation times in minutes or hours. In recent work[5], 
Lucente used a massively parallel supercomputer to cal­culate holographic fringes at interactive rates. 
In this paper we describe the .rst use of a standard graphics workstation to render and display holograms 
at interactive rates. The graphics workstation provided both a platform for generating image information 
and the computing power for generating holo­graphic fringes. Our use of a computer graphics workstation 
also eliminated data transfer bottlenecks that often prohibit interactive computation. We believe that 
new holographic displays will con­tinue to emerge in the future, necessitating rapid rendering (com­putation) 
of holograms. Our work is an important .rst step toward practical computation systems for holovideo. 
The following Background section gives a brief summary of the principles of holography. The computer 
generation of holograms and their display are reviewed, including a brief description of the real-time 
holographic display that we used in this research. In the section Method for Rendering Hologram we describe 
the hologram rendering algorithm, including the initial processing of object scene data to provide for 
realistic lighting, shading, occlusion, and other pictorial depth cues. Finally, we present results, 
future work, and a conclusion.  2. BACKGROUND This section contains an introduction to holography, computa­tional 
holography, holographic displays, and stereograms. The con­cepts discussed here were essential to the 
development of a faster method for hologram rendering. 2.1 Optical Holography Optical holography[2] 
uses the physical phenomena of interfer­ence and diffraction to record and reconstruct a 3-D image. Holo­graphic 
imaging became practical in the 1960 s with the advent of coherent monochromatic laser light. To produce 
a hologram, light is scattered from the object to be recorded. A photosensitive me­dium records the intensity 
(irradiance) pattern that results when the light scattered from an object interferes with a spatially 
clean mu­tually coherent reference beam. The reference beam allows the me­dium to record both the magnitude 
and phase of the incident object wavefront, in essence recording variations in both the intensity and 
the direction of the light. The recording medium must have suf.­cient resolution to record spatial frequencies 
that are typically 1500 linepairs/millimeter or more. To reconstruct an image, the recorded interference 
pattern mod­ulates an illuminating beam of light. The modulated light diffracts (bends and focuses) and 
reconstructs a 3-D replica of the wave­front that was scattered from the object scene. Optical wavefront 
reconstruction makes the image appear to be physically present and tangible. The image possesses all 
of the depth cues exhibited by the original object, including continuous parallax (vertical and horizontal) 
and ocular accommodation. Both the image resolution and parallax resolution of an optical holographic 
image are virtu­ally unlimited. GLOSSARY Visual Depth Cues binocular disparity: the binocular depth 
cue effected by the slight differences between the two retinal images captured by each eye. The depth 
sensation caused by binocular disparity is called stereopsis. convergence: a binocular depth cue effected 
when the eyes rotate to align the retinal images captured by each eye. occlusion, overlap: monocular 
depth cue effected when one part of image is obstructed by another overlapping part. ocular accommodation: 
a monocular depth cue in which the eye senses depth by focusing at different distances. parallax, motion 
parallax: a (monocular) depth cue sensed from the apparent change in the lateral displacements among 
objects in a scene as the viewer moves. A display which provides parallax allows the viewer to move around 
the object scene. pictorial depth cues: the monocular depth cues found in 2-D images, including occlusion, 
linear perspective, texture gradient, aerial perspective, shading, and relative sizes. 3-D Display Types 
 stereoscopic: a 3-D display type that presents a left view of the imaged scene to the left eye and a 
right view to the right eye. Examples include boom-mounted, head-mounted, and displays using polarizing 
glasses. autostereoscopic: a 3-D display type that presents left and right views of the imaged scene 
without special viewing aids. Examples include lenticular, parallax barrier, slice stacking, and holography. 
Some provide motion parallax by presenting more than two views. holovideo: a real-time 3-D electro-holographic 
display. Additional Terms basis fringe: an elemental fringe pattern computed to diffract light in a 
speci.c manner. The name basis fringe is an analogy to math­ematical basis functions. Linear summations 
of basis fringes were used as holographic patterns. computational holography, computer-generated holography: 
the numerical synthesis of holograms. hololine: a horizontal line of samples of a holographic fringe 
pattern. horizontal-parallax-only or HPO: possessing horizontal parallax but not vertical parallax. The 
viewer sees the same vertical perspec­tive of the imaged scene regardless of the vertical location of 
the eyes. image volume: the volume occupied by a 3-D image. image resolution: the number of resolvable 
image features in the lateral dimensions of an image. parallax resolution: the number of different perspective 
views available to the viewer. 2.2 Computational Holography To produce dynamic holographic images, researchers 
can com­pute and display holographic fringe patterns. Computational holog­raphy[4] generally begins with 
a 3-D numerical description of the object or scene to be reproduced. Light is numerically scattered from 
the object scene and propagated to the plane of the hologram. The object wavefront is calculated, and 
a reference beam wave­front added, imitating optical interference. The resulting total in­tensity - the 
fringe pattern - is used by a holographic display to produce the 3-D image. Such a display spatially 
modulates a beam of light with the fringes, mimicking the reconstruction step in opti­cal holography. 
Computational holography produces 3-D imaging with a high degree of realism. Only recently have researchers 
achieved compu­tational holography at interactive rates, impeded mainly by the enormous data content 
and by the computational complexity asso­ciated with holographic fringes[5]. A typical full-parallax 
holo­gram 100 mm × 100 mm in size has a space-bandwidth product (SBWP) of over 100 gigasamples of information. 
A larger image requires a proportionally larger number of samples. Several tech­niques have been used 
to reduce information content to a manage­able size. The elimination of vertical parallax provides great 
savings in display complexity[3] and computational require­ments[5] without greatly compromising the 
overall display perfor­mance. Other less desirable sacri.ces include reducing the size of the object 
scene or the size of the viewing zone. There are several methods of computing holographic fringes. Most 
imitate the interference that occurs when a hologram is pro­duced optically using coherent light. Early 
methods made use of the Fourier transform to calculate the phase and amplitude of the object wavefront 
at the plane of the hologram[4]. At each sample point in the hologram plane, this object light wavefront 
was added to a reference beam wavefront, and the magnitude squared became the desired fringe pattern. 
This approach was used to create planar images. Multiple image planes were separately processed and com­bined 
to produce 3-D images. Consequently, the Fourier-transform approach was slow and inef.cient for computing 
3-D images. A more straightforward approach to the computation of holo­graphic fringes resembled 3-D 
computer graphics ray-tracing. Light from a given point or element of an object contributed a complex 
wavefront at the hologram plane[2]. Each of these com­plex wavefronts was summed to calculate the total 
object wave­front which was subsequently added to a reference wavefront. Black (non-scattering) regions 
of the image volume were ignored, enabling very rapid computations of simple object scenes. Again, for 
more complex images, computation was prohibitively slow. In recent work[5], Lucente described a computational 
method that greatly simpli.ed the ray-tracing approach. By computing fringes that diffracted light only 
to speci.c regions of the image volume, this approach had a number of advantages: (1) it enabled the 
real-valued linear superposition of fringe patterns; (2) it in­creased the speed of computation by a 
factor of 2.0; (3) it elimi­nated the need for an explicit reference beam wavefront; and (4) it eliminated 
certain imaging artifacts and noise that generally de­grade the quality of holographic images. Real-valued 
summation enabled the ef.cient use of precomputed elemental fringe patterns (called basis fringes), an 
approach that achieved CGH computation at interactive rates when implemented on a supercomputer[5]. We 
chose to implement an approach similar to the Lucente ap­proach - using an array of precomputed basis 
fringes - because it promised to provide the fastest holographic computation. We fo­cused on further 
reducing computational complexity and on the implementation of holographic computation on hardware designed 
for computer graphics rendering. 2.3 Holographic Displays A holographic fringe pattern is used to modulate 
a beam of monochromatic light to produce an image. In the earliest work in computational holography, 
the computed fringe pattern was re­corded (permanently) in a piece of .lm[4]. The .lm provided the SBWP 
suf.cient to represent the fringes. In some cases, the .lm also provided grayscale. Light passing through 
this .lm created a static holographic image. To create dynamic holographic images, a dynamic spatial 
light modulator (SLM) must be used. The SBWP of a holographic SLM must be as high as that of holographic 
.lm. Ideally, a holographic SLM must display over 100 gigasamples. Current SLMs, however, can provide 
a maximum of 10 megasamples. Examples of SLMs include the .at-panel liquid-crystal display (LCD) and 
the magne­to-optic SLM. These SLMs are capable of displaying a very small CGH pattern in real time. Early 
researchers employed a liquid-­crystal SLM with an SBWP of 10,000 elements to produce a tiny planar image[6]. 
Most researchers reported images that were still quite small and essentially two-dimensional[7]. An ideal 
holographic SLM does not yet exist, but time-multi­plexing of a very fast SLM provides a suitable substitute. 
The dis­play system that we used in this research was similar to previously reported holovideo displays[3]. 
Our display used the combination of an acousto-optic modulator (AOM) and a series of lenses and scanning 
mirrors to assemble a small full-color 3-D holographic image at video frame rates. A partial schematic 
is shown in Figure 2. A general description follows, and a detailed description can be found in the reference 
by St. Hilaire et al.[3] It is important to note that the holographic rendering method described in this 
paper is not speci.c to one display. By incorporating the proper physical parameters, wavelengths and 
sample size, a hologram generated using this method can be viewed using other holographic displays. In 
our holovideo display, as each line (hololine) of the fringe pattern was read out of a high-speed framebuffer, 
it passed through a radio-frequency (RF) signal processing circuit and into the AOM. At any instant, 
as one line of the holographic pattern traversed the aperture of the AOM in the form of an acoustic wave 
(at a speed of 617 m/s), a portion equal to roughly 2000 samples modulated the phase of the wavefront 
of laser light that passed through the AOM. Two lenses reconstructed the diffracted light at a plane 
in front of the viewer. By re.ecting the light off of a synchronized horizon­tally scanning mirror, the 
apparent motion of the holographic pat­tern was cancelled. The scanning motion of the mirror also acted 
to angularly multiplex the image of the acoustic wave. It extended the apparent width of the imaged holographic 
pattern to 32768 sam­ples, each sample representing a physical spacing of 1.0 micron. The viewer saw 
a real 3-D image located just in front of the out­put lens of the system. The image occupied a volume 
that was 40 mm wide, 35 mm high and 50 mm deep. The size of the viewing zone - i.e., the range of eye 
locations from which the viewer can see the image - was 16 degrees horizontal. (Both viewing zone and 
image dimensions were minimized in order to reduce the required bandwidth of the display system.) The 
viewer experienced the depth cue of horizontal motion parallax. This was a horizontal-par­allax-only 
(HPO) image. Vertical parallax was sacri.ced to sim­plify the display. This restriction does not limit 
the display s usefulness in most applications. Because the holographic image possessed no vertical parallax, 
there was no need for diffraction in the vertical dimension. The vertical resolution of 64 lines over 
35 mm was equivalent to that of a common 2-D display. Because our display was a full-color system, the 
hologram was displayed as a vertical stack of 64 three-color hololines. The dis­play modulated three 
parallel channels, one for each of the red, green and blue primary colors. The AOM had 3 separate but 
paral­lel channels, each modulating a separate beam of incident laser light (633 nm red, 514 nm green, 
476 nm blue). Three separate fringe patterns were simultaneously read out of the 3-channel framebuffer. 
To be displayed on our holovideo system, a fringe pattern must adhere to three speci.cations. First, 
for each of three color separa­tions, the size of the fringe pattern must be 64 hololines of 32768 samples. 
Second, the sample size is 1.0 micron. Third, a separate fringe pattern must be computed at each of three 
wavelengths: 633 nm for red, 514 nm for green, and 476 nm for blue. 2.4 Stereograms A stereogram is 
a type of hologram that is composed of a series of discrete 2-D perspective views of the object scene[8][9]. 
A ste­reogram has two essential characteristics: it has discretized paral­lax; and it creates an image 
at a single plane, the image plane. Parallax discretization simpli.es fringe computation (and display). 
Computation is much faster than in the general case of a non-ste­reogram image. Provided that the number 
of discrete views is high enough, parallax appears to change smoothly, and the image ap­pears to be 3-D. 
An HPO stereogram produces a view-dependent image which presents in each horizontally displaced direction 
the corresponding perspective view of the object scene. The vertical perspective re­mains unchanged from 
any location in the viewing zone. In an im­age-plane stereogram, the image plane coincides with the physical 
plane of the hologram. As a result, the contribution to the object wavefront from each view-image pixel 
does not overlap with con­tributions from adjacent pixels. This reduces computational com­plexity. Speci.cally, 
discrete samples of the fringe pattern coinciding with a particular pixel are computed using only the 
view-dependent values of that pixel. In a stereogram, the viewing zone is treated as an array of dis­cretized 
subzones. An eye of the viewer, when moving from side to side, sees one discrete view of the scene after 
another. To compute a stereogram, a sequence of view images is rendered using a differ­ent camera geometry 
for each view image. These images are com­bined to calculate fringes for display. The only disadvantage 
of a stereogram is that it may not provide the depth cue of ocular ac­commodation. Ocular accommodation, 
however, is generally less important than binocular disparity, motion parallax and occlusion. A stereogram 
provides all other depth cues, and produces a realis­tic 3-D image. The obvious advantage of a stereogram 
is that conventional computer graphics hardware can be used to render the sequence of view images. This 
makes pre-existing models and rendering meth­ods applicable to holography.  Figure 3a: Three stereogram 
components. (Top view.) Light is diffracted from each in a speci.c direction. Figure 3b: The composite 
hologram.  3. METHOD FOR HOLOGRAM RENDERING The fast computation of a stereogram involved two parts: 
ren­dering different view images, and using them to create holographic fringes. Beginning with a .nite 
number of views (in this case eight), our goal was to construct an HPO image-plane stereogram that showed 
the correctly rendered view image for each viewing angle. The development of a computational approach 
began by consid­ering how light should be diffracted (or scattered) by the fringes. Consider an image 
displayed on a CRT: the light from the 2-D im­age scatters over the entire (often wide) viewing angle. 
In contrast, a stereogram must diffract light selectively. A stereogram causes a particular view image 
to be visible only from a particular part of the viewing zone. Figure 3a shows light diffracting over 
only a part of the viewing zone for three different views. In Figure 3b, the composite holographic fringe 
pattern in a single pixel region dif­fracts speci.c amounts of light in three different viewing direc­tions. 
The amount of light corresponds to the pixel brightness values at this particular pixel location for 
each view image. This section describes each step of the computational algorithm that we implemented 
on a graphics workstation. We began by pre­computing a set of basis fringes and storing them in memory. 
As described in the following subsection, each basis fringe controlled the speci.c directional behavior 
of diffracted light. The next three subsections describe the three steps used to compute the actual fringe 
pattern: rendering a set of views (using conventional com­puter graphics rendering methods), generating 
a component stereo­gram fringe pattern from each view using an array of precomputed basis fringes, and 
compositing these separate stereogram compo­nents into the .nal fringe pattern. The .nal subsection is 
a descrip­tion of the implementation speci.cs of our research. 3.1 Precomputing the Set of Basis Fringes 
The arrays of precomputed basis fringes were designed to dif­fract light in speci.c directions. One basis 
fringe was the holo­graphic pattern that caused light to diffract from a particular view-image pixel 
location to a particular viewing subzone. An ar­ray of basis fringes - one for each view-image pixel 
- was com­puted for each viewing subzone. This entire set of basis fringe arrays - one for each viewing 
subzone - was computed for each of the three (primary color) wavelengths used to create the full-color 
holographic image. The set of basis fringes can be computed in several ways. We used an iterative numerical 
approach to solve the common integral expression that describes the diffraction of optical wavefronts. 
We began by specifying the direction that light must be diffracted by each basis fringe. For an image-plane 
stereogram, each basis fringe was characterized by the diffraction from one view-image pixel re­gion 
to one viewing subzone, as illustrated in Figure 4. The sub­zones, each of angular width ., divided the 
viewing zone into non-overlapping pieces across the horizontal range of the viewing zone. The horizontal 
parallax resolution was equal to the number of subzones. Each pixel region of width wcoincided physically 
with a single rendered view-image pixel location. To make a view image visible from one of the viewing 
subzones, the basis fringe within each pixel region must be modulated by the pixel value of the rendered 
view image. In the analysis that follows, we denoted the modulated light in the plane of the image (and 
hologram) asux in complex scalar () notation. The light at the viewing zone distance zwas represented 
by the complex scalarv. expressed as a function of viewing an­ () gle .. We de.ned each basis fringe 
through a set of constraints on the magnitudes ofux and v. () (). Then, by relating related()andv. through 
the law of diffraction, we calculated each ux() basis fringe by numerically solving for the phase of 
ux. () For a single view-image pixel, the constraint on ux () was straightforward: within the pixel 
region, the diffracted intensity should be constant. Therefore, ux () must be virtually constant. For 
a single view, the modulated lightux should diffract light () only to a particular viewing subzone to 
avoid leakage or ghosting. Within the viewing subzone,v. should be constant. Therefore, () the constraint 
onv. was that () v. () =1 for .. - <./2, v v. () =0 elsewhere, where. is the angle (from normal) connecting 
the center of a v pixel region to the center of the viewing subzone. Finally, the phase ofv. was invisible 
and therefore was left unconstrained. () When computed to satisfy these constraints, a basis fringe diffracts 
a unity brightness from a given pixel region to a speci.c viewing subzone. The values ofux and ofv. () 
()were related through the laws of optical diffraction[10].We invoked the far-.eld (Fraunhofer) dif­fraction 
approximation (expressed in complex time-harmonic nota­tion): 2 ) w/2 ikz(1 +./2 e -ikx. v.= uxe dx(1) 
() () . ikz -w/2 wherek=2 p.and .is the free-space wavelength of the light. / The far-.eld approximation, 
valid for zw 2 /., was appropriate » for the dimensions that we used: w= 0.250 mm and z=600 mm. w u(x) 
 z   viewing zone viewer Figure 4: Diffraction caused by an array of basis fringes. Light diffracts 
from all pixel regions to a particular viewing subzone. The integral expression in Equation 1 is essentially 
a Fourier trans­form integral. The wavefront at the viewing zone, v., is a Fou­ () rier transform of 
(), with additional leading terms to account uxfor phase curvature and for power conservation. Equation 
1 reveals that while the constraints onux were spatial, the constraints on () ()determined the spectrum 
of (). An appropriately band-­ v.uxlimitedux resulted in an angularly limitedv. () ()that satis.es the 
constraints on v. () . For one pixel location and one viewing subzone, the uncon­strained phase ofux 
was numerically computed using an itera­ () tive constraint approach[11] that uses both the forward and 
the inverse Fourier transforms: 1. Generate a random fringe pattern of width w. 2. Apply the (spatial) 
constraints on ux.  () 3. Transform into the spatial frequency domain. 4. Apply the (spectral) constraints 
on v..  () 5. Inverse transform back to the spatial domain. 6. Iterate starting at step 2. The iteration 
continued untilux converged. Satisfactory con­ () vergence resulted after typically 10 to 20 iterations. 
As in work by Lucente[5], the real part ofux was used as the basis fringe. A () selection of precomputed 
basis fringes is shown in Figure 5. We computed the array of basis fringe patterns for each viewing subzone 
in the same way, with each basis fringe occupying differ­ent view-image pixel locations in the image 
plane. The array for each viewing subzone was computed using a unique viewing direc­tion .. Three different 
basis sets were computed - one for each v color - at the correct red, green or blue wavelengths (633 
nm, 514 nm, and 476 nm). These basis sets were stored in memory and used during actual hologram computation. 
 3.2 Generating the View Images The .rst step during the actual computation of the CGH pattern was to 
render a series of perspective view images from left to right. Each discrete camera location had a unique 
angle with respect to the scene being rendered. The .nal hologram diffracted light in the directions 
corresponding to these camera view-angles. This section describes the recentering shear-camera geometry 
that we used to render the view images, as well as the bounding and discretization of the image volume. 
To render each view image, the camera geometry was changed by moving the camera from left to right at 
a constant spacing along a straight line. (See Figure 6.) When lateral motion is the only Figure 5: 
A selection of basis fringes that correspond to the center view-image pixel for each of the eight viewing 
sub­zones. Each is 256 samples long. The top fringe represents a mean diffraction angle of 1.8 degrees 
in red light, corre­sponding to the left-most viewing subzone. The bottom fringe represents a mean diffraction 
angle of 14.7 degrees in red light, corresponding to the right-most viewing subzone. change in camera 
geometry, the object scene tends to walk out of the frame and becomes invisible in the far left or far 
right views. Therefore it was necessary to recenter the object scene in the frame of the camera as the 
camera moved from left to right. The correct way to recenter the object scene for each camera lo­cation 
was to move the view window, as shown in Figure 6. For the center view (position A in Figure 6), where 
the camera was directly in front of the scene, the camera geometry was simple. The view normal faced 
the scene, and the view window was centered with respect to the view normal. For any other view, the 
view window was off center. As the camera was moved off center (Figure 6, po­sition B), the view window 
was moved off center in the opposite direction. This is called a shear camera geometry. Note that the 
view normal does not change. This has the effect of keeping the objects in the center of the frame and 
in holography is called a re­centering camera. If the recentering is done correctly, a point (in the 
3-D object scene) that lies on the image plane renders to the same pixel location in each rendered view 
image. When rendering the view images, we restricted the scene vol­ume to match the size of the image 
volume achievable in our holo­video display. If an object crosses the frame, clipping occurs. This clipping 
(called image vignetting) is a window violation, which of­ten hinders the feeling of depth. Therefore, 
we chose to con.ne ob­jects to lie inside the restricted volume so that objects did not cross the frame 
boundaries. We chose the resolution of the rendered view images to be 128 by 64. In the horizontal dimension, 
the CGH was capable of imag­ing to resolutions that were beyond the typical acuity of the human visual 
system. We chose a horizontal resolution of 128 to give an image pixel width of 0.25 mm, or roughly the 
smallest feature size that a human viewer can discern at typical viewing distances. We matched the vertical 
resolution of the view images to that of our holovideo display (64 lines). 3.3 Computing the Component 
for Each View We chose to use the image-plane stereogram geometry because it ensured that a view-image 
pixel contributed only to the part of the fringe pattern that coincided with that pixel region. A given 
precomputed basis fringe was multiplied by its respective pixel value at each sample in the pixel region. 
When sent to the display, the resulting fringe pattern diffracted the correct amount of light to the 
correct viewing subzone. For one view, the fringe pattern in this pixel region was not a function of 
any other pixel values. In other words, in a given stereogram view component, the fringe contributions 
from each pixel did not overlap. Therefore, because of the image-plane stereogram geometry, each of the 
pixel values for one view image independently modulated each basis fringe in one array. We designed the 
holovideo rendering algorithm to perform the multiplication of basis fringes by pixel values using a 
single tex­ stereogram image plane view plane camera track Figure 6: Camera geometry for rendering view 
images. ture-mapping instruction, eliminating the need to multiply the basis fringe for each pixel one 
by one. The array of precomputed basis fringes for a particular view was multiplied by the appropriately 
re­sized pixel map of the rendered view image. The result, a single stereogram component, diffracted 
light to form the image for a given viewing subzone. 3.4 Compositing the View Components After each 
stereogram component was generated, it was com­posited into the .nal CGH pattern. The approach that we 
use al­lowed for the linear superposition of computed fringe patterns[5]. Therefore, the components were 
simply summed. An array of basis fringes patterns were modulated by an array of weighting values (a view 
image) and accumulated in the accumulation buffer. The pro­cess is illustrated in Figure 7. Below is 
a piece of pseudocode that describes the process. Set the accumulation buffer to zero. For each view 
i { Load ith array of basis fringes. Generate ith view image. Modulate fringes with the view image. 
 Add result to accumulation buffer. } Display sum contained in accumulation buffer.  lation buffer 
Figure 7: The modulate and sum process of rendering. 3.5 Implementation Speci.cs Our algorithm for rendering 
holographic fringe patterns was im­plemented initially on an SGI Skywriter with a VGXT graphics subsystem 
and more recently on an Onyx with a RealityEngine2 (RE2). In both cases, the graphics hardware subsystems 
had two enabling features: an integrated accumulation buffer and a recon­.gurable framebuffer. The accumulation 
buffer was an essential part of the fast algorithm. The recon.gurability of the framebuffer enabled the 
graphics subsystem to drive our holovideo display di­rectly, but was not essential to the rendering algorithm. 
We recon.gured the graphics framebuffers to provide three ana­log output signals in a format that was 
suited to our holovideo dis­play. The active data array was set to 2048×1024 in each of the three 8-bit 
buffers. (The display system unfolds the 2048×1024 video signal to a 32768×64 pattern.) We adjusted the 
clock rate of the three 2-MB buffers to generate signals at a rate of 110 MB/­s/channel. We also adjusted 
blanking intervals to minimize the im­age noise caused by signal blanking that occurred within each hololine. 
We could not eliminate blanking, but the length of the blanking intervals was reduced to less than 4 
percent of the active data line. The following is a description of the algorithm illustrated in Fig­ure 
7 as we implemented it on the SGI graphics subsystems. The algorithm had three steps: render the view 
images, modulate the ar­rays of basis fringes using the rendered view images, and sum the results in 
the accumulation buffer. The view images were rendered using straightforward GL library calls. The camera 
geometry was adjusted for each view using the method described earlier in the section 3.2 Generating 
the View Images. Each of the 128×64-pixel view images was stored in memory. The framebuffer was loaded 
with the array of basis fringes for a particular view. The 32768×64 fringe pattern was folded into the 
2048×1024 framebuffer. The 128×64 view image was mapped over the entire framebuffer and used to modulate 
the basis fringes. The graphics subsystem used the view image as a texture map on the surface of a polygon 
that covered the entire framebuffer. The blending feature of the graphics subsystem was set to multiply 
the current value in the framebuffer with the pixel value from the texture map. Effectively, the basis 
fringes were weighted by the view-image pixels. The re­sulting stereogram component was added to the 
accumulation buffer and the process was repeated for each view. The .nal holo­graphic pattern was moved 
from the accumulation buffer to the framebuffer for viewing. All of these steps were performed in par­allel 
for all three colors, generating a total of 6 MB of CGH.  4. RESULTS Holographic fringes were computed 
on either SGI platform and used to generate 3-D images on our real-time holographic display. Figures 
8, 9, and 10 are photographs of typical images, each gener­ated using eight full-color view images at 
a resolution of 128 by 64. Figure 8 shows a human head and a car, both generated from polygonal models. 
Figure 9 shows a simple image of three cut cubes, photographed from different locations in the viewing 
zone. Figure 10 shows a time sequence of an object generated using an interactive modeling system. The 
horizontal image resolution of 128 was suf.cient to produce the appearance of continuous surfaces. Some 
noticeable image arti­fact was due to the 4-percent blanking, which produced the faint vertical stripes 
visible in Figures 8, 9, and 10. The vertical resolu­tion, limited by the display system to 64, is also 
evident in these photographs. (We remark that photographing holographic images poses a challenge, and 
the quality of these illustrations suffers as a result.) As discussed in the Future Work section, image 
quality can be improved using a new approach that is more precise than the stereogram approach. Image 
depth and color were very good. The images gave a good sensation of depth, though the limited viewing 
zone and lateral di­mensions weakened the impact. Objects appeared to occupy a depth of over 50 mm. Color 
was superb. The three wavelengths used as primary colors spanned a very broad color space that was much 
larger than that of a typical high-quality 2-D display. Computation time was 2 seconds (for the 6-MB 
fringe pattern) using the Onyx/RE2 platform (two-processor Onyx, 4-raster-man­ager RE2) and 5 seconds 
for the Skywriter/VGXT platform. (For comparison, the same hologram required over 8 minutes when im­plemented 
using a standard method on the SGI Skywriter.) Compu­tation time comprised two parts: view-image rendering 
and fringe computation. The time spent rendering the view images was less than 10 percent of total hologram 
computation time. Fringe com­putation was O(v*n), where v was the number of views (parallax resolution) 
and n was the total number of samples in the fringe pat­tern. The sample count n was essentially a measure 
of image vol­ume. Fringe computation time was independent of view-image resolution, which only affected 
the rendering time. The precompu­tation of the basis fringes did not contribute to the computation time 
per CGH frame. The precomputed basis fringes were com­puted only once and stored in memory. Transfer 
of the 6-MB fringe pattern from the accumulation buffer to the framebuffer required negligible time since 
these two buffers resided in the same hardware. Therefore, our implementa­tion on either graphics subsystem 
avoided a common data transfer bottleneck. For comparison, in past work by Lucente[5] using a data-parallel 
supercomputer, 0.4 s was spent transferring the 6-MB fringe pattern from the 16 K processors to the framebuffer. 
  The benchmark computation time of 2 s for our rendering method was fast enough to provide interactivity 
to a patient viewer. We anticipate that computation speeds will continue to increase as computational 
power increases and further computational ef.cien­cies are obtained. In fact, the work using the Skywriter/VGXT 
platform was performed two years ago. Our recent upgrade to the Onyx/RE2 platform more than doubled speed, 
and new graphics hardware should continue to offer faster implementations of our hologram rendering algorithm. 
 5. FUTURE WORK The holovideo rendering algorithm presented here is applicable not only to image-plane 
stereograms but to all types of synthetic holograms. Research is underway to adapt this rendering method 
to the computation of 3-D volume (non-stereogram) images using the newly developed method of diffraction-speci.c 
fringe computa­tion that also allows for holographic bandwidth compression[12]. Such a 3-D volume image 
possesses more depth and tangibility be­cause it produces an actual 3-D real image and provides the depth 
cue of ocular accommodation. Finally, the computation of full-par­allax holographic images is possible, 
though no full-parallax holo­graphic 3-D display yet exists. We have generated holograms from live scenes. 
The simplest approach is to use an array of horizontally displaced video cameras to provide view images 
in our hologram rendering algorithm. Faster frame rates will enable the production of real-time holo­graphic 
live video of actual scenes. 6. CONCLUSION Three-dimensional displays are superior to 2-D displays in 
many applications of computer graphics. Holographic displays produce images with the greatest number 
of depth cues and therefore the greatest sensation of 3-D. With their ability to produce images in real 
time with high resolutions and full color, holographic displays promise the highest degree of image realism. 
Our research has demonstrated that existing computer graphics workstation technology can be used to generate 
holographic fringe patterns at interactive rates for the real-time display of 3-D images. A workstation 
such as the SGI Onyx coupled with a specialized rendering engine is capable of driving a holovideo display 
directly. As holographic displays become larger and more practical, the computational method presented 
here can be scaled up to produce holographic fringes for images that occupy a larger volume and provide 
a larger viewing zone. We have presented only a single case in a wide range of holo­gram types, computation 
techniques, display architectures, applica­tions, and hardware and software support. Nevertheless, the 
importance of this work is that it breaks the deadlock that has im­peded the development of methods for 
both the computing and the displaying of 3-D holographic images. We have reported the use of an existing 
computer graphics system to generate a high-band­width holographic signal for a holovideo display. Our 
research shows that a computer graphics workstation provides suf.cient computational support for the 
development of real-time electronic holography. Furthermore, our work heralds the marriage of the two 
.elds of computer graphics and electronic holography.  ACKNOWLEDGMENTS We thank many: Stephen A. Benton 
for guidance in the holo­video project; Michael Halle for contributions to implementation issues; David 
Zeltzer; Pierre St. Hilaire and John D. Sutter for help with the holovideo display optics; Scott Pritchett, 
John Hallesy, Gregory Eitzmann, and Jim Foran at Silicon Graphics, Inc. for help with framebuffer recon.guration; 
and Katherine Wrean for help in preparing this document. Components of this research have been sponsored 
by Honda Re­search and Development Co., Ltd.; NEC Corporation; International Business Machines Corp.; 
the Advanced Research Projects Agency (ARPA) through the Naval Ordnance Station, Indian Head, Maryland 
(under contract No. N00174-91-C0117); the Television of Tomorrow research consortium of the Media Laboratory, 
MIT; Apple Computer Inc.; and NHK (Japanese Broadcasting Corp.). Silicon Graphics, Inc. (Mountain View, 
CA, USA) are the cre­ators of Skywriter and Onyx computer graphics workstations, the VGXT and RealityEngine 
graphics subsystems, and GL graphics software. REFERENCES [1] Michael McKenna and David Zeltzer. Three 
Dimensional Visual Display Systems For Virtual Environments. Presence: Teleoperators and Virtual Environments. 
Vol. 1 #4, 1992, pp. 421-458. [2] P. Hariharan. Optical Holography. Cambridge: Cambridge University Press. 
1984. [3] Pierre St. Hilaire, Stephen A. Benton and Mark Lucente. Syn­thetic Aperture Holography: A Novel 
Approach To Three Dimensional Displays. Journal of the Optical Society of America A, Vol. 9, #11, Nov. 
1992, pp. 1969 - 1977. [4] William J. Dallas. Computer-Generated Holograms. Chapter 6 in The Computer 
in Optical Research, Topics in Applied Physics, Vol. 41, ed. B.R. Frieden. Berlin: Springer-Verlag. 1980, 
pp. 291-366. [5] Mark Lucente. Interactive Computation Of Holograms Using A Look-up Table. Journal of 
Electronic Imaging, Vol. 2, #1, Jan 1993, pp. 28-34. Also: Mark Lucente. Optimization of hologram computation 
for real-time display. in SPIE Proc. #1667 Practical Holography VI, 1667-04 (SPIE, Bellingham, WA, 1992), 
pp. 32-43. [6] F. Mok, J. Diep, H.-K. Liu and D. Psaltis. Real-time Comput­er-generated Hologram By Means 
Of Liquid-Crystal Televi­sion Spatial Light Modulator. Optics Letters, Vol. 11 #11, Nov. 1986, pp. 748-750. 
[7] S. Fukushima, T. Kurokawa and M. Ohno. Real-time Holo­gram Construction And Reconstruction Using 
A High-resolu­tion Spatial Light Modulator. Applied Physics Letters, Vol. 58 #8, Aug. 1991, pp. 787-789. 
[8] Stephen A. Benton. Survey of Holographic Stereograms. In Processing and Display of Three-Dimensional 
Data. Proceed­ings of the SPIE 367, 1983, pp. 15-19. [9] Michael Halle. The Generalized Holographic Stereogram. 
Master s thesis, Massachusetts Institute of Technology, 1991. [10] Joseph W. Goodman. Introduction to 
Fourier Optics. New York: McGraw-Hill. 1968. [11] J. R. Fienup. Iterative Method Applied to Image Reconstruc­tion 
And to Computer-Generated Holograms. Optical Engi­neering, Vol. 19 #3, May/June 1980, pp.297-305. [12] 
Mark Lucente. Diffraction-Speci.c Fringe Computation for Electro-Holography. Ph.D. Thesis, Massachusetts 
Institute of Technology, 1994.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218494</article_id>
		<sort_key>395</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[An integrated environment to visually construct 3D animations]]></title>
		<page_from>395</page_from>
		<page_to>398</page_to>
		<doi_number>10.1145/218380.218494</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218494</url>
		<keywords>
			<kw><![CDATA[3D animation]]></kw>
			<kw><![CDATA[3D interaction]]></kw>
			<kw><![CDATA[3D widgets]]></kw>
			<kw><![CDATA[data reduction]]></kw>
			<kw><![CDATA[local propagation constraints]]></kw>
			<kw><![CDATA[object-oriented graphics]]></kw>
			<kw><![CDATA[virtual tools]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P77645</person_id>
				<author_profile_id><![CDATA[81100044134]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Enrico]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gobbetti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Advanced Studies, Research and Development in Sardinia, CRS4, Scientific Visualization Group, Via Nazario Sauro 10, 09123 Cagliari, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P136551</person_id>
				<author_profile_id><![CDATA[81100088835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Francis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Balaguer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Advanced Studies, Research and Development in Sardinia, CRS4, Scientific Visualization Group, Via Nazario Sauro 10, 09123 Cagliari, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baecker RM (1969) Picture-driven Animation. Proc. Spring Joint Computer Conference 34: 273-288.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Balaguer JF (1993) Virtual Studio: Un syst~me d'animation en environnement virtuel. PhD Thesis, Swiss Federal Institute of Technology in Lausanne.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Balaguer JF, Gobbetti E (1995) Animating Spaceland. To appear in IEEE Computer Special Isssue on Real-world Virtual Environments 28(7).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Balaguer JF, Gobbetti E (1995) Sketching 3D Animations. To appear in Proc. EUROGRAPHICS.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Balaguer JF, Gobbetti E (1995) Supporting Interactive Animation using Multi-way Constraints. Submitted for publication.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91425</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Banks M, Cohen E (1990) Real-time Spline Curves from Interactively Sketched Data. Proc. SIGGRAPH Symposium on Interactive 3D Graphics: 99-107]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378509</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Barzel R, Barr A (1988) A Modeling System Based on Dynamic Constraints. Proc. SIGGRAPH: 179-188.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91446</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bier EA (1990) Snap-Dragging in Three Dimensions. Proc. SIGGRAPH Symposium on Interactive 3D Graphics: 193- 204.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>161303</ref_obj_id>
				<ref_obj_pid>161291</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Borning A, Freeman-Benson B, Wilson M (1992) Constraint Hierarchies. Lisp and Symbolic Computation 5(3): 221-268.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147182</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Butterworth J, Davidson A, Hench S, Olano TM (1992) 3DM: A Three Dimensional Modeler Using a Head-Mounted Display. Proc. SIGGRAPH Symposium on Interactive 3D Graphics: 135-138.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108874</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Card SK, Robertson GG, Mackinlay JD (1991) The Information Visualizer: An Information Workspace. Proc. SIGCHI : 181-188.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617751</ref_obj_id>
				<ref_obj_pid>616023</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Chou JJ, Piegl LA (1992) Data Reduction Using Cubic Rational Splines. IEEE Computer Graphics and Applications 12(3): 60-68.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>147199</ref_obj_id>
				<ref_obj_pid>147156</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Conner DB, Snibbe SS, Herndon KP, Robbins DC, Zeleznik RC, Van Dam A (1992) Three-Dimensional Widgets. SIGGRAPH Symposium on Interactive 3D Graphics: 183- 188.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192276</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Elliott C, Schechter G, Yeung R, Abi-Ezzi S (1994) TBAG: A High Level Framework for Interactive, Animated 3D Graphics Applications. Proc. SIGGRAPH: 421-434.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>168653</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Gleicher M (1993) A Graphics Toolkit Based on Differential Constraints. Proc. UIST: 109-120.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134088</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Gleicher M, Witkin A (1992) Through-the-Lens Camera Control. Proc. SIGGRAPH : 331-340.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Gobbetti E (1993) Virtuality Builder H: Vers une architecture pour l'interaction avec des modes sysnth{tiques. PhD Thesis, Swiss Federal Institute of Technology in Lausanne.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>168659</ref_obj_id>
				<ref_obj_pid>168642</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Gobbetti E, Balaguer JF (1993) VB2: A Framework for Interaction in Synthetic Worlds. Proc. UIST: 167-178.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Herndon KP, van Dam A, Gleicher M (1994) Report: Workshop on the Challenges of 3D Interaction, CHI Bulletin, October.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134087</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Kass M (1992) CONDOR: Constraint-based Dataflow. Proc. SIGGRAPH: 321-330.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37407</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Lasseter J (1987) Principles of Traditional Animation Applied to 3D Computer Animation. Proc. SIGGRAPH: 35 - 44.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>43651</ref_obj_id>
				<ref_obj_pid>43647</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Lyche T, M~rken K (1987) Knot Removal for Parametric B- spline Curves and Surfaces. Computer Aided Geometric Design 4: 217-230.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>91441</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[McKenna M, Pieper S, Zeltzer D (1990) Control of a Virtual Actor: The Roach. Proc. SIGGRAPH Symposium on Interactive 3D Graphics: 165-174.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801153</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Plass M, Stone M (1983) Curve Fitting with Piecewise Parametric Cubics. Proc. SIGGRAPH: 229-239.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Pudet T (1994) Real Time Fitting of Hand Sketched Pressure Brushstrokes. Proc. EUROGRAPHICS : 205-220.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617702</ref_obj_id>
				<ref_obj_pid>616020</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Sachs E, Roberts A, Stoops D (1990) 3-Draw: A Tool for Designing 3D Shapes. IEEE Computer Graphics and Applications 11 (6): 18-26.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192485</ref_obj_id>
				<ref_obj_pid>192426</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Sannella M (1994) Skyblue: A Multi-Way Local Propagation Constraint Solver for User Interface Construction. Proc. UIST : 137-146.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>153498</ref_obj_id>
				<ref_obj_pid>153493</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Sannella M, Maloney J, Freeman-Benson B, Borning A (1992) Multi-way versus One-way Constraints in User- Interfaces. Software Practice and Experience 23(5): 529-566.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Schneider PJ (1988) Phoenix: An Interactive Curve Design System Based on the Automatic Fitting of Hand-Sketched Curves. Master's Thesis, University of Washington.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197517</ref_obj_id>
				<ref_obj_pid>192426</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Shaw C, Green M (1994) Two-Handed Polygonal Surface Design. Proc. UIST: 212-215.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801276</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Shelley KL, Greenberg DP (1982) Path Specification and Path Coherence. Proc. SIGGRAPH: 157-166.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134089</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Strauss PS, Carey R (1992) An Object-Oriented 3D Graphics Toolkit. Proc. SIGGRAPH : 341-347.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Tice S (1993) VActor Animation Creation System. SIGGRAPH Tutorial 1.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617498</ref_obj_id>
				<ref_obj_pid>616006</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Upson C, Fulhauber T, Kamins D, Laidlaw D, Schlegel D, Vroom J, Gurwitz R, van Dam A (1989) The Application Visualization System: A Computational Environment for Scientific Visualization. IEEE CG&amp;A 9(4): 30-42.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Walters G (1993) Performance Animation at PDI. SIGGRAPH Tutorial 1.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>898820</ref_obj_id>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Vander Zanden B (1995) An Incremental Algorithm for Satisfying Hierarchies of Multi-way, Dataflow Constraints. Technical Report, University of Tennessee, Knoxville.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122730</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Zeleznik RC, Conner DB, Wlocka MM, Aliaga DG, Wang NT, Hubbard PM, Knepp B, Kaufman H, Hughes JF, van Dam A (1991) An Object-Oriented Framework for the Integration of Interactive Animation Techniques. Proc. SIGGRAPH: 105-112.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Zeltzer D, Pieper S, Sturman DJ (1989) An Integrated Graphical Simulation Platform. Proc. Graphics Interface: 266-274.]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Integrated Environment to Visually Construct 3D Animations Enrico Gobbetti and Jean-Francis Balaguer 
Center for Advanced Studies, Research and Development in Sardinia ABSTRACT In this paper, we present 
an expressive 3D animation environment that enables users to rapidly and visually prototype animated 
worlds with a fully 3D user-interface. A 3D device allows the specification of complex 3D motion, while 
virtual tools are visible mediators that live in the same 3D space as application objects and supply 
the interaction metaphors to control them. In our environment, there is no intrinsic difference between 
user­interface and application objects. Multi-way constraints provide the necessary tight coupling among 
components that makes it possible to seamlessly compose animated and interactive behaviors. By recording 
the effects of manipulations, all the expressive power of the 3D user interface is exploited to define 
animations. Effective editing of recorded manipulations is made possible by compacting all continuous 
parameter evolutions with an incremental data-reduction algorithm, designed to preserve both geometry 
and timing. The automatic generation of editable representations of interactive performances overcomes 
one of the major limitations of current performance animation systems. Novel interactive solutions to 
animation problems are made possible by the tight integration of all system components. In particular, 
animations can be synchronized by using constrained manipulation during playback. The accompanying video-tape 
illustrates our approach with interactive sequences showing the visual construction of 3D animated worlds. 
All the demonstrations in the video were recorded live and were not edited.  Keywords 3D Interaction, 
3D Widgets, Virtual Tools, 3D Animation, Local Propagation Constraints, Data Reduction, Object-Oriented 
Graphics. CRS4, Scientific Visualization Group, Via Nazario Sauro 10, 09123 Cagliari, Italy. E-mail: 
{gobbetti|balaguer}@crs4.it WWW: http://www.crs4.it/~gobbetti http://www.crs4.it/~balaguer  Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 1. INTRODUCTION 
Modern 3D graphics systems allow a rapidly growing user community to create and animate increasingly 
sophisticated worlds. Despite their inherent three-dimensionality, these systems are still largely controlled 
by 2D WIMP user-interfaces. The lack of correlation between manipulation and effect and the high cognitive 
distance from users to edited models are the major drawbacks of this solution [13]. The inadequacy of 
user-interfaces based on 2D input devices and mindsets becomes particularly evident in the realm of interactive 
3D animation. In this case, the low-bandwidth communication between user-interface and application and 
the restrictions in interactive 3D motion specification capabilities make it extremely difficult to define 
animations with straight-ahead actions. This inability to interactively specify the animation timing 
is a major obstacle in all cases where the spontaneity of the animated object's behavior is important 
[21; 35; 4]. In this paper, we present an expressive 3D animation environment that enables users to rapidly 
and visually prototype animated worlds with a fully 3D user-interface. A 3D device allows the specification 
of complex 3D motion, while virtual tools supply the interaction metaphors to control application objects. 
In our environment, there is no intrinsic difference between user­interface and application objects. 
Multi-way constraints provide the necessary tight coupling among components that makes it possible to 
compose animated and interactive behaviors. By recording the effects of manipulations, all the expressive 
power of the 3D user interface is exploited to define animations. Effective editing of recorded manipulations 
is made possible by compacting all continuous parameter evolutions with our data-reduction algorithm, 
designed to preserve both geometry and timing. Novel interactive solutions to animation problems are 
made possible by the tight integration of all system components. In particular, animations can be synchronized 
using constrained manipulation during playback. In the following sections, we present an overview of 
the system, we make comparisons with related work, and we conclude with a view of future directions. 
The accompanying video-tape illustrates our approach with interactive sequences showing the visual construction 
of 3D animated worlds. All demonstrations in the video were recorded live and were not edited. 2. SYSTEM 
OVERVIEW 2.1 Dynamic Model Our animation environment is built on top of VB2 [17; 18], a graphics architecture 
based on objects and constraints. During interaction, the user is the source of a flow of information 
propagating from input device sensors to manipulated models. VB2 applications are represented by a network 
of interrelated objects, and the maintenance of relationships is delegated to a constraint-based change 
propagation mechanism. Different primitive elements represent the various aspects of the system's state 
and behavior: active variables store the system's state, domain-independent hierarchical constraints 
[9] maintain multi­way relations between active variables, daemons provide support for discrete simulation 
tasks, and indirect expressions allow constraints and daemons to dynamically locate their variables. 
Constraints are maintained using an efficient local propagation algorithm based on Skyblue [27; 17; 18]. 
The solver is domain­independent and can maintain a hierarchy of multi-way, multi­output dataflow constraints. 
The fact that constraint solving consists in performing method selection on the basis of constraint priorities 
and graph structure, without considering the variables' values, allows an effective application of a 
lazy evaluation strategy [17; 18]. The main drawback of such a local propagation algorithm is the limitation 
to acyclic constraint graphs. However, as noted by Sannella et al. [28], cyclic constraint networks are 
seldom encountered in the construction of user interfaces, and limiting the constraint solver to graphs 
without cycles gives enough efficiency and flexibility to create highly responsive complex interactive 
systems. In VB2 , the objects' internal constraint networks are designed so as to reduce the possibility 
of creating cyclic constraint graphs. Runtime introduction of a constraint that would create a cyclic 
graph causes an exception that can be handled to remove the offending constraint1. The state manager 
behavior and the constraint solving techniques are detailed in [17; 18]. 2.2 Interaction The system's 
desktop configuration uses keyboard commands to trigger mode changes and animation playback, a Spaceball 
for continuous specification of spatial transformations, and a mouse for picking. Both hands are thus 
used simultaneously to input information. LCD shutter glasses provide binocular perception of the synthetic 
world. Since our main research goal is to explore the potentialities of 3D interaction, we do not provide 
a two­dimensional graphical user interface. A 3D cursor, controlled by the Spaceball, is used to select 
and manipulate objects of the synthetic world. Direct manipulation and virtual tools are the two techniques 
used to input information. Both techniques involve using mediator objects that transform the cursor's 
movements into modifications of manipulated objects. Virtual tools are visible first class objects that 
live in the same 3D space as application objects and offer the interaction metaphor to control them. 
Their visual appearance is determined by a modeling hierarchy, while their behavior is controlled by 
an internal constraint network [18]. As in the real world, users configure their workspaces by selecting 
tools, positioning and orienting them in space, and binding them to application objects. At the moment 
of binding, the tool decides whether to accept the connection by checking if the application object contains 
all the needed information and by verifying that the constraint graph obtained by connecting the tool 
to the model can be handled by the underlying solver (i.e. it is acyclic). The binding mechanism is defined 
in a declarative way by using indirect constraints [18]. 1VB2's current constraint solver [17; 28] is 
unable to find acyclic solutions of potentially cyclic constraint graphs. An algorithm that removes this 
limitation is presented in [36]. Information control direct reference indirect reference Information 
display (a) (b) Figure 1a. Design notation Figure 1b. Model and virtual tool When bound, the tool changes 
its visual appearance to a shape that provides information about its behavior and offers semantic feedback. 
During manipulation, the tool's and the application object's constraint networks remain continuously 
connected, so as to ensure information propagation. Multiple tools can be active simultaneously in the 
same 3D environment in order to control all its aspects. The environment's consistency is continuously 
ensured by the underlying constraint solver. The bi-directionality of the relationships between user-interface 
and application objects makes it possible to use virtual tools to interact with a dynamic environment, 
opening the door to the integration of animation and interaction techniques. 2.3 Animation By recording 
the effects of manipulations, animations can be sketched. In order to be able to edit the captured performance, 
a compact representation of continuous parameter evolution must be obtained. This representation must 
not only precisely approximate the shape of the initial parameter curves but also their timing. The data 
reduction algorithm must therefore treat the geometry and time components simultaneously in order to 
avoid the introduction of errors that would be difficult to control. We have developed an algorithm that 
incrementally builds, from the input sequence, a parametric B-spline preserving value and time of each 
input sample within a given tolerance. It is an incremental version of the Lyche and Mørken algorithm 
[22] that works in parallel with the interactive specification by considering only a small portion of 
the input curve at any time. Latency time and memory requirements for handling each portion of the curve 
are constant. Data reduction may therefore be performed concurrently with interactive parameter input, 
and the responsiveness of the application can be ensured when handling animations defined by any number 
of samples. The algorithm is presented in detail in [2; 4]. This performance-based approach complements 
key-framing by providing the ability to create animations with straight-ahead actions. It provides complete 
control over the animation shape and timing, while key-framing offers control only at a limited number 
of points. The mediation of virtual tools makes it possible to sketch the evolution of non-geometric 
attributes, while constrained or free motion can be specified with 3D devices. Since these devices offer 
continuous control of spatial transformations, subtle synchronizations between position and orientation 
components can be directly specified. In our environment, straight-ahead animations are defined by expressing 
the desire to record parameter evolution during interaction. This is done simply by pressing a different 
mouse button when starting an interaction task. A controller object is connected to each animatable model 
and is responsible for monitoring model state changes. While recording, all changes are handled by the 
controller to feed the animation tracks. Continuous tracks apply the data reduction algorithm to the 
incoming information, while discrete tracks simply store a change value event. During playback, information 
propagates from the animation tracks through the controllers and down to the models. All connections 
are realized by bi-directional constraints. Since playback constraints are weaker than interaction constraints, 
the user can take control over animated models during playback. Animations involving synchronizations 
with the environment's evolution can thus be specified by interacting during playback [5]. Figure 2. 
Interactive animation and playback  3. RELATED WORK 3.1 Constraint-based Architectures Constraint-based 
architectures have long been used for 2D graphics systems (see [28] for a survey). In the 3D graphics 
world, one-way constraints are commonly employed to maintain dependencies between components [20; 34; 
37; 38]. This type of constraint cannot easily model mutual relations between objects, thus hindering 
the tight coupling between user-interface and application objects [28]. Our system uses instead multi-way 
local propagation constraints, which offer support for two-way communication between objects while remaining 
efficient enough to ensure the responsiveness of the system [17; 18; 27]. TBAG [14] also uses multi-way 
constraints maintained by Skyblue [27], but its functional approach concentrates more on modeling time­varying 
behaviors than on creating interactive systems. Much effort has been spent in developing powerful numerical 
solvers for computer graphics (e.g. [7; 15; 16]). This work is complementary to ours, which focuses more 
on providing ways to interact with constrained environments. Such advanced solvers could replace local 
propagation in our system for the maintenance of numerical relationships. 3.2 Three-dimensional User 
Interfaces Much recent research has focused on obtaining rich interaction with 3D environments by means 
of advanced devices and 3D interaction metaphors [8; 10; 11; 13; 16; 19; 26; 30; 32]. 3D widgets or manipulators, 
similar to our virtual tools, are presented in [13; 32]. These works focused on providing support for 
3D widget construction, while we concentrate more on the integration of multiple tools in a single dynamic 
environment. We are not aware of any attempts to apply the results of 3D interaction research to enhance 
animation capabilities. 3.3 Performance Animation A number of authors have proposed using live performances 
to drive computer animations (e.g. [1; 23; 33; 35]). We strive to bring the expressiveness of these approaches 
to general purpose animation systems running on graphics workstations. Instead of relying on advanced 
motion capture devices, we exploit our fully 3D user-interface to control the animated environment at 
a higher level of abstraction. The guiding approach proposed in [23] also seeks to provide better control 
of synthetic objects by raising the abstraction level of user interaction. That work concentrates on 
modeling complex behaviors in a discrete simulation framework, while we focus on providing intuitive 
user interfaces. A major limitation of current performance animation systems is the inability to build 
editable representations out of captured performances [35]. 3.4 Data Reduction Data reduction or curve 
fitting techniques have been successfully applied for the interactive specification of 2D or 3D curves 
or surfaces (e.g. [12; 24; 25; 29]). These techniques cannot be easily adapted to sketching animations 
of multi-dimensional parameters because they all exhibit one or more of the following problems: (i) restriction 
to 2D or 3D geometric constructions, (ii) lack of control on parameterization errors, and (iii) need 
to consider the entire input curve before reduction. An early attempt to use data­reduction for animation 
is described in [29]. In that system, path geometry and path timing specifications were decoupled, loosing 
thus the advantages of performance approaches. Banks and Cohen [6] proposed for their drafting tool an 
incremental version of the Lyche and Mørken algorithm [22] that does not have the aforementioned drawbacks 
and could be used in a performance animation context. Their method shares with ours the idea of processing 
successive portions of the input curve which are then spliced together, but is unable to ensure constant 
latency times and memory needs [4].  4. CONCLUSIONS AND FUTURE WORK In this video-paper, we have presented 
an integrated environment for the rapid and visual prototyping of 3D animated worlds. Using our fully 
3D user-interface, non-professional users can swiftly create complex animations with pose-to-pose and 
straight-ahead techniques. Thanks to automatic data-reduction, animations created by interactive performances 
can then be effectively edited. In our future work, we intend to develop new virtual tools and visualizations 
that will improve our 3D user interface for discrete and continuous track manipulation. To allow the 
system to adhere to timing requirements, we are developing time-critical techniques for controlling rendering 
complexity and constraint evaluation.  ACKNOWLEDGMENTS The authors would like to thank Ronan Boulic 
for providing the walking engine used in the interactive sequences, Sally Kleinfeldt as well as Dean 
Allaman for helpful comments and suggestions, Angelo Mangili for technical help, and Michele Müller for 
doing the voice on the video. This research was conducted by the authors while at the Swiss Federal Institute 
of Technology in Lausanne. REFERENCES [1] Baecker RM (1969) Picture-driven Animation. Proc. Spring Joint 
Computer Conference 34: 273-288. [2] Balaguer JF (1993) Virtual Studio: Un système d'animation en environnement 
virtuel . PhD Thesis, Swiss Federal Institute of Technology in Lausanne. [3] Balaguer JF, Gobbetti E 
(1995) Animating Spaceland. To appear in IEEE Computer Special Isssue on Real-world Virtual Environments 
28(7). [4] Balaguer JF, Gobbetti E (1995) Sketching 3D Animations. To appear in Proc. EUROGRAPHICS. [5] 
Balaguer JF, Gobbetti E (1995) Supporting Interactive Animation using Multi-way Constraints. Submitted 
for publication. [6] Banks M, Cohen E (1990) Real-time Spline Curves from Interactively Sketched Data. 
Proc. SIGGRAPH Symposium on Interactive 3D Graphics: 99-107 [7] Barzel R, Barr A (1988) A Modeling System 
Based on Dynamic Constraints. Proc. SIGGRAPH: 179-188. [8] Bier EA (1990) Snap-Dragging in Three Dimensions. 
Proc. SIGGRAPH Symposium on Interactive 3D Graphics : 193­ 204. [9] Borning A, Freeman-Benson B, Wilson 
M (1992) Constraint Hierarchies. Lisp and Symbolic Computation 5(3): 221-268. [10] Butterworth J, Davidson 
A, Hench S, Olano TM (1992) 3DM: A Three Dimensional Modeler Using a Head-Mounted Display. Proc. SIGGRAPH 
Symposium on Interactive 3D Graphics: 135-138. [11] Card SK, Robertson GG, Mackinlay JD (1991) The Information 
Visualizer: An Information Workspace. Proc. SIGCHI : 181-188. [12] Chou JJ, Piegl LA (1992) Data Reduction 
Using Cubic Rational Splines. IEEE Computer Graphics and Applications 12(3): 60-68. [13] Conner DB, Snibbe 
SS, Herndon KP, Robbins DC, Zeleznik RC, Van Dam A (1992) Three-Dimensional Widgets. SIGGRAPH Symposium 
on Interactive 3D Graphics : 183­ 188. [14] Elliott C, Schechter G, Yeung R, Abi-Ezzi S (1994) TBAG: 
A High Level Framework for Interactive, Animated 3D Graphics Applications. Proc. SIGGRAPH: 421-434. [15] 
Gleicher M (1993) A Graphics Toolkit Based on Differential Constraints. Proc. UIST: 109-120. [16] Gleicher 
M, Witkin A (1992) Through-the-Lens Camera Control. Proc. SIGGRAPH: 331-340. [17] Gobbetti E (1993) Virtuality 
Builder II: Vers une architecture pour l'interaction avec des modes sysnthétiques. PhD Thesis, Swiss 
Federal Institute of Technology in Lausanne. [18] Gobbetti E, Balaguer JF (1993) VB2: A Framework for 
Interaction in Synthetic Worlds. Proc. UIST: 167-178. [19] Herndon KP, van Dam A, Gleicher M (1994) Report: 
Workshop on the Challenges of 3D Interaction, CHI Bulletin, October. [20] Kass M (1992) CONDOR: Constraint-based 
Dataflow. Proc. SIGGRAPH: 321-330. [21] Lasseter J (1987) Principles of Traditional Animation Applied 
to 3D Computer Animation. Proc. SIGGRAPH: 35 ­ 44. [22] Lyche T, Mørken K (1987) Knot Removal for Parametric 
B­spline Curves and Surfaces. Computer Aided Geometric Design 4: 217-230. [23] McKenna M, Pieper S, Zeltzer 
D (1990) Control of a Virtual Actor: The Roach. Proc. SIGGRAPH Symposium on Interactive 3D Graphics: 
165-174. [24] Plass M, Stone M (1983) Curve Fitting with Piecewise Parametric Cubics. Proc. SIGGRAPH: 
229-239. [25] Pudet T (1994) Real Time Fitting of Hand Sketched Pressure Brushstrokes. Proc. EUROGRAPHICS: 
205-220. [26] Sachs E, Roberts A, Stoops D (1990) 3-Draw: A Tool for Designing 3D Shapes. IEEE Computer 
Graphics and Applications 11(6): 18-26. [27] Sannella M (1994) Skyblue: A Multi-Way Local Propagation 
Constraint Solver for User Interface Construction. Proc. UIST : 137-146. [28] Sannella M, Maloney J, 
Freeman-Benson B, Borning A (1992) Multi-way versus One-way Constraints in User-Interfaces. Software 
Practice and Experience 23(5): 529-566. [29] Schneider PJ (1988) Phoenix: An Interactive Curve Design 
System Based on the Automatic Fitting of Hand-Sketched Curves. Master's Thesis, University of Washington. 
[30] Shaw C, Green M (1994) Two-Handed Polygonal Surface Design. Proc. UIST: 212-215. [31] Shelley KL, 
Greenberg DP (1982) Path Specification and Path Coherence. Proc. SIGGRAPH: 157-166. [32] Strauss PS, 
Carey R (1992) An Object-Oriented 3D Graphics Toolkit. Proc. SIGGRAPH: 341-347. [33] Tice S (1993) VActor 
Animation Creation System. SIGGRAPH Tutorial 1. [34] Upson C, Fulhauber T, Kamins D, Laidlaw D, Schlegel 
D, Vroom J, Gurwitz R, van Dam A (1989) The Application Visualization System: A Computational Environment 
for Scientific Visualization. IEEE CG&#38;A 9(4): 30-42. [35] Walters G (1993) Performance Animation 
at PDI. SIGGRAPH Tutorial 1. [36] Vander Zanden B (1995) An Incremental Algorithm for Satisfying Hierarchies 
of Multi-way, Dataflow Constraints. Technical Report, University of Tennessee, Knoxville. [37] Zeleznik 
RC, Conner DB, Wlocka MM, Aliaga DG, Wang NT, Hubbard PM, Knepp B, Kaufman H, Hughes JF, van Dam A (1991) 
An Object-Oriented Framework for the Integration of Interactive Animation Techniques. Proc. SIGGRAPH: 
105-112. [38] Zeltzer D, Pieper S, Sturman DJ (1989) An Integrated Graphical Simulation Platform. Proc. 
Graphics Interface: 266-274.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218495</article_id>
		<sort_key>399</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Navigation and locomotion in virtual worlds via flight into hand-held miniatures]]></title>
		<page_from>399</page_from>
		<page_to>400</page_to>
		<doi_number>10.1145/218380.218495</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218495</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39045485</person_id>
				<author_profile_id><![CDATA[81100493478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Randy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pausch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia, Thornton Hall, University of Virginia, Charlottesville, VA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P284001</person_id>
				<author_profile_id><![CDATA[81100061772]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tommy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burnette]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia, Thornton Hall, University of Virginia, Charlottesville, VA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P57954</person_id>
				<author_profile_id><![CDATA[81100538169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brockway]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Science Applications International Corporation, 4301 N. Fairfax Drive, Suite 200, Arlington, VA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P199237</person_id>
				<author_profile_id><![CDATA[81100381197]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Weiblen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Science Applications International Corporation, 4301 N. Fairfax Drive, Suite 200, Arlington, VA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>223938</ref_obj_id>
				<ref_obj_pid>223904</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Stoakley, M. Conway, R. Pausch Virtual Reality on a WIM, Proceedings of the ACM SIGCHI Human Factors in Computer Systems Conference, May, 1995, (to appear).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>319127</ref_obj_id>
				<ref_obj_pid>319120</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. Fisher, M. McGreevy, J. Humphries, W. Robinett, Virtual Environment Display System, Proceedings on the 1986 Workshop on Interactive 3D Graphics, October, 1986, pages 77-87.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Furness, Configuring Virtual Space for the Super Cockpit, Human Interface Technology (HIT) Laboratory of the Washington Technology Center, Technical Report HITL-M- 89-1, 1989.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37407</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Lasseter, Principles of Traditional Animation Applied to 3D Computer Animation, Proceedings of SIGGRAPH "87, July 1987, pages 35-44.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108874</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[S. Card, G. Robertson, J Mackinlay, The Information Visualize~, an Information Workspace, Proceedings of the ACM SIGCHI Human Factors in Computer Systems Conference, May, 1991, pages 181-188.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617763</ref_obj_id>
				<ref_obj_pid>616024</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[S. Bryson, C. Levit, The Virtual Wind Tunnel, IEEE Computer Graphics and Applications, pages 25-34, July 1992]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218496</article_id>
		<sort_key>401</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>50</seq_no>
		<title><![CDATA[A frequency-domain analysis of head-motion prediction]]></title>
		<page_from>401</page_from>
		<page_to>408</page_to>
		<doi_number>10.1145/218380.218496</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218496</url>
		<keywords>
			<kw><![CDATA[HMD]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[delay compensation]]></kw>
			<kw><![CDATA[spectral analysis]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.4</cat_node>
				<descriptor>Kalman filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003683</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Kalman filters and hidden Markov models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14044783</person_id>
				<author_profile_id><![CDATA[81100099945]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Azuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hughes Research Laboratories, 3011 Malibu Canyon Road MS RL96; Malibu, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14081142</person_id>
				<author_profile_id><![CDATA[81100206034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bishop]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill, CB 3175 Sitterson Hall; Chapel Hill, NC]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>241384</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Azuma, Ronald. Predictive Tracking for Augmented Reality. Ph.D. dissertation. UNC Chapel Hill Department of Computer Science technical report TR95-007 (February 1995).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192199</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Azuma, Ronald, and Gary Bishop. Improving Static and Dynamic Registration in an Optical See-Through HMD. Proceedings of SIGGRAPH '94 (Orlando, FL, 24-29 August 1994), 197-204.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brown, Robert Grover, and Patrick Y.C. Hwang. Introduction to Random Signal and Applied Kalman Filtering, 2nd edition. John Wiley &amp; Sons. (1992).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134039</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Deering, Michael. High Resolution Virtual Reality. Proceedings of SIGGRAPH '92 (Chicago, IL, 26-31 July 1992), 195-202.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Emura, Satoru and Susumu Tachi. Compensation of Time Lag Between Actual and Virtual Spaces by Multi-Sensor Integration. Proceedings of the 1994 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (Las Vegas, NV, 2-5 October 1994), 463- 469.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Harris, Frederic J. On the Use of Windows for Harmonic Analysis with the Discrete Fourier Transform. Proceedings of the IEEE 66, 1 (January 1978), 51-83.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Jenkins, Gwilym M. and Donald G. Watts. Spectral Analysis and its Applications. Holden-Day. (1968).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Lawton, W., T. Poston and L. Serra. Calibration and Coordination in a Medical Virtual Workbench. Proceedings of Virtual Reality Applications (Leeds, UK, 7-9 June 1994).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lewis, Frank L. Optimal Estimation. John Wiley &amp; Sons, 1986.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>120784</ref_obj_id>
				<ref_obj_pid>120782</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Liang, Jiandong, Chris Shaw, and Mark Green. On Temporal-Spatial Realism in the Virtual Reality Environment. Proceedings of the 4th Annual ACM Symposium on User Intelface Software &amp; Technology (Hilton Head, SC, 11-13 November 1991), 19-25.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[List, Uwe H. Nonlinear Prediction of Head Movements for Helmet-Mounted Displays. Technical report AFHRL-TP-83- 45 {AD-A136590}, Williams AFB, AZ: Operations Training Division (1984).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Oppenheim, Alan V. and Alan S. Willsky. Signals and Systems. Prentice-Hall Inc. (1983).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Paley, W. Bradford. Head-Tracking Stereo Display: Experiments and Applications. SPIE Vol. 1669 Stereoscopic Displays and Applications III (San Jose, CA, 12-13 February 1992), 84-89.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>68411</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Phillips, Charles L., and H. Troy Nagle. Digital Control System Analysis and Design, 2nd edition. Prentice-Hall, Inc. (1990).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Press, William H., Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. Numerical Recipes in C. Cambridge University Press (1988).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Priestley, M.B. Spectral Analysis and Time Series, Vol. 1. Academic Press ( 1981 ).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Rebo, Robert. A Helmet-Mounted Virtual Environment Display System. MS Thesis, Air Force Institute of Technology (December 1988).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Riner, Bruce and Blair Browder. Design Guidelines for a Carrier-Based Training System. Proceedings of IMAGE VI (Scottsdale, AZ, 14-17 July 1992), 65-73.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[So, Richard H. Y. and Michael J. Griffin. Compensating Lags in Head-Coupled Displays Using Head Position Prediction and Image Deflection. Journal of Aircraft 29, 6 (November-December 1992), 1064-1068.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Zikan, Karel, W. Dan Curtis, Henry A. Sowizral, and Adam L. Janin. A Note on Dynamics of Human Head Motions and on Predictive Filtering of Head-Set Orientations. SPIE Proceedings volume 2351: Telemanipulator and Telepresence Technologies (Boston, MA, 31 October - 4 November 1994).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218497</article_id>
		<sort_key>409</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>51</seq_no>
		<title><![CDATA[A frequency based ray tracer]]></title>
		<page_from>409</page_from>
		<page_to>418</page_to>
		<doi_number>10.1145/218380.218497</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218497</url>
		<keywords>
			<kw><![CDATA[DCT]]></kw>
			<kw><![CDATA[JPEG]]></kw>
			<kw><![CDATA[Monte Carlo]]></kw>
			<kw><![CDATA[adaptive sampling]]></kw>
			<kw><![CDATA[color]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[reconstruction]]></kw>
			<kw><![CDATA[visual perception]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010395</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image compression</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010254</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Reconstruction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39069629</person_id>
				<author_profile_id><![CDATA[81339490696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Bolin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer and Information Science, University of Oregon, Eugene, OR]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42053997</person_id>
				<author_profile_id><![CDATA[81339516843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gary]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Meyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer and Information Science, University of Oregon, Eugene, OR]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Albert, A. E. and Gardner Jr., L. A., Stochastic Approximation and Nonlinear Regression, M.I.T. Press: Massachusetts, 1967.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bedford, R. E. and Wyszecki, G. "Axial Chromatic Aberration of the Human Eye," Journal of the Optical Society of America, Vol. 47, No. 6, pp. 564-565, 1957.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bodewig, E., Matrix Calculus, North Holland Publishing Company: Amsterdam, 1956.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Buchsbaum, G. and Gottschalk, A. "Trichromacy, Opponent Colours Coding and Optimum Colour Information Transmission in the Retina," Proceedings of the Royal Society of London, Series B, Vol. 220, pp. 89-113, 1983.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Buchsbaum, G. "Color Signal Coding: Color Vision and Color Television," Color Research and Application, Vol. 12, No. 5, pp. 266-269, 1987.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chiu, K., Heft, M., Shirley, E, Swamy, S., Wang, C., and Zimmerman, K., "Spatially Non-Uniform Scaling Functions for High Contrast Images," Proceedings of Graphics Interface 1993, pp. 245-253, 1993.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cicerone, C. M. and Nerger, J. L., "The Relative Numbers of Long- Wavelength-Sensitive to Middle-Wavelength-Sensitive Cones in the Human Fovea Centralis," Vision Research, Vol. 29, No. 1, pp. 115-128, 1989.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[DeValois, R. L., Smith, C. J., Karoly, A. J., and Kitai, S. T., "Electrical Responses of Primate Visual System, I. Different Layers of Macaque Lateral Geniculate Nucleus," Journal of Comparative and Physiological Psychology, Vol. 51, pp. 662-668, 1958.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Estevez, O., "On the Fundamental Data-base of Normal and Dichromatic Color Vision," Ph.D. Thesis, University of Amsterdam, Krips Repro Meppel, Amsterdam 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Faugeras, O. D., "Digital Color Image Processing Within the Framework of a Human Visual Model," IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 27, No. 4, pp. 380-393, 1979.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192202</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gondek, J. S., Meyer, G. W., and Newman, J. G., "Wavelength Dependent Reflectance Functions," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 213-220, 1994.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166155</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Guenter, B. K., Yun, H. C., and Mersereau, R. M., "Motion Compensated Compression of Computer Animation Frames," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 297-304, 1994.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Guth, S. L., Massof, R. W., and Benzschawel, T., "Vector Model for Normal and Dichromatic Color Vision," J. Opt. Soc. Am., Vol. 70, pp. 197-211, 1980.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hamming, R. W., Digital Filters, Prentice-Hall: New Jersey, 1977.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hurvich, L. M. and Jameson, D., "Some Quantitative Aspects of an Opponent-Colors Theory, II. Brightness, Saturation, and Hue in Normal and Dichromatic Vision," Journal of the Optical Society of America, Vol. 45, pp. 602-616, 1955.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ingling, C. R., "The Spectral Sensitivity of the Opponent-Colors Channels," Vision Research, Vol. 17 pp. 1083-1090, 1977.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Jameson, D. and Hurvich, L. M., "Some Quantitative Aspects of an Opponent-Colors Theory, I. Chromatic Responses and Spectral Saturation," Journal of the Optical Society of America, Vol. 45, pp. 546-552, 1955.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122735</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Kirk, D. and Arvo, J., "Unbiased Sampling Techniques for Image Synthesis," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 153-156, 1991.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617557</ref_obj_id>
				<ref_obj_pid>616011</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Lee, M. E., and Redner, R. A., "A Note on the Use of Nonlinear Filtering in Computer Graphics," IEEE Computer Graphics and Applications, Vol. 10, No. 3, pp. 23-29, 1990.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>45601</ref_obj_id>
				<ref_obj_pid>45596</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Meyer, Gary W., "Wavelength Selection for Synthetic Image Generation," Computer Vision, Graphics, and Image Processing, Vol. 41, pp. 57-79, 1988.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Meyer, G. W., and Liu, A., "Color Spatial Acuity Control of a Screen Subdivision Image Synthesis Algorithm," Human Vision, Visual Processing, and Digital Display III, Bernice E. Rogowitz, Editor, Proc. SPIE 1666, pp. 387-399, 1992.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37410</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. P., "Generating Antialiased Images at Low Sampling Densities," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 65-72, 1987.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Mullen, K. T., "The Contrast Sensitivity of Human Colour Vision to Red-Green and Blue-Yellow Chromatic Gratings," J. Physiol. (Lond.), Vol. 359, pp. 381-400, 1985.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74362</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Painter, J. and Sloan, K. "Antialiased Ray Tracing by Adaptive Progressive Refinement," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 281-288, 1989.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Pearson, D. E., Transmission and Display of Pictorial Information, John Wiley and Sons, 1975.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Poirson, A. B. and Wandell, B. A. "Pattern-Color Separable Pathways Predict Sensitivity to Simple Colored Patterns," to appear in Vision Research, 1996.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Powell, M. J. D., "A Theorem on Rank One Modifications to a Matrix and its Inverse," Computer Journal, Vol. 12, pp. 288-290, 1969.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>130597</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K., Digital Image Processing, Second Edition, John Wiley and Sons, 1991.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_obj_id>96810</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Rao, K. R. and Yip, R, Discrete Cosine Transform, Academic Press: Boston, 1990.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192189</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Rushmeier, H. E. and Ward, G. J., "Energy Preserving Non-Linear Filters," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 131-138, 1994.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Stark, P. A., Introduction to Numerical Methods, Macmillan Publishing Co.: New York, 1970.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617873</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Tumblin J. and Rushmeier, H., "Tone Reproduction for Realistic Images," IEEE Computer Graphics and Applications, pp. 42- 48, 1993.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192286</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Ward, G., "The RADIANCE Lighting Simulation and Rendering System," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 459-472, 1994.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Ward, G., Rubinstein, E M., and Clear, R. D., "A Ray Tracing Solution for Diffuse Interreflection," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 85-92, 1988.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_obj_id>103089</ref_obj_id>
				<ref_obj_pid>103085</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Wallace, G. K., "The JPEG Still Picture Compression Standard," Communications of the ACM, Vol. 34, No. 4, pp. 30-44, 1991.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192198</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Wallach, D. S., Kunapalli, S., and Cohen, M. E, "Accelerated MPEG Compression of Dynamic Polygonal Scenes," Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 193-196, 1994.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Williams, D. R., MacLeod, D. I. A., and Hayhoe, M. M., "Punctuate Sensitivity of the Blue-Sensitive Mechanism," Vision Research, Vol. 21, pp. 1357-1375, 1981.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Frequency Based Ray Tracer Mark R. Bolin Gary W. Meyer Department of Computer and Information Science 
University of Oregon Eugene, OR 97403  Abstract A ray tracer has been developed that synthesizes images 
directly into the frequency domain. This makes it possible to use a simple vision model to control where 
rays are cast into a scene and to decide how rays should be spawned once an object is intersected. In 
this manner the most visible artifacts can be removed .rst and noise can be channeled into those areas 
of an image where it is least noticeable. The resulting image is produced in a format that is consistent 
with many image compression and transmission schemes. CR Categories and Subject Descriptors: I.3.3 [Computer 
Graphics]: Picture/Image Generation; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; 
I.4.2 [Image Process­ing]: Compression (coding). Additional Key Words and Phrases: visual perception, 
color, JPEG, DCT, Monte Carlo, ray tracing, adaptive sampling, recon­struction 1 Introduction The .elds 
of image science and image processing have long sought to exploit certain characteristics of the human 
visual sys­tem in the image representation schemes that they employ. The researchers in these .elds have 
realized that there are limits to the sensitivity of the visual system in the intensity, spatial, and 
temporal domains. This has allowed them, among other things, to minimize the effects of noise in a picture 
and to decrease the amount of memory necessary to store an image. With only a few exceptions, the .eld 
of computer graphics has yet to exploit the limitations of the visual system in the realistic imaging 
algorithms that have been developed. In the meantime, the image compres­sion and transmission techniques 
created by image scientists and image processors are routinely being applied after the fact to com­puter 
graphic pictures. This results in wasted effort as super.uous pictorial information is .rst synthesized, 
and then eliminated once the picture has been completed. In this paper we present an image synthesis 
algorithm that takes advantage of three characteristics of the human visual system that are commonly 
exploited in image representation schemes: 1 e-mail: mbolin j gary@cs.uoregon.edu Permission to make 
digital/hard copy of part or all of this work for personal or classroom use is granted without fee provided 
that copies are not made or distributed for profit or commercial advantage, the copyright notice, the 
title of the publication and its date appear, and notice is given that copying is by permission of ACM, 
Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior 
specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 contrast sensitivity, 
spatial frequency response, and masking. It is well established that the contrast sensitivity response 
of the eye is nonlinear. This is shown in Figure 1 where it can be seen that an equal increment of light 
is not equally different from two backgrounds at two different intensities. This fact is used in image 
representation schemes to minimize the visibility of noise. The spatial frequency response of the human 
visual system is known to be less for patterns of pure color than for patterns that include luminance 
differences. This can be seen in Figure 2 where it is easier to resolve the leftmost achromatic wedge 
than either of the two color wedges. Image representation schemes make use of this fact to decrease the 
bandwidth used to transmit pictures. Finally, it is known that high spatial frequency detail in the .eld 
of view can mask the presence of other high frequency information. This is illustrated in Figure 3 where 
the quantization noise is more visible in the image of the low frequency mountain than in the   Figure 
3: Visibility of noise when images with different spatial frequency content (top row) are quantized to 
4 bits (bottom row). picture of the mountain with high frequency detail. This fact is used in image compression 
algorithms to decrease the amount of memory necessary to store an image. The ray tracing algorithm that 
will be presented in this paper is able to exploit the above characteristics of the visual system by 
synthesizing an image directly into the frequency domain. This makes it possible to decide how the synthesis 
should proceed by applying a simple vision model that has been developed by the image processing community. 
It also permits us to see the fre­quency representation for the image as it evolves and to use this information 
to control the sampling that is done to create the pic­ture. This allows the algorithm to resolve the 
most apparent visual artifacts before those that are less visible. The iterative frequency representation 
that is employed is also able to interpolate between low density samples in order to provide the user 
with a useful early representation of the image as the ray tracing progresses. To permit physically correct 
illumination calculations a Monte Carlo ray tracer is employed. This makes it possible to simu­late global 
illumination effects, such as diffuse to diffuse inter­re.ections, and local illumination effects that 
require the use of bidirectional re.ection distribution functions (BRDF s). We will show how the interpolation 
properties of our approach allow us to take relatively few samples with high ray spawning to model low 
frequency features. Conversely, the quantization property of our method permits us to spawn few rays 
when many samples are necessary to capture high frequency detail. Finally, the frequency domain representation 
into which the image is synthesized has many similarities to the JPEG image compression scheme that is 
now widely used to store pictures. This has the practical advantage of decreasing the number of steps 
necessary to JPEG compress the .nal image. Including this introduction, the paper is divided into seven 
major sections. In the second section we review related previ­ous work in computer graphics. In the third 
and fourth sections we provide background information regarding vision models de­veloped in the image 
processing .eld and discrete cosine based image compression schemes. The algorithm for synthesizing im­ages 
directly into the frequency domain is presented in the .fth section followed by the sixth section which 
shows how the algo­rithm performs on several test cases. The paper concludes with a summary and conclusions. 
 2 Previous Work Some attempts have already been made to incorporate what is known about the human visual 
system into the image synthe­sis process. The most successful examples of this work are the tone reproduction 
operators that have been developed by Tumblin and Rushmeier [32] and by Ward [33]. In both of these cases, 
a mapping function is provided between the radiances that are com­puted by the image synthesis algorithm 
and the light energy that is emitted from the cathode ray tube. These tone operators take into account 
the adaptive state of both the observer in the syn­thetic scene and the observer of the cathode ray tube. 
A spatially nonuniform mapping function has also been recently developed [6]. The limited color spatial 
frequency response of the human visual system was exploited in an image synthesis algorithm de­veloped 
by Meyer and Liu [21]. In their work an adaptive spatial subdivision method [24] was used to cast more 
rays at luminance differences than at color differences. While synthesis directly into the frequency 
domain has not been attempted, there has been work done on the use of nonlinear .lters for image sampling. 
The most closely related research to that presented in this paper is that of Mitchell [22]. He used multi­stage 
.lters to interpolate samples that were taken nonuniformly and at low density. To decide where these 
samples should be acquired, he used a contrast metric in RGB space and applied a differential weighting 
for each of the R, G, and B channels. While this work has the elements of a perceptual metric it is at 
best a crude model for the visual system. It also does not take into account the effect of masking and 
only provides for two levels of adaptivity in sampling. Other work with nonlinear .lters includes the 
alpha trimmed .lters of Lee and Redner [19] and the energy preserving nonlinear .lters of Rushmeier and 
Ward [30]. Although synthesis of static images directly into a compressed format has not been attempted, 
information produced during the generation of animated sequences has been used to assist in the compression 
of these frames. Guenter et al. [12] has employed transformation information in the animation script 
to perform mo­tion prediction computations. They also made use of identi.cation information that was 
stored at each pixel to assist in this calcula­tion. Wallach, Kunapalli, and Cohen [36] use workstation 
hard­ware to compute the optical .ow for a scene. This information is then used in the MPEG compression 
of the generated sequence. 3 Image processing vision models A simple three stage model of human vision 
has been devel­oped by the digital image processing community. The model has been employed by image processors 
to solve a number of differ­ent image compression and transmission problems. It is similar, in many respects, 
to the NTSC encoding scheme that has been used for many years to broadcast commercial television programs. 
Be­cause of the success which it has already enjoyed, we feel that this model is a good starting point 
from which to develop a percep­tually based image synthesis technique. The model consists of a receptor 
.rst stage with logarithmic response to light, followed by a matrix to an opponent color space representation, 
and, .nally, spatial frequency .ltering which is applied to each of the color channels. There are a couple 
of different variations of the model that have been developed [28]. Our version, which is presented below, 
is most closely related to the model proposed by Faugeras [10]. It employs color spaces and spatial .lters 
that were also used in [21]. The cones in the retina of the eye form the .rst stage of the model. The 
spectral sensitivities of the cones can be found from a linear transformation of the CIE XY Z color matching 
functions. Using the dichromatic confusion points suggested by Estevez [9] this transformation from CIE 
XY Z to SML space becomes [20]: S 0.0000 0.0000 0.5609 X M -0.4227 1.1723 0.0911 Y (1) L0.1150 0.9364 
-0.0203Z Figure 4 shows the short (S), medium (M), and long (L) wave­length spectral sensitivity functions 
that result. In the image processing vision models that have been devel­oped, the contrast sensitivity 
of the eye is typically modeled as being a logarithmic response to light. This nonlinearity can be approximated 
by the use of a power law function [25]. We have chosen to do this for each of the fundamental sensitivity 
functions by using a power law with exponent of 2.2. In making this de­cision we assume that we are working 
within the dynamic range of a television monitor. (This does not preclude the use of more complicated 
tone reproduction operators [32,33] which could be applied to an initial low quality version of the image 
or to the key frames of an animated sequence in order to establish the mapping between the dynamic range 
of the environment and the dynamic range of the reproduction device.) We also acknowledge that this introduces 
a nonlinearity into the color matrixing and .ltering that is done beyond this point. We accept this distortion 
because we are attempting to apply an existing model that has been used with some success. We also note 
that this same distortion is found in Figure 4: Spectral sensitivities for S (dashed), M (solid), and 
L (dot-dashed) components of SML space.  C Figure 5: Spectral sensitivities for A (solid), C1 (dot-dashed), 
and 2 (dashed) components of AC1 C2 space. practical image transmission systems such as the NTSC encoding 
standard (see below). In the second stage of the vision model, the cone signals are matrixed into an 
opponent representation. There is now abundant evidence from neurophysiological [8], psychophysical [15,17], 
and theoretical [4] studies that the SML signals produced by the cones undergo this type of transformation. 
Several different oppo­nent representations have been proposed [13,16]. We have chosen to use a transformation 
developed by Buchsbaum and Gottschalk that is based upon the application of the discrete Karhunen-Loeve 
expansion to the SML fundamentals. This produces a represen­tation that is optimal from the perspective 
of statistical commu­nication theory and that is consistent with other opponent spaces that have been 
derived psychophysically. When this transforma­tion is applied to the fundamentals de.ned above, the 
following opponent color space is de.ned: C S0 1 0.0018 2.9468 -2.5336 M0 (2) C21.0111 -0.3877 0.2670 
A 0.0001 0.2499 0.7647 L0 where S0 , M0 , and L0 represent the S, M, and L signals respec­tively after 
application of the power law transformation. Figure 5 shows the achromatic (A) and chromatic (C1 and 
C2 ) color chan­nels that result (without the power law applied). This opponent representation has been 
used to select the wavelengths at which to perform synthetic image generation [20]. The third and .nal 
stage of the model applies a spatial .lter Figure 6: Achromatic versus chromatic (either red-green or 
yellow-blue) spatial frequency response (after Mullen, 1985). to the achromatic and chromatic channels. 
As was mentioned above, the visual system is more sensitive to black and white spatial detail than it 
is to color spatial detail. This is partly the result of strong axial chromatic aberration in the eye 
[2] that causes shorter wavelengths of light to focus in front of the retina. The relative densities 
of the SML receptors also contributes to the difference in chromatic spatial frequency response. The 
ratio of L to M receptors has been shown to be approximately two to one [7] and Williams, MacLeod, and 
Hayhoe [37] have found that the spacing of the S receptors is very sparse (approximately one in every 
10 minutes of arc). The combination of chromatic aberration and receptor spacing produces differences 
in the spatial frequency bandpass of the achromatic and chromatic channels. Mullen [23] has shown that 
cutoff for the luminance channel is 34 cycles/degree and for the red/green and yellow/blue channels it 
is 11 cycles/degree (Figure 6). Recent work by Poirson and Wandell [26] incorporates the effects of chromatic 
aberration and indicates that the cutoff for the yellow/blue channel may be as low as 4 cycles/degree. 
There is good correspondence between the dimensions of the AC1C2 system and the color space directions 
employed by Mullen and by Poirson and Wandell. While this model has been developed by the image processing 
community from a .rst principles analysis of the human visual sys­tem, it has many similarities to the 
image representation scheme that is used in NTSC television encoding. The .rst stage of the NTSC system 
consists of the red, green, and blue taking sensitiv­ities of the color television camera. These functions 
are different than the SML sensitivities used above, but they are colorimetri­caly related to them. Gamma 
correction for the display monitor is also done as part of the .rst stage of the NTSC system. Because 
there is a power law relationship between the voltage applied to the display primaries and the light 
emitted from the display screen, this gamma correction step is identical to the contrast sensitivity 
adjustment in the vision model. The second stage of the NTSC system consists of a matrixing of the camera 
signals into YIQ color space. This color representation scheme has been shown [5] to be very similar 
to the opponent representation that is used above. In the third and .nal stage of the NTSC system, the 
signals are band limited to 4.2 MHz (Y ),1.6MHz (I),and .6 MHz (Q). It is interesting to note that the 
ratio between these bandwidths is similar to the ratio between the spatial frequency cutoffs used in 
the vision model. 4 DCT based compression algorithms The Discrete Cosine Transform (DCT) is an integral 
part of several image compression schemes. The DCT works by decom­posing the original image into a series 
of harmonic cosine waves in a manner similar to the Discrete Fourier Transform (DFT). The original image 
can then be exactly reproduced by the sum of these cosine waves. The DCT provides a formula to determine 
the co­ef.cients of these harmonically related cosine waves which sum to reproduce the original image 
[29]. M -1M-1  1 XX F(u,v) ,(u),(v) f(x,y)Cx,uCy,v (3) 4 x=0 y=0 M -1M-1  1 XX f(x,y) ,(u),(v)F(u,v)Cx,uCy,v 
(4) 4 u=0 v=0 where CA,B cos [(2A +1)B16 7 ] and ,(u),,(v) 1/p2 for u,v 0 ,(u),,(v) 1 otherwise This 
transform also has the nice property that an M by M image may be exactly reproduced by M by M harmonically 
related cosine waves multiplied by the coef.cients given by the transform. The image may therefore be 
completely represented by an M by M array of coef.cients, the majority of which will tend to zero in 
the higher frequencies. JPEG is a compression scheme that is based upon the DCT. JPEG .rst breaks the 
image into 8 by 8 pixel blocks. The values of the 8 by 8 block are then transformed into the frequency 
domain by means of the DCT. The new values take the form of an 8 by 8 block of frequency coef.cients 
where each value is the amplitude of a harmonic cosine wave. These values are organized with the zero 
frequency (DC) value in the upper left corner of the block and increase in frequency to the highest frequency 
(AC) values in the lower right corner. These coef.cients are then quantized by means of a scalar quantization 
table speci.ed by the application. This step allows the algorithm to take advantage of masking and achieve 
further compression by discarding information that is not visually signi.cant. This is accomplished by 
assigning less bits to the representation of the higher frequency elements. In the JPEG scheme, additional 
encoding is done to the fre­quency coef.cients. The DC values are handled separately from the AC values. 
The DC values are encoded as the difference from the DC term in the previous 8 by 8 block. This is known 
as Dif­ferential Pulse Code Modulation (DPCM). This is done because of the strong correlation between 
the average color of adjacent 8 by 8 blocks. The AC values are then read in zigzag order going from the 
upper left hand corner to the lower right. This helps to facilitate the following entropy encoding step, 
by placing the low frequency coef.cients, which are more likely to be non-zero before the high frequency 
coef.cients, which are more likely to be zero. This output is then encoded using either Huffman or Arithmetic 
encoding [35]. 5 Synthesis into the Frequency Domain We have chosen the initial frequency representation 
scheme that is used in JPEG as the format into which we synthesize our images. At this stage of the JPEG 
compression process, the image has been divided into 8 by 8 pixel blocks, the DCT has been ap­plied to 
each of the blocks, and quantization has been performed on each of the terms in the resulting 8 by 8 
array of frequency coef.cients. We do not take account of the DPCM or Huffman Figure 7: Ordering metric 
used to drive the selection of samples within the quad-tree subdivision method. encoding that is applied, 
in the JPEG approach, to this initial representation. However, as we construct the frequency represen­tation, 
we do take advantage of the zigzag order in which the frequency coef.cients are read out of the 8 by 
8 array to be en­coded. We represent the color for each block using the AC1 C 2 color space described 
in the previous section. Our algorithm seeks to .nd the frequency representation for the A, C1 , and 
Ccomponents of each 8 by 8 pixel block in 2 the image. The order in which the blocks are processed is 
deter­mined by a list that is sorted according to the level of re.nement required by each block. The 
.rst block on the list is the next block that is worked on, and a special sampling scheme is used to 
determine where to cast new rays in this block. Once the samples have been taken, a least squares .t 
is used to .nd the cosine terms which interpolate the data. As more samples are generated in each block, 
additional frequency terms are added to the solution. To take advantage of masking, the precision with 
which the illumi­nation model is evaluated decreases as higher frequency terms are included. The vision 
model described in the previous section is used to decide where to put the updated block back on the 
list of sorted blocks. When the desired level of re.nement is achieved and the block is displayed, low 
pass .ltering is used to minimize any aliasing that might remain. The remainder of this section provides 
additional details re­garding each of the steps in this algorithm. 5.1 Deciding Where to Take the Next 
Sample After selecting the next block to be processed from the list of sorted blocks, a decision must 
be made about where to take the next sample within this block. The same sampling sequence is chosen for 
all blocks in order to minimize the expense of comput­ing the cosine terms (see Section 5.2). The samples 
taken within each block are spread as wide as possible to enhance the speed and quality of the reconstructed 
function as the algorithm progresses. Spreading the samples out allows for the most accurate calcula­tion 
of the cosine terms, provides even coverage of the sample space, and constrains the interpolation that 
is done between sam­ples. The regular sampling pattern that results can give rise to bias [18] but we 
feel that this sampling approach is warranted due to the computational savings that it provides in our 
case. The sampling positions within a block are chosen based on a quadtree subdivision method. The .rst 
four samples in the sequence are taken through the outermost four corners of the 8 by 8 block in the 
order given in Figure 7. The .fth sample, which marks the beginning of the repeated portion of the sequence, 
is taken through the center of the block. A random choice is then made between positions A and B to determine 
the sixth sample position. This step is necessary to evenly distribute samples in both the horizontal 
and vertical directions. In the general case (which, as we will see below, is not the situation for this 
.rst block), the unselected A or B is chosen next, and the block is subdivided into sub-blocks o, j, 
x, and 8 which form the next four leaves on the quadtree. The sequence then repeats with a breadth .rst 
search of the quadtree at the new level to take a sample at the center of each sub-block. This is followed 
by another breadth .rst search and a random sample at position A or B for each sub-block. After samples 
are taken at the unselected A or B for each sub-block, the sub-block subdivides and another new level 
of the quadtree is started. An exception to the repeated portion of this sequence occurs when either 
position A or B lies at the boundary of the 8 by 8 block at the top of the quadtree. In this case, position 
A (see Figure 7) immediately follows position A and position B immediately follows B. This means that 
all A and B positions will lie at the bottom-most and left-most edges respectively of the 8 by 8 block. 
In the case of the .rst block we see that the actual sampling sequence, for B chosen .rst, would be 1, 
2, 3, 4, 5, B, B , A, and A . This method generates a list of sample positions which is shared and indexed 
into at varying positions by all blocks. The .rst block to exceed the number of samples contained within 
the list will calculate the position of the next sample. Otherwise the sample position is merely obtained 
from the current position within the list. Note should be taken that samples which border the 8 by 8 
block may be used to re.ne the frequency representa­tion of all blocks that they border. A sample cache 
is therefore employed at each block, to store bordering AC1 Cvalues re­ 2 turned by surrounding blocks. 
This is necessary since a strict ordering on samples has been imposed. To avoid casting redun­dant rays, 
these values are then used by the block when it reaches the corresponding position in its sampling sequence. 
 5.2 Computing the frequency representation From the samples that have been taken in the block that is 
currently being re.ned, a least squares .t is used to determine the cosine terms that best interpolate 
the data. From Equation 4 we see that a block may be reconstructed from N cosine terms by the following 
equation N-1 X f(x,y) G(ui,vi)Cx,uCy,v (5) i=0 where the subscript iimposes a zig-zag ordering on the 
frequency terms u and v given by ui and vi and 1 G(ui,vi) ,(ui ),(vi )F(ui ,vi) (6) 4 The least squares 
solution for S monochromatic samples, taken at positions (xs,ys) within the block, is given when the 
following equation is at a minimum S X [f(xs,ys) -o(xs,ys)]2 (7) s=0 where o(xs,ys) represents the sampled 
value for either the A, C1 or C2 color channel. The formula that solves this least squares problem is 
given by the following equation [X] [J] -1[Y] (8) where X and Y form Nx1 column vectors, J is a NxN matrix, 
and S X Jij Cxs,ui Cys,vi Cxs,uj Cys,vj s=0 S X Yi o(xs,ys)Cxs,ui Cys,vi s=0 Xi G(ui,vi) The actual frequency 
coef.cients may then be obtained by invert­ing Equation 6. This solution requires the inversion ofaNbyN 
matrix to be computed for each sampling position, where N is the number of frequency terms currently 
necessary to represent the portion of the image within the block. However, this operation is not as bad 
as it initially appears. Since this is an iterative solution we are able to reduce the time necessary 
to invert this matrix to O(N2 ) by using a rank one inverse matrix modi.cation technique [27] which is 
tuned to suppress the accumulation of errors. In this technique )-1 the inverse matrix (J0 which is used 
to solve the least squares problem for (S +1) samples, may be obtained from the inverse matrix (J)-1 
which solves the problem for S samples, by the following equation T (J0 ) -1 (J +AA) -1 (9) J-1(A -J-1 
,)AT J -1 + (10) (AT J-1 ,) where A forms a N x 1 column vector and T ,A(AA)+ JA Ai CxS+1,ui CyS+1,vi 
C Since the matrix is dependent on only the sampling positions and not the sampled value, and, since 
we chose to use the same sam­pling sequence for all blocks, this matrix does not need to be inverted 
independently for all blocks. Instead a shared matrix inverse list is associated with a speci.c sampling 
position and or­dering in the sampling sequence. This list is grown dynamically with the next matrix 
inverse being computed for the .rst block to reach the new sampling position. This inverse matrix may 
then be shared by all blocks that follow. The computation of the frequency description solution vector 
is then simply the result of .nding the current sampling position from the sample list, sam­pling the 
image, and performing a matrix multiplication on the associated inverse matrix from the list for each 
of the A, C1 and 2 color channels. 5.3 Adding additional frequency terms Within the block that is presently 
being modi.ed, new fre­quency terms are added to the calculation as additional rays are cast. In a manner 
identical to how the frequency terms are read out of the 8 by 8 JPEG frequency block, a zig-zag pattern 
is used to select the next frequency term to be added. This sequence emphasizes horizontal and vertical 
frequencies over diagonal fre­quencies, and is therefore consistent with the anisotropic spatial frequency 
response of the human visual system. A new frequency term is added after each set of two to three samples 
that are taken in the block, up to the maximum of 64 frequency terms. Since the sampling theorem requires 
that sampling be done at twice the highest frequency present in the solution, this keeps us from ad­mitting 
a frequency term into the solution until there have been enough samples taken to represent it. It also 
does not allow sin­gularities to arise in the inverse matrices. Since the number of frequency terms is 
limited, information will not be stored beyond the display resolution, even though the frequency coef.cients 
will continue to be re.ned. When the sampling progresses to a point at which a new fre­quency term should 
be added, it is necessary to expand the inverse matrix before placing it on the list. Use was made of 
the Frobe­nius relation to do this expediently. As a result this can also be done in O(N2 ) time [3]. 
Frobenius relation tells us that the in­verse matrix (JN+1 )-1 that solves the least squares equation 
for (N +1) terms may be computed from the inverse matrix (JN )-1 for N terms through the following equation 
JN B -1 (JN )-1 0 (JN+1 ) -1 + BT D 00 -1---­ 11 11 (JN )B BT (JN )-(JN )B (11) -1-­ - BT (JN )1 1 where 
B forms a N x 1 column vector and T -1 D -B(JN ) B S X Bi Cxs,ui Cys,vi Cxs,uN+1 Cys,vN+1 s =0 S X DCxs,uN+1 
Cys,vN+1 s=0 The resulting inverse matrix may then be multiplied by the ex­panded Y vector to obtain 
the (N +1) frequency terms given by X in Equation 8. The 64 term versions of J and Y are stored to yield 
B, D and YN+1 in an expedient manner. This matrix is then stored on the shared inverse list for use by 
other blocks. Because noise is less visible in the high frequency terms, the number of rays spawned from 
a surface by the Monte Carlo ray tracer decreases as more frequency terms are added to the solu­tion. 
As was shown in Section 1, the masking property of the human visual system makes it dif.cult for high 
frequency noise to be seen. Therefore, as higher and higher frequency terms are added to the solution 
for the block that is currently being pro­cessed, the number of rays spawned from each surface by the 
Monte Carlo ray tracer decreases. The precision with which the local illumination model is evaluated 
at each ray/object intersec­tion contributes to the accuracy of the .nal result for each ray cast. A 
bidirectional re.ectance function is used to model local illumi­nation, and this function is represented 
using a geodesic sphere and a novel data structure [11]. The decrease in rays spawned results in a less 
dense sampling of the bidirectional re.ectance function and a poorer estimate of the light reaching the 
surface at the point where the ray intersects it. The rate at which the number of rays spawned decreases 
(from the number necessary to determine diffuse re.ection) was modeled after the quantization table that 
is used in JPEG compression. Figure 8 shows a plot of the two dimensional function that was used to select 
a value for a particular frequency. 5.4 Choosing the next block to process When the new frequency representation 
for the current block has been determined, the block must be placed back on the sorted list. Dividing 
the image into 8 by 8 blocks provides a certain amount of adaptive subdivision since the same level of 
detail (and therefore the same frequency representation) is not required to represent each region of 
the picture. Those blocks with the most visually Figure 8: Rate at which the number of rays spawned decreases 
as higher frequency terms are added.  apparent artifacts should be scheduled for processing .rst. We 
employ the vision model described previously to decide where to put the current block back on the sorted 
list of blocks. First the frequency representation for each AC1 C2 channel is .ltered with rolloff set 
according to the cutoffs given in Section 3. This attaches more visual signi.cance to the changes in 
the lower fre­quency terms. Next, the root mean squared change for each fre­quency term in the AC1 C2 
representation is computed as a result of the current sample. This value is taken as an approximate mea­sure 
of the visually relevant aliasing that was removed. This is assumed to be an indicator that more re.nement 
may or may not need to be done on this block. This is due to the assumption that blocks in which a large 
amount of aliasing has occurred tend to need re.nement the most and blocks in which a small amount of 
aliasing has occurred tend to be well de.ned. This value is then combined with previous values and an 
initial uncertainty value to compute a gradient of change. The initial uncertainty value is in­troduced 
to insure that non-changing blocks still receive adequate sampling. These gradients form the sorted listed 
that is kept for all of the blocks in the image. The block at the head of this list is the one into which 
the next set of rays is to be cast. 5.5 Displaying the .nal result Due to undersampling, there is aliasing 
as the solution for each block progresses. As more samples are taken, the aliasing de­creases and the 
frequency representation for the block converges to the .nal solution. To control this aliasing, a low 
pass .lter is applied to the frequency representation of each of the blocks. The frequency coef.cients 
of the blocks are transformed back into a linear luminance RGB space before a simple 2-dimensional But­terworth 
.lter is applied. The parameters of the Butterworth .lter are set to roll the .lter off as a ratio of 
the square root of the number of samples that have been taken. As the sampling rate grows larger and 
the .nal solution for the block is determined, the width of the .lter grows large and its effect is greatly 
diminished.  6 Results The scene depicted in Figure 1 was actually rendered using the frequency based 
ray tracing technique. Figure 9 shows the sam­pling densities that were employed. In this example, the 
two small squares are equally different in luminance from the backgrounds on which they are positioned. 
However, due to the nonlinear con­trast sensitivity of the visual system, they are not perceived (in 
terms of brightness) to be equally different from the backgrounds. Figure 9: Sampling density for light 
patches in Figure 1. The difference in brightness between the two darker squares is greater than the 
difference in brightness between the two lighter squares. It can be seen in Figure 9 that the darker 
pair of squares received more samples than the brighter pair. This result shows that the .rst stage of 
the vision model is correctly converting luminance differences into approximate brightness differences. 
C The resolution fans in Figure 2 were also rendered by the frequency based ray tracer and the sampling 
densities used are shown in Figure 10. The fan on the left represents a change in only the A channel, 
the fan in the middle a change in only the 1 channel, and the fan on the right a change in only the C2 
channel. The amplitude of the oscillation in each fan is equal. The fact that it is more dif.cult to 
resolve the bottom of the color fans than it is for the monochromatic fan demonstrates that the visual 
system is able to resolve monochromatic differences more readily than chromatic differences. As can be 
seen in Figure 10, the monochrome resolution fan receives more samples at higher frequencies than the 
color resolution fans. This demonstrates that, in the vision model, the color is being properly transformed 
into an opponent space and that the correct spatial .ltering is being performed. The frequency based 
ray tracer has been used to render the fractal mountainsides in Figure 11. These mountains were created 
using a Fourier synthesis technique to produce two dimensional fractional Brownian motion. The 1 frequency 
distribution has f been cut off at a lower frequency for the mountain on the left than the mountain on 
the right. The lighting in the scene is very directional, with a small light placed in the foreground. 
This potentially creates a very noisy situation for the Monte Carlo ray tracer. In the top two images, 
1000 rays were spawned to evaluate the BRDF for each ray that struck the mountain. In the bottom two 
images, the number of rays spawned was reduced to 100. Notice that the noise in the lower two images 
is visible on the low frequency mountainside but not on the high frequency mountain. This demonstrates 
the masking effect of the visual system and shows that a smaller number of rays can be spawned when high 
frequency terms are being computed.  Figure 12 compares the result of using the frequency based ray 
tracer with a traditional ray tracer. The top two versions of the mountain were made by casting a ray 
through every pixel in the screen with a Monte Carlo ray tracer. In both cases, 1000 rays were spawned 
for each intersection with the mountain. No­tice the noise that is evident in the low frequency mountain 
and the aliasing that is produced around the edges of the scene. The bottom two examples of the mountain 
were made by using the frequency based ray tracer. As frequency terms were added to the solution, the 
number of rays spawned was decreased as in Figure 8. In the bottom left image, on average, a half of 
a ray was cast through every pixel. In this case, the noise is reduced even though fewer rays were cast 
into the scene. This shows that the interpo­lation that is done by .tting the cosine terms reduces the 
number of samples required. For the low frequency mountain the effect is similar to the ray cache that 
was used by Ward, Rubinstein, and Clear [34]. In the bottom right image, on average, one ray was cast 
through every pixel. In this case, far fewer rays were spawned from all of the intersections as the algorithm 
attempted to resolve the higher frequencies present in the mountain. This shows that the masking effect 
allows us to severely quantize the quality of the shading model in high frequency regions without reducing 
the perceptual quality of the image.  7 Summary and Conclusions A new frequency based approach to ray 
tracing has been pre­sented in this paper. This method makes it possible to use a simple model of human 
vision developed by the image process­ing community to control where rays are cast into a scene. In this 
way, the contrast sensitivity, spatial frequency response, and masking properties of the visual system 
are taken advantage of. In deciding where to take samples, a speci.c luminance differ­ence at low intensity 
is considered to be more important than the same difference at high intensity. Color spatial frequency 
vari­ations are given fewer samples than spatial frequency variations in luminance. When used in conjunction 
with a Monte Carlo ray tracer, more rays are spawned when low frequency terms are being determined than 
when high frequency terms are being found. The approach is also novel when considered simply as an im­age 
synthesis technique. The use of the frequency domain instead of the spatial domain to determine where 
additional rays should be cast is a new method of adaptively controlling a ray tracer. At low spatial 
frequencies, widely spaced samples, for which many rays are spawned to evaluate the BRDF, are interpolated 
in the im­age plane. This provides a result similar to a ray cache technique performed in object space 
[34]. The fact that high spatial frequen­cies cannot be added to the solution until many samples have 
been taken means that spurious high frequency artifacts cannot occur and eliminates the need for nonlinear 
.lters [30]. The size of our implementation has not yet allowed us to re­.ne each of its pieces as fully 
as we would have liked. Among the issues requiring further investigation are unbiased sampling patterns 
that permit ef.cient computation, the point at which to add new frequency terms to the calculation, the 
precise way in which ray spawning effects quantization, and how the solution is changed by the choice 
of priority metric. Nevertheless we feel that the architecture that we have outlined represents a .rst 
step on the path to an image synthesis technique that generates pic­tures directly in a compressed format. 
This is a topic that will continue to grow in importance as JPEG and MPEG become more widely used within 
the computer industry and digital high de.ni­tion television eventually makes its appearance in the consumer 
marketplace.  8 Acknowledgments The authors would like to thank Jon Newman for writing the Monte Carlo 
ray tracer that was used to perform part of this work and Jay Gondek for developing the software used 
to generate the fractal mountains. This research was funded by the National Science Foundation under 
grant number CCR 90-08445. 9 References [1] Albert, A. E. and Gardner Jr., L. A., Stochastic Approximation 
and Nonlinear Regression, M.I.T. Press: Massachusetts, 1967. [2] Bedford, R. E. and Wyszecki, G. Axial 
Chromatic Aberration of the Human Eye, Journal of the Optical Society of America, Vol. 47, No. 6, pp. 
564-565, 1957. [3] Bodewig, E., Matrix Calculus, North Holland Publishing Com­pany: Amsterdam, 1956. 
[4] Buchsbaum, G. and Gottschalk, A. Trichromacy, Opponent Colours Coding and Optimum Colour Information 
Transmission in the Retina, Proceedings of the Royal Society of London, Series B, Vol. 220, pp. 89-113, 
1983. [5] Buchsbaum, G. Color Signal Coding: Color Vision and Color Tele­vision, Color Research and Application, 
Vol. 12, No. 5, pp. 266-269, 1987. [6] Chiu, K., Herf, M., Shirley, P., Swamy, S., Wang, C., and Zim­merman, 
K., Spatially Non-Uniform Scaling Functions for High Contrast Images, Proceedings of Graphics Interface 
1993, pp. 245-253, 1993. [7] Cicerone, C. M. and Nerger, J. L., The Relative Numbers of Long­Wavelength-Sensitive 
to Middle-Wavelength-Sensitive Cones in the Human Fovea Centralis, Vision Research, Vol. 29, No. 1, pp. 
115-128, 1989. [8] DeValois, R. L., Smith, C. J., Karoly, A. J., and Kitai, S. T., Elec­trical Responses 
of Primate Visual System, I. Different Layers of Macaque Lateral Geniculate Nucleus, Journal of Comparative 
and Physiological Psychology, Vol. 51, pp. 662-668, 1958. [9] Estevez, O., On the Fundamental Data-base 
of Normal and Dichro­matic Color Vision, Ph.D. Thesis, University of Amsterdam, Krips Repro Meppel, Amsterdam 
1979. [10] Faugeras, O. D., Digital Color Image Processing Within the Frame­work of a Human Visual Model, 
IEEE Trans. Acoustics, Speech, and Signal Processing, Vol. 27, No. 4, pp. 380-393, 1979. [11] Gondek, 
J. S., Meyer, G. W., and Newman, J. G., Wavelength Dependent Re.ectance Functions, Computer Graphics, 
Annual Conference Series, ACM SIGGRAPH, pp. 213-220, 1994. [12] Guenter, B. K., Yun, H. C., and Mersereau, 
R. M., Motion Com­pensated Compression of Computer Animation Frames, Computer Graphics, Annual Conference 
Series, ACM SIGGRAPH, pp. 297-304, 1994. [13] Guth, S. L., Massof, R. W., and Benzschawel, T., Vector 
Model for Normal and Dichromatic Color Vision, J. Opt. Soc. Am., Vol. 70, pp. 197-211, 1980. [14] Hamming, 
R. W., Digital Filters, Prentice-Hall: New Jersey, 1977. [15] Hurvich, L. M. and Jameson, D., Some Quantitative 
Aspects of an Opponent-Colors Theory, II. Brightness, Saturation, and Hue in Normal and Dichromatic Vision, 
Journal of the Optical Society of America, Vol. 45, pp. 602-616, 1955. [16] Ingling, C. R., The Spectral 
Sensitivity of the Opponent-Colors Channels, Vision Research, Vol. 17 pp. 1083-1090, 1977. [17] Jameson, 
D. and Hurvich, L. M., Some Quantitative Aspects of an Opponent-Colors Theory, I. Chromatic Responses 
and Spectral Saturation, Journal of the Optical Society of America, Vol. 45, pp. 546-552, 1955. [18] 
Kirk, D. and Arvo, J., Unbiased Sampling Techniques for Image Synthesis, Computer Graphics, Annual Conference 
Series, ACM SIGGRAPH, pp. 153-156, 1991. [19] Lee, M. E., and Redner, R. A., A Note on the Use of Nonlinear 
Filtering in Computer Graphics, IEEE Computer Graphics and Applications, Vol. 10, No. 3, pp. 23-29, 1990. 
[20] Meyer, Gary W., Wavelength Selection for Synthetic Image Gen­eration, Computer Vision, Graphics, 
and Image Processing, Vol. 41, pp. 57-79, 1988. [21] Meyer, G. W., and Liu, A., Color Spatial Acuity 
Control of a Screen Subdivision Image Synthesis Algorithm, Human Vision, Visual Processing, and Digital 
Display III, Bernice E. Rogowitz, Editor, Proc. SPIE 1666, pp. 387-399, 1992. [22] Mitchell, D. P., Generating 
Antialiased Images at Low Sampling Densities, Computer Graphics, Annual Conference Series, ACM SIGGRAPH, 
pp. 65-72, 1987. [23] Mullen, K. T., The Contrast Sensitivity of Human Colour Vision to Red-Green and 
Blue-Yellow Chromatic Gratings, J. Physiol. (Lond.), Vol. 359, pp. 381-400, 1985. [24] Painter, J. and 
Sloan, K. Antialiased Ray Tracing by Adaptive Pro­gressive Re.nement, Computer Graphics, Annual Conference 
Series, ACM SIGGRAPH, pp. 281-288, 1989. [25] Pearson, D. E., Transmission and Display of Pictorial Infor­mation, 
John Wiley and Sons, 1975. [26] Poirson, A. B. and Wandell, B. A. Pattern-Color Separable Path­ways Predict 
Sensitivity to Simple Colored Patterns, to appear in Vision Research, 1996. [27] Powell, M. J. D., A 
Theorem on Rank One Modi.cations to a Matrix and its Inverse, Computer Journal, Vol. 12, pp. 288-290, 
1969. [28] Pratt, W. K., Digital Image Processing, Second Edition, John Wiley and Sons, 1991. [29] Rao, 
K. R. and Yip, P., Discrete Cosine Transform, Academic Press: Boston, 1990. [30] Rushmeier, H. E. and 
Ward, G. J., Energy Preserving Non-Linear Filters, Computer Graphics, Annual Conference Series, ACM SIGGRAPH, 
pp. 131-138, 1994. [31] Stark, P. A., Introduction to Numerical Methods, Macmillan Pub­lishing Co.: New 
York, 1970. [32] Tumblin J. and Rushmeier, H., Tone Reproduction for Realistic Images, IEEE Computer 
Graphics and Applications, pp. 42­48, 1993. [33] Ward, G., The RADIANCE Lighting Simulation and Render­ing 
System, Computer Graphics, Annual Conference Series, ACM SIGGRAPH, pp. 459-472, 1994. [34] Ward, G., 
Rubinstein, F. M., and Clear, R. D., A Ray Tracing Solution for Diffuse Interre.ection, Computer Graphics, 
Annual Conference Series, ACM SIGGRAPH, pp. 85-92, 1988. [35] Wallace, G. K., The JPEG Still Picture 
Compression Standard, Communications of the ACM, Vol. 34, No. 4, pp. 30-44, 1991. [36] Wallach, D. S., 
Kunapalli, S., and Cohen, M. F., Accelerated MPEG Compression of Dynamic Polygonal Scenes, Computer Graphics, 
Annual Conference Series, ACM SIGGRAPH, pp. 193-196, 1994. [37] Williams, D. R., MacLeod, D. I. A., and 
Hayhoe, M. M., Punctuate Sensitivity of the Blue-Sensitive Mechanism, Vision Research, Vol. 21, pp. 1357-1375, 
1981.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218498</article_id>
		<sort_key>419</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>52</seq_no>
		<title><![CDATA[Optimally combining sampling techniques for Monte Carlo rendering]]></title>
		<page_from>419</page_from>
		<page_to>428</page_to>
		<doi_number>10.1145/218380.218498</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218498</url>
		<keywords>
			<kw><![CDATA[Monte Carlo]]></kw>
			<kw><![CDATA[distribution ray tracing]]></kw>
			<kw><![CDATA[global illumination]]></kw>
			<kw><![CDATA[lighting simulation]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[variance reduction]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.9</cat_node>
				<descriptor>Fredholm equations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003738</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Integral equations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39045737</person_id>
				<author_profile_id><![CDATA[81100497843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Robotics Laboratory, Stanford University, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15025665</person_id>
				<author_profile_id><![CDATA[81452606669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Leonidas]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Guibas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, Robotics Laboratory, Stanford University, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>97886</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. Arvo and D. Kirk. Particle transport and image synthesis. Computer Graphics (SIGGRAPH '90 Proceedings), 24, 63-66 (1990).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122737</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. Chen, H. Rushmeier, G. Miller, and D. Turner. A progressive multipass method for global illumination. Computer Graphics (SIGGRAPH '91 Proceedings),25, 165-174(1991).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>154731</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[M. Cohen and J. Wallace. Radiosity and Realistic Image Synthesis. Academic Press, 1993.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[R. Cook, T. Porter, and L. Carpenter. Distributed ray tracing. Computer Graphics (SIGGRAPH '84 Proceedings), 18, 137-146 (1984).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Kajiya. The rendering equation. Computer Graphics (SIGGRAPH '86 Proceedings),20, 143-150(1986).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>7050</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Kalos and E Whitlock. Monte Carlo Methods, Volume I: Basics. J. Wiley, New York, 1986.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122735</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Kirk and J. Arvo. Unbiased sampling techniques for image synthesis. Computer Graphics (SIGGRAPH '91), 25, 153-156 (1991).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[E. Lafortune and Y. Willems. Bi-directional path tracing. Proceedings ofCompuGraphics, Alvor, Portugal, 145-153 (Dec. 1993).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[E. Lafortune, Y. Willems. A theoretical framework forphysically based rendering. Computer Graphics Forum, 13(2), 97-108 (1994).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>127967</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[G. Nelson, editor. Systems Programming with Modula-3. Prentice Hall, 1991. An implementation of Modula-3 is available at http ://www.research.digital.com/SRC/.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>914720</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[H. Rushmeier. Realistic Image Synthesis for Scenes with Radiatively Participating Media. Doctoral Thesis, Cornell University, May 1988.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[E Shirley and C. Wang. Distribution ray tracing: theory and practice. Proceedings of the Third Eurographics Workshop on Rendering, Bristol, England, 33-44 (1992).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>226151</ref_obj_id>
				<ref_obj_pid>226150</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[E Shirley, C. Wang, and K. Zimmerman. Monte Carlo Techniques for Direct Lighting Calculations. ACM Transactions on Graphics, to appear.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[E. Veach and L. Guibas. Bidirectional estimators for light transport. Proceedings of the Fifth Euro g raphics Workshop on Rendering, Darmstadt, Germany, 147-162 (June 1994).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Optimally Combining Sampling Techniques for Monte Carlo Rendering Eric Veach Leonidas J. Guibas Computer 
Science Department Stanford University Abstract Monte Carlo integration is a powerful technique for 
the evaluation of dif.cult integrals. Applications in rendering include distribution ray tracing, Monte 
Carlo path tracing, and form-factor computation for radiosity methods. In these cases variance can often 
be signi.­cantly reduced by drawing samples from several distributions, each designed to sample well 
some dif.cult aspect of the integrand. Nor­mally this is done by explicitly partitioning the integration 
domain into regions that are sampled differently. We present a power­ful alternative for constructing 
robust Monte Carlo estimators, by combining samples from several distributions in a way that is prov­ably 
good. These estimators are unbiased, and can reduce variance signi.cantly at little additional cost. 
We present experiments and measurements from several areas in rendering: calculation of glossy highlights 
from area light sources, the .nal gather pass of some radiosity algorithms, and direct solution of the 
rendering equation using bidirectional path tracing. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism; I.3.3 [Computer Graphics]: Picture/Image Generation; G.1.9 [Numerical Analysis]: 
Integral Equations Fredholm equations. Additional Keywords: Monte Carlo, variance reduction, render­ing, 
distribution ray tracing, global illumination, lighting simulation. 1 Introduction Technically, rendering 
is all about clever ways to approximate in­tegrals. For example, the pixel values in an ideal image usu­ally 
involve integration over the image plane, lens position, and so on. Furthermore, the quality of a rendering 
algorithm is frequently measured by the accuracy and ef.ciency with which these integrals are approximated. 
In this paper, we focus on Monte Carlo (MC) methods for evaluating such integrals. These methods use 
random sampling to simplify the integration problem, by expressing the integral as the expected value 
of a random variable. The major drawback of MC integration is that the resulting estimates can have high 
variance; this is perceived as noise in a rendered image. Address: Computer Science Department, Robotics 
Laboratory Stanford University, Stanford, CA 94305-2140 E-mail: ericv@cs.stanford.edu, guibas@cs.stanford.edu 
Web: http://www graphics.stanford.edu/ Permission to make digital/hard copy of part or all of this work 
for personal or classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, 
and notice is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post 
on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169;1995 
ACM-0-89791-701-4/95/008 $3.50 Unfortunately,thefunctionsthatweneedtointegratein computer graphics are 
often ill-behaved. They are almost always discontin­uous, and often have singularities or very large 
values over small portions of their domain. Because of this, we often need more than one sampling technique 
to estimate an integral with low variance. Normally this is accomplished by explicitly partitioning the 
domain of integration into several regions, and designing a sampling tech­nique for each region. For 
example, a simple distribution ray tracer may use one technique to evaluate direct lighting, another 
to esti­mate glossy re.ections, and a third for ideal specular contributions. In thispaper,weexplorethegeneralproblemofconstructinglow­variance 
estimators by combining samples from several techniques. We do not construct new sampling methods all 
the samples we use come from one of the given distributions. Instead, we look for better ways to combine 
the samples; in particular, strategies that compute weighted combinations. We show that there is a large 
class of unbiased estimators of this type, parameterized by a set of weighting functions. We then seek 
weighting functions within this class that minimize variance. In a sense, we are asking the inverse problem: 
given several sampling techniques, how should the domain be partitioned among them? (Or more generally, 
how should the samples be weighted?) A good solution to this problem turns out to be surprisingly simple. 
We show how to combine samples from several distributions in a way that is provably good, both theoretically 
and practically. This allows us to construct MC estimators that have low variance for a broad class of 
integrands we call such estimators robust. The signi.cance of our methods is not that we can take several 
bad sampling techniques and concoct a good one out of them, but rather that we can take several potentially 
good techniques and combine them so that the strengths of each are preserved. In Sec. 2, we review the 
fundamentals of MC integration for rendering, and give an example to motivate our variance reduction 
framework. Sec. 3 explains our ideas on combining samples from several distributions, and gives theoretical 
justi.cation under several models (proofs can be found in App.A). In Sec.4 we present computed images 
and numerical results for several application areas: glossy highlights from area light sources, the .nal 
gather pass of some radiosity algorithms, and direct solution of the rendering equation using bidirectional 
path tracing. Finally, Sec. 5 discusses of a number of tradeoffs and open issues related to our work. 
 2 Monte Carlo rendering 2.1 Integrals for radiance We have chosen two basic problems in rendering to 
illustrate our techniques: evaluation of the radiance leaving a surface given a description of the incoming 
illumination (as in distribution ray trac­ing or some .nal gather approaches), and direct solution of 
the rendering equation[5]. For further details and background see [3]. 00 Given the incident radiance 
distribution Li(x' wi at a point x, .r x x . i . r . i x Figure 1: Geometry for the re.ectance equation. 
00 the re.ected radiance Lr(x'wris given by the re.ectanceequation Z 00 0000 0 Lr(x'wr=fr(x'wi'wrLi(x'wicos(Oi0 
dO(wi(1) 2 S where fris the bidirectional re.ectance distribution function (BRDF), S 2is the set of all 
unit direction vectors, Ois the usual 0 solid angle measure, and Oi0is the angle between wiand the surface 
0 normal at x(see Fig. 1). We allow frto model transmission as well (in this case fris the bidirectional 
scattering distribution function). Sometimes it is preferable to express the re.ectance equation as an 
integral over the domain Mof scene surfaces (e.g. for direct lighting calculations). This form is given 
by Z 00000000 Lr(x'x=fr(x'x'x G(x'xLi(x'xdA(x(2) M cos(Orcos(O0 00i where G(x'x=V(x'x x02 kx_k Here Ais 
the usual measure of surface area, Orand Oi0measure 00 the angle between x'xand the surface normals at 
xand x 00 respectively, while V(x'xis 1 if xand xare mutually visible 0 and 0 otherwise. The term G(x'xmeasures 
the differential 0 throughput of a beam[3] from xto x. Often the incident radiance distribution is unknown, 
and we must solve for it. This leads to the global illumination problem: given an emitted radiance distribution 
Le, .nd the equilibrium radiance distribution Lsatisfying 000000 L(x'x =Le(x'x (3) Z 000 0 0 +fr(x'x'xG(x'xL(x'xdA(x 
 M This is known as the three-point rendering or light transport equa­tion[5]. Equation (3) can be written 
concisely in operator form as L=Le+TL,where Tis the light transport operator. Under weak assumptions, 
the solution is given formally by the Neumann series 1 X L=T i Le (4) i=0 This says that the equilibrium 
radiance Lis the sum of emitted light, plus light that bounces once, twice, etc. Our goal is to compute 
a .nite set of measurements that approx­imately represent L. Each measurement Ipis expressed as an inner 
product or weighted average of the radiance distributionL,as modeled by the measurement equation: Z 0000 
Ip=hWp 'Li=Wp(x'xL(x'xG(x'xdA(xdA(x MxM (5) 0 where Wp(x'xis the weighting function corresponding to 
a particular measurement Ip. For example, the value of each pixel pin an image can be ex­pressed in the 
form (5), using a weighting function Wpthat is non-zero on the set of rays mapped to pixel pby the virtual 
lens. Wpcan model arbitrary lens systems used to form the image, as well as any linear .lters used for 
anti-aliasing. 2.2 Monte Carlo integration We review the basic principle of MC integration, and establish 
some notation for the following sections. Our goal is to estimate Z 1 F=f(xdJ(x where fo'Rand Jis a measure 
function. We de.ne a sampling technique as an algorithm for choosing random points in the domain o.Let 
p(xdJ(xbe the probability distribution of the points generated. The idea of MC integration is to generate 
a sample X, and then use f(X/p(Xas an estimate of F. As long as the sample value f(X/p(Xis .nite for 
all samples X, it is easy to show that this estimate is unbiased: ZZ f(Xf(x E= p(xdJ(x=f(xdJ(x=F(6) p(Xp(x 
11 where E[Z]denotes the expected value of Z. In practice, we esti­mate Fby taking several samples X1 
''Xndistributed according to p, and computing n X 1 f(Xi F (7) np(Xi i=1 MC integration has one inherent 
drawback,which manifests itself as a tradeoff between variance and running time. Letting Fbe the sample 
value f(X/p(X, the variance of Fis Z V[F]=E[F 2]_E[F]2 = f(xdJ(x_F 2 (8) 1p2 (x If we take nindependent 
samples according to (7), variance is reduced by a factor of n, while running time is increased by a 
factor of n. This tradeoff is summarized by the ef.ciency [1,6]ofaMonte Carlo estimator, 1 E[F]= V[F] 
T[F] where T[F]is the time required to take a sample from F. The higher the ef.ciency, the less time 
required to achieve a given variance. The design of ef.cient estimators, often simply called variance 
reduction, is a fundamental goal of MC research. Notice that the variance in (8) is strongly affected 
by the sampling distribution p e.g. if pis proportional to f(assuming f.0), the variance V[F]is zero. 
Unfortunately the normalization p=f/F requires knowledge of F, so this is not practical. However, by 
choosing a distribution pwhose shape is similar to f, variance can be reduced. This idea is known as 
importance sampling[6]. On the other hand, suppose that we sample finadequately in some region Uwhere 
its value is large (i.e. p<f/F). By (8) we see that samples from Ucan make a large contribution to the 
variance, even if Uis relatively small. This effect is a major cause of noise in Monte Carlo images. 
Our primary goal is to show how this problem may be avoided, by combining samples from several distributions 
designed to sample well each signi.cant region of f. 2.3 An example: glossy highlights Consider how 
a distribution ray tracer might render the highlight produced by an area light source Son a nearby glossy 
surface (see Fig. 2). Given a viewing ray that strikes the glossy surface, there are two obvious strategies 
for MC evaluation of the re.ected radiance, corresponding to forms (1) and (2) of the re.ectance equation. 
With area sampling, we randomly sample points on Sto evaluate the integral (2). To compute the estimate 
(7), we must know the distribution p(xdA(xof the samples for example, they may be chosenuniformly on 
Swith respectto surfaceareaoremittedpower. Since there is considerable freedom in choosing p, area sampling 
 (a) Sampling the light sources (c) A combination of samples from (a) and (b). is really a family of 
techniques. The glossy highlights in Fig. 2(a) were computed with an area sampling strategy. With directional 
sampling,we estimate the integral (1) by random 0 sampling of the incident direction wi. Evaluation of 
Lirequires casting a ray; only the rays that strike Scontribute to the highlight 00 calculation. Typically 
the distribution p(widO(wiis chosen to 000 0000 be proportional to fr(x'w'wor to fr(x'w'wjcos(Oj. iriri 
Fig. 2(b) was computed with a directional sampling strategy. One of these strategies can have a much 
lower variance than the other (see Fig. 2). For example, if the light source is very small, we are unlikely 
to hit it with rays chosen by randomly sampling the BRDF. On the other hand, if the BRDF is nearly specular, 
ran­domly chosenpoints onthe light sourcewill probablynotcontribute signi.cantly to the radiance re.ected 
along the viewing ray. In both these cases,noise is causedby inadequate sampling where the integrand 
is large. To understand this, notice that the integrand in the re.ectance equation (2) is the product 
of various unrelated factors the BRDF, the emitted radiance Le, and several geometric quantities. However, 
the area sampling distribution used in Fig. 2(a) does not take into account the BRDF for example, while 
the direc­tionalsamplingin Fig.2(b)doesnotdependonthe emittedradiance. When an unconsidered factor is 
dominant (e.g. a small bright light, or a shiny surface), that sampling technique will do poorly. It 
is important to realize that both strategies are importance sam­pling techniques aimed at generating 
sample points on the same (b) Sampling the BRDF Figure 2: Sampling of glossy highlights from area light 
sources (Sec. 2.3, 4.1). There are four spherical light sources of varying radii and color, plus a spotlight 
overhead. All spherical light sources emit the same total power. There are also four shiny rectangular 
plates of varyingsurfaceroughness,eachonetilted sothatwe see the re.ected light sources. Given a viewing 
ray that strikes a glossy surface, images (a), (b), (c) use different techniques for the highlight calculation. 
All images are 500 by 450 pixels. 0 (a) A sample direction wis chosen uniformly (with respect to i solid 
angle) within the cone of directions subtended by each light source, using n14samples per pixel. 0 (b) 
wis chosen with probability proportional to the BRDF i 00 fr(x0w w)dI(w),using n24samples per pixel. 
' 0 iri (c) A weighted combination of the samples from (a) and (b) is computed, using the power heuristic 
with (2. The glossy BRDF is a symmetric, energy-conserving variation of the Phong model. The Phong exponent 
is n1/r-1,where ris a surface roughness parameter, 0r 1. The glossy surfaces also have a small diffuse 
component. Similar results could be obtained with other glossy BRDF s. domain (in this case, the light 
source S). Area sampling chooses a point x2Sdirectly, while directional sampling chooses xby cast­ 0 
ing a ray in the chosen direction wi. Given a directional distribution 00 p(widO(wi, the corresponding 
area distribution p(xdA(xis 0 0dO(w0cos(Or i p(x=p(wi =p(w (9) i dA(x kx_x02 k (see Fig. 1)1. This lets 
us compute the probability densities assigned by area and directional methods to the same point x. 2.4 
Our framework for variance reduction When choosing a Monte Carlo sampling technique, we rarely know exactly 
what the integrand is. Instead, we have some model for the integrand, de.ned by a set of parameters (e.g. 
the BRDF, the scene geometry, etc). Given several sampling techniques to choose from, the variance of 
each one can change dramatically as these parameters vary. Our main goal is to show how Monte Carlo integration 
can be made more robust, by constructing estimators that have low variance for a broad class of integrands. 
To achieve this, we must avoid 1One could argue that V(x x0)should appear in (9). But if V(x x0 )0, the 
integrand (2) is also zero, which makes p(x)irrelevant. insuf.cient sampling of each candidate integrand 
fwhere its value is large. Our approach to this problem has three steps. First, we design a set of importance 
sampling distributions p1 ''pn. For each region where fhas the potential to be large, we try to construct 
a sampling distribution that approximates fwell over that portion of the domain. An excellent source 
of these distri­butions is the situation in the example above, where fis a product of several unrelated 
functions, and each piis proportional to the product of a subset of these. Next, we determine how many 
samples to take from each pi.We assume this is .xed in advance, based on knowledge of fand pi. Finally, 
the integral is estimated as a weighted combination of all the sample values. The main subject of this 
paper is how to do this, such that the estimate is unbiased and has low variance.  3 Combining sampling 
techniques We are given an integrand fo'R, and several impor­tance sampling distributions R p1 ''pn. 
Our goal is to estimate f(xdJ(x. We assume that only two operations are available: 1 we can take a sample 
from any of the distributions pi, and we can evaluate f(x,and pi(xfor any x2o. Each sample is assumed 
to be independent, i.e. we generate new random bits to control its selection. As mentioned above, we 
must also decide how many samples to take from each pi.We de.ne cias the relative number of samples P 
taken from pi,where ici=1. In this paper, we assume that the ciare .xed in advance, i.e. before any samples 
are taken. The choice of the ciis an interesting problem that we discuss further in Sec. 5.2. The key 
ideas in this section are simple. First, notice that by drawing a fraction ciof the samples from each 
pi, the resulting group P of samples has the distribution p(x= icipi(x. We propose that the natural way 
to combine importance sampling techniques is to consider this combined sample distribution when computing 
the unbiased estimate f(X/p(X. Second, we show that this method of combining samples is prov­ably good 
(compared to partitioning, simple weighted combinations, etc). To justify this claim, we explore a much 
larger class of un­biased combination strategies, parameterized by a set of weighting functions. We then 
look for weighting functions that minimize the variance of the combined estimator, and show that the 
combination strategy above is close to optimal. This gives us con.dence that our methods compare favorably 
with other possible techniques. Third, we use our framework of unbiased estimators to reduce variance 
further in an important special case. Speci.cally, it is common in practice that for the particular integrand 
fwe are given, one of the given sampling distributions is far superior to the rest (e.g. a small bright 
light or shiny surface in Fig. 2). We study two families of weighting functions that perform signi.cantly 
better in this situation, while retaining provably good behavior in general. 3.1 The combined sample 
distribution Suppose that ni=ciNindependent samples Xi,jare taken from distribution pi, for a total of 
Nsamples. As a group, the samples have the distribution n X p(x= cipi(x i=1 More precisely, p(xis the 
distribution of a random variable X which is equal to each Xi,jwith probability 1/N. We call this the 
combined sample distribution. From this point of view, the standard estimator (7) gives nni XX 1 f(Xi,j 
F= (10) Np(Xi,j i=1j=1 As we will show, this is a provably good way to combine samples from several distributions. 
Within the framework described below, this strategy is called the balance heuristic (Sec. 3.3). 3.2 
The multi-sample model In this section we consider unbiased estimators that allow samples to beweighteddifferently, 
dependingonwhichunderlyingdistribution pithey were chosen from. Each estimator is parameterized by a 
set of weighting functions w1 ''wn,where wi(xgives the weight associated with a sample xdrawn from pi.The 
combined estimator is given by nni XX 1 f(Xi,j F= wi(Xi,j (11) ni pi(Xi,j i=1 j=1 where the Xi,jare independent 
samples from distribution pi, as before. For this estimator to be unbiased, we require that P iwi(x=1for 
all x, since this gives ZZ n X 1wi(xf(x E[F]= ni pi(xdJ(x=f(xdJ(x ni pi(x 11 i=1 Think of this as a weighted 
sum of the estimators f(Xi,j/pi(Xi,j. The weights are allowed to vary with position, but must always 
sum to one. For example, if at every point xall but one of the wiare zero, we get a simple partitioning 
of the domain into nregions. This represents a heuristic such as dividing the visible hemisphere into 
light source regions and non-light-source regions, which are then sampled using different methods. 3.3 
The balance heuristic We now have a large parameter space over which to optimize (the space of allowable 
weighting functions wi). Our goal is to minimize the variance of Fby choosing the wiappropriately. Consider 
the weighting functions cp(x ii wFi(x= P (12) cjpj(x j These Fhave the unique property that the sample 
wivalue fwFi(xf(xg/fnipi(xgfrom (11) does not depend on i. Because the sample value at a particular xis 
the same for all underlying dis­tributions, we call this strategy the balance heuristic. Substituting 
F wiinto (11), this is simply a reformulation of the estimator (10) we obtained using the combined probability 
distribution. The following theorem gives evidence that these weighting func­tions are good: Theorem 
1. Let w1 ''wnbe any non-negative functions with P iwi=1, and let F''wFn w1 be the weighting functions 
above (the balance heuristic). Let Fand FFbe the corresponding combined estimators (11). Then 11 2 V[FF]:V[F]+_ 
PF minini ni i See App. A for a proof. This theorem says that no choice of the wican improve upon the 
variance of the balance heuristic by more than (1/minini_1/NF 2(recall that Fis the quantity we are trying 
to estimate). This variance gap is very small relative to the variance caused by a poorly chosen sampling 
distribution, as we saw in Fig. 2. Also, the variance gap goes to zero as the number of samples increases 
(assuming all niare increased). Furthermore, these weighting functions are practical to evaluate. The 
key requirement is that given a sample Xifrom pi,we must be able to evaluate pj(Xifor all j. Any unbiased 
Monte Carlo algorithm must be able to evaluate pi(Xi, so thisisoften just a matter of reorganizing the 
routines that compute probabilities. The time to evaluate these probabilities is generally insigni.cant 
compared to other rendering calculations, as we show in Sec. 4. Figure 3: Two distributions for sampling 
the integrand. 3.4 Other weighting heuristics Theorem 1 implies that although the balance heuristic 
is good, there is still room for improvement. In this section we discuss two families of heuristics that 
in practice often have lower variance than the P balance heuristic. These heuristics satisfy iwi(x=1and 
thus give unbiased estimates. We are motivated by the common situation where one of the piis an almost 
perfect match for f(e.g. BRDF sampling with the mirror­like surface in Fig. 2). To develop our ideas, 
consider the situation in Fig. 3, where fis a very peaked distribution, p1is proportional to f,and p2is 
the uniform distribution. Assume that we take an equal number of samples from both pi, and form a weighted 
combination using the multi-sample model (11). Since p1is a zero-variance importance sampling distribution 
(f(X1/p1(X1=Fis constant), the optimal weighting func­tions are obviously w1(x=1, w2(x=0. We cannot expect 
to guess this using only pointwise evaluation of the piand f;however, we would like to get as close to 
this ideal as possible. How well does the balance heuristic perform in this situation, and how can we 
improve it? Consider the contributions of samples from p1and p2separately. Most samples from p1occur 
near the peak, where the weighted sample value (see (12)) is approximately equal to F. Similarly, most 
samples from p2occur away from the peak, where their sample value is zero (because fis zero there). So 
far, this is very close to optimal. However there are two effects that lead to additional variance. Occasionally 
a sample from p1occurs away from the peak (i.e. where p1:p2does not hold). In this case the weight p1/(p1+p2produces 
a sample value smaller than F; in an image, this shows up as dark spots. On the other hand, sometimes 
a sample X2from p2occurs near the peak of f.These have a weighted sample value slightly smaller than 
F(see Sec. 3.3). In an image, this shows up as occasional bright spots. However, these spikes are relatively 
small in magnitude, because a sample from p2contributes the same as an equivalent sample from p1. We 
present two families of heuristics that reduce variance in this important limiting case. They are variations 
on the balance heuristic, where the weighting functions have been sharpened by making large weights closer 
to one and small weights closer to zero. This is effective at reducing both types of noise above. The 
cutoff heuristic modi.es the weighting functions by discard­ing samples with low weight:2 8 0 if piapmax 
wi= pi otherwise (13) P jfpjjpj a pmaxg where pmax =maxjpj. The constant adetermines how small pi must 
be compared to pmaxbefore we assign it a zero weight. The power heuristic raises all weights to a power 
f,and then normalizes: p wi= P i (14) p jj 2All piand wiare implicitly functions of x. For simplicity 
we have assumed all niare equal; otherwise replace piby nipieverywhere. Notice that when a=0or f=1, we 
get the balance heuristic. When a=1or f=1,we get the maximum heuristic: 1if pi=pmax wi=(15) 0otherwise 
This heuristic simply partitions the domain according to which dis­tribution pigenerates samples there 
with the highest probability. The advantage of these heuristics is reduced variance when one of the piis 
much better than the rest. Their performance is otherwise similar to the balance heuristic; it is possible 
to show they are never much worse (we give bounds in App. A, measurements in Sec. 4.1). 3.5 The one-sample 
model: optimality In this section, we consider a sampling model where we our com­bination methods are 
optimal. Under this one-sample model, each sample is taken from a randomly selected distribution pi. 
Distri­bution piis chosen with probability ci. This idea is used in path tracing for example, where at 
each bounce we choose randomly between the diffuse, specular, or transmitted distributions. Again, each 
estimator is parameterized by a set of weighting functions fwi(xg. The process of choosing a distribution, 
taking a sample, and computing the weighted sample value is described mathematically by the combined 
estimator i X wI(XIf(XI F= 'where I=minfijUcjg(16) cIpI(XI j=1 Here Uis a uniformly distributed random 
variable on [0'1, Iis the index of the randomly chosen distribution, and XIis asamplefrom P distribution 
I. This estimator is unbiased as long as iwi=1. In this case, the balance weighting strategy is optimal: 
Theorem 2. Let w1 ''wnbe any non-negative functions with P iwi=1, and let wF1 ''wFnbe the weighting functions 
(12). F Let Fand Fbe the corresponding combined estimators (16). Then V[FF]:V[F].  4 Experiments 4.1 
Distribution ray tracing Our .rst test is the computation of glossy highlights from area light sources 
(see also Sec. 2.3 and Fig. 2). The area sampling technique3used in Fig. 2(a) works well for small light 
sources and rough surfaces. The directional sampling technique in (b) does well for large light sources 
and smooth surfaces. In (c), the power heuristic with f=2is used to combine both kinds of samples. This 
method works very well for all light source/surface combinations. We have also measured variance numerically 
as a function of roughness. Fig. 4 shows the test setup, and the results are sum­marized in Fig. 5. Notice 
that all four weighting heuristics yield a variance that is close (on an absolute scale) to the minimum 
vari­ance when either sampling technique is used alone. In particular, Thm. 1 guarantees that the variance 
O 2of the balance heuristic is within J 2 /2of the best input technique. The plots in Fig. 5(a) are well 
within that bound. At the extremes of the roughness axis there are signi.cant dif­ferences among the 
heuristics. As expected, the balance heuristic (a) performs worst at the extremes, since the other heuristics 
were speci.cally designed for the case when one sampling technique is much better than the rest. The 
power heuristic (c) with f=2works especially well over the whole range of roughness values. 0 3Direction 
wis used to compute a point xon the light source directly, i rather than casting a ray to .nd the .rst 
visible point. Thus form (2) of the re.ectance equation is used, making this an area sampling technique. 
 2  2 1.5 1.5 1 1 0.5 0.5 0 0  -6-4-2 0 -6-4-2 0 1010101010101010 (a) The balance heuristic. (b) The 
cutoff heuristic (O0 1).  2  2 1.5 1.5 1 1 0.5 0.5 0 0  -6-4-2 0 -6-4-2 0 1010101010101010 (c) The 
power heuristic ((2). (d) The maximum heuristic. 2 Figure 5: Variance measurements for the test case 
in Fig. 4. Each graph plots I/ivs. surface roughness, where Iis the variance of a single sample and iis 
the mean. Three curves are shown, corresponding to the area sampling technique from Fig. 2(a), the directional 
sampling technique from Fig. 2(b), and a weighted combination of both sample types using the (a) balance, 
(b) cutoff, (c) power, and (d) maximum heuristics. The images above each graph are computed with the 
corresponding heuristic, for the three roughness values circled (one sample per pixel, box .lter). The 
center pixel of these images corresponds to the viewing ray used for the variance measurements. spherical 
light source glossy surface Figure 4: A scale diagram of the setup used to measure the variance of the 
highlight calculation. The light source occupies a solid angle of 0.063 radians. The variance for each 
roughness value was measured by taking 100,000 samples using the viewing ray shown. Above the graphs 
we show how the variance of each method appears in an image, for three circled roughness values. Notice 
how the cutoff, power, and maximum heuristics reduce the bright spot and dark spot noise (Sec. 3.4) at 
the extremes. Recall that to evaluate the weights at a point x, we must com­pute the probabilities with 
which both methods generate x.For example, if xis a point on the light source generated by (a), we 00 
.nd the probability p2(widO(withat (b) generates the direction 0 wipointing toward x, and convert this 
probability to the measure p2(xdA(xusing (9). The total time spent evaluating probabilities and weighting 
functions in our tests was less than 5%.  4.2 Final gather In this section we consider a simple test 
case motivated by multi­pass global illumination algorithms. These algorithms typically compute an approximate 
solution using the .nite element method, followed by one or more ray tracing passes to replace parts 
of the so­lution that are poorly approximated or missing. For example, some radiosity algorithms use 
a local pass or .nal gather to recompute certain coef.cients more accurately. We examine a variation 
called per-pixel .nal gather.The idea is to compute an approximate radiosity solution, and then use it 
to illuminate the visible surfaces during a ray tracing pass[11, 2]. Essentially, this type of .nal gather 
is equivalent to ray tracing with many area light sources (one for each patch, or one for each link in 
a hierarchical solution). As with the glossy highlight example, there are two common sampling techniques. 
The brightest patches are classi.ed as light sources [2], and are handled with an area sampling technique 
(e.g. samples are distributed on the light sources according to emitted power). The remaining patches 
are sampled by casting rays randomly into the scene (i.e. directional sampling from the point intersected 
by the viewing ray). If one of these rays hits a light source patch, the sample value is zero (to avoid 
counting those patches twice). Within our framework for combining sampling techniques, this is clearly 
a partitioning of the integration domain into two regions. Given some classi.cation of patches into light 
sources and non­  5 4 3 2 1 0 0 0.2 0.4 0.6 0.8 1  (a) (b) (c) (d) Figure 6: A simple test scene 
consisting of one area light source (i.e. a bright patch) and an adjacent diffuse surface. The images 
were computed by (a) sampling the light source according to emitted power, with n13samples per pixel, 
(b) sampling the hemisphere according to the projected solid 0 angle[3] cos(B0 )dI(w)(see Fig. 1), with 
n26samples per pixel, and (c) a weighted combination of samples from (a) and (b) using the power ii 
heuristic with (2. (d) Aplot of I/i(standard deviation divided by mean) as a function of distance from 
the light source, for n11and n22. light sources, we consider alternative ways of combining the two types 
of samples. To test our weighting strategies, we used the extremelysimpletestsceneshowninFig.6. Twiceasmanysamples 
are taken in (b) than (a); in practice this ratio would be substantially higher (i.e. the number of directional 
samples vs. the number of samples for any one light source). Notice that Fig. 6(a) does poorly for points 
near the light source, because the sample distribution does not take into account the 1/r 2 distance 
term of the re.ectance equation (2). On the other hand (b) does poorly far away from the light source, 
when the light subtends a small solid angle. In Fig. 6(c), the power heuristic is used to combine samples 
from (a) and (b). As expected, this method performs well at all distances. Although (c) uses more samples 
(the sum of (a) and (b)), this is a valid comparison with the partitioning approach (which also uses 
both kinds of samples). Variance measurements are plotted in Fig. 6(d).  4.3 Bidirectional path tracing 
The basic goal of Monte Carlo path tracing is to estimate the value of each pixel in an image by direct 
sampling of the rendering and measurement equations (Sec. 2.1). In this section, we show that by combining 
samples from several importance sampling techniques, this process can be made more ef.cient. As a source 
of sampling distributions, we use bidirectional path tracing (introduced inde­pendently in [14] and [8, 
9]). We brie.y overview the theory below. To apply our methods, we must .rst express the value Ipof a 
R pixel pin the standard form f(xdJ(x. Todothis, we write 1 out equations (3), (4), and (5) explicitly: 
P Ip=hWp 'Li=hWp ' iT i Lei (17) Z =Le(x0 'x1G(x0 'x1Wp(x0 'x1dA(x0dA(x1 2 M Z +Le(x0 'x1G(x0 'x1fr(x0 
'x1 'x2 3 M G(x1 'x2Wp(x1 'x2dA(x0dA(x1dA(x2 + R To write this as a single integral f(xdJ(x,let obe the 
set of 1 transport paths of all lengths. Each transport path 7of length kis a sequence x0x1 xkof points 
xi2M. The measure dJ(7on o is de.ned by dJ(7=dA(x0 dA(xk. 4Finally, the integrand f(7is simply the appropriate 
term from the expansion above, for example f(x0x1=Le(x0 'x1G(x0 'x1Wp(x0 'x1. Path tracing algorithms 
can be interpreted as methods for sam­pling this integral directly, by generating transport paths 7randomly 
and using the standard estimate f(7/p(7. Observe that paths where f(7is large satisfy two conditions: 
they carry a relatively 4di(7)dA(x0)G(x0x1)dA(x1)G(xx)dA(xk)is k 1k another possibility this measures 
the differential throughput of a path. x2 x0 x4 x3 x1  Figure 7: A transport path from a light source 
to the camera lens, created by concatenating two separately generated pieces. (a) m0 (c) m2 (b) m1 (d) 
m3 Figure 8: The four bidirectional sampling strategies for paths of length two (direct lighting). Intuitively, 
they can be described as (a) Monte Carlo path tracing with no special handling of light sources, (b) 
standard MC path tracing with direct lighting, (c) depositing (splatting) light on the image when a photon 
hits a visible surface, and (d) depositing light when a photon hits the camera lens. large amount of 
light, and they have a relatively large weight in the measurement process that generates the .nal image. 
Bidirectional path tracing uses this idea to construct a family of importance­sampling techniques that 
trade off one property against the other. Unlike standard path tracing, which generates transport paths 
by starting from the eye and following random bounces backward to the light sources, the bidirectional 
approach builds a path by connecting two independently generated pieces, one starting from the light 
sources and the other from the eye. For example, in Fig. 7 the light subpath x0x1is constructed by choosing 
a random point x0on a light source (area sampling), followed by casting a random ray (directional sampling) 
to .nd x1.The eye subpath x2x3x4 is constructed by a similar process starting from a random point x4on 
the camera lens. A complete transport path is formed by concatenating these two pieces. (This path may 
carry no light, for example if x1and x2are not mutually visible.) Thisidealeadsto asetofsamplingtechniquesfortransportpaths. 
Each technique generates paths of a speci.c length k, by randomly generating a light subpath with mvertices, 
randomly generating an eye subpath with k+1_mvertices, and concatenating them. In total there are k+2distinct 
bidirectional sampling techniques for paths of length k(letting m=0''k+1, see Fig. 8). Each of these 
is really a framework for sampling rather than a speci.c technique,  (a) The weighted contribution that 
each bidirectional sampling technique makes to image (b)  (b) Combines samples from all the bidirectional 
techniques (c) Standard path tracing using the same amount of work Figure 9: The scene contains a spot 
light, a .oor lamp, a table, and a big glass egg. Image (b) uses the power heuristic (with (2) to combine 
samples from a family of bidirectional path tracing techniques, whose weighted contributions are shown 
in (a).Row ishows techniques that sample transport paths of length i+1;the m-th image uses the distribution 
pi.1,m(see Sec. 4.3). Images in row ihave been over-exposed by if-stops so that details can be seen. 
since the paths generated depend on the distributions used to choose each vertex (area sampling for the 
.rst vertex of each subpath, usually directional sampling for the rest). These methods can be very diverse, 
e.g. sophisticated direct lighting techniques can be used to choose the .rst vertex of the light subpath. 
Each technique de.nes a probability distribution pk,m(7dJ(7 on paths of length k. We can compute pk,m(7explicitly 
by mul­tiplying the probabilities p(xidA(xiwith which the individual vertices were generated. Vertices 
that were chosen using a direc­tional distribution p(wdO(wcan be converted to the area measure using 
(9). To see why these distributions are good candidates for importance sampling, consider the integrand 
(17) for paths of length k. It is a product of many unrelated functions: Le, Wp, kdifferent Gfactors, 
and k_1different frfactors. Each bidirectional tech­nique includes a different subset of these factors 
in its sampling distribution; among them, we are more likely to generate paths that contribute signi.cantly 
to the image. We now have all the tools to combine samples from these tech­niques using the methods of 
Sec. 3: we can take a sample from any of the distributions pk,m, and given any path 7of length kwe can 
evaluate f(7and pk,m(7. Fig. 9 shows a scene that we used to test these ideas. Diffuse, glossy, and pure 
specular surfaces are present. Transport paths of lengths up to k=5were sampled using the bidirectional 
distribu­tions pk,mdescribed above. For ef.ciency, we randomly generate maximum-length eye and light 
subpaths in pairs. We then take sam­ples from all pk,mby joining each pre.x of the light subpath to each 
suf.x of the eye subpath. For example, to sample p2,1we concate­nate the .rst vertex of the light subpath 
and the last two vertices of the eye subpath. Each such group of samples is dependent, but this does 
not appear to signi.cantly affect our results. Another impor­tant optimization reduces the number of 
visibility tests between the eye and light subpaths, by using Russian roulette [6] to randomly suppress 
small potential contributions without adding bias. The .nal image in Fig. 9(b) was created by combining 
samples from all distributions using the power heuristic (with f=2). The image is 500 by 500 with 25 
samples per pixel. The weighted con­tribution from each technique is shown in the pyramid in Fig. 9(a). 
The pyramid does not show the complete set of sampling techniques; paths of length one are not shown 
because the light sources are not directly visible, and one column has been stripped from the left and 
right sides of each row because these images are virtually black (i.e. the weighted contributions are 
very small). Observe the caustics on the table, both directly from the spot­light and indirectly from 
re.ected light on the ceiling. The unusual caustic pattern to the left is caused by the square shape 
of the spot­light s emitting surface. Notice that some effects, such as caustics and specular re.ections, 
get their contributions almost entirely from one sampling technique. This says that the other techniques 
are very poor estimators of these contributions. For comparison, Fig. 9(c) shows standard MC path tracing 
with 56 samples per pixel (the same computation time as Fig. 9(b)). Di­rect lighting was used on all 
paths except for caustics, which were rendered by following paths right back to the light sources (the 
caustics would otherwise not be visible).  5 Discussion 5.1 Conclusions As we have shown, our methods 
for combining sampling techniques can substantially reduce the variance of Monte Carlo rendering calculations. 
These techniques are practical, and the additional cost is small less than 10% of the time in our tests 
was spent evaluating probabilities and weighting functions. We also have strong bounds on their performance 
relative to other combination strategies. Overall, we found that the power heuristic (with f=2)gave the 
best results. It is similar to the balance heuristic in general, but has signi.cantly lower variance 
when one of the piis a good match for f. When none of the given sampling distributions is a good match 
for f(e.g. Fig. 6), the differences among the various weighting strategies are small. 5.2 Choosing the 
number of samples First, observe that no strategy is greatly superior to that of simply setting all ciequal. 
If we are allocating Nsamples among n sampling techniques, it is easy to show that n_12 V[FF]:nV[F]+ 
F N F where Fuses the balance heuristic with all ciequal, and Fuses P any unbiased weighting functions 
and ci(satisfying iwi=1 and wi=0if ci=0). Thus, changing the cican improve the variance by at most a 
factor of n, plus a small additive term. On the other hand, a poor choice of the wi(e.g. a poor partitioning 
of the integration domain) can increase variance by an arbitrary amount. Also, there are situations where 
the ciare naturally constrained. For example, in bidirectional path tracing it is more ef.cient to take 
one sample from all distributions at once (Sec. 4.3). In the glossy highlights example, the ciare constrained 
because the samples are used for other purposes (direct lighting samples for the diffuse com­ponent, 
and directional samples for glossy re.ections of objects other than light sources). Often these other 
purposes will dictate the number of samples taken. In this case, by taking a weighted combination of 
both types of samples we can reduce the variance of the highlight calculation essentially for free. 
5.3 Comments on direct lighting The examples of Sec. 4.1, 4.2 are essentially direct lighting prob­lems. 
They differ only in the terms of the re.ectance equation that cause high variance the BRDF, the 1/r 2distance 
term, or the emitted radiance distribution Le. In Sec. 4.2, we used a simple light source sampling tech­nique. 
Although there are more sophisticated techniques for direct lighting[13], it can still be useful to combine 
several kinds of sam­ples. Observe that any strategy for sampling a group of patches as light sources 
induces some probability distribution on the patch surfaces. Since these strategies are always approximations, 
some factors of the re.ectance equation (2) will not be approximated well. In parts of the scene where 
these omitted factors become dominant, simple directional sampling can be more ef.cient. By combining 
both kinds of samples, we can make such strategies more robust. Shirley and Wang[12] also compare directional 
and area sampling techniques for glossy highlights (Sec. 4.1). They analyze a speci.c Phong-like BRDF 
and light source sampling method, and derive an expression for when to switch from one to the other (as 
a function of surface roughness and light source solid angle). In contrast, our methods work for general 
BRDF s and sampling techniques, and can combine samples from any number of distributions. 5.4 Approximating 
the weighting functions The models in Sec. 3 assume that given a sample Xifrom distribu­tion pi, we can 
compute pj(xexactly for all other j. Sometimes this is problematic e.g. pj(xmay be expensive or complicated 
to evaluate. More dif.culties arise when a sampling technique pj uses random numbers that cannot be determined 
from the resulting sample point x. For example, some direct lighting strategies[13] generate several 
candidate sample points xi, and then choose one randomly. Given an arbitrary point x, itis dif.cultto 
evaluate pj(xbecause this probability depends on information other than the sample location xitself. 
The easiest way to handle these problems is to recall that the P results are unbiased as long as iwi(x=1. 
When computing 0 the wi, it is perfectly reasonable to use an approximation pjof the true probabilities 
pj. This will give unbiased results even if the 0 approximations pjare poor, as long as they are consistently 
used 0 (i.e. pj(Xidoes not depend on i). Of course, poor approximations may lead to increased variance. 
Note that pi(Ximust always be evaluated exactly in (11) to avoid bias; however this is required of any 
unbiased Monte Carlo algorithm. 5.5 Future work We would like to explore other applications where it 
makes sense to use several sampling distributions. Even within the framework of global illumination, 
there are many such problems. For example, bidirectional path tracing can be used to estimate the coef.cients 
of basis functions de.ned on scene surfaces (let Wpin (5) be the dual basis function). This is an unexplored 
alternative to particle tracing models for Monte Carlo radiosity, and may be an effective solution to 
the problem of patches that do not receive enough particles. We think that there is great potential for 
designing better sampling distributions we hope that the existence of good methods to com­bine the samples 
will spur further work in this area. Again, global illumination provides a rich framework, because of 
the complexity of the domain and the integrand. Another interesting problem is how to choose the ci.One 
re­search area is the derivation of apriori rules for speci.c applications (similar to [12]). Another 
goal is to .nd strategies for the general case; adaptive methods seem promising here. Note that adaptive 
methods can introduce bias, unless two-stage sampling is used [7].  Acknowledgments Thanks to Pat Hanrahan, 
Marc Levoy, Luanne Lemmer, and the anonymous reviewers for helpful comments that improved the pre­sentation. 
Discussions with John Tukey were also useful. Thanks to Bill Kalsow for answering lots of questions about 
Modula-3 [10], the language we used for our rendering system. This research was supported by the National 
Science Foundation (CCR-9215219), the Digital Systems Research Center, and the Digital External Research 
Program. References [1] J. Arvo and D. Kirk. Particle transport and image synthesis. Computer Graphics 
(SIGGRAPH 90 Proceedings), 24, 63 66 (1990). [2] S. Chen, H. Rushmeier, G. Miller, and D. Turner. A progressive 
multi­pass method for global illumination. Computer Graphics (SIGGRAPH 91 Proceedings), 25, 165 174 (1991). 
[3] M. Cohen and J. Wallace. Radiosity and Realistic Image Synthesis. Academic Press, 1993. [4] R. Cook, 
T. Porter, and L. Carpenter. Distributed ray tracing. Computer Graphics (SIGGRAPH 84 Proceedings), 18, 
137 146 (1984). [5] J. Kajiya. The rendering equation. Computer Graphics (SIGGRAPH 86 Proceedings), 20, 
143 150 (1986). [6] M. Kalos and P. Whitlock. Monte Carlo Methods, Volume I: Basics. J. Wiley, New York, 
1986. [7] D. Kirk and J. Arvo. Unbiased sampling techniques for image synthe­sis. Computer Graphics (SIGGRAPH 
91), 25, 153 156 (1991). [8] E. Lafortune and Y. Willems. Bi-directional path tracing. Proceedings of 
CompuGraphics, Alvor, Portugal, 145 153 (Dec. 1993). [9] E.Lafortune,Y.Willems.Atheoreticalframeworkforphysicallybased 
rendering. Computer Graphics Forum, 13(2), 97 108 (1994). [10] G. Nelson, editor. Systems Programming 
with Modula-3.Pren­tice Hall, 1991. An implementation of Modula-3 is available at http://www.research.digital.com/SRC/. 
[11] H. Rushmeier. Realistic Image Synthesis for Scenes with Radiatively Participating Media. Doctoral 
Thesis, Cornell University, May 1988. [12] P. Shirley and C. Wang. Distribution ray tracing: theory and 
prac­tice. Proceedings of the Third Eurographics Workshop on Rendering, Bristol, England, 33 44 (1992). 
[13] P. Shirley, C. Wang, and K. Zimmerman. Monte Carlo Techniques for Direct Lighting Calculations. 
ACM Transactions on Graphics,to appear. [14] E. Veach and L. Guibas. Bidirectional estimators for light 
transport. Proceedingsofthe Fifth EurographicsWorkshopon Rendering,Darm­stadt, Germany, 147 162 (June 
1994). Appendix A Proofs Proof of Thm. 1: The variance is nni XX 1 wi(Xi,j)f(Xi,j) V[F]VFi,jwhere Fi,j 
ni pi(Xi,j) i=1 j=1 nni nni XXXX 1 1 E[F2]­ 2 i,j2 E[Fi,j]2 n n i i i=1 j=1 i=1 j=1 where the covariance 
terms are zero because the Xi,jare sampled indepen­dently. We bound the two terms separately. For the 
.rst term, we get Z nni n XXX 2 1 wi(x)f2 (x) 2 i,j E[F2 ] di(x) nnipi(x) i 1 i=1 j=1 i=1 Using the method 
of Lagrange multipliers, we minimize the integrand inde- P pendently at each point xsubject to the condition 
wi1. Noting that i f2 (x)is a constant and dropping xfrom our notation, we must minimize X w X 2 i +Awi-1nipi 
ii Setting all n+1partial derivatives to zero, we obtain wiwFi(12). Thus no other weighting strategy 
can reduce this term further. The second term makes a negative contribution to the variance, so we will 
prove an upper bound F 2/mininifor the wiand a lower bound P F 2/niwi. Letting ii for the FE[Fi,j](this 
is independentof j), for i the upper bound we have nni n n XXXX 1 11 i2 i2 i2 ; iii 2 nni minini i i=1 
j=1 i=1 i=1 P P 2 Since iiF,we have maxiii;F, and thus i2 ;Fwhich i ii PP 2 proves the upper bound. The 
lower bound iF2/niF/niis iii easily proven with Lagrange multipliers. 2 Proof of Thm. 2: Because Fis 
.xed in (8), it is enough to minimize the second moment E[F2].We have Z n X 2 w(x)f2 (x)E[F2 ] idi(x)' 
cipi(x) 1i=1 which is virtually identical to the second moment term that we minimized in the proof of 
Thm. 1. We also present worst-case bounds for the weighting heuristics from Sec. 3.4. The bounds have 
the form 11 2 V[FF];cV[F*]+ -P F' minini ni i where FFuses the indicated heuristic, and F*uses the (unknown) 
optimal weighting functions. For the cutoff heuristic, we can show c1+O(n-1), while for the power heuristic 
we can show 1 11/ c1+((n-1)((-1))( p 1 When (2, we can prove the stronger bound c(1+n). 2  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218499</article_id>
		<sort_key>429</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>53</seq_no>
		<title><![CDATA[Analytic antialiasing with prism splines]]></title>
		<page_from>429</page_from>
		<page_to>436</page_to>
		<doi_number>10.1145/218380.218499</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218499</url>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Intensity, color, photometry, and thresholding</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.5.4</cat_node>
				<descriptor>Signal processing</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10003247</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Signal processing systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP48023113</person_id>
				<author_profile_id><![CDATA[81100112613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[McCool]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>808589</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Amanatides, John. Ray Tracing with Cones. Computer Graphics (SIGGRAPH '84 Proceedings), 18(3): 129-135, July 1984.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807360</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin E. A Hidden-surface Algorithm with Antialiasing. Computer Graphics (SIGGRAPH '78 Proceedings), 12(3):6-11, August 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Dahmen, Wolfgang, Charles A. Micchelli, and Hans-Peter Seidel. Blossoming Begets B-spline Bases Built Better by B- patches. Math. of Comp., 59(199):97-115, July 1992.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>173337</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[de Boor, Carl, Klaus H611ig, and Sherman Riemenschneider. Box Splines. Academic Press, 1994.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Duff, Tom. Polygon Scan Conversion by Exact Convolution. In Jacques Andr6 and Roger D. Hersch, editors, Raster Imaging and Digital Typography, pages 154-168. Cambridge University Press, 1989.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot A., Marc Levoy, and Robert L. Cook. Synthetic Texturing using Digital Filters. Computer Graphics (SIG- GRAPH '80 Proceedings), 14(3):294-301, July 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>155295</ref_obj_id>
				<ref_obj_pid>155294</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Fong, Philip and Hans-Peter Seidel. An Implementation of Multivariate B-spline Surfaces over Arbitrary Triangulations. In Proceedings of Graphics Intelface '92, pages 1-10, May 1992.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Goodman, T. N. T. Polyhedral Splines. In Collection: Computation of Curves and Sulfaces (Puerto de la Cruz, 1989), volume 307 of NATO Adv. Sci. Inst. Ser. C: Math. Phys. Sci., pages 347-382. Kluwer Acad. Publ., Dordrecht, 1990.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>325184</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Grant, Charles W. Integrated Analytic Spatial and Temporal Anti-aliasing for Polyhedra in 4-space. Computer Graphics (SIGGRAPH '85 Proceedings), 19(3):79-84, July 1985.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gr~nbaum, Branko. Convex Polytopes. John Wiley &amp; Sons, 1967.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul S. and Pat Hanrahan. Beam Tracing Polygonal Objects. Computer Graphics (SIGGRAPH '84 Proceedings), 18(3):119-127, July 1984.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806784</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kajiya, James T. and Mike Ullner. Filtering High Quality Text for Display on Raster Scan Devices. Computer Graphics (SIGGRAPH '81 Proceedings), 15(3):7-15, August 1981.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617531</ref_obj_id>
				<ref_obj_pid>616009</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson L. Antialiasing Scan-line Data. IEEE Computer Graphics and Applications, 10(1): 18-30, January 1990.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>269783</ref_obj_id>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[McCool, Michael D. Analytic Signal Processing for Computer Graphics using Multivariate Polyhedral Splines. PhD thesis, University of Toronto, Department of Computer Science, 1995. Also available as Technical Report CS-95-05 from the University of Waterloo, Department of Computer Science, or from ftp: //dgp. utoronto, ca:/pub/mccool]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Unser, Michael, Akram Aldroubi, and M. Eden. B-spline Signal Processing. IEEE Transactions on Signal Processing, 41(2):821-848, February 1993.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>107217</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Watt, Alan and Mark Watt. Advanced Animation and Rendering Techniques. Addison-Wesley, 1992.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Analytic Antialiasing with Prism Splines Michael D. McCool. Computer Graphics Laboratory, University 
of Waterloo ABSTRACT The theory of the multivariate polyhedral splines is applied to an­alytic antialiasing: 
a triangular simplex spline is used to represent surface intensity, while a box spline is used as a .lter. 
Their con­tinuous convolution is a prism spline that can be evaluated exactly via recurrence. Evaluation 
performance can be maximized by ex­ploiting the properties of the prism spline and its relationship to 
the sampling grid. After sampling, digital signal processing can be used to evaluate exactly and ef.ciently 
the sampled result of any analytic spline .lter in the span of the box spline basis used as the original 
analytic .lter.  1INTRODUCTION Splines are most often used in computer graphics as modelling prim­itives. 
However, their powerful approximation and signal process­ing properties can also be used to advantage 
in the representation and processing of image intensities. A long-standing problem in computer graphics 
is aliasing: the appearance of jaggies, Moir´e patterns, and other undesirable arti­facts caused by undersampling. 
Under the assumptions of linear systems theory, aliasing can be eliminated by convolving the im­age with 
a low pass .lter before sampling [16]. For images, doing such .ltering exactly requires a multivariate 
integration, which is not only often analytically dif.cult but requires a representation of the image 
as a generalized function de.ned over the real plane. The multivariate polyhedral splines provide a mechanism 
for both the representation of the image and the exact analytic computation of the convolution. Polyhedral 
splines are formed from sums of multivariate piece­wise polynomial basis functions de.ned by the projection 
of n­dimensional convex polyhedra (or more correctly, polytopes) into m-dimensional space, with m:n. 
Some examples are given in Figure 1. Polyhedral spline basis functions are generalizations of the univariate 
B-spline basis functions; the latter can be viewed either as projections of simplicesyor (in the uniform 
case) hyper­cubes. An important property of the polyhedral splines is the fact that convolving two polyhedral 
spline basis functions is equivalent to projecting the Cartesian product of the two de.ning polytopes. 
*Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1, (519) 888-4567 
x4422, mmccool@cgl.uwaterloo.ca http://www.cgl.uwaterloo.ca/ mmccool/ yA simplex is a generalized triangle, 
de.ned as the convex hull of n1 points in R n.See Gr¨unbaum [10]. Permission to make digital/hard copy 
of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 This elegant convolution property can be 
used to derive .ltering al­gorithms for analytic antialiasing. Our result advances the state of the art 
by providing not only high quality analytical antialiasing but also a connection to a variety of useful 
spline techniques. Polyhedral spline techniques can also po­tentially be extended to other rendering 
tasks that require multivari­ate integration. AB C D Figure 1: Some sample polyhedral spline basis 
functions and maps of their discontinuities. Multivariate generalizations of B-spline ba­sis functions, 
polyhedral spline basis functions are projections of n­dimensional convex polytopes. On the left (A,B) 
we show two box splines, which are projections of hypercubes. The prism splines on the right are formed 
by (C) convolving a triangle with a segment (a degenerate 1D box spline) and (D) a square (a 2D box spline). 
 2BACKGROUND AND PRIOR ART Both antialiasing and polyhedral splines have had a long history of development 
in their respective subdisciplines. Here a short review of the relevant theory is given, along with some 
highlights of the literature. 2.1 Analytic Antialiasing We review only analytic or near-analytic antialiasing 
techniques here, and assume the reader is familiar with the linear theory de­scribing the causes of aliasing 
[16]. Approximate supersampling and subdivision techniques for antialiasing are also important, but are 
beyond the scope of this paper, as are speci.c techniques for texture antialiasing. We also only consider 
linear convolutional an­tialiasing, although other, nonlinear approaches are possible [12]. Feibush, 
Levoy and Cook [6] proposed a table based near­analytic antialiasing technique that could .lter constant 
shaded polygons with radially symmetric .lter functions. This approach was based on the precomputation 
of antialiased triangles and the decomposition of general polygons into summations of triangles. Grant 
[9] generalized this technique to antialiased motion blur, us­ing a tetrahedral decomposition of space-time. 
Area sampling, as proposed by Catmull [2], was the .rst truly an­alytic antialiasing technique. Area 
sampling clips polygons to the extent of each pixel and weights the contribution of each polygon to the 
pixel by its clipped area. This technique can only support constant shaded polygons and box .ltering, 
and is not entirely suc­cessful at removing aliasing artifacts. By scaling the .lter, Catmull could approximate 
motion blur but without proper interpenetration or occlusion. Extensions of area sampling include beam 
[11] and cone sampling [1], which are generalizations of ray tracing. Duff [5] extended Catmull s approach 
by replacing the area com­putation with a contour integral. He also describes several opti­mizations 
that exploit the coherence of scan-conversion. In theory, Duff s technique can handle any .lter function 
and any intensity variation as long as the required integrals are computable. How­ever, both Duff and 
Catmull s techniques require potentially expen­sive clips to the boundaries of each pixel. Finally, Max 
[13] proposed a hybrid scanline technique which an­alytically antialiases one dimension and supersamples 
in the other.  2.2 Polyhedral Splines Only a short sketch of the relevant polyhedral spline theory is 
possi­ble here. The reader is referred to the references [4, 8, 14] for more information. Polyhedral 
spline basis functions are based on projections of n­dimensional convex polytopes into n-dimensional 
space. Simplices lead to simplex splines, hypercubes lead to box splines, and in.nite polyhedral cones 
lead to cone splines. The cone splines are a gen­ n eralization of the truncated power functions [max(xF0)]used 
in univariate B-spline theory. nm If 2is an mXnprojection matrix that maps Rto Rand B m is a convex n-dimensional 
polytope, then at the point x2Rthe value of the polyhedral spline MB(xj2)is givenzby -1 MB(xj2) voln-m2xB. 
Here we use the set-valued inverse 2-1,where 2-1 xis an (n-m)­dimensional af.ne subspace given by fz:x2zg; 
thus the value of a polyhedral spline at a point is equal to the (n-m)-dimensional volume of the slice 
of the de.ning polytope that projects to that point. Polyhedral splines satisfy the following recurrence 
[8] which is a generalization of the de Boor recurrence for univariate B-splines. Let the convex n-dimensional 
polytope Bhave sfaces Bieach of dimension n-1. Let each face Bihave normal ni,and let bibe any point 
in the plane of Bi.Then Bilies in the af.ne subspace given n by all points ysuch that ni.(bi-y)0.Let 
z2Rbe any point that satis.es x2zwith x2R m.Then s X (n-m)MB(xj2) ni.(bi-z)MBi(xj20 i). i=1 Here 20is 
a projection of 2to the plane of Bi. i Box splines are projections of the unit hypercube n Xni=1[0F1]; 
they have the following property, which is reminiscent of the recursive convolutional de.nition of the 
uniform univariate zThis is an informal de.nition. Formal de.nitions are based on a test function ¢from 
a given continuity class and the distributional identity RR MB(xj3)¢(x)dx=¢(3z)dz. mn RR B-splines: 
M0(x) 8(x); Z 1 M (xj2) M (x-Tej2ne)dT. n n.1nn 0 Here 2nerefers to the removal of the column vector 
efrom 2. nn The column vectors eof the projection matrix 2can be interpreted i as the directions in 
which the spline is smeared during each step of the above recursion. Often, these direction vectors will 
be repeated, in which case we say eis of multiplicity ji. It should be noted that i the m-dimensional 
uniform tensor product B-spline basis is a box spline with only munique direction vectors; the multiplicity 
of each direction vector determines the order of the spline in that direction. A simpler evaluation recurrence 
can be derived speci.cally for box splines. Assume that there are runique vectors e22, each i with multiplicity 
ji. Choose mvectors eF...Fefrom 2that j1 jm form a basis of R m, and invert this basis to .nd zsuch 
that x2z, P m with x e; set the remaining n-melements of zto k=1zjkjkn zero and index them with fjkgm+1. 
Now the recurrence is given by (n-m)M n (xj2) m X zjkM n.1 (xj2nejk) k=1 m X +(jjk-zjk)M(x-ej2ne) n.1jkjk 
k=1 n X +jjkM n.1(x-ejkj2nejk). k=m+1 The recursive convolution property satis.ed by box splines is 
an instance of an important general theorem. If MF(xjF)and MG(xjG)are polyhedral splines, then their 
convolution is a poly­hedral spline derived by forming the Cartesian product of the poly­topes Fand Gand 
concatenating the projection matrices: MF(xjF)*MG(xjG) MFG(xj[FFG]). When Fis the interval [0F1]and 
Gis a unit hypercube, the convo­lution property of the box splines given above develops. However, if 
the polytopes Fand Gare of different types, a hybrid is formed. A prism spline is a hybridization of 
a simplex spline and a box spline. 3PRISM SPLINE EVALUATION Simplex splines can support the approximation 
of functions de.ned on arbitrary triangular grids [3, 7]. Images that arise in rendering can be cast 
in this form by tessellation of curved objects, geometric hid­den surface removal, and .nally subdivision 
of any non-triangular polygons. Gouraud shading corresponds to the use of linear inter­polation and can 
be evaluated with a sum of three linear simplex splines. A linear simplex spline is the projection of 
a tetrahedron into two dimensions. If two corners of the tetrahedron project to a single vertex of a 
triangle, a function results which has a triangu­lar support, is zero at the corners corresponding to 
the projections of the single vertices of the tetrahedron, and ramps linearly up to the remaining double 
vertex corner. By mapping the extra vertex of the tetrahedron to each corner of a triangle in turn we 
derive the ba­sis functions for linear interpolation. Gouraud shaded triangles will provide the examples 
we will use in our results. Other choices, in­cluding higher order simplex splines, are also possible. 
Once the image function has been described as a summation of simplex spline basis functions, it can be 
.ltered by convolving each simplex spline with a box spline .lter. The .ltered image is thus a summation 
of prism splines. An evaluation recurrence for a prism spline can be derived by slicing a higher-dimensional 
box spline [14]. This is convenient be­cause the box spline already has a simple evaluation recurrence 
and we can avoid a derivation of the prism spline recurrence from .rst principles. However, the basic 
recurrence is too expensive, so in Section 3.2 we give several techniques for optimizing the recurrence 
statically and dynamically. Term reuse is particularly important, but we also give incremental algorithms 
which are especially useful in the context of a renderer. 3.1 Prism Spline Recurrence We can evaluate 
a prism spline by slicing a box spline of one higher dimension by a hyperplane. Consider Figure 2. Let 
wbe the last element of each three-dimensional direction vector (i; a three­dimensional box spline is 
described by a matrix 8of ndirection vectors (iarranged as column vectors. Relative to the slice w1, 
direction vectors describing a simplex spline emanate from the ori­gin to the plane w1, while direction 
vectors describing box spline .lters are parallel to this plane and have w0. Figure 2: A prism spline 
can be interpreted as a slice of a box spline. Here the derivation for a constant triangle and a box 
.ltered constant triangle are shown as particular slices of a box spline. Direction vec­tors from the 
origin to the plane de.ne the triangle, while direction vectors parallel to the plane de.ne the .lter. 
Given a matrix 2of ntwo-dimensional direction vectors ede­ i scribing a box spline .lter and a set Vof 
k+1two-dimensional points vjdescribing the vertices of a simplex spline, form the m+ 1Xn+k+1matrix 8as 
follows: ee vv 1 k+1 81 ...n .... 0011 Choose a basis Zof m+1linearly independent vectors from 8, and 
compute zZ -1[xF1]Tfor x2R m. Then a recurrence for the (n+k)th order prism spline Pn+k(xj2FV)is given 
by (n+k-m)Pn+k(xj2FV) X ziPn-1+k(xj2neiFV) e 0]T2Z i X +(ji-zi)Pn-1+k(x-eij2neiFV) e 0]T2Z i X + jiPn-1+k(x-eij2neiFV) 
e 0]T62Z i X + zjPn+k-1(xj2FVnvj). v 1]T2Z j Suppose that we wish to render an image with antialiased 
Gouraud shaded, i.e. linearly interpolated, triangular primitives. We will begin with constant triangles, 
.lter them, and then gener­alize to triangular linear interpolation basis functions. With 8111[(1F(2F(3],let 
M111(xFyFw)M3(xFyFwj8111). The subscript 111is used to indicate the unit multiplicity of each of the 
direction vectors (i[viF1]T . P111(xFy)M111(xFyF1) de.nes an un.ltered constant triangle. Convolution 
of this triangle with a tensor product B-spline for the purposes of .ltering is accom­plished by adding 
the appropriate direction vectors with w0: M111nn(xFyFw)  nn ee xy M111(xFyFw)*M x 2n 0 0 M 3 2n(xj[8F(nF(n])F 
xy where e[1F0]Tand e[0F1]Tare packed into (xand (y xy and each is repeated ntimes. Slice this .ltered 
box spline to obtain a .ltered triangle: P111nn(xFy)M111nn(xFyF1). Finally, we repeat each of the vectors 
de.ning the corners of the triangle in turn to obtain symmetric basis functions for a .ltered linear 
interpolant: P211nn(xFy), P121nn(xFy),and P112nn(xFy). A two-dimensional version of this process is shown 
in Figure 3. Figure 3: In the upper row, the constant and linear interpolation ba­sis functions in one 
dimension are derived by slicing a two dimen­sional box spline. By adding direction vectors parallel 
to the plane of the slice, we can .lter these basis functions, as shown in the lower row. The zjkfactors 
used in the recurrence are coordinates relative to each of the possible choices of direction vectors. 
In order to com­pute the zjkneeded in each term of the recurrence, we have to invert ten (5choose 3) 
3X3matrices. However, most of these inverses are trivial as a consequence of the special form of(yand 
(x. If only one of these vectors is chosen in a given basis, then the correspond­ing inverse computes 
coordinates of a point relative to an edge of the triangle. If two are chosen, coordinates are evaluated 
relative to the corner of a triangle, and the transformation amounts simply to a translation. The one 
non-trivial inverse involves (1, (2,and (3but this is simply an evaluation of the barycentric coordinates 
of a point within the triangle v1v2v3, and can be evaluated through Gouraud interpolation. As with Gouraud 
interpolation, the zjkcoordinates for every choice of three basis vectors from 8can be updated incrementally; 
in addition to the standard barycentric coordinates, six other coordi­nate systems de.ned relative to 
the edges need to be updated. The corner coordinates are so inexpensive to compute that incremental update 
is normally not advantageous. A problem can arise if an edge of a triangle is perfectly (or nearly) horizontal 
or vertical; one of the edge coordinate systems will col­lapse. The theoretically justi.ed solution is 
to set the offending term of the recurrence to zero, as the corresponding transformed polytope has zero 
volume. At polytope edges (which project to discontinu­ities in the spline) it is also important to evaluate 
only one of the incident faces. This can be accomplished by treating the supports of the lowest level 
constant box splines as semi-open.  3.2 Recurrence Optimization Simply following the prism spline evaluation 
recurrence blindly will lead to a combinatoric-time algorithm unsuitable for a render­ing system. Fortunately, 
because of the form of the vectors (xand (ythe recurrence is redundant and the following optimizations 
can be made: 1. Terms can appear more than once in the evaluation tree. By sharing terms the tree can 
be converted into a directed acyclic graph (DAG), resulting in a polynomial-time algorithm. 2. Many 
terms in the recurrence are zero for any given evalua­tion point. This can be exploited dynamically by 
checking the current evaluation point against the convex support of the sub­spline. 3. If a prism spline 
is sampled on a grid de.ned by some of its direction vectors, terms in previous recurrence evaluations 
can be reused. This leads to incremental algorithms. 4. Only two of the basis functions P211nn, P121nn,and 
P112nn need to be evaluated and combined with P111nn, which would be part of either recurrence tree anyway. 
 5. The two remaining basis functions share many of the same re­currence terms, and can be evaluated 
simultaneously. 6. Recurrence terms and coordinate system evaluations from adjacent primitives can be 
reused, particularly along shared edges. 7. A decomposition of the underlying sliced box spline into 
a dif­ference of truncated power functions (cone splines) leads to an incremental algorithm that eliminates 
the need to remem­ber terms from previous recurrences. Unfortunately, this opti­mization risks some numerical 
instability and precludes most other optimizations. 8. Terms of the form P001.k, P010.kand P100.kcan 
be eval­uated as tensor product B-splines. Likewise, terms of the form P011.0, P101.0, P110.0, P0110k, 
P1010k,and Pcan  1100k be evaluated using univariate B-splines. Unfortunately, for higher orders of 
.lters both kinds of terms become relatively sparse. 9. Finally, recurrence terms can be evaluated and 
statically opti­mized of.ine. A code generator can be used to de.ne the inner loop of the scan convertor. 
Some of these optimizations are in con.ict. For example, simul­taneously applying both support culling 
(2) and the cone spline op­timization (7) is not worthwhile, because cone splines have semi­in.nite support. 
Likewise, not all these optimizations lead to the same increase in performance. Some attempt has been 
made to present the most useful optimizations .rst in the list above. In the following sections some 
additional discussion is given to the most useful but less obvious optimizations: recurrence term reuse 
(1), support culling (2) and incremental algorithms (3). 3.2.1 Recurrence Term Reuse The recurrence 
for the prism spline is similar to the de Boor recur­rence for the univariate B-splines. The latter is 
an ef.cient evalua­tion technique because of the existence and reuse of repeated terms. Consider Figure 
4, which shows an application of the de Boor re­currence to evaluate a segment of an order nB-spline 
curve. This algorithm is O(n 2)because of the shared terms; without sharing, it would be O(2n). Now consider 
Figure 5, which shows the recurrence for the prism spline P11111. The recurrence graph is no longer planar, 
but we see that some sharing does occur. For higher orders of .lter, the sharing becomes even more important. 
4-x x-2 4-x 3-x f[0] f[1] 01234 Figure 4: Points on B-spline curves can be evaluated using an O(n 
2)triangular algorithm. The evaluations shown are for the seg­ment of a uniform cubic B-spline over x2[3F4). 
Dashed lines show how a sample at the same joint-relative position, x+12 [4F5), can be evaluated incrementally 
in only O(n)time. The lin­ear weights at each level of the recurrence are shown here without normalization 
factors. The code generator implements term reuse by recording the ex­istence and name of every term 
as it is generated during a postorder traversal of the recurrence tree. This record is called a memo 
table. When a term is needed, the table is .rst checked to see if it has al­ready been computed. To implement 
sharing of terms between the recurrences for the prism splines P211nnand P121nnthe memo ta­ble is simply 
not cleared between the generation of terms. 3.2.2 Support Culling At a given point in the interior 
of the spline typically only a few terms in the recurrence are non-zero. In Figure 5, the non-zero terms 
for an evaluation point are emboldened. We can reduce the amount of work we have to do considerably if 
we can quickly determine which terms are non-zero. The supports of all subsplines are convex, and are 
intersections of a .nite number of halfspaces. The borders of these halfspaces, in turn, are translates 
of an even smaller number of lines. In the case of the tensor product prism spline, three lines are due 
to the edges of the triangle and two are the result of eand e. xy Point-against-convex-support tests 
can be implemented ef.­ciently with outcodes. First, the evaluation point is tested against each halfspace; 
the result (in or out) is stored as a bit in an integer. Figure 5: The recurrence tree of a prism spline 
evaluated at the point indicated with a dot is shown, with non-zero recurrence terms em­boldened. Note 
that shared faces are evaluated twice and that any particular point in the support of the spline only 
involves a limited subgraph of the recurrence tree. These tests can also take advantage of the redundancy 
that exists in the speci.cation of the halfspaces. Halfspaces that are just translates of one another 
can reuse partial results from previous tests, perhaps even short-circuiting later evaluations. Before 
each recurrence term is evaluated, the outcode integer is masked and compared to a bit pattern unique 
to that term. If the bit pattern does not match, the evaluation point is not inside the support of the 
term, and evalua­tion can be avoided. The interior of the triangle is a special case. Here we can drop 
back to simple linear interpolation (in the context of a linearly inter­polated prism spline) because 
in the absence of edge effects a lin­ear function is reproduced under convolution with a symmetric .l­ter. 
Of course, the interior of the triangle vanishes if the minimum dimension of the triangle is smaller 
than the .lter diameter.  3.2.3 Incremental Algorithms Figure 4 shows another optimization in the evaluation 
of the uni­form univariate B-spline: a new sample can be generated in O(n) time if it is spaced at a 
unit distance from the last sample and a subset of the old recurrence terms are recalled. Something similar 
can be done for the prism spline. The exis­tence of eand eresult in many recurrence terms shifted by 
1 in xy xand y. Rather than regenerate these shifted values, the unshifted values can be recalled from 
the last recurrence evaluation. Initial­ization requires setting recurrence terms outside the support 
of the spline to zero. Terms can be recalled for both xand y. This requires storage for an entire scanline 
of partial results and the resulting data move­ment may degrade performance. Scanline incremental algorithms 
remember terms only from the very last evaluation. This limits the performance improvement, but results 
in much less memory use and .ts in quite nicely with scanline rendering.  4DIGITAL POSTPROCESSING The 
prism splines only allow analytic .ltering with box spline ba­sis functions, including as a special case 
the tensor product B-spline basis functions. This is in fact suf.cient to support all analytic .l­ter 
functions that can be expressed as splines relative to these bases, because of the relationship (f(x)*¢(x))(i)f[i]*(B(x)*¢(x))(i). 
 which depends only on the linearity and shift-invariance of con­volution. Here *is continuous convolution, 
*is discrete convolu­tion, ¢(x)is an input signal, and f(x)is a .lter function de.ned by P f(x)f[i]B(x-i)relative 
to a basis function B(x). i No approximations are necessary. If we apply a digital .lter with coef.cients 
f[i]to samples of B(x)*¢(x)at the integers, we will achieve exactly the same result as if we had originally 
applied the .lter f(x)analytically to ¢(x)and then sampled the result at the in­tegers. Since we have 
made no assumptions in the above regarding the dimension of the parameter space, this result also applies 
in mul­tiple dimensions. The result actually applies to any shift-invariant basis function but is most 
useful in the context of a tensor product B-spline basis because of its separability. Useful analytic 
.lters with in.nite support relative to the separa­ble tensor product B-spline basis can be represented 
with ef.cient univariate recursive digital .lters. Recursive .lters reuse old output values to obtain 
in.nite impulse responses with a .nite amount of arithmetic. For example, the inner loop of a causal 
recursive .lter with input sequence g[i]and output sequence f[i]is j1 m XX f[i];ajg[i-j]+bkf[i-k]for 
iincreasing. j=j0 k=1 An anticausal .lter runs in the opposite direction: j1 m XX f[i];cjg[i-j]+dkf[i+k]for 
idecreasing. j=j0 k=1 A linear .lter has m1and so recycles only one output value. A quadratic .lter 
recycles two output values with m2. The fundamental spline and the smoothing spline bases are useful 
alternative antialiasing .lters and can be generated with cascaded causal and anticausal recursive .lters 
[15] with m1and m2 respectively. Fundamental splines interpolate their control points. A recursive .lter 
for converting the control points of a fundamental spline to those for an equivalent B-spline can be 
found by inverting the rela­ *bn c tionship g[i]f[i][i].Here the g[i]are the samples to interpo­ n c 
 late and the discrete centered B-spline basis bn c[i]B(i)with Bn cn-n c (x)B(x+n/2).Let b[i]be the convolutional 
inverse of bn c [i]; it can be generated with a pair of linear causal/anticausal recursive digital .lters 
applied to an impulse 8[i]. Each of these re­cursive .lters only needs to recall one past output value. 
The sequence b -n c[i]when used as the control points for an or­der nB-spline gives the order nfundamental 
spline basis function. This function converges to the sinc function as the order nincreases, and is an 
excellent approximation to the perfect low-pass .lter. If we reconstruct the image with a tensor product 
B-spline as well and ap­ply the fundamental spline .lter twice, we in theory achieve a min­imum least-squares 
representation of the geometric image. Unfor­tunately, this representation rings excessively. Let ¢(x)be 
a signal and g(x)an order 2rspline approximation. The smoothing splines are de.ned as the spline functions 
that min­imize the linear functional Z dr2 g(x) 2(gF¢)(g(x)-¢(x))2 +Adx. dxr R for any positive integer 
rand positive real parameter A. Smoothing splines can be implemented with a pair of quadratic recursive 
.lters that need only remember two past output values. The parameter A controls the balance between the 
.t and the smoothness of the spline, thus providing some control over ringing. If we reconstruct g(x) 
with a fundamental spline rather than a B-spline basis, then A0 corresponds to the perfect least-squares 
reconstruction of ¢(x). These digital .lters can be applied to the output of any antialias­ing system 
that can compute an adequately accurate result relative to a spline basis .lter kernel. However, these 
.lters operate by en­hancing high frequencies. Approximate techniques, i.e. stochastic sampling, often 
hide noise at high frequencies, so enhancement will decrease the signal-to-noise ratio. Images generated 
through ana­lytic antialiasing will not suffer from this problem.  5RESULTS Our prototype implementation 
scan converts one primitive at a time and sums it into a frame buffer. This approach precludes sharing 
of recurrence terms along edges, and assumes that hidden surface removal has already been performed. 
The primitive scan conver­tor supports independent recurrence evaluation at each pixel as well as both 
scanline and xyincremental evaluation. Tensor product B­spline .lters of orders one (box) through four 
(cubic) were imple­mented. A .nely striped sphere tessellated into triangles was used as a test model. 
Use of a sphere permits hidden-surface removal with only back-face culling. The sphere was shaded at 
the vertices of each tri­angle with a Phong lighting model, and each triangle was Gouraud shaded. A point-sampled 
rendering of this model is given in the right half of Figure 6 at a resolution of 200X200. xOn average, 
a triangle covers 2.54pixels in this image over the support of the sphere. To control the reconstruction 
kernel and minimize postal­iasing and halftone interference artifacts, all images have been en­larged 
with a cubic B-spline reconstruction kernel to the resolution of the device.1 Several aliasing artifacts 
are obvious: Moir´e patterns due to the stripes meeting the sampling grid at an angle, jaggies along 
the edges of the stripes, Moir´e patterns at the silhouette due to foreshort­ening, and .nally similar 
problems at the pole of the sphere. In Figure 8 the test case has been rendered with .rst through fourth 
order tensor-product B-spline .ltering. Even the .rst order box .lter, corresponding to area sampling, 
is a signi.cant improve­ment. The cubic .lter essentially eliminates aliasing artifacts, but does result 
in some blurring of the image. To resolve the blurring problem, digital postprocessing can be applied. 
In Figure 7, quadratic and cubic fundamental spline ba­sis functions were used as the antialiasing .lters. 
A double appli­ xAn enlarged 50X50rendering is also shown on the left. 1Unfortunately, space restricts 
us from enlarging these images as much as we would like. Please consider examining the electronic versions 
on the CD-ROM for a critical comparison. cation of the corresponding digital .lter, in conjunction with 
the ap­propriate order of reconstruction kernel, results in a perfect least­squares .t of the output 
image to the geometric input. Unfortu­nately, this image (not shown) suffers from excessive ringing, 
occa­sionally going out of gamut. In Figure 9, the result of a regularized least-squares .lter is shown 
for various values of Aand r2.This .lter tends to the double cubic fundamental .lter as Atends to 0, 
but is controllable. At A0.05, this .lter results in a very high level of quality. 6PERFORMANCE The 
performance of the prism spline evaluation algorithm is pro­portional to the number of terms in the recurrence 
tree that need to be evaluated for each output pixel. The evaluation of each term is roughly proportional 
to one Gouraud interpolation, discounting data movement costs against overlap in the computation of the 
blending factors. Table 1 compares the number of recurrence terms for unopti­mized recurrences, scanline 
(x) incremental recurrences, xyincre­mental recurrences, recurrences with and without term reuse, and 
cone spline (truncated power function) recurrences. We did not im­plement zero term culling. Our .rst 
observation is that unoptimized recurrences are com­pletely unreasonable. Therefore any practical implementation 
must consider some form of optimization. Term reuse is the most straight­forward and least complex optimization 
to implement, and results in major savings. We also note that the number of terms in the cone spline 
is the same as the number of terms in the xyincremental eval­uation. This is to be expected, actually. 
Finally, we observe that the xyincremental evaluation uses about a third of the terms required by the 
scanline (x) incremental evaluation. This has to be balanced against the greater data movement in the 
xyincremental algorithm. In our implementation, the two variants of the incremental algorithm performed 
almost identically. These numbers need to be taken in context. A non-adaptive supersampling algorithm 
on a jittered 8X8grid would require (roughly) 64times as much computation as point sampling, and would 
still only be an approximation. A 3rd order xyincremental prism spline algorithm requires 84terms but 
would compute an ex­act, noise-free answer. We also need to emphasize that this is a pro­totype implementation; 
more optimizations remain to be exploited. 7FUTURE WORK The polyhedral spline framework can be extended 
in many direc­tions: Other polyhedral spline .lter functions ...  Better intensity approximants ... 
 Motion blur using tetrahedral decomposition [9] ...  Antialiased line and volume elements ...  Approximation 
of penumbra ...  Ef.cient execution also needs to be further addressed. Imple­mentation of the zero-cull 
optimization and sharing of terms be­tween adjacent primitives should result in another order of magni­tude 
improvement in performance. Implementation of the zero cull optimization should consider the hierarchical 
nesting of the supports of each of the subsplines. To implement interprimitive term shar­ing, hidden-surface 
removal needs to be closely tied to evaluation of the primitives so information about the relationships 
between prim­itives can be maintained. Terms in Optimized Recurrences nOptimization Number of Terms P211nnP121nnP112nn 
P111nn 1 none 29 29 29 6 2 none 669 669 669 148 3 none 12875 12875 12875 2539 4 none 224961 224961 224961 
39112 1 x incr 20 20 20 5 2 x incr 287 287 287 78 3 x incr 3020 3020 3020 712 4 x incr 28251 28251 28251 
5750 1 xy incr 13 13 13 4 2 xy incr 101 101 101 34 3 xy incr 487 487 487 142 4 xy incr 2149 2149 2149 
538 1 reuse 19 13 13 6 2 reuse 107 54 54 53 3 reuse 343 172 172 195 4 reuse 829 325 325 504 1 reuse, 
x incr 14 9 9 5 2 reuse, x incr 60 27 27 33 3 reuse, x incr 149 58 58 91 4 reuse, x incr 293 105 105 
188 1 reuse, xy incr 10 6 6 4 2 reuse, xy incr 32 13 13 19 3 reuse, xy incr 62 22 22 40 4 reuse, xy incr 
100 33 33 67 1 cone, no reuse 13 13 13 4 2 cone, no reuse 101 101 101 34 3 cone, no reuse 487 487 487 
142 4 cone, no reuse 2149 2149 2149 538 1 cone, reuse 10 6 6 4 2 cone, reuse 32 13 13 19 3 cone, reuse 
62 22 22 40 4 cone, reuse 100 33 33 67 Table 1: Number of terms in various optimized linear and constant 
prism spline recurrences. Also shown are the number of terms in the cone spline recurrence which could 
be used to evaluate the prism spline at some loss in stability. When reuse is enabled we merge the trees 
for the linear prism spline basis functions; the constant prism spline P111nntree is a subtree of the 
P211nntree. Since we only need the P211nn, P121nn,and P111nnterms to form a basis for lin­ear interpolation, 
add the number of terms under the P211nnand P121nncolumns to determine the total number of recurrence 
terms required.  8CONCLUSIONS An analytical antialiasing technique based on the theory of poly­hedral 
splines has been presented. Using the convolutional proper­ties of the polyhedral splines, we can derive 
evaluation recurrences for triangular simplex splines convolved with box spline .lters. We study and 
give results for a linearly interpolated triangle convolved with a tensor product B-spline .lter. Several 
optimization tech­niques are given which are essential to an ef.cient implementa­tion. Through digital 
signal postprocessing, the effect of any .lter spanned by shifted linear combinations of the basic box 
spline .lter can be evaluated. If the .lter is a tensor product B-spline then uni­variate recursive .lters 
can be used to evaluate a regularized least­squares .t of a spline reconstruction to the underlying geometrically 
de.ned image. 9ACKNOWLEDGEMENTS This research was conducted while I was a Ph.D. student under the capable 
supervision of Eugene Fiume at the Dynamic Graphics Project at the University of Toronto. I am indebted 
to Eugene and to all the people at DGP for providing such a stimulating and pro­ductive environment. 
DGP is supported by the National Science and Engineering Re­search Council of Canada and by the Information 
Technology Re­search Centre. My Ph.D. work was supported by an NSERC Post­graduate Scholarship. REFERENCES 
[1] Amanatides, John. Ray Tracing with Cones. Computer Graphics (SIGGRAPH 84 Proceedings), 18(3):129 
135, July 1984. [2] Catmull, Edwin E. A Hidden-surface Algorithm with Anti­aliasing. Computer Graphics 
(SIGGRAPH 78 Proceedings), 12(3):6 11, August 1978. [3] Dahmen, Wolfgang, Charles A. Micchelli, and Hans-Peter 
Seidel. Blossoming Begets B-spline Bases Built Better by B­patches. Math. of Comp., 59(199):97 115, July 
1992. [4] de Boor, Carl, Klaus H¨ollig, and Sherman Riemenschneider. Box Splines. Academic Press, 1994. 
[5] Duff, Tom. Polygon Scan Conversion by Exact Convolution. In Jacques Andr´e and Roger D. Hersch, editors, 
Raster Imag­ing and Digital Typography, pages 154 168. Cambridge Uni­versity Press, 1989. [6] Feibush, 
Eliot A., Marc Levoy, and Robert L. Cook. Synthetic Texturing using Digital Filters. Computer Graphics 
(SIG-GRAPH 80 Proceedings), 14(3):294 301, July 1980. [7] Fong, Philip and Hans-Peter Seidel. An Implementation 
of Multivariate B-spline Surfaces over Arbitrary Triangulations. In Proceedings of Graphics Interface 
92, pages 1 10, May 1992. [8] Goodman, T. N. T. Polyhedral Splines. In Collection: Com­putation of Curves 
and Surfaces (Puerto de la Cruz, 1989), volume 307 of NATO Adv. Sci. Inst. Ser. C: Math. Phys. Sci., 
pages 347 382. Kluwer Acad. Publ., Dordrecht, 1990. [9] Grant, Charles W. Integrated Analytic Spatial 
and Temporal Anti-aliasing for Polyhedra in 4-space. Computer Graphics (SIGGRAPH 85 Proceedings), 19(3):79 
84, July 1985. [10] Gr¨unbaum, Branko. Convex Polytopes. John Wiley &#38; Sons, 1967. [11] Heckbert, 
Paul S. and Pat Hanrahan. Beam Tracing Polygonal Objects. Computer Graphics (SIGGRAPH 84 Proceedings), 
18(3):119 127, July 1984. [12] Kajiya, James T. and Mike Ullner. Filtering High Quality Text for Display 
on Raster Scan Devices. Computer Graphics (SIGGRAPH 81 Proceedings), 15(3):7 15, August 1981. [13] Max, 
Nelson L. Antialiasing Scan-line Data. IEEE Computer Graphics and Applications, 10(1):18 30, January 
1990. [14] McCool, Michael D. Analytic Signal Processing for Computer Graphics using Multivariate Polyhedral 
Splines.PhD the­sis, University of Toronto, Department of Computer Science, 1995. Also available as Technical 
Report CS-95-05 from the University of Waterloo, Department of Computer Science, or from ftp://dgp.utoronto.ca:/pub/mccool 
[15] Unser, Michael, Akram Aldroubi, and M. Eden. B-spline Signal Processing. IEEE Transactions on Signal 
Processing, 41(2):821 848, February 1993. [16] Watt, Alan and Mark Watt. Advanced Animation and Render­ing 
Techniques. Addison-Wesley, 1992.  Figure 6: Point sampled 200X200and 50X50test images, recon­structed 
with a cubic B-spline kernel.   Figure 8: Test model analytically .ltered with tensor product B­spline 
.lters, orders one through four.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218500</article_id>
		<sort_key>437</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>54</seq_no>
		<title><![CDATA[Stratified sampling of spherical triangles]]></title>
		<page_from>437</page_from>
		<page_to>438</page_to>
		<doi_number>10.1145/218380.218500</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218500</url>
		<keywords>
			<kw><![CDATA[Monte Carlo]]></kw>
			<kw><![CDATA[solid angle]]></kw>
			<kw><![CDATA[spherical triangle]]></kw>
			<kw><![CDATA[stratified sampling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Monte Carlo</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010341.10010349</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003682</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Sequential Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003670.10003677</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic reasoning algorithms->Markov-chain Monte Carlo methods</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14183802</person_id>
				<author_profile_id><![CDATA[81100529394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arvo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, 580 Engineering and Theory Center Building, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BERGER, M. Geometry, Volume II. Springer-Verlag, New York, 1987. Translated by M. Cole and S. Levy.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>8927</ref_obj_id>
				<ref_obj_pid>7529</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[CooK, R. L. Stochastic sampling in computer graphics. ACM Transactions on Graphics 5, 1 (1986), 51-72.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. A rapid hierarchical radiosity algorithm. Computer Graphics 25, 4 (July 1991), 197-206.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>539488</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[RUBINSTEIN, R.Y. Simulation and the Monte Carlo Method. John Wiley ~z Sons, New York, 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>226151</ref_obj_id>
				<ref_obj_pid>226150</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[SHIRLEY, P., WANG, C., AND ZIMMERMAN, K. Monte carlo methods for direct lighting calculations. A CM Transactions on Graphics (1995). To appear.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[SPANIER, J., AND GELBARD, E. M. Monte Carlo Principles and Neutron Transport Problems. Addison-Wesley, Reading, Massachusetts, 1969.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>90772</ref_obj_id>
				<ref_obj_pid>90767</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[TURK, G. Generating random points in triangles. In Graphics Gems, A. S. Glassner, Ed. Academic Press, New York, 1990, pp. 24-28.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Strati.ed Sampling of Spherical Triangles James Arvo Program of Computer Graphics. Cornell University 
 Abstract We present an algorithm for generating uniformly dis­tributed random samples from arbitrary 
spherical triangles. The algorithm is based on a transformation of the unit square and easily accommodates 
strati.ed sampling, an ef­fective means of reducing variance. With the new algorithm it is straightforward 
to perform strati.ed sampling of the solid angle subtended by an arbitrary polygon; this is a fun­damental 
operation in image synthesis which has not been addressed in the Monte Carlo literature. We derive the 
re­quired transformation using elementary spherical trigonom­etry and provide the complete sampling algorithm. 
CR Categories and Subject Descriptors: I.3.5 [Com­putational Geometry and Object Modeling]: Geometric 
Al­gorithms. Additional Key Words and Phrases: Monte Carlo, solid angle, spherical triangle, strati.ed 
sampling. 1 Introduction Monte Carlo integration is used throughout computer graph­ics; examples include 
estimating form factors, visibility, and irradiance from complex or partially occluded lumi­naires [3, 
5]. While many specialized sampling algorithms exist for various geometries, relatively few methods exist 
for sampling solid angles; that is, for regions on the unit sphere. The most common example that arises 
in computer graphics is the solid angle subtended by a polygon. We attack this problem by solving the 
sub-problem of sampling a spherical triangle. The new sampling algorithm can be formulated using el­ementary 
spherical trigonometry. Let T be the spherical triangle with area Aand vertices A, B and C.Let a, b,and 
cdenote the edge lengths of T, and let a, f,and ,denote the three internal angles, which are the dihedral 
angles be­tween the planes containing the edges. See Figure 1a. To generate uniformly distributed samples 
over T we seek a bi­jection f[0 1]2-T with the following property: given any two subsets S1 and S2 of 
the unit square with equal areas, f(S1)and f(S2)will also have equal areas. The function fcan be derived 
using standard Monte Carlo methods for sampling bivariate functions; for example, see Spanier and .580 
Engineering and Theory Center Building, Ithaca, New York 14853, http://www.graphics.cornell.edu Permission 
to make digital/hard copy of part or all of this work for personal or classroom use is granted without 
fee provided that copies are not made or distributed for profit or commercial advantage, the copyright 
notice, the title of the publication and its date appear, and notice is given that copying is by permission 
of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires 
prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 Gelbard [6] or 
Rubinstein [4]. To apply these methods to sampling spherical triangles we require the following three 
identities: A=a+f+,-7 (1) cosf=-cos,cosa+sin,sinacosb (2) cos,=-cosfcosa+sinfsinacosc (3) The .rst is 
known as Girard s formula and the other two are spherical cosine laws for angles [1]. 2 The Sampling 
Algorithm The algorithm proceeds in two stages. In the .rst stage bb we randomly select a sub-triangle 
T CTwhose area Ais uniformly distributed between 0 and the original area A.In the second stage we randomly 
select a point along an edge of the new triangle. Both stages require the inversion of a probability 
distribution function. B z C 0 [C | B] (a) (b) b Figure 1: (a) The vertex C is determined by specifying 
the area of the sub-triangle. (b) The point P is then chosen to lie along the arc between b C and B. 
bb Sub-triangle T is formed by choosing a new vertex C on the edge between A and C, as shown in Figure 
1a. The sample point P is then chosen in the arc between B and b C.The point P is determined by its distance 
efrom B and by the length of the new edge b b; these values are computed using the conditional distribution 
functions b A 1-cose F1( b b) and F2(ejb) b A 1-cosa b where both Aab.Given two b and bare taken to be 
functions of b random variables 6and 6uniformly distributed in [0 1],we 12 .rst .nd b such that F1(b 
)=61 , and then .nd esuch that bb bb F2(ejb)=6.Then bwill be distributed with a density 2 proportional 
to the di.erential area of each edge bwill a,and ebe distributed along the edge with a density proportional 
to the di.erential area of the triangle with a vertex at B and base through P,which is (1-cose)df. To 
.nd the edge length b bthat attains the area Ab =A61, we use equations (1) and (2) to obtain bb cos(j-f)cos0-cosf 
b cosb= , (4) b sin(j-f)sin0 wherej=Ab -0. Then from equations (1) and (3) we have bb ucosf+vsinf=0,where 
u =cos(j)-cos0, v =sin(j)+sin0cosc It follows that 'u ±v b b sinf=pand cosf=p 22 22 u+v u+v The sign 
is determined by the constraint 0<fb < , but is immaterial in what follows. Simplifying equation (4) 
using the above expressions, we obtain [vcosj-usinj]cos0-v cosb= (5) b [vsinj+ucosj]sin0 b b Note that 
cosbdetermines b b,since 0<b<,and that b bin turn determines the vertex C. Finally, we may easily solve 
b b b for z=coseusing F2(ejb)=62 and cosb=C .B. a To succinctly express the sampling algorithm let [x 
jy ] denote the normalized component of the vector x that is orthogonal to the vector y.That is, [x jy 
]=Normalize (x -(x .y)y) (6) The algorithm for mapping the unit square onto the triangle T takes two 
variables 61 and 62, each in the unit interval, and returns a point P 2T CIR3 . point SampleTriangle( 
real 61, real 62) Use one random variable to select the new area. b Af61 *A; Save the sine and cosine 
of the angle j. b sfsin(A-0); b tfcos(A-0); Compute the pair (u,v)that determines fb . uft-cos0; vfs+sin0*cosc; 
 Let qbe the cosine of the new edge length b b. [v*t-u*s]*cos0-v qf; [v*s+u*t]*sin0 Compute the third 
vertex of the sub-triangle. p b q*A +-1q*[C jA ]; C f2 Use the other random variable to select cose. 
zf1-62*(1-C .B); b Construct the corresponding point on the sphere. p P fz*B +-2b 1z*[C jB ]; return 
P; end If 6and 6are independent random variables uniformly 12 distributed in [0,1], as produced by most 
pseudo-random number generators, then P will be uniformly distributed in triangle T. Note that cos0, 
sin0, cosc,and [C jA ]need only be computed once per triangle, not once per sample. 3 Results Results 
of the algorithm are shown in Figure 2. On the left, the samples are identically distributed, which produces 
a pattern equivalent to that obtained by rejection sampling; however, each sample is guaranteed to fall 
within the trian­gle. The pattern on the right was generated by partitioning the unit square into a regular 
grid and choosing one pair (61,6)uniformly from each grid cell, which corresponds to 2 strati.ed or jittered 
sampling [2]. The advantage of strati.ed sampling is evident in the resulting pattern; the samples are 
more evenly distributed, which generally reduces the vari­ance of Monte Carlo estimates based on these 
samples. Figure 2: Uniform and strati.ed sampling. The samples on the right were generated from strati.ed 
points in the unit square. The sampling algorithm can be applied to spherical poly­gons by decomposing 
them into triangles and performing strati.ed sampling on each component independently, which is analogous 
to the method for planar polygons [7]. This provides an e.ective means of sampling the solid angle sub­tended 
by a polygon.  Acknowledgments Many thanks to Pete Shirley for his valuable suggestions and for urging 
the author to write this paper. This work was supported by the NSF/ARPA Science and Technology Center 
for Computer Graphics and Scienti.c Visualization (ASC-8920219) and performed on workstations generously 
provided by the Hewlett-Packard Corporation. References [1] Berger, M. Geometry, Volume II. Springer-Verlag, 
New York, 1987. Translated by M. Cole and S. Levy. [2] Cook, R. L. Stochastic sampling in computer graphics. 
ACM Transactions on Graphics 5, 1 (1986), 51 72. [3] Hanrahan, P., Salzman, D., and Aupperle, L. A rapid 
hierar­chical radiosity algorithm. Computer Graphics 25, 4 (July 1991), 197 206. [4] Rubinstein, R. Y. 
Simulation and the Monte Carlo Method. John Wiley &#38; Sons, New York, 1981. [5] Shirley, P., Wang, 
C., and Zimmerman, K. Monte carlo methods for direct lighting calculations. ACM Transactions on Graphics 
(1995). To appear. [6] Spanier, J., and Gelbard, E. M. Monte Carlo Principles and Neutron Transport Problems. 
Addison-Wesley, Reading, Mas­sachusetts, 1969. [7] Turk, G. Generating random points in triangles. In 
Graph­ics Gems, A. S. Glassner, Ed. Academic Press, New York, 1990, pp. 24 28.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218501</article_id>
		<sort_key>439</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>55</seq_no>
		<title><![CDATA[Image metamorphosis using snakes and free-form deformations]]></title>
		<page_from>439</page_from>
		<page_to>448</page_to>
		<doi_number>10.1145/218380.218501</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218501</url>
		<keywords>
			<kw><![CDATA[image metamorphosis]]></kw>
			<kw><![CDATA[morphing]]></kw>
			<kw><![CDATA[multilevel B-spline interpolation]]></kw>
			<kw><![CDATA[multilevel free-form deformation]]></kw>
			<kw><![CDATA[snakes]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>G.1.1</cat_node>
				<descriptor>Spline and piecewise polynomial interpolation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Morphological</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003714.10003715.10003722</concept_id>
				<concept_desc>CCS->Mathematics of computing->Mathematical analysis->Numerical analysis->Interpolation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
			<gt>Verification</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39041819</person_id>
				<author_profile_id><![CDATA[81430598275]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Seung-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dept. of Computer Science, City College of New York, 138th St. at Convent Ave., Rm. R8/206, New York, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39049421</person_id>
				<author_profile_id><![CDATA[81100579855]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kyung-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chwa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Korea Advanced Institute of Science and Technology, Taejon 305-701, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39064006</person_id>
				<author_profile_id><![CDATA[81406593243]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sung]]></first_name>
				<middle_name><![CDATA[Yong]]></middle_name>
				<last_name><![CDATA[Shin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Korea Advanced Institute of Science and Technology, Taejon 305-701, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>578131</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ballard, Dana H., and Christopher M. Brown. Computer Vision. Prentice-Hall, 1982.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Beier, Thaddeus, and Shawn Neely. Feature-Based Image Metamorphosis. Computer Graphics 26, 2 (1992), 35-42.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Benson, Philip J. Morph Transformation of the Facial Image. Image and Vision Computing 12, 10 (1994), 691-696.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97900</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Coquillart, Sabine. Extended Free-Form Deformation: A Sculpturing Tool for 3D Geometric Modeling. Computer Graphics 24, 4 (1990), 187-196.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122720</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Coquillart, Sabine, and Pierre Jancene. Animated Free-Form Deformation: An Interactive Animation Technique. Computer Graphics 25, 4 (1991), 23-26.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Goodman, Tim, and Keith Unsworth. Injective Bivariate Maps. Tech. Rep. CS94/02, Dundee University, U.K., 1994.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134036</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hsu, William M., John F. Hughes, and Henry Kaufman. Direct Manipulation of Free-Form Deformations. Computer Graphics 26, 2 (1992), 177-184.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kass, Michael, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active Contour Models. International Journal of Computer Vision (1988), 321-331.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>926966</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lee, Seung-Yong. Image Morphing Using Scattered Feature Interpolations. PhD thesis, KAIST, Taejon, Korea, February 1995.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lee, Seung-Yong, Kyung-Yong Chwa, James Hahn, and Sung Yong Shin. Image Morphing Using Deformable Surfaces. In Proceedings of Computer Animation '94 (Geneva, Switzerland, 1994), IEEE Computer Society Press, pp. 31-39.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lee, Seung-Yong, Kyung-Yong Chwa, James Hahn, and Sung Yong Shin. Image Morphing Using Deformation Techniques. The Journal of Visualization and Computer Animation 6, 3 (1995).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192270</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Litwinowicz, Peter, and Lance Williams. Animating Images with Drawings. In SIGGRAPH 94 Conference Proceedings (1994), ACM Press, pp. 409-412.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Meisters, G.H., and C. Olech. Locally One-to-one Mappings and a Classical Theorem on Schlicht Functions. Duke Mathematical Journal 30 (1963), 63-80.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Nishita, Tomoyuki, Toshihisa Fujii, and Eihachiro Nakamae. Metamorphosis Using Bdzier Clipping. In Proceedings of the First Pacific Conference on Computer Graphics and Applications (Seoul, Korea, 1993), World Scientific Publishing Co., pp. 162-173.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>97916</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Nishita, Tomoyuki, Thomas Sederberg, and Masanori Kakimoto. Ray Tracing Trimmed Rational Surface Patches. Computer Graphics 24, 4 (1990), 337-345.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617967</ref_obj_id>
				<ref_obj_pid>616035</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Ruprecht, Detlef, and Heinrich Mtiller. Image Warping with Scattered Data Interpolation. IEEE Computer Graphics and Applications 15, 2 (1995), 37-43.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Sederberg, Thomas W., and Scott R. Parry. Free-Form Deformation of Solid Geometric Models. Computer Graphics 20, 4 (1986), 151-160.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Terzopoulos, Demetri. Multilevel Computational Processes for Visual Surface Reconstruction. Computer Vision, Graphics, and Image Processing 24 (1983), 52-96.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Wolberg, George. Digital Image Warping. IEEE Computer Society Press, Los Alamitos, CA, 1990.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Image Metamorphosis Using Snakes and Free-Form Deformations Seung-Yong Leey, Kyung-Yong Chwa, Sung 
Yong Shin Department of Computer Science Korea Advanced Institute of Science and Technology Taejon 305-701, 
Korea George Wolberg Department of Computer Science City College of New York / CUNY New York, NY 10031 
 Abstract This paper presents new solutions to the following three problems in image morphing: feature 
speci.cation, warp generation, and transi­tion control. To reduce the burden of feature speci.cation, 
we .rst adopt a computer vision technique called snakes. We next propose the use of multilevel free-form 
deformations (MFFD) to achieve C2-continuous and one-to-one warps among feature point pairs. The resulting 
technique, based on B-spline approximation, is sim­pler and faster than previous warp generation methods. 
Finally, we simplify the MFFD method to construct C2-continuous surfaces for deriving transition functions 
to control geometry and color blending. Keywords: Image metamorphosis, morphing, snakes, multilevel free-form 
deformation, multilevel B-spline interpolation. 1. Introduction Image metamorphosis deals with the .uid 
transformation from one digital image into another. This technique, commonly referred to as morphing, 
has found widespread use in the entertainment industry to achieve stunning visual effects. Smooth transformations 
are realized by coupling image warping with color interpolation. Image warping applies 2D geometric transformations 
on the images to retain geometric alignment between their features, while color interpolation blends 
their color. Given two images, their correspondence is established by an animator with pairs of points 
or line segments. Each point or line segment speci.es an image feature, or landmark. The feature correspondenceis 
thenusedto computewarpsto interpolate the po­sitions of the features across the morph sequence. Once 
the source and destination images have been warped into alignment for inter­mediate feature positions, 
ordinary color interpolation (i.e., cross­dissolve) is performed to generate an inbetween image. Since 
color interpolation is straightforward, research in image morphing has concentrated on warp generation 
from the feature correspondence. In mesh-based techniques, such as mesh warping [19] and the method of 
Nishita et al. [14], nonuniformmeshesareusedto specify the image features. Spline interpolation or B´ezier 
clipping [15] yCurrent address: Dept. of Computer Science, City College of New York, 138th St. at Convent 
Ave., Rm. R8/206, New York, NY 10031. [cssyl jwolberg]@cs-mail.engr.ccny.cuny.edu [kychwa jsyshin]@jupiter.kaist.ac.kr 
Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
computes warps from the correspondence of mesh points. Though fast and intuitive, the mesh-based techniques 
have a drawback in specifying features. A control mesh is always required although the features on an 
image may have an arbitrary structure. Field morphing [2] uses a set of line segments to effectively 
specify the features on an image. A pair of line segments on two images determines a warp from their 
local coordinate systems. When two or more pairs of line segments are speci.ed, the in­.uences of line 
segments are blended by their weighted average. This technique suffers from unexpected distortions, referred 
to as ghosts, which prevent an animator from realizing a precise warp in a complex metamorphosis. If 
the features on an image are speci.ed by a set of points, the x-and y-components of a warp can be derived 
by constructing the surfaces that interpolate scattered points. Warp generation by this approach was 
extensively surveyed in [16, 19], and recently two similar methods were independently proposed using 
the thin plate surface model [10, 12]. These techniques generate smooth warps that exactly re.ect the 
feature correspondence. When a warp is applied to an image, the one-to-one property of the warp guaranteesthat 
the distorted image does not fold back upon itself. An energy minimization method has been proposed which 
derives a C1-continuous and one-to-one warp from a set of feature point pairs [11]. The performance of 
that method is hampered by its high computational cost. This paper presents a new multilevel free­form 
deformation (MFFD) technique to derive a C2-continuous and one-to-one warp that exactly satis.es the 
feature correspondence. The proposed technique, based on 2D B-spline approximation, is much simpler and 
faster than the energy minimization method. Another interesting problem in image morphing is transition 
control. If transition rates differ from part to part in inbetween images, more interesting animations 
are possible. In mesh warping [19], a transition curve is assigned to each point of the mesh to control 
the transition behavior. When complicated meshes are used to specify the features, it is tedious to assign 
a proper transition curve to every mesh point. Nishita et al. mentioned that the transition behavior 
can be controlled by a B´ezier function de.ned on the mesh [14]. However, the details of the method were 
not provided. An effective method for transition control has been proposed in [11]. Transition rates 
on an inbetween image are derived from transition curves by constructing a smooth surface. The surface 
represents the propagation of transition rates de.ned by the user at sparse positions across the image. 
In this paper, the MFFD technique for warp generation is simpli.ed and applied to ef.ciently generate 
a C2-continuous surface. The most tedious aspect of image morphing is feature speci.ca­tion. Despite 
their usefulness, computer vision techniques remain only marginally utilized for this task [3]. In this 
paper, we adopt snakes [8], an active contour model made popular in computer vi­sion, to reduce the burden 
of feature speci.cation for animators.  2. Preliminaries The authors have proposed a general framework 
for generating an inbetween image from two images, including transition behavior control [9, 11, 19]. 
The framework is also taken for the new image morphing technique given in this paper. 2.1 Metamorphosis 
framework Let F0 and F1 be two sets of features speci.ed by an animator on the source and destination 
images I0 and I1, respectively. For each feature f0 in F0, there exists a corresponding feature f1 in 
F1.Let W0 and W1 be the warp functions that specify the corresponding point in I1 and I0 for each point 
in I0 and I1, respectively. When it is applied to I0, W0 generates a distorted image so that the fea­tures 
in F0 coincide with their corresponding features in F1.The requirement for W1 is to map features f1 onto 
f0 when it distorts I1. Although W1 is the inverse of W0 at the features, this is not necessarily true 
at other positions across the image. Together, W0 and W1 serve to retain geometric alignment of the features 
during the morph. Transition functions specify a transition rate for each point on the given images over 
time. Let T0 be a transition function de.ned for source image I0. For a given time t, T0(p; t)is a real-valued 
function that determines how fast each point pin I0 moves to­wards the corresponding point qin destination 
image I1. T0(p; t) also determines the color contribution of each point pin I0 to the corresponding point 
in an inbetween image I(t). Let T1 be the transition function for destination image I1.For each point 
qin I1, T1(q; t)is de.ned to have the same transition rate as T0(p; t)if qcorresponds to pin I0. Hence, 
T1(q; t)can be derived from T0(p; t)using warp function W1.That is, T1(q; t) T0(W1(q); t). For simplicity, 
we treat the transition functions for both geometry and color to be identical, although they may be different 
in practice. Let W.Idenote the application of warp function Wto image I. The procedure for generating 
an inbetween image I(t)can be described as follows. W0(p; t) (1 -T0(p; t))·p+T0(p; t)·W0(p) W1(q; t) 
T1(q; t)·q+(1 -T1(q; t))·W1(q) I0(p; t) W0(p; t).((1 -T0(p; t))·I0(p)) I1(q; t) W1(q; t).(T1(q; t)·I1(q)) 
I(r; t) I0(r; t)+I1(r; t). Note that 0 :T0T1 :1and 0 :t:1. Transition rates 0 and 1 imply the source 
and destination images, respectively. To complete the procedure, a solution is needed to each of the 
following problems: .how to specify feature sets F0 and F1, .how to derive warp functions W0 and W1,and 
.how to derive transition functions T0 and T1.  2.2 The energy minimization method An energy minimization 
method has been proposed for deriving warp functions in [11]. That method allows extensive feature spec­i.cation 
primitives such as points, polylines, and curves. Feature correspondence is established by converting 
non-point features to feature points using point sampling. A warp is interpreted as a 2D deformation 
of a rectangular plate. The feature correspondence assigns a new position to each feature point to derive 
a warp. A deformation technique is provided to derive C1-continuous and one­to-one warps from the positional 
constraints. The requirements for a warp are represented by energy terms and satis.ed by minimiz­ing 
their sum. The technique generates natural warps since it is based on physically meaningful energy terms. 
It is, however, a bit involved to implement. Transition functions are obtained by selecting a set of 
points on a given image and specifying a transition curve for each point. The transition curves determine 
the transition behavior of the selected points over time. For a given time, transition functions must 
have the values assigned by the transition curves at the selected points. Considering a transition rate 
as the vertical distance from a plane, transition functions are reduced to smooth surfaces that interpolate 
a set of scattered points. The thin plate surface model [18] was employed to obtain C1-continuous surfaces 
for transition functions.  2.3 Overview With the metamorphosis framework described in Section 2.1, we 
present a more effective technique than the previous energy mini­mization method. To help an animator 
specify image features, we use snakes [8], a technique popularized in computer vision. Snakes make it 
possible to capture the exact position of a feature easily and precisely. To derive warps from positional 
constraints, we propose the MFFD as an extension to free-form deformation (FFD) [17]. We take the bivariate 
cubic B-spline tensor product as the deformation function of FFD. A new direct manipulation technique 
for FFD, based on 2D B-spline approximation, is developed in this paper. We apply it to a hierarchy of 
control lattices to exactly satisfy the positional constraints. To guarantee the one-to-one property 
of a warp, we present a suf.cient condition for a 2D cubic B-spline surface to be one-to-one. The MFFD 
generates C2-continuous and one-to-one warps which yield .uid image distortions. It is much simpler and 
faster than the energy minimization method. We also present a hybrid approach that combines the two methods. 
To obtain smooth surfaces for transition functions, we simplify the MFFD to obtain multilevel B-spline 
interpolation. This inter­polation algorithm ef.ciently generates a C2-continuous surface through a set 
of scattered points.  3. Feature Speci.cation Features consist of image landmarks, e.g., the pro.le, 
eyes, noise, and mouth of a facial image. The position of a feature is usually identi.ed by a boundary 
curve at edges, where color values change abruptly. We adopt the use of snakes to assist us in the precise 
positioning of features. 3.1 Snakes Snakes [8] are energy-minimizing splines under the in.uence of image 
and constraint forces. The spline energy serves to impose a piecewise smoothness constraint on a snake. 
The image forces push the snake toward salient image features such as lines, edges, and subjective contours. 
The constraint forces are used for pulling the snake to a desired image feature among the nearby ones. 
Snakes have proven to be useful for the interactive speci.cation of image features. Representing the 
position of a snake in parametric form, v(s) (x(s)y(s)), its energy functional can be written as Z 1 
Esnake(v) [Espline(v)+Eimage(v)1ds. 0 Esplinerepresents the spline energy due to bending, and Eimage 
is the energy de.ned from the intensity distribution of an image. We have removed the term related to 
the constraint forces because it is not used in this paper. We also simplify the spline energy to v 
Espline,jd2 2, which makes a snake act like a thin plate. ds2 jFor a gray-scale image I, the gradient 
rImeasures the local changes of image values and can be computed by a difference operator or the Sobel 
operator [1]. The image energy functional can be de.ned by Eimage-r 2I-jrI(xy)j2. It makes a snake precisely 
localize a feature at a boundary having large image gradients. While minimizing the energy functional 
Esnake,the snake slithers from its initial position to a nearby feature. A feature is allowed to attract 
a distant snake if image gradients are convolved with a smoothing .lter. For example, the convolution 
results in an image energy functional, Eimage-(GOr 2I), where GOis a Gaussian of standard deviation (. 
Other image energy functionals and the details of the energy minimization procedure can be found in [8]. 
 3.2 Feature speci.cation primitives In this paper, the feature speci.cation primitives include points, 
polylines, and curves as in [11]. However, the positions of features can be derived more effectively 
by generating snakes from polylines and curves. To specify a feature having large image gradients, a 
snake is initialized by positioning a polyline or curve near the fea­ture. Wethenuniformlysampleasequenceofpointsonthepolyline 
or curve, e.g., 20 points per segment. As the snake minimizes its energy, it slithers and .nally locks 
onto the feature by the image force. To tailor the response of the snake, the user may clamp any of the 
sampled points in place. Internally, this is achieved by assigning a large value to the parameter ,in 
[8] for the selected points. Since it is often tedious to select among the many sampled points, we provide 
an option for .xing those that lie on the control points of the user-speci.ed primitive. When a feature 
speci.cation primitive f0 is placed on image I0, a primitive f1 is also deposited on the other image 
I1. We either move f0 repeatedly or generate a snake from f0 to identify a feature on I0. f1 is then 
moved to designate the corresponding feature on I1, and a snake is initiated if necessary. 2 If f0 and 
f1 are polylinesorcurves,the correspondencebetween 1 them is established by their vertices or control 
points, respectively. parallelepiped lattice containing the object. The manipulated lattice determines 
a deformation function that speci.es a new position for each point on the object. Coquillart extended 
the FFD method to handle non-parallelepiped lattices [4] and proposed a technique for animating objects 
modeled by FFD [5]. Hsu et al. employed the FFD method to directly control the shape of an object under 
complex deformations [7]. They took the trivariate cubic B-spline tensor product as the deformation function 
instead of the Bernstein polynomials used by Sederberg and Parry. In thispaper,weconsidera2DFFDto generatea 
C2-continuous and one-to-one warp from positional constraints. A rectangular plate in the xy-plane is 
deformed by manipulating a parallelepiped lattice overlaid on it. We take the bivariate cubic B-spline 
tensor product as the deformation function of FFD because a B-spline has local control. This property 
makes it possible to locally manipulate the lattice when a point on the plate is moved to the speci.ed 
position. Therefore, the new lattice producing this movement can be ef.ciently computed even for a large 
number of control points.  4.1 Free-form deformation and the 1-to-1 property Let O bearectangularplate 
placedonthe xy-plane. We assumethat O contains points p(uv)where 1 :u:mand 1 :v:n. When plate O is deformed 
in the xy-plane, its shape can be repre­sented by a vector-valued function, w(p)(x(p)y(p)).Let F be an 
(m+2)x(n+2)lattice of control points overlaid on plate O. In the initial con.guration of F,the ij-th 
control point lies at its initial position, r0 (ij). With the FFD method, a desired deformation wof plate 
O is derived by displacing the control points on lattice F from their initial positions (Fig. 1). ij 
y F n+1 n  The correspondence between two snakes can be derived from the polylines or curves that provide 
their initial positions. The feature correspondence between two images is internally translated to a 
set of point pairs sampled on the speci.ed feature primitives. Fig. 7 shows an example. Fig. 7(a) is 
the input image. We convert it to a gray-scale image and apply the Sobel operator [1] to compute image 
gradients. Fig. 7(b) shows the image gradients convolved with a Gaussian .lter, where bright intensities 
denote large gradients. In Fig. 7(c), we place a polyline near the pro.le of the image. The snake starting 
from the polyline exactly captures the pro.le, as in Fig. 7(d). Fig. 7(e) illustrates the speci.ed feature 
primitives overlaid on the image. The cyan points in Fig. 7(f) represent internally sampled feature points 
on the primitives. We typically use a uniform sampling rate of 20 points per primitive segment, although 
only .ve points per segment are shown in the .gure.  4. Warp Generation Free-form deformation (FFD) 
was proposed by Sederberg and Parry as a powerful modeling tool for 3D deformable objects [17]. The basic 
idea of FFD is to deform an object by manipulating a 3D x0 1 2 mm+1 Figure 1: The initial arrangement 
of the plate and control lattice Let rijbe the position of the ij-th control point on lattice F. The 
function wis de.ned in terms of rijby 33 XX w(uv) Bk(s)Bl(t)r(i+k)(j+l) (1) k=0 l=0 where ibuc-1, jbvc-1, 
su-buc,and tv-bvc. Bk(s)and Bl(t)are the uniform cubic B-spline basis functions evaluated at sand t, 
respectively. Since a B-spline curve through collinear control points is itself linear, the initial con.guration 
of lattice F generates the undeformed shape of the plate. That is, 33 XX w 0(uv)(uv) Bk(s)Bl(t)r(0 i+k)(j+l).(2) 
k=0 l=0 From Eq. (1), we know that the deformed position w(p)of point p on plate O relates to the sixteen 
control points in its neighborhood. Now, we consider the one-to-one property of the function w de.ned 
in Eq. (1). Function wcan be regarded as a 2D uniform cubic B-spline surface where plate O is the parameter 
space. The one-to-one property of a 2D B-spline surface has not been studied because B-spline surfaces 
are usually considered in three dimen­sions to model free-form surfaces. Recently, Goodman and Unsworth 
presented a suf.cient con­dition for a 2D B´ezier surface to be one-to-one [6]. They com­mented that 
the condition can also be applied to a 2D B-spline surface. For an mxnlattice of control points, the 
condition con­tains 2m(m+1)+2n(n+1)linear inequalities. If the number of control points is large, the 
time to check the condition becomes pro­hibitive. Moreover, if the condition does not hold, there is 
no simple way for manipulating the control lattice to satisfy the condition. In this paper, we present 
a suf.cient condition for the function wto be one-to-one in terms of the displacements of control points. 
With the following theorem, a 2D uniform cubic B-spline surface can be made one-to-one by limiting the 
displacements of control points. Theorem 1 The function w given in Equation (1) is one-to-one if (-0.48 
-0.48):rij-rij0 :(0.48 0.48)for all i j. Proof: See Appendix. 2 Theorem 1 provides a tight, suf.cient, 
although not necessary, con­dition. There are examples in which the B-spline surface is not one-to-one 
even though all control points are displaced by amounts less than 0.5. This means that a B-spline surface 
may violate the one-to-one property even when control lattice gridlines do not in­tersect among themselves. 
4.2 Manipulation of free-form deformation Suppose that plate O should be deformed to place a point pat 
the speci.ed position q,that is, w(p)q. Without loss of generality, we may assume that p(u v),1 :u v.2. 
Then, the dis­placements of the kl-th control points, kl0 1 2 3, on lattice F determine the deformed 
position w(p)of point p. See Fig. 2(a). Let .qw(p)-w0(p)q-pbe the movement of the point pfrom its original 
position. Let .rklrkl -r 0 be the displacement of kl the kl-th control point from its initial position. 
From Eqs. (1) and (2), the displacements .rklmust satisfy Eq. (3): 33 XX .qBk(s)Bl(t).rkl (3) k=0 l=0 
 where su-1and tv-1. There are many values of .rklthat are solutions to Eq. (3). We choose one in the 
least-squared sense such that wkl.q .rkl P3 P3 (4) 2 w a=0 b=0 ab where wabBa(s)Bb(t). Among all the 
solutions to Eq. (3), Hsu et al. showed that this minimizes the squared sum of control point displacements 
[7]. In this solution, the control points near point p get larger displacements than the others because 
wkldepends on the distance between the kl-th control point and point p. Hence, it generates the deformation 
wwhereby the effect of the movement of ptapers off smoothly. Now, suppose that plate O should be deformed 
to place a set of points Pat a set of positions Q.That is, w(p)qfor each point pin Pand its position 
qin Q. A point pin Pcan be moved to the speci.ed position qif its surrounding control points are displaced 
by the amount .rklgiven in Eq. (4). However, these displacements (a) single constraint (b) multiple 
constraints Figure 2: Examples of the positional constraints may mislead another point in Pto another 
position than the one speci.ed in Q. 0 Let Pf(ucvc)gbe the set of points in Psuch that i-2 : uc.i+2and 
j-2 :vc.j+2. Let rbe the ij-th control point on lattice F whose initial position is (i j), as in Fig. 
2(b). The displacement of control point rin.uences the movement of 0 points in Pwhen we evaluate the 
deformation function w.For each point pcin P0, Eq. (4) gives the displacement .rcof control point rrequired 
for moving pcto the speci.ed position. Since the displacement .rcmay be different from point to point 
in P0,the displacement .rof control point ris chosen to minimize an error. The error is de.ned as the 
squared sum of differences between wc.rand wc.rc,where wcBk(s)Bl(t), k(i+1)-bucc, l(j+1)-bvcc, suc -bucc,and 
tvc -bvcc, for each point pc(ucvc)in P0. That is, the error is X (wc.r-wc.rc)2 . c wc.ris the movement 
of point pcdue to the displacement .rof control point r. wc.rcrepresents the contribution of control 
point r, determined by Eq. (4), for moving pcto its speci.ed position. By differentiating the error with 
respect to .rand equating the derived formula to zero, we get P wc2.rc c .rP2 . (5) c wc When the displacements 
of the control points on lattice F are computed by Eq. (5), the resulting deformation function wis not 
guaranteed to be one-to-one. To make the function wone-to­one, we truncate the displacement .rof a control 
point rso that (-0.48 -0.48):.r:(0.48 0.48). Then, the condition of Theorem 1 holds and the derived function 
wis one-to-one. Hsu et al. presented a technique for manipulating control points so that the points on 
an object modeled by FFD may be moved to the speci.ed positions [7]. That technique calculates the pseu­doinverse 
of a matrix to derive the displacements of control points that minimize the squared sum of distances 
between the speci.ed and actual positions. The matrix contains the values of B-spline basis functions 
and its size depends on the number of positional constraints. When a large number of points must be moved, 
the computation for calculating the pseudoinverse is prohibitive. On the other hand, the technique proposed 
in this section runs very fast even when the number of moved points is large. The defor­mation of plate 
O nicely re.ects the movements of points because the displacement of each control point minimizes a reasonable 
er­ror. Fig. 3 shows examples. In the .gures, black spots represent the positions of the selected points 
in the undeformed and deformed shapes. Thick curves show the control lattice F overlaid on plate O. The 
control lattice is a rectangular grid in its initial con.guration. It is transformed when plate O is 
deformed. Figure 3: Examples of the FFD manipulation  4.3 Multilevel free-form deformation Let Pbe 
a set of points on plate O and Qbe a set of corresponding positions. An application of the FFD manipulation 
presented in Section 4.2 cannot always deform plate O to place each point in P at its speci.ed position 
in Q. One reason is that the displacement of a control point on lattice F is the weighted average of 
the dis­placements required for moving its neighboring points in P.The other reason is that we limit 
the maximum displacement of a control point to approximately a half of the spacing between control points 
in order to make the deformation function one-to-one. We may circumvent the .rst problem if we make the 
control lattice .ner until every point in Pcan be moved by its surround­ing control points without interfering 
with other points in P.The second can be overcome if we repeatedly apply the FFD manipula­tion to plate 
O so that the accumulated movement of a point in P can be suf.ciently large. Hence, an obvious method 
for deriving a one-to-one deformation function from the positional constraints is to overlay a suf.ciently 
.ne control lattice over plate O and iterate the FFD manipulation. However, in this case, the resulting 
shape of plate O will show only sharp local deformations near the points in P. Moreover, a large number 
of FFD manipulations may be required to satisfy the positional constraints because a point in P can only 
move a short distance by the FFD manipulation when a .ne control lattice is used. In this section, we 
present the multi­level free-form deformation (MFFD) technique that overcomes the drawbacks of the simple 
method. In MFFD, a hierarchy of control lattices, F0 F1 ...Fm,is used to derive a sequence of deformation 
functions with the FFD manipulation. Let hkbe the spacing between control points on the initial con.guration 
of lattice Fk. We assume that h0 and hm are given and that hk2hk+1. When plate O is deformed with a coarse 
control lattice, the positional constraints merge with each other and result in a smooth deformation, 
although they are not exactly satis.ed. The remaining deviations between the deformed and speci.edpositionswill 
be handledby subsequentdeformations with .ner control lattices. Let w0 w1 ...wnbe the sequence of deformation 
functions derived in the MFFD. Then, the deformation of plate O is de.ned by the composite function wwnOwn.1 
O...Ow0.That is, w(O) wn(On),where O0 O and Oi+1 wi(Oi). w(O)and wi(Oi) denote the resulting shapes when 
the deformation functions wand wiare applied to the plate O and deformed plate Oi, respectively. Let 
Pi+1 wi(Pi),where P0 P. Piis the set of points on the deformed plate Oithat lie at the deformed positions 
of the points in P. The deformation function wiis computed to move the points in Pito their speci.ed 
positions in Q. When the deformation function wis applied to plate O, wede.ne theerroras max kw(pc)-qck2 
c where qcis the position in Qspeci.ed for the point pcin P. When we deform a plate Oiwith a control 
lattice Fk, a point in Pican move at most (0.48hk0.48hk)if and only if all 16 surrounding control points 
are displaced by (0.48hk0.48hk).Note that this maximum movement follows from Theorem 1 whereby the displacements 
of control points must be truncated to keep the one-to­one property of the deformation function. If each 
point in Pimoves by (0.48hk0.48hk), the error decreases by at least (0.48hk)2.In this case, more FFD 
manipulations with the control lattice Fkmay be helpful for moving the points in Pito their speci.ed 
positions. In MFFD, the FFD manipulation starts with the coarsest control lattice F0. With a control 
lattice Fk, the FFD manipulation iterates until the change in error falls below a(0.48hk)2. Then, the 
next .ner control lattice Fk+1 is used for the successive FFD manipulation, as long as Fkis not the .nest 
control lattice. This process continues while the error exceeds a user-speci.ed threshold. The parameter 
ais a real value between 0 and 1. A small agenerates a smooth deformation of plate O because FFD manipulations 
tends to be performed on coarser control lattices. We usually use 0.5 as the value of a. The FFD manipulation 
generates a C2-continuous and one-to­one deformation function. In the MFFD, a deformation function is 
the composition of several functions derived by FFD manipulations. Hence, the resulting deformation of 
plate O is C2-continuous and one-to-one. Furthermore, the result is guaranteed to remain one­to-one even 
when the positional constraints are prone to foldovers. This is achieved by relaxing the requirement 
to exactly satisfy the positional constraints in order to retain the one-to-one property. Fig. 4 gives 
an example in which the MFFD is applied to gen­erate a deformation of plate O from positional constraints. 
Fig. 4(a) shows the selected points in the undeformed shape of the plate. Figs. 4(b) through (e) show 
a sequence of deformations in which the deformed positions of the selected points gradually approach 
the speci.ed positions. Fig. 4(f) shows the resulting deformation with the speci.ed positions. In this 
example, the FFD manipulations are performed no more than twice at each level of the control lattice. 
Most of the computation for the MFFD is consumed in evalu­ating the deformation function won plate O. 
When the function wis evaluated on a 64 x64 grid, it takes 0.2 seconds for an SGI Crimson to generate 
the deformation in Fig. 4(f). When the size of the grid is 512 x512, the computation time is 9.2 seconds. 
 4.4 A hybrid approach An energy minimization method has been proposed to derive C1­continuous and one-to-one 
warps from positional constraints [11]. It generates natural warps but requires much computation if warps 
are evaluated on large grids. The MFFD can be combined with that method to derive warps more effectively. 
Suppose that warps are to be evaluated on an mxngrid. First, the energy minimization method is used to 
obtain a warp weon acoarse (m/k)x(n/k)grid using positional constraints derived   (a) (b)  (c) (d) 
 (e) (f)   Figure 4: An example of the MFFD from the original grid by weighted averaging. Then, a 
warp w0 is derived on the mxngrid by constructing a 2D uniform cubic B-spline surface that interpolates 
the values of the function we. The function w0 is C2-continuous and one-to-one but does not satisfy the 
given positional constraints exactly. Finally, the MFFD is applied to handle the remainder of the positional 
constraints, where (m/k)x(n/k)is the size of the coarsest control lattice. In the hybrid approach, the 
energy minimization method deter­mines the global shape of the generated warp on a coarse grid in a short 
time. The MFFD on the original grid makes it possible to avoid the excessive computation required for 
energy minimization on a .ne grid. Hence, the hybrid approach generates a nice warp similar to the energy 
minimization method in a computation time comparable to the MFFD. Fig. 5 gives an example. Fig. 5(a) 
shows the selected points in the undeformed shape of the plate. Figs. 5(b), (c), and (d) show the deformations 
of the plate derived by energy minimization, MFFD, and the hybrid method, respectively. In the .gures, 
warps are evaluated on a 512 x512 grid. For the hybrid approach, the energy minimization method is applied 
to a 128 x128 grid. The computation times for Figs. 5(b), (c), and (d) on an SGI Crimson are 26.7, 6.4, 
and 7.5 seconds, respectively. (a) undeformed shape (b) energy minimization  (c) MFFD (d) hybrid method 
Figure 5: Comparison of the deformed shapes of a plate  5. Transition Control B-spline surfaces are 
widely used to model free-form surfaces be­cause they offer nice properties such as continuity and local 
control. In this section, we consider uniform cubic B-splines to generate a surface that interpolates 
a scattered set of 3D height .eld points. The purpose of deriving this surface is to propagate the transition 
control information speci.ed at only sparse positions. This infor­mation is given by the user with transition 
curves speci.ed along primitives. Note that these primitives do not necessarily relate to feature primitives. 
That is, a different set of points, polylines, or curves may be de.ned to specify transition behavior. 
 5.1 Manipulation of B-spline surfaces Let O be a rectangular region in the uv-plane which contains points 
p(uv)such that 1 :u:mand 1 :v:n.Let F be a (m+2)x(n+2)lattice of control points overlaid on the region 
O. In the initial con.guration of F,the ij-th control point lies at its initial position (ij)in the uv-plane. 
When the control points on lattice F are displaced only in the direction perpendicular to the uv-plane, 
the resulting B-spline surface can be represented by a real-valued function f. The function value f(p)for 
a point p(uv)on O implies that the point pis placed at the position (uvf(p))when the surface is generated. 
Let rijbe the height of the ij-th control point from the uv-plane. Then, the function fis given by 33 
XX f(uv) Bk(s)Bl(t)r(i+k)(j+l) k=0 l=0 where ibuc-1, jbvc-1, su-buc,and tv-bvc. Bk(s)and Bl(t)are the 
uniform cubic B-spline basis functions evaluated at sand t, respectively. The above formula for fis in 
the same form as Eq. (1) for the deformation function win Section 4.1. Suppose that a B-spline surface 
is required to interpolate a set of scattered points (ucvctc),where (ucvc)is a point in the region O.That 
is, f(ucvc)tcfor each point in the set. A surface that approximately satis.es the positional constraints 
can be obtained by following the same approach for FFD manipulation described in Section 4.2. The required 
heights of control points from the uv-plane are derived by Eqs. (4) and (5), replacing .qwith tc.The 
computed heights of control points are not truncated in this case because it is not necessary to consider 
the one-to-one property. 5.2 Multilevel B-spline interpolation Let Pbe a set of points (ucvctc)in 3D 
space, where (ucvc) is a point in the region O. As in the case for a warp, the B­spline surface derived 
by Eqs. (4) and (5) does not necessarily interpolate the points in P. A straightforward solution is to 
use a suf.ciently .ne control lattice so that every point in Pcan be interpolated without interfering 
with other points. However, the resulting surface will show only sharp local deformations near the points 
in P. Thus, we introduce multilevel B-spline interpolation to overcome this drawback. In multilevel B-spline 
interpolation, a hierarchy of control lat­tices, F0 F1 ...Fm, is overlaid over the region O to derive 
a se­quence of functions, f0 f1 ...fm.Let hibe the spacing between control points in the initial con.guration 
of lattice Fi. We assume that h0 and hmare given and that hi2hi+1. The .nal function f P is de.ned by 
the sum of the functions fi,that is, f(p) fi(p), i for each point pon O. The coarsest spacing h0 determines 
the area of the resulting surface on which an interpolated point has effect. The .nest spacing hmcontrols 
the precision to which the resulting surface interpolates the given points. The manipulation of a B-spline 
surface starts with the coarsest control lattice F0. The heights of control points on lattice F0 are 
derived to generate the surface f0 that interpolates the points in P. Sometimes, however, surface f0 
only passes near the points in P, leaving the deviation .0tctc -f0(ucvc)for each point (ucvctc)in P. 
Then, the next .ner control lattice F1 is used to obtain the B-spline surface f1 that interpolates the 
set of points (ucvc.0tc). In general, we manipulate the control points on lattice Fk+1 to derive the 
B-spline surface fk+1 that interpolates the kP k set of points (ucvc.tc),where .ktctc -fi(ucvc). i=0 
This process continues to the .nest control lattice Fmuntil the maximum difference between the points 
in Pand the .nal surface ffalls below a given threshold. Unlike the MFFD case, the B-spline surface manipulation 
is applied only once on each control lattice because the heights of the control points are not truncated. 
 Figure 6: An example of multilevel B-spline interpolation A surface generated by multilevel B-spline 
interpolation is C2­continuous because it is the sum of C2-continuous B-spline sur­faces. Fig. 6 shows 
an example. Black spots in the .gure represent the interpolated points. Most of the computation time 
is spent by evaluating function fin the region O.For a 64 x64 grid, it takes 0.1 secondsfor anSGI Crimsonto 
generatethe surfacegiven in Fig. 6. Fora512 x512 grid, the surface is generated in 1.5 seconds.  6. 
Metamorphosis examples Fig. 10 gives metamorphosis examples. Figs. 10(a) and (b) show selected frames 
from a morph sequence between Seung-Yong and George (two of the authors), and Linda. All the images in 
Fig. 10(a) were generated using the same transition rate everywhere. The transition rates were allowed 
to vary to generate the images in Fig. 10(b). Fig. 10(c) shows the speci.ed features overlaid on the 
input images. The bottom two inbetween images in that column demonstrate the effect of a procedural transition 
function. Fig. 10(d) shows the frames of a morph sequence between Linda and Seung-Yong. Note that transition 
control is applied to overcome the considerable differences between the hairlines. One set of transition 
curves for the source image is suf.cient to relatethetransitionbehaviorinamorphsequence. Figs.8(a)and(b) 
show the primitives upon which transition curves are de.ned for the inbetween images given in Figs. 10(b) 
and (d), respectively. Each primitive may have a different transition curve, with all points along a 
primitive sharing the same transition rate. Figs. 8(c) and (d) depict the transition curves for the respective 
outer and inner primitives of Figs. 8(a) and (b). For the inbetween images shown in Fig. 10(c), linear 
functions in yare applied to vary the transformation between the source and destination images. In the 
.rst inbetween image, Seung-Yong is changed into George from top to bottom. Similarly, George is transformed 
into Seung-Yong in the second image. Fig. 9(a) illustrates the warp function generated for transforming 
Seung-Yong to George in Fig. 10(a). Dark lines represent the new positions of feature points that have 
been internally sampled on the source image. Fig. 9(b) shows the surface interpolating through the transition 
rates that are evaluated from the transition curves along the primitives in Fig. 8. All images shown 
in Fig. 10 are 468x480 and were generated on a SUN SPARCsystem 10. We used the hybrid method to derive 
the warp functions and multilevel B-spline interpolation to compute the surfaces for transition control. 
It took 22.0 and 1.0 seconds, respectively, to generate the warp function and surface in Fig. 9. 7. 
Conclusions This paper has presented new solutions to the following three prob­lems in image morphing: 
feature speci.cation, warp generation, and surface generation for transition control. The features in 
an image can be speci.ed with snakes [8], a popular computer vision technique. Snakes help an animator 
to easily and precisely capture the exact position of a feature. They also may reduce the work of ananimatorin 
establishingthe feature correspondencebetweentwo image sequences. We introduced a new deformation technique, 
the MFFD, which derives C2-continuous and one-to-one warps from feature point pairs. The technique is 
fast, even when the number of features is large. The resulting warps provide visually pleasing im­age 
distortions. We also presented multilevel B-spline interpolation to construct smooth surfaces that are 
used to control geometry and color blending. The method ef.ciently generates a C2-continuous surface 
that interpolates a set of scattered points. The warp and surface generation techniques in this paper 
may be applied to other areas of computer graphics. The MFFD can be readily extended to 3D and used to 
directly manipulate the shape of deformable objects. Multilevel B-spline interpolation can be used to 
rapidly generate free-form surfaces from positional constraints. 8. Acknowledgements This work was supported 
in part by the Korean Ministry of Science and Technology (contract 94-S-05-A-03 of STEP 2000), NSF PYI 
award IRI-9157260, and PSC-CUNY grant RF-665313.  References [1] Ballard, Dana H., and Christopher 
M. Brown. Computer Vi­sion. Prentice-Hall, 1982. [2] Beier, Thaddeus, and Shawn Neely. Feature-Based 
Image Metamorphosis. Computer Graphics 26, 2 (1992), 35 42. [3] Benson, Philip J. Morph Transformation 
of the Facial Image. Image and Vision Computing 12, 10 (1994), 691 696. [4] Coquillart, Sabine. Extended 
Free-Form Deformation: A Sculpturing Tool for 3D Geometric Modeling. Computer Graphics 24, 4 (1990), 
187 196. [5] Coquillart, Sabine, and Pierre Jancene. Animated Free-Form Deformation: An Interactive Animation 
Technique.Computer Graphics 25, 4 (1991), 23 26. [6] Goodman, Tim, and Keith Unsworth. Injective Bivariate 
Maps. Tech. Rep. CS94/02, Dundee University, U.K., 1994. [7] Hsu, William M., John F. Hughes, and Henry 
Kaufman. Direct Manipulation of Free-Form Deformations. Computer Graph­ics 26, 2 (1992), 177 184. [8] 
Kass, Michael, Andrew Witkin, and Demetri Terzopoulos. Snakes: Active Contour Models. International Journal 
of Computer Vision (1988), 321 331. [9] Lee, Seung-Yong. Image Morphing Using Scattered Feature Interpolations. 
PhD thesis, KAIST, Taejon, Korea, February 1995. [10] Lee, Seung-Yong, Kyung-Yong Chwa, James Hahn, 
and Sung Yong Shin. Image Morphing Using Deformable Sur­faces. In Proceedings of Computer Animation 94 
(Geneva, Switzerland, 1994), IEEE Computer Society Press, pp. 31 39. [11] Lee, Seung-Yong, Kyung-Yong 
Chwa, James Hahn, and Sung Yong Shin. Image Morphing Using Deformation Tech­niques. The Journal of Visualization 
and Computer Animation 6, 3 (1995). [12] Litwinowicz, Peter, and Lance Williams. Animating Images with 
Drawings. In SIGGRAPH 94 Conference Proceedings (1994), ACM Press, pp. 409 412. [13] Meisters, G.H., 
and C. Olech. Locally One-to-one Mappings and a Classical Theorem on Schlicht Functions. Duke Math­ematical 
Journal 30 (1963), 63 80. [14] Nishita, Tomoyuki, Toshihisa Fujii, and Eihachiro Nakamae. Metamorphosis 
Using B´ezier Clipping. In Proceedings of the First Paci.c Conference on Computer Graphics and Applica­tions 
(Seoul, Korea, 1993), World Scienti.c Publishing Co., pp. 162 173. [15] Nishita, Tomoyuki, Thomas Sederberg, 
and Masanori Kaki­moto. Ray Tracing Trimmed Rational Surface Patches. Com­puter Graphics 24, 4 (1990), 
337 345. [16] Ruprecht, Detlef, and Heinrich M¨uller. Image Warping with Scattered Data Interpolation. 
IEEE Computer Graphics and Applications 15, 2 (1995), 37 43. [17] Sederberg, Thomas W., and Scott R. 
Parry. Free-Form Defor­mation of Solid Geometric Models. Computer Graphics 20, 4 (1986), 151 160. [18] 
Terzopoulos, Demetri. Multilevel Computational Processes for Visual Surface Reconstruction. Computer 
Vision, Graph­ics, and Image Processing 24 (1983), 52 96. [19] Wolberg, George. Digital Image Warping. 
IEEE Computer Society Press, Los Alamitos, CA, 1990. Appendix: Proof of Theorem 1 0 *x *x Let .rijrij-rijand 
w(xy). Supposethat jjand *u*v *y *y jjat each point on the domain O when (-0.48 -0.48): *v*u *x*y .rij:(0.48 
0.48)for all ij. Then, the Jacobian, J­ *u*v *x*y , is greater than zero at all points in O including 
the bound­ *v*uary, which implies that function wis one-to-one [13]. Let rij 000 0 *y *y (xij yij), rij(xijyij),and 
.yijyij-yij. jjif *v*uy*y y *y and only if * and * -. In what follows, we only *v*u *v*u *y *y show that 
if -0.48 :.yij:0.48. A symmetrical *v*u y *y argument can be applied to the case when * -. Similarly, 
*v*u *x *x we can prove the remaining case, jj. *u*v From Eqs. (1) and (2), we have 3 3 XX yBk(s)Bl(t)y(i+k)(j+l) 
k=0 l=0 3 3 XX v+ Bk(s)Bl(t).y(i+k)(j+l) k=0 l=0 and hence, yy - vu 3 3 X X 00 1 +(Bk(s)Bl(t)-Bk(s)Bl(t)).y(i+k)(j+l). 
k=0 l=0 00 Let cklBk(s)B(t)-B(s)Bl(t). From the formulae of B-spline basis functions, it holds that Bk(t)20, 
for i0 1 23, B00(t):0, B10(t):0, B20(t)20, and B30(t)20when 0 :t:1. Therefore, it immediately follows 
that c20 c30 c21 c31 :0and c02 c12 c03 c13 20. Ifwelet ts+.t,then c10 -(s+.t­1)2((s-2)2+(4s-3s 2).t)/12. 
Since .t2-sand (4s-3s 2)20 when 0 :s:1, we get (4s-3s 2).t2-s(4s-3s 2)2-1, which implies c10 :0. Similarly, 
it can be proved that c32 :0, c01 20, and c23 20when0 :st:1. From the fact that c00 (s-t)(s-1)2(t-1)2/12 
and c33 (s-t)s 2t2/12, it follows that if s2t,then c00 c33 20and if s.t,then c00 c33 :0. By manipulating 
the formula of c11, we get c11 (s-t)(3(st+1)(3st-4s-4t+5)+1)/12. lk*f Let f(st)3st-4s-4t+5. Then, 3t-4 
.0and *s *f 3s-4 .0when 0 :st:1, which implies that fhas a *t global minimum at (11). Because f(11)0, 
c11 20if s2t and c11 .0if s.t. Similarly, it can be shown that c22 20if s2tand c22 .0if s.t. In summary, 
ckl20if k.land ckl:0if klwhen 0 :st:1. Also, if s2t,then ckk20, and if s.t,then ckk:0. We consider the 
case when s2t.Let 3 k.1 33  XXXX Cckl -ckl. k=0 l=0 k=0 l=k Cis a function of sand tde.ned on 0 :st:1. 
From the condition that -0.48 :.yij:0.48 and the properties of the values of ckl, it holds that yy - 
vu  3 k. 1 33  X XXX 1 +ckl.y(i+k)(j+l)+ ckl.y(i+k)(j+l) k=0 l=0 k=0 l=k 3 k.1 33  XXXX 21 +0.48 
ckl -ckl k=0 l=0 k=0 l=k 1 +0.48C. To derive a lower bound of C, we partition the domain 0 :st: 1 to 
the grid in which the internode distance .dis 0.0001. When Cis evaluated at each grid point by 64 bits 
double-precision arithmetic, the minimum value is -2.0463927 at (s0 t0)(0.7552 0.2448). Let (sgtg)be 
a grid point and DC(sg+.stg+.t)­ ..6 C(sgt),where 0 :.s.t..d. Dconsists of terms st.s'.t, ggg where 
a,, 0 1 2 3. To simplify the formula of D,we as­sign sgtg0and sgtg1to the termsin Dhaving positive ' 
 and negative coef.cients, respectively. Then, from .d(.s)and (a) (b) 64230 .d(.t), it holds that D-c.dfor 
c36 . transition transition Let (st)be a point on the domain 0 :st:1. Let sg rate rate .dbs/.dcand 
tg.dbt/.dc.Let .ss-sgand .tt-tg. 1 Then, C(st) C(sg+.stg+.t)C(sgtg)-c.d2 1 C(s0 t0)-c.d2-2.0581427. 
Hence, 1 +0.48C0on the *y*y domain 0 :st:1, which implies that . The case when *v*u s.tcan be treated 
similarly. 2 0 time 0 time 0.3 0.7 1 (c) (d) (a) (b) Figure 9: Warp function and surface  Figure 8: 
Primitives with transition curves    Figure 7: Feature speci.cation; (a)-(f) are shown from left-to-right 
and top-to-bottom. (a) (b) (c) (d) Figure 10: Metamorphosis examples 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218502</article_id>
		<sort_key>449</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>56</seq_no>
		<title><![CDATA[Feature-based volume metamorphosis]]></title>
		<page_from>449</page_from>
		<page_to>456</page_to>
		<doi_number>10.1145/218380.218502</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218502</url>
		<keywords>
			<kw><![CDATA[blending]]></kw>
			<kw><![CDATA[computer animation]]></kw>
			<kw><![CDATA[rendering]]></kw>
			<kw><![CDATA[sculpting]]></kw>
			<kw><![CDATA[shape interpolation]]></kw>
			<kw><![CDATA[transformation]]></kw>
			<kw><![CDATA[volume morphing]]></kw>
			<kw><![CDATA[warping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.10</cat_node>
				<descriptor>Morphological</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P21201</person_id>
				<author_profile_id><![CDATA[81100347910]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Apostolos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lerios]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Integrated Systems, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P44270</person_id>
				<author_profile_id><![CDATA[81100053569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chase]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Garfinkle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Integrated Systems, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15037068</person_id>
				<author_profile_id><![CDATA[81100593780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Levoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Center for Integrated Systems, Stanford University, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely. Pacific Data Images. Personal communication.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134003</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Beier and S. Neely. Feature-based image metamorphosis. In Computer Graphics, vo126(2), pp 35-42, New York, NY, July 1992. Proceedings of SIGGRAPH '92.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[B. R Bergeron. Morphing as a means of generating variation in visual medical teaching materials. Computers in Biology and Medicine, 24( 1 ): 11-18, Jan. 1994.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>197972</ref_obj_id>
				<ref_obj_pid>197938</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[B. Cabral, N. Cam, and J. Foran. Accelerated volume rendering and tomographic reconstruction using texture mapping hardware. In A. Kaufman and W. Krueger, editors, P1vceedings of the 1994 Symposium on Volume Visualization, pp 91-98, New York, NY, Oct. 1994. ACM SIGGRAPH and IEEE Computer Society.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Chen, M. W. Jones, and R Townsend. Methods for volume metamorphosis. To appear in Image P~vcessing for Broadcast and Video P~vduction, Y. Paker and S. Wilbur editors, Springer-Verlag, London, 1995.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M. Covell and M. Withgott. Spanning the gap between motion estimation and morphing. In P1vceedings of IEEE International Conference on Acoustics, Speech and Signal P1vcessing, vol 5, pp 213-216, New York, NY, 1994. IEEE.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>122747</ref_obj_id>
				<ref_obj_pid>127719</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[T.A. Galyean and J. F. Hughes. Sculpting: An interactive volumetric modeling technique. In Computer Graphics, vo125(4), pp 267-274, New York, NY, July 1991. Proceedings of SIGGRAPH '91.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951107</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[T. He, S. Wang, and A. Kaufman. Wavelet-based volume morphing. In D. Bergeron and A. Kaufman, editors, P~vceedings of Visualization '94, pp 85-91, Los Alamitos, CA, Oct. 1994. IEEE Computer Society and ACM SIGGRAPH.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134004</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[J. F. Hughes. Scheduled Fourier volume morphing. In Computer Graphics, vol 26(2), pp 43-46, New York, NY, July 1992. Proceedings of SIGGRAPH '92.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>558409</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[A. Kaufman, D. Cohen, and R. Yagel. Volume graphics. Computer, 26(7):51-64, July 1993.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[A. Kaul and J. Rossignac. Solid-interpolating deformations: Construction and animation of PIPs. In F. H. Post and W. Barth, editors, Eurographics '91, pp 493-505, Amsterdam, The Netherlands, Sept. 1991. Eurographics Association, North-Holland.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>134007</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[J.R. Kent, W. E. Carlson, and R. E. Parent. Shape transformation for polyhedral objects. In Computer Graphics, vo126(2), pp 47-54, New York, NY, July 1992. Proceedings of SIGGRAPH '92.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>192187</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[R Litwinowicz. Efficient techniques for interactive texture placement. In Computer Graphics Proceedings, Annual Conference Series, pp 119-122, New York, NY, July 1994. Conference Proceedings of SIGGRAPH '94.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3-D surface construction algorithm. In Computer Graphics, vol 21 (4), pp 163-169, New York, NY, July 1987. Proceedings of SIGGRAPH '87.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74359</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[K. Perlin and E. M. Hoffert. Hypertexture. In Computer Graphics, vo123(3), pp 253-262, New York, NY, July 1989. Proceedings of SIGGRAPH '89.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>166118</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[T.W. Sedeberg, R Gao, G. Wang, and H. Mu. 2-D shape blending: An intrinsic solution to the vertex path problem. In Computer Graphics P~vceedings, Annual Conference Series, pp 15-18, New York, NY, Aug. 1993. Conference Proceedings of SIGGRAPH '93.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>15903</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[T.W. Sederberg and S. R. Parry. Free-form deformations of solid geometric models. In Computer Graphics, vo120(4), pp 151-160, New York, NY, Aug. 1986. Proceedings of SIGGRAPH '86.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[R A. van den Elsen, E.-J. D. Pol, and M. A. Viergever. Medical image matching -- a review with classification. IEEE Engineering in Medicine and Biology Magazine, 12(1):26-39, Mar. 1993.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>199430</ref_obj_id>
				<ref_obj_pid>199404</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[S.W. Wang and A. Kaufman. Volume sculpting. InP1vceedings of 1995 Symposium on Interactive 3D Graphics, pp 151-156,214, New York, NY, Apr. 1995. ACM SIGGRAPH.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>949864</ref_obj_id>
				<ref_obj_pid>949845</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[S.W. Wang and A. E. Kaufman. Volume sampled voxelization of geometric primitives. In G. M. Nielson and D. Bergeron, editors, P~vceedings of Visualization '93, pp 78-84, Los Alamitos, CA, Oct. 1993. IEEE Computer Society and ACM SIGGRAPH.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>528718</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[G. Wolberg. Digital Image Warping. IEEE Computer Society R, Los Alamitos, CA, 1990.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Feature-Based Volume Metamorphosis Apostolos Lerios, Chase D. Gar.nkle, Marc Levoy . Computer Science 
Department Stanford University Abstract Image metamorphosis, or image morphing, is a popular tech­nique 
for creating a smooth transition between two images. For synthetic images, transforming and rendering 
the underlying three-dimensional (3D) models has a number of advantages over morphing between two pre-rendered 
images. In this paper we con­sider 3D metamorphosis applied to volume-based representations of objects. 
We discuss the issues which arise in volume morphing and present a method for creating morphs. Our morphing 
method has two components: .rst a warping of the two input volumes, then a blending of the resulting 
warped volumes. The warping component, an extension of Beier and Neely s image warping technique to 3D, 
is feature-based and allows .ne user control, thus ensuring realistic looking intermediate objects. In 
addition, our warping method is amenable to an ef.cient approximation which gives a 50 times speedup 
and is computable to arbitrary accuracy. Also, our technique corrects the ghosting problem present in 
Beier and Neely s technique. The second component of the morphing process, blending, is also under user 
control; this guarantees smooth transitions in the renderings. CR Categories: I.3.5 [Computer Graphics]: 
Computational Geometry and Object Modeling; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and 
Realism. Additional Keywords: Volume morphing, warping, render­ing; sculpting; shape interpolation, transformation, 
blending; computer animation. 1 Introduction 1.1 Image Morphing versus 3D Morphing Image morphing, the 
construction of an image sequence depicting a gradual transition between two images, has been extensively 
in­vestigated [21] [2] [6] [16]. For images generated from 3D models, there is an alternative to morphing 
the images themselves: 3D mor­phing generates intermediate 3D models, the morphs, directly from the given 
models; the morphs are then rendered to produce an image sequencedepictingthetransformation. 3Dmorphingovercomesthe 
following shortcomings of 2D morphing as applied to images gen­erated from 3D models: . Center for Integrated 
Systems, Stanford University, Stanford, CA 94305 flerios,cgar,levoyg@cs.stanford.edu http://www-graphics.stanford.edu/ 
 Permission to make digital/hard copy of part or all of this work for personal or classroom use is granted 
without fee provided that copies are not made or distributed for profit or commercial advantage, the 
copyright notice, the title of the publication and its date appear, and notice is given that copying 
is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute 
to lists, requires prior specific permission and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 
 In 3D morphing, creating the morphs is independent of the viewing and lighting parameters. Hence, we 
can create a morph sequence once, and then experiment with various cam­era angles and lighting conditions 
during rendering. In 2D morphing, a new morph must be recomputed every time we wish to alter our viewpoint 
or the illumination of the 3D model.  2Dtechniques,lackinginformation onthemodel sspatialcon­.guration, 
are unable to correctly handle changes in illumina­tion and visibility. Two examples of this type of 
artifact are:  (i) Shadows and highlights fail to match shape changes occur­ing in the morph. (ii) When 
a feature of the 3D object is not visible in the original 2D image, this feature cannot be made to appear 
during the morph; for example, when a singing actor needs to open her mouth during a morph, pulling her 
lips apart thickens the lips instead of revealing her teeth.  1.2 Geometric versus Volumetric 3D Models 
The models subjected to 3D morphing can be described either by ge­ometric primitives or by volumes (volumetric 
data sets). Each rep­resentation requires different morphing algorithms. This dichotomy parallels the 
separation of 2D morphing techniques into those that operate on raster images [21] [2] [6], and those 
that assume vector­basedimagerepresentations[16]. Webelievethatvolume-basedde­scriptions are more appropriate 
for 3D morphing for the following reasons: The quality and applicability of geometric 3D morphing tech­niques 
[12] is highly dependent on the models geometric primitives and their topological properties. Volume 
morphing is independent of object geometries and topologies, and thus imposes no such restrictions on 
the objects which can be suc­cessfully morphed.  Volume morphing may be applied to objects represented 
either by geometric primitives or by volumes. Geometric descrip­tions can be easily converted to high-quality 
volume represen­tations, as we will see in section 2. The reverse process pro­duces topologically complex 
objects, usually inappropriate for geometric morphing.  1.3 Volume Morphing The 3D volume morphing 
problem can be stated as follows. Given two volumes Sand T, henceforth called the source and target vol­umes, 
we must produce a sequence of intermediate volumes, the morphs, meeting the following two conditions: 
Realism: The morphs should be realistic objects which have plau­ sible 3D geometry and which retain the 
essential features of the source and target. Smoothness: The renderings of the morphs must depict a 
smooth transition from Sto T. From the former condition stems the major challenge in designing a 3D morphing 
system: as automatic feature recognition and match­inghaveyettoequalhumanperception,userinputis crucialinde.n­ing 
the transformation of the objects. The challenge for the designer Manual, but done once Automatic, repeated 
for each frame of the morph Figure 1: Data .ow in a morphing system. Editing comprises retouching and 
aligning the volumes for cosmetic reasons. of a 3D morphing technique is two-fold: the morphing algorithm 
must permit .ne user control and the accompanying user interface (UI) should be intuitive. Our solution 
to 3D morphing attempts to meet both conditions of the morphing problem, while allowing a simple, yet 
powerful UI. To this end, we create each morph in two steps (see .gure 1): Warping: Sand Tare warped 
to obtain volumes S0and T0.Our warping technique allows the animator to de.ne quickly the exact shape 
of objects represented in S0and T0, thus meeting the realism condition. 0 Blending: S0and Tare combined 
into one volume, the morph. Our blending technique provides the user with suf.cient con­trol to create 
a smooth morph.  1.4 Prior Work Prior work on feature-based 2D morphing [2] will be discussed in section 
3. Prior work in volume morphing comprises [9], [8], and [5]. These approaches can be summarized in terms 
of our warp­ing/blending framework. [5] and [8] have both presented warping techniques. [5] exam­ined 
the theory of extending selected 2D warping techniques into 3D. A UI was not presented, however, and 
only morphs of simple objects were shown. [8] presents an algorithm which attempts to au­tomatically 
identify correspondencesbetween the volumes,without the aid of user input. [9] and [8] have suggested 
using a frequency or wavelet repre­sentation of the volumes to perform the blending, allowing different 
interpolation schedules across subbands. In addition, they have ob­served that isosurfaces of the morphs 
may move abruptly, or even completely disappear and reappear as the morph progresses, de­stroying its 
continuity. This suggests that volume rendering may be superior to isosurface extraction for rendering 
the morphs. Our paper is partitioned into the following sections: Section 2 covers volume acquisition 
methods. Sections 3 and 4 present the warping and blending steps of our morphing algorithm. Section 4.2 
describes an ef.cient implementation of our warping method and section 5 discusses our results. We conclude 
with suggestions for future work and applications in section 6.  2 Volume Acquisition Volume data may 
be acquired in several ways, the most common of which are listed below. Scanned volumes: Some scanning 
technologies, such as Comput­erized Tomography (CT) or Magnetic Resonance Imaging (MRI) generate volume 
data. Figures 5(a) and 5(c) show CT scans of a human and an orangutan head, respectively. Scan converted 
geometric models: A geometric model can be voxelized [10], preferably with antialiasing [20], generating 
a volume-based representation of the model. Figures 6(a), 6(b), 7(a), and 7(b) show examples of scan-converted 
volumes. Interactive sculpting: Interactive modeling, or sculpting [19] [7], can generate volume data 
directly. Procedural de.nition: Hypertexture volumes [15] can be de.ned procedurally by functions over 
3D space. 3 Warping The .rst step in the volume morphing pipeline is warping the source and target volumes 
Sand T. Volume warping has been the subject of several investigations in computer graphics, computervision, 
and medicine. Warping techniques can be coarsely classi.ed into two groups: (i) Techniques that allow 
only minimal user control, con­sisting of at most a few scalar parameters. These algorithms au­tomatically 
determine similarities between two volumes, and then seek the warp which transforms the .rst volume to 
the second one [18]. (ii) Techniques in which user control consists of manually specifying the warp for 
a collection of points in the volume. The rest of the volume is then warped by interpolating the warping 
function. This group of algorithms includes free-form deformations [17], as well as semi-automatic medical 
data alignment [18]. As stated in section 1.3, user control over the warps is crucial in designing good 
morphs. Point-to-point mapping methods [21], in the form of either regular lattices or scattered points 
[13], have worked in 2D. However, regular grids provide a cumbersome inter­face in 2D; in 3D they would 
likely become unmanageable. Also, prohibitively many scattered points are needed to adequately spec­ify 
a 3D warp. Our solution is a feature-based approach extending the work of [2] into the 3D domain. The 
next two sections will introduce our feature-based 3D warping and discuss the UI to feature speci.ca­tion. 
3.1 Feature-Based 3D Warping using Fields The purpose of a feature element is to identify a feature of 
an object. For example, consider the X-29 plane of .gure 6(b); an element can be used to delineate the 
nose of the plane. In feature-based mor­phing, elements come in pairs, one element in the source volume 
S, and its counterpart in the target volume T. A pair of elements identi.es corresponding features in 
the two volumes, i.e. features that should be transformed to one another during the morph. For instance, 
when morphing the dart of .gure 6(a) to the X-29 plane, the tip of the dart should turn into the nose 
of the plane. In order to obtain good morphs, we need to specify a collection of element pairs which 
de.ne the overall correspondence of the two objects. These element pairs interact like magnets shaping 
a pliable volume: Warp Warp    (a) (b) Figure 2: 2D warp artifacts (not to scale). (a) shows the 
result of squeezing a circle using two feature lines placed on opposite sides of the circle. The warped 
circle spills outside the corresponding, closely spaced, lines. Similarly, in (b), the narrow ellipsoid 
with two lines on either side does not expand to a circle when the lines are drawn apart; we get instead 
three copies of the ellipsoid. while a single magnet can only move, turn, and stretch the volume, multiple 
magnets generate interacting .elds, termed in.uence .elds, which combine to shape the volume in complex 
ways. Sculpting with multiple magnetsbecomeseasierif wehavemagnetsofvarious kinds in our toolbox, each 
magnet generating a differently shaped in.uence.eld. Theelementsinourtoolkitarepoints,linesegments, rectangles, 
and boxes. In the following presentation, we .rst describe individual ele­ments, and discuss how they 
identify features. We then show how a pair of elements guarantees that corresponding features are trans­formed 
to one another during the morph. Finally, we discuss how multiple element pairs interact.  Individual 
Feature Elements Individual feature elements should be designed in a manner such that they can delineate 
any feature an object may possess. How­ever,expressivenessshouldnot sacri.cesimplicity, ascomplexfea­tures 
can still be matched by a group of simple elements. Hence, the de.ning attributes of our elements encode 
only the essential charac­teristics of features: Spatial con.guration: The feature s position and orientation 
are encoded in an element s local coordinate system, comprising four vectors. These are the position 
vector of its origin c,and three mutually perpendicular unit vectors x, yand z, de.ning the directions 
of the coordinate axes. The element s scaling factors sx, sy,and sde.ne a feature s extent along each 
of z the principal axes. Dimensionality: The dimensionality of a feature depends on the subjective perception 
of a feature s relative size in each dimen­sion: the tip of the plane s nose is perceived as a point, 
the edge of the plane s wing as a line, the dart s .n as a surface, and the dart s shaft as a volume. 
Accordingly, our simpli.ed elements have a type, which can be a point, segment, rectan­gle, or box. In 
our magnetic sculpting analogy, the element type determines the shape of its in.uence .eld. For example, 
a box magnet de.nes the path of points within and near the box; points further from the box are in.uenced 
less as their distance increases. The reader familiar with the 2D technique of [2] will notice two differences 
between our 3D elements and a direct extention of 2D feature lines into 3D; in fact, these are the only 
differences as far as the warping algorithm is concerned. First, in the 2D technique, the shape of a 
feature line s in.uence .eld is controlled by two manually speci.ed parameters. Instead, we provide four 
simple types of in.uence .elds point, segment, rectangle, and box thus allowing for a more intuitive, 
yet equally powerful, UI. Second, our feature elements encode the 3D extent of a 3D fea­ture via the 
scaling factors sx, sy,and s; by contrast, feature lines z in [2] capture only the 1D extent of a 2D 
feature, in the direction of each feature line. These scaling factors introduce additional degrees of 
freedom for each feature element. In the majority of situations, these extra degrees have a minor effect 
on the warp and may thus be ignored. However, under extreme warps, they permit the user to solve the 
ghosting problem, documented in [2] and illustrated in .g­ure 2. For instance, in part (b) of this example, 
the ellipsoid is repli­cated because each feature line requires that an unscaled ellipsoid appear by 
its side: the feature lines in [2] cannot specify any stretch­ingintheperpendiculardirection. However,ina2Danalogueofour 
technique, the user would use the lines scaling factors to stretch the ellipsoid. First, the user would 
encode the ellipsoid s width in the scaling factors of the original feature lines. Then, in order to 
stretch the ellipsoid into a circle, the user would not only move the feature lines apart, but will also 
make the lines scaling factors encode the desired new width of the ellipsoid. In fact, using our technique, 
a single feature line suf.ces to turn the ellipsoid into a circle. Element Pairs As in the 2D morphing 
system of [2], the animator identi.es two corresponding features in Sand T, by de.ning a pair of elements 
(eset). Thesefeaturesshouldbetransformedtooneanotherduring the morph. Such a transformation requires 
that the feature of Sbe moved, turned, and stretched to match respectively the position, ori­entation, 
and size of the corresponding feature of T. Consequently, for each frame of the morph, our warp should 
generate a volume S0 from Swith the following property: the feature of Sshould possess an intermediate 
position, orientation and size in S0. This is achieved by computing the warp in two steps: Interpolation: 
We interpolate the local coordinate systems1and scaling factors of elements esand etto produce an interpo­lated 
element e 0. This element encodes the spatial con.gura­tion of the feature in S0 . 0 Inverse mapping: 
For every point in pof S0, we.nd thecorre­sponding point pin Sin two simple steps (see .gure 3): (i) 
We 0 .nd the coordinates of pin the scaled local system of element e 0 by px (p 0-c 0).x 0/s0 x py 
 (p 0-c 0).y 0/s0 y pz (p 0-c 0).z 0/s0 z . (ii) p is the point with coordinates px, py and pz in the 
scaled local system of element es, i.e. the point 2 +px+py+pz csxxsyyszz.  Collections of Element Pairs 
In extending the warping algorithm of the previous paragraph to multiple element pairs, we adhere to 
the intuitive mental model of magnetic sculpting used in [2]. Each pair of elements de.nes a .eld that 
extends throughout the volume. A collection of element pairs de.nes a collection of .elds, all of which 
in.uence each point in the volume. We therefore use a weighted averaging scheme to deter­ 0 mine the 
point pin Sthat corresponds to each point pof S0.That is, we .rst compute to what point pieach element 
pair would map 0 pin the absence of all other pairs; then, we average the pi s using 0 a weighting 
function that depends on the distance of pto the inter­ 0 polated elements ei. Our weighting scheme 
usesan inverse square law: piis weighted 00 by (d+E).2where dis the distance of pfrom the element e; 
Eis a i 1The axes directions x, y,and zare interpolated in spherical coordinates to ensure smooth rotations. 
0 2Tis warped into Tin a similar way, the only difference being that et is used in this last step in 
place of es. p z c z z x x Warped volume S Figure 3: Single element warp. In order to .nd the point 
pin vol­ 0 ume Sthat corresponds to pin S0, we .rst .nd the coordinates 0 (pxpy p)of pin the scaled local 
system of element e 0; pis then z the point with coordinates (pxpy p)in the scaled local system of zelement 
es. To simplify the .gure, we have assumed unity scaling factors for all elements. small constant used 
to avoid division by zero.3The type of element 0 eidetermines how dis calculated: 0 Points: dis the distance 
between pand the origin cof the local 0 coordinate system of element ei. This de.nition is identical 
to [21]. Segments: The element is treated as a line segment centered at the origin c, aligned with the 
local x-axis and having length sx; d 0 is the distance of pfrom this line segment. This de.nition is 
identical to [2]. Rectangles: Rectangles have the same center and xextent as seg­ments, but also extend 
into a second dimension, having width 0 syalong the local y-axis. dis zero if pis on the rectangle, 0 
otherwise it is the distance of pfrom the rectangle. This def­inition extends segments to area elements. 
Boxes: Boxes add depth to rectangles, thus extending for szunits 0 along the local z-axis. dis zero if 
pis within the box, other­ 0 wise it is the distance of pfrom the box s surface. The reader will notice 
that the point, segment, and rectangle ele­ment types are redundant, as far as the mathematical formulation 
of our warp is concerned. However, a variety of element types main­tains best the intuitive conceptual 
analogy to magnetic sculpting. 3.2 User Interface The UI to the warping algorithm has to depict the 
source and tar­get volumes, in conjunction with the feature elements. Hardware­assisted volume rendering 
[4] makes possible a UI solely based on direct visualization of the volumes, with the embedded elements 
interactively scan-converted. Using a low-end rendering pipeline, however, the UI has to resort to geometric 
representations of the models embedded in the volumes. These geometric representations can be obtained 
in either of two ways: Pre-existing volumes are visualized by isosurface extraction via marching cubes 
[14]. Several different isosurfaces can be extracted to visualize all prominent features of the volume, 
a volume rendering guiding the extraction process.  For volumes that were obtained by scan converting 
geometric models, the original model can be used.  Once geometric representations of the models are 
available, the animator can use the commercial modeler of his/her choice to spec­ify the elements. Our 
system, shown in .gure 6(d), is based on In­ventor, the Silicon Graphics (SGI) 3D programming environment. 
Models are drawn in user-de.ned materials, usually translucent, in 3Distance measurements postulate cubical 
volumes of unit side length. Also, we always set .to 0.001. order to distinguish them from the feature 
elements. These, in turn, are drawn in such a way that their attributes local coordinate sys­tem, scaling 
factors, and dimensionality are graphically depicted and altered using a minimal set of widgets.  
4 Blending Thewarping stephasproducedtwo warpedvolumes S0and Tfrom the source and target volumes Sand 
T. Any practical warp is likely to misalign some features of Sand T, possibly because these were not 
speci.cally delineated by feature elements. Even if perfectly aligned, matching features may have different 
opacities. These ar­eas of the morph, collectively called mismatches, will have to be smoothly faded 
in/out in the rendered sequence, in order to maintain the illusion of a smooth transformation. This is 
the goal of blending. We have two alternatives for performing this blending step. It may either be done 
by cross-dissolving images rendered from S0 and T0, which we call 2.5D morphing, or by cross-dissolving 
the volumes themselves, and rendering the result, i.e. a full 3D morph. The 2.5D approachproducessmooth 
image sequencesandprovides the view and lighting independence of 3D morphing discussed in section 1.1; 
however, some disadvantages of 2D morphing are rein­troduced, such as incorrect lighting and occlusions. 
Consequently, 2.5D morphs do not look as realistic as 3D morphs. For example, the missing link of .gure 
5(f) lacks distinct teeth, and the base of the skull appears unrealistically transparent. For this reason, 
we decided to investigate full 3D morphing, whereby we blend the warped volumes by interpolating their 
voxel values. The interpolation weight w(t)is a function that varies over time, where time is the normalized 
frame number 4.We have the option of using either a linear or non-linear w(t). 4.1 Linear Cross-Dissolving 
The pixel cross-dissolving of 2D morphing suggests a linear w(t). Indeed, it works well for blending 
the color information of S0and T0. However, it fails to interpolate opacities in a manner such that therenderedmorphsequenceappearssmooth. 
Thisisduetotheex­ponential dependence of the color of a ray cast through the volume on the opacities 
of the voxels it encounters. This phenomenon is il­lustrated in the morph of .gure 5. In particular, 
the morph abruptly snaps into the source and target volumes if a linear w(t)is used: .g­ure 5(g) shows 
that at time 0.06, very early in the morph, the empty space towards the front of the human head has already 
been .lled in by the warped orangutan volume. 4.2 Non-Linear Cross-Dissolving In order to obtain smoothly 
progressing renderings, we would like to compensate for the exponential dependence of rendered color 
on 0 opacity as we blend S0and T. This can be done by devising an appropriate w(t). In principle, there 
cannot exist an ideal compensating w(t).The exact relationship between rendered color and opacity depends 
on thedistancetheraytravelsthroughvoxelswiththisopacity. Hencea globally applied w(t)cannotcompensateat 
oncefor all mismatches since they have different thickness. Even a locally chosen w(t) cannot work, as 
different viewpoints cast different rays through the morph. 00 In practice, the mismatches between Sand 
Tare small in num­ber and extent. Hence, the above theoretical objections do not pre­vent us from empirically 
deriving a successful w(t). Our design goal is to compensate for the exponential relation of rendered 
color 4In other words, time is a real number linearly increasing from 0 to 1 as the morph unfolds. Image 
I Warped Image I  Figure 4: 2D analogue of piecewise linear warping. A warped im­age I0is .rst subdividedby 
an adaptive grid of squares, here marked by solid lines. Then, each square vertex is warped into I. Finally, 
pixels in the interior of each grid cell are warped by bilinearly in­terpolating the warped positions 
of the vertices. The dashed arrows demonstrate how the interior of the bottom right square is warped. 
The dotted rectangles mark image buffer borders. to opacity by interpolating opacities at the rate of 
an inverse expo­nential. The sigmoid curve given by tan.1(2s(t-0.5))1 + 2tan.1s 2 satis.es this requirement. 
It suppresses the contribution of T0 s opacity in the early part of the morph, the degree of suppression 
controlled by the blending parameter s. Similarly, the contribution of T0 s opacity is enhanced in the 
latter part of the morph. Fig­ure 5(h), illustrates the application of compensated interpolation to the 
morph of .gure 5: in contrast to .gure 5(g), .gure 5(h) looks very much like the human head, as an early 
frame in the morph se­quence should. sectionPerformance and Optimization A performance drawback of our 
feature-based warping technique is thateachpointin thewarpedvolumeis in.uencedbyallelements, since the 
in.uence .elds never decay to zero. It follows that the time to warp a volume is proportional to the 
number of element pairs. An ef.cient C++ implementation, using incremental calculations, needs 160 minutes 
to warp a single 3003volume with 30 element pairs on an SGI Indigo 2. We have implemented two optimizations 
which greatly acceler­ate the computation of the warped volume V0, where we henceforth use Vto denote 
either Sor T. First, we approximate the spatially non-linear warping function with a piecewise linear 
warp [13]. Sec­ond, we introduce an octree subdivision over V.  4.3 Piecewise Linear Approximation The 
2D form of this optimization, shown in .gure 4, illustrates its key steps within the familiar framework 
of image warping. In 3D, piecewise linear warping begins by subdividing V0into a coarse, 3D, regular 
grid, and warping the grid vertices into V, using the al­gorithm of section 3.1. The voxels in the interior 
of each cubic grid cell are then warped by trilinearly interpolating the warped positions of the cube 
s vertices. Using this method, V0can be computed by scan-converting each cube in turn. Essentially, we 
treat Vas a solid texture, with the warped grid specifying the mapping into texture space. The expensive 
computation of section 3.1 is now performed only for a small fraction of the voxels, and scan-conversion 
domi­nates the warping time. This piecewise linear approximation will not accurately capture the warp 
in highly non-linear regions, unless we use a very .ne grid. However, computing a uniformly .ne sampling 
of the warp defeats the ef.ciency gain of this approach. Hence, we use an adaptive grid which is subdivided 
more .nely in regions where the warp is highly non-linear. To determine whether a grid cell requires 
subdivision, wecomparethe exactandapproximatedwarpedpositionsofseveral pointswithinthecell. Iftheerrorisaboveauser-speci.edthreshold, 
thecellissubdividedfurther. Inordertoreducecomputation,weuse the vertices of the next-higher resolution 
grid as the points at which to measure the error. Using this technique, the non-linear warp can be approximated 
to arbitrary accuracy.5 Since we are subsampling the warping function, it is possible that this algorithm 
will fail to subdivide non-linear regions. Analytically boundingthe varianceofthewarpingfunctionwouldguaranteecon­servative 
subdivision. However, this is unnecessary in practice, as the warps used in generating morphs generally 
do not possess large high-frequency components. This optimization has been applied to 2D morphing systems, 
as well; by using common texture-mapping hardware to warp the im­ages, 2D morphs can be generated at 
interactive rates [1]. 4.4 Octree Subdivision Vusually contains large empty regions, that is, regions 
which are completely transparent. The warp will map these parts of Vinto empty regions of V0. Scan conversion, 
as described above, need not take place when a warped grid cell is wholly contained within such a region. 
By constructing an octree over V, we can identify many such cells, and thus avoid scan converting them. 
 4.5 Implementation Our optimized warping method warps a 3003 volume in approxi­mately 3 minutes per 
frame on an SGI Indigo 2. This represents a speedup of 50 over the unoptimized algorithm, without notice­able 
loss of quality. The running time is still dominated by scan­conversion and resampling, both of which 
can be accelerated by the use of 3D texture-mapping hardware.  5 Results and Conclusions Our color .gures 
show the source volumes, target volumes, and halfway morphs for three morph sequences we have created. 
The human and orangutan volumes shown in .gures 5(a) and 5(c) were warped using 26 element pairs to produce 
the volumes of .g­ures 5(b) and 5(d) at the midpoint of the morph. The blended middle morph appears in 
.gure 5(e). Figures 6 and 7 show two examples of color morphs, requiring 37 and 29 element pairs, respectively. 
The UI, displaying the elements used to control the morph of .gure 6, is shown in 6(d). The total time 
it takes to compose a 50-frame morph sequence for 3003 volumes comprises all the steps shown on .gure 
1. Our experience is that about 24 hours are necessary to turn design into reality on an SGI Indigo 2: 
Hours Task 10 CT scan segmentation, classi.cation, retouching 1 Scan conversion of geometric model 8 
Feature element de.nition (novice) 3 Feature element de.nition (expert) 5 Warping 3 Blending: 1 hour 
for each s: 2, 4, 6; retain best 4 Hi-res volume rendering (monochrome) 12 Hi-res volume rendering (color) 
 We have presented a two step feature-based technique for realis­tic and smooth metamorphosis between 
two 3D models represented by volumes. In the .rst step, our feature-based warping algorithm allows .ne 
user control, and thus ensures realistic morphs. In addi­tion, our warping method is amenable to an ef.cient, 
adaptive ap­proximation which gives a 50 times speedup. Also, our technique 5We always use an error tolerance 
of a single voxel width and an initial subdivision of 153cells. corrects the ghosting problem of [2]. 
In the second step, our user­controlled blending ensures that the rendered morph sequence ap­pears smooth. 
 6 Future Work and Applications We see the potential for improving 3D morphing in three primary aspects: 
Warping Techniques: Improved warping methods could allow for .ner user control, as well as smoother, 
possibly spline-based, interpolation of the warping function across the volume. More complex, but more 
expressive feature elements [11] may also be designed. User Interface: We envision improving our UI by 
adding computer-assisted feature identi.cation: the computer sug­gesting features by landmark data extraction 
[18], 3D edge identi.cation, or, as in 2D morphing, by motion estimation [6]. Also, we are considering 
giving the user more .exible control over the movement of feature elements during the morph, i.e. the 
rule by which interpolated elements are constructed, perhaps by key-framed or spline-path motion. Blending: 
Blending can be improved by allowing local de.nition oftheblendingrate, associatinganinterpolation schedulewith 
each feature element. Morphing s primary application has been in the entertainment industry. However, 
it can also be used as a general visualization tool for illustration and teaching purposes [3]; for example, 
our orangutan to human morph could be used as a means of visualizing Darwinian evolution. Finally, our 
feature-based warping technique can be used in modeling and sculpting.  Acknowledgments Philippe Lacroute 
helped render our morphs, and designed part of thedarttoX-29.y-bymovieshownonourvideo. Weusedthehorse 
mesh courtesy of Rhythm &#38; Hues, the color added by Greg Turk. John W. Rick provided the plastic cast 
of the orangutan head and Paul F. Hemler arranged the CT scan. Jonathan J. Chew and David Ofelt helped 
keep our computer resources in operation.  References [1] T. Beier and S. Neely. Paci.c Data Images. 
Personal communication. [2] T. Beier and S. Neely. Feature-based image metamorphosis. In Computer Graph­ics, 
vol 26(2), pp 35 42, New York, NY, July 1992. Proceedings of SIGGRAPH 92. [3] B. P. Bergeron. Morphing 
as a means of generating variation in visual medical teaching materials. Computers in Biology and Medicine,24(1):11 
18,Jan. 1994. [4] B.Cabral,N.Cam,andJ.Foran.Acceleratedvolumerenderingandtomographic reconstruction using 
texture mapping hardware. In A. Kaufman and W. Krueger, editors, Proceedings of the 1994 Symposium on 
Volume Visualization, pp 91 98, New York, NY, Oct. 1994. ACM SIGGRAPH and IEEE Computer Society. [5] 
M. Chen, M. W. Jones, and P. Townsend. Methods for volume metamorphosis. To appear in Image Processing 
for Broadcast and Video Production,Y. Paker and S. Wilbur editors, Springer-Verlag, London, 1995.  
[6] M. Covell and M. Withgott. Spanning the gap between motion estimation and morphing. In Proceedings 
of IEEE International Conference on Acoustics, Speech and Signal Processing, vol 5, pp 213 216, New York, 
NY, 1994. IEEE. [7] T. A. Galyean and J. F. Hughes. Sculpting: An interactive volumetric modeling technique. 
In Computer Graphics, vol 25(4), pp 267 274, New York, NY, July 1991. Proceedings of SIGGRAPH 91. [8] 
T. He, S. Wang, and A. Kaufman. Wavelet-based volume morphing. In D. Berg­eron and A. Kaufman, editors, 
Proceedings of Visualization 94, pp 85 91, Los Alamitos, CA, Oct. 1994. IEEE Computer Society and ACM 
SIGGRAPH. [9] J. F. Hughes. Scheduled Fourier volume morphing. In Computer Graphics, vol 26(2), pp 43 
46, New York, NY, July 1992. Proceedings of SIGGRAPH 92. [10] A. Kaufman, D. Cohen, and R. Yagel. Volume 
graphics. Computer, 26(7):51 64, July 1993. [11] A. Kaul and J. Rossignac. Solid-interpolating deformations: 
Construction and animation of PIPs. In F. H. Post and W. Barth, editors, Eurographics 91,pp 493 505, 
Amsterdam, The Netherlands, Sept. 1991. Eurographics Association, North-Holland. [12] J. R. Kent, W. 
E. Carlson, and R. E. Parent. Shape transformation for polyhedral objects. In Computer Graphics, vol 
26(2), pp 47 54, New York, NY, July 1992. Proceedings of SIGGRAPH 92. [13] P. Litwinowicz. Ef.cient techniques 
for interactive texture placement. In Com­puter Graphics Proceedings, Annual Conference Series, pp 119 
122, New York, NY, July 1994. Conference Proceedings of SIGGRAPH 94. [14] W. E. Lorensen and H. E. Cline. 
Marching cubes: A high resolution 3-D sur­face construction algorithm. In Computer Graphics, vol 21(4), 
pp 163 169,New York, NY, July 1987. Proceedings of SIGGRAPH 87. [15] K. Perlin and E. M. Hoffert. Hypertexture. 
In Computer Graphics, vol 23(3), pp 253 262, New York, NY, July 1989. Proceedings of SIGGRAPH 89. [16] 
T. W. Sedeberg, P. Gao, G. Wang, and H. Mu. 2-D shape blending: An intrinsic solution to the vertex path 
problem. In Computer Graphics Proceedings, Annual Conference Series, pp 15 18, New York, NY, Aug. 1993. 
Conference Proceed­ings of SIGGRAPH 93. [17] T.W.SederbergandS.R.Parry.Free-formdeformationsofsolidgeometricmod­els. 
In Computer Graphics, vol 20(4), pp 151 160, New York, NY, Aug. 1986. Proceedings of SIGGRAPH 86. [18] 
P. A. van den Elsen, E.-J. D. Pol, and M. A. Viergever. Medical image match­ing a review with classi.cation. 
IEEE Engineering in Medicine and Biology Magazine, 12(1):26 39,Mar. 1993. [19] S. W. Wang and A. Kaufman. 
Volume sculpting. In Proceedings of 1995 Sympo­sium on Interactive 3D Graphics, pp 151 156, 214, New 
York, NY, Apr. 1995. ACM SIGGRAPH. [20] S.W.WangandA.E.Kaufman.Volumesampledvoxelizationofgeometricprim­itives. 
In G. M. Nielson and D. Bergeron, editors, Proceedings of Visualization 93, pp 78 84, Los Alamitos, CA, 
Oct. 1993. IEEE Computer Society and ACM SIGGRAPH. [21] G. Wolberg. Digital Image Warping. IEEE Computer 
Society P., Los Alamitos, CA, 1990.   (a) Original CT human head. (b) Human head warped to midpoint 
of morph. Figure 5: Human to orangutan morph.    (a) Dart volume from scan-converted polygon mesh. 
(b) X-29 volume from scan-converted polygon mesh. (c) Volume morph halfway between dart and X-29. (d) 
User interface showing elements used to establish correspondences between models. Points (not shown), 
segments, rectangles, and boxes are respectively drawn as pink spheres, green cylinders, narrow blue 
slabs, and yellow boxes. The x, y, and z axes of each element are shown only when the user clicks on 
an element in order to change its attributes; otherwise, they remain invisible to prevent cluttering 
the work area (see section 3.2). Figure 6: Dart to X-29 morph. (a) Lion volume from scan-converted 
polygon mesh. (c) Volume morph halfway between lion and leopard-horse. Figure 7: Lion to leopard-horse 
morph.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218503</article_id>
		<sort_key>457</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>57</seq_no>
		<title><![CDATA[Time-dependent three-dimensional intravascular ultrasound]]></title>
		<page_from>457</page_from>
		<page_to>464</page_to>
		<doi_number>10.1145/218380.218503</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218503</url>
		<categories>
			<primary_category>
				<cat_node>I.4.9</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Motion</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Splines</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003648.10003649.10003657.10003659</concept_id>
				<concept_desc>CCS->Mathematics of computing->Probability and statistics->Probabilistic representations->Nonparametric representations->Spline models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010238</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion capture</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010380</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Motion processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P137139</person_id>
				<author_profile_id><![CDATA[81100361368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lengyel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University, One Microsoft Way, Redmond WA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University, 580 ETC, Ithaca, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090540</person_id>
				<author_profile_id><![CDATA[81332521613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Popp]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University, School of Medicine, Stanford, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HYCHE, M., EZQUERRA, N., AND MULLICK, R. Spatiotemporal detection of arterial structure using active contours. Proceedings of Visualization in Biomedical Computing 1808 (1992), 52-62.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ISNER, J. M., ROSENFIELD, K., LOSORDO, D. W., AND KRISH- NASWAMY, C. Clinical experience with intravascular ultrasound as an adjunct to percutaneous revascularization. In Tobis and Yock {13}, part 16, pp. 186-197.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[KASS, M., WITKIN, A., AND TERZOPOULOS, D. Snakes: Active contour models. International Journal of Computer Vision (1988), 321-331.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[KRISHNASWAMY, C., D'ADAMO, A. J., AND SEHGAL, C. M. Threedimensional reconstruction of intravascular ultrasound images. In Tobis and Yock {13}, part 13, pp. 141-147.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>704603</ref_obj_id>
				<ref_obj_pid>646769</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[LENGYEL, J., GREENBERG, D. P., YEUNG, A., ALDERMAN, E., AND POPP, R. Three-dimensional reconstruction and volume rendering of intravascular ultrasound slices imaged on a curved arterial path. In Proceedings of CVRMed'95 (Apr. 1995).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801283</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[MACKAY, S. A., SAYRE, R. E., AND POTEL, M. J. 3d galatea: Entry of three-dimensional moving points from multiple perspective views. Proceedings of SIGGRAPH'82 (Boston, Massachusetts, July 26-30, 1982) 16, 3 (July 1982), 213-222.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ROELANDT, J. R., DI MARIO, C., PANDIAN, N. G., WENGUANG, L., KEANE, D., SLAGER, C. J., DE FEYTER, P. J., AND SERRUYS, P. W. Three-dimensional reconstruction of intracoronary ultrasound images. Circulation 90 (1994), 1044-1055.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[ROSENFIELD, K., LOSORDO, D. W., RAMASWAMY, K., PASTORE, J. O., LANGEVIN, E., RAZVI, S., KOSOWSKY, B. D., AND ISNER, J. M. Three-dimensional reconstruction of human coronary and peripheral arteries from images recorded during two-dimensional intravascular ultrasound examination. Circulation 84 (1991), 1938-1956.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[ST. GOAR, F., PINTO, F. J., ALDERMAN, E. L., FITZGERALD, P. J., STADIUS, M. L., AND POPP, R. L. Intravascular ultrasound imaging of angiographically normal coronary arteries: An in vivo comparison with quantitative angiography. Journal of the American College of Cardiology 18 (1991), 952-958.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[ST. GOAR, F., PINTO, F. J., ALDERMAN, E. L., VALANTINE, H. A., SCHROEDER, J. S., GAO, S.-Z., STINSON, E. B., AND POPP, R. L. Intracoronary ultrasound in cardiac transplant recipientsin vivo evidence of "angiographically silent" intimal thickening. Circulation 85 (1992), 979-987.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., WITKIN, A., AND KASS, M. Symmetry-seeking models and 3d object reconstruction. International Journal of Computer Vision 1 (1987), 211-221.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>48878</ref_obj_id>
				<ref_obj_pid>48873</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[TERZOPOULOS, D., WITKIN, A., AND KASS, M. Contraints on deformable models : Recovering 3d shape and nonrigid motion. Artificial Intelligence 35 (1988), 91-123.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Toms, J. M., AND YOCK, P. G., Eds. Intravascular Ultrasound Imaging. Churchill Livingstone Inc., New York, 1992.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[YOCK, P., JOHNSON, E., AND DAVID, D. Intravascular ultrasound: Development and clinical potential. American Journal of Cardiac Imaging 2 (1988), 185-193.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[YOCK, P., LINKER, D., AND ANGELSON, A. Two-dimensional intravascular ultrasound: Technical development and initial clinical experience. Journal of the American Society of Echocardiography 2 (1989), 296-304.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Time-Dependent Three-Dimensional Intravascular Ultrasound Jed Lengyel* Donald P. Greenbergy Richard 
Poppz Cornell University Cornell University Stanford University Abstract Intravascular ultrasonography 
and x-ray angiography pro­ vide two complimentary techniques for imaging the mov­ ing coronary arteries. 
We present a technique that com­ bines the strengths of both, by recovering the moving three­ dimensional 
arterial tree from a stereo pair of angiograms through the use of compound-energy snakes , placing the 
intravascular ultrasound slices at their proper positions in time and space, and dynamically displaying 
the combined data. Past techniques have assumed that the ultrasound slices are parallel and that the 
vessel being imaged is straight. For the .rst time, by applying simple but e.ective tech­ niques from 
computer graphics, the moving geometry of the artery from the angiogram and the time-dependent images 
of the interior of the vessel wall from the intravascular ul­ trasound can be viewed simultaneously, 
showing the proper geometric and temporal relations of the slice data and the angiogram projections. 
By using texture-mapped rectangles the combined ultrasound slice/angiogram display technique is well 
suited to run in real time on current graphics work­ stations. CR Categories and Subject Descriptors: 
I.3.0 [Computer Graphics]: General; I.3.8 [Computer Graphics]: Applications. J.3 [Life and Medical Sciences]. 
1 Introduction Although the current trend in medical imaging is towards non-invasive and low-radiation 
techniques such as MRI (magnetic resonance imaging), for cardiac patients there is still a need for high-resolution, 
high-detail information particularly for planning coronary treatments such as by­ pass surgery, balloon 
angioplasty, and atherectomy. No cur­ rent non-invasive imaging technique can provide the accu­ rate 
high-resolution data required for these decisions at the present time. Intravascular ultrasound imaging 
is a relatively new technique for imaging the interior structure of arteries and provides one method 
for obtaining such high-detail images.[14][15] Unlike traditional cardiac ultrasound that uses an exterior 
probe and is limited to imaging between the patient s ribs or a transesophageal probe, intravascular 
*Current address: One Microsoft Way, Redmond WA 98052-6399, jedl@microsoft.com. y580 ETC, Ithaca, NY 
14850. dpg@graphics.cornell.edu. zSchool of Medicine, Stanford, CA 94305. Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 ultrasound uses a miniature ultrasound transducer 
mounted on the tip of a catheter. (Figure 1(a)) To image the coronary arteries, both intravascular ul­trasound 
and standard contrast angiograms use the same catheter placement technique. The catheter is threaded 
in­side the patient s arterial system through an artery in the thigh, and then maneuvered through the 
descending aorta, around the aortic arch, and into the coronary arteries. For contrast angiograms, radio-opaque 
dye is injected at the catheter s tip so that the blood .ow in the lumen of the vessel appears in .uoroscopic 
x-ray images. For intravascu­lar ultrasound, the transducer at the tip of the catheter is rotated by 
a drive shaft that runs the length of the catheter. The rotating transducer can then image cross-sections 
by emitting pulses of ultrasound (currently in the 20-50 MHz range) and then receiving time-delayed echos. 
(Figure 1(b)) The main advantage of intravascular ultrasound over the standard contrast angiogram is 
that intravascular ultrasound can make images of the interior structure of the artery wall. Although 
the resolution of ultrasound devices is generally lower than other imaging methods, because of the small 
.eld of view and lack of any obstructing tissue, and the reduction of noise with intravascular positioning, 
.ne detail structure can be obtained. The standard angiogram shows only a two-dimensional projection 
of the lumen of the vessel. (Fig­ure 1(c) and Figure 1(d)) Recently it has been shown that intravascular 
ultrasound can both reveal disease that does not appear in the standard constrast angiogram and accu­rately 
measure the vessel lumen.[10][9] (Figure 1(e) and Fig­ure 1(f)) Previous work on three-dimensional reconstruction 
of two­dimensional ultrasound slices has been limited to static ge­ometries and has assumed that the 
vessel being imaged was straight. Visualizations were composed of stacked slices to form three dimensional 
cylindrical images. [8] [4] [2] The images produced in this way show the three-dimensional re­lations 
of vessel structures, but distort the geometry.[7] Fur­thermore, slices collected from di.erent times 
in the heart cycle are shown together. Previous research by the authors has extended these procedures 
to handle a static but curved arterial tree.[5] This paper describes a prototypical system which uses 
ad­vanced computer graphics techniques to: .Accurately reconstruct the three-dimensional geom­etry of 
the coronary arterial tree from two-dimensional angiograms through the use of compound-energy snakes. 
.Precisely position time-dependent, two-dimensional ul­trasound slices on the dynamic arterial tree. 
.Dynamically display the combined data for medical di­agnosis. The interactive system can run on an 
advanced graphics workstation and provide immediate feedback to the cardiac surgeon/cardiologist. (a) 
Catheter for Intravascular Imaging (c) Standard Angiogram Left Anterior Oblique (e) Typical Intravascular 
Ultra­sound Cross-Section of a Normal Adult Coronary Atery   (b) Detail of Transducer  (d) Standard 
Angiogram Right Anterior Oblique (f) Cross-Section of a Diseased Coronary Artery The eccen­tric plaque 
(soft gray echoes) sur­rounds the blood-.lled vessel lumen (black).   Figure 1: Data Input Devices 
and Examples Figure 2: Hospital with patient being imaged by a biplane angiogram. 2 Geometry of the 
Arterial Centerline Determining the centerline of a dynamic three-dimensional arterial tree is non-trivial. 
Standard angiograms have poor contrast, poor signal to noise ratios, and, most importantly, changing 
points of discontinuity caused by the overlapping projections of the branches of the arterial tree. Standard 
edge-tracking methods easily get confused, requiring fre­quent user intervention. Our initial technique 
for reconstructing a segment of the arterial tree was an interactive one in which the stereo an­giograms 
were presented to the user who then positioned points along the length of the artery segment in both 
im­ages. Standard stereo inverse techniques were then used to calculate the three-dimensional points.[6] 
This technique was far too labor intensive to be useful, especially when consider­ing the goal of capturing 
the moving geometry of the arterial tree. The goal of automatic tracking motivated the use of the technique 
described below. Since arteries are made of elastic material, the model we use to .t to the arteries 
should capture this behavior. One such model used in computer vision is energy-based splines, or snakes 
. Snakes have been used to track edges, to fol­low moving features, and to perform stereo matching all 
of which are needed for tracking a moving artery. [3][1] Our work di.ers from the previous work by the 
use of compound snakes with o.set energy functions. The use of a centerline and symmetric o.sets is similar 
to the more general three­dimensional symmetry-seeking models found in [11].  Attracted to edges Attracted 
to minima SR Figure 3: Snake Physically-based spline attracted to im­age features speci.ed in energy 
functions. 2.1 Energy-Minimizing Splines Snakes are modeled with an internal elastic energy, and external 
energy functions, Eexternal.The Einternal, internal energy gives the snake its elastic character. The 
external energy functions are important for getting a de­sired behavior. For example, if one uses an 
energy such as )=I(x,y)where Iis the image intensity, by Eexternal(x,y minimizing the energy function, 
the snake will seek dark ar­eas in the image. To make a good artery tracker, we use a combination snake 
with a center and two o.set sides and the following energies (Figure 3): . Minima-seeking, Eminima =I(x,y) 
.j2 Edge-tracking, Eedge =-jrI(x,y) Let sC(u)be the centerline of the snake. We will de.ne sL(u)and 
sR(u)to be perpendicularly o.set from sC(u)at each u by rwidth.Let sL(u)=sC(u)+r widthn(u) sR(u)=sC(u)­ 
rwidthn(u) where n(u)is the normal to the curve of sC(u).Using these curves, we can de.ne the following 
energy to minimize: = Eexternal(u)kminimaEminima(sC(u))+ kedge(Eedge(sL(u))+Eedge(sR(u))) The center 
of the combination snake seeks minima and is intended to .nd the center of an artery, where the image 
is most opaque. The o.set parts of the combination snake seek edges. The combination of the two energies 
is more e.ective than the individual energies alone. There are several free parameters to set. kminima, 
the strength of the minima-seeking energy  kedge, the strength of the edge-seeking energy  rwidth, 
radius to seek   -1 M M left right 3D Geometry Left Projection Right Projection Figure 4: Stereo 
Matching For the work described here, the constants were set with initial rough estimates and then tuned 
interactively for good performance. Ideally, these parameters should be assigned from measurements of 
the physical properties of the actual catheter and actual vessels. This might eliminate the need for 
a tuning step. Note that for this particular formulation, ran width is input parameter specifying the 
desired size of the matching artery. To make the combination snake more general, the radius could be 
made a function of u. 2.2 Stereo Matching We require at least two views to reconstruct the centerlines 
of the arteries (Figures 3 and 4). We use two of the compound snakes as described above, with an additional 
energy that tries to minimize the distance between the two snakes in three-dimensions:[3] .1 .12 Estereo(u)=jM(S0left(u))-Mt(S0(u))j 
leftrighright where M .1 is the inverse of the projection transformation and the subscripts leftand 
rightrefer to the two projected views. M .1 is usually taken as a 4x4perspective ma­trix, but the snake 
energy based method can also include nonlinear warping corrections for distorted camera images. Although 
we have not yet incorporated this, the warping function can be calculated by imaging a regular grid and 
calculating the inverse warping function. 2.3 Snakes in 3D Since we are reconstructing a three-dimensional 
geometric structure, it makes sense to use a three-dimensional model. The technique we use is similar 
to the two-dimensional en­ergy functionals described previously, but we use one snake in three dimensions 
and project to two dimensions to express the energy functionals (Figure 2.3) This is similar to other 
work by the original snake authors[12]. We use the two two-dimensional snakes as above to get an initial 
position for the three-dimensional snake. Once we have a single snake, we no longer have the parametrization 
problems that occur when using two di.erent snakes. As described in the original paper by Kass [3], the 
internal energy is given by E=( (s)jvs(s)j2+ (s)jvss(s)j2) 2 internal which works in two and three dimensions, 
but with a slightly di.erent interpretation of the terms (there is no torsion in two dimensions). Let 
sC(u)be the centerline of the snake in three dimen­sions. We will de.ne s 0(u)and s 1(u)to be sC(u)trans- 
CCleftright formed by the projections Mand M, respectively. 0 left sC(u)=MsC(u) 1 right sC(u)=MsC(u) 
 Then as above we de.ne sLiand sRito be perpendicularly o.set from s i(u)at each uby r Cwidth.Let ii 
i s(u)=s(u)+r(u) LCwidthn iii sR(u)=sC(u)-r(u) widthn where n i(u)is the normal to the curve of sCi(u)in 
the pro­jected plane. Since sCiis a projection, it is possible that non-zero segments of the original 
curve sCwill project onto a single point. For such cases, n iwill have degeneracies. To handle these 
degenerate cases and to prevent sudden sign .ips, the discretized version of n iis constrained to be 
pos­itive and on just one side of sCi . This is accomplished by sequentially testing the dot product 
of the candidate normal with the previous discretized point s normal. If the dot prod­uct is close to 
or equal to zero, the previous point s normal is used. If less than zero, then the new normal is .ipped. 
Once we have the two o.set curves and centerlines in both projections, we use the same formulation for 
the energies as in the previous section the centerline seek minima and the o.set curves seek edges. The 
energy equation is solved by taking a variational derivative which gives rise to a set of Euler equations. 
The solution technique as described in [3] requires the partial derivatives of the external energy function. 
Since the snake is now 3D, we need derivatives in the x, y,and zcoordinates of the artery. But the energy 
terms are described in the coordinates of the plane of the projections. Thus, we must multiply the vector 
of partial derivatives by the inverse jacobian matrix of the transformation. If we consider the two projection 
transformations as 4x4matrices, then the inverse jacobian matrices are just the inverses of the matrices. 
If we include the de-warping grid, then we also need a local inverse jaco­bian of the de-warping function. 
Once we have the partial derivatives fx, fy,and fzin the artery coordinates, we can use the same solution 
technique as for the two-dimensional snakes, using the LU-decompos­ition inverse of a pentadiagonal banded 
matrix that handles the implicit step for the internal energy and an explicit Euler step for the external 
energy. Figure 5 shows the two-dimensional snakes in each of the angiograms tracking the same section 
of artery and the re­constructed result. The three-dimensional display of the an­giograms along with 
the computed three-dimensional artery centerline clearly shows the geometric relation of the two projections 
with the result. Figure 6: Motion Tracking Sequence of frames showing inter-frame tracking of arteries 
by composite snake. 3 Motion Tracking The computation for the snakes involves a simulated elastic system 
that is evolved forward in time. Since time is built into the calculation, it is straightforward to let 
the images change with time. The snakes will track the local minima (Figure 6) as long as the jumps in 
the motion between succes­sive frames are not too large.[3] We worked with angiogram data sets from heart-transplant 
patients whose heart rates were elevated up to 120 beats per minute. This made the jumps between frames 
more pronounced, and our snakes tended to jump out of the proper local minima. Since arter­ies have mass 
and a smoothly changing velocity, we added a mass term to the snake which helped to predict where the 
artery would be.[3][1] This mass term is calculated by keep­ing track of the previous two time steps 
for the snake and estimating acceleration. The mass term only became e.ec­tive when the weighting term 
and the number of relaxation steps were tuned. Since the snakes seek local minima in the energy func­tions, 
it is important to start them with the proper initial conditions, or they will .nd the wrong local minima. 
For the .rst frame, we set the initial position of the snake in both of the stereo images by interactively 
sketching a series of line segments over the desired artery segment. The snake was then activated and 
allowed to relax into the local minimum in each frame. Then the stereo energy weight was ramped from 
0 to 1, allowing the two snakes to .nd a good stereo match. The above steps were all done on a single 
stereo-pair angiogram. The rest of the motion was tracked automat­ically, using the previous frames to 
estimate the starting location of subsequent frames. Certain highly curved sec­tions of the artery needed 
interactive adjustment to make the snake track properly. 4 Positioning of Time-Dependent 2D Ul­trasound 
Slices Landmark sites, located at branching points in the ultra­sound sequence, are used to orient the 
slices (Figure 7). Each landmark shows a correspondence between a sidebranching vessel in the ultrasound 
data and in the angiographic data. These sidebranches are used to orient the slice data around the centerline 
of the artery. The current arclength of the catheter, as measured relative to a landmark site (or by 
a linear encoder on the shaft), is used to get the slice s distal location along the arterial tree. The 
distal location maps directly to a 3D location, since we have already calculated the 3D geometry of the 
arterial tree and catheter path. Figure 7: Landmark Sites This calculation is complicated by the time-dependence 
of both the arterial tree and the catheter arclength as it is pulled back. Assume that the arterial tree 
motion is cyclic (regular heartbeat) and that either the catheter arclength versus time is linear or 
measurable with a linear encoder. Also assume that the distortion of the vessel due to the catheter is 
negligible. Then for a given time, we know both the phase of the arterial tree (point in the cardiac 
cycle) and the distal location and thus the geometric position of the ul­trasound slice relative to the 
current arterial tree geometry. Future testing with physical phantoms will be needed to get error bounds 
on these assumptions. Figure 8 plots the spacetime path of one coordinate (x, y, or z) of the moving 
arterial tree and shows the path of the transducer moving in the surface that results. The di.culty is 
that sequential slices represent not only di.erent positions along the artery centerline, but are taken 
at di.erent phases of the cardiac cycle. The slices active at a given cycle phase are the slices from 
transducer distal locations whose times modulo the cycle period Tcorrespond to the same phase. Thus slices 
t0,t0 +T,t0 +2T,...are all active slices of the same cycle. Thus the problem is reduced to one of retrieving 
the appropriate ultrasound slice for each speci.c space-time position. To dynamically display the cross-sectional 
images, the complete set of ultrasound slices must be assembled for each phase of the cardiac cycle. 
x, y,or z  Figure 8: Path of transducer relative to heart cycle. The slices are reorganized so that 
for each frame the only slices shown are those that match the artery centerline s phase of the heart 
cycle. 5Examples Figure 9(a) shows a static image of the ultrasound slices placed along the artery centerline, 
with a sagittal split ap­plied to each slice to reveal the internal structure of the arterial wall. Figure 
9(b) shows a coronal split applied to each slice. The dark semi-circle in the very center of each ultrasound 
frame is the ring-down region in which no data is received. The larger dark circular region is the lumen 
of the vessel through which the blood .ows. The white region beyond the lumen in each of the slices shows 
the interior structure of the vessel wall. Since the x-ray contrast dye .ows in the blood, the lumen 
is all that appears in the two projected angiograms. Note how the positioning of the slices with the 
reference of the two projected angiograms clearly shows the relation of the slices to the curved geometry 
of the artery. 6 Volume Rendering Originally we attempted to use a parametric interpolation volume imaging 
technique to portray the three-dimensional information.[5] Unfortunately, with the pullback rates used, 
the sample spacing between slices was an order of magnitude larger than the in-slice resolution, yielding 
interpolated vol­umes which were too smooth to be useful for diagnosis (Fig­ure 10). We are currently 
try to obtain images at slower pullback rates providing more closely packed slices. Another drawback 
to volume rendering is the long time typically needed to render a frame (approximately one hour with 
the current testbed.) 7 Conclusion By recovering the moving three-dimensional arterial tree and placing 
the slices at their proper positions in time and space, we are able to combine the strengths of both 
intravascular ultrasound and x-ray angiograms. For the .rst time, the moving geometry of the artery from 
the angiogram and the time-dependent images of the interior of the vessel wall from the intravascular 
ultrasound can be viewed simultaneously, with proper geometric relations. There are several limitations 
to the technique described here which should be the subject for future work. First, the use of the centerline 
of the artery to place the slices is an approximation, since the catheter is, in general, o.-center and 
tilted with respect to the artery. This may be corrected by recovering the path of the catheter and transducer 
rela­tive to the artery centerline we are pursuing a technique that uses the o.set and elliptical shape 
of the lumen in the ultrasound slices. Second, the correlation of ultrasound slice and angiogram timing 
is inexact, but this is easily corrected by using digital data marked with the current heart cycle from 
an electro­cardiogram. Third, there is a substantial tuning step required to .nd the proper parameters 
for the artery-seeking snakes. One possible approach to avoid this tuning step would be to use a three-dimensional 
snake with the internal elastic energy constants corresponding to the measured elastic properties of 
actual arteries and catheters. The image-seeking energy weights could then be set relative to the resulting 
physically­based snake. Other future work includes using a slow pullback to obtain a dataset with more 
closely spaced slice data, and applying the techniques described here to experimentally reconstruct, 
render, and compare a physical phantom with known geom­etry. Ultimately, with greatly increased machine 
speed and closely spaced slice data, volume rendering the dynamic data may prove to be more e.ective 
than showing just the slice data, particularly for views that are nearly parallel to a slice. By using 
texture-mapped rectangles the combined ultra­sound slice/angiogram display technique is well suited to 
run in real time on current graphics workstations. Accu­rate knowledge of the dynamic interior anatomy 
of the dis­eased vessel will permit improved diagnostic techniques for the physician attempting angioplasty 
or atherectomy. We hope that this kind of display will move into the interven­tional catheter laboratory. 
Acknowledgements Thanks to Jon Blocksom without whom the paper would not have been completed. Thanks 
to Peter Shirley and Dan Kartch for help with the document preparation, to Alan Yeung, Edwin Alderman, 
and Peter Fitzgerald for providing datasets and insight. Thanks to Allison Hart. This work was supported 
by the NSF/ARPA Science and Technology Center for Computer Graphics and Scienti.c Visu­alization (ASC-8920219) 
and performed on workstations generously provided by the Hewlett-Packard Corporation. References [1] 
Hyche, M., Ezquerra, N., and Mullick, R. Spatiotemporal detection of arterial structure using active 
contours. Proceedings of Visualization in Biomedical Computing 1808 (1992), 52 62. [2] Isner, J. M., 
Rosenfield, K., Losordo, D. W., and Krish­naswamy, C. Clinical experience with intravascular ultrasound 
as an adjunct to percutaneous revascularization. In Tobis and Yock [13], part 16, pp. 186 197. [3] Kass, 
M., Witkin, A., and Terzopoulos, D. Snakes: Active contour models. International Journal of Computer 
Vision (1988), 321 331. [4] Krishnaswamy, C., D Adamo, A. J., and Sehgal, C. M. Three­dimensional reconstruction 
of intravascular ultrasound images. In Tobis and Yock [13], part 13, pp. 141 147. [5] Lengyel, J., Greenberg, 
D. P., Yeung, A., Alderman, E., and Popp, R. Three-dimensional reconstruction and volume render­ing of 
intravascular ultrasound slices imaged on a curved arterial path. In Proceedings of CVRMed 95 (Apr. 1995). 
[6] MacKay, S. A., Sayre, R. E., and Potel, M. J. 3d galatea: En­try of three-dimensional moving points 
from multiple perspective views. Proceedings of SIGGRAPH 82 (Boston, Massachusetts, July 26 30, 1982) 
16, 3 (July 1982), 213 222. [7] Roelandt, J. R., di Mario, C., Pandian, N. G., Wenguang, L., Keane, D., 
Slager, C. J., de Feyter, P. J., and Serruys, P. W. Three-dimensional reconstruction of intracoronary 
ultra­sound images. Circulation 90 (1994), 1044 1055. [8] Rosenfield, K., Losordo, D. W., Ramaswamy, 
K., Pastore, J. O., Langevin, E., Razvi, S., Kosowsky, B. D., and Isner, J. M. Three-dimensional reconstruction 
of human coronary and peripheral arteries from images recorded during two-dimensional intravascular ultrasound 
examination. Circulation 84 (1991), 1938 1956. [9] St. Goar, F., Pinto, F. J., Alderman, E. L., Fitzgerald, 
P. J., Stadius, M. L., and Popp, R. L. Intravascular ultrasound imag­ing of angiographically normal coronary 
arteries: An in vivo com­parison with quantitative angiography. Journal of the American College of Cardiology 
18 (1991), 952 958. [10] St. Goar, F., Pinto, F. J., Alderman, E. L., Valantine, H. A., Schroeder, J. 
S., Gao, S.-Z., Stinson, E. B., and Popp, R. L. Intracoronary ultrasound in cardiac transplant recipients 
in vivo evidence of angiographically silent intimal thickening. Circulation 85 (1992), 979 987. [11] 
Terzopoulos, D., Witkin, A., and Kass, M. Symmetry-seeking models and 3d object reconstruction. International 
Journal of Computer Vision 1 (1987), 211 221. [12] Terzopoulos, D., Witkin, A., and Kass, M. Contraints 
on deformable models : Recovering 3d shape and nonrigid motion. Arti.cial Intelligence 35 (1988), 91 
123. [13] Tobis, J. M., and Yock, P. G.,Eds. Intravascular Ultrasound Imaging. Churchill Livingstone 
Inc., New York, 1992. [14] Yock, P., Johnson, E., and David, D. Intravascular ultrasound: Development 
and clinical potential. American Journal of Car­diac Imaging 2 (1988), 185 193. [15] Yock,P., Linker,D., 
and Angelson,A. Two-dimensional in­travascular ultrasound: Technical development and initial clini­cal 
experience. Journal of the American Society of Echocardio­graphy 2 (1989), 296 304. (a) Coronal Split 
(b) Sagittal Split Figure 9: Slices Positioned Along the Transducer Path (a) Coronal Split (b) Sagittal 
Split Figure 10: Volume-Rendered Images  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218504</article_id>
		<sort_key>465</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>58</seq_no>
		<title><![CDATA[Extracting surfaces from fuzzy 3D-ultrasound data]]></title>
		<page_from>465</page_from>
		<page_to>474</page_to>
		<doi_number>10.1145/218380.218504</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218504</url>
		<keywords>
			<kw><![CDATA[3D ultrasound]]></kw>
			<kw><![CDATA[morphology]]></kw>
			<kw><![CDATA[multiresolution analysis]]></kw>
			<kw><![CDATA[volume rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.2.3</cat_node>
				<descriptor>Uncertainty, "fuzzy," and probabilistic reasoning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Filtering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.3</cat_node>
				<descriptor>Smoothing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.6</cat_node>
				<descriptor>Edge and feature detection</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010187.10010191</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Knowledge representation and reasoning->Vagueness and fuzzy logic</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010246</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Interest point and salient region detections</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010187.10010190</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Knowledge representation and reasoning->Probabilistic reasoning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Measurement</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P96258</person_id>
				<author_profile_id><![CDATA[81100065239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Georgios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Computer Graphics, Wilhelminenstr. 7, 64283 Darmstadt, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P267493</person_id>
				<author_profile_id><![CDATA[81100009204]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Walter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Computer Graphics, Wilhelminenstr. 7, 64283 Darmstadt, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Antoniu, E: Trends in Edge Detection Techniques, EURO- GRAPHICS'91 State of the Art Report, pp. 111-162, Vol. EG 91 STAR, ISSN 1017-4656, 1991]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Burt, P.: The Pyramid as a Structure for Efficient Computation, A. Rosenfeld (Ed.), Multi-Resolution Image Processing and Analysis, Springer Veflag, New York, 1984]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359846</ref_obj_id>
				<ref_obj_pid>359842</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Kedem, Z., Uselton, S.: Optimal Surface reconstruction from Planar Contours, Comm. of the ACM, Vol. 20, No. 10, pp. 693-702, 1977]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>74352</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Gallagher, R., Nagtegaal, J.: An Efficient 3D Visualization Technique for Finite Elements, Computer Graphics, Vol. 23, No. 3, pp. 185-194, July 1989]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42389</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[C.R. Giardina, E.R. Dougherty: Morphological Methods in Image and Signal Processing, Prentice Hall, Englewood Cliffs, New Jersey, 1988]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>22881</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Gonzalez, R. C., Wintz, P.: Digital Image Processing, Second Edition, Addison Wesley Publishing Company, 1987]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Herman, G., Liu, H.: 3D Display of Human Organs from Computer Tomograms, Computer Graphics and Image Processing, Vol. 9, No. 1, pp. 1-21, January 1991]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>44652</ref_obj_id>
				<ref_obj_pid>44650</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Levoy, M.: Display of Surfaces from Volume Data, IEEE Computer Graphics &amp; Applications, Vol. 8, No. 3, pp. 29-37, May 1988]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>37422</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Lorensen, W., Cline, H.: Marching Cubes: A High Resolution 3D Surface Construction Algorithm, ACM Computer Graphics, SIGGRAPH-87, Vol. 21, No. 4, pp. 163-169, July 1987]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>132007</ref_obj_id>
				<ref_obj_pid>132005</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[S. Mallat and S. Zhong, Characterization of Signals from Multiscale Edges, IEEE Transactions on Pattern Analysis and Machine Inteligence, Vol. 14, pp. 710-732, 1992]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Millner, R.: Ultraschalltechnik, Grundlagen und Anwendungen, Physik Veflag, Weinheim, ISBN 3-87664-106-3, 1987]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>617875</ref_obj_id>
				<ref_obj_pid>616030</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Nelson, T., Elvis, T.: Visualization of3D Ultrasound Data IEEE Computer Graphics &amp; Applications, Vol. 13, No. 6, pp. 50-57, November 1993]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>378476</ref_obj_id>
				<ref_obj_pid>378456</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Sabella, P.: A Rendering Algorithm for Visualizing 3D Scalar Fields, ACM Computer Graphics, Vol. 22, No. 4, pp. 51-58, August 1988]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Sakas, G., Gerth, M.: Sampling andAnti-Aliasing Discrete 3D Volume Density Textures,EUR OGRAPHICS' 91 Award Paper, Computer and Graphics, Vol. 16, No. 1, pp. 121-134, Pergamon Press, 1992]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sakas, G.: Interactive Volume Rendering of Large Fields, 'The Visual Computer, Vol. 9, No. 8, pp. 425-438, August 1993]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>618287</ref_obj_id>
				<ref_obj_pid>616037</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Sakas, G., Schreyer, L., Grimm, M.: Pre-Processing, Segmenting and Volume Rendering 3D Ultrasonic Data, IEEE CG &amp; A, Special Issue, Vol. 15, No. 4, July 1995. Revised version of Case Study: Visualization of 3D Ultrasonic Data, "Outstanding Case Study Award", Proceedings Visualization'94, pp. 369-373]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Salesin, D., Stolfi, J.: The ZZ-Buffer" A Simple and Efficient Rendering Algorithm with Reliable Antialiazing, Proceedings 'PIXIM' 89 Conference', pp. 451-466, Hermes Editions, Paris, September 1989]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[B. G. Schunk : Edge detection with Gaussian Filters at Multiple Scales of Resolution, in: 'Advances in Image Analysis', Y. Mahdavieh and R. C. Gonzales (Eds.), McGraw Hill 1987]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>951156</ref_obj_id>
				<ref_obj_pid>951087</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Stata, A., Chen, D., Tector, C., Brandl, A., Chen, H., Ohbuchi, R., Bajura, M., Fuchs, A.: Case Study: Observing a Volume Rendered Fetus within a Pregnant Patient, Proceedings Visualization'94, pp. 364-368, 1994]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sakas, G., Walter, S., Hiltmann, W., Wischnik, A.: Foetal Visualization Using 3D Ultrasonic Data, Proceedings CAR'94, Berlin/Germany, 21-24 June 1995]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Sohn, C., Zoller, W. (Eds.): Proceedings II Symposium 3D Ultraschalldiagnostik, Munich, 25-26 June 1993, 'Imaging - Application &amp; Results', Vol. 61, No. 2, pp. 81-135, Karger Pub., June 1994]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Extracting Surfaces from Fuzzy 3D-Ultrasound Data Georgios Sakas, Stefan Walter Fraunhofer Institute 
for Computer Graphics1 Abstract Rendering 3D models from 3D-ultrasonic data is a complicated task due 
to the noisy, fuzzy nature of ultrasound imaging containing a lot of artifacts, speckle etc. In the method 
presented in this paper we .rst apply several .ltering techniques (low-pass, mathematical morphology, 
multi-resolution analysis) to separate the areas of low coherency containing mostly noise and speckle 
from those of useful information. Our novel BLTP .ltering can be applied at interactive times on-the-.y 
under user control &#38; feed-back. Goal of this pro­cessing is to create a region-of-interest (ROI) 
mask, whereas the data itself remains unaltered. Secondly, we examine several alterna­tives to the original 
Levoy contouring method. Finally we introduce an improved surface-extraction volume rendering procedure, 
app­lied on the original data within the ROI areas for visualizing high quality images within a few seconds 
on a normal workstation, or even on a PC, thus making the complete system suitable for routine clinical 
applications. CR Descriptors: General Terms: Algorithms. I.3.3 [Compu­ter Graphics]: Picture/image generation; 
I.3.8 [Computer Gra­phics]: Applications; I.4.3 [Image Processing]: Enhancement, Smoothing, Filtering; 
I.4.6 [Image Processing]: Segmentation, Edge and Feature Detection, Pixel Classi.cation; J.3 [Life and 
Medical Sciences]. Additional Keywords: 3D ultrasound, multi­resolution analysis, morphology, volume 
rendering  1Introduction 3D ultrasound is a very new and most interesting application in the area of 
tomographic medical imaging, able to become a fast, non-radiative, non-invasive, and inexpensive volumetric 
data ac­quisition technique with unique advantages for the localisation of vessels and tumours in soft 
tissue (spleen,kidneys, liver, breast etc.). In general, tomographic techniques (CT, MR, PET etc.) allow 
for a high anatomical clarity when inspecting the interior of the human body. In addition, they enable 
a 3D reconstruction and examination 1Fraunhofer Institute for Computer Graphics Wilhelminenstr. 7, 64283 
Darmstadt, Germany fax: +49/6151/155-199, email: fgsakas,walterg@igd.fhg.de Permission to make digital/hard 
copy of part or all of this work for personal or classroom use is granted without fee provided that copies 
are not made or distributed for profit or commercial advantage, the copyright notice, the title of the 
publication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy 
otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission 
and/or a fee. &#38;#169;1995 ACM-0-89791-701-4/95/008 $3.50 of regions of interest, offering obvious 
bene.ts (reviewing from any desired angle, isolation of crucial locations, visualization of internal 
structures, .y-by , accurate measurements of distances, angles, volumes etc.). Transducer Tissue Figure 
1: The principal function of ultrasound The physical principle of ultrasound is as following [11]: sound 
waves of high frequency (1 15 MHz) emanate from a row of sour­ces that are located on the surface of 
a transducer which is in direct contact with the skin. The sound waves penetrate the human tissue travelling 
with a speed of 1450 1580 m/s, depending upon the type of tissue. The sound waves are re.ected partially 
if they hit an interface between two different types of tissue (e.g. muscle and bone). The re.ected wavefronts 
are detected by sensors (micro­phones) located next to the sources on the transducer. The intensity of 
re.ected energy is proportional to the sound impedance diffe­rence of the two corresponding types of 
tissue and depends on the difference of the sound impendances Z1 and Z2: Z2 1 Z1 Ir Ie (1) Z2 1 Z1 
 An image of the interior structure can be reconstructed based upon the total travelling time, the (average) 
speed, and the energy inten­sity of the re.ected waves. The resulting 3D images essentially represent 
hidden internal surfaces . The principle is similar to radar with the difference being that it uses mechanical 
instead of electromagnetic waves. In contrast to the common 2D case where a single image slice is acquired, 
3D ultrasonic techniques cover a volume within the body with a series of subsequent image slices. The 
acquisition of these slices can be achieved by means of various scan and tracking techniques and will 
be not discussed further in this paper; please refer to [12], [19], and [21] for more details. 2 Previous 
Works &#38; Drawbacks One of the major reasons for the limited acceptance of 3D ultra­sound to date is 
the complete lack of an appropriate visualisation technique, able to display clear surfaces out of the 
acquired data. Our very .rst approach was to use well known techniques, used for MRI and CT data to extract 
surfaces. Such techniques include bi­narization [7], iso-surfacing [4], contour connecting [3], marching 
cubes [9], and volume rendering either as semi-transparent cloud [13], or as fuzzy gradient shading [8]. 
Manual contouring [19] is too slow and impractical for real-life applications. Unfortunately, ultrasound 
images posses several features causing all these techni­ques to fail totally. The most important of these 
features as reported in [16] are: 1. signi.cant amount of noise and speckle 2. much lower dynamic range 
as compared to CT or MR 3. high variations in the intensity of neighbouring voxels, even within homogeneous 
tissue areas 4. boundaries with varying grey level caused by the variation of surface curvature and 
orientation to the sound source 5. partially or completely shadowed surfaces from objects closer and 
within the direction of the sound source (e.g. a hand shadows the face) 6. the regions representing 
boundaries are not sharp but show a width of several pixels 7. poor alignment between subsequent images 
(parallel scan devi­ces only) 8. pixels representing varying geometric resolutions depending on the 
distance from the sound source (fan scanning devices only)  The general appearance of a volume rendered 
3D ultrasound da­taset is that of a solid block covered with noise snow (.g. 13 right). A closer analysis 
proved that noise and speckle contained in the image caused so many obstacles (blobs) around the objects, 
that rays usually fail to penetrate deep enough to reach the crucial internal surfaces. The low dynamic 
range makes a straight-forward discrimination between speckle and information (e.g. by means of thresholding) 
impossible. Even when a surface is reached, methods based on a single threshold value ([4], [7], [9] 
etc.) fail to detect a continuous surface due to the features 3, 4, &#38; 5 listed above. Lastly, the 
nature of gradient shading employed, e.g. in [8], is very sensitive to high frequency noise and speckle 
and therefore it is not suitable to generate a smooth surface because of reasons 3, 6, 7, &#38; 8 (see 
.g. 2 upper left). However, Levoy s continuous opacity classi.cator (eq. 2) has been found to give good 
estimations of the presence of a surface contour (aand rare scaling factors and Sa threshold value): 
8 1: g(u,v,w)S 1 jg(u,v,w)1Sj opacitya1 : jrg(u,v,w)j>0 r jrg(u,v,w)j 0: otherwise (2) Some time ago 
we started to search for ways of discriminating the tissue of interest from the lerge amount of apparently 
noisy signal. Initially we .ltered the original voxels with an approximation of a Gaussian kernel with 
discrete binomial coef.cients due to its separability, normality and symmetry [6]: 1 2 G(x,O) 2 e2.x.2(3) 
2 O In a discrete implementation the factors of this kernel are usually calculated by the binomial coef.cients: 
 nn! 0 k n kk! (nk)! f(x) 1 x 0  Filtering a function gwith a .lter Fis a convolution [6]: 1 0 g(x)g(x) 
F(x) g(x)F(xx1)dx1 (4) Z 11 In the discrete case,, a convolution is expressed as a weighted sum of the 
signal gover the (2n1)size of the .lter kernel F: n X 0 g(u) g(u)F(ui)(5) i 1 n An extension to higher 
dimensions is straightforward by convolu­ting the .lter function by itself. The main effect of such a 
low-pass .ltering is a smoothing of sharp details together with a reduction of noise and speckle. The 
separability enables a .ltering of a 2563 dataset by a Gaussian kernel of 33 within 6.51 seconds, see 
also tab. 2. On the other hand, the intermediate results must be buffered, which temporally requires 
a duplication of memory space. Figure 2: Filtering and volume rendering of an original 2563 dataset 
with Gaussian kernels of 33,53,and 73 Fig. 2 presents a 2563 dataset after .ltering with Gaussian ker­nels 
of increasing size and volume rendering using Levoy s method [8]. Although noise and speckle artifacts 
are reduced, the surfaces gradually appear over-smoothed, non-sharp, unnatural, and do not show much 
details. In an earlier study [16] we tested the processing of the ori­ginal data by the combination of 
three different .lters: Gaussian for noise/speckle reduction, speckle removal for contour smoo­thing, 
and median for both noise/speckle reduction and closing of small gaps caused by misalignments and differences 
in the ave­rage luminosity between subsequent images. The .lters had been implemented in 3D and applied 
to the acquired data during a pre­processing stage. Although the results have been very encouraging, 
such .lters require signi.cant computing times and therefore they remain pre-processing stages. Further, 
the large pre-processing ti­mes do not allow interactive control of the .ltering level by the user, thus 
making this approach in.exible, non-interactive and not suitable for clinical applications. Finally .ltering 
also changes the original nature of the data and is therefore not generally accepted in the medical community. 
Multi-Resolution Surface Identi.cation The aim of this paper is to develop a fast, intuitive and effective 
method for rendering clear contours from fuzzy data. All techni­ques must allow for a user-de.ned, interactive, 
on-line processing on average computers in order to be used under routine clinical conditions. This goal 
has been achieved through a combination of multi-resolution analysis [2], [18], [10], .ltering [6], and 
volume rendering [8], and consists of three subsequent tasks: 1. identify and separate noisy areas carrying 
effectively no infor­mation (segmentation), discussed in section 4 2. process the data within the remaining 
areas (.ltering), discussed in section 5 3. use adequate visualization methods (volume rendering), discus­sedinsection 
6.  The .rst task is used to allow the rays to penetrate up to the internal surfaces of interest. In 
addition, it accounts for signi.cant speed­up, since the following computer intensive tasks can be performed 
only within the remaining areas. The second task enables the recon­struction of a well-de.ned surface 
and the third task is necessary to create a smooth-looking surface meaningful to the human eye. All three 
tasks are performed in a multi-resolution framework. After reading the original data, a Gaussian pyramid 
of levels-of-detail (.g. 3) is initially generated [14]. Such a pyramid requires only 14% additional 
memory space and is computed within 0.27 additio­nal seconds for a 1283 and 0.88 additional seconds for 
a 2563 .eld for a kernel of 33 on a SUN Sparc20 due to the separability of the Gaussian kernel (see tab. 
2). Figure 3: The Gaussian pyramid 4 Segmentation The main idea for the multi-resolution feature identi.cation 
and segmentation is that real surfaces posses a higher coherency as compared to speckle or noise. This 
means that when .ltering the datato successivelowerlevelsofdetail(correspondingtohigherpy­ramid levels 
with lower voxel resolution), noise, speckle and minor structures could be expected to disappear faster 
than useful surfa­ces due to their lower coherency, whereas larger surfaces should still remain detectable 
even on lower resolutions. By inverting this idea, after generating a level-of-detail representation 
of the original data, we .rst try to identify crucial regions on higher pyramid levels where little or 
no noise/speckle is expected. Information carrying areas are then propagated (mapped) to higher resolution 
levels and the process is repeated until the original data level is reached. Regi­ons masked out by this 
processing are not further examined by any of the subsequent levels. The result of the segmentation processing 
is a binary mask de.ning a region of interest ROI (since we operate in voxel space, we generate a 3D 
mask de.ning a volume of interest VOI). Only within this VOI region are valid contours expected and therefore 
only these are accessed during volume rendering. The remaining non-valid areas are regarded to be empty. 
 In a multi-resolution framework, the process theoretically starts with the .rst node (the root) of the 
pyramid. In practice, however, only 1 to 3 levels over the highest resolution (pyramid basement) are 
used. Starting on a certain level n, regions of interest are identi.ed: wetestedtwodifferent selectionmethodsdiscussedin 
thefollowing sections 4.1 and 4.2. Then each valid voxel is projected on the next higher resolution level. 
Due to the multi-resolution framework, one voxel of level nprojects onto 8 voxels of the level n1. Changing 
the levels, the same procedure is repeated with levels n1and n2 and so on until the highest resolution 
level 0 is reached. However, after leaving the starting level n, only voxels falling within the already 
selected areas are considered, reducing the amount of computing time, a crucial fact with increasing 
volume resolution. An implication of each voxel projecting on 8 successors is that the selection procedure 
is a crucial decision for the success of the method. A voxel misclassi.ed as empty on level nwill introduce 
a gap of 8 voxels on level n1,64 voxels on level n2etc. Such gaps introduce serious visible artifacts 
similar to those shown in .g. 7 left. On the other hand, if the selection criterion is rather relaxed, 
the effect of noise/speckle reduction will be missed. A possible solution is to start with a rather relaxed 
criterion and become gradually more restrictive when propagating to higher resolutions. Instead of .ltering 
the grey values of the voxels Vn[u,v,w],we decided to process the opacity volume On[u,v,w]and leave the 
data itself unchanged. Thus, for every level of the data pyramid we allocate a second opacity .eld of 
equal size thereby creating an opacity pyramid and assigning to each element the corresponding opacity 
value as calculated by Levoy s classi.cator (eq. 2). 4.1 Mathematical Morphology Mathematical morphology 
can be used to implement a wide spec­trum of operations on binary images or datasets. The fundamental 
morphological operations on a binary image Pwith an element E are erosion and dilation, as well as their 
combination opening. EROSION(P, E ) : f p j E p included inP g DILATION(P, E ) : f p j E p P 6 fgg OPENING(P, 
E ) : DILATION(EROSION(P, E ) , E ) The theoretical details are given in [5] and will not be repeated 
here. In simple words, depending on the shape and size of E, erosion removes structures smaller than 
a certain size and can be used for removing noise or speckle blobs. Dilation enlarges details larger 
than a minimum size and can be used to .ll up small contour gaps. Opening is an erosion followed by a 
dilation. All operations can be used in an iterative way, i.e. an already eroded volume can be eroded 
again etc. The element for the morphology can have several .eld is then morphologically processed and 
the result is the mask shapes. The most common candidates are a small cube of 33 voxels Mn[u,v,w], which 
is then used to mask the local level opacities or a 3D cross containing the central voxel and its 6 face-connected 
by means of the AND operation and is propagated to the higher neighbours (see .g. 4). resolutions as 
shown schematically in .g. 5. Figure 4: Two possible morphological elements E27 and E7 An additional 
degree of freedom is to decide how many voxels of Eshould be occupied in order to select the central 
voxel as to be valid. For reasons of computational ef.ciency, we choose to use the cross with 3 occupied 
voxels on higher and 4 on lower pyramid levels. This means that our selection criterion is more relaxed 
in the beginning and becomes more restrictive on higher resolution levels.  Figure 5: The morphological 
pipeline The .rst action is to calculate for each voxel .eld Vn[u,v,w] on pyramid level nthe corresponding 
opacity .eld On[u,v,w]. For each opacity .eld we calculate a binary volume Bn[u,v,w] by setting all elements 
with a non-zero opacity to 1. This binary Figure 6: From upper left to lower right: 2563 dataset showing 
liver vessels before .ltering and processed with erosion after 20, 30 and 40 iterations Fig. 6 demonstrates 
the effect of erosion after several iterations The example shows and erosion performed with one level 
and an element E6 size of 3. Small and isolated noise/speckle areas have been successfully suppressed 
but the surface of the objects has also been eroded, producing image artifacts clearly visible on the 
magni.cation shown in .g. 7 left. To suppress aliasing of the surfaces we apply the opening operation, 
i.e. a dilation after the erosion in order to .ll the surface gaps created by the erosion. Opening successfully 
suppresses the surface artifacts, preserving the shape of major structures. A comparison of erosion and 
opening can be seen in .g. 7. However, more than 20 30 iterations are necessary in order to obtain a 
good image, resulting in a very time intensive procedure as shown on tab. 2. To conclude, applying erosion 
alone is not suf.cient, since it removes parts of the useful surface together with speckle and noise, 
whereas opening gives better results. Both .lters require several iterations before providing an acceptable 
image quality. An additional drawback of using morphological .lters lies in the large number of parameters 
to be adjusted, i.e. Levoy threshold and Figure 7: Left a magni.cation of .g. 6 after 30 erosion iterations 
showing visible artifacts. Right the same data after 30 opening operations. tolerance, shape &#38; order 
of E, and number of iterations for erosion and dilation. This gives a total of over 6 degrees of freedom, 
making the whole process non-intuitive and impractical for clinical applications.  4.2 BLTP-Method In 
order to overcome the drawbacks of the morphological .lters in intuiton and computational ef.ciency, 
we developed a method we called BLTP meaning binarize, low-pass, threshold &#38; propagate. The principle 
of the .lter is better explained in a 1D example shown in .g. 8 and 9, an extension to 3D is straightforward. 
A Original Opacities Figure 8: The principle of extracting a VOI mask using BLTP .ltering in 1D Again, 
the .rst action is to create for each opacity .eld On[u,v,w]atwo-valued real numbers volume Dn[u,v,w]by 
setting all voxels with an non-zero opacity to 1.0, see .g. 10. Small details, speckle and isolated noisy 
voxels will be converted to small unity steps, whereas large occupied areas will generate more extended 
structures as demonstrated in .g. 8. Due to the usage of discrete unity steps, the convolution (low-pass 
.ltering) of Dn[u,v,w]with a Gaussian kernel is represented by a discrete A 1 u0 A 1.0 0.5 u 0.0 
Figure 9: Result of the convolution of unity steps (upper) with a Gaussian kernel of size 3 (lower) sum 
of Gaussian kernels,symmetrical to the position of the occupied voxel and accordingly shifted in relation 
to each other: n . u.i 2 0 X1 g(u) g(u) 2 e2. 2 2 O i1n 0 where grepresents the original grey values, 
gthe .ltered function, and 2n1 the size of the Gaussian kernel. The graphical represen­tation of such 
a convolution applied on structures of different size is shown in .g. 9. The numerical result of such 
a .ltering for the 1D case can be seen on tab. 1. The structures are symmetrical over the correspon­ding 
maximum value; Due to lack of space some over.owing values have been omitted in tab. 1. We call the low-pass 
.ltered .eld LPn[u,v,w]. Kernel 3 5 1 Vox. .25 .5 .25 .0625 .25 .375 .25 .0625 2 Vox. .25 .75 .75 .25 
.0625 .3125 .625 .625 .3125 3 Vox. .25 .75 1.0 .75 .25 .0625 .3125 .6875 .875 .6875 4 Vox. .25 .75 1.0 
1.0 .75 .0625 .3125 .6875 .9375 .9375 5 Vox. .25 .75 1.0 1.0 1.0 .0625 .3125 .6875 .9375 1.0 Table 1: 
1D low-pass .ltering discrete unit jumps of varying size with different Gaussian kernels. The structures 
are symmetrical over the corresponding maximum value; Due to lack of space some values have been omitted 
In simple words, low-pass .ltering will change the sharp edges of the unity steps to smoother slopes 
and change the maximum of small structures to a lower value. The shape of a structure after .ltering 
(inclination of the slope, value of maximum) depends on both the order of the kernel and the size of 
the initial structure. From tab. 1 one can easily see that .ltering a structure of kunit steps width 
with a kernel of 2n1 results in a structure with an extend of k2npixels. In addition, if k2n, all values 
of the result will be below 1.0, and when k.2n1, there will be k2n coef.cients having the value 1.0. 
This allows .ltering of structures up to a given size as explained below. There are two different possibilities 
to process the .eld LPn[u,v,w]. The .rst arises from the above observation that structures smaller than 
the kernel size will have all their coef.­cients smaller than 1.0. Therefore, in order to eliminate structures 
up to a given structure size k, one has to select a .lter with the next greater odd size 2n1 .kand threshold 
the resulting .eld LPn, setting all opacities below 1.0 to zero. The binary mask Mn[u,v,w]is calculated 
by dilating the result .eld after threshol­ding Tn[u,v,w]by an element Eof the size kin order to expand 
again the remaining 1.0 coef.cients to the original extend of the structures1. After dilation the resulting 
mask Mn[u,v,w]is used for masking the opacities of the current level (AND operation) as well as propagating 
to the next level as shown in .g. 10. The method can effectively and accurately suppress noise and speckle 
blobs. Due to the fact that thresholding with 1.0 is a trivial opera­tion and dilation is performed only 
in regions where 1.0 coef.cients are presented, this processing is completed quick even with large datasets. 
 A simpli.ed and faster version of the above algorithm avoids dilation. After choosing the appropriate 
kernel size and .ltering the binary .eld Dn[u,v,w]in a way similar as above, LPn[u,v,w] is subsequently 
.ltered to a binary .eld Tn[u,v,w]by means of a free de.nable threshold ths, i.e. values below thsare 
mapped to 0 and above thsto 1. By varying the width kof the kernel and the threshold value of thsone 
can easily control the degree of blob suppression. As an example, one can see on tab. 1 that when using 
a kernel of size 5, a threshold just above 0.375 will eliminate structures of unit size and leave larger 
ones unchanged, a 1To eliminate structures with size 1 and 2 one has to use an appropriately scaled kernel 
of size 3 threshold just above 0.625 will eliminate structures with a size of one or two voxels etc. 
A graphical representation of this effect can also be seen in .g. 8. The result after thresholding is 
the binary .eld Mn[u,v,w]which is used to mask the opacities of level n and propagated to level n1 as 
shown schematically2 in .g. 10. Thus this variant is an approximation of the previous one, using a free 
adjustable threshold and avoiding the dilation after calculating Tn[u,v,w]. The advantage of this variant 
of the BLTP method as compared with morphological operation lies in its simplicity and computa­tional 
ef.ciency. A BLTP re-calculation including thresholding, mask calculation and propagation without low-pass 
re-calculation requires only 24.8 seconds for a 2563 pyramid and 8.78 seconds for a 1283 one (see tab. 
2), making the process suitable for interactive purposes. Although the exact size of the eliminated structures 
is only approximately known, the interactive adjustment of the thres­hold thnand the quick feed-back 
allows a comfortable and intuitive processing of the dataset. The effects of using the BLTP .lter can 
be shown in .g. 11. By varying the threshold thn, a signi.cant amount of speckleand noise could be reduced 
whereas the liver ves­sels remained effectively unchanged. To date, this BLTP variant is 2Fig. 10 demonstrates 
only the principle of the method. For reasons of computatio­nal ef.ciency the C-code implementation includes 
several differences, e.g. buffering stages are omitted or merged together with .lter stages etc. Volume 
Kernel Lowpass Pyramid Opacity Dilation Opening Threshold, BLTP Morphology resolution size level 0 level 
1...n .eld 1 iteration 1 iteration Mask, Project pipeline 20 iterations 1283 33 1.0 0.27 3.38 3.15 6.54 
8.78 10.05 139.53 53 1.2 0.38 10.36 2563 33 53 6.51 12.17 0.88 1.24 20.36 28.21 54.78 24.8 32.19 38.21 
1099.6 Table 2: CPU times on SUN Sparc 20 for various .ltering operations with two different kernel 
sizes and volume resolutions. Lowpass &#38; pyramid are computed once per dataset, opacity only when 
changing Levoy s iso-value; these three stages are regaded as offset times since they change very rarily. 
Morphological operations given for an element E3. On the right the total processing time for BLTP with 
variable threshold and opening after 20 iterations. the one most frequently used. All three examined 
methods suppress only blobs of noise or speckle. Other obstacles, such as the ultrasound .eld near the 
transducer or the hand of the fetus obscuring the face, show the same characteristics as the real surfaces 
and therefore must be removed semi-automatically using methods reported in [16].  5 Contour Filtering 
After segmenting the regions of interest, our second task was to improve the appearance of the contours 
within these VOIs. Le­voy s opacity formula (eq. 2), being essentially a combination of an iso-surfacing 
weighted by the local grey level gradient, has been used as starting point for further experiments. We 
examined two additional edge detectors more closely as an alternative to the gra­dient proposed by Levoy 
(see [1] for an excellent survey). In the .rst case we employed a 3D version of the Sobel-operator instead 
of the gradient in the denominator of eq. 2. The 2D kernel is: Sx opacity 1 8 a 8 101 202 101 Sy 1 
8 1 2 0 0 1 2 1: g(u,v,w) 1 1 r jg(u,v,w)1Sj pS2 x S2 y S2 z : pS2 x S2 y 0: otherwise S 1 0 1S2 z > 
0 The result can be seen in .g. 12. The differences in the quality as compared to Levoy s original method 
are rather small, however, the computation time has been almost doubled. In the second case we used a 
Laplacian-of-Gaussian (LOG) operator. Again, the 2D kernel is: 0 00 121 11 Lu,v 0160 242 1616 0 00121 
 121 212 2 121 8 1: g(u,v,w)S 1 jg(u,v,w)1Sj opacitya1: jLu,v,wj>0 r jLu,v,wj 0: otherwise  We implemented 
several other experiments with the goal to im­prove the selectivity of Levoy s formula, e.g. by using 
information for contours and/or grey values from neighbouring resolution levels. All these experiments 
failed to provide signi.cant improvements and therefore are not reported here. As a result, the original 
Levoy formula has so far provided the best trade-off between quality and speed. 6 Volume Rendering This 
.lter is a combination of a low-pass followed by an edge detector and is expected to give good results 
even with noisy images. In order to volume render the processed datasets we had to apply Although the 
computing time has been doubled again, the result three modi.cations to the standard volume rendering 
pipeline [15]. shown on .g. 12 has not been signi.cantly improved as compared The .rst modi.cation is 
due to the nature of ultrasonic data as to Levoy s original method. is shown in .g. 13. The ultrasound 
cone shown on the left is surrounded by empty space, i.e. voxels with zero value. Due to the opacity 
formula (eq. 2) this causes a very high gradient and thus a high opacity value along the interface to 
the empty space, which is manifested as a solid curtain obscuring the volume interior. This effect can 
be totally suppressed by turning every opacity neighbouring an empty voxel to zero: Due to the nature 
of ultrasonic imaging, voxels with a zero value may occur only in the empty space and not within the 
data itself. The size of this neighbourhood must be adjusted in accordance with the size of the Gaussian 
kernel for the higher pyramid levels. For a kernel with a size of 2n1 the suppression neighbourhood is: 
. opacity: g(ui,vj,wk)60 opacity(u,v,w) 8(i,j,k)2(n...n) 0: otherwise Figure 13: Left a grey image of 
the liver, middle the corresponding opacity values, right a volume rendered dataset. Note the high opacity 
values along the interface between data and empty space (middle) causing a solid curtain obscuring the 
volume interior (right) The second modi.cation deals with the visual appearance of the reconstructed 
surface. As mentioned in section 2, Levoy s method used the local gradient as the surface normal,thus 
being very sensitive to intensity .uctuations typically apparent in ultrasonic data, thereby creating 
an extremely rough surface. Fig. 14 upper left shows how rough such a surface can be after removing almost 
100% of the noise and speckle using BLTP .ltering. It is important to remember that all .ltering operations 
only mask out regions not containing coherent edges leaving thereby the original data unchanged. Thus, 
the small intensity .uctuations remain on the surface and cause large normal vector perturbations resulting 
in an unrecognisably rough surface. Our solution is to accumulate the opacity values as originally proposed 
but to use a normal vector from higher pyramid levels. These levels contain low-pass .ltered data, so 
that the normal vectors are also less perturbed and change more smoothly. Similarly, one can use normal 
vectors from a lower level to add sharpness to a smooth surface. By means of a slider the degree of smoothness 
or sharpness can be changed continuously through (linearly) interpolating the normal vectors from the 
corresponding pyramid levels. N(u,v,w)N(u,v,w)(1 jtj)Ndte(u,v,w)jtj nn 1.0 t1.0(6) This effect is shown 
in .g. 14. It is important to note that for all images exactly the same opacities have been used; the 
different visual appearance is caused only by the modi.cation of the normal vector. The last feature 
is that we extract both surface as well as vo­lumetric information during a single traversing. Extracted 
features are depth-sorted and stored together with their individual depth in a 2D structure similar to 
a ZZ-buffer [17]. After traversing and during visualisation one can select the type of desired visual 
repre­sentation interactively. Also mixed values (e.g. 40% surface, 60% X-ray) can be displayed as shown 
in .g. 15. The visualisation evaluates the values of the ZZ-buffer and since it requires no new ray-traversing, 
it can be performed at rates of several frames per second. The O(n 2)requirements of space allow the 
pre-calculation and storage of a complete rotation sequence in the main memory. The visual representation 
can be selected interactively during the play-back of a pre-recorded loop.   7 Results &#38; Further 
Works For our tests we used datasets from different devices showing eit­her fetuses around the 25th week 
of pregnancy (3 4 months before birth, .g. 16) or scans of the liver (.g. 5, 11). More than a dozen datasets 
like those presented in .g. 16 have been acquired under routine clinical conditions in the Heidelberg 
University Hospital, using a Kretz Voluson 530 device. The typical resolution in a Car­tesian coordinate 
system is 10 Mvoxels, the maximum resolution 16 Mvoxels (2563) . Using the methods reported here and 
in [16], we succeeded in visualizing all of the provided datasets. The time required for the complete 
processing of each set was less than .ve minutes and included loading from the disc, .ltering, segmenting, 
smoothing, visualization and loop generation. The .rst results from the clinical routine have been very 
encouraging indicating clinical evidence at least in the area of the fetal diagnosis [20]. Fig. 17 compares 
an image reconstructed from data acquired in the 25th pregnancy week with a photo of the baby 24 hours 
after birth. The time for volume rendering one image with resolution of 5122 pixels is about 10 seconds 
on an IBM R6000, a rotation sequence requi­red only 3 minutes. Diagnosis of abnormal cases can be de.nitely 
improved. Tab. 3 presents the results for volume rendering a .eld of 4 MVoxels in a resolution of 3002 
pixels using various hardware platforms. The major advantages of our method in visualizing ultrasonic 
data are: Understanding the content of 3D ultrasound images has been signi.cantly improved. Our visualisation 
methods provide a true added value for diagnosis.  Noise and speckle included in the 3D ultrasound data 
has been signi.cantly reduced without altering the original data.   Figure 17: Left data of the 25th 
pregnancy week, right a photograph 24 hours after birth Hardware Surface Vol. Rendering Surface &#38; 
MIP Vol. Rendering Platform Standard High Highest Standard High Highest SUN Sparc10 5.1 11.7 13.6 5.3 
12.2 22 SUN Sparc20 2.3 7.8 9.6 3.6 6.8 14.2 SGI Indigo 2.5 7.7 8.4 2.9 5.9 11.0 IBM R6000 1.9 5.3 5.5 
2.5 4.9 9.1 486 40 MHz 10.9 27.5 44.6 16,5 42.4 68.9 586 90 MHz 1.8 4.4 5.9 3.1 5.4 11.0 Table 3: Runtimes 
in CPU seconds for volume rendering a dataset of 2563 .64 voxels creating both surface and transparent 
(MIP) images at a resolution of 3002 pixels at three different image quality levels The type and degree 
of noise/speckle reduction, surface smoo­thing, visual representation, etc., can be interactively selected. 
The original data remains thereby unchangedand can be referred to at all times. This allows the user 
(physician) to control the degree of altering the original data.  The speed of visualisation enables 
an immediate evaluation of the acquired data during examination of the patient.  The system runs on 
common commercial workstations including PCs and needs no special hardware.  In addition, the BLTP 
.ltering is clearly the method of choice because it provides results similar to mathematical morphology 
but runs at least one order of magnitude faster, is more intuitive and easy-to-use and allows interactive 
manipulation. After several years of research we recognise that the work just started. The most important 
conclusion from this work is that we showed that global .ltering provides good results but is unac­ceptably 
slow for practical applications. On the other hand, VOI identi.cation and local, possibly adaptive, .ltering 
provides even better results within interactive times. Advanced .ltering that ad­apts to the local data 
features in a multi-resolution framework is the most important goal for future work. First of all, the 
BLTP method should be further extended. In­stead of using a Gaussian low-pass .lter, we should consider 
the possibility of employing more suitable band-limiting .lters, enab­ling an elimination of structures 
of a speci.c size. A rich amount of literature exists on this topic. As a very promising avenue of rese­arch 
we are currently investigating the possibility of using wavelets as an alternative for BLTP .ltering. 
Wavelets offer very attractive possibilities, because they combine highly desired signal proces­sing 
features with the multi-resolution framework employed here. Furthermore, one could also investigate the 
usage of 3D-correlation functions, most likely implemented in the frequency domain, for identi.cation 
of surfaces. Both these methods require substantial computing power and have therefore not been considered 
yet for the desiredinteractivelow-costsystem. However,processingtimescan be reduced when working in the 
frequency domain by using signal processors, which makes the additional hardware costs rather rea­sonable. 
Different possibilities employ .lters for edge-preserving resolution reduction, possibly combined with 
local speckle remo­val. We expect that such .lters may provide better results than the Gaussian kernels 
employed here. A remarkable piece of work is presented in [18] claiming ex­cellent results in both precision 
and robustness for extracting edges from noisy images. The author also uses a pyramid of Gaussian .ltered 
2D images to recognise edges in a multi-resolution frame­work. In addition he provides a very good review 
of the theory and mathematics proo.ng of the applicability and superior perfor­mance of his method. The 
main idea there is to calculate edges on different scales using the LOG operator and then multiply the 
corresponding magnitudes. Major edges will thereby be ampli.ed whereas minor edges will be attenuated 
below a given threshold. This shows similarities to our concept of mask propagation along resolution 
levels. An additional step eliminates false positives , i.e. points not surrounded by edges of similar 
strength along the direction perpendicular to the local gradient. This re.ects the idea that edges must 
show a continuity perpendicular to their surface and shows similarities to morphological operators. Although 
the principle ideasaresimilar, the twoapproachesalsoshowsubstantial differences. Schunck s method works 
without resolution reduction, a fact introducing a memory space explosion not tractable in 3D imaging. 
Secondly, he detects edges whereas we identify areas (VOI). Thirdly, the calculation and multiplication 
of edge magnitu­des at different scales is extremely computation intensive whereas we restrict the calculations 
within the areas masked from the lower resolution. Nevertheless, a combination of our approach with his 
advancedand robust edge estimators seems possible and promising. Acknowledgements ThankstoDr. MeindlfromtheDKD,W.Duda 
and K. Holle from Kretz Ultrasound for providing clinical data, as well as to Prof. Wischnik and Dr. 
Hiltmann from the Universit¨ats-Frauenklinik Heidelberg for medical advice and for being the .rst users 
of the system.   REFERENCES [1] Antoniu, E: Trends in Edge Detection Techniques, EURO-GRAPHICS 91 State 
of the Art Report, pp. 111-162, Vol. EG 91 STAR, ISSN 1017-4656, 1991 [2] Burt, P.: The Pyramid as a 
Structure for Ef.cient Computa­tion, A. Rosenfeld (Ed.), Multi-Resolution Image Processing and Analysis, 
Springer Verlag, New York, 1984 [3] Fuchs, H., Kedem, Z., Uselton, S.: Optimal Surface recon­struction 
from Planar Contours, Comm. of the ACM, Vol. 20, No. 10, pp. 693-702, 1977 [4] Gallagher, R., Nagtegaal, 
J.: An Ef.cient 3D Visualization Technique for Finite Elements, Computer Graphics, Vol. 23, No. 3, pp. 
185-194, July 1989 [5] C.R. Giardina, E.R. Dougherty: Morphological Methods in Image and Signal Processing, 
Prentice Hall, Englewood Cliffs, New Jersey, 1988 [6] Gonzalez, R. C., Wintz, P.: Digital Image Processing, 
Second Edition, Addison Wesley Publishing Company, 1987 [7] Herman, G., Liu, H.: 3D Display of Human 
Organs from Com­puter Tomograms, Computer Graphics and Image Processing, Vol. 9, No. 1, pp. 1-21, January 
1991 [8] Levoy, M.: Display of Surfaces from Volume Data, IEEE Computer Graphics &#38; Applications, 
Vol. 8, No. 3, pp. 29-37, May 1988 [9] Lorensen, W., Cline, H.: Marching Cubes: A High Reso­lution 3D 
Surface Construction Algorithm, ACM Computer Graphics, SIGGRAPH-87, Vol. 21, No. 4, pp. 163-169, July 
1987 [10] S. Mallat and S. Zhong, Characterization of Signals from Multiscale Edges, IEEE Transactions 
on Pattern Analysis and Machine Inteligence, Vol. 14, pp. 710-732, 1992 [11] Millner, R.: Ultraschalltechnik, 
Grundlagen und Anwendun­gen, Physik Verlag, Weinheim, ISBN 3-87664-106-3, 1987 [12] Nelson, T., Elvis, 
T.: Visualization of 3D Ultrasound Data IEEE Computer Graphics &#38; Applications, Vol. 13, No. 6, pp. 
50-57, November 1993 [13] Sabella, P.: A Rendering Algorithm for Visualizing 3D Scalar Fields, ACM Computer 
Graphics, Vol. 22, No. 4, pp. 51-58, August 1988 [14] Sakas,G.,Gerth,M.: Sampling and Anti-Aliasing Discrete 
3D Volume Density Textures,EUROGRAPHICS 91 Award Paper, Computer and Graphics, Vol. 16, No. 1, pp. 121-134, 
Perga­mon Press, 1992 [15] Sakas,G.: Interactive Volume Rendering of Large Fields, The Visual Computer, 
Vol. 9, No. 8, pp. 425-438, August 1993 [16] Sakas, G., Schreyer, L., Grimm, M.: Pre-Processing,Segmen­ting 
and Volume Rendering 3D Ultrasonic Data, IEEE CG &#38; A, Special Issue, Vol. 15, No. 4, July 1995. Revised 
version of Case Study: Visualization of 3D Ultrasonic Data, Outstan­ding Case Study Award , Proceedings 
Visualization 94, pp. 369-373 [17] Salesin, D., Stol., J.: The ZZ-Buffer: A Simple and Ef.cient Rendering 
Algorithm with Reliable Antialiazing, Proceedings PIXIM 89 Conference , pp. 451-466,Hermes Editions, 
Paris, September 1989 [18] B. G. Schunk : Edge detection with Gaussian Filters at Mul­tiple Scales of 
Resolution, in: Advances in Image Analysis , Y. Mahdavieh and R. C. Gonzales (Eds.), McGraw Hill 1987 
 [19] Stata, A., Chen, D., Tector, C., Brandl, A., Chen, H., Ohbuchi, R., Bajura, M., Fuchs, A.: Case 
Study: Observing a Volume Rendered Fetus within a Pregnant Patient, Proceedings Vi­sualization 94, pp. 
364-368, 1994 [20] Sakas, G., Walter, S., Hiltmann, W., Wischnik, A.: Foetal Vi­sualization Using 3D 
Ultrasonic Data, Proceedings CAR 94, Berlin/Germany, 21-24 June 1995 [21] Sohn, C., Zoller, W. (Eds.): 
Proceedings II Symposium 3D Ultraschalldiagnostik, Munich, 25-26 June 1993, Imaging -Application &#38; 
Results , Vol. 61, No. 2, pp. 81-135, Karger Pub., June 1994  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218506</article_id>
		<sort_key>475</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>59</seq_no>
		<title><![CDATA[David vs. Goliath or mice vs. men? (panel session)]]></title>
		<subtitle><![CDATA[production studio size in the entertainment industry]]></subtitle>
		<page_from>475</page_from>
		<page_to>476</page_to>
		<doi_number>10.1145/218380.218506</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218506</url>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003458</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing industry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P222661</person_id>
				<author_profile_id><![CDATA[81100183025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pauline]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ts'o]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P279998</person_id>
				<author_profile_id><![CDATA[81332497293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Theresa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ellis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[In-Sight Pix]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P238429</person_id>
				<author_profile_id><![CDATA[81100332863]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ralph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guggenheim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P31907</person_id>
				<author_profile_id><![CDATA[81100349506]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pacific Data Images]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P248820</person_id>
				<author_profile_id><![CDATA[81543216656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thornton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Foundation Imaging]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>Author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218509</article_id>
		<sort_key>477</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>60</seq_no>
		<title><![CDATA[A national research agenda for virtual reality (panel)]]></title>
		<subtitle><![CDATA[report by the National Research Council Committee on VR R&amp;D]]></subtitle>
		<page_from>477</page_from>
		<page_to>478</page_to>
		<doi_number>10.1145/218380.218509</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218509</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.5.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003456.10003462</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003462.10003588</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy->Government technology policy</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39045485</person_id>
				<author_profile_id><![CDATA[81100493478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Randy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pausch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P296140</person_id>
				<author_profile_id><![CDATA[81100455894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Walter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aviles]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207643</person_id>
				<author_profile_id><![CDATA[81100655362]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nathaniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Durlach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P296785</person_id>
				<author_profile_id><![CDATA[81100253164]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Warren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Virtual Reality Games, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39048109</person_id>
				<author_profile_id><![CDATA[81100549374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zyda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naval Postgraduate School]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218511</article_id>
		<sort_key>479</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>61</seq_no>
		<title><![CDATA[Set-top boxes&#8212;the next platform (panel)]]></title>
		<page_from>479</page_from>
		<doi_number>10.1145/218380.218511</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218511</url>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.7</cat_node>
				<descriptor>Consumer products</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010481.10003558</concept_id>
				<concept_desc>CCS->Applied computing->Operations research->Consumer products</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003462</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P147484</person_id>
				<author_profile_id><![CDATA[81100342397]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steinhart]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jonathan Steinhart Consulting, Incorporated]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P66186</person_id>
				<author_profile_id><![CDATA[81100421088]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Derrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burns]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P132345</person_id>
				<author_profile_id><![CDATA[81100031518]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gosling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sun Microsystems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269388</person_id>
				<author_profile_id><![CDATA[81100643785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McGeady]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P243885</person_id>
				<author_profile_id><![CDATA[81406598829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Short]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218513</article_id>
		<sort_key>480</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>62</seq_no>
		<title><![CDATA[Museums without walls (panel session)]]></title>
		<subtitle><![CDATA[new media for new museums]]></subtitle>
		<page_from>480</page_from>
		<page_to>481</page_to>
		<doi_number>10.1145/218380.218513</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218513</url>
		<categories>
			<primary_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P15644</person_id>
				<author_profile_id><![CDATA[81100199461]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alonzo]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Addison]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P69850</person_id>
				<author_profile_id><![CDATA[81100047652]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[MacLeod]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Banff Centre for the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P96437</person_id>
				<author_profile_id><![CDATA[81100629793]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gerald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Margolis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Wiesenthal Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P28508</person_id>
				<author_profile_id><![CDATA[81100507361]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Beit]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashoah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Museum of Tolerance]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17009882</person_id>
				<author_profile_id><![CDATA[81100242173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naimark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interval Research Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P107232</person_id>
				<author_profile_id><![CDATA[81100235697]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schwarz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MediaMuseum, ZKM (Center for Arts and Media Technology)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218515</article_id>
		<sort_key>482</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>63</seq_no>
		<title><![CDATA[Interactive multimedia (panel session)]]></title>
		<subtitle><![CDATA[a new creative frontier or just a new commodity?]]></subtitle>
		<page_from>482</page_from>
		<page_to>483</page_to>
		<doi_number>10.1145/218380.218515</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218515</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P250634</person_id>
				<author_profile_id><![CDATA[81100228737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ruth]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Iskin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Head, Visual Arts & New Media, UCLA Extension]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P200865</person_id>
				<author_profile_id><![CDATA[81100211952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mikki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Halpin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14103765</person_id>
				<author_profile_id><![CDATA[81537567456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nash]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P95690</person_id>
				<author_profile_id><![CDATA[81320492170]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Legrady]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P247587</person_id>
				<author_profile_id><![CDATA[81100539706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Rodney]]></first_name>
				<middle_name><![CDATA[Alan]]></middle_name>
				<last_name><![CDATA[Greenblat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218517</article_id>
		<sort_key>484</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>64</seq_no>
		<title><![CDATA[Integrating interactive graphics techniques with future technologies (panel session)]]></title>
		<page_from>484</page_from>
		<page_to>485</page_to>
		<doi_number>10.1145/218380.218517</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218517</url>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P280014</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa]]></first_name>
				<middle_name><![CDATA[Marie]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lockheed Martin/U.S. EPA Scientific Visualization Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P78095</person_id>
				<author_profile_id><![CDATA[81100047948]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gidney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[College of Fine Arts/University of New South Wales, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P283921</person_id>
				<author_profile_id><![CDATA[81100164746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomasz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imieli&#324;ski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rutgers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14041101</person_id>
				<author_profile_id><![CDATA[81100086306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Pattie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P249300</person_id>
				<author_profile_id><![CDATA[81100523492]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vetter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[North Dakota State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>184515</ref_obj_id>
				<ref_obj_pid>184514</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gidney, C.E., Chandler, A. and McFarlane, G., CSCW for Film TV Preproduction, IEEE Multimedia, Summer 94, 16-26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gidney, C.E. and Robertson, T., Remote Collaborative Drawing Visual Designers, workshop notes, Computational Support Distributed Collaborative Design workshop, Key Centre of Design Computing, University of Sydney, Sept. 1993, 53 - 66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>194317</ref_obj_id>
				<ref_obj_pid>194313</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Imielinski, Tomasz and B.R. Badrinath, Mobile Wireless Computing, Communications of the ACM, October 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>176792</ref_obj_id>
				<ref_obj_pid>176789</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Maes, Pattie "Agents that reduce work and information overload", Communications of the ACM, July 1994]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Maes, Pattie "Animated Agents: Artificial Life meets Interactive Entertainment", Communications of the ACM, March 1995]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rhyne, Theresa Marie, GIS and Visualization Integration - An Unresolved Computing Challenge for the Environmental Sciences, Proceedings of the International Specialty Conference Computing in Environmental Management (Dec. 1994), Air Waste Management Association (A&amp;WMA), 1995, pp. 133 - 144]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>619653</ref_obj_id>
				<ref_obj_pid>182452</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Rhyne, Theresa Marie, "Collaborative modeling and visualiza- An EPA HPCC initiative", Hot Topics (R. Williams, editor), IEEE Computer, Vol. 27, No. 7, July 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Vetter, R., Juell, R, Brekke, D., and Wasson, J., "Storage and Network Requirements of a Low-Cost, Computer-Based Virtual Classroom", Journal of Computing and Information Technology, 2, No. 4, December 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Vetter, R., Tong, S., and Du, D., "Design Principles for Multi- Wavelength and Time Division Multiplexed Optical Passive Networks", Journal of High Speed Networks, Vol.4, No. 2, March 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>620073</ref_obj_id>
				<ref_obj_pid>618990</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Vetter, R., Spell, C., and Ward, C., "Mosaic and the World Wide Web", IEEE Computer, Vol. 27, No. 10, October 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218521</article_id>
		<sort_key>486</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>65</seq_no>
		<title><![CDATA[Videogame industry overview (panel session)]]></title>
		<subtitle><![CDATA[technology, markets, content, future]]></subtitle>
		<page_from>486</page_from>
		<page_to>487</page_to>
		<doi_number>10.1145/218380.218521</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218521</url>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Gaming</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003458</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing industry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010099</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Algorithmic game theory and mechanism design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010293.10010318</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning approaches->Stochastic games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP42049735</person_id>
				<author_profile_id><![CDATA[81339533859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Time Warner Interactive]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31090253</person_id>
				<author_profile_id><![CDATA[81100302938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[Stephen]]></middle_name>
				<last_name><![CDATA[Pierce]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Time Warner Interactive]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P79546</person_id>
				<author_profile_id><![CDATA[81100573630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarvis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Williams/Bally/Midway]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P144724</person_id>
				<author_profile_id><![CDATA[81100283925]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Latta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[4th Wave, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P108599</person_id>
				<author_profile_id><![CDATA[81100631172]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Heidi]]></first_name>
				<middle_name><![CDATA[Therese]]></middle_name>
				<last_name><![CDATA[Dangelmaier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hilt;supgt;*lt;/supgt;D]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P139448</person_id>
				<author_profile_id><![CDATA[81100608404]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Jez]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[San]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonaut Software, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218523</article_id>
		<sort_key>488</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>66</seq_no>
		<title><![CDATA[New developments in animation production for video games (panel session)]]></title>
		<page_from>488</page_from>
		<page_to>489</page_to>
		<doi_number>10.1145/218380.218523</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218523</url>
		<categories>
			<primary_category>
				<cat_node>I.6.8</cat_node>
				<descriptor>Gaming</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Animations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>K.8.0</cat_node>
				<descriptor>Games</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011190</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Computer games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010341.10010349.10011310</concept_id>
				<concept_desc>CCS->Computing methodologies->Modeling and simulation->Simulation types and techniques->Simulation by animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969.10010970</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software->Interactive games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003251.10003258</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems->Massively multiplayer online games</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010070.10010099</concept_id>
				<concept_desc>CCS->Theory of computation->Theory and algorithms for application domains->Algorithmic game theory and mechanism design</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010257.10010293.10010318</concept_id>
				<concept_desc>CCS->Computing methodologies->Machine learning->Machine learning approaches->Stochastic games</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP42049735</person_id>
				<author_profile_id><![CDATA[81339533859]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Veeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Time Warner Interactive]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P221742</person_id>
				<author_profile_id><![CDATA[81332511795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Time Warner Interactive]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P31368</person_id>
				<author_profile_id><![CDATA[81100637314]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zigado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inter-Multi Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P246569</person_id>
				<author_profile_id><![CDATA[81100213848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stein]]></last_name>
				<suffix><![CDATA[III]]></suffix>
				<affiliation><![CDATA[Trilobyte, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39084824</person_id>
				<author_profile_id><![CDATA[81100573310]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Upson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Studio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218526</article_id>
		<sort_key>490</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>67</seq_no>
		<title><![CDATA[Aesthetics &amp; tools in the virtual environment (panel session)]]></title>
		<page_from>490</page_from>
		<page_to>491</page_to>
		<doi_number>10.1145/218380.218526</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218526</url>
		<categories>
			<primary_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Virtual device interfaces</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010969</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Virtual worlds software</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31033698</person_id>
				<author_profile_id><![CDATA[81100243875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Greuel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fakespace, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P219870</person_id>
				<author_profile_id><![CDATA[81536514956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patrice]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Caire]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Virtual Reality and Multimedia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P135034</person_id>
				<author_profile_id><![CDATA[81100438887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Janine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cirincione]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cirincione + Ferraro]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P223391</person_id>
				<author_profile_id><![CDATA[81100260999]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Perry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoberman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telepresence Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P198782</person_id>
				<author_profile_id><![CDATA[81100405840]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scroggins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of the Arts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218529</article_id>
		<sort_key>492</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>68</seq_no>
		<title><![CDATA[Visualizing the Internet (panel session)]]></title>
		<subtitle><![CDATA[putting the user in the driver's seat]]></subtitle>
		<page_from>492</page_from>
		<page_to>494</page_to>
		<doi_number>10.1145/218380.218529</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218529</url>
		<categories>
			<primary_category>
				<cat_node>H.4.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>C.2.1</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003033.10003034</concept_id>
				<concept_desc>CCS->Networks->Network architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P206638</person_id>
				<author_profile_id><![CDATA[81100493939]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nahum]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Gershon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The MITRE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P32121</person_id>
				<author_profile_id><![CDATA[81100252047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bran]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Imagineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39036758</person_id>
				<author_profile_id><![CDATA[81100302796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Georgia Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40024574</person_id>
				<author_profile_id><![CDATA[81100178531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hardin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Center for Supercomputing Applications]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P85943</person_id>
				<author_profile_id><![CDATA[81100040876]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kappe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graz University of Technology, Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P299896</person_id>
				<author_profile_id><![CDATA[81100187762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Ruh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The MITRE Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andrews, K. and F. Kappe, Soaring through Hyper Space: A Snapshot of Hyper-G and its Harmony Client, in Proc. of Eurographics Symposium of multimedia/Hypermedia in Open Distributed Environments, Graz, Austria, W. Hezner and F. Kappe, (editors), pages 181-191, Springer Verlag, June 1994.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gershon, N.D., and W.A. Ruh, The Information Highway: Putting the User in the Driver's Seat (If we do not balance user needs with technical innovation, we will create useless information systems), IEEE Spectrum, to be published]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>260497</ref_obj_id>
				<ref_obj_pid>259963</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mukherjea, S., and J. Foley, Navigational View Builder: A Tool for Building Navigational Views of Information Spaces, CHI '94 Conference Companion, p. 289-290.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Schatz, B.R, and J.B. Hardin, NCSA Mosaic and the World- Wide Web: Global Hypermedia Protocols for the Internet, Science, 265,895-901 (1994).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218531</article_id>
		<sort_key>495</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>69</seq_no>
		<title><![CDATA[Algorithms and the artist (panel session)]]></title>
		<page_from>495</page_from>
		<page_to>496</page_to>
		<doi_number>10.1145/218380.218531</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218531</url>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.5</cat_node>
				<descriptor>Arts, fine and performing**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010469.10010470</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Fine arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010471</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Performing arts</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P223638</person_id>
				<author_profile_id><![CDATA[81414610900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beyls]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[St Lukas Art Institute, Brussels]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77043919</person_id>
				<author_profile_id><![CDATA[81408592586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14170103</person_id>
				<author_profile_id><![CDATA[81100485458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Evans]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vanderbilt University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P136927</person_id>
				<author_profile_id><![CDATA[81100021157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jean-Pierre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hebert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[independent artist]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14090393</person_id>
				<author_profile_id><![CDATA[81100232492]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Musgrave]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[George Washington University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027263</person_id>
				<author_profile_id><![CDATA[81100298718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Roman]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Verostko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Minneapolis College of Art and Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218532</article_id>
		<sort_key>497</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>70</seq_no>
		<title><![CDATA[Performing work within virtual environments (panel session)]]></title>
		<page_from>497</page_from>
		<page_to>498</page_to>
		<doi_number>10.1145/218380.218532</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218532</url>
		<categories>
			<primary_category>
				<cat_node>H.1.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor>Artificial, augmented, and virtual realities</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Virtual reality</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010392</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Mixed / augmented reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003124.10010866</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction paradigms->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387.10010866</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces->Virtual reality</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39083901</person_id>
				<author_profile_id><![CDATA[81100054773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sowizral]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Information and Support Services]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P115353</person_id>
				<author_profile_id><![CDATA[81100351577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Angus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Information and Support Services]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P269698</person_id>
				<author_profile_id><![CDATA[81100575006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bryson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSC/NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14164966</person_id>
				<author_profile_id><![CDATA[81100472471]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Center of Research in Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31086794</person_id>
				<author_profile_id><![CDATA[81341493929]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Mine]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39045485</person_id>
				<author_profile_id><![CDATA[81100493478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Randy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pausch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218534</article_id>
		<sort_key>499</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>71</seq_no>
		<title><![CDATA[Standardisation&#8212;opportunity or constraint? (panel session)]]></title>
		<page_from>499</page_from>
		<page_to>501</page_to>
		<doi_number>10.1145/218380.218534</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218534</url>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Standardization</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP48022709</person_id>
				<author_profile_id><![CDATA[81100021762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arnold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UEA, Norwich, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31038378</person_id>
				<author_profile_id><![CDATA[81100349862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bresenham]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Winthrop University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P159379</person_id>
				<author_profile_id><![CDATA[81100127332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brodlie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Leeds, UK and Rapporteur GKS-94]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P95307</person_id>
				<author_profile_id><![CDATA[81451599728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Carson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GSC Associates, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31080773</person_id>
				<author_profile_id><![CDATA[81100051809]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hardenbergh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Oki Advanced Products]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P222529</person_id>
				<author_profile_id><![CDATA[81100175201]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van Binst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Brussels and EWOS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027057</person_id>
				<author_profile_id><![CDATA[81452592989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Andries]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van Dam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[IBM 3270 Personal Computer/G or GX Reference Information for Picture Interchange Format, SC33-0244, IBM Corporation, Armonk, NY]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>42738</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Arnold, D.B. and Bono, RR. CGM and CGI: Metafile and Interface Standards for Computer Graphics. Springer-Verlag. 1988.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>575421</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Henderson, L.R. and Mumford, A.M. The Computer Graphics Metafile. Butterworths. 1990.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J.E. Computer Graphic Attributes and Reference Model Alternatives. Proceedings Ausgraph'90, Australasian Computer Graphics Association. 1990. pp413-424.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>135787</ref_obj_id>
				<ref_obj_pid>135786</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J.E. Attribute Considerations in Raster Graphics. Computer Graphics Techniques: Theory and Practice edited by D.E Rogers and R.A. Earnshaw. Springer-Verlag. 1990. pp9-41.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[ISO, Information processing systems - Computer graphics - Interfacing techniques for dialogues with graphical devices, Pts 1-6 IS9636 (1991)]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Arnold, D.B. and D.A.Duce, ISO Standards for Computer Graphics: The First Generation, Vol. 1 in Computer Graphics Standards Reference Series, Butterworth's Scientific. ISBN 0- 408-04017-3, pp264 (March 1990)]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[How to choose the right ICT Standardization Policy? Report of and commentary on the CEC workshop. Submitted to Standard View, ACM Perspectives on Standardization (1995) 501]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218536</article_id>
		<sort_key>503</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>72</seq_no>
		<title><![CDATA[Grids, guys and gals]]></title>
		<subtitle><![CDATA[are you oppressed by the Cartesian coordinate system? (panel session)]]></subtitle>
		<page_from>503</page_from>
		<page_to>505</page_to>
		<doi_number>10.1145/218380.218536</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218536</url>
		<categories>
			<primary_category>
				<cat_node>H.1.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>K.7.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003457.10003580.10003583</concept_id>
				<concept_desc>CCS->Social and professional topics->Professional topics->Computing profession->Computing occupations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010383</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Image processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
			<gt>Management</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P99397</person_id>
				<author_profile_id><![CDATA[81100031851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garvey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31049512</person_id>
				<author_profile_id><![CDATA[81100603693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brenda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Laurel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interval Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P243896</person_id>
				<author_profile_id><![CDATA[81100510098]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interval Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31026656</person_id>
				<author_profile_id><![CDATA[81100088065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Staveley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P15555</person_id>
				<author_profile_id><![CDATA[81100387943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Allucqu&#232;re]]></first_name>
				<middle_name><![CDATA[Rosanne]]></middle_name>
				<last_name><![CDATA[Stone]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Texas at Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Quoted by Fritjof Capra, The Turing Point, New York, A Bantam Book, (1982), p. 56]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harding, Sandra, The Science Question in Feminism, Cornell University Press, Ithaca and London, 1986, p. 9]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[McLuhan, Marshall, Understanding Media-The Extensions of Man, A Mentor Book, New York, 1964, p. 112]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[McLuhan, Marshall, Understanding Media-The Extensions of Man, p. 88]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Saul, John Ralston, Voltaire's Bastards, Toronto, London, New York, etc, 1992]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Rabinow, Paul, The Foucault Reader, Panopticism (from Truth and Method)., Pantheon Books, New York, 1984, p.45]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Quoted by George Johnson in the New York Times, The Week in Review, 1994]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Russell, Bertrand, Principles of Mathematics, New York, W. W. Norton &amp; Company, (Originally published 1903 revised in 1938), p. XI]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Quoted by George Johnson in the New York Times]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>180657</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Kelly, Kevin, Out of Control-The Rise of Neo-Biological Civilization, Addison Wesley, Redding, MA, 1994.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Turing, Alan, Mind-A Quarterly Review of Psychology and Philosophy, Vol. LIX. No. 236., Oct. 1950.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218538</article_id>
		<sort_key>506</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>73</seq_no>
		<title><![CDATA[Visual effects technology&#8212;do we have any? (panel session)]]></title>
		<page_from>506</page_from>
		<page_to>507</page_to>
		<doi_number>10.1145/218380.218538</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218538</url>
		<categories>
			<primary_category>
				<cat_node>I.3.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P66143</person_id>
				<author_profile_id><![CDATA[81100082417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Derek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spears]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite Digital Film Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39071335</person_id>
				<author_profile_id><![CDATA[81100123645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Scott]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Windlight Studios, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31033299</person_id>
				<author_profile_id><![CDATA[81100234905]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joblove]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[R/GA-LA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P44202</person_id>
				<author_profile_id><![CDATA[81100256415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Charlie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm & Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14096853</person_id>
				<author_profile_id><![CDATA[81100253911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lincoln]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light & Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>218540</article_id>
		<sort_key>508</sort_key>
		<display_label></display_label>
		<article_publication_date>09-15-1995</article_publication_date>
		<seq_no>74</seq_no>
		<title><![CDATA[3D graphics through the Internet&#8212;a &#8220;shoot-out&#8221; (panel session)]]></title>
		<page_from>508</page_from>
		<page_to>509</page_to>
		<doi_number>10.1145/218380.218540</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=218540</url>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>C.2.1</cat_node>
				<descriptor>Internet</descriptor>
				<type>P</type>
			</other_category>
			<other_category>
				<cat_node>H.4.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.5.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227.10003251</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Multimedia information systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003033.10003034</concept_id>
				<concept_desc>CCS->Networks->Network architectures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Management</gt>
			<gt>Performance</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027810</person_id>
				<author_profile_id><![CDATA[81100488149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Machover Associates Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14143468</person_id>
				<author_profile_id><![CDATA[81547627756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gavin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39024769</person_id>
				<author_profile_id><![CDATA[81100047885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tamara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Munzner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Geometry Center, University of Minnesota]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P83349</person_id>
				<author_profile_id><![CDATA[81100207226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fabio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pettinati]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apple Computer]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P289836</person_id>
				<author_profile_id><![CDATA[81100375614]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Val]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www, info.apple.com/dev/ appledlrectlons/apr95/editornote.html]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[http://www.info.apple.com/cgi-bin/ read.wais.doc.pl?/wais/ TIL/Macintosh{Software/QuickDraw{3D/ QuickDraw!3D!!Q-A]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://www.sgi.com/Products/WebFORCE/WebSpace/]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[http://www.geom.umn.edu/docs/weboogl/ weboogl.html]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[http://vrml.wired.com/]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[http://www.nas.nasa.gov/FAST/Nationalgoals/]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>author</copyright_holder_name>
				<copyright_holder_year>1995</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
