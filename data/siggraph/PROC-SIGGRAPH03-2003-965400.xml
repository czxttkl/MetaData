<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/27/2003</start_date>
		<end_date>07/31/2003</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[San Diego]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>965400</proc_id>
	<acronym>SIGGRAPH '03</acronym>
	<proc_desc>ACM SIGGRAPH 2003 Sketches &amp; Applications</proc_desc>
	<conference_number>2003</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2003</copyright_year>
	<publication_date>07-27-2003</publication_date>
	<pages>142</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P15779</person_id>
			<author_profile_id><![CDATA[81100334998]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Alyn]]></first_name>
			<middle_name><![CDATA[P.]]></middle_name>
			<last_name><![CDATA[Rockwood]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[Colorado School of Mines]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>2003</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<section>
		<section_id>965401</section_id>
		<sort_key>1</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering]]></section_title>
		<section_page_from>1</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39036514</person_id>
				<author_profile_id><![CDATA[81100296840]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barzel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965403</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Object-space point blending and splatting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965403</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965403</url>
		<abstract>
			<par><![CDATA[We present a novel point-based rendering approach based on object-space point interpolation. We introduce the concept of a transformation-invariant covariance matrix of a set of points to efficiently determine splat sizes in a multiresolution hierarchy. We analyze continuous point interpolation in object-space, and define a new class of parametrized blending kernels to achieve smooth blending. Furthermore, we present a hardware accelerated rendering algorithm based on &alpha;-texture mapping and &alpha;-blending.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP17013188</person_id>
				<author_profile_id><![CDATA[81100087869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Renato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pajarola]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Irvine]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P481419</person_id>
				<author_profile_id><![CDATA[81100048438]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sainz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Irvine]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P220360</person_id>
				<author_profile_id><![CDATA[81100440830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guidotti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Irvine]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>581904</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[M. Botsch, A. Wiratanaya, and L. Kobbelt. Efficient high quality rendering of point sampled geometry. In Proceedings Eurographics Workshop on Rendering, pages -, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. Gross. Are points the better graphics primitives? Computer Graphics Forum 20(3), 2001. Plenary Talk Eurographics 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[L. Ren, H. Pfister, and M. Zwicker. Object space EWA surface splatting: A hardware accelerated approach to high quality point rendering. In Proceedings EUROGRAPHICS 2002, pages -, 2002. also in Computer Graphics Forum 21(3).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[S. Rusinkiewicz and M. Levoy. Qsplat: A multiresolution point rendering system for large meshes. In Proceedings SIGGRAPH 2000, pages 343--352. ACM SIGGRAPH, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383300</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[M. Zwicker, H. Pfister, J. van Baar, and M. Gross. Surface splatting. In Proceedings SIGGRAPH 2001, pages 371--378. ACM SIGGRAPH, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Object-Space Point Blending and Splatting Renato Pajarola Miguel Sainz Patrick Guidotti Information 
and Computer Science Electrical Engineering and Computer Scien ce Mathematics University of California 
Irvine University of California Irvine University of California Irvine pajarola@acm.org msainz@ece.uci.edu 
gpatrick@math.uci.edu Abstract We present a novel point-based rendering approach based on object-space 
point interpolation. We introduce the concept of a transformation-invariant covariance matrix of a set 
of points to efficiently determine splat sizes in a multiresolution hierarchy. We analyze continuous 
point interpolation in object-space, and define a new class of parametrized blending kernels to achieve 
smooth blending. Furthermore, we present a hardware accelerated render­ing algorithm based on a-texture 
mapping and a-blending. 1. Introduction Point-based surface representations have recently been established 
as viable graphics rendering primitives [Gross 2001]. In particular they lead to compact multiresolution 
models that can provide effi­cient level-of-detail (LOD) rendering. Recent efforts in point-based rendering 
(PBR) have focused on compact and very efficient multiresolution models [Rusinkiewicz and Levoy 2000, 
Botsch et al. 2002], as well as on high-quality texture sampling and rendering [Zwicker et al. 2001, 
Ren et al. 2002]. The major challenges are to achieve smooth and continuous surface interpolation from 
irregular distributed point samples, sup­port correct visibility as well as provide efficient rendering. 
We propose a novel point blending and rendering technique that is based on the direct interpolation between 
point samples in 3D. In contrast to previous methods, we define the blending of surface points as a weighted 
interpolation in object-space. We analyze the smooth interpolation between points in object-space and 
define a new class of parametrized blending kernels. We also provide an efficient technique to calculate 
splat sizes in a multiresolution point hierarchy. Furthermore, our approach exploits hardware acceleration. 
An example rendering result of our approach for the textured David statue of Michelangelo is shown in 
Figure 1. FIGURE 1. The head of Michelangelo s David statue rendered with t=16 pixels screen tolerance, 
at 1/4 of the full resolution (510827 out of 2000606 points). 2. Exposition In this project we consider 
blending and rendering techniques for surfaces represented as dense sets of point-samples organized in 
a space-partitioning multiresolution hierarchy (octree). The data set consists of surface point samples 
(surfels) s with attributes for spa­tial coordinates p, normal orientation n, surface color c and spatial 
extent as elliptical disk e centered at p and perpendicular to n. These surfels cover the sampled object 
in object-space without holes. In the 4-dimensional homogeneous space of points pi.R3 , thus p' i T = 
(pi T ,1), we define the transformation-invariant generic homogeneous covariance matrix M of points pi 
by M'= .p' i·p' i T , and we can express the homogeneous covari­ance matrix in any local coordinate system 
given by translation T 1T and rotation R as M'= n RTM' TR. Given two differ­ ··· ·T ent sets of points 
P = {p1.. pn} and Q = {q1.. qm} as well as T M' P= .p' i·p' i T and M' Q= .q' i·q' i , the combined generic 
covariance matrix M of the union PQ . is then given byM'= M' P+ M' Qwithout the need to recalculate the 
sum of tensor products of all points PQ. This concept of a generic homoge­ . neous covariance matrix 
is used to efficiently calculate tangential bounding ellipses in the hierarchical multiresolution point 
repre­sentation. We interpret the interpolation of surface parameters such as color in object-space between 
surfels s1 s n as a weighted sum. In fact we interpolate the color c of a pixel p (projection of a point 
 p p) from surfels si whose elliptical disks ei intersect p as c = .()·ciusing conforming blending functions 
.i with p .ip local support over the elliptical disk ei. Using rotation-symmetric ( arb)/( rb) ·(/ 
) 1 (/ ) blending kernels .r= e nn we can define a () conforming blending function .i for the surfel 
si as a normaliza­tion .()= .()/(.()+ ..()) given its overlapping ipipipjp adjacent surfels sj. Based 
on this blending and normalization we developed an efficient rendering algorithm that generates the cor­rect 
pixel colors cp = (..() ·ci)/(..())by rendering an ipip a-textured polygon for each surfel si which represents 
its blending kernel .i in object-space to get the intermediate result c' p = ..()·ciand by a post-process 
per-pixel normalization ip with the accumulated pixel blending weight ..(). ip Table 1 shows experimental 
results of our point rendering algorithm on a 1.5GHz Pentium4 CPU and nVIDIA GeFroce4 Ti4600 GPU. It 
lists the number of visible splats, the time for LOD selection, the time for blending and visibility 
splatting, and the time for color normalization all given in seconds per frame. Model Tol. t #Splats 
LOD Splatting Normalization Total 0.0% 904121 0.315s 1.386s 0.0002s 1.710s David 0.01% 454656 0.269s 
0.791s 0.00019s 1.061s 0.12% 340537 0.195s 0.612s 0.0002s 0.808s TABLE 1. Rendering performance is given 
for each task in seconds used per frame. Tolerance tin percent of viewport.  References M. Botsch, 
A. Wiratanaya, and L. Kobbelt. Efficient high quality rendering of point sampled geometry. In Proceedings 
Eurographics Workshop on Ren­dering, pages , 2002. M. Gross. Are points the better graphics primitives? 
Computer Graphics Forum 20(3), 2001. Plenary Talk Eurographics 2001. L. Ren, H. Pfister, and M. Zwicker. 
Object space EWA surface splatting: A hardware accelerated approach to high quality point rendering. 
In Pro­ceedings EUROGRAPHICS 2002, pages , 2002. also in Computer Graphics Forum 21(3). S. Rusinkiewicz 
and M. Levoy. Qsplat: A multiresolution point rendering sys­tem for large meshes. In Proceedings SIGGRAPH 
2000, pages 343 352. ACM SIGGRAPH, 2000. M. Zwicker, H. Pfister, J. van Baar, and M. Gross. Surface splatting. 
In Pro­ceedings SIGGRAPH 2001, pages 371 378. ACM SIGGRAPH, 2001. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965404</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Multipass rendering in mental ray]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965404</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965404</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31028947</person_id>
				<author_profile_id><![CDATA[81100137101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Haarm-Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duiker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14020855</person_id>
				<author_profile_id><![CDATA[81100022415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Driemeyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[mental images]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multipass Rendering in mental ray Haarm-Pieter Duiker* Thomas Driemeyer* ESC Entertainment mental images 
 Introduction Rendering resources are by definition constrained and yet the task of rendering many objects 
at high quality is frequently encountered in the effects industry. Breaking up scenes into renderable 
pieces, or passes, is a common approach to rendering large scenes. To achieve the quantity of photo-real 
effects in frame that were required by the scripts of the Matrix Reloaded and the Matrix Revolutions, 
mental images and ESC Entertainment developed a system for rendering and automatically compositing many 
passes. The system we developed has the key advantage that it takes as its compositing primitive not 
the pixel but the sample. The System One difficulty with the standard, pixel-based, approach to depth 
compositing is that automatically assembling passes can often show artifacts because the depth and color 
values present in the pixels of each pass are the filtered average of possibly many samples taken from 
the rendered scene. Each pixel being a filtered average means that in an area of partial occlusion or 
transparency, meaningful values for transparency or depth may be obscured almost completely if not lost. 
This can make all but impossible the job of producing a composite that is the same as if all passes had 
been rendered at once. Taking the scene sample as the primitive for compositing operations means that 
unnecessary filtering operations can be cut out of the process, removing many standard depth­compositing 
artifacts. The system as implemented in mental ray consists of three possible stages. The first stage 
includes the actual rendering of each pass and the creation of files to hold the original scene samples. 
The second, an optional stage, allows for arbitrary processing operations on individual samples files. 
In the third stage, the standard depth compositing operation is applied to the all passes at sample level 
before the resulting samples are filtered down for pixel resolution output. The Multipass Rendering system 
afforded us two main benefits. First and foremost, the system allowed us to render scenes with large 
amounts of geometry as well as finely detailed lighting and shading calculations. We didn t have to sacrifice 
one or other, which given the demands of the movies, was not an option. Large amounts of low resolution 
geometry were used for shadowing purposes while rendering other pieces of high resolution geometry in 
full detail. The second benefit it afforded us was the use of a two-dimensional motion-blurring algorithm 
with a minimum of the traditional depth and transparency related artifacts inherent in such a system. 
Using multipass and two-dimensional motion blurring, we were able to achieve a two and a half dimensional 
motion blurring system with all the speed and parallel processing benefits of a multipass depth compositing 
system with a minimum of the artifacts that traditionally show up with either approach. Conclusion We 
have developed and implemented a system for rendering large scenes in multiple passes and compositing 
the resulting passes in a manner that closely approximates the original scene. *e-mail: hp@escfx.com 
thomas@mental.com Figure 1: The top image is the full composited result of the combination of the all 
the passes below. Acknowledgements The team at mental images deserves credit for the implementation of 
this system. Dev Mannemela has been instrumental in completing a set of tools for the system. Ray Haleblian, 
Scott Liedtka, and Andrew Harris braved the dangers of using the system in production. Andy Lomas, Olivier 
Maury, and Scot Shinderman s algorithms helped, and the advice of John Schlag, George Borshukov, and 
Kim Libreri proved invaluable. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965402</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An efficient spatio-temporal architecture for animation rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965402</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965402</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P398139</person_id>
				<author_profile_id><![CDATA[81100133503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vlastimil]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Havran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P590112</person_id>
				<author_profile_id><![CDATA[81100186442]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cyrille]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Damez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43128557</person_id>
				<author_profile_id><![CDATA[81332517742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Karol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Myszkowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15028898</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E. 1996. Mathematical Models and Monte Carlo Algorithms. PhD thesis, Katholieke Universiteit Leuven.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[VEACH, E. 1997. Robust Monte Carlo Methods for Light Transport Simulation. PhD thesis, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An E.cient Spatio-Temporal Architecture for Animation Rendering Vlastimil Havran, Cyrille Damez, Karol 
Myszkowski, and Hans-Peter Seidel MPI Informatik, Saarbr¨ucken, Germany Producing high quality animations 
featuring rich object appear­ance and compelling lighting effects is very time consuming us­ing traditional 
frame-by-frame rendering systems. In this work we present a rendering framework for computing multiple 
frames at once by exploiting the coherence between image samples in the temporal domain. For each sample 
representing a given point in the scene we update its view-dependent components for each frame and we 
add its contribution to pixels identi.ed through the compen­sation of camera and object motion. In this 
text we describe in more details two major challenges that we face in our framework: The visibility and 
global illumina­tion computation in dynamic environments. Also, we discuss some standard rendering tasks 
such as shading and motion blur. The visibility computation in our rendering framework is based on a 
multi-frame visibility data structure (MFVDS). MFVDS for a given ray provides the visibility information 
for all consid­ered frames at once. A kd-tree data structure is used to implement MFVDS. Static objects 
are stored in a global kd-tree. Dynamic ob­jects are instantiated for every frame. The instantiated objects 
are processed using a hierarchical clustering algorithm to separate them in space. For each cluster of 
instantiated objects a separate kd-tree is constructed that is inserted to a global kd-tree. Finally, 
global kd­tree is re.ned in spatial regions where kd-trees have been inserted. During processing of visibility 
queries the performance of MFVDS is improved by intensive caching of intermediate results. A single visibility 
query using our data structures provides a ray intersection information for a given frame and marks those 
frames for which this information becomes invalid due to dynamic changes in the scene. This allows us 
to avoid redundant computation for each frame if the ray does not hit any instantiated object. In regions 
not populated by animated objects the same ray traversal cost is achieved as for completely static scenes. 
The global illumination computation in our framework is based on a bidirectional path tracing (BPT) algorithm 
[Lafortune 1996; Veach 1997] which uses MFVDS to query visibility for all consid­ered frames at once. 
Each bi-directional estimation of a given pixel color is reused for several frames before and after the 
one it was originally computed for. To reuse these estimates, the BRDF val­ues at the .rst hit point 
of the eye path needs to be recomputed to take into account the new viewpoint. The corresponding estimates 
are then added to the pixel through which this hit point can be seen for the considered frame. Since 
it involves only the evaluation of direct visibility from the viewpoint and a few BRDF recomputa­tions, 
reusing a sample is much faster than recomputing a new one from scratch. Reusing samples for several 
frames also makes the noise inherent to stochastic methods .xed in object space, which enhances the quality 
of the resulting animations. Shaders add rich appearance of objects and can be ef.ciently computed in 
our animation framework. We split our shading functions into view-independent and view-dependent components, 
where the former is computed only once for a given sample point and the latter is recomputed for each 
frame. It is worth noting that in our BPT technique we need to recompute the view-dependent com­ponent 
only for sample points that are hit by primary rays, while for the remaining path segments shading results 
are just reused. Motion blur is an important visual effect in high quality anima­tion rendering, which 
is extremely easy to obtain in our framework. A fragment of the motion-compensation trajectory traversed 
by a given BPT sample within the camera shutter opening time is com­puted and projected to each frame. 
The sample radiance contributes to each pixel traversed by the projected trajectory roughly in a pro­portion 
to the length of traversed path. In practice, we use 2DDA algorithm for piecewise-linear approximation 
of this trajectory and linear interpolation of sample radiance between a pair of frames, which leads 
to very good visual results (refer to Figure 2). We use disc caching, instead of storing in memory all 
frames, which makes the resolution of our animations limited only by the disc capacity. Since the computation 
for the subsequent frames is localized in coherent regions in the image space located along the motion-compensation 
trajectory, we achieve a pretty high cache­hit ratio. As a consequence, disk accesses only increase the 
total computation time of our method by about 10% on average. Conclusions: The main advantage of our 
framework is signi.­cant speedup of animation rendering, which is usually over one or­der of magnitude 
in respect to traditional frame-by-frame rendering, while the obtained quality is always much higher 
due to signi.cant reduction of .ickering. new viewpoint original viewpoint  Figure 1: Bidirectional 
estimations for a given pixel can be used for several camera positions. Only direct visibility and the 
BRDF corresponding to connections C01, C11, and C21 need to be recom­puted.  Figure 2: A sample animation 
frame featuring global illumination and motion blur. References LAFORTUNE, E. 1996. Mathematical Models 
and Monte Carlo Algorithms. PhD thesis, Katholieke Universiteit Leuven. VEACH, E. 1997. Robust Monte 
Carlo Methods for Light Transport Simu­lation. PhD thesis, Stanford University. Copyright held by the 
author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965405</section_id>
		<sort_key>2</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[BRDF]]></section_title>
		<section_page_from>2</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP15038070</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Anyhere Software]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965407</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[View planning for BRDF acquisition]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965407</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965407</url>
		<abstract>
			<par><![CDATA[The estimation of the bi-directional reflectance distribution function (BRDF) of a 3D object requires reflectance measurements under numerous viewing and lighting directions. This sketch summarizes our method to select advantageous directions. Uncertainty minimization of the estimated BRDF parameters forms the theoretical underpinning of our acquisition planner. Our hardware-accelerated planner can aid manual and automatic measurement, reduces measurement effort and increases the quality of the acquired models.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP309742300</person_id>
				<author_profile_id><![CDATA[81541955556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jochen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15028898</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P433357</person_id>
				<author_profile_id><![CDATA[81100504611]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hendrik]]></first_name>
				<middle_name><![CDATA[P. A.]]></middle_name>
				<last_name><![CDATA[Lensch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>636891</ref_obj_id>
				<ref_obj_pid>636886</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LENSCH, H., KAUTZ, J., GOESELE, M., HEIDRICH, W., AND SEIDEL, H.-P. 2003. Image-based reconstruction of spatial appearance and geometric detail. ACM Transactions on Graphics 22, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>248862</ref_obj_id>
				<ref_obj_pid>248860</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WHAITE, P., AND FERRIE, F. 1997. Autonomous exploration: Driven by uncertainty. IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 3, 193--205.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 View Planning for BRDF Acquisition Jochen Lang* Hendrik P. A. Lensch Hans-Peter Seidel MPI Informatik, 
Saarbr¨ucken, Germany Abstract The estimation of the bi-directional re.ectance distribution function 
(BRDF) of a 3D object requires re.ectance measurements under numerous viewing and lighting directions. 
This sketch summarizes our method to select advantageous directions. Uncertainty mini­mization of the 
estimated BRDF parameters forms the theoretical underpinning of our acquisition planner. Our hardware-accelerated 
planner can aid manual and automatic measurement, reduces mea­surement effort and increases the quality 
of the acquired models. 1 Incremental View Planning Samples of the re.ectance of an object are most commonly 
ac­quired by a sensor (a digital camera in our set-up) and a point-light source. A view, i.e. a pair 
of light source and camera position, cap­tures a single re.ectance sample for each point that is visible 
and lit. Camera and light source need to be (manually or mechanically) re­positioned w.r.t. the object 
to sample the re.ection properties of the complete object. As a result the measurement task is laborious 
and resource consuming. Planning can result in gains in ef.ciency by reducing the number of measurements 
required, and it can increase quality by ensuring an appropriate sampling over the complete sur­face. 
Our acquisition planning algorithm incrementally reduces the uncertainty in the parameters of a BRDF 
model. We compute where to sample next such that the uncertainty in the parameters is minimized, i.e., 
where to place the camera and the light source given a set of previously acquired views. At any point 
in the ac­quisition process, all necessary information is contained in the co­variance matrix CoV of 
the model parameters ß . The parameter uncertainty is proportional to the norm of the covariance matrix 
.CoV(ß ,.1,...,.v). for a set of views .1 ....v. The task in view planning then is to maximize the incremental 
certainty gain with each next view . v+1. F = .CoV(ß ,.1,...,.v).-.CoV(ß ,.1,...,.v+1).. (1) For 3D objects 
we evaluate and combine the predicted uncer­tainty of each single surface point over the complete surface, 
taking into account shadowing. As a result, our view planning algorithm selects views that will measure 
specular highlights at each point *e-mail: lang@mpi-sb.mpg.de e-mail: lensch@mpi-sb.mpg.de e-mail:hpseidel@mpi-sb.mpg.de 
on the surface several times with varied viewing and lighting di­rections. We compute the uncertainty 
measure in modern graphics hardware with .oating-point frame buffers. The evaluation is per­formed directly 
on the texture atlas of the object. 2 Results and Conclusion Figure 1: Comparison of Human Expert (on 
the left) and Planner (on the right). The large images show a 3D model rendered with a spatially varying 
Lafortune BRDF. The model acquired by the human expert contains holes in the BRDF due to undersampling 
(black area). The model obtained with the same number of views suggested by our planner samples the surface 
evenly. The small images show .CoV. (see text) -large uncertainty is red and small is blue. The number 
of certain estimates (blue) is larger for the planner than for the human expert and the planner covers 
much more of the surface. It is extremely hard for humans to select good camera and light source positions 
in order to obtain a high-quality re.ectance model. Our view planning technique selects advantagous directions. 
The main contributions of our technique to computer graphics are: We apply measurement theory to quantify 
the uncertainty re­duction in the estimated BRDF parameters by adding a view (camera and light source 
position) of an object.  We perform view planning based on parameter uncertainty re­duction that takes 
into account geometric constraints.  Our hardware-accelerated planner evaluates the objective function 
directly on the texture atlas to .nd the next best view for ef.cient measurement.  In summary, our view 
planning algorithm can assist experts and enables novices to measure the BRDF of 3D objects and it will 
make automatic measurements more ef.cient.  References LENSCH, H., KAUTZ, J., GOESELE, M., HEIDRICH, 
W., AND SEIDEL, H.-P. 2003. Image-based reconstruction of spatial ap­pearance and geometric detail. ACM 
Transactions on Graphics 22, 2. WHAITE, P., AND FERRIE, F. 1997. Autonomous exploration: Driven by uncertainty. 
IEEE Transactions on Pattern Analysis and Machine Intelligence 19, 3, 193 205. Copyright held by the 
author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965408</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A smooth, fast, accurate representation of reflectance]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965408</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965408</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P193680</person_id>
				<author_profile_id><![CDATA[81100527555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kameya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39038865</person_id>
				<author_profile_id><![CDATA[81339512067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Washington State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>649002</ref_obj_id>
				<ref_obj_pid>645310</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[KOENDERINK, J. J., VAN DOORN, A. J., AND STAVRIDI, M. 1996. Bidirectional reflection distribution function expressed in terms of surface scattering modes. Proc. 4th European Conference on Computer Vision, 28--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258801</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LAFORTUNE, E. P. F., FOO, S. C., TORRANCE, K. E., AND GREENBERG, D. P. 1997. Non-linear approximation of reflectance functions. Proceedings of SIGGRAPH 97 (August), 117--126.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614376</ref_obj_id>
				<ref_obj_pid>614267</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[LEE, S., WOLBERG, G., AND SHIN, S. Y. 1997. Scattered data interpolation with multilevel b-splines. IEEE Transactions on Visualization and Computer Graphics 3, 3 (July-September), 228--244.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Smooth, Fast, Accurate Representation of Re.ectance Masaki Kameya Robert R. Lewis School of EECS; 
Washington State University The interaction of light with a surface is typically described by a bidirectional 
re.ectance distribution function (BRDF). The BRDF, which represents all visible properties of the surface 
at the sub­textural level, is a function of (some parameterization of) incident and re.ected directions. 
We describe here a method to model any (especially measured) BRDF data by applying a modi.ed form of 
the multilevel B-spline approximation ( MBA ) algorithm presented by [Lee et al. 1997] to .t that data. 
Previous Work Many researchers have addressed the problem of representing BRDFs. We can generally classify 
their models as ad-hoc, physically-based, or data-driven. We will address the latter class here. Its 
members usually represent the BRDF as a weighted sum of basis functions, the most popular of which have 
included wavelets, orthogonal polynomials, and cosine lobes. MBA does not require uniform sampling, allows 
arbitrary accu­racy, is fast to compute and evaluate and, since it uses cubic B­splines, yields a C2 
-continuous .t. The authors show how to merge MBA levels to replace a mesh hierarchy with a single mesh, 
making the evaluation time of the .t independent of resolution level. Results Selecting data for felt, 
leather, aluminum foil, and blue latex paint from publically-accessible databases, the .tting procedure 
was straightforward: Apply a three-dimensional (for an isotropic BRDF) instance of MBA to determine a 
set of coef.cients. Figure 1 shows accuracy results for several levels. For compar­ison, we chose the 
most accurate forms of the models of [Koen­derink et al. 1996] (order 8) and [Lafortune et al. 1997] 
(3 cosine lobes). In all cases, our level 6 shows greater accuracy than the other two by a factor of 
at least 4, and usually an order of magni­tude. Our level 4 has accuracy comparable to the other two 
models. Figure 2 compares the .t evaluation times. Generally, basis func­tion representations must increase 
the number of terms to obtain greater accuracy, thus leading to longer evaluation times. On the other 
hand, the time required for our MBA-based method doesn t increase with the accuracy of the .t: We always 
evaluate a 4 × 4 × 4 submesh of the grid. Conclusions and Future Work Non-parametric, multi-level B-splines 
provide a smooth and fast BRDF representation. By increasing the size of the control lattice, MBA .nds 
an arbitrarily good .t to the data. Other popular models are constrained by an accuracy vs. evaluation 
time tradeoff. A drawback of MBA is the amount of storage required. While we have had some success (typically, 
a size reduction of 20:1) by splitting the .t into coarse, non-sparse diffuse and .ne, sparse specular 
components, we continue to seek further reductions. We also hope to use MBA s arbitrary dimensionality 
to extend this work to anisotropic surfaces, subsurface scattering, and texture representation and to 
implement this algorithm on programmable shading hardware.  Figure 1: Accuracy Comparison. We use several 
error metrics: the root mean square error ( RMSE ), the maximum absolute er­ror ( MAE ), and maximum 
relative error ( MRE ). Lafortune (3 lobe) Koenderink (order 8) Ours (All Levels) log10 (Evaluation Time 
(nsec)) Figure 2: Fit Evaluation Time Comparison.  References KOENDERINK, J. J., VAN DOORN, A. J., 
AND STAVRIDI, M. 1996. Bidirectional re.ection distribution function expressed in terms of surface scattering 
modes. Proc. 4th European Confer­ence on Computer Vision, 28 39. LAFORTUNE, E. P. F., FOO, S. C., TORRANCE, 
K. E., AND GREENBERG, D. P. 1997. Non-linear approximation of re­.ectance functions. Proceedings of SIGGRAPH 
97 (August), 117 126. LEE, S., WOLBERG, G., AND SHIN, S. Y. 1997. Scattered data interpolation with multilevel 
b-splines. IEEE Transactions on Visualization and Computer Graphics 3, 3 (July -September), 228 244. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965406</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Fast specular highlights by modifying the Phong-Blinn model]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965406</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965406</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P382476</person_id>
				<author_profile_id><![CDATA[81100317308]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Anders]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hast]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of G&#228;vle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P648040</person_id>
				<author_profile_id><![CDATA[81100465962]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tony]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barrera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Barrera Kristiansen AB]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P80088</person_id>
				<author_profile_id><![CDATA[81100181895]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ewert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bengtsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Uppsala University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>180929</ref_obj_id>
				<ref_obj_pid>180895</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[SCHLICK, C. 1994. A fast alternative to phong's specular model. Graphics Gems 4, 385--387.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Specular Highlights by modifying the Phong-Blinn Model Anders Hast Tony Barrera Ewert Bengtsson 
Creative Media Lab, Barrera Kristiansen AB Centre for Image Analysis University of G¨avle 1 Introduction 
 Specular highlights are usually modeled with the Phong-Blinn illu­mination model, as: Is =(n · h)s , 
(1) where n is the normal, h is the halfway vector and s is the shininess coef.cient. The evaluation 
of the power function is computation­ally expensive and therefore some alternatives have been proposed. 
Schlick [Schlick 1994] tries to produce the very same result as the power function. However, neither 
the power function nor the dot product are based on physical behavior of highlights. Therefore, we could 
as well try some other physically plausible function. We propose such a function which is faster, than 
the power function, to compute. 2 A new Approach Let . =(n ·h) and s = 75. The result of varying . 
from 0.92 to 1.0 is shown in .gure 1. It is quite clear that there will be no highlight if . is less 
than 0.92. We could utilize this fact and produce a similar curve with a polynomial of order d, with 
all roots at t = 0.92. Let . = 1/(1 - t)d , then our function is: f (.)= .(. - t)d , (2) where . assures 
that f (1)= 1. Figure 2 shows four different functions where d varies from 2 to 5. A second order curve 
will of course be faster than higher order curves. However, a higher order curve will give a highlight 
with other properties, that might be desirable for certain kind of lighting conditions and materials. 
The function f (.) will be negative for values of . < t. However, this is no problem because we can simply 
skip the whole specular computation if . < t, since this means that we do not have any highlight at all! 
 3 Conclusions The new approach is faster than using a power function. However, it means that we have 
to de.ne the specular light with t or as an alternative with both t and d. Figure 3 shows an ordinary 
highlight at the left. In the middle image d = 2 and t = 0.92. The highlighted area is larger with this 
method. The area could be decreased by using a larger d as in the image to the right. An alternative 
way of decreasing the highlighted area would be to increase t. The purpose is not really to mimic the 
ordinary specular computation. Instead we propose a new way of computing highlights. References SCHLICK, 
C. 1994. A fast alternative to phong s specular model. Graphics Gems 4, 385 387. Copyright held by the 
author Uppsala University   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965409</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Interactive rendering of translucent deformable objects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965409</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965409</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP45024187</person_id>
				<author_profile_id><![CDATA[81335494509]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mertens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Limburg, Diepenbeek, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40022735</person_id>
				<author_profile_id><![CDATA[81100016395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kautz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14042897</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Limburg, Diepenbeek, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15028898</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14183416</person_id>
				<author_profile_id><![CDATA[81100528244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Van Reeth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Limburg, Diepenbeek, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. 1991. A Rapid Hierarchical Radiosity Algorithm. In Computer Graphics (Proceedings of SIGGRAPH 91), vol. 25, 197--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W., AND BUHLER, J. 2002. A Rapid Hierarchical Rendering Technique for Translucent Materials. ACM Transactions on Graphics 21, 3 (July), 576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826632</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[LENSCH, H. P. A., GOESELE, M., BEKAERT, P., KAUTZ, J., MAGNOR, M. A., LANG, J., AND SEIDEL, H.-P. 2002. Interactive Rendering of Translucent Objects. In Proceedings of Pacific Graphics 2002, 214--224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[SLOAN, P.-P., KAUTZ, J., AND SNYDER, J. 2002. Precomputed Radiance Transfer for Real-Time Rendering in Dynamic, Low-Frequency Lighting Environments. In Proceedings of SIGGRAPH 2002, 527--536.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Rendering of Translucent Deformable Objects Tom Mertens* Jan Kautz Philippe Bekaert* Hans-Peter 
Seidel Frank Van Reeth* Expertise Center for Digital Media* Max-Planck-Institut f¨ur Informatik University 
of Limburg, Diepenbeek, Belgium Saarbr¨ucken, Germany  Introduction Realistic rendering of materials 
such as milk, fruits, wax, marble, and so on, requires to simulate subsurface scat­tering of light. This 
sketch presents an algorithm for plausible re­production of subsurface scattering effects. Unlike previously 
pro­posed work, our algorithm allows to interactively change lighting, viewpoint, subsurface scattering 
properties, and even object geom­etry. The key idea behind our algorithm is two-fold. We derive an ef.cient 
and accurate form factor for computing the amount of light scattered from a whole surface patch to a 
given sample point, which greatly reduces the amount of computation compared to sampling based schemes. 
Secondly, we use a hierarchical data structure to facilitate fast integration, effectively reducing the 
integration com­plexity from quadratic to linear. In contrast to other representations, our hierarchical 
structure allows for ef.cient incremental updates and rendering of deformed geometry. Our experiments 
show that translucent objects can be re-rendered interactively, even if they are deformed or if the subsurface 
scattering properties change. Discussion Computer graphics literature describes many tech­niques for 
realistic and ef.cient visualization of subsurface scatter­ing effects. However, most of them target 
of.ine production quality endering, and none even touch on the problem of dynamic geome­ try. A recently 
proposed technique [?], produces very good results for translucent objects. It is accelerated by using 
hierarchical integra­tion, but is still not interactive. Our work is based on the same subsurface scattering 
model, but achieves interactive rates even for deformable models. The method proposed by [?] is also 
interac­tive, but only for rigid objects with .xed, possibly inhomogeneous, subsurface scattering properties. 
Our technique utilizes the following two main components. First, an ef.cient and accurate form factor 
algorithm for the BSSRDF model used in [?]. This area to point form factor computes the con­tribution 
of an arbitrary planar polygon onto a point in space due to subsurface scattering. It is cheaper to evaluate 
than other numeri­cal integration techniques, especially for large polygons. Our form factor constitutes 
a semi-analytic procedure for computing the ker­nel function of the BSSRDF model, which converges rapidly 
after a few iterations. Second, a hierarchical mesh-based data structure and evaluationmethod, inspired 
by hierarchical radiosity [?] allows interactive deformations of the object. It clusters parts of the 
object together, * {Tom.Mertens,Philippe.Bekaert,Frank.VanReeth}@luc.ac.be {jnkautz,hpseidel}@mpi-sb.mpg.de 
to facilitate fast integration of the scattered energy: less important interactions are handled higher 
in the hierarchy, while the more lo­cal ones are computed at the deeper levels. In practice this reduces 
the complexity of the integration from quadratic to linear. Results The .gure at the top of this page 
shows .ve frames from an interactive deformation of a hollow object. As the dent gets deeper, more light 
from the sky scatters through. The .gure above shows two frames from a real-time animation of the same 
object. Here, a moving, .ickering point light source inside the object simulates the appearance of a 
.ame inside a can­dle. Also, the geometry is dented interactively while the shading remains consistent. 
The .gure on the left shows a rendering with environment mapping. Based on precomputed radiance transfer 
[?], the irradiance for each cluster face can be computed quickly using a spherical harmonics representation 
of the incoming light. Rendering quality is be in­creased by employing high dynamic range rendering, 
environment mapping (including fresnel factor) and tone mapping. The addi­tional computation time is 
negligible since it is implemented in a hardware accelerated vertex and fragment shader. Other experiments 
show that also global deformations can be han­dled ef.ciently (albeit a longer convergence time) as well 
as in­teractively changing material properties. See http://lumumba. luc.ac.be/~tommertens/itrans for 
more info. References HANRAHAN, P., SALZMAN, D., AND AUPPERLE, L. 1991. A Rapid Hierarchical Ra­diosity 
Algorithm. In Computer Graphics (Proceedings of SIGGRAPH 91), vol. 25, 197 206. JENSEN, H. W., AND BUHLER, 
J. 2002. A Rapid Hierarchical Rendering Technique for Translucent Materials. ACM Transactions on Graphics 
21, 3 (July), 576 581. LENSCH, H. P.A.,GOESELE, M., BEKAERT, P., KAUTZ, J., MAGNOR, M. A., LANG, J., 
AND SEIDEL, H.-P. 2002. Interactive Rendering of Translucent Objects. In Proceedings of Paci.c Graphics 
2002, 214 224. SLOAN, P.-P., KAUTZ, J., AND SNYDER, J. 2002. Precomputed Radiance Transfer for Real-Time 
Rendering in Dynamic, Low-Frequency Lighting Environments. In Proceedings of SIGGRAPH 2002, 527 536. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965410</section_id>
		<sort_key>3</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling]]></section_title>
		<section_page_from>3</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P590095</person_id>
				<author_profile_id><![CDATA[81320488260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ioana]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Boier-Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T.J. Watson Research Center]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965414</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA["Knot insertion" on subdivision surfaces]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965414</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965414</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653939</person_id>
				<author_profile_id><![CDATA[81474643213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ram&#243;n]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Montoya-Vozmediano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation,]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CATMULL, E., AND CLARK, J. 1978. Recursively generated b-spline surfaces on arbitrary topological meshes. In Computer-Aided Design, vol. 10, no. 6, 350--355.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Knot Insertion on Subdivision Surfaces   Ramón Montoya-Vozmediano Walt Disney Feature Animation ramon.montoya@disney.com 
   We present a technique that provides the functionality of NURBS knot insertion for Catmull-Clark 
subdivision surfaces. Adding Detail to a Model When using NURBS surfaces it is frequent to start modeling 
with rough low resolution shapes and gradually increase the resolution of the model. This is achieved 
by knot insertion, which adds a new row of control vertices in a given patch without changing its shape. 
But with subdivision surfaces inserting a row of vertices in a given mesh will create unwanted bumps 
and planarities on the limit surface (Figure 1, top). This makes the modeler s work slower and more difficult 
since resolution can not be incremented locally independently of shape. One approach to add new detail 
is to introduce a hierarchy of meshes. Though this is a very flexible technique, it complicates the surface's 
description and introduces issues that impact heavily the production pipeline. We used instead a fast 
approximation technique on the affected area. This combination proved to be very useful in modeling tasks. 
 Mesh Connectivity Modification Our goal is to have the ability of inserting a row of edges in the input 
mesh while keeping its limit shape (Figure 1, bottom). The main requirement on the initial mesh is that 
it should be composed mainly of quadrilaterals, though non-quads can be present. First of all, the user 
selects a given quad face to start the edge insertion. This new edge is propagated all over the surface 
as long as connectivity allows it. The user can also limit the extent of the insertion by preselecting 
a number of faces before doing the operation. At this point we have a 2.n quad strip on the surface that 
has to match the old single row of quad faces. Also, extraordinary points (points with valence different 
to four) might be present on its boundaries. Surface Approximation The real problem is that there is 
no exact way to insert additional vertices locally without disturbing the limit surface. Neighboring 
vertices will need to be adjusted, but at the same time users don t want to disturb too many vertices. 
In practice we have found that working with the neighboring vertices to the edge insertion is a good 
compromise. A neighborhood of faces is then built around the original quads. The vertices on the borders 
of this bigger region are kept fixed, and we will adjust the internal ones. Each internal vertex in 
the new surface is assigned a position in the limit surface of the initial mesh that its own limit position 
will try to match. Again, we have conflicting requirements: in order to keep derivatives from changing 
these points should be distributed unevenly on the surface, but the user expects the insertion to behave 
like a local refinement. In practice, an even distribution on the surface seems to provide the most useful 
tool. The approximation itself is performed by evaluating the difference in limit surface positions 
for each matched point. The difference is added to the corresponding mesh vertices in the new topology. 
Then we repeat until convergence is reached. This fast approximation performs a good job in the presence 
of extraordinary vertices inside the area where the insertion happened. However, the more regular the 
topology the better the approximation will be. Texture coordinates and properties associated with faces 
are also propagated to the newly created faces. Special thanks to Chris Cowan and Miguel Sepulveda. 
 Image converted using ifftoany Image converted using ifftoany Image converted using ifftoany Image 
converted using ifftoany Figure 1 Top row: standard edge insertion on a subdivision surface Bottom row: 
proposed edge insertion  Image converted using ifftoany Image converted using ifftoany Figure 2 Left: 
Original mesh and its limit surface Right: After multiple mesh refinements around the eye References 
 CATMULL, E., AND CLARK, J. 1978. Recursively generated b-spline surfaces on arbitrary topological meshes. 
In Computer-Aided Design, vol.10, no. 6, 350-355. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965413</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Tile representation of subdivision surfaces]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965413</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965413</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31028357</person_id>
				<author_profile_id><![CDATA[81100124724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gonzalez-Ochoa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP28017027</person_id>
				<author_profile_id><![CDATA[81100544780]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hammel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tile Representation of Subdivision Surfaces Carlos Gonzalez-Ochoa* Walt Disney Feature Animation Contributors 
Ramon Montoya VozmedianoDavid Eberle Rasmus Tamstorf Chris Cowan Chuck TappanJason MacLeod Philip Schneider 
Kevin Geiger Introduction The geometric advantages of subdivision surfaces are well known:they provide 
a lightweight, continuous geometric representationfor arbitrary topology meshes. This sketch describes 
our approach at Walt Disney Feature Animation for texture mappingsubdivision surfaces. We have used a 
NURBS-based, 3D painting methodology forseveral productions, which has proven to be very successful inachieving 
our desired level of detail by ensuring that we canprovide enough texture resolution to support the extreme 
range ofcamera views required in a movie. Therefore, we decided to adapt this methodology to subdivision 
surfaces. Our solution is robust enough to support the hundreds of models required by alarge production, 
while maintaining our standards of quality. Approach The key concept is to allow a subdivision surface 
to be broken upinto discrete pieces akin to NURBS patches. While this idea is not new, we have extended 
the paradigm to make each piece itsown separate object, which we call a tile. Each tile has a unique 
name and a well-defined texture space. Tiles keep the geometriccontinuity of the subdivision model and 
fit together seamlesslyregardless of their topology or the shape of their boundaries.They maintain live 
connections to their source subdivision surfaceso that as the subdivision surfaces deforms, the tiles 
are updated. For visualization and interactivity, tiles are represented to the useras a tessellation 
of the subdivision surface. The tessellation level can be controlled to allow finer or rougher approximations, 
asrequired by the user. Having tiles as separate pieces also allowsfor direct manipulation of portions 
of the model, includinginteractive selection, hiding, material assignments, and lightlinking, which would 
be difficult or impossible on the originalsurface. Workflow Tiles are defined by an adjacent set of 
faces in a subdivisionsurface control cage. Because our cages are based on quadmeshes with sparse extraordinary 
vertices, we use heuristics toanalyze the cage topology to create an initial tile layout. These heuristics 
are designed to produce rectangular-shaped tiles thatmaximize texture space usage. The layout can then 
be modifiedmanually by merging or splitting tiles. This provides texture mapartists with some flexibility 
in determining paintable areas of amodel independent of the modeler. *e-mail: cgo@acm.org e-mail: mark.hammel@disney.com 
Mark Hammel Walt Disney Feature Animation The rectangular shape of tiles allows UV coordinates to be 
easily and automatically assigned. This saves us the trouble of analyzing each mesh in order to determine 
how to unwrap it or find a suitable projection method. These two approaches can be very time consuming 
and cumbersome processes when trying to achieve high texture quality. Our UV assignment procedure has 
the additional benefit of allowing other tools (such as fur generation) to evaluate tiles like other 
parametric surfaces. Tiles provide an abstraction layer that hides the specifics of the underlying geometry. 
In order for painters to quickly manage and understand the multitude of textures and their application 
to the model, it is useful to have a consistent flow of UV parameterization across neighboring tiles. 
For this reason we again apply heuristics to automatically orient the UV coordinates on tiles. The adjustable 
tessellation levels on tiles allow painters to achievea closer correspondence between what is painted 
and what willactually be rendered. However, the tessellation is just an approximation and is therefore 
unsuitable for rendering. Instead, the actual subdivision surface definitions for tiles are used, as 
calculated from the cage. At this point the workflow for setting rendering attributes assigning materials, 
binding lights, etc. is no different than for any other geometric primitive. Conclusion Our tile representation 
allows us to take advantage of many of the best features of subdivision surfaces, while still supporting 
a robust texture mapping methodology.  Figure 1. a) Cages of a subdivision surface model, b) face setsfor 
tiles, c) UV orientation of tiles, d) tiles with spacing to show they are separate pieces, e) render 
of tiles with paintedtextures, f) render of tiles with procedural geometry. Imagesand character are &#38;#169;Disney. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965415</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Fair and robust circle splines]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965415</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965415</url>
		<abstract>
			<par><![CDATA[For many applications, such as aesthetic designs or camera paths, nicely rounded, smooth, interpolatory paths -- free of cusps and abrupt hairpin turns -- are most important. Such curves can be obtained from globally optimized minimum variation curves (MVC) [Moreton and S&#233;quin 1992], but at high computational costs. We present a blending scheme between circles that robustly produces equally good-looking G2-continuous curves through very challenging sets of interpolation points. One basic method produces such curves in the plane, on a sphere, and in 3D space.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P41115</person_id>
				<author_profile_id><![CDATA[81100058395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carlo]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[S&#233;quin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653906</person_id>
				<author_profile_id><![CDATA[81100389928]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[U.C. Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134035</ref_obj_id>
				<ref_obj_pid>133994</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[MORETON, H. P. AND SEQUIN, C. H. 1992. Functional Optimization for Fair Surface Design. Proc. ACM SIGGRAPH'92, 167--176.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>342836</ref_obj_id>
				<ref_obj_pid>342822</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SZILVASI-NAGY, M. AND VENDEL, T. P. 2000. Generating Curves and Swept Surfaces by Blended Circles. CAGD 17, 197--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fair and Robust Circle Splines Carlo H. Séquin* CS Division, U.C. Berkeley Abstract For many applications, 
such as aesthetic designs or camera paths, nicely rounded, smooth, interpolatory paths free of cusps 
and abrupt hairpin turns are most important. Such curves can be obtained from globally optimized minimum 
variation curves (MVC) [Moreton and Séquin 1992], but at high computational costs. We present a blending 
scheme between circles that robustly produces equally good-looking G2-continuous curves through very 
challenging sets of interpolation points. One basic method produces such curves in the plane, on a sphere, 
and in 3D space. 1 Introduction When the constraints permit it, the MVC will produce circular arcs as 
solutions, since these have zero variation cost. Thus it is natural to use blends between circular arcs 
to generate the kind of fair curves mentioned above, and several such schemes have been published, e.g. 
[Szilvasi-Nagy and Vendel 2000]. But even the best of those can produce unwanted hairpin turns (Fig.1a). 
We have found that this is caused by the positional interpolation between corresponding arc points, but 
that it can be overcome, if a suitably chosen angle-based parameterization is used instead. Figure 1. 
Blended circle spline segment using (a) positional interpolation, and (b) angle-based parameterization. 
2 Construction Given a sequence of constraint points P0, P1, ... Pi, ... Pn to be interpolated (the control 
polygon ), we form a blend between two circular arcs for every segment (Pi, Pi+1). The first arc (A) 
is defined to go through points Pi-1, Pi, Pi+1 in sequence, and the second one (B) through points Pi, 
Pi+1, Pi+2. These two base arcs define the tangent vectors ti and ti+1 and the curvatures of the composite 
curve at points Pi and Pi+1, respectively. Our approach guarantees, that the blend curve picks up these 
end conditions at points Pi, Pi+1 and that it is well-behaved in between, i.e., has no cusps and no self-intersections, 
and finite curvature, as long as the control polygon does not have a joint with a turning angle of 180o. 
*e-mail: sequin@cs.berkeley.edu e-mail: kiha@lma.berkeley.edu Kiha Lee ME Department, U.C. Berkeley 
Figure 1b shows the construction in the plane. The blend between the top (A) and bottom (B) arcs does 
not occur by simply interpolating circle point positions, as is the case in Figure 1a. Instead, as the 
point P(u) travels across an arc from Pi to Pi+1, the arc morphs from A (u=0) to B (u=1). Any intermediate 
Arc(u) is defined by the two points Pi and Pi+1 and by its tangent t(u) at Pi. The direction angle t(u) 
of this tangent is used to parameterize the morphing process of these arcs. It performs a trigonometric 
blend between the two extreme directions ti and ti+1 given by the tangent vectors ti and ti+1 of the 
base arcs A and B: t(u) = ti cos2(u p/2) + ti+1 sin2(u p/2). (1) To handle robustly the case of arcs 
of arbitrary large radii, including straight-line connections between Pi and Pi+1, the point P(u) traveling 
on Arc(u) is parametrically described as a distance f(u) = |Pi, Pi+1| sin(u t(u)) / sin(t(u)) from endpoint 
Pi and a deviation angle f(u) = (1-u) t(u) from line segment (Pi, Pi+1). This blending scheme guarantees 
that the parameter lines of constant u do not cross each other (Fig.1b), thereby preventing the blend 
curve from creating cusps or loops. If the four points involved in the construction of one blend segment 
do not lie in a plane, then the blend operation also causes the plane p(u) that contains Arc(u) to swivel 
around line segment (Pi, Pi+1). This happens automatically as the tangent vector t(u) rotates according 
to Eqn(1) in the plane defined by its two extreme positions ti and ti+1. Thus the blend curve segment 
will automatically lie on the sphere that passes through the four defining points. If all points of the 
control polygon lie on the same sphere, then the whole composite curve will lie on this sphere (Fig.2a). 
If the control points lie in arbitrary positions in 3D space, Figure 2. Angle-interpolated circle splines 
(a) on a sphere, and (b) in unconstrained 3D space (cross-eye stereo view). The resulting curves have 
local support and exhibit linear and circular precision by construction, and in general show MVC-like 
behavior. They are G2-continuous, and with a simple re­parametrization based on arc-length, can also 
be made C2­continuous. They preserve all symmetries exhibited by the original set of points, including 
front-to-back symmetry, i.e., the curve is not dependent on the direction of evaluation, unlike some 
quaternion splines. References MORETON, H.P. AND SEQUIN, C.H. 1992. Functional Optimization for Fair 
Surface Design. Proc. ACM SIGGRAPH 92, 167-176. SZILVASI-NAGY, M. AND VENDEL, T.P. 2000. Generating Curves 
and Swept Surfaces by Blended Circles. CAGD 17, 197-206. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965411</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[XGen]]></title>
		<subtitle><![CDATA[arbitrary primitive generator]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965411</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965411</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35029452</person_id>
				<author_profile_id><![CDATA[81328490545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[II]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653868</person_id>
				<author_profile_id><![CDATA[81100305675]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ernest]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Petti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653844</person_id>
				<author_profile_id><![CDATA[81100279764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chuck]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tappan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 XGen: Arbitrary Primitive Generator Thomas V Thompson II Ernest J Petti Chuck Tappan Walt Disney Feature 
Animation Walt Disney Feature Animation Walt Disney Feature Animation 1 Introduction The visual richness 
of a CG movie comes in large part from the complexity of its characters and backgrounds. The high grass 
of Shrek, the trees of Ice Age, and the fur of Monsters Inc. are all examples of this. While each of 
these poses its own unique challenges, they all have one thing in common: they involve populating primitives 
(grass, leaves, fur) across large pieces of geometry (ground, tree, monster) in a controllable manner. 
XGen, an Arbitrary Primitive Generator, is designed as a replacement for Disney s existing fur system, 
FurTool. Unlike its predecessor, XGen supports combinations of generation pattern, primitive type, geometric 
patch, and renderer. Further, it layers grooming capabilities from rapid prototyping to intricate detail. 
 2 Design One of the strengths of XGen comes from its flexibility. Unlike previous tools that coupled 
a primitive to its generation method (for example, the fur primitive would be paired with a random placement 
generator), XGen represents these as independent modules. XGen provides four basic modules: primitive, 
generator, renderer, and patch. These modules are combined into a collection called a description. A 
model can have multiple descriptions bound to it. As an example, a curve primitive can be combined with 
a random generator, and a Renderman renderer to create a description of grass. This can then be bound 
to a model composed of subdivision surfaces. Any of the modules can be swapped out. The primitive can 
be replaced with a card primitive to produce leaves on a tree (Figure 1). The Renderman renderer can 
be switched to a GL renderer for interactive previews. Each module is compatible with all others, making 
for a highly flexible system. &#38;#169; Disney Figure 1. Changing the description to produce grass 
and leaves. As a front-end interface, we chose Maya. This allows for a great deal of user interaction 
within a workflow already familiar to the users. For efficiency reasons, the back-end of the system is 
completely independent of Maya and implemented in C++. Having this independence allows the rendering 
pipeline to operate completely outside of Maya. 3 Grooming The power to develop new looks and characters 
quickly ismanaged with a set of grooming tools that includes support forinteractively manipulating guides 
and writing expressions todescribe the overall look. For interactive grooming, users manipulate guides. 
Guides are a small set of primitives that influence the attributes of generatedprimitives, thus representing 
the look of the entire description. Adescription defaults to four guides per patch, one at each corner.User-defined 
guides may be added for fine-tuning. Severalgrooming methodologies are supported for manipulating theseguides. 
Reference Object Grooming (Figure 2) provides for broad scaleguide grooming. The Reference Object is 
a simple sphere that canbe groomed independently of any other models. The pose for eachguide on the model 
is derived from a generated primitive on thereference sphere with a corresponding normal to that guide. 
Thisproduces the model s prototype groom. &#38;#169; Disney Figure 2. XGen Maya interface with Reference 
Object active. Since guides are component-based shapes in Maya, lattices canalso be used for broad scale 
grooming. Fine tuning of the groom isaccomplished through utilities including: copy/paste tools inglobal 
and local space, auto-stitching of corner and edge guides,and the use of a slider interface for manipulating 
individual guideattributes. On an even finer scale, a guides individual controlpoints may be manipulated 
for complete pose control. Layered on top of interactive guide grooming is XGen s expression system. 
Each primitive attribute, such as length, width,and bend, is controlled on a description level through 
an expression that can be a combination of mathematical equationsand custom functions. This allows for 
the rapid global change of agroom by adding maps, MEL scripts, noise, linking attributes andguide interpolation 
values. In addition, expressions supportreferencing surface values such as the point, normal and tangents.These 
allow for point-based noise, silhouette detection, andlighting operations. Other global controls include 
Scrunch, whichadds noise along the length of a curve, creating a more dynamic,natural look ranging from 
coarse hair to steal wool. These globalcontrols allow the creation of many different looks from the samebase 
guide groom (Figure 3). &#38;#169; Disney Figure 3. Three characters produced through one Reference 
Object Groom and additional expression changes. {Tom.Thompson,Ernest.Petti,Chuck.Tappan}@disney.com Copyright 
held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965412</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Mesh modelling with curve analogies]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965412</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965412</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P345895</person_id>
				<author_profile_id><![CDATA[81100173972]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zelinka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Urbana-Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14179493</person_id>
				<author_profile_id><![CDATA[81100516743]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Urbana-Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., AND SALESIN, D. H. 2001. Image analogies. In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM SIGGRAPH, New York. E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581926</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., OLIVER, N., CURLESS, B., AND SEITZ, S. M. 2002. Curve analogies. In Proceedings of the Thirteenth Eurographics Workshop on Rendering, 233--245.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280946</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[SINGH, K., AND FIUME, E. 1998. Wires: A geometric deformation technique. In Proceedings of ACM SIGGRAPH 98, ACM Press / ACM SIGGRAPH, New York. M. Cohen, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 405--414.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mesh Modelling with Curve Analogies Steve Zelinka* Michael Garland University of Illinois at Urbana 
Champaign   Figure 1: Left to right: original mesh, with un.ltered target curves; sample curves: un.ltered 
green, .ltered blue; .ltered target curves; transformed mesh. 1 Introduction True Mesh Analogies, in 
which a mesh is transformed by producing an example of the desired transformation, would be a useful 
mod­elling tool. Analogies support a very wide range of transformations without having to write speci.c 
code for each, as well as allowing unskilled users to make relatively complex transformations so long 
as they can .nd or create an example of the transformation. How­ever, generalizing analogies from images 
[Hertzmann et al. 2001] or curves [Hertzmann et al. 2002] to meshes, beyond the techni­cal challenges, 
puts an increasing burden on the user in creating or .nding the example transformation; while even novices 
may be comfortable editing images or curves, few methods allow novice users to easily edit a mesh. We 
therefore develop an alternate, less powerful approach using Curve Analogies directly. First, the user 
selects a set of target curves embedded in the surface. The user then speci.es a transformation to be 
applied to the curves by sketching a pair of sample curves. Using Curve Analogies, we reproduce the transformation 
between the two sam­ple curves on all of the surface curves. Finally, these transformed curves are used 
to drive a transformation of the surface. 2 Curve Selection The space of .lters which may be applied 
to a mesh using our ap­proach is contingent on the set of curves selected to be transformed. Typically, 
we simply slice the surface by sets of parallel or rotating planes, and use the resulting intersection 
curves. Note that one set of parallel planes would preclude control over features emergent over the direction 
of the plane normal, so we typically use two or more (usually orthogonal) sets of parallel planes. Rotating 
planes can be useful for objects which are (or nearly are) rotationally sym­metric. Other useful sets 
of target curves include iso-parameter lines, for parameterized surfaces, and silhouette curves. 3 Surface 
Transformation Our surface transformation procedure is inspired by the Wires sys­tem [Singh and Fiume 
1998]. Each curve in.uences all mesh ver­tices within a user-speci.ed distance. Each in.uenced vertex 
is *e-mail: zelinka@uiuc.edu e-mail: garland@uiuc.edu Figure 2: Silhouette modi.cation. Figure 3: Low-frequency 
enhancement. moved in the direction its closest point on the untransformed curve is moved, with the 
magnitude of the change a function of the ap­proximate geodesic distance of the vertex to the curve. 
When multi­ple curves in.uence a vertex, its .nal position is a weighted average of the positions suggested 
by each curve, using the same distance­based weighting. Note that we assume the sample density of the 
original mesh is high enough to support the transformation, and we may produce some smoothing at vertices 
with multiple in.uences. 4 Discussion We have used our approach to apply a number of different .lters 
to meshes, including local feature additions, smoothing, and low frequency enhancements. Results typically 
take a few minutes, de­pending on the number of curves and the required sample densities. Rotationally 
invariant Curve Analogies can be dif.cult to con­trol, as the orientation of a .lter can change (e.g., 
from adding bumps to creating indentations). We are addressing this by incorpo­rating the orientation 
of the surface into the Curve Analogy gener­ation. Also, we may create self-intersections in the surface, 
though this should be relatively easy to avoid. References HERTZMANN, A., JACOBS, C. E., OLIVER, N., 
CURLESS, B., AND SALESIN, D. H. 2001. Image analogies. In Proceedings of ACM SIGGRAPH 2001, ACM Press 
/ ACM SIGGRAPH, New York. E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 
327 340. HERTZMANN, A., OLIVER, N., CURLESS, B., AND SEITZ, S. M. 2002. Curve analogies. In Proceedings 
of the Thirteenth Eurographics Workshop on Rendering, 233 245. SINGH, K., AND FIUME, E. 1998. Wires: 
A geometric deformation technique. In Proceedings of ACM SIGGRAPH 98, ACM Press / ACM SIGGRAPH, New York. 
M. Cohen, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 405 414. Copyright held 
by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965416</section_id>
		<sort_key>4</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visual innovations]]></section_title>
		<section_page_from>4</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653955</person_id>
				<author_profile_id><![CDATA[81100035576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derrick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vicarious Visions, Inc.]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965418</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Design-by-example]]></title>
		<subtitle><![CDATA[a schema for designing visualizations using examples from art]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965418</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965418</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P412624</person_id>
				<author_profile_id><![CDATA[81100382211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eileen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vote]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24016921</person_id>
				<author_profile_id><![CDATA[81100403423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Acevedo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653846</person_id>
				<author_profile_id><![CDATA[81100096002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Cullen]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Jackson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653887</person_id>
				<author_profile_id><![CDATA[81100299761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sobel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P62493</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Laidlaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383295</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HERTZMANN, A., JACOBS, C. E., OLIVER, N., CURLESS, B., SALESIN, D. H., "IMAGE ANALOGIES", In Proceedings of ACM SIGGRAPH 2001, ACM Press/ACM SIGGRAPH, New York. E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 327--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SOBEL, J. 2003. A Descriptive Language for 2D Multivariate Scientific Visualization Synthesis. Master's thesis. Brown University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Design-By-Example: A Schema for Designing Visualizations Using Examples from Art Eileen Vote Daniel 
Acevedo Cullen D. Jackson Jason Sobel David H. Laidlaw *Visualization Research Lab, Brown University 
1 Introduction We present a design schema for generating data visualizations using examples from art. 
With this approach a user: 1.) chooses a composition or detail from a favorite painting, 2.) generates 
a template by extracting characteristics or brushstrokes suitable for representing data variables, 3.) 
generates a pre-visualization of the data using a rendering framework [Sobel 2003] and, 4.) transfers 
the features from the painting using the Image Analogies, Texture-By-Numbers algorithm [Hertzmann et 
al. 2001]. Our process eliminates the need for a user to design a visualization from scratch since compositional 
elements such as color, contrast, lighting and texture have already been worked out by the painter or 
artist. The user can then focus on adjusting the relationships among graphical elements in the pre-visualization 
image (Figure 1c) to make sure the output image (Figure 1d) is effective for use in analysis. 2 Process 
 First Step: Select a Source Image. The user chooses a source image from classical or abstract painting, 
graphic art or photography. However, in any source image choice, the detail that will be used to represent 
data must have primitive features that can be easily extracted to signify data elements. In the example 
source image (Figure 1a), we chose a small detail from a larger painting by the author because it had 
features such as circular glyphs and distinct brushstrokes ideally suited to represent our fluid flow 
dataset. Second Step: Label Source Image. Label the component textures of the source image by applying 
a specific color to each distinct area. In Figure 1b, the blue areas represent yellow regions of the 
source image and red areas indicate black regions. The two colors (blue and red) in Figure 1b will be 
used together to represent velocity and magnitude of two-dimensional fluid flow. The green areas indicate 
the greenish brushstrokes of the source image; they will be used to represent vorticity.  Third Step: 
Generate a Pre-Visualization Image. We generated this image with a rendering framework from Sobel [2003]. 
We based it on graphics from the original painting by the author. Fourth Step: Transfer Features from 
Source Image. We used a texture transfer algorithm introduced by Hertzmann et al. to apply features from 
the painting to our pre-visualization image.  3 Results The resulting image (Figure 1d) is a meaningful 
representation of our dataset and it exhibits some of the most inspiring qualities of the painting. In 
addition, by using an example from art, we generated a much more sophisticated result than if we had 
attempted to design the visualization from scratch. 4 References HERTZMANN, A., JACOBS, C. E., OLIVER, 
N., CURLESS, B., SALESIN, D.H., "IMAGE ANALOGIES", In Proceedings of ACM SIGGRAPH 2001, ACM Press / ACM 
SIGGRAPH, New York. E. Fiume, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 327 
340. SOBEL, J. 2003. A Descriptive Language for 2D Multivariate Scientific Visualization Synthesis. Master 
s thesis. Brown University.  * Department of Computer Science, Brown University, Providence, RI 02912 
{evote, daf, cj, jsobel, dhl} @cs.brown.edu Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965419</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Im/possible images]]></title>
		<subtitle><![CDATA[registering the unnoticeable the collapse series (No.2, No.3, No.4, No.10)]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965419</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965419</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18002018</person_id>
				<author_profile_id><![CDATA[81328491221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrzej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zarzycki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Im/possible images; Registering the unnoticeable The Collapse Series (No.2, No.3, No.4, No.10) Andrzej 
Zarzycki This series of images (fig. 3 thru 6) is a bridge between artistic and scientific, between imagined 
and empirical. They represent a quest on the part of the artist to apply principles of the scientific 
method in exploring human perception of the physical environment. I began this quest by asking a question 
vital and central to my work as an artist, what if . For this study, I was specifically interested in 
the human capacity for seeing and perceiving light. What if we could see light in a different way than 
we are now capable? What if we could see individual, distributed strokes of light after they strike one 
object and prior to interacting with any other physical object? If we could only control the physicality 
of our world: time, behavior of light, properties of materials? While it is impossible to change the 
way we physically view light and world around us, it is the act of pursuing these impossibilities that 
brings us from the framework of the scientific method into the realm of imaginative investigation. 1 
Pursuing Realities I felt the best method of exploring these questions was to bring my investigation 
into the virtual environment. Here, I was able to utilize radiosity based ray-tracing software to specifically 
and closely study reflections and refractions in transparent objects in the virtual world. The creative 
process in this case was derived from: the creation of reality in the virtual, the manipulation of tightly 
controlled condi­tions (the scientific), the ability to imagine (the artistic), and the final registration 
of facts in image format. The images presented depict the progressive refinement of an object by methodically 
increasing a number of reflections. With each reiteration, light is allowed another bounce, thus revealing 
more and more of the object s form. In this process of refinement it is intriguing that transparent objects 
can become tempo­rarily opaque and colors behave as mo­mentary attributes not permanent prop­erties. 
This process of a continuous refinement is analogous to the multi-dimensional space formed by the set 
of two facing each other mirrors. A real life observa­ tion of the mirror set (fig.1) reveals an infinite 
number of the three-dimensional spaces as a result of an infinite number of reflections bounced back 
and forth be­tween mirrors. However, with the digital technology this instant and final state of perception 
can be scrutinized and de­layered to reveal all the in-between stages of its existence. (fig.2)  2 Realizing 
Impossibilities My aspiration was not to mimic or test with computer models the reality we observe, nor 
was it to arrive at a photorealistic and em­pirical representation of a change in seeing. My aspiration 
was to fill the gap of what the everyday experience and our physical limita­tions preclude us from seeing 
and in this way inform the perception of reality. I was seeking to use my art in the same way other past 
genre s (e.g. Cubism, Impressionism) utilized their art to converge upon a greater, holistic and unified 
vision of the world about us.  The resulting images represent the process of refining light until ultimately 
we arrive at the closest portrayal of the final reality. These images, as a group, inform human perception 
regarding the act of seeing. The art not only provides a vehicle toward understanding the physical world 
but also pushes the traditional boundaries of perception. With the introduction of the virtual, we are 
able to broaden the picture of reality beyond the human eye and the eye of scientific instruments. Thus, 
we have an increasing understanding of our reality and our world. Furthermore, the artist strongly believes 
once these envisioned im/pos­sible realities become more real to us, they will also af­fect our expectation 
of the surrounding us physical en­vironment. Perhaps, we would be able to control the physicality of 
our world: time, behavior of light, properties of materials. Per­haps, we could experience with our eyes 
what computer simulation is doing for us. (But meantime, we have to rely on a digital tool.) If selected 
for the Sketches and Applications, I would present these and other simi­lar images showing the pro­gression 
of the visual refine­ment and the change in per­ception this art creates.  Figures 3 thru 6. Copyright 
held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965421</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Firefly flash synchronization]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965421</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965421</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31046656</person_id>
				<author_profile_id><![CDATA[81100537885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raghavachary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DreamWorks Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BUCK, J. AND BUCK, E. 1976. Synchronous fireflies. Scientific American, 5, 74--85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[STROGATZ, S. H. AND STEWART, I. 1993. Coupled oscillators and biological synchronization. Scientific American, 12, 68--75.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Firefly flash synchronization Saty Raghavachary DreamWorks Feature Animation saty@anim.dreamworks.com 
1. Introduction Fireflies are bioluminescent insects that emit precisely timed flash­es of light through 
an enzyme reaction. The flashes are thought to serve as mating communication signals. When fireflies 
form a group they synchronize their flashes so that the entire group puls­es on and off as a collective 
unit, a spectacular sight in nature. This sketch presents a simple model of flash synchronization. 2. 
Synchronization model What is presented here is a simplified version of the models found in literature 
[Buck and Buck 1976; Strogatz and Stewart 1993]. The model is easy to describe and also easy to program 
in a vari­ety of languages and animation packages such as Java, StarLogo and Maya. Each firefly is assumed 
to build up its excitation level (say, chemical concentration) until a threshold is reached, at which 
point it emits a flash and resets its excitation level to zero. This can be illustrated using a classic 
'sawtooth' curve (Figure 1). In isola­tion from neighboring fireflies, this would yield precisely timed 
flashes. Figure 1. The buildup and flash cycle for a single firefly. In a group, each firefly slightly 
modifies its flashing pattern to eventually be in flash synchrony with all its neighbors. One way this 
can be achieved is through phase advancement. Figure 2 shows how this works. 3 4 6 9 10 0 1 t = 1 6 2 
t = 2 7 4 t = 3 9 Figure 2. A flash (at t=2) from the middle firefly resets its excita­tion to 0 and 
also causes its three neighbors adjust their own excitation thresholds by skipping a unit each (at t=3). 
Assume that the each fly has a flashing threshold of 10 units. Left to its own, each fly 'counts' up 
from 0, increments its excitation by 1 unit in each time step, flashes when it reaches its threshold, 
resets level to 0, then starts again. But upon seeing a neighbor flash it advances its excitation quicker, 
by 2 units instead of 1 (this is shown in the third panel of Figure 2, at t=3). This eventually leads 
to all flies in a group flashing in unison, each having adjusted its phase in small increments to achieve 
synchrony (Figure 3). The phases of all the flies are said to have become entrained . Fig. 3. Progressive 
stages of flash synchrony - frames taken from an animation sequence of the model implementation. The 
synchronization is not instantaneous for at least a couple of reasons. The flashing is the result of 
a chemical reaction, and the chemicals need time to build up concentration. Further delay is from the 
flies' nervous systems as they process the stimulus of see­ing neighbors flash. 3. Model variations 
The phase advancement model presented above assumes that each fly is capable of sensing flashes from 
neighbors regardless of their proximity. A refinement therefore is to introduce the notion of a 'flash 
radius' which limits the extent to which each fly can sense flashes. A fly will adjust its threshold 
only if it sees flashes from neighbors that are within its flash radius. Making this adjustment to the 
simulation shows clusters of fireflies flashing in synchrony within their own cluster but not with other 
clusters situated beyond the flash radii. Further, if flies from two clusters already in their own synchrony 
approach each other to form a bigger cluster, that leads to all the flies breaking their existing synchrony 
to match flashes with all members in the bigger group. Other variations to explore are having a few 'aging' 
flies with longer excitations (which would slow down the time required to obtain synchrony) and introducing 
'metronomes' with non-varying flash intervals to which all others would synchronize. The pres­ence of 
metronomes leads to 'wave synchrony' where the synchro­nization spreads outward from the metronomes in 
'ripples'. 4. Conclusion The synchronization model presented here might be useful in CG to simulate 
not just firefly flashing but also other collective visual oscillatory phenomena such as clapping of 
hands, dancing etc.  5. References BUCK, J. AND BUCK, E. 1976. Synchronous fireflies. Scientific American, 
5, 74-85. STROGATZ, S. H. AND STEWART, I. 1993. Coupled oscillators and biological synchronization. Scientific 
American, 12, 68-75. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965420</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Implementing a paper flier metaphor using cloth simulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965420</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965420</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P345021</person_id>
				<author_profile_id><![CDATA[81100521493]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Laurent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Denoue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FX Palo Alto Laboratory, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31040957</person_id>
				<author_profile_id><![CDATA[81100407787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Les]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nelson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FX Palo Alto Laboratory, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39029525</person_id>
				<author_profile_id><![CDATA[81100143170]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Elizabeth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Churchill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FX Palo Alto Laboratory, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566623</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BRIDSON, R., FEDKIW, R., ANDERSON, J. 2002. Robust Treatment of Collisions, Contact and Friction for Cloth Animation. In Proceedings of SIGGRAPH 2002, 594--603.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[JAKOBSEN, T. 2001. Advanced Character Physics. In Proceedings of GDCONF 2001, Game Developers Conference.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PROVOT, X. 1995. Deformation Constraints in a Mass-Spring Model to Describe Rigid Cloth Behavior. In Proceedings of Graphics Interface '95, 147--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[VNC, VIRTUAL NETWORK COMPUTING, HTTP://WWW.UK.RESEARCH.ATT.COM/VNC/WINVNC.HTML.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Implementing a paper flier metaphor using cloth simulation Laurent Denoue*, Les Nelson , Elizabeth Churchill 
FX Palo Alto Laboratory, 3400 Hillview Ave, Palo Alto, CA 94304 1 Overview In designing interfaces for 
large screen, digital, community bulletin boards, we have moved away from the desktop metaphor and have 
implemented a paper flier metaphor using a real-time cloth simulation algorithm. We use a real-time 3D 
approach to preserve the aesthetics of physical bulletin boards and add attractive effects like papers 
riffling in the wind (Figure 1). Our dynamic effects grab the attention of passers-by who are also able 
to manipulate the digital fliers with natural gestures. Gestures include folding the corners to see what 
is underneath, adding and removing virtual pushpins, and rotating the digital fliers. Letting go of a 
flier causes it to swing under the force of gravity. 2 Implementation: Simulation and manipulation The 
graphics simulation is a client connected to a server process. Users post new content to the digital 
bulletin board by sending any URL to the server. Upon receipt, the server opens a new Web browser window 
and asks the client to simulate a new flier. Periodically, the client requests the latest version of 
the flier; the server grabs the corresponding window and streams it back to the client. The client maps 
this bitmap as a texture onto the flier. When users actions occur inside a flier, we retrieve the (U,V) 
texture coordinate at this location and send it to the server, as in [VNC]. The server generates the 
same event (e.g. click) onto the corresponding window. With this technique, users can still interact 
with the original content. We represent each flier by a set of particles linked by distance constraints. 
The physics simulator moves each particle independently using a simple but fast verlet integrator [Jakobsen 
2001], taking into account gravity and wind. We don t model springs between particles. An iterative loop 
simply enforces distance constraints between the particles. If two particles are too close, they are 
moved away; if too far apart, they are moved closer. We use structural, bending and shearing constraints 
[Provot 95]. The number of particles is proportional to the size of the texture to display: X pixels 
correspond to (X*7) / 1024 + 4 particles. For example, a 512*512 texture is represented by 7*7 particles. 
These values provide a uniform behavior regardless of the size of the slier and are a good compromise 
between speed and visual realism. In order to get paper-like objects, the loop has to be iterated at 
least 15 times for each flier. With less iteration, the flier resembled a stretchy piece of fabric. To 
speed-up each iteration, we use an approximate square root function [Jakobsen 2001]. Our OpenGL implementation 
in C++ simulates 20 fliers of 7*7 particles on a Pentium II with a low-end NVIDIA Vanta graphics card 
at interactive rates. *e-mail: denoue@fxpal.com e-mail: nelson@fxpal.com e-mail: churchill@fxpal.com 
When a user grabs a border of a flier, the closest particles location is tied to the finger s location 
on the touch-screen. The particle s mass and velocity are set to zero and its Z position is incremented 
a little so that users feel they have lifted the flier from the screen. This technique provides very 
realistic folds but also removes potential self-cloth collisions. To this end, the bending constraints 
are very important to help the flier flip back to a flat resting position. We also add a force pushing 
each particle against a virtual background. 3 User impressions and future work We have carried out preliminary 
user studies. The simulation has proven attractive and appealing; people enjoyed lifting and folding 
fliers, and moving pushpins. Live content that allows hyperlink following proved particularly appealing. 
Our implementation currently lacks a robust collision-detection algorithm. We only check for proximity 
between triangles; when users lift the corner of a flier, we directly modify the Z position causing some 
triangles pass through each other. This is being addressed using Bridson et al. s solution [Bridson et 
al. 2002]. Figure 1. The Digital Bulletin Board runs on a touch-screen, showing folded fliers. Each 
flier is animated with a real-time cloth simulation. The textures are periodically refreshed over the 
network and remain interactive. References BRIDSON, R., FEDKIW, R., ANDERSON, J. 2002. Robust Treatment 
of Collisions, Contact and Friction for Cloth Animation. In Proceedings of SIGGRAPH 2002, 594 603. JAKOBSEN, 
T. 2001. Advanced Character Physics. In Proceedings of GDCONF 2001, Game Developers Conference. PROVOT, 
X. 1995. Deformation Constraints in a Mass-Spring Model to Describe Rigid Cloth Behavior. In Proceedings 
of Graphics Interface 95, 147-154. VNC, VIRTUAL NETWORK COMPUTING , HTTP://WWW.UK.RESEARCH.ATT.COM/VNC/WINVNC.HTML. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965417</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[A horizontal stereoscopic projection system for working at the artist studio & Mimesis, the function that made the organ]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965417</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965417</url>
		<abstract>
			<par><![CDATA[Stereoscopy is a very strong emotional and esthetic factor in visual arts and can be greatly explored using stereoscopic projection systems. However, due to their high cost these systems are very restricted to artists. In this sketch, I describe a horizontal stereoscopic projection system, inspired in the "Responsive Workbench" (GMD, Germany) and the "Taula estereosc&#243;pica" (UPC, Spain ), that is low cost and can be set up in a small studio. In addition, this system has advantages over vertical projection systems as it is shown in Mimesis, the function that made the organ.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31038020</person_id>
				<author_profile_id><![CDATA[81100341825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Agueda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sim&#243;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of the Basque Country]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Layer, H. 1980. Exploring Stereo Images: a Changing Awareness of Space in the Fine Arts. In Leonardo 4, 233--238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>312032</ref_obj_id>
				<ref_obj_pid>311625</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sim&#243;, A. 1999. Attractors: Navigational aids for Virtual Environments. In Conference Abstracts and Applications of ACM SIGGRAPH 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sorensen, V. and Russett, R. 1999. Computer Stereographics: the Coalescence of Virtual Space and Artistic Expression. In Leonardo 32, 41--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965422</section_id>
		<sort_key>5</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Building character]]></section_title>
		<section_page_from>5</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP18000965</person_id>
				<author_profile_id><![CDATA[81100257079]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Townsend]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965424</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Point constraint rig (PCR)]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965424</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965424</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653966</person_id>
				<author_profile_id><![CDATA[81100468243]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vico]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sharabani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[RhinoFX]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Point Constraint Rig (PCR) Vico Sharabani RhinoFX vico@rhinofx.tv This sketch presents a new animation 
technique that makes rotoscoping of movements easier than ever. I call this technique Point Constraint 
Rig (PCR). 1. Introduction. 'Point constraint rig' (PCR) uses tracked points from a 2D image in order 
to drive the animation of a 3D object. Unlike IK and FK rigs (where the distance between the joints is 
set), PCR is a flexible rig that can stretch and squeeze the space between the joints of the character. 
Independent points determine the joints, so the distance between them is being changed all the time, 
and the bones stretch and squeeze to fill up the gap between the points. As a result, the character poses 
achieved with Point Constraint Rig may have distorted proportions in 3D space but will look accurate 
to the viewer through the 3D camera. The idea is very simple and uses fairly basic mathematic expressions. 
 2. The basic concept A pair of two tracked points is being used to calculate the position, rotation 
and scale of each bone. The position of the bone (point3), will be exactly between point1 and point2. 
The rotation will be aligned to look at point1 and Yscale will be a function of the distance between 
the points 1&#38;2. 2. Point constraint rig in production An example for the usage of this rig is 'Fox 
Sports Net - NHL promo' directed by Robert Gottlieb (QuickTime links at the bottom of this page). I had 
to insert skeletons and other elements into the hockey players' bodies, in order to create an X-ray look. 
Schedule and budget did not allow full 3D animation of the player's extreme movements, and motion capture 
was out of the question. In order to rotoscope the preshot footage of the hockey players in action, I 
created a Point constraint rig in Inferno (Note: This concept can be implemented with almost any 3D software). 
 3. The process I started by roughly positioning a 3D model of the human body over a shot image. Then 
I tracked the joints of the hockey players from the live footage, and applied the tracking information 
to the corresponding joints of the 3D model. The rig stretched and squeezed the skeleton to fit the distance 
between the tracked points (as explained in section2). But the result, seen through the camera, has a 
natural movement as well as proportions. Not only does the bone stretch according to the distance between 
the points, but also the rotation of every bone is a function of the relationship between the joints 
on its ends. This function adds complexity to the movement and completes the illusion of a full 3D rig. 
 4. 2D tracking in 3D space A perspective camera was used for this project. The idea was to lock the 
joints in the 3D space to the image, therefore I had to compensate for the camera's FOV, because once 
you position a 2D tracked point further away from the camera, the movement looks as if it was scale down. 
In other words, the tracking information needs to be scaled in proportion to the Z position and the camera's 
FOV. This calculation can be done automatically, using mathematical expressions, or by aligning/scaling 
the tracking information manually. 5. Other usages for 'Point Constraint Rig' The rig works well for 
character animation rotoscoped from live footage. Nevertheless, this concept can be implemented in many 
different ways for building effects or elements such as shadow passes for a greenscreen person, rotoscoping 
2D animation, graphic design, typography etc. In some cases the PCR flexibility will be used to slightly 
correct and refine postures while in other cases the extreme distortion capabilities will come into play. 
 6. Conclusions and future work The animation techniques are blurring, and the distinction between 2D 
and 3D is fading away while concepts and ideas are being borrowed between the two disciplines. The usage 
of 2D tracking in 3D space, allows me to achieve complex effects in a fraction of the budget and tight 
schedules. Point constraint rig has gotten the nickname of Vico s 2.5D .  7. Related images and articles 
on the WEB To learn more about Point Constraint Rig visit: http://www.rhinofx.tv/PCR Copyright held by 
the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965423</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA["Jordan vs Jordan" recreating the basketball legend]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965423</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965423</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP24015798</person_id>
				<author_profile_id><![CDATA[81544927656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[George]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965425</section_id>
		<sort_key>6</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Augmented & mobile reality]]></section_title>
		<section_page_from>6</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P348897</person_id>
				<author_profile_id><![CDATA[81100567689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965429</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Consistent illumination within optical see-through augmented environments]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965429</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965429</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40029142</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18002481</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP45026391</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653948</person_id>
				<author_profile_id><![CDATA[81100029589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kn&#246;del]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>618863</ref_obj_id>
				<ref_obj_pid>616073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BIMBER, O., FR&#214;HLICH, B., SCHMALSTIEG, D., AND ENCARNA&#199;&#195;O, L. M. 2001. The Virtual Showcase. IEEE Computer Graphics & Applications, vol. 21, no. 6, pp. 48--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>854984</ref_obj_id>
				<ref_obj_pid>850976</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BIMBER, O. AND FR&#214;HLICH, B. 2002. Occlusion Shadows: Using Projected Light to Generate Realistic Occlusion Effects for View-Dependent Optical See-Through Displays. ACM/IEEE International Symposium on Mixed and Augmented Reality (ISMAR'02), pp. 186--195.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965427</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Real-time photo-realistic augmented reality for interior design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965427</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965427</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P146848</person_id>
				<author_profile_id><![CDATA[81100111573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cook]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P265425</person_id>
				<author_profile_id><![CDATA[81100256430]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31037417</person_id>
				<author_profile_id><![CDATA[81100328773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Toby]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Howard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15036246</person_id>
				<author_profile_id><![CDATA[81100561115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hubbold]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Manchester, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[S. Gibson et al. ICARUS: Interactive reconstruction from uncalibrated images sequences. In SIGGRAPH 2002 Sketches and Applications Programme, July 2002. San Antonio, Texas.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[P. Debevec and J. Malik. Recovering high dynamic range radiance maps from photographs. In Proceedings of SIGGRAPH 97, Computer Graphics Proceedings, Annual Conference Series.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618511</ref_obj_id>
				<ref_obj_pid>616051</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[G. Greger, P. Shirley, P. M. Hubbard and D. P. Greenberg. The irradiance volume. IEEE Computer Graphics & Applications 18, 2 (March-April 1998).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-time photo-realistic augmented reality for interior design Jon Cook*, Simon Gibson, Toby Howard 
and Roger Hubbold Advanced Interfaces Group, Department of Computer Science, University of Manchester, 
UK Introduction The aim of the ARIS (Augmented Reality Image Synthesis) project is the real-time photo-realistic 
augmentation of a photographic image with synthetic computer generated objects. The driving application 
for this development is interior design where a user wishes to interactively view and manipulate new 
items of furniture in the context of the room for which they are intended, checking for fit and aesthetics. 
The techniques that are being developed could easily be applied to many other scenarios. Novel rendering 
techniques have been developed which allow for the realistic illumination of synthetic objects, accounting 
for both hard and soft shadows, and the compositing of these into a photographic image at interactive 
rates on commodity hardware. Environment capture To provide correct occlusion for the synthetic objects 
a geometric reconstruction of the scene is constructed. This is performed with the ICARUS[1] system which 
allows interactive reconstruction of a geometric model from a single image. A light-probe and high-dynamic 
range (HDR) image are used to capture the lighting environment in the scene[2]. A calibration target 
present in both the HDR image and the image used to build the geometric reconstruction allows the light-probe 
to be located in the scene. Illumination information present in the HDR image is projected onto a geometric 
mesh generated from the reconstructed model. Individual mesh elements act as discrete light sources to 
shade and shadow the synthetic objects. Rendering An irradiance volume[3] of the scene is built as a 
pre-process and sampled at each frame to provide diffuse shading for the synthetic objects. Dynamically 
generated environment maps are used to simulate specular reflectance. Efficient data structures have 
been developed to rapidly identify those mesh elements in the scene which illuminate the synthetic objects 
and to compute where these cast shadows. A single hard­edged shadow for each source of illumination is 
rendered and these are blended together to approximate the real shadow. Image fidelity can be traded 
against performance by varying the number of illumination sources considered. The shaded synthetic objects 
and shadows are first depth­composited with the geometric model to generate occlusions, and then composited 
with the original photograph to produce the final image. Although these rendering techniques are approximations 
we have found them to be sufficient to generate believable representations (see figures 1 and 2). The 
rendering algorithms have been incorporated into an ActiveX control thus allowing the system to be embedded 
within a web page for potential use in e-commerce applications (see figure 3). *e-mail: cookj@cs.man.ac.uk 
A client-server architecture has been developed to support multiple participants viewing and interacting 
with the system simultaneously for example, to allow a remotely located interior design consultant to 
make comments and suggestions. Techniques to simplify the environment capture process are being considered. 
Stereo correspondence could be used to automate the reconstruction of the geometric model if suitable 
images of the scene were available. We are also investigating the accuracy of building the HDR image 
from a library of pre-defined camera response functions, together with a single deliberately under­exposed 
image, of the light-probe since this will capture the perceptually important sources of illumination 
for shadow casting. Fig. 1: Interactive shading and shadowing of a 2,500 triangle synthetic object into 
a background photograph at over 11 frames­per-second on an NVIDIA GeForce4 (left). For comparison, a 
ray-traced image is also shown rendered in 2 hours using existing differential rendering algorithms (right). 
 Fig. 2 (left): Multiple synthetic objects (chair, candlesticks, lamp) Fig. 3 (right): The system integrated 
into a web page. References [1] S. Gibson et al. ICARUS: Interactive reconstruction from uncalibrated 
images sequences. In SIGGRAPH 2002 Sketches and Applications Programme, July 2002. San Antonio, Texas. 
[2] P. Debevec and J. Malik. Recovering high dynamic range radiance maps from photographs. In Proceedings 
of SIGGRAPH 97, Computer Graphics Proceedings, Annual Conference Series. [3] G. Greger, P. Shirley, P.M. 
Hubbard and D.P. Greenberg. The irradiance volume. IEEE Computer Graphics &#38; Applications 18, 2 (March-April 
1998). Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965428</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Distributed mobile multi-user urban simulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965428</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965428</url>
		<abstract>
			<par><![CDATA[This sketch gives an overview of the completed and ongoing research involved in developing a 3D multi-user environment on low-end mobile devices.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653845</person_id>
				<author_profile_id><![CDATA[81100497357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clodagh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rossi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653818</person_id>
				<author_profile_id><![CDATA[81100067019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cummins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P339494</person_id>
				<author_profile_id><![CDATA[81100465557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Sullivan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HAMILL, J AND O'SULLIVAN, C. 2003. Virtual Dublin - A Framework for Real-Time Urban Simulation. In Journal of WSCG - Full Papers Vol. 1--3, No. 11, ISSN 1213-6972.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>820820</ref_obj_id>
				<ref_obj_pid>820738</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KARIMI, HASSAN, A. AND KRISHNAMURTHY, P. 2001. Real Time Routing in Mobile Networks using GPS and GIS techniques. At Hawaii International Conference on System Science.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ARONSON, J. 1997. Dead Reckoning: Latency Hiding for Networked Games. In Gamasutra Website: http://www.gamasutra.com/features/19970919/aronson_01.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Distributed Mobile Multi-User Urban Simulation  Clodagh Rossi Alan Cummins Carol O Sullivan* * e-mail: 
rossic@cs.tcd.ie, cumminsa@cs.tcd.ie, osullica@cs.tcd.ie  Abstract This sketch gives an overview of 
the completed and ongoing research involved in developing a 3D multi-user environment on low-end mobile 
devices. 1 Introduction The overall aim of our research is to produce a distributed multi-user virtual 
model of Dublin City (Figure 1) on low-end mobile devices such as Personal Digital Assistants (PDA, Figure 
2). The project uses the model developed for the Virtual Dublin Project [Hamill and O Sullivan 2003]. 
There are two main areas to be considered: Rendering: The display of 3D models of the buildings is an 
area, which is very much influenced by the limited amount of memory and storage space available on the 
PDAs. Distribution: This includes the multi-user factor, which allows users that are logged onto the 
system to communicate with each other.  2 Research To date three main areas of research have been 
focused upon to speed up rendering times: Visibility Culling: Removal of parts of a given scene if they 
are obscured by other buildings or are out of view. Level of detail Rendering: Rendering of a given 
model at various levels of detail allowing for a scene to be tailored for the desired frame rate. Impostor-based 
techniques: The use of flat 2D images to build up a false impression of 3D scene information.    Figure 
1. Screen Shots of TCD buildings Also influencing rendering times of buildings and the correct display 
of mobile clients locations is the factor that networked mobile units constantly change point of access 
[Karimi et al. 2001]. This can result in packages requiring multiple hops via mobile clients in the network 
from the point of leaving the server to being received by the required mobile client instead of the package 
being sent directly by the server to the client in one hop. These multiple hop packages require more 
time to be received and processed so that buildings are displayed in time and popping effects are reduced 
where the buildings appear to jump on screen. The database currently used by the server to locate and 
upload building data uses mobile clients locations to determine in which section they are situated. The 
buildings associated with this section are then uploaded and sent to the requesting client. The tables 
involved include: Section Table: Uses the client s current position to identify which section of the 
world it is located in. Buildings Table: Provides the number of required buildings in a section and the 
associated id numbers. Name Buildings Table: Contains the name and location of the building models. Extra 
Section Table: Depending on the client s position in the world, if the client is located in but near 
the edge of a certain section it requires extra buildings from surrounding sections. To reduce network 
traffic, a Dead Reckoning technique is used [Aronson 1997]. Dead Reckoning is a form of replicated computing 
in that everyone participating in a multi-user system must simulate all the entities in the environment. 
A predefined set of algorithms is used by all entity nodes to extrapolate the behaviour of entities in 
the simulation. When an agreed deviation from reality has been reached, a correction is issued.   Figure 
2. Screen Shots of TCD Buildings on an iPAQ 3 Future Work To date an interim rendering system has been 
developed that allows for multiple models to be displayed on screen and multiple users to communicate. 
Future work in the distribution of the system will include implementation, evaluation and customisation 
of distributed systems algorithms and further enhancement of the Dead Reckoning algorithm. Other areas 
will include the development of a wireless routing algorithm that will allow the clients to remain in 
contact with each other and the server in an environment without a fixed network topology. References 
 HAMILL, J AND O SULLIVAN, C. 2003. Virtual Dublin A Framework for Real-Time Urban Simulation. In Journal 
of WSCG Full Papers Vol.1-3, No. 11, ISSN 1213-6972. KARIMI, HASSAN, A. AND KRISHNAMURTHY, P. 2001. 
 Real Time Routing in Mobile Networks using GPS and GIS techniques. At Hawaii International Conference 
on System Science. ARONSON, J. 1997. Dead Reckoning: Latency Hiding for Networked Games. In Gamasutra 
Website: http://www.gamasutra.com/features/19970919/aronson_01.htm. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965426</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Augmented reality for air traffic control towers]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965426</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965426</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35031216</person_id>
				<author_profile_id><![CDATA[81100551194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reisman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48023357</person_id>
				<author_profile_id><![CDATA[81100164880]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ellis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schmidt-Ott, J., Ellis, S. R., Krozel, J., Reisman, R., & Gips, J. (2002) Augmented Reality in a Simulated Tower Environment: Effect of Field of View on Aircraft Detection. NASA TM-2002-211853 (October, 2002)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Augmented Reality for Air Traffic Control Towers Ronald Reisman Stephen Ellis NASA Ames Research Center 
NASA Ames Research Center Ronald.J.Reisman@nasa.gov Stephen.R.Ellis@nasa.gov 1. Introduction Augmented 
reality (AR) systems allow users to view the real world along with superimposed or composited virtual 
information. AR systems bear similarities to heads-up displays. NASA is developing head-mounted augmented-reality 
systems that visually superimpose air traffic control (ATC) radar and GPS­based sensor data onto a air 
traffic control tower (ATCT) controller s view of airborne and surface vehicles to overcome low visibility 
problems in ATC towers. The possible benefits of an ATCT AR system include improved situational awareness 
when visibility is hampered by weather or other obstructions, sensor-fusion opportunities, better display 
integration and presentation, and reduced controller memory workload. A principal innovation in the NASA 
initiative is the integration of actual realtime ATC data and GPS-based aircraft transponders. This has 
resulted in proof-of-concept demonstrations based on realistic aviation sensor data, as opposed to speculations 
based on simulations of such sensors. Virtual Displays of ATC radar data  2. Display of Airborne Traffic 
One experimental ATCT AR system uses a realtime interface to a Traffic Approach Control (TRACON) ATC, 
using flightplan information (e.g. calllsign, aircraft type, equipage) to create labeled three-dimensional 
representations of the aircraft, and radar data (e.g. altitude, speed, heading, position) to create key-frames 
that form the basis for realtime animated presentation. The TRACON radar updates every 4.8 seconds, and 
the AR system creates fill-frames to achieve a 50 Hz (or greater) animated display of aircraft behavior. 
This AR system possesses high fidelity head-position tracking, a high transmisivity combiner, adjustable 
binocular overlap, optical adjustments, and a bright virtual image (50-100 cd/m2). Particular attention 
has been paid to developing the brightness so that the AR system may perform adequately under realistic 
ATCT daylight conditions. 3. Human Factors The NASA AR system was used for human factors experimentation 
to determine that a total field of view of 46° and partial convergence overlap of between 15 and 46% 
are sufficient for ATCT AR applications. Surface Data Collection Control Center Surface Controllers 
View with CGI Image 4. Tracking Surface Vehicles NASA contracted Seagull Technology to construct an 
AR system that tracked a vehicle using a commercial Automatic Dependent Surveillance-Broadcast (ADS-B) 
system, demonstrating the suitability of using AR in an ATCT to track surface movements of aircraft and 
other vehicles at airports. A predictor-smoother algorithm was used to create fill­frames for the 60 
Hz display, based on the 1 second updates from the ADS-B. Seagull also demonstrated the efficacy of using 
low-cost commercial-off-the-shelf components to implement their AR head mounted display system. 5. Next 
Steps The next logical steps involve incorporating the Seagull Technology surface-tracking capability 
into the NASA AR system, and field-testing the system at an actual ATCT. 6. References Schmidt-Ott, J., 
Ellis, S.R., Krozel, J., Reisman, R., &#38; Gips, J. (2002) Augmented Reality in a Simulated Tower Environment: 
Effect of Field of View on Aircraft Detection. NASA TM-2002­211853 (October, 2002) Copyright held by 
the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965430</section_id>
		<sort_key>7</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Level of detail]]></section_title>
		<section_page_from>7</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653918</person_id>
				<author_profile_id><![CDATA[81100457004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elendt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software Inc.]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965434</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[GLOD]]></title>
		<subtitle><![CDATA[a driver-level interface for geometric level of detail]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965434</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965434</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP43137948</person_id>
				<author_profile_id><![CDATA[81452612087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johns Hopkins University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14056692</person_id>
				<author_profile_id><![CDATA[81100131290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luebke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653924</person_id>
				<author_profile_id><![CDATA[81100047495]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Nathaniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duca]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johns Hopkins University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P437431</person_id>
				<author_profile_id><![CDATA[81100295748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Brenden]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schubert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Virginia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>863276</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Luebke, D., M. Reddy, J. Cohen, A. varshney, B. Watson, and R. Huebner. Level of Detail for 3D Graphics. Morgan Kaufman. 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192262</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rohlf J. and J. Helman. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D Graphics. Proceedings of SIGGRAPH 94. July 24--29. pp. 381--395.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GLOD: A Driver-Level Interface for Geometric Level of Detail *+* + Jonathan Cohen David LuebkeNathaniel 
Duca Brenden Schubert *Johns Hopkins University +University of Virginia 1 INTRODUCTION Level of detail 
(LOD) techniques are widely used today among interactive 3D graphics applications, such as CAD design, 
scientific visualization, virtual environments, and gaming, allowing applications to trade off visual 
fidelity for interactive performance. Many excellent algorithms exist for LOD genera­tion as well as 
for LOD management [Luebke 2003]. However, no widely accepted programming model has emerged as a standard 
for incorporating LOD into programs. Existing tools generally fall into two categories: mesh simplifi­ers 
and scene graph toolkits. Mesh simplifiers address the LOD generation problem, taking a complex object 
and producing simpler LODs, but they do not attempt to address LOD manage­ment at all. Scene graphs such 
as OpenGL Performer [Rohlf 1994] perform LOD management, but go to the opposite extreme; they provide 
heavyweight all or nothing solutions that lump LOD in with myriad other aspects of an interactive computer 
graphics system, constraining the form of the overall application. In this sketch we present GLOD, a 
tool for geometric level of detail that provides a full LOD pipeline in a lightweight and flexible application 
programmer s interface (API). This API is a powerful, extendible, yet easy-to-use LOD system, supporting 
discrete, continuous, and view-dependent LOD, multiple simplifi­cation algorithms, and multiple adaptation 
modes. GLOD is not a scene graph system; instead, it is an API integrated with OpenGL, an existing and 
popular low-level rendering API. With this formulation, we start to think of geometric level of detail 
as a fundamental component of the graphics pipeline, much like mip­mapping is a fundamental component 
for controlling detail of texture images.  2 GLOD API Our design goals for the GLOD API focus on providing 
a light­weight model for the creation, management, and rendering of geometry. To maximize its appeal 
to multiple audiences, GLOD should be fast, extensible to different LOD algorithms, and easy to integrate 
into existing applications. Furthermore, it should allow incremental adoption rather than locking developers 
into all pieces of the GLOD framework. To accomplish these goals, GLOD API is tightly integrated with 
the industry standard OpenGL API, so our design decisions are guided as if GLOD were a component of OpenGL. 
The data handled by GLOD is organized into three principal units: patches, objects, and groups. A patch 
is the principal unit of rendering. A patch is specified to GLOD using the OpenGL vertex array interface. 
Drawing a patch is much like drawing a vertex array, the chief difference being that what you get is 
an LOD of the original arrays. The application may change rendering state, such as bound textures, on 
a per-patch basis at the time of rendering; GLOD does not interfere with rendering state. An object is 
the principal unit of LOD generation. The applica­tion designates one or more patches as an object before 
initiating the LOD generation process. Thus multiple patches may be simplified together into crack-free 
levels of detail. GLOD also supports memory-efficient instancing of objects to provide efficient LOD 
management for applications which render objects in multiple locations. A group is the principal unit 
of LOD management. An applica­tion places one or more objects into a group. At each frame, GLOD adapts 
the LOD of all patches of all objects in each group according to the specified adaptation mode and current 
OpenGL viewing matrices.  The GLOD pipeline is designed to allow flexible motion of data into and out 
of it as desired by the application, as illustrated above. The original geometry is specified as patches 
using the vertex array mechanism. The application can then set a number of per-patch and per-object LOD 
generation parameters to determine how the LOD hierarchy is constructed. For example, parameters may 
be used to select a simplification operator, error metric, hierarchy type (e.g. discrete, continuous, 
view-dependent), importance values, etc. A special hierarchy type allows the programmer to manually build 
discrete hierarchies from a set of existing LODs. An entire hierarchy may be read back by the application 
to save it to disk, allowing it to be re-used in a later execution without regenerating it. Group parameters 
specify management modes such as the error mode (object-space or screen-space), adaptation mode (error 
threshold or triangle budget), morphing parameters, etc. After adapting a group, the individual adapted 
patches may be read back, again through the vertex array mechanism. The application can store these vertex 
arrays, pass them to OpenGL for rendering, etc. This complete set of data paths allows applications to 
incrementally adopt GLOD.  3 DISCUSSION We have currently limited the scope of GLOD to filtering geo­metric 
detail without interfering with rendering state. This has several benefits. The application may safely 
employ complex rendering algorithms, including multi-pass algorithms, as well as custom vertex and fragment 
programs. For example, applications can use normal mapped LODs without difficulty in GLOD. Many user-defined 
vertex program parameters can pass through GLOD filtering. However, this is not applicable for all vertex 
programs. Also, our non-interference policy makes some forms of LODs, such as textured impostors, difficult 
to support because they require us to change rendering state. This system will ultimately become an open 
source system to encourage level of detail research to migrate from the research lab to full deployment. 
With a wide array of simplification algo­rithms, hierarchical data representations, and management policies 
in their hands, all available through the setting of a few parameters, application developers will have 
tremendous power to select the implementations that meet their needs. REFERENCES Luebke, D., M. Reddy, 
J. Cohen, A. Varshney, B. Watson, and R. Huebner. Level of Detail for 3D Graphics. Morgan Kaufman. 2003. 
Rohlf, J. and J. Helman. IRIS Performer: A High Performance Multiprocessing Toolkit for Real-Time 3D 
Graphics. Proceed­ ings of SIGGRAPH 94. July 24-29. pp. 381-395. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965433</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A fast hybrid geomorphing LOD scheme]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965433</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965433</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653913</person_id>
				<author_profile_id><![CDATA[81100484623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Louis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borgeat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council of Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653937</person_id>
				<author_profile_id><![CDATA[81538144656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pierre-Alexandre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fortin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Laval University, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40027892</person_id>
				<author_profile_id><![CDATA[81100494822]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Guy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Godin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Research Council of Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>288280</ref_obj_id>
				<ref_obj_pid>288216</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[GARLAND, M., AND HECKBERT, P. S. 1998. Simplifying surfaces with color and texture using quadric error metrics. In Proceedings of the conference on Visualization '98, IEEE Computer Society Press, 263--269.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258843</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HOPPE, H. 1997. View-dependent refinement of progressive meshes. Computer Graphics 31, Annual Conference Series, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A fast hybrid geomorphing LOD scheme Louis Borgeat* Pierre-Alexandre Fortin Guy Godin National Research 
Council of Canada Laval University, Canada National Research Council of Canada We present a new technique 
for fast, view-dependent, real-time visualization of large multiresolution geometric models with color 
or texture information. This method combines the rendering speed of optimized static models, the smooth 
transitions associated with geomorphing, and the viewpoint adaptivity of iterative vertex con­traction 
based methods. 1 System overview A signi.cant amount of recent work on real-time multiresolution rendering 
of triangulated geometric models is built on the concept of iterative edge contraction(e.g. [Hoppe 1997]). 
The system we have developed is a hybrid method that integrates aspects of this kind of approach and 
of more classical discrete Level of Detail (LOD) techniques. Our goal was to produce a multiresolution 
ren­dering system that would produce minimal visual artifacts when rendering high resolution scene or 
object datasets produced from sensor data while maintaining the same graphic performance we get with 
optimized static versions of those models. The .rst step of the method is to partition the triangular 
mesh into a set of groups. The model is then globally decimated into a se­ries of discrete LODs using 
an algorithm based on vertex pair con­traction. Each discrete level is repartitioned along the same bound­aries. 
Groups are shaped based on criteria such as compactness, orientation, texture, and desired granularity. 
At run-time, LOD levels and geomorph ratios between selected levels are computed for each group of the 
model, these groups are then rendered as precomputed triangle strips. Border points be­tween groups are 
geomorphed in order to maintain seamless conti­nuity between neighboring LOD groups at all time. To achieve 
this, we maintain a connexity graph between the various groups and ap­ply a set of simple rules when 
morphing these points. Morphing is done for space and texture coordinates, normals and color. Adjusting 
the granularity of the multiresolution rendering through this grouping process provides us with multiple 
advan­tages. First, it allows us to extend smooth geomorphs over nu­merous frames by morphing more triangles 
over a longer distance rather than expanding/contracting the model by a few polygons ev­ery frame. Secondly, 
each group having a static topology, it can be ef.ciently rendered as a set of pre-computed triangle 
strips. These groups can also serve as the units of an anticipative paging algo­rithm for larger models. 
Finally, groups can be shaped so that they correspond to rectangular texture images, therefore optimizing 
tex­ture memory usage and smoothing texture swapping for high reso­lution models. 2 Results Our current 
implementation uses quadric error metrics[Garland and Heckbert 1998] to decimate the models and SGI s 
OpenGL Per­former as the base of the run-time infrastructure. The multireso­lution processing is done 
asynchronously on a different CPU than the one used for rendering. We tested our method on various col­ored 
or textured models produced using range sensors and digital * e-mail: louis.borgeat@nrc-cnrc.gc.ca photographs. 
Our test system was a 1.4 GHz dual Athlon PC with a GeForce 4 graphic card. The .gure shows a slanted 
view of a rectangular cuneiform tablet to illustrate the similar pixel size of the triangles at different 
distances from the observer. The model is textured and varies in size from 25k to 2.5M polygons. The 
tablet, scanned using the National Research Council s color range sensor, was provided by Prof. Marc 
Levoy of Stanford University while it was on loan from Renee Kovacs. During those tests, the time required 
to perform the LOD selection and the morphing of all visible triangles (geometry, color/texture and normals) 
on one of the CPUs amounted to roughly 30% of the time required to render the corresponding frame, al­lowing 
free cycles for other processing. The number of rendered polygons per second was the same for a static 
model loaded with Performer s viewer and for its multiresolution counterpart loaded with our application. 
This con.rms that there is no graphic penality associated with the multiresolution processing, therefore 
allowing a maximal frame rate increase. Also, as expected, this method pro­duces almost unnoticeable 
transformations during LOD morphing, and produces especially good results with textured data. We are 
currently implementing the anticipative paging and com­pression strategy that will allow navigation through 
larger models. We are also pursuing work on automatic segmentation of the mod­els, on recursive subdivision 
of the groups, and we are implement­ing more advanced LOD selection criteria based on both resolution 
and frame rate constraints. References GARLAND, M., AND HECKBERT, P. S. 1998. Simplifying sur­faces with 
color and texture using quadric error metrics. In Pro­ceedings of the conference on Visualization 98, 
IEEE Computer Society Press, 263 269. HOPPE, H. 1997. View-dependent re.nement of progressive meshes. 
Computer Graphics 31, Annual Conference Series, 189 198. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965431</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[LOD of video avatar for walkthrough applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965431</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965431</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653897</person_id>
				<author_profile_id><![CDATA[81319502900]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Utsugi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Systems Development Laboratory, Hitachi, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14219170</person_id>
				<author_profile_id><![CDATA[81100638212]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moriya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Systems Development Laboratory, Hitachi, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14054625</person_id>
				<author_profile_id><![CDATA[81100125084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nonaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Systems Development Laboratory, Hitachi, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15021455</person_id>
				<author_profile_id><![CDATA[81100082668]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[H.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Systems Development Laboratory, Hitachi, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1242251</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[PRINCE, S., CHEOK, A. D., FARBIZ, F., WILLIAMSON, T., JOHNSON, N., BILLINGHURST, M., AND KATO, H. 2002. Real-Time 3D Interaction for Augmented and Virtual Reality. In Conference Abstracts and Application of SIGGRAPH 2002, ACM Press / ACM SIGGRAPH, 238.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[TAMAGAWA, K., YAMADA, T., OGI, T., AND HIROSE, M. 2001. Developing a 2.5D Video Avatar. IEEE Signal Processing Magazine 18, 3 (May), 35--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[UTSUGI, K., BENIYAMA, F., NAMAI, H., MORIYA, T., AND TAKADA, H. 2002. A High-Resolution Video Avatar System Using Partial Composition. In Proceedings of ICAT2002, VRSJ, 65--71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LOD of Video Avatar for Walkthrough Applications K. Utsugi, T. Moriya, N. Nonaka, H. Takeda Systems 
Development Laboratory, Hitachi, Ltd. {utsugi,moriya,nonaka,takeda}@sdl.hitachi.co.jp 1 Introduction 
We are developing an augmented reality (AR) application, called video avatar, in which real-time video 
streaming is used for visual characters (avatars) in a 3D virtual environment. This sketch de­scribes 
the dynamic quality control of video streaming according to the level of detail (LOD) requirements for 
walkthrough applica­tions. 2 Locomotion and LOD One of the problems with video avatars is converting 
video images to provide consistent positioning and angles in a virtual environ­ment [Tamagawa et al. 
2001; Prince et al. 2002]. For a walking avatar, an impractically huge studio would be required to capture 
images of a user s actual locomotion. In our previous work [Utsugi et al. 2002], to simplify the locomotion 
of the avatar, we com­bined polygon legs and video images of the upper body captured by multiple cameras 
(Figure 1). This combination avatar satis.es requirements for natural appearance in a walkthrough application 
and locomotes irrespective of the actual size of the studio. We can also prevent a network overload if 
we select a suitable streaming size according to the network capacity. The locomotion of the video avatars 
suggested to us that the importance of each video stream varied dynamically according to the observer 
s view­ing position. We used this .exibility in terms of the LOD of those avatars in order to control 
the network load. 3 Network Load Control By changing the image size and frame rate of the video streams, 
we can control the network load. Our video avatar system dynamically selects a suitable LOD for each 
avatar according to bottlenecks in the network environment and the distances among avatars in a vir­tual 
scene (Figure 2). To select an appropriate LOD, we need information about cur­rent network conditions 
and the state of each avatar. This feedback information and command messages to control each avatar s 
loco­motion are transferred with higher priority than video data by a prioritized network. In the prioritized 
network, multiple data.ows are routed according to the priority requirements of various appli­cations. 
A bottleneck in the network may decrease the data rate for receiving video compared with that for sending 
video. When the network data rate for the receiving system is inadequate for whole avatar images, the 
sending systems cooperate to select a smaller im­age and lower frame rate, taking into account the image 
size of the avatar in a display. For each avatar i, we can select the avatar frame rate fi and the image 
size of the video stream Si. The system con­tinually searches for the optical parameters { fi,Si} to 
maximize .i min( fi, f )·min(Si,Si.) , where f represents the viewer s scene re­fresh rate and Si . represents 
the size of the image area on the viewer s display. This is evaluated based on the feedback information 
be­cause it depends on nontrivial network conditions. Data type Priority Size Avatar control command 
High Small Network capacity data Medium Small Image data Low Large (.exible) Table 1: Data types and 
priorities Figure 2: Network load control. When the network capacity of the viewer s system is inadequate 
for video images from Capture 1 and 2, the avatar farthest from the viewing point switches to a lower 
LOD and frame rate. References PRINCE, S., CHEOK, A. D., FARBIZ, F., WILLIAMSON, T., JOHNSON, N., BILLINGHURST, 
M., AND KATO, H. 2002. Real-Time 3D Inter­action for Augmented and Virtual Reality. In Conference Abstracts 
and Application of SIGGRAPH 2002, ACM Press / ACM SIGGRAPH, 238. TAMAGAWA, K., YAMADA, T., OGI, T., AND 
HIROSE, M. 2001. De­veloping a 2.5D Video Avatar. IEEE Signal Processing Magazine 18,3 (May), 35 42. 
UTSUGI, K., BENIYAMA, F., NAMAI, H., MORIYA, T., AND TAKADA, H. 2002. A High-Resolution Video Avatar 
System Using Partial Com­position. In Proceedings of ICAT2002, VRSJ, 65 71. Acknowledgements This study 
was supported by the Telecommunications Advance­ment Organization of Japan (TAO). Copyright held by the 
author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965432</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[From layered depth images to continuous LOD impostors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965432</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965432</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653883</person_id>
				<author_profile_id><![CDATA[81100362705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[Andreas]]></middle_name>
				<last_name><![CDATA[B&#230;rentzen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14106349</person_id>
				<author_profile_id><![CDATA[81100285054]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Niels]]></first_name>
				<middle_name><![CDATA[J&#248;rgen]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EVERITT, C., 1999. Interactive order-independent transparency. NVIDIA OpenGL Applications Engineering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 From Layered Depth Images to Continuous LOD Impostors J. Andreas Bærentzen * Niels Jørgen Christensen 
 Technical University of Denmark Technical University of Denmark In this sketch we propose the use of 
point based impostors. A technique called depth peeling is used to generate a Layered Depth Image (LDI) 
representation of an object. From the LDIs we extract a set of points. We show how a random ordering 
of these points can be used as a continuous LOD representation. In fact, the random permutation of points 
seems to be the simplest conceivable contin­uous LOD representation. Creating Point Impostors Using Graphics 
HW In real-time rendering, we do not have to render objects in correct depth order because modern graphics 
cards employ a z-buffer to perform depth sorting. This makes it very easy to render the closest surfaces 
in a scene, but how do we render the surfaces immediately behind the closest (i.e. the second layer)? 
Provided we have two z-buffers this is almost as simple. Having rendered the .rst layer, we copy the 
normal z-buffer to our second z-buffer and render again. During this rendering the 1st z-buffer is used 
(as usual) to reject a fragment if its z value is greater than the stored z value. The 2nd z-buffer is 
used to reject a fragment if its z value is smaller or equal to the stored value. Finally, the 1st z-buffer 
is updated as usual with the depth of fragments that pass the depth test whereas second z-buffer is read-only. 
After rendering, the 1st z-buffer is copied to the 2nd z-buffer, and we are ready to render the third 
layer. Simply iterating this process until nothing is rendered produces all layers in the scene. For 
each layer we obtain a depth buffer and at least one color buffer (since it is possible to render each 
layer multiple times). The only problem is that graphics cards generally do not have more than one z-buffer. 
However, this problem can be overcome using extensions for shadow mapping which, essentially, provide 
an extra z-buffer using texturing. This method has been used by Cass Everitt to implement depth peeling, 
and details regarding the implementation are provided in [Everitt 1999]. Everitt used depth peeling as 
a part of a scheme for order-independent transparency. Our goal here is to create an impostor as explained 
below. To generate a point impostor representation, we compute three LDI representations using the above 
method. Each LDI corre­sponds to a viewing direction parallel to one of the X, Y, and Z axes. Two color 
buffers are generated for each layer. The .rst buffer actu­ally contains color, and the second contains 
normals (stored as RGB values). For each viewing direction and for each (non-background) pixel in each 
layer, we test whether the normal is most parallel to the given viewing direction. If that is the case, 
we compute the 3D position of the pixel from its xy position and depth. Finally, we add the <color, normal, 
position> triple to a vector of points. This technique has a number of virtues: Being hardware accelerated, 
depth peeling is likely to be faster than any software based technique for LDI generation. The impostor 
for the tree shown in Figure 1 consists of 70000 points and takes about a second to generate. Any layer 
can be rendered any number of times. Thus we can ex­tract any number of attributes for each point. It 
is possible to generate an impostor from an object rendered by a function from a closed source API. *e-mail: 
jab@imm.dtu.dk e-mail: njc@imm.dtu.dk Figure 1: The trees (and their re.ections) in this scene were 
ren­dered using our method. The inset shows a two times magni.cation of a distant tree. Impostor Rendering 
When all pixels have been processed, we select a random permuta­tion of the N points. This is our impostor 
representation. Since the points are in random order, the .rst k (for 0 « k < N) points can be expected 
to cover the object reasonably well. We use a quadratic at­tenuation function k = f (dist) to select 
a continuous level of detail, and instead of rendering the original object, we simply render the .rst 
k points. Points are rendered using the OpenGL point primitive with a point size of 1. For a certain 
value of k it is no longer faster to render the points. At that value we switch from the impostor to 
the original object. Results and Discussion The method has proven to be an extremely simple, yet effective 
technique for generating and rendering impostors for complex ge­ometry such as trees. The scene shown 
in Figure 1 contains 200 trees and their re­.ections. The re.ected trees are rendered in the same way 
as the normal trees except that the maximum value of k is clamped. Us­ing our method, the scene typically 
renders at around 25 frames per second on an NVIDIA Quadro 4 900 XGL. It is possible to go up to around 
2000 trees without loosing interactivity. Clearly holes may appear when rendering a random set of points, 
but if a suitable attenuation function is used, this causes few visual artifacts. More importantly, there 
is no popping, since the Level of Detail changes continuously.  References EVERITT, C., 1999. Interactive 
order-independent transparency. NVIDIA OpenGL Applications Engineering. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965435</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Per-pixel smooth shader level of detail]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965435</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965435</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP42053034</person_id>
				<author_profile_id><![CDATA[81341496532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Maryann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simmons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48025999</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>553838</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AKENINE-MOLLER, T., AND HAINES, E. 2002. Real-time Rendering, second ed. AK Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[OLANO, M., AND KUEHNE, B. 2002. SGI OpenGL Shader level-of-detail shader white paper. Tech. rep., SGI.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344976</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PEERCY, M. S., OLANO, M., AIREY, J., AND UNGAR, P. J. 2000. Interactive multi-pass programmable shading. In Proceedings of SIGGRAPH 2000, Computer Graphics Proceedings, Annual Conference Series, ACM, 425--432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965436</section_id>
		<sort_key>8</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Social networks]]></section_title>
		<section_page_from>8</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42058090</person_id>
				<author_profile_id><![CDATA[81341497557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tribe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhizome.org]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965437</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Social network fragments]]></title>
		<subtitle><![CDATA[an interactive tool for exploring digital social connections]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965437</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965437</url>
		<abstract>
			<par><![CDATA[We present a novel application for interactively visualizing the interpersonal networks that emerge during email interactions. While people have complex email interrelationships, no previous tools allow examining one's overall network.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15022433</person_id>
				<author_profile_id><![CDATA[81100111861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Danah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boyd]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[V-Day]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P652064</person_id>
				<author_profile_id><![CDATA[81100200791]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeffrey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Potter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Atof Inc]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Social Network Fragments (smg.media.mit.edu/projects/SNF/) emerged at MIT's Media Lab, Sociable Media Group {BOYD 2002} in collaboration with Atof Inc as an extension of BuddyGraph (www.buddygraph.com).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BOYD, D. 2002. Faceted Id/entity: Managing Representation in a Digital World. MIT Master's Thesis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>215255</ref_obj_id>
				<ref_obj_pid>217279</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DONATH, J. 1995. Visual Who. Proceedings of ACM Multimedia '95, Nov. 5--9, San Francisco CA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Social Network Fragments: An Interactive Tool for Exploring Digital Social Connections danah boyd Jeffrey 
Potter V-Day - zephoria@acm.org Atof Inc - jpotter@buddygraph.com Abstract We present a novel application 
for interactively visualizing the interpersonal networks that emerge during email interactions. While 
people have complex email interrelationships, no previous tools allow examining one's overall network. 
 Figure 1. Interactive application view with history panel. 1 Introduction Social networks define our 
relationship to others in society. Such connections help us define social context, which in turn affects 
the roles we embody. In online environments, the ability to derive context is challenged by the limited 
salient characteristics that are available. Email appears the same regardless of whom we are contacting, 
making it difficult to properly negotiate the social roles in which we intend to present. While social 
awareness is key to interaction, few tools exist to make these patterns available. Social Network Fragments 
is an interactive visualization tool that allows users to navigate the social network patterns that emerge 
through regular email conversations. By analyzing the sender and recipient relationships, we are able 
to visually present the graph of interpersonal connections and their resultant clusters. We use a graph 
visualization style that has previously proven effective in a similar domain [DONATH 1995]. This application 
offers a novel lens through which individuals reflect upon their digital habits and the data that they 
produce. 2 Exposition Derived from one person s email archive, Social Network Fragments reveals the 
relationships between hundreds of people. Connections are uncovered by analyzing the recipients of a 
given message. By assuming social awareness between the people associated with a message, albeit with 
different strength depending on role and number of recipients, we create a highly dimensional graph of 
the social relationships. Using a spring and repulsion system to visually position the graph, clusters 
of tightly connected individuals separate themselves from the masses. Color and font weight reveal elements 
of the relationships, including the shared social role and strength of connection between the subject 
and the individual. Animation shows how relationships emerge over time and a history panel reveals the 
quantitative email patterns such as frequency and quantity of messages. Users can interact with the system 
to explore individual relationships and clusters over time. Such an interactive application is particularly 
valuable to the subject for behavioral self-awareness. Familiarity with the data allows one to properly 
adjust for outliers and understand the reasons why logically unrelated individuals are socially connected 
in one s network. In our initial experiences using and sharing this tool, we recognize its effectiveness 
in revealing otherwise hidden patterns and connections, many of which are indicative of unconscious convergence 
of social clusters. Such awareness allows people to monitor their behavior to more appropriately fit 
the social situation. Figure 2. Data cluster and link close-up view.  References Social Network Fragments 
(smg.media.mit.edu/projects/SNF/) emerged at MIT s Media Lab, Sociable Media Group [BOYD 2002] in collaboration 
with Atof Inc as an extension of BuddyGraph (www.buddygraph.com). BOYD, D. 2002. Faceted Id/entity: Managing 
Representation in a Digital World. MIT Master s Thesis. DONATH, J. 1995. Visual Who. Proceedings of ACM 
Multimedia 95, Nov. 5-9, San Francisco CA. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965440</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Processing]]></title>
		<subtitle><![CDATA[a learning environment for creating interactive Web graphics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965440</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965440</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37027875</person_id>
				<author_profile_id><![CDATA[81100463130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Casey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Interaction Design Institute Ivrea, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38026245</person_id>
				<author_profile_id><![CDATA[81100613376]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Benjamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fry]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Processing: A Learning Environment for Creating Interactive Web Graphics Casey Reas Interaction Design 
Institute Ivrea, Italy c.reas@interaction-ivrea.it Introduction The Processing project introduces a 
new audience to computer programming and encourages an audience of hybrid artist/ designer/programmers. 
It integrates a programming language, development environment, and teaching methodology into a unified 
structure for learning. Its goal is to introduce program­ming in the context of electronic art and to 
open electronic art concepts to a programming audience. Unlike other popular web programming environments 
such as Flash and Director, Pro­cessing is an extension of Java and supports many of the exist­ing Java 
structures, but with a simplified syntax. The application runs locally and exports programs to Java applets, 
which may be viewed over the Internet. It is not a commercial production tool, but is build specifically 
for learning and prototyping. Concept Graphical user interfaces became mainstream nearly twenty years 
ago, but programming fundamentals are still primarily taught through the command line interface. Classes 
proceed from outputting text to the screen, to GUI, to computer graph­ics (if at all). It is possible 
to teach programming in a way that moves graphics and concepts of interaction closer to the surface. 
Making exercises created during learning viewable over the web supports the creation of a global educational 
community and provides motivation for learning. A view source method of programming enables the community 
to learn from each other. The concept of Processing is to create a text programming language specifically 
for making responsive images, rather than creating a visual programming language. The language enables 
sophisticated visual and responsive structures and has a bal­ance between features and ease of use. Many 
computer graphics and interaction techniques can be discussed including vector/ raster drawing, 2D/3D 
transformations, image processing, color models, events, network communication, information visualiza­tion, 
etc. Processing shifts the focus of programming away from technical details like threading and double-buffering 
and places emphasis on communication. Benjamin Fry MIT Media Laboratory fry@media.mit.edu  Programming 
Language/Environment Processing is a Java environment which translates programs written in its own syntax 
into Java code and then compiles to executable Java Applet 1.1 byte code. It includes a custom 2D/3D 
engine inspired by PostScript and OpenGL. The software is free to use and the source code will be made 
public. It runs on Windows, Mac OS X, Mac OS 9, and Linux and the software is currently in Alpha release. 
The Beta release is scheduled for Summer 2003. Processing Version 1.0 focuses on teaching basic concepts 
of interactive networked computer graphics. Processing provides three different modes of program­ming 
each one more structurally complex than the previous. In the most basic mode, programs are single line 
commands for drawing primitive shapes to the screen. In the most complex mode, Java code may be written 
within the environment. The intermediate mode allows for the creation of dynamic software in a hybrid 
procedural/object-oriented structure. It strives to achieve a balance between features and clarity, which 
encourages the experimentation process and reduces the learning curve. Skills learned through Processing 
enable people to learn lan­guages suitable for different contexts including web authoring (ActionScript), 
networking and communications (Java), micro­controllers (C), and computer graphics (OpenGL).  Networked 
Learning The Processing website houses a set of extended examples and a complete reference for the language. 
Hundreds of students, educators, and practitioners across five continents are involved in using the software. 
An active online discussion board is a platform for discussing individual programs and future soft­ware 
additions to the project. The software has been used at diverse universities and institutions in cities 
including: Boston, New York, San Fransisco, London, Paris, Oslo, Basel, Brussels, Berlin, Bogota (Colombia), 
Ivrea (Italy), Manila, and Tokyo. Figure 1: Example images created with Processing. See also: http://www.proce55ing.net 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965439</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Creating dome animations with the Digital Pueblo Project]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965439</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965439</url>
		<abstract>
			<par><![CDATA[The Digital Pueblo Project at the University of New Mexico is building the infrastructure for a digital arts and animation industry in a largely rural state with many diverse cultural groups. Key to this effort is project-based training of an "eight to eighty" workforce drawn from the community. This presentation shows a successful effort to create fulldome animations for the LodeStar Astronomy Center using a collection of local artists, students, and computer scientists.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14130585</person_id>
				<author_profile_id><![CDATA[81100366265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653880</person_id>
				<author_profile_id><![CDATA[81100383851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hue]]></first_name>
				<middle_name><![CDATA[Walker]]></middle_name>
				<last_name><![CDATA[Baumgarner-Kirby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653856</person_id>
				<author_profile_id><![CDATA[81100282965]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beining]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[atc.unm.edu/digitalpueblo.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Dome Animations with the Digital Pueblo Project Ed Angel* Hue Walker Baumgarner-Kirby David 
Beining Arts Technology Center Arts Technology Center LodeStar Astronomy Center University of New Mexico 
University of New Mexico University of New Mexico Abstract The Digital Pueblo Project at the University 
of New Mexico is building the infrastructure for a digital arts and animation industry in a largely rural 
state with many diverse cultural groups. Key to this effort is project-based training of an eight to 
eighty workforce drawn from the community. This presentation shows a successful effort to create fulldome 
animations for the LodeStar Astronomy Center using a collection of local artists, students, and computer 
scientists. 1 Introduction New Mexico is a largely rural state with a world-renown artistic community 
based on three distinct cultures. Due to the national laboratories, the state also has the highest concentration 
of computing power in the world. Although it might seem that these characteristics are ideal for an animation/graphics/game 
technology industry, the reality is somewhat different. The Digital Pueblo Project is an NSF sponsored 
Partnership for Innovation project at the University of New Mexico that is seeking to build the infrastructure 
for such an industry in a manner that can include groups around the state, included those in Native American 
pueblos and in the small rural towns. We are establishing technology pods (T-pods) around the state that 
will allow for training and projects. We are using networking technologies, such as the NSF Access Grid, 
to allow collaboration among participants. Participants are trained for specific projects, most of which 
are proposed by partners (industrial, cultural, governmental) in the effort. We have an open door policy 
towards who might participate in projects. As participants develop their skills, they can move from students 
to mentors, from mentors to instructors, and even to project leaders. As we started this effort, it was 
assumed but unproven, that we could take a group of untrained people and get to the point where they 
could produce professional quality animations. In the rest of this paper, we will describe one project 
with the LodeStar Astronomy Center. Lodestar is one of the first all digital domed environments. We have 
placed a T-pod at Lodestar and are creating animations, through a sophisticated multi-step process that 
are receiving much attention 2 Producing for the Dome All large digital dome theaters use multiple video 
imagery to tile a dome with a single digital image. The vast majority of theaters use commercial personal 
computers (PCs) to stream synchronized video clips to each projector. However, there is a series of challenging 
processing and rendering steps to provide the dome projection. *e-mail: angel@cs.unm.edu e-mail: huebk@unm.edu 
e-mail: DBeining@state.nm.us We have used the following process to develop three-dimensional content 
for use both with astronomical content and for artistic productions. Creating animations with sufficient 
resolution, lack of geometric distortion, and sufficient field-of-view for the dome requires using multiple 
virtual cameras within a 3D animation software program such as Maya or LightWave . We create a cube of 
images using five virtual cameras representing the top, front, left, right, and back sides of a cube. 
The result is five panels that, when stitched together, form one frame ina domemaster sequence. It is 
these domemasters sequences at resolution between 2400x2400 and 3600x3600 that are sliced for projection 
at 30 frames-per-second through the multiple CRT projectors. The rendering task is carried out on large 
clusters. The standard configuration is six-channels, five trapezoidal images along on the dome s edge 
and one tetrahedron at the zenith although the domemasters can be sliced for other projector configurations. 
In sum, creating one minute (1800 frames at 30fps) of fulldome animation requires rendering 9,000 frames 
( panels ) for the five virtual cameras, stitching the panels into 1800 domemasters and slicing these 
files into six video clips for playback. Since opening, LodeStar has has opened its facility to a diverse 
group of animators, artists and scientists creating immersive imagery from artistic, educational and 
scientific points of view through the Digital Pueblo Project. The result has been the development of 
a culture of immersive theater production and terabytes of data representing experiments in what and 
how the dome can be used to communicate and educate. More than 100 students and artists have learned 
the production process at Lodestar while creating more than two hours of immersive imagery from jellyfish 
swimming with the audience, to depictions of fusion in a stellar core, to fulldome art. At the present, 
we are running weekly classes in which the participants, range from high school students to retirees. 
 3 Conclusions Our productions were presented recently at the meeting of the International Planetarium 
Society and the meeting of the Astrophysical Society, where they were acclaimed as demonstrating the 
potential of domes for other than astronomical purposes and showed that professional quality results 
from a complex rendering process can indeed be accomplished using both amateurs and professionals. Reference: 
atc.unm.edu/digitalpueblo.html. Acknowledgements: This project was supported by the National Science 
Foundation under grant NSF HER-0227806. Participants in the creation of the images include Keith Baca, 
Marcos Baca, Joe Dean, Brian Hughes, Sheryl Hurley, and Chris Jordan Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965438</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[ViRdB]]></title>
		<subtitle><![CDATA[integrating virtual reality and multimedia databases for customized visualization of cultural heritage]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965438</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965438</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15026463</person_id>
				<author_profile_id><![CDATA[81100223605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mazzoleni]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#225; degli studi di Milano (Italy)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77038982</person_id>
				<author_profile_id><![CDATA[81409594658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[E.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bertino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#225; degli studi di Milano (Italy)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14174303</person_id>
				<author_profile_id><![CDATA[81100498987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Valtolina]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#225; degli studi di Milano (Italy)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP79026296</person_id>
				<author_profile_id><![CDATA[81339498938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[E.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ferrari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#225; dell'insubria, Como (Italy)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653833</person_id>
				<author_profile_id><![CDATA[81416593097]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boeri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Studio Boeri, Milano (Italy)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[B. Asmuss and Others. The LeMO Project-Development of an Internet Multimedia Information System of 20th Century German History: Aims and Results In Proc. International Cultural Heritage Informatics Meeting (ICHIM01), Milan, Italy.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Virtual Museum published in: Accomplishments in depth, periode 1993-1999, ed. Gerhard Eckel & Kosta Fostiropoulos, Institute for Media Communication, internal report, pp. 202--207, GMD, Sankt Augustin, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[http://www.vangoghmuseum.nl.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ViRdB: Integrating Virtual Reality and Multimedia Databases for Customized Visualization of Cultural 
Heritage* P. Mazzoleni, E. Bertino, S. Valtolina E. Ferrari C. Boeri § Universit´a degli studi di Milano 
(Italy) Universit´a dell insubria, Como (Italy) Studio Boeri, Milano (Italy) Introdution In recent years 
there has been a growing interest in the use of innovative computer graphic technologies to provide far 
better representations and user-friendly visualization of cultural her­itage and in general multimedia 
information. However, whereas a lot of effort has been devoted to create realistic reconstructions of 
relevant historical places or ad-hoc 3D tours of important museum exhibitions, not enough work has been 
carried out to enable dynamic interaction and customization for £nal-users. In this abstract we describe 
ViRdB a scalable system with the goal of providing a ¤exible and personalized environment for accessing 
multimedia contents. In particular, by using an innovative approach for querying a database within a 
VR environment, the user can retrieve and visualize all and only the information he/she is interested 
to explore (based on his/her interests, domain experiences, etc), and thus he/she has the freedom of 
fully customizing the visit. Motivations and related work The interest in publicizing their collections 
and the demand for attracting large public are leading needs for most cultural insti­tutions. Modern 
information and communication technologies have made available to such institutions a large variety of 
tools and techniques to address their communication and presentation requirements. Among the various 
alternatives today available, VR seems to be one of the most promising technology to bolster interest 
for heritage and to support mutual exchange of contents among institutions. Even though several projects 
have been started in this £eld, most of the museums still do not invest in virtual reality. One of the 
main drawbacks of existing proposals is the lack of ¤exibility which results in a limited support for 
user interactions. For instance, projects such as the LeMo[1], the Virtual Museum Design Environment 
project[2] -speci£cally developed for cave systems-or the virtual tour of Van-Gogh s museum[3] either 
do not allow user interaction at all, or they allow users to submit queries, but the query results are 
simply presented as HTML £les. ViRdB System The ViRdB system developed at the University of Milan aims 
at addressing the above mentioned shortcomings. Its goals are to provide suitable tools for managing 
queries entered from the vir­tual environment and for displaying back the results generating a customizable 
virtual tour according to user preferences. The overall architecture of the system is outlined in Figure 
1. In this abstract we overview the key aspects related to the presentation of the results, leaving the 
issues concerning query speci£cation and processing to the extended version of this paper. In ViRdB users 
can interact with the system in various moments during the tour: before starting it, to de£ne content 
and appearance; and during the tour itself, to obtain further information on the displayed artifacts. 
Among the preferences that can be speci£ed at the beginning the tour, the user has the possibility of 
choosing the appearance of the environment among different museum architectures, the type of tour (linear 
vs. not linear) and for how long his/her tour should last. We are investigating three different approaches 
to control the tour-duration: a rubber-band technique that gently pushes the e-mail: mazzoleni,bertino,valtolina@dico.unimi.it 
e-mail: elena.ferrari@uninsubria.it §e-mail chiaraboeri@tiscali.it  user from a room to the next one 
when time goes by; a time-based rollover among the artifacts visualized into the scenes; and, £nally, 
a query result pruning, based on information stored into the database. All the above user-stated preferences 
are integrated with information from the curator who, based on his/her knowl­edge, supplies the system 
with additional information useful to reconstruct historically correct tours. Hence, user preferences 
and query results are sent to the Scene Builder Engine that is in charge of generating the customized 
virtual world. The Scene Builder Engine loads from a scene repository both the rooms geometry and the 
virtual objects (frames, columns, etc.) used to wrap the query results before returning them to the user. 
According to the user preferences and based on number and type of information returned from the database, 
each scene is resized and dynamically £lled with works of arts. The £nal locations of each artifact into 
the scene and of each scene with respect to the tour are not pre-computed but again are dynamically calculated 
based on the area needed to display objects, the one available in each scene and the information from 
the user. Figure 1: Overall System Architecture  Results and Conclusion We have developed a £rst prototype 
of the system creating a customized virtual museum for querying and viewing paintings based on the new 
X3D standard. Our system has been applied to the development of a tour regarding the history of drama 
theaters in Italy. During a tour inside the reconstruction of a theater, the user is able to interact 
with the system, for querying and dynamically visualizing, based on his/her preferences, additional historically 
information from a database created by our experts. We foresee other applications of our system, namely 
in the £eld of archeology. References 1. B. Asmuss and Others. The LeMO Project-Development of an Internet 
Multimedia Information System of 20th Century German History: Aims and Results In Proc. International 
Cultural Heritage Informatics Meeting (ICHIM01), Milan, Italy. 2. Virtual Museum published in: Accomplishments 
in depth, periode 1993-1999, ed. Gerhard Eckel &#38; Kosta Fostiropoulos, Institute for Media Communication, 
internal report, pp. 202-207, GMD, Sankt Augustin, 1999. 3. http://www.vangoghmuseum.nl. * This project 
is partially funded by EU under the grant IST 2001-33476 Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965441</section_id>
		<sort_key>9</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Surface reconstruction]]></section_title>
		<section_page_from>9</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P590095</person_id>
				<author_profile_id><![CDATA[81320488260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ioana]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Boier-Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T.J. Watson Research Center]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965443</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Non convex mesh penetration distance for rigid body dynamics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965443</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965443</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14194797</person_id>
				<author_profile_id><![CDATA[81100561218]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teichmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CMLabs Simulations Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653973</person_id>
				<author_profile_id><![CDATA[81100529162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhaoheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[&#201;cole de Technologie Sup&#180;erieure, Universite&#180; du Qu&#233;bec]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EHMANN, S. A., AND LIN, M. C. 2001. Accurate and fast proximity queries between polyhedra using convex surface decomposition. In EUROGRAPHICS, vol. 20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237244</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GOTTSCHALK, S., LIN, M. C., AND MANOCHA, D. 1996. OBB-tree: A hierarchical structure for rapid interference detection. Comput. Graph. 30, 171--180. Proc. SIGGRAPH '96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[LARSEN, E., GOTTSCHALK, S., LIN, M. C., AND MANOCHA, D. 2000. Fast proximity queries with swept sphere volumes. In Intl Conf. Robotics and Automation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[PAETH, A. W., Ed. 1995. Graphics Gems V. Academic Press, Boston, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non Convex Mesh Penetration Distance for Rigid Body Dynamics Marek Teichmann* Zhaoheng Liu CMLabs Simulations 
Inc. Ecole de Technologie Sup´erieure, Universit´e du Qu´ebec ´ 1 Introduction We show that the framework 
of [Ehmann and Lin 2001] for .nd­ing the minimum distance and detecting collision between meshes generalizes 
to a large class of distance-like functions. As an appli­cation, we describe a fast new algorithms for 
computing the pen­etration distance along a direction between two arbitrary meshes. Penetration distance 
is important for computing collision response for rigid body dynamics solvers. Finally, we describe a 
simple way to .nd a good direction of penetration: the gradient of the pene­tration volume between the 
meshes. We give a simple algorithm to obtain this direction which requires only the intersection curve 
between the meshes. 2 Minimization framework The penetration distance algorithm assumes a mesh represented 
as a bounding volume tree with triangles at the leaves. The algorith­mic framework of [Ehmann and Lin 
2001] is applied to collision detection, tolerance and distance queries between two meshes, with convex 
bounding volumes. In each case, in fact, a certain function F(a,b) is being minimized over all pairs 
of points a,b, one taken from each mesh. The function whose minimum is being sought is a 0-1 function 
in the .rst two cases and the Euclidean distance in the last case. This framework can also be applied 
to time of impact calculations. The bounding volumes can also be oriented bounding boxes [Gottschalk 
et al. 1996], sphere-swept rectangles [Larsen et al. 2000], convex polytopes [Ehmann and Lin 2001] or 
any other vol­ume that optimizes the computation of the desired distance func­tion. 2.1 Minimization 
algorithm Consider a function F that takes two bounding volumes or mesh triangles as input and returns 
a numerical value representing some type of distance. We show that if F satis.es the following condition 
F(A,C) = F(B,C) if A . B for compact sets A, B and C, the follow­ing algorithm will minimize this function 
over a pair of meshes up to a requested precision and .nd a pair of bounding volumes or tri­angles as 
witnesses to this minimum. This property is equivalent to F being the minimum over all pairs of points 
(a, b) . (A × B) of some reasonable function F and is satis.ed all functions mentioned here. MIMIMIZE( 
Input: F(, ), Meshes M1,M2 as BV trees ) 1 push root pair of Bounding Volumes on queue or stack. 2 while 
(BV 1,BV 2) . pop pair (closest if queue) 3 do if F(BV 1,BV 2) satis.es query 4 then return (BV 1, BV 
2). 5 else 6 optionally swap (BV 1,BV 2) 7 push pair (BV1.leftChild, BV2) 8 push pair (BV1.rightChild, 
BV2) That this algorithm does indeed minimize F follows from the fact that the F-distance between bounding 
volumes is at most the F­ *Contact: marek@cm-labs.com distance between the content of those volumes and 
that the distance is not decreased at each step. 2.2 Directed penetration distance We de.ne the directed 
penetration distance as the distance in the given direction D by which one has to translate mesh M1 (or 
tri­angle or bounding volume) in order to come into surface con­tact with mesh M2 (or triangle or bounding 
volume resp.), i.e. F(A,B)= {min((b - a) · D) : b - a.D, a . A,b . B} for two sets A and B. This value 
is negative if M1 and M2 overlap and 8 if the projections of the sets onto the plane perpendicular to 
D do not overlap. In this algorithm, the penetration distance between two triangles and between two bounding 
volumes needs to be computed. We compute the former by clipping the projections of the triangles onto 
the plane orthogonal to the direction D with respect to each other and .nding the distance along D between 
the resulting vertices of one clipped triangle to the plane of the other.  3 Penetration direction For 
physical simulations it seems appropriate to use the direction in which the volume of two objects is 
reduced the fasted, as one object translates w.r.t. the other, as the direction of penetration. We show 
the following Theorem 3.1 The gradient of the penetration volume between one translating and one .xed 
mesh, and with a closed polygonal inter­section curve given by edges Pi,P(i+1) mod n,i = 0,...n - 1 is 
pro­ portional to n-1 . Pi × P(i+1) mod n i=0 In the planar case, this is simply the intersection polygon 
normal [Paeth 1995]. 4 Results We have implemented this framework as part of a collision detec­tion 
system and found that while the penetration distance compu­tation is slower than the computation of euclidean 
distance in the same framework, it is still usable for high .delity simulations. This is likely due to 
the discontinuity in the penetration distance func­tion. References EHMANN, S. A., AND LIN, M. C. 2001. 
Accurate and fast proximity queries between polyhedra using convex surface decomposition. In EUROGRAPHICS, 
vol. 20. GOTTSCHALK, S., LIN, M. C., AND MANOCHA, D. 1996. OBB-tree: A hierarchical structure for rapid 
interference detection. Comput. Graph. 30, 171 180. Proc. SIGGRAPH 96. LARSEN, E., GOTTSCHALK, S., LIN, 
M. C., AND MANOCHA, D. 2000. Fast prox­imity queries with swept sphere volumes. In Intl Conf. Robotics 
and Automation. PAETH, A. W., Ed. 1995. Graphics Gems V. Academic Press, Boston, MA. Copyright held by 
the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965445</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Casual 3D photography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965445</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965445</url>
		<abstract>
			<par><![CDATA[We describe the construction of a lightweight, point-and-shoot 3D color camera, intended for reconstruction of smooth surfaces. Our approach extracts 3D shape from a single image and is completely automatic. The 3D camera consists of an eye-safe laser pattern generator attached to a consumer-grade digital camera and some image processing software.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D camera]]></kw>
			<kw><![CDATA[3D photography]]></kw>
			<kw><![CDATA[3D shape acquisition]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.8</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>B.4.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P653828</person_id>
				<author_profile_id><![CDATA[81100504887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Askold]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Strat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Symbol Technologies, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP25004357</person_id>
				<author_profile_id><![CDATA[81100516478]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Manuel]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Oliveira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Instituto de Inform&#225;tica - UFRGS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Casual 3D Photography 1Symbol Technologies, Inc. 2InstitutodeInformática UFRGS 3SUNY at Stony Brook 
 Abstract We describe the construction of a lightweight, point-and-shoot 3D color camera, intended for 
reconstruction of smooth surfaces. Our approach extracts 3D shape from a single image and is completely 
automatic. The 3D camera consists of an eye-safe laser pattern generator attached to a consumer-grade 
digital camera and some image processing software. Categories and Subject Descriptors: I.4.8 [Image Processing 
and Computer Vision]: Digitization and Image Capture Imaging Geometry; B.4.2 [Input/Output and Data 
Communications]: Input/Output Devices. Additional Keywords: 3D camera, 3D photography, 3D shape acquisition. 
 Description We have designed and built a truly portable 3D color camera suitable for shape acquisition 
of smooth surfaces. This is our second-generation 3D camera prototype, and consists of a consumer grade 
digital camera, an eye-safe laser pattern generator and some image processing software. The 3D camera 
acquires a pair of pictures with a single press of the camera s trigger. One of the pictures contains 
a set of projected laser lines (created by the pattern generator) and is used to reconstruct the object 
s imaged geometry using triangulation. The other picture is used for texture mapping. Model reconstruction 
is carried out in three steps: first, the laser lines are extracted and each such line is assigned an 
elevation angle. This information is then used to compute depth for each of the lines pixels. Next, the 
3D coordinates of the recovered points are processed to remove outliers, minimize the effects of possibly 
incorrectly assigned angles, and fill in holes in the model. After 3D geometry has been recovered, the 
second picture is used for texture mapping. Figures 1 (left) and (center) show a pair of images acquired 
by our 3D camera. Figure 1 (left) is a photograph of a dummy head (about the size of a human head) exhibiting 
the laser lines. Figure 1 (center) shows the image acquired for texturing. Figure 1 (right) shows the 
reconstructed model seen from a novel viewpoint. The acquisition of the pair of pictures takes less than 
one second. Figure 2 shows a prototype of our 3D camera based on an Olympus C-2020 zoom (top portion 
of Figure 2). In order to keep the design as modular as possible, the only electrical connection between 
the camera and the raster generator is through the external flash connector, whose signal is used for 
synchronization. Such a design provides great flexibility, allowing any consumer-grade digital camera 
with support for external flash and exposure time control to be quickly turned into a 3D camera. The 
laser raster generator produces an eye-safe laser (Class IIa at 650nm), requiring about 2.7V at 80mA. 
It projects a set of quasi-horizontal scanning lines while a narrow laser beam bounces onto two mirrors, 
which oscillate around perpendicular axes. The laser beam bounces off a mirror with a vertical axis (x­mirror), 
oscillating at 2KHz, and then encounters the second mirror (y-mirror), which oscillates at 6Hz. This 
motion composition results in a zigzagged line. A raster-control circuit is used to turn off the laser 
every other line, eliminating the retrace. Like all other laser-based techniques, our approach is subject 
to several physical limitations due to the optical material properties, ambient illumination, occlusions, 
and speckle noise, which make the task of shape extraction significantly harder. A limiting factor in 
the amount of geometric detail that can be captured by our 3D camera is the density of the raster lines 
used to sample the surface. Currently, the raster generator projects sixty lines. While a larger number 
could potentially allow the recovery of more surface detail, reducing the spacing between two consecutive 
lines makes the extraction and disambiguation task more difficulty. The existence of depth discontinuities 
can make the proper segmentation of lines considerably difficult. An important step towards making our 
approach more robust is to use a global analysis for correctly handling the projection of raster lines 
at surface discontinuities. Disclaimer: This work reflects the personal understanding and opinions of 
the authors only and is not intended in any way to convey any position, policy, or opinion of Symbol 
Technologies, Inc. The authors and Symbol Technologies, Inc. disclaim any liability for errors or omissions 
in this paper. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965442</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Spectral watertight surface reconstruction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965442</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965442</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31041449</person_id>
				<author_profile_id><![CDATA[81100419269]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ravi]]></first_name>
				<middle_name><![CDATA[Krishna]]></middle_name>
				<last_name><![CDATA[Kolluri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044484</person_id>
				<author_profile_id><![CDATA[81100474718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[Richard]]></middle_name>
				<last_name><![CDATA[Shewchuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133156</person_id>
				<author_profile_id><![CDATA[81100311781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[O'Brien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2295344</ref_obj_id>
				<ref_obj_pid>2295319</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AMENTA, N., CHOI, S., AND KOLLURI, R. 2001. The Power Crust, Unions of Balls, and the Medial Axis Transform. Computational Geometry: Theory and Applications 19, 2--3 (July), 127--153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>84521</ref_obj_id>
				<ref_obj_pid>84514</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[POTHEN, A., SIMON, H. D., AND LIOU, K.-P. 1990. Partitioning Sparse Matrices with Eigenvectors of Graphs. SIAM Journal on Matrix Analysis and Applications 11, 3 (July), 430--452.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965444</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Automated meshing of sparse 3D point clouds]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965444</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965444</url>
		<abstract>
			<par><![CDATA[We propose a novel method that uses simulated annealing to create an optimal surface mesh by selecting a subset of a 3D point cloud and a triangulation that reliably represents the actual topology of the scene. This method provides a number of advantages: it copes well with noisy data, it produces a simplified mesh, particularly for scenes that contain many planes and, unlike greedy search techniques, it is much more likely to converge to a global minimum.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653928</person_id>
				<author_profile_id><![CDATA[81392599797]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oli]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cooper]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, U.K.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P571380</person_id>
				<author_profile_id><![CDATA[81100210477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Neill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, U.K.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14097948</person_id>
				<author_profile_id><![CDATA[81100256862]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gibson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, U.K.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[MORRIS, D. D. AND KANADE, T. 2000. Image-consistent surface triangulation, CVPR, 1, 332--338.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614477</ref_obj_id>
				<ref_obj_pid>614281</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KREYLOS, O. AND HAMANN, B. 2001. On simulated annealing and the construction of linear spline approximations for scattered data, IEEE Transactions on Visualisation and Computer Graphics, 7, 1, 17--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automated Meshing of Sparse 3D Point Clouds Oli Cooper*, Neill Campbell, David Gibson University of 
Bristol, U.K. Abstract We propose a novel method that uses simulated annealing to create an optimal 
surface mesh by selecting a subset of a 3D point cloud and a triangulation that reliably represents the 
actual topology of the scene. This method provides a number of advantages: it copes well with noisy data, 
it produces a simplified mesh, particularly for scenes that contain many planes and, unlike greedy search 
techniques, it is much more likely to converge to a global minimum. 1 Introduction Even with relatively 
sparse 3D point clouds it is unlikely that it will be necessary to include all the points in the set 
to produce an accurate polygonal model of the scene. Critical points include those on corners, boundaries, 
edges and those which define complex surfaces. The reliability of Structure from Motion (SfM) algorithms 
is rapidly increasing and the automated creation of accurate, calibrated clouds of 3D points from image 
sequences is now a reality. However, in most cases, the desired end product is not simply a cloud of 
points but a texture-mapped polygonal model. There is therefore a need to be able to create surface meshes 
from these 3D point clouds automatically. Generally SfM produces sparse, unstructured point clouds that 
are unsuitable for meshing using conventional methods. One possible approach to this problem is to create 
an initial rough triangulation of all the points in 2D and then refine this triangulation by edge swapping 
until a surface that best accounts for the images of the scene is found using a greedy search. Other 
methods have ranged from manually specifying the triangulation to attempting to fit primitives to the 
point cloud. Our method proceeds from an initial, random, subset of the data. Using the simulated annealing 
algorithm, a cost function is Fig. 1 A texture-mapped model of a scene using 30 points  chosen as the 
vertices for the triangulation. The convex hull, H, of these vertices is computed and, from the remaining 
interior points, a random subset, S, is chosen. The Delaunay triangulation of S and H is then calculated. 
We then enter the iterative stage of the algorithm. In each iteration a vertex, v, is randomly chosen 
from S and moved . Moving is ~ defined as substituting v for another randomly selected vertex, v , that 
is in neither S nor H. Movement to a nearby point is favoured through use of a Gaussian probability distribution. 
The Delaunay triangulation of this new subset is then calculated and optionally refined using edge swapping 
techniques. In a manner similar to [Morris and Kanade 2000], the cost of the new triangulation is calculated 
as the error between one or more other images in the sequence and an image formed by warping the textures 
to the triangulation of the vertices in the new image. If this cost is lower than any encountered previously, 
the new subset/triangulation is accepted, otherwise it may be accepted according to an annealing schedule 
of decreasing probability. This procedure repeats for either a set number of iterations or until convergence 
is achieved. 13  Cost 12 11 10 9 8 7 Fig. 2 (left) Number of interior points against cost of the final 
mesh. (right) Triangulation of 30 from a cloud of 293 points. It can be seen that for this scene only 
30 interior points are sufficient to produce a good surface reconstruction. After this, adding more points 
does not significantly reduce the final cost. References MORRIS, D.D. AND KANADE, T. 2000. Image-consistent 
surface triangulation, CVPR, 1, 332-338. KREYLOS, O. AND HAMANN, B. 2001. On simulated annealing and 
the construction of linear spline approximations for scattered data, IEEE Transactions on Visualisation 
and Computer Graphics, 7, 1, 17-31. iteratively minimised by selecting a point in the current subset 
and moving it by swapping it for a point not in the subset. The nature of automated vision algorithms 
means that outliers are frequently present. By selecting an optimal subset of the point cloud, outliers, 
that can severely affect the quality of the final mesh, are eliminated and a simpler mesh is produced. 
Additionally, by using simulated annealing as opposed to greedy search techniques, the chance of getting 
stuck in a local minimum, rather than converging to a global minimum, is greatly decreased. 2 Method 
 The simulated annealing algorithm is an iterative minimisation technique well suited for large-scale 
optimisation problems. The basis of this algorithm is a stochastic search, where changes that increase 
the error may sometimes be accepted according to a probability schedule. This allows the possibility 
of escape from a local minimum, thus increasing the chance of converging to a global minimum. A set of 
the corresponding 2D features in one of the images is *email: Oli.Cooper@bristol.ac.uk Copyright held 
by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965446</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Interaction]]></section_title>
		<section_page_from>10</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P348897</person_id>
				<author_profile_id><![CDATA[81100567689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965447</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Sensory design for virtual environments]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965447</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965447</url>
		<abstract>
			<par><![CDATA[The Sensory Environments Evaluation (SEE) Project seeks to formulate a new design methodology for virtual environments that utilize multiple sensory inputs to induce presence. Deviating from virtual reality's twenty-year focus on photo-realism, our "feels-real" design alternative aims directly at understanding the interdependencies of sensory stimuli in their creation of mental constructs, and the subsequent degree of realism perceived.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653885</person_id>
				<author_profile_id><![CDATA[81100180689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Morie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14168879</person_id>
				<author_profile_id><![CDATA[81100482542]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iyer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653898</person_id>
				<author_profile_id><![CDATA[81335498849]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Valanejad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653938</person_id>
				<author_profile_id><![CDATA[81100444713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sadek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14161956</person_id>
				<author_profile_id><![CDATA[81100462763]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miraglia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653848</person_id>
				<author_profile_id><![CDATA[81100357162]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Milam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14015197</person_id>
				<author_profile_id><![CDATA[81100006639]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Williams]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653847</person_id>
				<author_profile_id><![CDATA[81375598258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[D.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Luigi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653884</person_id>
				<author_profile_id><![CDATA[81100644826]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leshin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>835733</ref_obj_id>
				<ref_obj_pid>554230</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DINH, H. Q., WALKER, N., SONG, C., KOBAYASHI, A., AND HODGES, L. F. 1999. Evaluating the Importance of Multi-Sensory Input on Memory and the Sense of Presence in Virtual Environments, in IEEE Virtual Reality Proceedings, 222--228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[TULVING, E. AND CRAIK, F. I. M., Editors 2000. The Oxford Handbook of Memory. Oxford University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sensory Design for Virtual Environments J. F. Morie*, K. Iyer, K. Valanejad, R. Sadek, D. Miraglia, 
D. Milam, J. Williams, D-P. Luigi, and J. Leshin USC Institute for Creative Technologies Abstract The 
Sensory Environments Evaluation (SEE) Project seeks to formulate a new design methodology for virtual 
environments that utilize multiple sensory inputs to induce presence. Deviating from virtual reality 
s twenty-year focus on photo-realism, our feels-real design alternative aims directly at understanding 
the interdependencies of sensory stimuli in their creation of mental constructs, and the subsequent degree 
of realism perceived.  1 Introduction At the University of Southern California s Institute for Creative 
Technologies (ICT) we are developing a series of virtual environments for training, art, and education 
entitled SEE : DarkCon simulated reconnaissance of a life-threatening environment by a single participant 
 Memory Stairs allegorical navigation through an artist s memory associations by a single participant 
 Coral Reef pedagogical experience of the ocean s most fertile communities by multiple participants 
  Interest in embarking on such a project resulted from a deep dissatisfaction with the state of current 
virtual environments. Research has proven the cumulative effects of visual, aural, and olfactory stimuli 
increase presence induction [Dinh et. al., 1999], but their relation to one another beyond additive measure 
remains undetermined. SEE proposes to create a design approach that will help identify effective orchestrations 
of these multiple sensory stimuli, and thereby further the bounds of virtual realism.  2 Exposition 
 Our prototype environment for this design approach is DarkCon, a scenario in which the participant is 
asked to make their way through an unfamiliar hostile environment at night without being discovered. 
While there, the participant is also tasked with making a number of observations whose accuracy is purported 
to be the gauge by which their success will be measured. These conditions stem from SEE s scenario-specific 
goal of increasing the participant s emotional engagement. Research has shown that experiences with emotional 
attachment are retained longer than those without [Tulving, 2000]. Creating this engagement with multiple 
sensory inputs is where our design methodology becomes necessary. We first describe the environment 
in terms of its sensory inputs, and then filter according to scenario parameters, expected degree of 
emotional response, and available technology. A number of these elements might seem incidental (ambient 
sounds, shadows, reflections), but their inclusion is essential towards providing realism. SEE currently 
employs real-time delivery hardware capable of textured graphics, spatialized sounds, infrasound, and 
low-level haptics, and is in development of a proprietary scent technology. These tools allow for a relatively 
large number of identified elements to be incorporated, but resource management demands selective inclusion 
in all cases. These elements are then woven into a story, compressing the scenario down to its essential 
events. This type of interactive narrative is challenged by the participant s free will, defying a contiguous 
event structure and increasing the likelihood of a perplexing experience. To confront these conditions 
while preserving autonomy, we further enhance the scenario with techniques we collectively term coercive 
narrative, constructing psychological cues with available elements to persuade a desired course of action. 
 Having identified the most likely path the participant will follow within the environment, a corresponding 
emotional score can be developed. Given the order of desired emotional responses, selected elements from 
each of the sensory inputs can be utilized to increase and decrease the participant s state of arousal 
based on their location in the environment. This orchestration of emotions can serve to ensure increased 
engagement of the participant for the duration of the experience. The presiding goal of SEE is to scientifically 
test our design methodology with evaluation studies. In regard to DarkCon, questionnaire information 
will be coupled with data retrieved from a physiological monitor. This data will allow us to determine 
degrees of emotional arousal and potentially differentiate between specific emotions. Follow-up questionnaires 
administered at a subsequent date will determine if the degree of emotional engagement was great enough 
to increase memory retention, and if differentiated, the respective effect of identified emotions on 
retention. This model will be applied to further testing of memory association and learning within virtual 
environments. These studies will help specify the most effective components in eliciting presence, and 
the effect presence has on potential uses of virtual reality.  References DINH, H. Q., WALKER, N., 
SONG, C., KOBAYASHI, A., AND HODGES, L. F. 1999. Evaluating the Importance of Multi-Sensory Input on 
Memory and the Sense of Presence in Virtual Environments, in IEEE Virtual Reality Proceedings, 222-228. 
 TULVING, E. AND CRAIK, F.I.M., Editors 2000. The Oxford Handbook of Memory. Oxford University Press. 
 -------------------------------------------- *e-mail: morie@ict.usc.edu + The work described in this 
abstract was developed with funds of the Department of the Army under contract number DAAD 19-99-D-0046. 
Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors 
and do not necessarily reflect the views of the Department of the Army. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965449</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Evaluation of a combined 2D/3D interface for micro electro mechanical systems (MEMS) design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965449</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965449</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP45026758</person_id>
				<author_profile_id><![CDATA[81405594548]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Payne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Design Studio, Glasgow School of Art]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>260001</ref_obj_id>
				<ref_obj_pid>259963</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CHAPIN, W. L., LACEY, T. A., AND LEIFER, L. 1994. Designspace: A manual interaction environment for computer aided design. In Proceedings of ACM CHI'94 Conference on Human Factors in Computing Systems, ACM Press/ACM SIGGRAPH, DEMONSTRATIONS: Virtual Reality Multimedia, ACM, 33--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>835707</ref_obj_id>
				<ref_obj_pid>554230</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HIX, D., II, J. E. S., GABBARD, J. L., MCGEE, M., DURBIN, J., AND KING, T. 1999. User-centered design and evaluation of a real-time battlefield visualization virtual environment. In Proceedings IEEE Virtual Reality 1999, IEEE Computer Society Press, IEEE, 96--103.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[YOHSHIMORI, H., MATSUMIYA, M., TAKEMURA, H., AND YOKOYA, N. 2000. Combination of two and three dimensional space for solid modelling. In SIGGRAPH 2000 Conference Abstracts and Applications, ACM Press/ACM SIGGRAPH, T. Appolloni, Ed., Computer Graphics Annual Conference Series, 2000, ACM, 191.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Evaluation of a Combined 2D/3D Interface for Micro Electro Mechanical Systems (MEMS) Design J. Payne* 
Digital Design Studio, Glasgow School of Art 1 Introduction This sketch describes a novel hybrid 2D/3D 
interaction system be­ing developed at the Digital Design Studio in conjunction with QinetiQ. The system 
consists of a large scale rear projection tablet which allows the user to utilise familiar WIMP (Windows 
Icons Menu Pointer) interaction tasks along with multiple views of the data or its input components in 
3D space including 2D windows and 3D objects. This large design space will be navigated via the use of 
a customised pen/.nger tool which incorporates a 2D tablet stylus and a 6 DOF magnetic tracker. This 
tool will be designed so as to allow intuitive use of a keyboard during work. A focus on the particular 
user needs from the burgeoning new market of MEMS engineering design will provide a discrete set of data 
for the evalu­ation of the system s interaction and productivity through the many stages of the MEMS 
design process. 2 Motivation The project provides a novel concept through the use of large scale coordinated 
multiple views of information in the 3D realm while retaining a hi-level interaction paradigm in the 
familiar 2D plane. Previous work in the .eld includes the Immersive Modelling En­vironment[Yohshimori 
et al. 2000], and DesignSpace [Chapin et al. 1994] . These research interests prove that there is a speci.c 
com­mercial interest in highly functional 3D design systems. The com­puting technology is enabling through 
it s increasing advancement and decreasing cost. However, due to cost and usability issues, the research 
concepts have not .ltered into mainstream system usage The methods of evaluation will involve usability 
engineering of which Hix et al. .rst proposed in 1999. By utilising Expert, For­mative and Summative 
evaluation techniques, it will be possible to formulate an empirical study of these interaction tasks 
and rec­ommend future work in the area of engineering visualisation and design systems.[Hix et al. 1999] 
The interface will provide the user with a 2D work space which allows high resolution interaction in 
the 2D plane while also provid­ing pointer and alphanumeric input through a stylus and keyboard. Above 
the 2D area is a 3D workspace in which the user can view 3D objects interact with them and carry out 
simple manipulation tasks (Fig 1). These visuals would .oat in the 3D view space above the desk in the 
user s periphery view allowing them to be grabbed via the 3D stylus and brought to the 2D tablet for 
manipulation. This system will allow the full functionality of the software to be imple­mented in expert 
applications. The novel input devices complement the full range of existing inputs required through today 
s computing environment. 3 Conclusion This concept builds upon previous research and works towards an 
increasingly functional system for professional engineers. Also, ap­plying a user centred approach in 
the selection and evaluation of the * e-mail: j.payne@gsa.ac.uk system will provide increased interaction 
and further bridge the gap between WIMP GUI s and increasingly functional 3D interaction systems. With 
it s focus on a particular market area and goal the project should reap some marketable rewards and further 
integrate 3D interaction systems into industrial and consumer markets. The data gathered from the evaluation 
criteria will be documented and will provide a useful source for future hybrid multiple view visual­isation 
systems. Acknowledgments to QinetiQ Malvern UK, who partially fund this project. Figure 1: Image highlighting 
the relationship between user and the 2D/3D elements. References CHAPIN, W. L., LACEY, T. A., AND LEIFER, 
L. 1994. De­signspace: A manual interaction environment for computer aided design. In Proceedings of 
ACM CHI 94 Conference on Human Factors in Computing Systems, ACM Press / ACM SIGGRAPH, DEMONSTRATIONS: 
Virtual Reality Multimedia, ACM, 33 34. HIX, D., II, J. E. S., GABBARD, J. L., MCGEE, M., DURBIN, J., 
AND KING, T. 1999. User-centered design and evaluation of a real-time battle.eld visualization virtual 
environment. In Proceedings IEEE Virtual Reality 1999, IEEE Computer Society Press, IEEE, 96 103. YOHSHIMORI, 
H., MATSUMIYA, M., TAKEMURA, H., AND YOKOYA, N. 2000. Combination of two and three dimensional space 
for solid modelling. In SIGGRAPH 2000 Conference Abstracts and Applications, ACM Press / ACM SIGGRAPH, 
T. Appolloni, Ed., Computer Graphics Annual Conference Se­ries, 2000, ACM, 191. Copyright held by the 
author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965448</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Authoring of a mixed reality furniture assembly instructor]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965448</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965448</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653882</person_id>
				<author_profile_id><![CDATA[81100163214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#252;rgen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zauner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria University of Applied Sciences (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37023145</person_id>
				<author_profile_id><![CDATA[81100048383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria University of Applied Sciences (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P641143</person_id>
				<author_profile_id><![CDATA[81100175407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Brandl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Upper Austria University of Applied Sciences (MTD)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14178777</person_id>
				<author_profile_id><![CDATA[81100514321]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Werner]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hartmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Applied Knowledge Processing]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AMIRE, 2002. http://www.amire.net.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>741481</ref_obj_id>
				<ref_obj_pid>647988</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ANTIFAKOS, S., MICHAHELLES, F., AND SCHIELE, B. 2002. Proactive Instructions for Furniture Assembly. In Proceedings of the The Fourth International Conference on Ubiquitous Com- puting (UbiComp 2002).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[KATO, H., BILLINGHURST, M., BLANDING, B., AND MAY, R. 1999. ARToolKit. Technical report, Hiroshima City University, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965450</section_id>
		<sort_key>11</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Sound & dance]]></section_title>
		<section_page_from>11</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P414508</person_id>
				<author_profile_id><![CDATA[81100572315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Colin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dalton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965452</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Cati dance]]></title>
		<subtitle><![CDATA[self-edited, self-synchronized music video]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965452</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965452</url>
		<abstract>
			<par><![CDATA[This sketch presents a real-time system that aims to bridge a gap between machine listening technology, and self-editing, self-synchronizing video: a movie organizes itself from "listening" to music. In our current demonstration, a series of short video clips of "Cati" dancing, originally shot at different tempi, are arbitrarily sequenced, always in sync with the analyzed beat, i.e., if the music slows down, the dance slows down accordingly. When no beat is found, e.g., the music is mellow, then Cati stops dancing and waits, apparently bored.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P345115</person_id>
				<author_profile_id><![CDATA[81100596381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tristan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jehan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14209592</person_id>
				<author_profile_id><![CDATA[81100606713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lew]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Lab Europe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP38024765</person_id>
				<author_profile_id><![CDATA[81452607629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Cati]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaucelle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Lab Europe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[SCHEIRER, E., 1998, Tempo and Beat Analysis of Acoustic Music Signals, Journal of the Acoustic Society of America, 103(1).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cati Dance: self-edited, self-synchronized music video Tristan Jehan Michael Lew Cati Vaucelle MIT Media 
Lab tristan@media.mit.edu Media Lab Europe lew@mle.ie Media Lab Europe cati@mle.ie Abstract This sketch 
presents a real-time system that aims to bridge a gapbetween machine listening technology, and self-editing, 
self­synchronizing video: a movie organizes itself from listening tomusic. In our current demonstration, 
a series of short video clipsof Cati dancing, originally shot at different tempi, are arbitrarilysequenced, 
always in sync with the analyzed beat, i.e., if themusic slows down, the dance slows down accordingly. 
When nobeat is found, e.g., the music is mellow, then Cati stops dancingand waits, apparently bored. 
1 Introduction When Kenneth Anger presents a costumed character moving through a garden of fountains 
in Eaux d'Artifice in the 50's, he manipulates lights, fountains, and the water synchronized with Vivaldi's 
music. The drama of expectation is often driven by the rhythm between music and images. If today, existing 
systems allow us to synchronize them properly, it is typically a manual and time-consuming process. Cati 
Dance, however, engages the user to direct the manipulation of video by changing music on a portable 
MP3 player connected via audio to the computer. Not only he/she can express a musical choice, but also 
the video then reacts in a narrative manner, e.g., a lack of beat is causing the dancer Cati to stop 
dancing, and to wait, apparently bored. When the rhythm changes abruptly, Cati reacts in surprising ways, 
trying to keep up with the new beat. It takes generally a few seconds to adjust. The system extends the 
user s artistic expression by satisfying his/her expectation of the inter-relationship between music 
and video. 2 Description The Cati Dance project is a real-time system that plays arbitrary video clips 
of a database, always in sync with incoming music. Our demonstration counts about 20 clips (from 5 to 
15 seconds each) of a woman dancing (see Cati in Figure 1). We also use about 10 clips of Cati simply 
waiting (see Figure 2). The video database must be previously prepared, whereas the audio remains unknown 
in advance, and can arbitrarily be chosen. Preparation of a video clip consists of manually cropping 
the video sequence on particular visual beats (i.e., found visually in the movie, or with the help of 
the original audio track), and annotating the clip name with its total number of beats in a text file. 
The audio track can be discarded, and the video is re-sampled at 60 bpm, i.e., 1 second per beat. The 
system relies on two main components: an audio beat tracker, and a feedback-controlled movie editor. 
Beat tracker: Our audio beat-tracking algorithm (also called foot­taping) was inspired by Eric Scheirer 
s [1], and performs equally well on most popular music, however using less processing power. It was required 
to save power on audio analysis, in order to edit, and display full-screen video on the same machine. 
Our model analyzes the audio signal with sliding G4-optimized-FFTs provided by Apple. The power spectrum 
is converted into an auditory-model filterbank, which mimics the human cochlea. A large series of comb 
filters return the resonant periodicity amplitude in each channel for 60 tempi ranging from 80 to 140 
bpm, which then are summed across to give the global energy­per-tempo estimations. The maximum corresponds 
to the current music tempo. Analysis of the internal phase of the best-matching resonator, allows us 
to predict the location in time of the next beat. The internal gain gives us an estimation of strength. 
Movie editor: The movie-editing algorithm takes only two inputs (the audio beat, and strength information), 
three arguments (two text files and a strength threshold), and displays the final movie on the screen. 
Each text file contains a list of names for the available video clips (a list for dancing clips, and 
a list for waiting clips), and their corresponding number of beats (for the dancing list only, as described 
above). The inferred beat of the video is constantly compared to the analyzed beat of the audio, and 
a feedback-controlled mechanism allows us to regulate the internal frame rate of the movie, changing 
its speed appropriately. The displayed frame rate is however kept constant at about 30 fps. A new clip 
is randomly selected from a list, loaded, and queued so that transitions between clips are seamless. 
If the strength parameter is below the given threshold, then the clips from the waiting list are chosen. 
A video of the system in action can be found at: http://www.media.mit.edu/~tristan/Projects/catidance.html 
 Figure 2. Screen shot of Cati waiting. References [1] SCHEIRER, E., 1998, Tempo and Beat Analysis 
of Acoustic Music Signals, Journal of the Acoustic Society of America, 103(1). Copyright held by the 
author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965451</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Sound articulation, lip movement and letter visualisation in the Arabic language]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965451</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965451</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653946</person_id>
				<author_profile_id><![CDATA[81100070871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Roula]]></first_name>
				<middle_name><![CDATA[Salim]]></middle_name>
				<last_name><![CDATA[Ghalayini]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[American University of Beirut]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ABIFARES SMITSHUIJZEN, H. 2001. Arabic Typography.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SUNGHIL CHO, P. Computational Models for Expressive Typography.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HELFAND, J. 2001. Screen: Essays on Graphic Design.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sound Articulation, Lip movement and Letter visualisation in the Arabic Language * Roula Salim Ghalayini 
American University of Beirut 1 Introduction We all know that digital media opens possibilities for typography 
that go beyond the constraints of traditional typesetting. I became interested in exploring ways in which 
digital typography could facilitate the creation of letterforms that related more to the way we speak, 
more specifically, ways in which it could retain the human qualities of speech. 2 The Process Since 
Arabic was my alphabet of choice , I aspired to appropriate my interest to aspects that were specific 
to the Arabic writing system. Consequently, the Arabic vocalisational marks, or harakat came into my 
area of study. In Arabic, like in most languages, there is no relationship between the sound of a particular 
letter and its corresponding letterform. The Arabic alphabet consists of essentially three overlapping 
systems for sound representation. These are the basic letterforms, the diacritic dots and the vocalisational 
marks. In Arabic, the written does not necessarily mimic the spoken, and a lot of judgement on the sound, 
and hence meaning, is connected to the context. The signs that are pronounced but not written, the "harakat"or 
the invisible sounds, are represented as small vocalisation marks. These vocalisational marks represent 
the vowels [ABIFARES 2001]. The fact that vocalisational marks are always sounded and are essential to 
the meaning of words, yet are mostly omitted from texts, was the driving push behind the work. This project 
consequently attempts to create letterforms that incorporate the vocalisational marks into their own 
form. The letterforms are designed according to the movement of the lips during their pronunciation, 
i.e. during the articulation of their sound. Hence tying back to my initial interest of bridging the 
written and spoken. 3 The Design The type family consists of four typefaces. One to account for each 
of the main vocalisational marks. 1 Dummeh Typeface The dummeh is pronounced as an "ooo" sound. In articulating 
this sound, the lips undergo a circular inward movement, which I felt was best represented by the circle. 
 2 Fat'ha Typeface The fat'ha is pronounced as an "aaa" sound. In articulating this sound, the lips undergo 
a vertical upward/downward motion, which is best represented by the oval. 3 Kasra Typeface The kasra 
is pronounced as an "eee" sound. In articulating this sound, the lips stretch sideways in a movement 
that is best represented by a horizontal oval.  4 Sukoon Typeface The sukoon represents the lack of 
vocalisational mark. The most neutral of forms being the square was chosen, but this was then injected 
with an organic feel by breaking its sharp corners, to create a stronger visual link to the other 3 forms. 
All the forms have the same stroke thickness, further linking them visually. Finally, within each typeface, 
there are four forms for each letter, because in the Arabic language, the shape of the letterform changes 
according to its placement in a word, i.e. whether it lies in the beggining of a word, the middle of 
a word, the end of a word or if it is a freestanding letterform. Again, this had to be accounted for 
in the design.  4 Exploring Different Applications In applying the font into context, the examples 
enforce that ideally the Harakat typeface can be used both on and off the screen, both animated and still. 
When letterforms are being animated, they first appear on the screen in the form that does not contain 
a vocalisational mark, namely the Sukoon typeface. They then morph into the form that represents their 
vocalisational mark. If a letter does not have a vocalisational mark, it remains unchanged. 1 Area Signage 
Harakat are often omitted and words still understood because of their acquired meanings. But what happens 
with the names of areas for example, which are often hard to pronounce, and simultaneously have no apparent 
meaning? This more often than not results in miscommunication. 2 Neon Signage Neon signs or light projections 
on building facades is another possible application. Pubs, restaurants and bars in Beirut are increasingly 
taking on names that are foreign. They are translated phonetically into Arabic, and thus often cannot 
be deciphered due to their lack of acquired meaning. 3 Online Chatting Program Here I tried to explore 
an Arabic adaption of the MSN Messenger Chatting program. In online chatting programs, one constantly 
encounters miscommunication with the writing of colloquial language that does not initially have a strong 
written base, and is drifting linguisitically further away from written arabic over the years.  4 Broadcast 
The broadcast medium, is used as a context in which the use of the Harakat font gives the human quality 
of speech to the written text of a film's credits.  References ABIFARES SMITSHUIJZEN, H. 2001. Arabic 
Typography. SUNGHIL CHO, P. Computational Models for Expressive Typography. HELFAND, J. 2001. Screen: 
Essays on Graphic Design. * e-mail: roula.ghalayini@beirut.leoburnett.com Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965454</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Sound synthesis for the Web, games, and virtual reality]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965454</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965454</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP48025960</person_id>
				<author_profile_id><![CDATA[81321496403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Parker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653952</person_id>
				<author_profile_id><![CDATA[81344488911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sonny]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Efros, A. and Leung, T., Texture Synthesis by Non Parametric Sampling, Seventh International Conference on Computer Vision, Corfu, Greece, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Parker, J. R., Chan, S., and Behm, B., Generating Audio Textures by Example, CHI 2003, Ft. Lauderdale, FLA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wei, L. Y. and Levoy, M., Fast Texture Synthesis Using Tree Structured Vector Quantization, Proc. of ACM SIGGRAPH Conference, New Orleans, LA, July 25--27, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sound Synthesis for The Web, Games, and Virtual Reality J. R. Parker* and Sonny Chan Digital Media Laboratory, 
University of Calgary 1 Introduction Here we describe a new technique for synthesizing sounds from samples, 
using the block spectral Gaussian pyramid method originally devised for texture synthesis in computer 
graphics. Computer games and digital animations often impress the viewer with the quality of both the 
graphics and the audio. Sometimes real sounds are recorded and played back when needed, often as a loop. 
Other times the sounds are created in a studio using synthesizers and other devices. It should be possible 
to create realistic sounds from small samples using computer techniques, and thus create as much of any 
given sound as is needed. Sound synthesis is frequently not sufficiently realistic yet; one solution 
is to use small sam­ples of a desired sound and to reconstitute them to for a new, longer, and non-repeating 
sample. This is the subject that we wish to explore. There are many reasons for pursuing sound synthesis. 
Sound quality reflects more than frequency content or noise: repetitive sounds can be quite irritating. 
Gathering real sounds can be very expensive, even for large studios. Reusing existing samples is a cost 
effective option if high quality can be assured. The basic method proposed has been borrowed from work 
on graphical textures. We have based methods on three different texture synthesis algorithms; for instance, 
Efros starts with purely random pixels and grows the tex­ture, one pixel at a time - a very similar method 
will work for audio. 2 Block Spectral Gaussian Pyramid for Audio The use of a Gaussian pyramid, as expounded 
by Wei and Levoy for generating visual textures, could be a sound basis for the synthesis of audio textures, 
especially consid­ering the smaller dimensionality of audio data. The algo­rithm uses a tree to represent 
the audio texture, in which each level represents a different level of detail. One important difference 
between audio textures and visual textures is that a sample of an audio texture is smaller, relative 
to the total data set size, than is a pixel in an image. An important difference between the visual tex­ture 
algorithm of Wei and Levoy and our method for audio textures is that instead of using single pixels as 
texture ele­ments, we use small blocks or chunks of data - a block length of 0.1 seconds was selected 
initially. The input signal is the bottom level of the pyramid. The next level up is found by convolving 
the input with a Gaus­sian kernel, essentially performing a low-pass filter; thus the new level contains 
half as many samples as the original. This process is repeated until the desired tree height is achieved, 
producing the input pyramid. *e-mail parker@cpsc.ucalgary.ca Figure 1 - Sound samples (top) and their 
synthesized copies (bottom). Left is a crowd sound, right is a wood .re. Synthesis must create the output 
pyramid in the reverse direction, from the top downward. Starting with a randomly initialized top level, 
the input pyramid is used to create each subsequent lower level by .nding the most probable subse­quent 
block, using a neighborhood structure derived from that of Wei and Levoy. In selecting the value for 
each block, we consider the already synthesized blocks in the level above, both ahead and before, as 
well as the synthesized blocks in the same level before. The block in the input pyr­amid with the neighborhood 
most like that of the position being synthesized is selected as the subsequent block. Thus the texture 
is modeled as a Markov random .eld. In the comparison of two neighborhoods, we use a dis­tance measure 
that is the sum of the distances between cor­responding blocks in the neighborhoods. This is done by 
computing the RMS energy of the signal, with the distance between the blocks being the difference between 
their ener­gies. This method is most effective with textures containing distinct and separated elements, 
such as footsteps, thunder. After completing the synthesis of the bottom level of the output pyramid, 
it is necessary to construct the actual wave­form from the selected indices. To do this, sufficient space 
is allocated for the output waveform, which is then copied block by block from the input waveform according 
to the indices. When two blocks are copied that were adjacent in the input waveform, the playback is 
seamless. However, when the blocks were not adjacent, an alpha blend is used to smooth the transition 
between blocks.  3 Results and Conclusions In the absence of an objective comparison method for audio 
.les, assessments were made by students in our game design program, and the method is, by that standard, 
a suc­cess. The web page http://pages.cpsc.ucalgary.ca/ ~parker/AUDIO/ contains the original sounds and 
synthe­sis output for evaluation by researchers at large. References Efros, A. and Leung, T., Texture 
Synthesis by Non Parametric Sampling, Seventh International Conference on Computer Vision, Corfu, Greece, 
1999. Parker, J.R., Chan, S., and Behm, B., Generating Audio Textures by Example, CHI 2003, Ft. Lauderdale, 
FLA. Wei, L.Y. and Levoy, M., Fast Texture Synthesis Using Tree Struc­ tured Vector Quantization, Proc. 
of ACM SIGGRAPH Confer­ ence, New Orleans, LA, July 25-27, 2000. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965453</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Exploring MIDI datasets]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965453</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965453</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653941</person_id>
				<author_profile_id><![CDATA[81538746156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Reiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyazaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ochanomizu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39039141</person_id>
				<author_profile_id><![CDATA[81100355096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ochanomizu University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P250311</person_id>
				<author_profile_id><![CDATA[81100156121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Rumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hiraga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bunkyo University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>267131</ref_obj_id>
				<ref_obj_pid>266989</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[SMITH, S. M., AND WILLIAMS, G. N. 1997. A Visualization of Music. In Proc. IEEE Visualization '97, October, pp. 499--503.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KAPER, H. G., AND TIPEI, S. 1998. Manifold Compositions, Music Visualization, and Scientific Sonification in an Immersive Virtual-Reality Environment. In Proc. International Computer Music Conference, October, pp. 339--405.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>523237</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[SHNEIDERMAN, B. 1998. Designing the User Interface Strategies for Effective Human-Computer Interaction, Addison-Wesley, 3rd edition, Chapter15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>108883</ref_obj_id>
				<ref_obj_pid>108844</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ROBERTSON, G. G. et al. 1991. ConeTrees: Animated 3D Visualizations Hierarchical Information, In Proc. CHI'91, ACM Conference on Human Factors in Computing System, April, pp. 189--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965455</section_id>
		<sort_key>12</sort_key>
		<section_seq_no>12</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[2D or not 2D]]></section_title>
		<section_page_from>12</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP18000965</person_id>
				<author_profile_id><![CDATA[81100257079]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Townsend]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965457</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Automatic Asian art]]></title>
		<subtitle><![CDATA[computers converting photos to Asian paintings using humanistic fuzzy logic rules]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965457</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965457</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39040972</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P400736</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653935</person_id>
				<author_profile_id><![CDATA[81100424526]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lincoln]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lasalle-SIA College of the Arts, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>15911</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[STRASSMANN, S. 1986. Hairy brushes. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, ACM Press, 225--232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WUCIUS, W. 1991. The Tao of Chinese Landscape Painting. New York Design Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automatic Asian Art: Computers Converting Photos to Asian Paintings using Humanistic Fuzzy Logic Rules 
Farzam Farbiz * Adrian David Cheok Paul Lincoln National University of Singapore National University 
of Singapore Lasalle-SIA College of the Arts, Singapore 1 Introduction We describe a novel system that 
allows generation of Asian ink paintings from photographs. Asian ink painting began as a tradi­tional 
art form over three thousand years ago. This form of art work uses what is traditionally known as the 
four treasures : the ink, brush, stone, and paper (commonly kenzo .bre type paper). Western art also 
has the type of ink watercolor style of painting and art that uses both water as well as brushes. However, 
the Asian art technique has signi.cant differences with Western watercolor styles in both technique and 
media. In Asian art, the artist is not so much concerned with an exact or literal of the objects or the 
scene. Instead the interest is to express only the essence of the ob­jects in the scene. Hence, Asian 
style ink art normally consists of only a few simple strokes on the paper that are intended to convey 
the artists heart-felt feelings regarding the object being painted. In fact in Asian art simplicity is 
a key concept, and feelings are expressed by the speed, placement, pressure, and movement of the brush, 
as well as the shading of the brush strokes [Wucius 1991]. 2 Algorithm The main difference between our 
proposed algorithm and other proposed methods for non-photo realistic rendering is that in our method 
the output image objects are not in the same position as what they have in the input image. Because in 
the Asian painting, the vertical line shows the depth in the image. Also the objects po­sition may change 
horizontally according to the symmetry in the output painting. Based on this fact, all image objects 
must be sepa­rated .rst and then be recomposed. The four steps of our proposed algorithm are as follows: 
1-Image Object Segmentation: The propose of this step is to sperate the objects in the input image from 
each other. To perform this task, we assume that the input image is an up-right front view image that 
where the sky is on the top and earth on the bottom. For segmenting the image objects, at .rst we partition 
the input image into small grids with a region growing based method and then these grids are merge to 
each other using histogram thresholding on image features (hue, fractal dimension). This part needs user 
assistant to merge the result partitions as separated objects in the image and arrange them in vertical 
order based on their depth. 2-Filling the Occluded Parts: Since in the image some of the ob­jects are 
in front of the other objects, after object segmentation there will be some empty (hole) area in the 
background object which must be .lled with the suitable data for each object. Since the amount of lost 
data is comparable with the total information that we have from each object, the general interpolation 
methods are not applicable in this case. Here we apply a kind of fuzzy averag­ing method to .ll the missed 
part, 3-Rendering: Normally Chinese art utilizes sharper de.ned lines for areas of importance and washes 
for not so important areas. Therefore the main edges and the boundaries have highest degree of *e-mail: 
eleff@nus.edu.sg e-mail:adriancheok@nus.edu.sg e-mail:paul@codedpixels.org Figure 1: From left to right: 
the test photo, the segmentation results, the result of .lling the occluded parts, and the .nal painting 
output. importance and they should be sharpened and drawn clearly in the image. It is also found that 
mountains are down faded in the Chi­nese painting style [Wucius 1991]. We also change luminance level 
on the rendering result to form harmony on the luminance scale. For the rendering of the brush strokes 
we apply the algorithm of Strassmann [Strassmann 1986]. We also .ltered the object pixels colors to make 
a smoothed version of the object and then we com­bine both the brush stroke image and the smoothed image 
together to have a rendered image. 4-Composition: After redering all image objects, we must ar­range 
and recompose them to have Asian like painting output. We perform this task with arranging elements of 
the painting by ana­lyzing dark areas in the composition, to have balance in term of the weight of darkness 
in the .nal image. We applied our algorithm on different test images. Figure 1 shows an example of our 
proposed method in providing Asian style painting from a natural scene. As can be seen in this .gure 
the image is segmented into .ve objects: earth, home, trees, mountain and sky.After object segmentation, 
we apply our .lling method to .ll the occluded parts (the hole inside the trees because of the home and 
the empty parts in mountain caused by the trees ). As can be seen from the segmented objects, there are 
quite large areas in each object that must be .lled which can not be achieved by previous proposed methods. 
The .lling outputs of our system are shown in Figure 1. As it seen, the result of our method is promising 
since the .lled occluded parts have almost the same nature of the other parts for each object. After 
that each object is rendered using our redering mechanism and we apply the composition step to gather 
the objects in the .nal painting result. References STRASSMANN, S. 1986. Hairy brushes. In Proceedings 
of the 13th annual conference on Computer graphics and interactive techniques, ACM Press, 225 232. WUCIUS, 
W. 1991. The Tao of Chinese Landscape Painting. New York Design Press. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965458</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A new form of traditional art]]></title>
		<subtitle><![CDATA[visual simulation of Chinese shadow play]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965458</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965458</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653971</person_id>
				<author_profile_id><![CDATA[81100415711]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yi-Bo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fudan University, Shanghai, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653838</person_id>
				<author_profile_id><![CDATA[81100479267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chen-Jia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fudan University, Shanghai, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653881</person_id>
				<author_profile_id><![CDATA[81342511028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[I.]]></first_name>
				<middle_name><![CDATA[Fan]]></middle_name>
				<last_name><![CDATA[Shen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fudan University, Shanghai, China]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15032496</person_id>
				<author_profile_id><![CDATA[81452595774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kwan-Liu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P412738</person_id>
				<author_profile_id><![CDATA[81100258597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Aleksander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stompel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Davis]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W. 2001. Realistic Image Synthesis using Photon Mapping. A K Peters, Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A New Form of Traditional Art Visual Simulation of Chinese Shadow Play Yi-Bo Zhu Chen-Jia Li I-Fan 
Shen * Kwan-Liu Ma Aleksander Stompel Fudan University, Shanghai, China University of California at 
Davis 1 Introduction This sketch presents a set of techniques developed for the simula­tion of Chinese 
Shadow Play. The objective is to turn traditional shadow shows into electronic forms such that they become 
more accessible and be preserved. Computer models of the puppets are created in a semiautomatic process. 
To correctly capture the gleam­ing shadowing effect, a rendering technique based on the photon mapping 
method [Jensen 2001] has been developed. Our initial ef­fort has resulted in an animation that is composed 
of excerpts of a famous Chinese folk story. In the animation, lighting effects and delicate motions close 
to a real show are featured. 2 About Shadow Play With a history of two thousand years, Chinese Shadow 
Play is a splendid art, combined with delicate hand craftsmanship and folk drama. Leather silhouette, 
including puppets and scenes, are man­ually carved and painted by folk craftsmen. Each puppet is made 
of several parts and linked by joints. In a shadow play, a white curtain is hung in the front. The light 
from several spotlights goes through the semitransparent leather-made objects and projects them onto 
the screen. Audience on the other side watch the performance of these colored shadows of puppets, which 
are manipulated by performers. For a .ghting scene, the action of the puppets can be very dramatic, sometimes 
supplemented with special lighting effect. Performance is also accompanied by vocal and musical performance. 
 Behind the screen of real shadow shows. 3 Shadow Play Modeling Because of the lively contours and exquisite 
carving designs of leather silhouette, modeling of the puppets is a complicated pro­cess. A physical 
model of the puppet is .rst carefully digitally photographed into high-resolution images. The images 
are then enhanced with digital image processing techniques for subsequent modeling operations. The edges 
in the images are approximated with Bezier spline curves and eventually a triangular mesh is cre­ated. 
Meanwhile, numerous experiments and comparisons were made between the real puppets and computer models 
for appropri­ately assigning material properties in order to simulate the simple but .orid colors used 
for rendering. A texture image represention of each puppet would be easier to generate but the image 
resolu­tion, and thus the corresponding storage requirement, needed for rendering high-quality, close-up 
views would be too high. *yfshen@fudan.ac.cn ma@cs.ucdavis.edu stompel@cs.ucdavis.edu To achieve photorealistic 
effects we use photon mapping com­bined with traditional Monte Carlo ray-tracing. Due to the nature of 
our work various optimizations and simpli.cations are possible. Since the puppets are .at, a 2-d polygonal 
hash tables can be used to replace the whole parts of the puppet geometry with single poly­gons for quick 
intersection tests. Once the intersection point has been found the hash table is searched and only very 
limited number of ray-triangle intersection tests are needed to determine the .nal intersected triangle. 
As each part of the puppet could have up to 10000 triangles, such an optimization has resulted in tremendous 
speedup. In addition, because the screen is well de.ned and its properties well know, a trivial importance 
map could be used in the process of emitting the photons. Rather than using k-d trees, again a 2-d hash 
table can be used to store the photons on the screen. We ig­nored all the other photons as they would 
not contribute to the .nal image. With all these optimizations the original rendering time has been reduced 
from hours to seconds for small images, and to a few minutes for high-resolution images. An animation 
has been made by using our custom-made software through manual generation of key frames, and interpolation 
for in-between frames.  4 The Animation The content of the animation is based on the famous Chinese 
folk story The Butter.y Lovers , also known as the Chinese Romeo and Juliet. The story is about two young 
people, Liang Shanbo and Zhu Yingtai, who were classmates and fell in love with each other. (Zhu who 
is a girl disguised herself as a boy to receive education as man did.) However, social prejudice prevented 
them from being together. Liang pined and died of melancholy. His death drove Zhu into despair, upon 
which she killed herself at Liang s tomb. Their spirits turned into a pair of butter.ies, .uttering and 
dancing among the .owers. This project represents our beginning effort to preserve tra­ditional Chinese 
art in electronic forms. The full animation has four scenes, excerpts of which can be downloaded from 
http://www.cs.ucdavis.edu/ ma/ShadowShow. Further work will fo­cus on improving the modeling and animation 
process, lowering the rendering cost, and augmenting scenes with special lighting effects. We also plan 
to create web-based interactive shows for children.  Frames from the animation.  References JENSEN, 
H. W. 2001. Realistic Image Synthesis using Photon Mapping.A K Peters, Ltd. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965456</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Effective toon ink rendering for episodic television]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965456</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965456</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653874</person_id>
				<author_profile_id><![CDATA[81100596080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farrell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14045708</person_id>
				<author_profile_id><![CDATA[81310488522]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taylor-Hell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39049652</person_id>
				<author_profile_id><![CDATA[81100584369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[F.]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Fracchia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Effective Toon Ink Rendering for Episodic Television Gordon Farrell Julia Taylor-Hell F. David Fracchia 
Mainframe Entertainment Inc. Mainframe Entertainment Inc. Mainframe Entertainment Inc. gord@reboot.com 
juliath@cpsc.ucalgary.ca drdude@reboot.com 1 Introduction When producing episodic CGI television series, 
time and budgetary restrictions require that renders be both fast and as error-free as possible. Faced 
with series that demanded a cartoon­style look, we first evaluated existing technology. Our primary renderer 
is mental images ray tracer, mental ray. Applicable commercial toon shaders performed well in simple 
cases, but were far too slow and offered too few artistic controls and rendering options. As a result 
we developed a toon ink shader fast, effective, and flexible enough to meet all of our production needs. 
The sections to follow outline several rules and methods we borrowed and developed in order to maximize 
flexibility and accuracy while minimizing render time. 2 Flexibility Truly effective toon ink shading 
is as much about the rules for ink placement and appearance than about the technology used to draw the 
ink. We began with rules found in any good ink shader: distance- and curvature-based sensitivity, sensitivity 
fading based on the distance from the camera, ink based on materials or color contrast, ink around transparency 
maps, and line thickness varying with image resolution and camera field of view. However, we also required 
ink to vary in color within the same scene, and in some cases on different parts of the same model; for 
example, a character's hairline, lips, face, and shirt collar. Using commercial shaders, the ink color 
used would be that of the surface first intersected, resulting in discontinuous contours and flickering. 
Increased model resolution and antialiasing settings reduced the error but were impossibly impractical. 
Our solution was to add priority settings which allowed the artist to specify which rules took precedence 
when the difference in the distance to surfaces was less than a specified delta value (Figure 1). Our 
shader was also required to handle semi-transparent objects. Ink color must be affected by that of a 
foreground object or the illusion that the contour is part of the scene is shattered. We blended the 
color appropriately as it passed behind any number of semi-transparent objects (Figure1). Furthermore, 
unlike the commercial shaders we tested, we placed nearly all of the controls in a material shader instead 
of a camera/lens shader so that rules could differ between objects in a scene (this proved invaluable). 
 3 Accuracy The biggest challenge faced by any contour shader is achieving accurate, solid, animated 
ink lines. In many cases our ink lines turned out just fine, but in some cases surface orientation would 
generate values that teetered on the edge of the curvature rule, resulting in a broken contour which 
flickered as the model was animated. Increasing model resolution to the point where polygons were pixel-sized 
would solve the problem, but this was too impractical for episodic television. Clever model building 
methods greatly reduced the scope of the problem, but frame error rates of 50-90% were common, which 
would have crippled production. Our shader provided two solutions. First, it based antialiasing on changes 
in curvature as well as color. Denser Copyright held by the author  Figure 2. Left: broken ink lines 
due to model resolution; Right: results of heuristic connection algorithm  sampling at critical points 
on the surface was often enough to eliminate flickering. Second, we applied heuristics to the results, 
connecting broken segments or removing isolated dots. Fine tuning the parameters has reduced frame error 
rates to within 5% (Figure 2). 4 Speed As the contour shader draws ink lines as a post-process rather 
than the point-sampling techniques used by the commercial shaders we evaluated, render times were much 
faster. In tests, our shader rendered over three times faster, and was more accurate/robust. 5 Results 
and Future Work The shader has proven very successful on Spider-Man: The Animated Series and Hot Wheels: 
Highway 35. We are currently extending the shader to better handle color blending from reflections, more 
texture map controls, and varying ink line styles. Acknowledgements Many thanks to Tim Belsher, Martin 
Talbot, and Sonja Struben for contributing to the development of this technology. &#38;#169; 2003 API. 
All rights reserved. SPIDER-MANCharacter TM &#38; &#38;#169; 2003 Marvel Characters, Inc.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1077121</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A Non-Photorealistic Camera: Detecting Silhouettes With Multi-Flash]]></title>
		<subtitle><![CDATA[A method for capturing scenes that pose a problem for traditional cameras (low-contrast, geometrically complex, or action scenes). The resulting images are simpler and easier to understand for illustrating complex mechanical parts, image simplification, image resizing, non-photorealistic rendering, and video surveillance]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.1077121</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1077121</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP40022816</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mitsubishi Electric Research Laboratories (MERL)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14165089</person_id>
				<author_profile_id><![CDATA[81100472930]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jingyi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Massachusetts Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24020438</person_id>
				<author_profile_id><![CDATA[81100546195]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ilie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965459</section_id>
		<sort_key>13</sort_key>
		<section_seq_no>13</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Video techniques]]></section_title>
		<section_page_from>13</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14034439</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trinity College Dublin]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965460</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Acquisition of large-scale surface light fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965460</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965460</url>
		<abstract>
			<par><![CDATA[This sketch reports the development of an acquisition methodology to acquire high-resolution surface light fields of an office-size environment.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP80029982</person_id>
				<author_profile_id><![CDATA[81423593965]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wei-Chao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVidia Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P168880</person_id>
				<author_profile_id><![CDATA[81100368207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lars]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nyland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P20148</person_id>
				<author_profile_id><![CDATA[81100264426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Anselmo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lastra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43136082</person_id>
				<author_profile_id><![CDATA[81339500019]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Henry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fuchs]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina at Chapel Hill]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>936241</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[W.-C. Chen. Light Field Mapping: Efficient Representation of Surface Light Fields. PhD thesis, Dept. of Computer Science, UNC-Chapel Hill, May 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383309</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. Buehler et al. Unstructured Lumigraph Rendering. In SIGGRAPH 2001, pages 425--432, August 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344925</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. N. Wood et al. Surface Light Fields for 3D Photography. In SIGGRAPH 2000, July 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. E. Debevec et al. Efficient View-Dependent Image-Based Rendering with Projective Texture-Mapping. In Eurographics Rendering Workshop 1998, June 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566601</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[W.-C. Chen et al. Light Field Mapping: Efficient Representation and Hardware Rendering of Surface Light Fields. In SIGGRAPH 2002, July 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Acquisition of Large-Scale Surface Light Fields Wei-Chao Chen * Lars Nyland Anselmo Lastra Henry Fuchs 
NVidia Corporation University of North Carolina at Chapel Hill Abstract: This sketch reports the development 
of an acquisition methodology to acquire high-resolution surface light .elds of an of.ce-size environment. 
1 Introduction In recent years we have witnessed exciting new research in image­based rendering and modeling 
that brings us closer to the realization of 3D photography, e.g.,[4; 3; 2]. Despite the tremendous progress, 
however, these techniques have not demonstrated the combination of real, high-resolution scenes with 
unconstrained camera loca­tions, and these scenes are often limited to outside-looking-in con­.guration 
such as buildings and statues. With recent techniques, we are capable of rendering much more complicated 
models than ever before. Lacking suitable datasets to stress our rendering algorithms, we set forth to 
develop a complete pipeline for acquiring surface light .elds beyond the desktop scale. Our acquisition 
system uses off-the-shelf hardware components and is applicable to most static environments.  2 Methodology 
Acquisition To date, large-scale fully-automatic scanning re­mains an active research topic. In our system, 
we perform acqui­sition planning and part of the registration with human assistance. It consists of a 
commercial laser range.nder1 (Figure 1,left) and an off-the-shelf digital camera. The two-step acquisition 
process starts with the acquisition and production of an uni.ed geometric model. We then acquire color 
photographs and register the photographs to the geometric model. The laser range.nder acquires depth 
images on a spherical coor­dinate system. For the Of.ce scene, we scan seven panoramic depth images, 
each containing approximately 8 million depth samples (Figure 1,center). The depth images are registered 
together using a commercial implementation of Iterative Closest Point (ICP) algo­rithm2. We then merge 
the geometry from individual depth scans into a uni.ed geometric model and simplify it. For the color 
infor­mation, we take photographs at grid locations in the environment. Because the surfaces are quite 
diffuse, 100 wide-angle images cover most of the surfaces several times. We calibrate the camera and 
re­move nonlinear distortions from the photographs (Figure 1, right). We then compute a local camera 
pose for each photograph by using manually-selected correspondences between the photograph and a depth 
image. The global camera pose is equivalent to transforming the local camera pose from depth image coordinate 
to that of the uni.ed geometry. Please refer to [1] for details on the process. Compression and Rendering 
The size of the raw dataset is on the order of several GBs. To render it ef.ciently, we choose our *e-mail: 
ciao@cs.unc.edu (Research done while at UNC-Chapel Hill) e-mail: {nyland,lastra,fuchs}@cs.unc.edu 1DeltaSphere 
3000, 3rd Tech Corporation, North Carolina, USA. 2Polyworks 6.0, InnovMetrics Incorporated, Quebec, Canada 
  Figure 1: Left: Range.nder. Center: Partial depth image (red: low­con.dence samples). Right: Photograph 
of the Of.ce.  Figure 2: Of.ce rendered with 3-term PCA+Texture Compression. Its physical dimension 
is 15 ft(W ) × 10 ft(D) × 8 ft(H). work[5] because of its compactness and applicability to hardware­accelerated 
rendering. The algorithm process uses multi-pass texture-mapping during decoding. To limit the size of 
light .eld texture maps, we partition the geometric model with breadth-.rst search and generate light 
.eld maps for each partition indepen­dently. We found this approach is applicable to geometric culling 
algorithms without introducing a texture thrashing problem. Fig­ure 2 shows the synthesized Of.ce scene. 
This model can be ren­dered at highly interactive rates on a PC with commodity graphics.  3 Discussion 
and Conclusion This sketch reports our .rst step in in large-scale surface light .eld acquisition. Although 
the process still requires some human inter­vention, we believe this is one of the .rst successful experiments 
of similar kind. With a proper tracking device, autonomous acquisi­tion can be realized in a very near 
future, and image-based models will be a viable alternative for applications such as interactive walk­throughs 
and virtual presence. References [1] W.-C. Chen. Light Field Mapping: Ef.cient Representation of Surface 
Light Fields. PhD thesis, Dept. of Computer Science, UNC-Chapel Hill, May 2002. [2] C. Buehler et al. 
Unstructured Lumigraph Rendering. In SIGGRAPH 2001, pages 425 432, August 2001. [3] D. N. Wood et al. 
Surface Light Fields for 3D Photography. In SIGGRAPH 2000, July 2000. [4] P. E. Debevec et al. Ef.cient 
View-Dependent Image-Based Rendering with Pro­jective Texture-Mapping. In Eurographics Rendering Workshop 
1998, June 1998. [5] W.-C. Chen et al. Light Field Mapping: Ef.cient Representation and Hardware Rendering 
of Surface Light Fields. In SIGGRAPH 2002, July 2002. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965463</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Live 3D video in soccer stadium]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965463</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965463</url>
		<abstract>
			<par><![CDATA[A live 3D video system in a real soccer stadium is presented. Players on the pitch are represented with a simplified 3D model. The model is reconstructed from multiple videos obtained by nine CCD cameras surrounding the pitch. Observers can watch the game from arbitrary viewing position. By using real textures of the players, realistic video presentation is possible. All processes are fully automatic and real time. The system can transmit live 3D video to distant places.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP36026722</person_id>
				<author_profile_id><![CDATA[81100308728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takayoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P529389</person_id>
				<author_profile_id><![CDATA[81100515664]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Itaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitahara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14058539</person_id>
				<author_profile_id><![CDATA[81100136649]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohta]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Live 3D Video in Soccer Stadium  Takayoshi Koyama University of Tsukuba koyama@image.esys.tsukuba.ac.jp 
Itaru Kitahara University of Tsukuba kitahara@computer.org Yuichi Ohta University of Tsukuba ohta@acm.org 
  Abstract A live 3D video system in a real soccer stadium is presented. Players on the pitch are 
represented with a simplified 3D model. The model is reconstructed from multiple videos obtained by 
nine CCD cameras surrounding the pitch. Observers can watch the game from arbitrary viewing position. 
 By using real textures of the players, realistic video presentation is possible. All processes are 
fully automatic and real time. The system can transmit live 3D video to distant places. 1 Representation 
of Players The simplified 3D model is similar to the billboarding technique. A vertical rectangle plane 
located at the player s position on the pitch represents the 3D shape of each player, as illustrated 
in Figure 1. Live textures selected from the videos obtained from eight CCD cameras surrounding the 
pitch are mapped to the plane. The texture of each player is segmented from the background by using 
the background subtraction technique. The position of each player is extracted from the video obtained 
by the CCD camera looking down from the ceiling of the stadium. All CCD cameras are completely synchronized 
by supplying an external sync signal. The overlapping of textures of two players in a video is expected 
from the 3D positions of the players. In such a case, a stereo matching technique by using a video obtained 
from adjacent camera, as illustrated in Figure 2, segments the texture of the foreground player. The 
texture of the occluded background player is selected from another video without occlusion. 2 Transmission 
of Live 3D Video The 3D positions and the textures of all players are transmitted to the observer s 
PC as stream data via Ethernet. A CG model of the stadium is separately supplied to the observer s PC 
before the game start, as illustrated in Figure 3. Synthesis of 3D video is done at the observer s PC 
based on the viewing position specified by the observer. Multiple observers can share the transmitted 
data and can enjoy their own views. 3 A Live 3D Video System A prototype of live 3D video system has 
been made and tested at a real soccer stadium, Oita Stadium. Eight CCD cameras surrounding the pitch 
obtain the textures of players, as illustrated in Figure 4. One camera looking down from the ceiling 
is used to extract the positions of players. Its height is about 60 meters from the pitch. A separate 
PC is assigned to each camera to digitize the video and to perform image processing. The 3D positions 
and the textures of all players are gathered at a data server and transmitted to observer s PCs. All 
PCs are connected via Ethernet, 1000Mbps and 100Mbps. Figure 5 shows some shots of synthesized live 3D 
video. Nevertheless a simplified 3D model represents each player, the relative positions of the players 
and the occlusion between the players reproduced in the 3D video present realistic 3D visual sense to 
the observer. The frame rate of our current system is about 26fps, from video digitizing to 3D video 
display including network transmission. Overlapped Texture Another View Figure 2. Extraction of Foreground 
Player s Texture Real Players CG Stadium Figure 1. Simplified 3D Model Virtualized World Real World 
 Figure 3. Synthesizing Real Players and CG Stadium Figure 4. Capturing Multiple Videos Figure 5. Shots 
from Live 3D Video 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965462</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Real-time view morphing of video streams]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965462</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965462</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653900</person_id>
				<author_profile_id><![CDATA[81100495816]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Timm]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois at Chicago, GE Medical Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237196</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{SD96} SEITZ, S. and DYER, C. 1996, "View Morphing". Proceedings of SIGGRAPH 1996, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>964967</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{Will83} WILLIAMS, L. "Pyramidal Parametrics", Computer Graphics (Proc. SIGGRAPH 83), Vol. 17, No. 3, July 1983. 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965461</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Robust depth estimation from multiple video streams for dynamic light field rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965461</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965461</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653829</person_id>
				<author_profile_id><![CDATA[81100412961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goldl&#252;cke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P544237</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max-Planck-Institut f&#252;r Informatik, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1965930</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[GOLDL&#220;CKE, B. AND MAGNOR, M. 2003. Joint 3D-reconstruction and background separation in multiple views using graph cuts. To appear in Proceedings of CVPR.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GORTLER, S., GRZESZCZUK, R., SZELISKI, R., AND COHEN, M. 1996. The lumigraph. Proc. ACM Conference on Computer Graphics (SIGGRAPH'96), New Orleans, USA (Aug.), 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>756415</ref_obj_id>
				<ref_obj_pid>645317</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[KOLMOGOROV, V., AND ZABIH, R. 2002. Multi-camera scene reconstruction via graph cuts. In ECCV (3), 82--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965464</section_id>
		<sort_key>14</sort_key>
		<section_seq_no>14</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art installations]]></section_title>
		<section_page_from>14</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653912</person_id>
				<author_profile_id><![CDATA[81100487195]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Linda]]></first_name>
				<middle_name><![CDATA[Lauro]]></middle_name>
				<last_name><![CDATA[Lazin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pratt Institute]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965465</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA["Falling over you" interactive wall]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965465</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965465</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39041780</person_id>
				<author_profile_id><![CDATA[81100415993]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pape]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University at Buffalo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14187573</person_id>
				<author_profile_id><![CDATA[81100539989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Josephine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anstey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University at Buffalo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Falling Over You Interactive Wall Dave Pape:, Josephine Anstey University at Buffalo 1 Introduction 
 The Falling Over You Interactive Wall is a large scale, interactive video environment. It was set up 
at the Albright Knox Art Gallery in conjunction with an exhibition of paintings Frank Moore: Green Thumb 
in a Dark Eden (Feb-April 2003). Looking through the door, the Interactive Wall is merely a large screen 
in an empty room showing video footage of Niagara Falls. If you enter the room and move about in front 
of the screen, scientists in white coats, buf­falo, and eye-balls tumble over the falls; mixed swarms 
of butte­.ies, snow.akes and chemical symbols drift across with the spray; hybrid telephone/cornstalks 
and .ower pistil/hypodermic needles grow up against the current. If you get closer, the video images 
part around you and reveal a painting beneath. The visuals are accom­panied by a sound recording of Niagara 
Falls. The interactive wall is a meditation, an interpretation, and a point of access to Frank Moore 
s paintings, which deal with environmental and healthcare activism. Moore s Niagara Falls series was 
the initial inspiration for the wall s design; the graphic elements are taken from his paint­ings, and 
the painting behind the virtual Falls is Niagara .  2 Technical Implementation The installation uses 
video footage of the falls, played back on a computer and projected on a 20 x 9 screen. The footage consists 
of .ve seamless loops, ranging from 7 to 25 seconds each. They were converted into series of 512x512 
compressed textures, so that the full 2100 frames can be preloaded into RAM and played back at a steady 
30 frames per second. As visitors enter the room, infra-red cameras detect their motion, and trigger 
changes in the display. The different results are keyed to regions of the room. The objects that appear, 
triggered by visitors motions, are 109 small elements from digitized copies of Moore s paintings, with 
added alpha channels. * e-mail: dave.pape@acm.org When a visitor is only a few feet from the screen 
she occludes the projection of the video. A camera pointed at the screen detects the relative darkness 
of the person and her shadow on the screen, and the video water appears to bounce off her detected position 
(treated as a set of large blobs), uncovering bits of the underlying painting. The parting of the video 
imagery is accomplished by making it into a particle system rather than displaying the video texture 
on a single large square, it is split into 1500 small squares. These squares are continually falling 
down the display, but normally are lined up evenly, to present a single image. When a visitor gets in 
the way, the video particles bounce off her, fracturing the image. The software was developed in three 
separate parts graph­ics, motion detection, and shadow detection. The video input parts run as daemons 
that send their results to the graphics program via sockets. The programs were written in a mix of C 
and Python; by wrapping the video-tracking code into Python modules, we were able to prototype and plug 
the system together more rapidly. The installation runs on a single Linux PC, with a dual-headed graphics 
card and two TV input cards. It uses two infra-red cam­eras, and two LCD projectors. It is designed to 
run autonomously in the museum environment. Guards turn the projectors on and off for museum hours, and 
the computer turns itself off at night and reboots itself in the morning.  3 Conclusion Over the years 
the Albright Knox has created a series of interactive rooms to accompany exhibits. The rooms are explicitly 
aimed at families but visitors of all ages enter them. The Falling Over You Interactive Wall has no instructions 
and the interaction is unencum­bered, so it is possible that some visitors never realize that their presence 
affects the visuals. Observing visitors in the room, it is ap­parent that young adults and children are 
quicker than adults to per­ceive and experiment with the interactive potential of the wall. Chil­dren 
in particular marvel as they discover that their actions trigger visual responses. Short people jump 
up close to the screen trying to make larger, taller shadows so that they can see more of the hid­den 
painting. Methodical people work out exactly where to stand to trigger the events they liked. The interactive 
wall is a concise demonstration of one of Frank Moore s main themes: our interac­tions with the world 
and the ways we impact the greater landscape. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965466</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Hydrogen wishes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965466</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965466</url>
		<abstract>
			<par><![CDATA[Hydrogen Wishes, presented at MIT's Center for Advanced Visual Studies, explores the themes of wishes and peace. It dramatizes the intimacy and power of transforming one's breath and vocalized wishes into a floating sphere, a bubble charged with hydrogen. The floating bubble represents transitory anticipation as a wish is sent on its trajectory toward fulfillment. Light, heat sensors, microphones, projected imagery, hydrogen and ordinary soap bubbles come together in this exploration of human aspiration. As in our lives, many wishes escape, but many others are catalyzed by the heat of the candle and become ethereal. The fulfilled wishes then become living artifacts within projected photographs of Earth cities as seen from outer space.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP61023271</person_id>
				<author_profile_id><![CDATA[81366590839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Winslow]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burleson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P429861</person_id>
				<author_profile_id><![CDATA[81100431132]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nemirovsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35022932</person_id>
				<author_profile_id><![CDATA[81100024075]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Overholt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[<u>http://www.burningman.com/</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Manzoni, Piero; 1959-1961; the breath of the artist, <u>http://www.artinvest2000.com/manzoni_english.htm</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hydrogen Wishes Winslow Burleson, Paul Nemirovsky, Dan Overholt MIT Media Lab Abstract Hydrogen Wishes, 
presented at MIT s Center for Advanced Visual Studies, explores the themes of wishes and peace. It dramatizes 
the intimacy and power of transforming one s breath and vocalized wishes into a floating sphere, a bubble 
charged with hydrogen. The floating bubble represents transitory anticipation as a wish is sent on its 
trajectory toward fulfillment. Light, heat sensors, microphones, projected imagery, hydrogen and ordinary 
soap bubbles come together in this exploration of human aspiration. As in our lives, many wishes escape, 
but many others are catalyzed by the heat of the candle and become ethereal. The fulfilled wishes then 
become living artifacts within projected photographs of Earth cities as seen from outer space. 1 Introduction 
Fire and breath have been important media elements of the contemporary arts (e.g. Burning Man [1]). The 
Italian conceptual artist Piero Manzoni produced a series entitled The Breath of the Artist [2]. In Hydrogen 
Wishes we capture the breath of the visitor and use the serenity of candle flame as an activating agent. 
Referencing the engaging process of making a wish, we allude to the power of dreams, the mystique of 
a fire breathing dragon as a granter of wishes. The wishes granted are kept and contextualized, both 
visually and acoustically and presented to subsequent viewers.  Fig. 1. The installation: candles, tube, 
and the projected city. the wish is recorded, a bevy of soap bubbles emerge from the far end of the 
tube and float up to the candles. Along their way they cast translucent circular shadows upon the city, 
augmented by ongoing transformations of the projected image. Some of the bubbles continue past the candles, 
but others are energized by the candles and explode in a small burst [Fig 3 &#38; 4]. Coinciding with 
the burst, the recorded wishes are played back for the visitor, accompanied by music and transformed 
imagery of the city. The new wish becomes a part of a colony of wishes visually presented as small flickering 
points wandering around the projected city landscapes. The overall expressive experience is that of celebration. 
3 System The hardware consists of a tube into which audience members whisper their wishes. The tube is 
supported by a pedestal containing a computer and a small tank of hydrogen gas with a solenoid regulator. 
A sensor composed of a thermistor and a microphone within the tube measures the presence of a visitor 
s breath and triggers the creation of a hydrogen soap bubble. The bubbles rise rapidly from the far end 
of the tube to the candle wreath. As the bubbles reach the candles, some of them burst. These bubbles 
release a puff of flame and a soft but energetic sound. A phototransistor, supported by the candle wreath, 
detects the light produced by the exploding bubble. The computer responds by creating a digital version 
of the bubble within the image of the city. These digital bubbles traverse the landscape of the available 
cities; their messages are heard in conjunction with the release of new wishes. In summary, the computer 
takes on the tasks of noting when the user makes a wish; recording the wish; creating hydrogen bubbles; 
generating image transformations, sensing when bubbles explode; and juxtaposing the new wishes with the 
previously recorded wishes within the context of the Fig. 3 &#38; 4. Bubble floating up and bursting. 
References 1. http://www.burningman.com/ 2. Manzoni, Piero; 1959-1961; the breath of the artist,  http://www.artinvest2000.com/manzoni_english.htm 
{win, pauln, dano} @ media.mit.edu Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965467</section_id>
		<sort_key>15</sort_key>
		<section_seq_no>15</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[The Matrix revealed]]></section_title>
		<section_page_from>15</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP18000965</person_id>
				<author_profile_id><![CDATA[81100257079]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Townsend]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965468</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Measured BRDF in film production]]></title>
		<subtitle><![CDATA[realistic cloth appearance for "The Matrix Reloaded"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965468</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965468</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18001553</person_id>
				<author_profile_id><![CDATA[81100409357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borshukov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965471</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Generating realistic human hair for "The Matrix Reloaded"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965471</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965471</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP24022602</person_id>
				<author_profile_id><![CDATA[81322501803]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tadao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mihashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653843</person_id>
				<author_profile_id><![CDATA[81100624763]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tempelaar-Lietz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18001553</person_id>
				<author_profile_id><![CDATA[81100409357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borshukov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generating Realistic Human Hair for The Matrix Reloaded Tadao Mihashi, Christina Tempelaar-Lietz, George 
Borshukov ESC Entertainment Introduction In recent years, there have been a few successful applications 
of realistic computer generated hair in the entertainment industry. For The Matrix Reloaded we had to 
face new challenges. First, we had to handle both close ups and scenes with hundreds of human characters 
with hair. The task was further complicated by the fact that the CG hair needed to exactly match the 
styling of familiar heroes from the film: Agent Smith and Neo. Also because of the rendering methods 
we chose for our virtual actors the hair solution needed to work in a ray tracing context with many lights. 
Our in­house hair styling tool together with some key rendering techniques made it all possible. Styling 
Hair During the early stages of hair R&#38;D for The Matrix Reloaded we tried to use Maya Fur as our 
hair styling tool. Eventually we decided against it, as it did not provide us with enough freedom to 
match the detailed hairstyles of the principal actors. Alternatively, we developed an in-house hair styling 
tool called Gossamer. Gossamer is a set of plugins and scripts for Alias|Wavefront Maya. It has all the 
parameters of Maya Fur, but also provides functionality that allows the user to directly move control 
points on "guide" hairs which are then used to generate many interpolated hairs. The interpolated hair 
is rendered in real-time using OpenGL in Maya (see Fig. 1). This allows artists to see the hairstyle 
as it would appear in the final rendered image while they are working. Gossamer also provides the ability 
to clump areas of hair for further hairstyle control. (We would like to acknowledge Eric Soulvie for 
his early development work on the Gossamer tool.) Rendering Hair Hair objects are generated procedurally 
during rendering time from a hairstyle description generated by Gossamer. In the beginning, each hair 
was generated as a very thin NURBS patch because originally mental ray, our renderer of choice, did not 
support curve rendering. This approach worked well only up to a certain level of complexity. We collaborated 
with mental images to implement a new fast and memory efficient "hair" primitive in the renderer. A couple 
of more techniques are used to reduce the memory usage. One is to subdivide a hair object into partitions. 
Each partition is treated as a separate object with its own bounding box. If a partition is in the back 
of character's head and its bounding box is completely occluded by the head object, hairs in that partition 
will never get generated. The other one is using mental ray's geometry shader object instancing feature. 
It was very effective especially when rendering many Agent Smiths. Rendering hair for ten Smiths was 
virtually the same as rendering hair for one Smith.  Hair Shadow and Lighting Reconstruction With either 
of two commonly used shadowing methods, shadow maps and ray traced shadows, it is difficult to produce 
nice, soft hair shadows. One proven solution to this problem is the deep shadow map, but our real world 
Lighting Reconstruction setups consisted of many light sources. Generating and maintaining a deep shadow 
map for each light would have been a pipeline nightmare. Instead, we use a volumetric approach which 
we call a shadow tree. The shadow tree is an octree representation of the hair's density which is constructed 
during the initialization stage of the render. During rendering, the shadow tree is only visible to shadow 
rays. When a shadow ray intersects the shadow tree's bounding volume, it reads the density value of each 
intersected voxel and uses these values to attenuate the light accordingly. Results Below we show the 
results of our techniques. Fig. 2 and 3 show full CG renderings for Agent Smith and Neo. Fig. 4 is a 
photograph, which demonstrates how close we can match the real hairstyle and look. The image on the bottom 
is a frame from the film. All hair but the one in the middle is computer generated for this 3-d head 
replacement shot. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965472</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Lighting reconstruction for "The Matrix Reloaded"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965472</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965472</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31028947</person_id>
				<author_profile_id><![CDATA[81100137101]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Haarm-Pieter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duiker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lighting Reconstruction for The Matrix Reloaded Haarm-Pieter Duiker* ESC Entertainment Introduction 
The demands of photo-realism required of the effects for The Matrix Reloaded and The Matrix Revolutions 
led us to create a system for directly and accurately reconstructing real world lighting environments. 
The Lighting Reconstruction Toolkit builds on research in the area of Image-based Lighting and extends 
current techniques to enable the reconstruction of lighting that more closely matches the real world. 
 Lighting Reconstruction Lighting Reconstruction has two main stages: Data Acquisition, and Lighting 
Reconstruction. On-set photography is the primary means of real world lighting Data Acquisition. Of the 
on-set photography, multiply exposed images of chrome balls play a central roll. Kodak 20-step grey cards 
and Macbeth color charts play a not as obvious but also central roll. They are photographed for film 
response and color neutrality reference. Color temperature, exposure settings, photogrammetic reference 
images, and set diagrams round out the lot of information gathered. The images and data are used to create 
from each group of chrome ball exposures an extended, or high, dynamic range panorama, i.e. an image 
representing the true color and intensity of all the lighting at a certain point in the real world. The 
chrome ball images serve as the raw data. The Kodak 20-step grey scale images serve as the basis for 
solving for the response function of the film, the key to turning film exposure measurements back in 
to true measurements of intensity. Given a high dynamic range panorama, the Lighting Reconstruction, 
of which there are currently three main modes, can begin. In the first and simplest mode, a panorama 
directly drives the creation of a set of directional lights. The lights represent the integrated color 
and intensity of either key or fill areas of the panorama. The key lights are separated out from the 
fill light portion of the panorama. Roughly equal solid angle sub-regions are integrated over to create 
the fill lights to a user-specified granularity. These lights will not change in any way with respect 
to the location of the object. In the second mode, multiple panoramas, their real world locations, and 
a model of the environment with light source locations allow us to create lighting that varies as the 
key and fill lighting varied in the real world. The input key light locations and sizes are verified 
by triangulating their position from the panoramas. The intensity, color, cone angle, and other parameters 
of a spot light model are then fit to the data in each panorama. The intensity of the fill lighting is 
established by creating a triangulation of the panorama locations. At render time, for each object, an 
interpolation weight for each of the panoramas is established based on the proximity of the object to 
each panorama as well as the proximity of each panorama to the others. An object specific panorama is 
generated using the weights. From this panorama an arbitrary number of directional lights may be generated. 
The third mode of Lighting Reconstruction is based on a new light primitive whose area is dynamically 
subdivided for each object it will light. The light fulfills the criteria that none of its sub-regions 
may be bigger than a solid angle threshold relative to an object being lit. The primitive when used in 
the creation of a simple model of the real world geometry and textured using the panoramas projected 
on to the model, *e-mail: hp@escfx.com replaces the panorama-interpolating scheme presented in the previous 
mode. The primitive allows for better allocation of a limited number of light samples and a better reconstruction 
of the object specific lighting environments.  Figure 1: Row 1 shows two differently exposed images 
of the same chrome ball. Row 2 shows a real image of Keanu Reeves (left) and a version of Keanu Reeves 
(right) rendered using the lighting from above chrome ball. Row 3 shows an image from the Burly Brawl 
sequence. Row 4 shows two differently exposed images of one of the chrome balls used to generate the 
lighting for that sequence. Conclusion The Lighting Reconstruction Toolkit has survived a true production 
environment and was combined with the Universal Capture, Skin, BRDF, Subsurface Scattering, Hair, and 
Cloth Simulation development efforts at ESC to create many of the realistic images seen in the movie. 
Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965469</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Universal capture]]></title>
		<subtitle><![CDATA[image-based facial animation for "The Matrix Reloaded"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965469</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965469</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18001553</person_id>
				<author_profile_id><![CDATA[81100409357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borshukov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P58196</person_id>
				<author_profile_id><![CDATA[81100284377]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Piponi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31032726</person_id>
				<author_profile_id><![CDATA[81322498745]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oystein]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Larsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77035888</person_id>
				<author_profile_id><![CDATA[81409595942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653843</person_id>
				<author_profile_id><![CDATA[81100624763]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tempelaar-Lietz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Universal Capture Image-based Facial Animation for The Matrix Reloaded George Borshukov, Dan Piponi, 
Oystein Larsen, J.P.Lewis, Christina Tempelaar-Lietz ESC Entertainment Introduction The VFX R&#38;D 
stage for The Matrix Reloaded was kicked off in January 2000 with the challenge to create realistic human 
faces. We believed that traditional facial animation approaches like muscle deformers or blend shapes 
would simply never work, both because of the richness of facial movement and because of the human viewer 
s extreme sensitivity to facial nuances. Our task was further complicated as we had to recreate familiar 
actors such as Keanu Reeves and Lawrence Fishburne. Our team had been very successful at applying image-based 
techniques for photorealistic film set/location rendering, so we decided to approach the problem from 
the image-based side again. We wanted to produce a 3-d recording of the real actor's performance and 
be able to play it back from different angles and under different lighting conditions. Just as we can 
extract geometry, texture, or light from images, we are now able to extract movement. Universal Capture 
combines two powerful computer vision techniques: optical flow and photogrammetry. HiDef Capture Setup 
We used a carefully placed array of five synchronized cameras that captured the actor's performance in 
ambient lighting. For the best image quality we deployed a sophisticated arrangement of Sony/Panavision 
HDW-F900 cameras and computer workstations that captured the images in uncompressed digital format straight 
to hard disks at data rates close to 1G/sec. Optical Flow + Photogrammetry We use optical flow to track 
each pixel's motion over time in each camera view. The result of this process is then combined with a 
cyberscan model of a neutral expression of the actor and with photogrammetric reconstruction of the camera 
positions. The algorithm works by projecting a vertex of the model into each of the cameras and then 
tracking the motion of that vertex in 2-d using the optical flow where at each frame the 3-d position 
is estimated using triangulation. The result is an accurate reconstruction of the path of each vertex 
though 3-d space over time. Keyshaping, Adapt, Removing Global Motion Optical flow errors can accumulate 
over time, causing an undesirable drift in the 3-d reconstruction. To minimize the drift we make use 
of reverse optical flow. On this production the problem was eliminated by introducing a manual keyshaping 
step: when the flow error becomes unacceptably large the geometry is manually corrected and the correction 
is then algorithmically propagated to previous frames. The reconstructed motion contains the global "rigid" 
head movement. In order to attach facial performances to CG bodies or blend between different performances 
this movement must be removed. We estimate the rigid transformation using a least squares fit of a neutral 
face and then subtract this motion to obtain the non-rigid deformation.  Texture Map Extraction No believable 
facial rendering can be done without varying the face texture over time. The fact that we did not use 
any markers on the face to assist feature tracking gave us the important advantage that we could combine 
the images from the multiple camera views over time to produce animated seamless UV color maps capturing 
important textural variation across the face, such as the forming of fine wrinkles or changes in color 
due to strain, in high-res detail on each side of the face.  Rendering Although the extracted facial 
animation had most of the motion nuances it lacked the small-scale surface detail like pores and wrinkles. 
We obtained that by using a highly detailed 100-micron scan of the actor s face. The detail is then extracted 
in a bump (displacement) map. Dynamic wrinkles were identified by image processing on the texture maps; 
these are then isolated and layered over the static bump map. We then combine these with image-based 
skin BRDF estimation, subsurface scattering approximation, and real-world lighting reconstruction for 
the highly photorealistic human face renderings below. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965470</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Realistic human face rendering for "The Matrix Reloaded"]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965470</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965470</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP18001553</person_id>
				<author_profile_id><![CDATA[81100409357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Borshukov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP77040773</person_id>
				<author_profile_id><![CDATA[81409595942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESC Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Realistic Human Face Rendering for The Matrix Reloaded George Borshukov and J.P.Lewis ESC Entertainment 
 Introduction The ultimate challenge in photorealistic computer graphics is rendering believable human 
faces. We are trained to study the human face since birth, so our brains are intimately familiar with 
every nuance and detail of what human skin is supposed look like. The challenge of rendering human skin 
is further complicated by some technical issues such as the fact that skin is a highly detailed surface 
with noticeable features in the order of ~100 microns and the fact that skin is translucent. On The Matrix 
Reloaded we had to create completely photorealistic renderings for most of the principal actors including 
Keanu Reeves, Lawrence Fishborne, and Hugo Weaving. Facial Surface Detail The geometry used for our 
rendering was based on a 100-micron resolution scan of a plaster cast mold of the actors faces. Arius3d 
provided the scanning technology. These scans had extremely high polygonal counts (10 million triangles; 
see Fig. 1). To use these models in production and preserve the detail we deployed the following technique. 
A low-res ~5K quad model was constructed using Paraform software. The model was given a UV parameterization 
and then used as a subdivision surface. The high resolution detail was extracted using the lightmapping 
feature of the mental ray renderer combined with custom shaders that performed ray tracing from the low-res 
subdivision surface model to the high-detailed 10M triangle raw scan; the distance difference is stored 
in a displacement map. We applied the low frequency component of this map as displacement; the high frequency 
component was applied using bump mapping. Image-based Derivation of Skin BRDF Our skin BRDF was derived 
using an image-based approach. In Summer 2000 as part of the early stages of Matrix Reloaded R&#38;D 
we had a setup, which consisted of 30 still cameras arranged around the actor s head. Actors were photographed 
illuminated with a series of light sources from different directions (see Fig. 2). The setup was carefully 
color calibrated and photogrammetry was used to precisely reconstruct the camera positions and head placement 
with respect to each camera for each image. The collected image data from each camera was brought into 
a common UV space through reprojection using a cyberscan model of the actor. This convenient space (see 
Fig. 3) allowed us to analyze the skin reflectance properties for many incident and outgoing light directions. 
We derived parameters for an approximate analytical BRDF that consisted of a Lambertian diffuse component 
and a modified Phong-like specular component with a Fresnel-like effect. (We would like to acknowledge 
Matthew Landauer for his contributions to this section).  Subsurface Scattering of Skin As production 
progressed it became increasingly clear that realistic skin rendering couldn t be achieved without subsurface 
scattering simulation. There are a number of published methods for rendering translucent materials however 
they are all fairly complex, require large amounts of CPU power and produce somewhat disappointing results. 
To address this we developed a technique for producing the appearance of subsurface scattering in skin 
that is computationally inexpensive and fairly easy to implement. The result of the diffuse illumination 
reflecting off the face in the camera direction is stored in a 2-d light map (see Fig. 3). We then approximately 
simulate light diffusion in the image domain. To simulate the different mean free path for different 
light colors we vary the diffusion parameters for each color channel. For animations the lightmap needs 
to be computed at every frame, so our technique computes an appropriate lightmap resolution depending 
on the size of the head in frame. For objects like ears where light can pass directly through, we employed 
a more traditional ray tracing approach to achieve the desired translucency effect.  Results The above 
components are combined with our Universal Capture, real world Lighting Reconstruction technologies, 
and a ray tracer such as mental ray to produce the synthetic images in Fig. 5 and 6. For comparison Fig. 
7 shows a photograph of Keanu Reeves (Neo). The bottom image is a fully virtual frame from The Matrix 
Reloaded. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965473</section_id>
		<sort_key>16</sort_key>
		<section_seq_no>16</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware algorithms]]></section_title>
		<section_page_from>16</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653918</person_id>
				<author_profile_id><![CDATA[81100457004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Elendt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Side Effects Software, Inc.]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965475</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The Primal#8482; seas]]></title>
		<subtitle><![CDATA[water on PlayStation&#174;2]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965475</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965475</url>
		<abstract>
			<par><![CDATA[In this sketch we present a variety of water and sea effects seen in the PlayStation&#174;2 game Primal#8482; (released Q1 2003), culminating in a sea surface with a real-time rendered parabolic environment map and refracted undersea view.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653822</person_id>
				<author_profile_id><![CDATA[81100310351]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ostler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Computer Entertainment Europe]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>507109</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LENGYEL, E. Mathematics for 3D Game Programming & Computer Graphics. pp. 327--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HEIDRICH, W. 1999. High-quality Shading and Lighting for Hardware-accelerated Rendering. PhD thesis, University of Erlangen, Computer Graphics Group. pp. 68--75.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Primal Seas Water on PlayStation®2 Andrew Ostler andrew_ostler@scee.net Sony Computer Entertainment 
Europe Abstract In this sketch we present a variety of water and sea effects seen in the PlayStation®2 
game Primal (released Q1 2003), culminating in a sea surface with a real-time rendered parabolic environment 
map and refracted undersea view. 1 Introduction Primal is an epic fantasy adventure, set in four demon 
realms . It was a design goal from the start that these realms should be lushly realised and include 
natural simulation e.g. weather effects, swaying tress, rustling bushes and realistic water effects. 
As lush as these effects needed to be, they also needed to run in real-time (30fps) on a machine which 
is also busy running a game. Another restriction to work within was that of video memory (4MB total, 
including frame and z buffers), a problem especially when using multiple pullout/render passes. 2 Simulation 
Dynamic water surfaces are simulated using a standard discretised 2D wave equation method [Lengyel]. 
Optionally a per-vertex water depth value is stored to cater for shelving edges and reflection from the 
sides of non-rectangular pools. The simulation code is highly optimised using SIMD instructions. The 
sea surface is the only water surface not simulated; rather, it is generated algorithmically from summed 
sinusoids. The generating functions are sampled at a range of detail levels according to a quadtree-based 
frustum pullout. Each quadtree leaf node generates a square heightfield which can be used with the same 
render code as the simulated surfaces. 3 Rendering The bulk of the rendering work is performed on the 
VU1 (vector unit) coprocessor. The heightfield array (of 32-bit floats) is transferred by DMA a row at 
a time, and a corresponding triangle strip is generated and sent to the GPU to be rasterised. Actual 
vertex positions, normals (for lighting) and texture coordinates are all generated from the height data. 
Alpha values are also generated for angle-of-view type effects (based on Fresnel s equation). Reflection 
and refraction effects are achieved by generating an appropriate texture map for the water surface and 
perturbing the texture coordinates appropriately, based on the eye ray and surface normals. Said texture 
map is not precomputed, rather it is rendered each frame from the view below the water surface (for refractive 
effects) or the view reflected in the water surface. These views are generated via the same portal/octree 
system that is used throughout the game for views through doors, mirrors, rift gates etc. A third type 
of water surface uses a fixed texture for its reflection pass, but is also transparent. The underwater 
view is enhanced by first rendering a surface with subtractive transparency (to simulate attenuation 
of light in the water) which also creates a stencil mask for a screen-space blur effect. The blurring 
further enhances the feeling of a water volume rather than simply a transparent surface. This type of 
surface differs from all the others in that its level can be changed at runtime, rather than the over/underwater 
geometry being defined in the modelling. The sea surface effect uses three layers of rendering. Firstly, 
a view to the underwater geometry is generated via a portal and rendered with refraction as above. Then, 
a simple tiling textured layer is drawn, which is the base to which the reflection is added. This base 
layer is made transparent around the camera, both to enable the underwater view to be seen and to mask 
the rectangular edge that comes from the portal rendering system. The reflective layer uses parabolic 
environment mapping [Heidrich]. The environment map is built up as follows. Firstly a static sky texture 
is laid down, rendered offline to match the in­game sky model. Then four views of the landscape are rendered, 
using four cameras angled 45° to the horizontal, with a vertical field-of-view of 90° and aspect ratio 
of v2, arranged at 90° intervals around the vertical. These cameras provide complete coverage of the 
hemisphere required for the environment map. The four views are combined into a single image by mapping 
a triangle from each view to one quarter circle of the map, with an appropriate linear-to-angular mapping. 
The final environment map is applied to a transparent layer which is added to the base sea surface (with 
transparency modulated by Fresnel s equation).  References LENGYEL, E. Mathematics for 3D Game Programming 
&#38; Computer Graphics. pp. 327-42. HEIDRICH, W. 1999. High-quality Shading and Lighting for Hardware-accelerated 
Rendering. PhD thesis, University of Erlangen, Computer Graphics Group. pp. 68-75. Copyright held by 
the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965476</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Implementing RenderMan on the Sony PS2]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965476</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965476</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35023662</person_id>
				<author_profile_id><![CDATA[81100072477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stephenson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bournemouth University, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Implementing RenderMan on the Sony PS2 Ian Stephenson National Centre for Computer Animation Bournemouth 
University, UK istephen@bournemouth.ac.uk 1 Introduction As graphics hardware increases in power, driven 
by low cost con­sumer level products, there is an inevitable blurring between real time, and high end 
production techniques. Games developers are in­creasingly able to use techniques which where previously 
too slow, while producers of animation cannot ignore the powerful custom hardware available for rendering. 
Though newer graphics cards provide greater .exibility, most are still highly customized, providing a 
very ef.cient, but .xed pipeline. The Sony PlayStation 2 by contrast provides the developer with an almost 
entirely soft pipeline. We chose to port our renderer, a fairly standard Micro-Polygon (MP) based implementation 
of RenderMan, to the PS2 Linux plat­form. It was not intended that the results would be real time, rather 
to implement RenderMan as fully as possible, and observe to what degree the hardware could be exploited. 
 2 The Emotion Engine Core The main processor of the PS2 is a 300MHz MIPS R5900. While moderately powerful, 
the performance of the EECore is restricted by its limited memory cache. Accessing memory is a major 
bottleneck. 16K of scratchpad memory is available and this was used for many key data structures, such 
as lighting information, and mesh data. The EECore s main function is to control the other units, and 
performs as little calculation as possible. It performs all front end parsing function, reading in RIB 
.les, and generating data structures for each object read, which are passed immediately to the backend. 
 3 VU0: Geometry The PS2 has 2 vector co-processors. These each have their own memory, and are capable 
of operating independently of the EECore. VU0 has 4K of Code and Data RAM which were used to evaluate 
parametric surfaces. VU0 is also used to measure the size of the micro-polygons generated. The EECore 
loads the parameters for a surface into VU0 s data ram, and asks it to generate a grid. Depending on 
the size of the MP s within that grid, the resulting points are copied to scratch ram. As grid generation, 
and measuring are both performed on the VU, excellent performance is achieved. 4 VU1: Shading Shading 
was implemented as a simple stack-based SIMD machine. VU1 data memory is used to store the stack, and 
push operations transfer values to the VUmem from main memory. This allows the actual calculations to 
be performed by VU1, independently of the core, which can decode the next instruction while the current 
one is still executing. Two separate stacks for .oats and vectors where Figure 1: The PS2 Architecture 
used, allowing vectors to be stored in the correct format for the VU instruction set. An SIMD machine 
would typically only perform operations upon nodes which are currently active. However the cost of testing 
the state of a node is usually higher then performing a calculation particularly as the VU1 unit is 
operating at high speed in parallel with the main processor, and memory access is so critical. The node 
status .ags are therefore only used to control pop operations, which write data back to main memory, 
changing the value of variables. As VU1 has only 16K of data memory, the size of grids was restricted 
to 128 points. Larger grids signi.cantly increase perfor­mance, but invariably cause stack over.ows. 
5 GS: Rasterization VU1 perspective projects the shaded grid, and passes it directly to the Graphic Synthesizer 
(GS) for rasterization. During this time the EECore and VU0 are generating a grid for the next object. 
The GS is designed to handle polygonal data, and is ill suited for use as the .nal stage of a MP renderer. 
It has many features such as texture mapping which we were unable to utilize. The depth resolution of 
the Z buffer proved not to be a signi.cant problem, even when used in 16 bit mode, provided that clipping 
planes were set correctly. However the 12.4 .xed point representation of 2D coordinates generates rounding 
errors, resulting in missing pixels in the .nal image. This is made more obvious by the single precision 
maths, and limited grid size of the previous stages. 6 Conclusion The renderer was capable of handling 
generic RenderMan shader and scene .les. The major limitations resulted from the restricted size of the 
VUmem stack, and the poor rasterization by the GS. DMA and threading are poorly supported by PS2 Linux. 
It is estimated that performance could be almost doubled were an effective implementation of these features 
available. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965474</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Image-based diffuse lighting using visibility maps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965474</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965474</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31040205</person_id>
				<author_profile_id><![CDATA[81100390217]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ivan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neulander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhythm and Hues Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CHRISTENSEN, P. H. 2002. Note #35: Ambient occlusion, image-based illumination, and global illumination. PhotoRealistic RenderMan Application Notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MILLER, G. S., AND HOFFMAN, C. R. 1984. Illumination and reflection maps: Simulated objects in simulated and real envi-ronments. In SIGGRAPH '84 Advanced Computer Graphics Animation seminar notes, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566612</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[SLOAN, P.-P., ET AL. 2002. Precomputed radiance transfer for real-time rendering in dynamic, low-frequency lighting environments. In SIGGRAPH '02 Proceedings, ACM SIGGRAPH.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965477</section_id>
		<sort_key>17</sort_key>
		<section_seq_no>17</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Different strokes]]></section_title>
		<section_page_from>17</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP28016299</person_id>
				<author_profile_id><![CDATA[81341492983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Teresa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[www.smokingmoose.com]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>1077119</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Polygon-Based Pastel-Like Rendering for Animation]]></title>
		<subtitle><![CDATA[Improved algorithms for pastel-like rendering suitable for animation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.1077119</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1077119</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37025854</person_id>
				<author_profile_id><![CDATA[81328489571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kyoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Murakami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP37025127</person_id>
				<author_profile_id><![CDATA[81331505544]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Reiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuruno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965479</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Sable]]></title>
		<subtitle><![CDATA[a painterly renderer for film animation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965479</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965479</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14181483</person_id>
				<author_profile_id><![CDATA[81341497366]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Teece]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Feature Animation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sable a Painterly Renderer for Film Animation Daniel Teece* Walt Disney Feature Animation 1 Introduction 
This sketch describes elements of visual development for a film set in a dynamic and colorful world created 
entirely from computer-generated brush strokes. The project, currently in production at Walt Disney Feature 
Animation, required new rendering tools to achieve a distinctive visual style. 2 Overview The first 
requirement for this work stemmed from the concept art. The director, Mike Gabriel, produced a series 
of paintings that embodied both the South American feel of the piece and a simple, stylised motif. While 
each painting contains a relatively small number of brush strokes, each stroke is highly expressive, 
using a combination of dry and smooth edges to suggest the texture of the underlying subject. Through 
the early stages of the project it became obvious that neither traditional ink-and-paint animation nor 
a typical computer-generated approach would preserve the character of the original artwork. Simply put, 
the primary objective was to keep the final imagery as close to the development art as possible while 
allowing the animated paintings to move in a coherent manner. From a technical perspective, the solution 
centers on Sable, a painterly renderer that replaces a NURBS curve geometry set with brush strokes. The 
generated strokes are based on a library of scanned paint marks provided by the director. The user is 
able to dictate the application of the base strokes in the final composition, as well as applying a set 
of parameters to each stroke such as width, color, alpha gradations and visibility cues. 3 Animation 
Process An additional requirement concerned driving the motion of the source curves. The evolved workflow 
allows a traditional animator to work on a scene as normal, without needing to be concerned about the 
subsequent processes. The pencil animation itself does not need to be clean in terms of line quality, 
but should be "on-model". The animation is then scanned to generate a clip that can be used as reference 
within Alias|Wavefront Maya. This allows the user to match the performance and timing of the curve set 
rig with the original animation. 4 Rendering The animated curves, together with the attributes assigned 
by the user, are then processed by Inka, Disney's in-house inklinerenderer. This step provides a great 
deal of flexibility byleveraging the features already available in that tool. A segmentedand transformed 
curve set is then passed out of Inka and intoSable, which can either generate the painted image directly 
orproduce a scene file in a standard format for external rendering. *e-mail: daniel.teece@disney.com 
5 Summary The development of the Sable stroke renderer met production needs on a number of levels. The 
use of a 2D or 3D curve set representation produces a set of paint strokes that can be animated without 
strobing or popping artifacts. The ability to use strokes scanned from the director's paintings allows 
the CG renders to follow the look of the original art. Furthermore, Sable was able to be integrated with 
other tools in a digital production pipeline. Lastly, while pencil lines were not included in the final 
look, the animators were able to work traditionally, remain independent of the stroke rendering process 
and focus on performance. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965478</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Sketchy drawings]]></title>
		<subtitle><![CDATA[a hardware-accelerated approach for real-time non-photorealistic rendering]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965478</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965478</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14181388</person_id>
				<author_profile_id><![CDATA[81100522687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nienhaus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute at the University of Potsdam]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35022853</person_id>
				<author_profile_id><![CDATA[81100019985]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J&#252;rgen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[D&#246;llner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasso-Plattner-Institute at the University of Potsdam]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DECAUDING, P. 1996. Cartoon-looking rendering of 3D-Scenes. Technical Report INRIA 2919. Universit&#233; de Technologie de Compi&#232;gne, France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KILGARD, M. 2003. NVIDIA OpenGL Extension Specifications. NVIDIA Corporation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[NIENHAUS, M., D&#214;LLNER, J. 2003. Edge-Enhancement - An Algorithm For Real-Time Non-Photorealistic Rendering. In Journal of WSCG'03. 11(2):346--353.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325165</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[PERLIN, K. 1985. An Image Synthesizer. In Proceedings of ACM SIGGRAPH '85. 19(3):287--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Sketchy Drawings A Hardware-Accelerated Approach for Real-Time Non-Photorealistic Rendering Marc Nienhaus 
Jürgen Döllner* Hasso-Plattner-Institute at the University of Potsdam Introduction In Non-Photorealistic 
Rendering (NPR), sketchy drawings are essential to visually communicate and illustrate drafts and ideas, 
for instance, in architectural or product design. However, current hardware-accelerated, real-time rendering 
techniques do not concentrate on sketchy drawings of arbitrary 3D scene geometries. We present an image-space 
rendering technique that uses today s texture mapping and fragment shading hardware to generate sketchy 
drawings of arbitrary 3D scene geometry in real-time. We stress sketchiness in our drawings by simulating 
uncertainty. For simulating uncertainty we have to adjust visibility information using depth sprites, 
which allow us depth testing and 3D scene composition. Sketchy Drawing Our sketchy drawings primarily 
include 1) visually important edges and 2) simple surface-style rendering to convey scene objects.We 
consider silhouette and crease edges as visually important edges of 3D scene geometry. We obtain these 
edges by extracting discontinuities in the normal and depth buffer [Decaudin 1996]. The assembly of edges 
and their constituting intensity values forms a single texture TEdge (Figure a) as described in [Nienhaus 
and Döllner 2003]. We opt for unlit geometry as simple surface-style representation of 3D scene geometry 
(Figure b). Therefore, we render designated geometry directly into the texture TSurface using a render-to-texture 
implementation. Sketchiness is controlled by the degree of uncertainty, which is applied for rendering 
edges and surfaces. To simulate uncertainty, we create a screen-aligned quad that fits completely into 
the viewport of the canvas and texture that quad using the product of TEdge and TSurface. Furthermore, 
we apply an additional texture TNoise whose texture values have been determined by a noise function [Perlin 
1985]. TNoise serves as an offset texture when accessing TEdge and TSurface, i.e., texture values of 
TNoise slightly perturb texture coordinates that access TEdge and TSurface. To perturb texture coordinates 
of TEdge and TSurface non­uniformly, we apply two different 2×2-matrices one shifts perturbed coordinates 
of TEdge and one shifts perturbed coordinates of TSurface. Then, we merge texture values of TEdge and 
TSurface resulting in a sketchy drawing. Figure a and b show intermediate results after perturbing texture 
coordinates. Adjusting Visibility Information When rendering a screen-aligned quad that is textured 
with the texture of 3D scene geometry, z-values as visibility information of that geometry get lost. 
Furthermore, visibility information of 3D scene geometry is not available in its periphery when uncertainty 
has been applied. To control visibility we use depth sprites. Conceptually, depth sprites are common 
2-dimensional sprites that provide an additional depth component at each pixel for depth testing. We 
implement depth sprites using fragment programs [Kilgard 2003]. Initially, we generate a high precision 
depth texture TDepth derived from 3D scene geometry (Figure c). Then, we render the screen-aligned quad 
textured with TDepth. Thereby, we replace fragment z-values produced by the rasterizer with texture values 
received from TDepth using the fragment program. To adjust visibility information of the preceding sketchy 
drawing perturbation adopts the offset used for accessing TSurface. The minimum value of both texture 
accesses produces the final fragment z-value (Figure c ). As a result our sketchy drawings include perturbations 
of visually important edges and simple surface-style rendering, and adjusts visibility information of 
3D scene geometry (Figure d). Conclusions and Future Work Our approach presents a first sketchy rendering 
technique that takes fully advantage of graphics hardware fragment programming capabilities and that 
actually achieves real-time performance. In our future work, we expect to mimic hand­drawn sketches more 
realistically by considering geometrical properties derived from 3D scene geometry to precisely control 
uncertainty offsets.  a) Visually important edges b) Simply shaded geometry c) Depth values of geometry 
 a ) Uncertainty applied to b ) Uncertainty applied to c ) Uncertainty applied to visually important 
edges simply shaded geometry depth values  References DECAUDING, P. 1996. Cartoon-looking rendering 
of 3D-Scenes. Technical Report INRIA 2919. Université de Technologie de Compiègne, France. KILGARD, M. 
2003. NVIDIA OpenGL Extension Specifications. NVIDIA Corporation. NIENHAUS, M., DÖLLNER, J. 2003. Edge-Enhancement 
 An Algorithm For Real-Time Non-Photorealistic Rendering. In Journal of WSCG 03. 11(2):346-353. PERLIN, 
K. 1985. An Image Synthesizer. In Proceedings of ACM SIGGRAPH 85. 19(3):287-296. we additionally access 
TDepth twice while applying the same perturbations to its texture coordinates. The first perturbation 
* {nienhaus,doellner}@hpi.uni-potsdam.de adopts the offset used for accessing TEdge and the second Copyright 
held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>1077120</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Real-Time 3D Sumi-e Painting]]></title>
		<subtitle><![CDATA[A method for real-time 3D Sumi-e rendering using a hardware-accelerated rendering algorithm]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.1077120</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=1077120</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P385976</person_id>
				<author_profile_id><![CDATA[81100656332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chang-Hun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P734906</person_id>
				<author_profile_id><![CDATA[81436596606]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shin-Jin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965480</section_id>
		<sort_key>18</sort_key>
		<section_seq_no>18</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Eyes, nose, and body]]></section_title>
		<section_page_from>18</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP40029075</person_id>
				<author_profile_id><![CDATA[81100615391]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slater]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College of London]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965483</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Practical eye movement model using texture synthesis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965483</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965483</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35025511</person_id>
				<author_profile_id><![CDATA[81100180219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhigang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Integrated Media System Center, U. Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP131034426</person_id>
				<author_profile_id><![CDATA[81409595942]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Lewis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Integrated Media System Center, U. Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39053030</person_id>
				<author_profile_id><![CDATA[81100662479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ulrich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Integrated Media System Center, U. Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>851569</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EFROS, A., AND LEUNG, T. K. 1999, Texture Synthesis by Non-parametric Sampling, ICCV'99, 1033--1038.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566629</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LEE, S. P., BADLER, J. B., AND BADLER, N. 2002, Eyes alive, ACM Transaction on Graphics, 21, 3, 637--644.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[WILLIAMS, L., 2003, Disney Feature Animation, Personal communication,]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical Eye Movement Model using Texture Synthesis Zhigang Deng * J.P.Lewis       Ulrich Neumann 
Integrated Media System Center, U. Southern California  Figure 1: eye blink synthesis the red curve 
is the texture sample and blue is the (longer) synthesized eye blink texture. 1 Introduction As humans 
we are especially sensitive to the appearance of the face, and on the face, the eyes are particularly 
important. In fact, in attempts to animate photo-realistic CG humans, the eyes are very often what destroys 
the illusion [Williams 2003]. The state of art in eye movement synthesis is the Eyes Alive model [Lee 
et al. 2002] that develops a custom statistical model specifically for eye movement. While its results 
are the best to date, the model is complex and one wonders if it could be improved by using additional 
or different statistics. In fact the problem of generating novel animation that captures the character 
of given training data is the same problem as texture synthesis. In this sketch we describe a practical 
eye movement model using non-parametric texture synthesis techniques ([Efros and Leung 1999]), simulating 
the eye gaze motion and eye blink motion simultaneously. This approach uses the data directly and without 
an intervening human­crafted statistical model, yet it produces results that appear as good or better 
than the more complex statistical model. 2 Approach In a training stage, both eye blink and eye gaze 
motion data are obtained from real persons. The eye blink motions were captured with a hardware motion 
capture rig, using markers on the left and right eyelids. Because the eye blink motions in X, Y, and 
Z are strongly correlated, the eye blink motion in three dimensions can be simulated by a one dimensional 
blink signal based on the dominant Y (vertical) direction. Examining the captured data, we found that 
the motion of the left eyelid is nearly synchronized with that of the right eyelid. As such, we only 
need the motion capture trace for one eye to create the eye blink texture signal. By scaling the y-coordinates 
of the eye blink motion into range [0,1] (Figure 1), we get a 1D eye blink texture signal -- here 0 denotes 
the closed eyelid, 1 denotes the eyelid fully open, and any value between 0 and 1 represents a partially 
open eye. Corresponding eye gaze direction signals were obtained by manually estimating the eye direction 
in several training videos frame by frame using an eyeball widget in a custom GUI (Figure 2). While the 
manually estimated direction data is not completely accurate, it qualitatively captures the character 
and cadence of real human gaze movement; the timing is also frame­accurate. Then, in the modeling stage, 
the non-parametric sampling technique [Efros and Leung 1999] is used to synthesize novel eye blinks and 
eye gaze motions. The idea is to grow one sample at a Figure 2: (left) eye direction training signals 
are manually digitized; (middle) eye gaze x- and y- movement synthesis; (right) 3D eye orientation calculated 
from the x and y signals. time from an initial seed, by identifying all regions of the sample texture 
that are similar (threshold e) to the neighborhood of the sample, and then selecting the corresponding 
sample from one of these regions chosen at random. For more details on the non­parametric sampling technique, 
see [Efros and Leung 1999]. Figures 1,2 illustrate synthesized eye blink and gaze signals. 3 Results 
and Evaluations We asked eight people (customers at a coffee shop) to view two videos comparing our algorithm 
with the eyes alive model augmented with a random Poisson blink process, and indicate in which video 
the eye movement appeared more natural . Seven preferred our model (one had no preference). These results 
are largely due to the more accurate eye blinks in our model (real eyelids are not simply open or shut, 
Fig. 1), but people also commented that the eye gaze in our model appeared less jumpy . 4 Conclusion 
In summary, this sketch presents a practical eye movement model that generates quite realistic eye motions 
based on real human motion data. It demonstrates that well-developed texture synthesis techniques can 
be applied to the modeling of incidental facial motions such as eye gaze and eye blinks.  References 
EFROS, A., AND LEUNG, T.K. 1999, Texture Synthesis by Non-parametric Sampling, ICCV 99, 1033 1038. LEE, 
S.P., BADLER, J.B., AND BADLER,N. 2002, Eyes alive, ACM Transaction on Graphics, 21, 3, 637 644. WILLIAMS, 
L., 2003, Disney Feature Animation, Personal communication, Figure 3: Face models used to evaluate eye 
motion. *email: zdeng@usc.edu Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965481</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A nose-tracked, personal olfactory display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965481</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965481</url>
		<abstract>
			<par><![CDATA[An interface that involves all five senses, including olfaction, would be the ultimate interface for virtual reality (VR). We are trying to construct an olfactory display that does not require the user to wear anything on the face. We used an "air cannon" to transport small packets of scented air to the user's nose from some nearby place. In this paper, we report the ongoing development of an olfactory display system with a nose-tracking feature by incorporating vision-based face tracking technology.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P305136</person_id>
				<author_profile_id><![CDATA[81100018748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yanagida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P493938</person_id>
				<author_profile_id><![CDATA[81100502981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shinjiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023757</person_id>
				<author_profile_id><![CDATA[81100109790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Haruo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Noma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P423255</person_id>
				<author_profile_id><![CDATA[81100105873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nobuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tetsutani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP16000390</person_id>
				<author_profile_id><![CDATA[81100468418]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tomono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>195927</ref_obj_id>
				<ref_obj_pid>195923</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HEILIG, M. L. 1992. EL CINE DEL FUTURO: THE CINEMA OF THE FUTURE. PRESENCE, 1, 3, 279--294.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BARFIELD, W. AND DANAS, E. 1996. COMMENTS ON THE USE OF OLFACTORY DISPLAYS FOR VIRTUAL ENVIRONMENTS. PRESENCE, 5, 1, 109--121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HIROSE, M., TANIKAWA, T., TANAKA, S., AND SAKIKAWA, S. 2000. A STUDY OF OLFACTORY DISPLAY. PROC. OF THE VIRTUAL REALITY SOCIETY OF JAPAN 5TH ANNUAL CONFERENCE, 193--196 (IN JAPANESE).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>766109</ref_obj_id>
				<ref_obj_pid>765891</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[YANAGIDA, Y., NOMA, H., TETSUTANI, N., AND TOMONO, A. 2003. AN UNENCUMBERING, LOCALIZED OLFACTORY DISPLAY, ACM CHI2003 EXTENDED ABSTRACTS, 988--989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[KAWATO, S. AND TETSUTANI, N. 2002. DETECTION AND TRACKING OF EYES FOR GAZE-CAMERA CONTROL, PROC. OF 15TH INTL. CONF. ON VISION INTERFACE (VI2002), 348--353.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965482</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Vital signs]]></title>
		<subtitle><![CDATA[exploring novel forms of body language]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965482</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965482</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP48024846</person_id>
				<author_profile_id><![CDATA[81100493577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Lyons]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653852</person_id>
				<author_profile_id><![CDATA[81100052767]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kluender]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653839</person_id>
				<author_profile_id><![CDATA[81100178223]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chi-Ho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P423255</person_id>
				<author_profile_id><![CDATA[81100105873]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nobuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tetsutani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR Media Information Science Labs]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ARNHEIM, R. 1969. Visual Thinking. University of California Press, Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>265013</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[PICARD, R. W. 2000. Affective Computing. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[VARELA, F. J., THOMPSON, E., AND ROSCH, E. 1991. The Embodied Mind: Cognitive Science and Human Experience. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Vital Signs: Exploring Novel Forms of Body Language Michael J. Lyons* , Daniel Kluender, Chi-Ho Chan, 
and Nobuji Tetsutani ATR Media Information Science Labs 1 Introduction The emotional aspects of human 
communication depend signi.­cantly on non-verbal signals or body language. By contrast, hu­man telecommunication 
is characterized by a sense of disembodi­ment. However, machine-mediated communication permits the ac­quisition, 
transmission, and graphical display of data about a user s physiological state. Here we explore novel 
forms of body language, not normally available in face-to-face interaction. 2 Visualizing A.ective Signals 
The physiological parameters, blood volume pulse (BVP), respira­tion (R), and skin conductance (SC), 
are related to human affective state [Picard 2000], correspond with felt bodily sensations accom­panying 
emotional arousal and/or stress, and can be measured with non-invasive sensors(see .gure 1). Much work 
in affective computing aims at machine understand­ing of emotional states from sensor data. Here we instead 
directly visualize the real-time physiological data and tap the powerful ca­pabilities of human vision 
[Arnheim 1969]. Simple, intuitive dis­plays were designed which correlate with felt sensations (.gure 
2). The BVP signal is expressed as an expanding and contracting red disk, which resembles a beating heart. 
The R signal becomes a ris­ing and falling blue column; like an abstract lung in.ating and de­.ating. 
The SC signal is expressed as an abstract skin patch which .oods with blue drops as the user perspires. 
The displays are in­tended to act as arti.cial expressions of a user s bodily experience. Combined with 
contextual knowledge, they permit users to estimate each others affective states. Sensor data digitized 
using the Procomp+ system (Thought Technologies Ltd.) is exchanged via TCP/IP sockets using a client/server 
model. Each local client samples the three signals at 20 Hz, and sends data in real-time to a server. 
Clients also access the server for data from other users and control the live display. 3 Experiment: 
Online Kanji Lesson We study an example interaction, in which one person teaches an­other how to write 
several Chinese characters (kanji). The lesson takes place over the Internet via shared whiteboards and 
an audio link. Preliminary results have shown that the arti.cial expressions convey salient information 
to both student and teacher. Under the conditions of our experiments, the SC is the easiest signal to 
in­terpret: SC increased noticeably with level of anticipated or actual effort for both users. The teacher 
has quickly learned to make use of the SC display to adjust pace and dif.culty of the lesson. 4Conclusion 
Attending to felt bodily sensations can aid in recognizing and re­ducing stress. The hypothesis guiding 
our future studies is that paying attention to shared vital signals will increase empathy, the awareness 
both of our own and others emotional states. This could have numerous bene.cial effects for online interaction. 
*e-mail: mlyons@atr.co.jp  Figure 2: Intuitive display of the three affective signals. A video clip 
of the live display can be found online at: http://www.mis.atr.co.jp/~mlyons/vitalsigns.html. References 
ARNHEIM, R. 1969. Visual Thinking. University of California Press, Berkeley. PICARD, R. W. 2000. Affective 
Computing. MIT Press. VARELA, F. J., THOMPSON, E., AND ROSCH, E. 1991. The Em­bodied Mind: Cognitive 
Science and Human Experience. MIT Press. Acknowledgement This work was supported in part by the Telecommunications 
Advancement Organization of Japan. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965485</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Interactive functional anatomy]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965485</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965485</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653964</person_id>
				<author_profile_id><![CDATA[81385598156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vassili]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hurmusiadis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Primal Pictures, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653950</person_id>
				<author_profile_id><![CDATA[81100321343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barrick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Primal Pictures, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653841</person_id>
				<author_profile_id><![CDATA[81100020644]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Briscoe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Primal Pictures, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aubel, A., and Thalmann, D. 2001, Interactive Modeling of the Human Musculature. In Proceedings of Computer Animation, Seoul.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258827</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Scheepers, F., Parent. R. E., Carlson, W. E., and May, S. F. 1997. Anatomy-Based Modeling of the Human Musculature. In Proceedings of ACM SIGGRAPH 1997, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965484</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Attention-driven eye gaze and blinking for virtual humans]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965484</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965484</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14181624</person_id>
				<author_profile_id><![CDATA[81100523411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Image Synthesis Group, Trinity College, Dublin 2, Ireland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P339494</person_id>
				<author_profile_id><![CDATA[81100465557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Carol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Sullivan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Image Synthesis Group, Trinity College, Dublin 2, Ireland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>791862</ref_obj_id>
				<ref_obj_pid>791221</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[PETERS, C. AND O' SULLIVAN, C. 2003. Bottom-up visual attention for virtual human animation. Computer Animation and Social Agents 2003, in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ITTI, L., KOCH, C., AND NIEBUR, E. 1998. A model of saliency-based visual attention for rapid scene analysis, California Institute of Technology, PhD Thesis.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BARD, C., FLEURY, M. AND PAILLARD, J. 1991. Different patterns in aiming accuracy for head-movers and non-head movers. In: A. Berthoz, W. Graf & P. P. Vidal (eds). The Head-neck Sensory-motor System. Oxford University Press, Oxford, pp 582--586.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[EVINGER, C., MANNING, K., PELLEGRINI, J., BASSO, M., POWERS, A., AND SIBONY, P. 1994. Not looking while leaping: the linkage of blinking and saccadic gaze shifts. Experimental Brain Research, 100, pp 337--344.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Attention-Driven Eye Gaze And Blinking For Virtual Humans Christopher Peters Carol O Sullivan Image 
Synthesis Group Image Synthesis Group Department of Computer Science Department of Computer Science Trinity 
College Trinity College Dublin 2, Ireland Dublin 2, Ireland Christopher.Peters@cs.tcd.ie Carol.OSullivan@cs.tcd.ie 
 Gaze and blinking have recently been considered for application to social interactions among computer 
characters based on emotional states. In this sketch, we present improvements to a gaze controller in 
terms of both the contributions by the joints involved in the motion and also by a closer analysis of 
the role of blinking and blink synchronisation in the realism of the final animation. Unlike previous 
approaches, our controllers are driven by internal character states derived from, among other factors, 
a model of visual attention and short-term memory. Given a position to look at and taking into account 
internal state information such as an agent s interests, motivation and memory, the gaze and blink controllers 
are charged with the responsibility of executing an appropriate gaze motion in an expressive manner. 
 Figure 1. Gaze evoked blink experiments (left) and resulting eye movement video (right). We have previously 
presented a system [Peters and O Sullivan 2003], based on research from the field of cognitive engineering 
[Itti et al. 1998], for the automatic generation of bottom-up visual attention behaviours in virtual 
humans. The system processes an input image from a synthetic vision module, calculating local contrast 
for intensity, orientation and colour features and then combining the results into a single saliency 
map using a normalisation procedure. The output of the attention system is passed to a gaze controller, 
in order to generate a realistic gaze motion towards the current most salient location. Gaze motions 
are parameterised in terms of the contribution of the eye, head, neck and shoulder joints to the movement 
and the total dwell time spent on the target before moving to the next target. Based on evidence from 
psychology [Bard et al. 1991], virtual humans are tagged with a head-move attribute, defining how likely 
they are to turn their heads to look at objects. Although subtle, we regard blinking motions as being 
highly important for conveying realistic gaze motions. Our primary focus is on the relationship between 
blinking and gaze shifts, referred to as gaze-evoked blinks in the psychology literature [Evinger et 
al. 1994]. These blinks typically begin simultaneously with the initiation of the head/eye movement and 
the probability of a blink increases with the size of the gaze shift. Furthermore, the amplitude of the 
gaze motion has been shown to affect the magnitude of the blink. These features have been incorporated 
into our gaze control system; when a gaze shift is imminent, a blink procedure is invoked and initialises 
a blink animation with the desired blink magnitude for the corresponding eccentricity. A number of preliminary 
experiments were conducted on three test subjects to examine both the timing and nature of gaze evoked 
blinking for varying eccentricities in human subjects (Figure 1). An S.M.I. Eyelink eye-tracker was used 
to obtain close-up images of the eye during gaze motions. We also generated a number of animations where 
a virtual actor was given the task of looking at targets located at similar eccentricities to those in 
the experiments (Figure 2). References PETERS, C. AND O SULLIVAN, C. 2003. Bottom-up visual attention 
for virtual human animation. Computer Animation and Social Agents 2003, in press. ITTI, L., KOCH, C., 
AND NIEBUR, E. 1998. A model of saliency­based visual attention for rapid scene analysis, California 
Institute of Technology, PhD Thesis. BARD, C., FLEURY, M. AND PAILLARD, J. 1991. Different patterns in 
aiming accuracy for head-movers and non-head movers. In: A.Berthoz, W.Graf &#38; P.P.Vidal (eds). The 
Head-neck Sensory­motor System. Oxford University Press, Oxford, pp 582-586. EVINGER, C., MANNING, K., 
PELLEGRINI, J., BASSO, M., POWERS, A., AND SIBONY, P. 1994. Not looking while leaping: the linkage of 
blinking and saccadic gaze shifts. Experimental Brain Research, 100, pp 337-344. Figure 2. Animation 
frames from the gaze generator for three different eccentricities (left). Gaze generator at work in a 
virtual city environment (right). Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965486</section_id>
		<sort_key>19</sort_key>
		<section_seq_no>19</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[High dynamic range & tone mapping]]></section_title>
		<section_page_from>19</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP15038070</person_id>
				<author_profile_id><![CDATA[81100633003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Greg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Anyhere Software]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965488</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Shadow removal from a real picture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965488</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965488</url>
		<abstract>
			<par><![CDATA[Since shadows in a real picture imply the geometric constraint between lights, objects and viewpoint, the augmented reality using the picture should include the consistent shadows with the real situation. This paper proposes a method to remove shadows from a real picture based on the RGB color space analysis and shows experimental results of actual shadow removal and virtual shadow addition.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P557185</person_id>
				<author_profile_id><![CDATA[81100395213]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P207182</person_id>
				<author_profile_id><![CDATA[81100515686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[I. SATO, Y. SATO, AND K. IKEUCHI. 1999. Illumination distribution from brightness in shadows: adaptive estimation of illumination distribution with unknown reflectance properties in shadow regions. In Proceedings of ICCV '99, 875--822.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344972</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[M. BERTALMIO, G. SAPIRO, V. CASELLES AND C. BALLESTER. 2000. Image Inpainting. In Proceedings of SIGGRAPH 2000, 417--424.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Shadow Removal from A Real Picture  Masashi Baba * Hiroshima City University Naoki Asada Hiroshima 
City University   Abstract Since shadows in a real picture imply the geometric constraint between 
lights, objects and viewpoint, the augmented reality using the picture should include the consistent 
shadows with the real situation. This paper proposes a method to remove shadows from a real picture based 
on the RGB color space analysis and shows experimental results of actual shadow removal and virtual shadow 
addition. 1 Introduction Shadows are physical phenomena observed in most natural scenes and shadowing 
is an important technique to enhance the reality in image generation. Sato et al. [Sato 1999] have shown 
a method to embed shadows of a virtual object in a real picture based on the actual light source environment, 
whereas the shadow generation under different lighting conditions is not considered. Shadow removal is 
necessary to apply real pictures in any virtual space, but the geometric and photometric conditions are 
hardly recovered from one picture. Bertalmio et al. [Bertalmio 2000] have tried to eliminate small cracks 
and noises in a picture by making up the lost information from surrounding region. Unlike such a small 
defect, since shadows often occupy large area in a picture, we analyze the color distribution of the 
whole picture in RGB space. 2 Shadow Removal Algorithm  Our shadow removal algorithm consists of three 
steps: shadow area detection, color adjustment, and boundary correction between sunshine and shadow areas. 
First, all pixel colors are mapped on the RGB space, and the color distribution is segmented by using 
K-means clustering method. We provide the seeds of each cluster by pointing the regions that we want 
to classify. Then, a pair of clusters that are almost aligned on a color vector is found. We consider 
the darker cluster as shadow area and the lighter as sunshine one. Second, since the lighting color of 
the shadow area is not always the same as that of the sunshine one, we perform the color adjustment between 
them by modifying the direction of shadow vector to be that of sunshine one. Then, the color average 
and variance of the shadow cluster are adjusted to be the same as those of the sunshine one. Finally, 
we apply a smoothing filter to correct the boundary discontinuity that sometimes occurs around fuzzy 
boundary between shadow and sunshine areas. -------------------------------------------- *e-mail: baba@its.hiroshima-cu.ac.jp 
e-mail: asada@its.hiroshima-cu.ac.jp 3 Experimental Results  Figure 1 shows experimental results of 
shadow removal and addition. A real picture of a building is shown in (a) and its color distribution 
in RGB space is illustrated in (b). In this case, we had three seeds for clustering: dark wall as shadow, 
light wall as sunshine and blue sky. (c) shows the result of shadow removal, and (d) shows that of shadow 
addition. We performed the new shadow addition by switching the color average and variance of clusters 
between shadow and sunshine.   (a) A real picture of building (b) Color distribution in RGB space 
 (c) Result of shadow removal (d) Result of shadow addition Figure 1. Shadow removal and addition. 
4 Conclusion This paper describes a shadow removal method from a real picture. Our algorithm based on 
the color cluster analysis in RGB space worked successfully in shadow removal and addition as well. 
References I. SATO, Y. SATO, AND K. IKEUCHI. 1999. Illumination distribution from brightness in shadows: 
adaptive estimation of illumination distribution with unknown reflectance properties in shadow regions. 
In Proceedings of ICCV 99, 875 822. M. BERTALMIO, G. SAPIRO, V. CASELLES AND C. BALLESTER. 2000. Image 
Inpainting. In Proceedings of SIGGRAPH 2000 , 417 424. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965489</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Local adaptation luminance via segmentation and assimilation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965489</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965489</url>
		<abstract>
			<par><![CDATA[We present a novel method for computing local adaptation luminance that can be used with several different visual adaptation based tone-reproduction operators for displaying high dynamic range images.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computer vision]]></kw>
			<kw><![CDATA[human factors]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653970</person_id>
				<author_profile_id><![CDATA[81100560007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yangli]]></first_name>
				<middle_name><![CDATA[Hector]]></middle_name>
				<last_name><![CDATA[Yee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI / Dream Works, Redwood City, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14020468</person_id>
				<author_profile_id><![CDATA[81350599694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sumanta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pattanaik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida, Orlando, Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[H. Yee and S. Pattanaik. Segmentation and Assimilation for Detail Preserving Display of High-Dynamic Range Images. To be published in Visual Computer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Local Adaptation Luminance via Segmentation and Assimilation Yangli Hector Yee, PDI / DreamWorks, Redwood 
City, California, yeehector@hotmail.com Sumanta Pattanaik, University of Central Florida, Orlando, Florida, 
sumant@cs.ucf.edu Abstract We present a novel method for computing local adaptation luminance that can 
be used with several different visual adaptation based tone-reproduction operators for displaying high 
dynamic range images. Keywords: Human Factors; Computer Vision 1 Introduction Some tone-mapping algorithms 
make use of local adaptation luminance in order to display high dynamic range images on display surfaces 
with low dynamic range. These algorithms typically use a box filter to compute local adaptation. This 
can lead to ugly gradient reversal artifacts. Our proposed method presents a way to compute the local 
adaptation luminance in such a way so that when it is used for tone mapping, the gradient reversal artifacts 
are minimized and the appearance of high dynamic range is maintained.  2 Method Our algorithm for local 
adaptation proceeds in four steps, segmentation, grouping, assimilation and layer averaging. Now we define 
some terms before we discuss the implementation. The output of the segmentation process puts pixels into 
categories. The categories are then clustered by spatial contiguity into groups. A process called assimilation 
combines different groups together. Pixels belonging to each group are set to the group s luminance value 
and the resulting image is called a layer. Multiple layers are generated by repeating the segmentation 
process with slightly different bin sizes and then are averaged together to obtain the local adaptation 
luminance image. User specified parameters are marked in bold. Step 1: Segmentation. The method begins 
by performing simple image segmentation on the log10 luminance of the high dynamic range image by binning 
each pixel into categories, which are spaced apart in units of bin size (see Equation 1 and Equation 
2). Equation 1: bin_size = bin_size1 + (bin_size2 bin_size1) * ( layer / (max_layers 1)); Equation 2: 
category(x,y) = ( log10 luminance(x,y) minimum image log10 luminance ) / bin_size; where layer runs 
from 0 to max_layers-1, and max_layers is user specified. The bin size is linearly interpolated between 
user specified bin_size1 and bin_size2. Step 2: Grouping. A flood fill is performed in a breadth first 
manner so that contiguous pixels of the same category are assigned to a group. During the flood fill, 
the sum of pixel log luminance for the group is accumulated, as well as the list of locations of pixels 
belonging to the group. At the same time, if the flood fill encounters an adjacent group, the adjacency 
list of both groups are updated to reflect the fact that the groups are next to each other. The adjacency 
list is a list that keeps track of which groups are neighbors to the current group being flood filled. 
Group luminance is calculated as the sum of pixel log luminance values divided by the number of pixels 
in each group. Step 3: Assimilation. All the groups are checked to locate singletons that are groups 
with only one neighbor. The larger group of the two neighbors then assimilates the smaller group if the 
number of pixels in the larger group exceeds a user specified big_threshold and if the number of pixels 
in the smaller group is less than a user specified small_threshold. Moving the pixel locations from the 
smaller group to the larger group and discarding the group luminance value from the smaller group summarizes 
the assimilation process. The list of groups is again traversed and groups with pixel count smaller than 
small_threshold have their neighbor list traversed. The largest neighbor with a pixel count larger than 
big_threshold assimilates the smaller group. Each pixel is then set to the luminance value of the group 
it belongs to. The image that is formed from this segmentation, grouping and assimilation process is 
called a layer. Step 4: Layer Averaging. Multiple layers separated by slightly different bin sizes are 
then averaged together to obtain the final local adaptation luminance image, which is then handed off 
to a tone-reproduction algorithm for processing. (a) (b) (c) (d) Figure 1. (a) Adaptation luminance 
computed using our technique. (b) Adaptation luminance computed using box filter. (c) Tone mapping of 
a desk image using adaptation luminance from our technique. Our technique eliminates the gradient reversal 
on the lamp and enhances the specular reflection of the lamp on the desk. (d) Tone mapping using adaptation 
luminance computed by box filter. Note the gradient reversal in the lamp and the book.  3 References 
[1] H. Yee and S. Pattanaik. Segmentation and Assimilation for Detail Preserving Display of High-Dynamic 
Range Images. To be published in Visual Computer. 1 Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965487</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Perceptual evaluation of tone mapping operators]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965487</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965487</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653872</person_id>
				<author_profile_id><![CDATA[81100125291]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fr&#233;d&#233;ric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drago]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iwate University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP1700021100</person_id>
				<author_profile_id><![CDATA[81554513956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Martens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Aizu]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43128557</person_id>
				<author_profile_id><![CDATA[81332517742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Karol]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Myszkowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15028898</person_id>
				<author_profile_id><![CDATA[81100315426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hans-Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seidel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MPI Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BORG, I., AND GROENEN, P. 1997. Modern Multidimensional Scaling: Theory and Applications. Spring-Verlag, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DEVLIN, K. et al. 2002. Tone Reproduction and Physically Based Spectral Rendering. In State of the Art Reports, Eurographics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965490</section_id>
		<sort_key>20</sort_key>
		<section_seq_no>20</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Production rendering]]></section_title>
		<section_page_from>20</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP2400031600</person_id>
				<author_profile_id><![CDATA[82358662957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Darren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hendler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965491</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Tetrad volume and particle rendering in X2]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965491</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965491</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14150280</person_id>
				<author_profile_id><![CDATA[81100426409]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[La Barge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite Digital Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35029562</person_id>
				<author_profile_id><![CDATA[81100445884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tessendorf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite Digital Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653967</person_id>
				<author_profile_id><![CDATA[81100386298]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vijoy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gaddipati]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cinesite Digital Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1242192</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alan Kapler and Lucio Flores, "Evolution of a VFX Voxel Tool", Sketches and Applications, Siggraph 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>282285</ref_obj_id>
				<ref_obj_pid>280953</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gokhan Kisacikoglu, "The Making of Black-Hole and Nebula Clouds for the Motion Picture "Sphere" with Volumetric Rendering and the F-Rep of Solids", Sketches and Applications, Siggraph 98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jerry Tessendorf, "Efficiently Rendering Gobs and Gobs of Particles", unpublished notes, May, 2002 (included in the proceedings cd-rom).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tetrad Volume and Particle Rendering in X2 Bill La Barge* Jerry Tessendorf Vijoy Gaddipati Cinesite 
Digital Studios 1 Motivation In the movie X2, Cerebro is a large spherical cavity that extends mutant 
mental capabilities. To depict its large, cavernous, dynamic nature, and the connnection between the 
machine and the charac­ters, the atmospheric element of the scene was built based on a fully 3D volumetric 
rendering technology developed at Cinesite. In ad­dition, this volumetric technique was also used to 
connect .oating vinnettes in the space with the land masses on the borders of Cere­bro. The teleportation 
effect of the character Nightcrawler is accom­panied by a dynamic smokey .lament effect using turbulent 
particle dynamics and particle rendering. 2 Tetrads as Volume Elements Volume rendering is usually based 
on some form of ray marching or casting into a rectangular volume that is subdivided into rectangular 
voxels. Such a regular array is limited in spatial resolution to the resolution that is achievable within 
the regular grid. For example, grids with dimensions of about 1000x1000x1000 are possible with considerable 
memory and computation time resources. For produc­tion purposes, we needed a volume representation that 
allows very detailed structure while keeping memory usage very low and com­putation time as low as feasible. 
The VoxelB [1] approach used at Digital Domain provides interesting structure in the volume via in­teractive 
feedback, but still requires very large memory resources for the detail we wished to achieve. An alternative 
implicit surface method [2] was used to dynamically voxelize regions of a volume as they were needed. 
This method kept the memory resources lower, but the rendering time would still be large for the amount 
of detail desired. In our approach we chose instead to represent the fundamental volume element as a 
tetrahedron (tetrad), i.e. a three sided pyramid constructed from four points in space. There were several 
reasons for this: (1) tetrads are the simplest completely volumetric object that can be built -three 
points in general de.ne a plane in space, so four points are the minimum number needed to describe a 
volume; (2) no matter how the four points of a tetrad are placed relative to each other, they still form 
a tetrad with relatively simple structure and well-de.ned and oriented faces; and (3) tetrads with a 
long as­pect ratio were easily used to create the impression of energy trans­fer and sunbeams. 3 Tetrad 
Generation A two-step process was developed to generate and control the dis­tribution of tetrads throughout 
the rendering volume. The .rst step is the generation of guide particles. These particles, along with 
their attributes, were written as PDB .les from Maya for subse­quent multiplication and rendering. The 
second step occurs inside the renderer developed at Cinesite. The renderer handles one guide particle 
at a time. For various types of distributions desired, emission shaders based on a general pur­pose c-parsing 
language were written to procedurally multiply each guide particle into many, typically in the range 
of 5-50 children each. Using additional vertex information from the PDB .les the renderer generates either 
tetrad volumes or spherical particles. *bill@cubehost.net jerryt@rhythm.com vgadu@yahoo.com Figure 1: 
Example of a volume rendered element using tetrads as the basic volume element. 4 Rendering Rendering 
of particles and tetrads was accomplished with Cine­site s proprietary renderer partman[3], which is 
a high precision renderer of spheres and tetrads, with a statistically-based z-buffer procession for 
occlusion. This produces images of enormous con­centrations of particles or tetrads in tiny memory footprints. 
Each child tetrad is rendered using an analytic formula for the volume rendering through a .nite segment 
of uniform medium. The color and opacity are procedurally modi.ed from point to point in a volume shader 
based on the same c-parsing language and stan­dard noise functions. The renderer is able to render single 
tetrads at a time and properly accumulate the effect of the many tetrads, keeping memory usage low. 
5 Conclusions The unique nature of the Cerebro scenes in X2 were a perfect prov­ing ground for new volume 
rendering techniques based on tetrahe­dral objects as the fundamental unit of volume. The smokey .la­ments 
required for Nightcrawler s teleportation were a good oppor­tunity to explore new methods of particle 
dynamics and rendering. Both forms of rendering tetrad volumes and particles were ac­complished using 
a specialized proprietary renderer developed by Cinesite Digital Studios. 6 References 1. Alan Kapler 
and Lucio Flores, Evolution of a VFX Voxel Tool , Sketches and Applications, Siggraph 2002. 2. Gokhan 
Kisacikoglu, The Making of Black-Hole and Nebula Clouds for the Motion Picture Sphere with Volumetric 
Ren­dering and the F-Rep of Solids , Sketches and Applications, Siggraph 98. 3. Jerry Tessendorf, Ef.ciently 
Rendering Gobs and Gobs of Particles , unpublished notes, May, 2002 (included in the pro­ceedings cd-rom). 
 Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965492</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Avalanche! snowy FX for XXX]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965492</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965492</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35023102</person_id>
				<author_profile_id><![CDATA[81100036373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kapler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Alan Kapler Avalanche! Snowy FX for XXX Digital Domain zima@d2.com Introduction Digital Domain was 
approached with the challenge of creating a fully computer-generated avalanche for the feature film XXX. 
The hero Xander cage races down a mountain ahead of an avalanche of his own creation. To our knowledge 
it was a natural phenomenon that had never been attempted before in CG. Compounding the problem was our 
short delivery schedule 6 months from initial R&#38;D to delivery. Recreating a violent, inherently 
simulation­based natural phenomenon is always a difficult task, and we knew from the start it would require 
the development of new technologies, as well as making full use of Houdini s procedural capabilities. 
This sketch will cover the technique used to bring the avalanche to the screen. Avalanche Breakdown 
 Viewing reference of real avalanche footage allowed us to identify the different phenomena that occur 
in an avalanche. Primarily, avalanches are billowing snow with a very distinct leading edge. Occasionally 
chunks of packed snow emerge, sheeting wispy particulate matter until they disintegrate. It s not quite 
exciting enough for Hollywood, so we also had to factor in the usual embellishments. The director envisioned 
an avalanche much more vicious than any in reality, with Volkswagen-sized chunks of ice and snow landing 
on the hero s heels something not in any of our reference. We knew that these chunks landing in front 
of our avalanche would need to spawn secondary interactions in the snow ahead of the leading edge. The 
Tools Houdini was the obvious framework for creating the avalanche dynamics, and the high level of control 
it would require. We knew that we would be using our proprietary voxel tool VoxelB- for the rendering 
of the snow, but it had never been pushed to these limits of resolution before. The avalanche dynamics 
were set up entirely within Houdini. Animating the leading edge of the avalanche would automatically 
trigger blooming billows and set off bombs . These bombs would send hundred of chunks bursting into the 
air, and each chunk would have snow sheeting from it, and would either disintegrate or react with the 
ground when it hit. The procedural system created to handle these dynamic aspects will be a major focus 
of the sketch. Figure 2. Chunks and particulate snow a go-go . Material sheeting off chunks is a tricky 
problem, and our initial tests involved voxel simulations. These yielded decent results but were always 
soft-looking (due to resampling issues), slow and tricky to control. Since the eye wants to see small 
particulates in the snow sheeting, we developed a particle system capable of handling tens of millions 
of particles, but we soon realized even this wasn t enough. Finally we ended up sheeting a small number 
of particles from each chunk and used VoxelB- to stamp hundreds of thousands of voxel snowflakes onto 
each of these, perturbed by noise to give a wispy look. Thicker billows were achieved using displacement-mapped 
voxel cloud points. When expanding and bursting through one another they create a billowy, pyroclastic 
effect.  Particles and more particles A large part of development was just keeping our Houdini particle 
counts under control. An avalanche is inherently, made up of a LOT of particles, and this was working 
against us. Each particle was fairly memory-intensive, holding a lot of information including radius, 
velocity, snow density, growth rate, density decay rate, and all the other numerous attributes that govern 
its simulation and rendered appearance. Therefore we had to develop smarter particles that could communicate 
with voxels, and cull themselves during simulation. Houdini s ability to infuse particles with information 
enabled our smart particles to contain information like where will I land relative to the leading edge? 
, am I visible from the camera? , am I inside the voxel billows? , how long have I been out of view? 
. These were all used to cull particles, and not have sheeting happen for chunks inside the volume. Houdini 
point attributes were pushed to new levels, as were the ability of particles to cross-communicate with 
the voxels they would create. Rendering Volumetrics are always a challenge to render, and it involved 
significant developments to VoxelB-. Frustum-shaped voxel buffers were critical to maintain resolution 
both far away and close up. A dedicated voxel renderer was written which could motion-blur voxels using 
a vector field. Proper shadowing of voxels and geometry was viewed as the key to making our different 
elements integrate, and care was taken to ensure that each element had proper lighting interaction with 
the others: the chunks, ground and voxels all cast shadows onto each other. Figure 1. The voxel avalanche 
races down the mountain behind our hero in XXX. Conclusion The task of making an art-directable, photo-real 
cg avalanche on a short production schedule was a difficult one, full of new obstacles to overcome. Through 
the development of several new technologies and utilizing Houdini s procedural capabilities to their 
fullest, we managed to bring this destructive phenomenon of nature to the big screen. Copyright held 
by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965493</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Mr. Gray B. puts on a happy face]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965493</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965493</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653858</person_id>
				<author_profile_id><![CDATA[81545717256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Horsley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Industrial Light + Magic]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Mr. Gray B. Puts on a Happy Face  David F. Horsley Industrial Light + Magic  Dreamcatcher, the film 
based on Stephen King's best-selling novel, tells of four young friends who perform a heroic act and 
are changed forever by the powers they gain in return. Years later the friends are reunited on a hunting 
trip in Maine when a blizzard sets in. Is it really a just blizzard or an encounter with an alien force? 
 Lawrence Kasden, whose film credits include Body Heat, The Big Chill, Silverado, Grand Canyon, is the 
director Industrial Light and Magic worked with to create the visual effects. One key visual effect, 
the alien monster Mrgrayb, will be the focus of this 2003 Siggraph Sketch.  To create Mrgrayb and give 
it it s distinctive look we were able to utilize some of the new technology developed by Christophe Hery 
and others at ILM. We used Renderman shaders to subsurface scatter the surface of the creature, while 
John Walker, one of our Technical Directors applied the technology to create a luminescent quality. I 
was directly involved in establishing the look, creating a jelly like surface, intentionally trying to 
move away from the traditional plastic surface qualities of computer graphics. I was also involved in 
supervising the effects for the End Sequence involving a fight between two creatures, which I ll explain 
in more detail. One effect in the end sequence involves the creation of a cocoon where John Walker and 
I with the help of Mayur Patel applied Worley cell noise to create web patterns that would move like 
veins on the surface of the cocoon. We used our proprietary cloth simulation to shrink-wrap the creatures 
with the cocoon showing cloth properties similar to spandex. Keiji Yamaguchi headed up the shrinking 
methods to achieve this very specialized effect. We also used the cloth simulation software to achieve 
the effects of drool and dangly bits on Mrgrayb's teeth.          John Walker perfected the 
shaders to simulate a growing effect that slowly encompasses the creatures and perfected the look of 
the cocoon for all shots. The film (and this talk) is meant to be entertaining and focuses on encouraging 
young people to work in special effects. The fact that Dreamcatcher is a film not following many traditional 
Hollywood norms can be helpful to encourage creative people follow the industry.  Email: horsely@ilm.com 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965494</section_id>
		<sort_key>21</sort_key>
		<section_seq_no>21</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Haptics]]></section_title>
		<section_page_from>21</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39045197</person_id>
				<author_profile_id><![CDATA[81100487435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carolina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz-Neira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iowa State University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965495</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Trans-world haptic collaboration]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965495</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965495</url>
		<abstract>
			<par><![CDATA[This sketch describes a collaborative virtual environment application involving haptic interaction over long Internet distances. We have developed algorithms to accommodate significant latency for certain applications, notably in the medical domain. The results have shown that we can manipulate simulated human body organs, as well as guide each other's 'hands' (and shake hands!) over 22,000 km.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14210371</person_id>
				<author_profile_id><![CDATA[81100609779]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gunn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSIRO, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14082234</person_id>
				<author_profile_id><![CDATA[81100209375]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hutchins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSIRO, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P414540</person_id>
				<author_profile_id><![CDATA[81100563474]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adcock]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSIRO, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309418300</person_id>
				<author_profile_id><![CDATA[81539302756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rhys]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CSIRO, Australia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[MATSUMOTO, S., ET AL. 2000. The Influences of Network Issues on Haptic Collaboration in Shared Virtual Environments. Fifth Phantom Users' Group Workshop, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[STEVENSON, D., ET AL. 1999. Haptic Workbench: A Multisensory Virtual Environment. The Engineering Reality of Virtual Reality, Electronic Imaging '99.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[JORDAN, J., ET AL. 2002. COLLABORATION IN A MEDIATED HAPTIC ENVIRONMENT. PRESENCE 2002: The 5th Annual International Workshop on Presence.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Trans-World Haptic Collaboration Chris Gunn, Matthew Hutchins, Matt Adcock, Rhys Hawkins, CSIRO, Australia 
 Abstract This sketch describes a collaborative virtual environment application involving haptic interaction 
over long Internet distances. We have developed algorithms to accommodate significant latency for certain 
applications, notably in the medical domain. The results have shown that we can manipulate simulated 
human body organs, as well as guide each other s hands (and shake hands!) over 22,000 km. 1. Haptics 
in Collaboration  In a graphics-only system, a user typically reacts to a change only after it has been 
consciously processed in the brain, creating, effectively, an adaptive low pass filter. With haptics, 
the effects of latency are significant, due to the direct exchange of energy between the user and the 
system, via the user s flesh, muscles and tendons. These responses can produce instability when there 
are dynamic objects in the scene and there is significant latency. It has previously been thought [Matsumoto, 
S. 2000] that latencies of greater than 60ms prevent usable collaborative haptics. [Jordan, J. 2002] 
found that with solid objects a 90 ms round trip latency produced stability issues. However, our work 
has shown that with specialized physics, in surgical environments with soft objects, round trip latencies 
of 320 milliseconds can be accommodated. This allows haptic environments to be shared by points on the 
opposite sides of the globe, and may even permit satellite communications to be used. 2. Plausible Physics 
 We achieved this by sending all participating forces, including collision impulses, to a single physics 
engine running a pseudo or plausible physics model. The physics model bears little relationship to reality. 
However, for our task, the outcome is similar enough to reality to satisfy our needs. 3. Experimental 
Environment  Figure 1. CSIRO Haptic Workbench The CSIRO Haptic Workbench uses a SensAble Technologies 
Phantom and 3D stereo shutter glasses. We use a mirror arrangement to co-locate the user s visual and 
haptic experience, without the haptic equipment or user s arm occluding objects in the scene.  *e-mail: 
{chris.gunn,matthew.hutchins,matt.adcock}@csiro.au This work was carried out under the CeNTIE project, 
supported by the Australian Commonwealth Government. 4. Surgical Simulation  Figure 2. Screen shot of 
surgical training application As a test bed for collaborative haptics, we are using surgical training 
 specifically a cholecystectomy (gall bladder removal). An instructor and student, communicating via 
a headset/microphone pair, can view, touch and interact with a 3D scene of body organs. The instructor 
can grasp the student s tool to haptically guide it while the student feels the force of the instructor 
s guiding hand. Similarly, the instructor feels any resistance caused by the student. They can also collaboratively 
push, stretch and pull the organs around the scene, with attached body organs stretching and moving accordingly. 
This system also allows the collaborative viewing and annotation of a video of a real operation and a 
medical scan. The tool can also leave a vapor trail of fading tool images following the tool motion. 
5. Software  The system is a toolkit extension to the Reachin API1, encapsulating the multi-threading 
and communications code within two new nodes, which can be simply added to a scene-graph. The nodes create 
remote routes between corresponding scene objects, allowing changes to be transmitted using either TCP 
or UDP at rates up to 1000hz. 6. Conclusion  This technology has the potential to allow surgeons in 
remote or regional areas to connect to experts in any part of the world, for a mentoring session on a 
particular procedure. The toolkit nature of this system allows this technology to be easily integrated 
into a range of other application areas. References MATSUMOTO, S., ET AL. 2000. The Influences of Network 
Issues on Haptic Collaboration in Shared Virtual Environments. Fifth Phantom Users Group Workshop, 2000. 
STEVENSON, D., ET AL. 1999. Haptic Workbench: A Multisensory Virtual Environment. The Engineering Reality 
of Virtual Reality, Electronic Imaging 99. JORDAN, J., ET AL.. 2002. COLLABORATION IN A MEDIATED HAPTIC 
ENVIRONMENT. PRESENCE 2002: The 5th Annual International Workshop on Presence. 1. www.reachin.se 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965496</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Haptic enhancements for collaborative scenarios in virtual environment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965496</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965496</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P550939</person_id>
				<author_profile_id><![CDATA[81100489594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jason]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Daly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653862</person_id>
				<author_profile_id><![CDATA[81100451271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Don]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Washburn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653961</person_id>
				<author_profile_id><![CDATA[81100044485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lazarus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653890</person_id>
				<author_profile_id><![CDATA[81546959856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P550524</person_id>
				<author_profile_id><![CDATA[81100390466]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Glenn]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Central Florida]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BOULIC, R., REZZONICO, S., AND THALMANN, D. 1996. Multi-Finger Manipulation of Virtual Objects. In Proceedings of ACM Symposium on Virtual Reality Software and Technology. 825--8, pp 67--74, Hong Kong, July 96]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[EBERLY, D. 1999. Distance Between Point and Triangle In 3D. http://www.magic-software.com/Documentation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[REZZONICO, S. BOULIC, R., ZHIYONG, H. THALMANN, N. M., THANMANN, D. 1995. Consistent Grasping Interactions with Virtual Actors Based on the Multi-sensor Hand Model. In "Virtual Environment", M. Gobel, ed. Springer Verlag Wien.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Haptic Enhancements for Collaborative Scenarios in Virtual Environments Jason Daly, Don Washburn, Todd 
Lazarus, John Reeder and Glenn A. Martin* Institute for Simulation and Training University of Central 
Florida 1 Introduction The more senses a virtual reality system can simulate, the greater the realism 
and sense of immersion experienced by the user. Nearly every system provides the vital visual sense, 
and most provide some degree of audio reproduction as well. However, when the task involves a significant 
amount of manual manipulation of virtual objects, or when the scenario calls for both non-verbal and 
non-visual communication between its participants, the sense of touch becomes vital. This work involves 
adding haptic feedback elements to a virtual reality system to support collaborative scenarios. One element 
is the VEST (Virtual Environment Stimulus Tool), developed in­house, which is a flexible vest providing 
tactile vibratory feedback. The other is an Immersion 3D CyberGrasp, providing force feedback for the 
user s right hand. Both elements combined provide essential elements of the sense of touch necessary 
for scenarios that might be carried out by infantry and special forces teams. 2 Hardware The VEST contains 
a total of eight banks of four vibrator motors (similar to the type used in cellular phones and pagers). 
The vibrators are spread around the torso, providing distinct zones of tactile stimulation. The user 
is therefore able to localize the simulation event that caused the stimulus. This can be very important 
depending on what the vibration represents. For example, one use of the VEST is to simulate being hit 
by gunfire. The user is thus able to discern the direction of the enemy based on where on his body he 
or she was hit. Another use of the VEST is to allow tactile interaction between members of the team. 
Before entering a potentially hostile room, special forces teams must stack or line up single file outside 
the door. Each team member then signals his readiness by placing his hand on the shoulder of the team 
member in front. When the team leader feels the hand on his shoulder, he knows his team is ready to move. 
The VEST can provide this non-verbal and non-visual communication by simulating the touch on the shoulder 
through gentle vibratory feedback. The CyberGrasp is actually two devices in one. It is both an input 
device, to measure the articulation of the many joints in the hand, and a force feedback system to restrict 
the fingers and thumb from flexing farther than the system allows. The CyberGrasp allows manual manipulation 
of virtual objects, such as doorknobs that need to be turned and stun grenades that the team may throw 
into the hostile room. The CyberGrasp s force feedback capability allows the objects to be felt by the 
user while they are manipulated. The CyberGrasp also allows a team member to physically grasp the virtual 
shoulder of the team member in front to signal readiness to move (as discussed above). *e-mail: {jdaly, 
dwashbur, tlazarus, jreeder, martin}@ist.ucf.edu 3 Software The software to control the VEST device 
is fairly straightforward. The interface is based on a BASIC Stamp II and provides direct software control 
of each of the vibrator zones. A higher-level interface allows a combination of zones to be activated 
based on an incident vector which impacts on the user s avatar in the virtual environment (for example, 
when the user is hit by enemy gunfire). The CyberGrasp software interface is more complex. Much of the 
work was based on techniques presented by Boulic, Rezzonico and others. They describe methods to grasp 
virtual objects with a glove interface, and to manipulate them precisely with the fingers. The system 
uses an array of spherical sensors, to handle collision detection between the hand and the virtual object 
being manipulated. Collision detection was performed using a point­triangle method presented by Eberly, 
and optimized using the bounding volume hierarchy of the scene graph. Next, we employ Boulic s method 
for grasping and manipulation of the virtual object. To Boulic s work, we add the CyberGrasp s force 
feedback capability, allowing the user to feel the object he or she is manipulating. Also, we add the 
ability to constrain virtual objects in space, preventing them from being moved or rotated in any given 
axis or set of axes. For example, the doorknob that one of the team members must turn to open can only 
be rotated around one axis and cannot be translated at all. If the user attempts to move the doorknob 
or rotate it in an invalid axis, his or her hand simply slips off of it (with appropriate feedback from 
the CyberGrasp). The software was developed as part of IST s Virtual Environment Software Sandbox (VESS), 
a software platform supporting virtual reality applications. This ensures that these software capabilities 
will be available to a wide range of applications. 4 Conclusion The addition of haptic feedback to a 
collaborative simulation both increases the sense of realism experienced by the users, and provides additional 
ways to communicate and work together. We have presented a set of hardware and software elements that 
can add haptic feedback to a wide variety of these simulations. References BOULIC, R., REZZONICO, S., 
AND THALMANN, D. 1996. Multi- Finger Manipulation of Virtual Objects. In Proceedings of ACM Symposium 
on Virtual Reality Software and Technology. 825-8, pp 67-74, Hong Kong, July 96 EBERLY, D. 1999. Distance 
Between Point and Triangle In 3D. http://www.magic-software.com/Documentation. REZZONICO, S. BOULIC, 
R., ZHIYONG, H. THALMANN, N. M., THANMANN, D. 1995. Consistent Grasping Interactions with Virtual Actors 
Based on the Multi-sensor Hand Model. In Virtual Environment , M. Gobel, ed. Springer Verlag Wien. Copyright 
held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965499</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Touchy.Internet]]></title>
		<subtitle><![CDATA[a cybernetics system for human-pet interaction through the Internet]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965499</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965499</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP30032968</person_id>
				<author_profile_id><![CDATA[81331501839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lee]]></first_name>
				<middle_name><![CDATA[Shang]]></middle_name>
				<last_name><![CDATA[Ping]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39040972</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P400736</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Touchy.Internet Lee Shang Ping * Farzam Farbiz National University of Singapore National University 
of Singapore 1 Introduction We have developed an innovative cybernetics interface system for human-pet 
physical touch interaction through the internet. The sys­tem enables a human to remotely fondle her pet 
which is kept at home backyard while she is away (e.g. in her of.ce or on holiday), and at the same time 
to see the pet s movement physically. This is realized by using a doll, which resembles the real pet 
located re­motely. The pet owner interacts with the real pet by touching the doll. Also she can see the 
doll being moved by a positioning mech­anism which follows the real pet s movement. The system is an 
amalgamation of haptic interface, internet and computer vision all together. This is a novel type of 
physical interaction and symbiosis between human and pet with computer and internet as a new form of 
media. The advantage of this system is to bring the sense of physical and emotional presence between 
man and animal. It attempts to recapture our sense of togetherness with our animal friends, just like 
times gone by on the prairie, village, or jungle. It can be used to effectively feel the remote presence 
of a pet. Also, people who have allergy in touching animals can use this system to have similar feeling 
of touching pets. It can even be used in zoos to let people have the feeling of touch and fondling live, 
wild animals which cannot be done under normal circumstances. 2 System Structure In our system, we propose 
a physical doll instead of a 3D virtual model as a pet-representation-object. Since it is a physical 
object, humans can have tangible interaction with it in terms of touch and fondling, which can not be 
done by virtual models or an icon on the computer monitor. In contrast with common human-computer-cybernetics 
systems, our system has a pet-computer-cybernetics mechanism. Our hand built cybernetics system contains 
a low cost PC which controls a XY positioning table based on the position of the pet in the back­yard. 
We use computer vision based tracking methods to .nd the position of the pet in the backyard and send 
this information through internet. The general schematic view of our system is depicted in Figure 1. 
As can be seen in this Figure, the rooster (pet) owner touches the movable doll which sits on a XY table. 
The touch action is transferred to the rooster through the internet. At the same time the rooster s movement 
is followed by the doll. As a result in our sys­tem, the internet carries the touch information from 
of.ce to back­yard and movement position of the pet from the backyard to the of.ce, as well as the pet 
audio-visual information. *e-mail: eleleesp@nus.edu.sg e-mail: eleff@nus.edu.sg e-mail:adriancheok@nus.edu.sg 
 Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965498</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A haptic virtual reality training tool for paper conservators]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965498</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965498</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653825</person_id>
				<author_profile_id><![CDATA[81100283707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Angie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geary]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The London Institute, Camberwell College of Arts, London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mohr, P. 1985. Backing Removal, Paper Conservation Catalogue, AIC Book and Paper Group, Washington.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mark, R. E. (ed.) 1983. Handbook of Physical and Mechanical Testing of Paper and Paperboard, Volumes 1 & 2. Marcel Dekker Inc., New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Reachin Technologies: <u>http://www.reachin.se</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[SensAble Technologies: <u>http://www.sensable.com</u>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Haptic Virtual Reality Training Tool for Paper Conservators Dr. Angie Geary* The London Institute, 
Camberwell College of Arts, London, UK. 1 Introduction In paper conservation training, students are 
often required to gain many hours of experience in specific manual tasks. This is necessary to acquire 
the skills for the successful and safe treatment of delicate and fragile works of art. The haptic virtual 
reality (VR) environment offers a means of creating an enhanced training tool for conservators. Simulated 
conservation operations, such as backing removal, can be performed without risk to the objects, while 
building skills. Training can be accelerated through enhanced performance feedback using visual, aural 
and touch stimuli. The simulation software can also track the student s progress by compiling statistical 
data on their performance that can be reviewed by the tutor.  Figure 1. A backing removal Figure 2. 
A 19th Century in progress. print with backing.  2 Context A significant problem for the preservation 
of works of art on paper is the presence of aged and degraded backing material adhered to the reverse 
of the original artefact1. The conservation task of removing such backings is the focus for the simulation 
training tool. Fragile and delicate watercolours or prints can be damaged by the presence of these often 
poor quality, highly acidic paper or board backings. Mechanical removal using hand tools such as scalpels 
and spatulas is in many cases the only viable means of detaching the backing. Moisture or solvent based 
treatments such as soaking or steam humidification can be effective for backing reversal, but may not 
be possible due to the sensitivity of the original object. Mechanical removal, although slow and exacting, 
allows a greater degree of control and precision. The time available in a typical conservation Masters 
degree course is limited and the number of hours needed to acquire diverse manual skills is great. This 
often means that students begin with training on genuine works of art. The extended period usually needed 
for students to master backing removal treatments can be shortened by the use of the VR training tool. 
It will also allow manual skills to be developed prior to commencing work on real objects. 3 Constructing 
the Simulation In the simulations currently under development the physical properties of the paper objects 
and their backings are modelled according to values from existing conservation research and paper industry 
standards2. Parameters within the virtual environment for properties such as strength, rigidity, elasticity, 
and flexibility are variable and may be adjusted according to specified age and embrittlement, a range 
of material types and environmental humidity modulation. The realism of the interaction experience within 
the simulation is also critical to the validity of the training tool. The authenticity of haptic characteristics, 
such as surface friction and texture, will be essential to its success in convincing the user. These 
are being modelled and tested in close consultation with a range of experienced conservators. 4 The 
Technology The project uses the Reachin® stereoscopic display system3 with a PHANTOM® Premium 1.5 3D 
haptic arm4 and application programming interface (API). The environment geometry and haptic characterisation 
will be built in Virtual Reality Modelling Language (VRML). The API also supports additional non-standard 
VRML nodes that can be used to control surface characteristics and dynamic properties such as frictional 
behaviour, texture and inertia. PythonScript(an interpreted, interactive, object-oriented programming 
language, extensible in C and C++) is also supported by the API and will be used to build more sophisticated 
behaviours involving user interactivity. C++ programming will be used to create the architecture and 
object structure for the final application. Figure 3. Developing the simulation on the Reachin® display. 
 See also: http://www.option5.co.uk/haptic References 1 Mohr, P. 1985. Backing Removal, Paper Conservation 
Catalogue, AIC Book and Paper Group, Washington. 2 Mark, R.E. (ed.) 1983. Handbook of Physical and Mechanical 
Testing of Paper and Paperboard, Volumes 1 &#38; 2. Marcel Dekker Inc., New York. 3 Reachin Technologies: 
http://www.reachin.se 4 SensAble Technologies: http://www.sensable.com *Email: a.geary@camb.linst.ac.uk 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965497</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[The stringed haptic workbench]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965497</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965497</url>
		<abstract>
			<par><![CDATA[This paper proposes a new approach for adding haptic feedback on a workbench. The integration of a stringed haptic device is discussed.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14025999</person_id>
				<author_profile_id><![CDATA[81100039995]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tarrin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[i3D-INRIA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P434687</person_id>
				<author_profile_id><![CDATA[81100270860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasegawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P346627</person_id>
				<author_profile_id><![CDATA[81100371012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Laroussi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouguila]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP48025204</person_id>
				<author_profile_id><![CDATA[81100567066]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TIT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HIRATA, Y., AND SATO, M. 1992. 3-dimensional interface device for virtual work space. In IEEE/RSJ International Conference on IROS.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KRUGER, W., AND FROELICH, B. 1994. The responsive work-bench. IEEE Computer Graphics and Applications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Stringed Haptic Workbench* Nicolas Tarrin Sabine Coquillart Shoichi Hasegawa Laroussi Bouguila 
Makoto Sato i3D-INRIA i3D-INRIA Abstract This paper proposes a new approach for adding haptic feedback 
on a workbench. The integration of a stringed haptic device is dis­cussed. 1 Introduction The workbench 
(see [Kruger and Froelich 1994] and Figure) is an interesting semi-immersive con.guration for interactive 
tasks. Many applications require interactively manipulating data located in front of the user. For that 
purpose, the bench speci.city of the workbench is very attractive. The workbench also provides stereo­scopic 
display, large screens, and head-tracking, but usually no force feedback. Adding force feedback capabilities 
to workbenches is not an easy task for two major reasons. First, there are not many types of force feedback 
devices and most of the existing ones are either heavy, in­vasive, expensive, or only work within a limited 
space. The second dif.culty comes from the con.guration itself. Most of the greatly appreciated bene.ts 
of the workbench turn out to be a constraint or even a dif.culty for the integration of force feedback. 
The large size of the visualization space requires an haptic device which works in large spaces. Furthermore, 
the superposition of the visualization and the manipulation spaces allows direct manipulation but compli­cates 
the integration of force feedback devices. The device has to be operational inside the display space, 
no scale factor is allowed, suf­.ciently accurate calibration must be provided, and the parts close to 
the hand and/or the handle have to be small enough not to hide the scene. To the authors knowledge, only 
two attempts to add .xed-base force feedback to workbenches have been proposed so far. The .rst one from 
the University of North Carolina and the second one from the University of Utah. Both approaches are 
quite similar. Both consider a one-screen workbench and propose installing an arm force feedback device, 
in both cases a PHANToMTM , on it. 2 The Stringed Haptic Workbench This paper introduces a new approach 
to perform virtual direct ma­nipulation with force feedback on a two-screens workbench. A Stringed Haptic 
Interface, the SPIDAR [Hirata and Sato 1992] is proposed. Offering a .exible system seems important to 
us, so we have choosen an 8-motor con.guration. This allows to provide 3DoF force feedback on two points 
(SPIDAR 2) or 6DoF force feedback on a physical object (SPIDAR G) using the same hard­ware con.guration. 
As previously stated, one of our main concerns is to provide force feedback for direct manipulation within 
a suf.ciently large space. More precisely, we want to .ll as fully as possible the work­bench manipulation 
space with the SPIDAR manipulation space (the space where the SPIDAR returns forces in every direction), 
also called SPIDAR space. * a full version is available in Eurographics 2003 proceedings e-mail: Nicolas.Tarrin/Sabine.Coquillart@inria.fr 
e-mail: hase/laroussi/msato@pi.titech.ac.jp TIT TIT TIT  The .rst approach which comes to mind consists 
in positioning the motors on the vertices of the workbench parallelepiped. This approach has the advantage 
of offering six of the eight motors .xa­tion points needed and of not making the con.guration bulkier 
than the workbench itself. This con.guration leads to a SPIDAR space of 0.3 m3. It is almost twice the 
size of the PHANToM space (0.18 m3). But the size of the workbench manipulation space not covered by 
the SPIDAR space is 0.85 m3. In order to enlarge the SPIDAR space, the four motors lying on the user 
side are moved to the side and backwards. The resulting SPIDAR space size is approximately 1m3 and the 
size of the workbench manipulation space not covered by the new SPIDAR space is now 0.37 m3. A software 
platform has been developed for this con.guration providing a framework that links together visual rendering, 
haptic rendering and physical simulation. A virtual prototyping (assem­bling and disassembling) application 
based on this plateform has also been developed. 3 Conclusion First informal tests show that the assembling/disassembling 
task bene.ts from the addition of force feedback and that strings do not impact on stereo perception 
of the scene. The main advantage of the Stringed Haptic Workbench is the size of its visual and haptic 
spaces. It allows direct manipulation on a far larger area than with previous haptic workbench solutions. 
Moreover, strings are extremely discreet, they are quickly forgotten while manipulating. The occultation 
is minimal and any part of the visualization space is visible. Evaluations are currently being conducted 
and the virtual proto­typing application is being tested on different industrial data sets. Other applications, 
like scienti.c visualization, are also under con­sideration. References HIRATA, Y., AND SATO, M. 1992. 
3-dimensional interface device for virtual work space. In IEEE/RSJ International Conference on IROS. 
KRUGER, W., AND FROELICH, B. 1994. The responsive work­bench. IEEE Computer Graphics and Applications. 
Copyright held by the author   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965500</section_id>
		<sort_key>22</sort_key>
		<section_seq_no>22</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>22</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653955</person_id>
				<author_profile_id><![CDATA[81100035576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derrick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vicarious Visions, Inc.]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965503</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[EMOCAP]]></title>
		<subtitle><![CDATA[driving 3D characters with real mood dynamics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965503</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965503</url>
		<abstract>
			<par><![CDATA[Motion capture, or "mocap," is now a standard method by which animators impress the trademark fluidity of an actor's movement on their digital characters. This animation sketch presents EMOCAP, a new method that extends the idea of mocap to the capture of <i>psychological</i> data defining complex human emotional states as they change over time. Although emotion capture is, like mocap, of general relevance to character animation, here we report the method and its first applications to visualizing long term patterns of psychiatric mood behavior.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653925</person_id>
				<author_profile_id><![CDATA[81100424842]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nicholas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Woolridge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653859</person_id>
				<author_profile_id><![CDATA[81100239482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kreindler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653837</person_id>
				<author_profile_id><![CDATA[81100261007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Charles]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lumsden]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ECKMAN, P., AND FRIESEN, W. V. 1978. Facial Action Coding System. Consulting Psychologists Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HOOIMEIJER, B. 2001. An Alternative Cartoon Render Technique. http://www.xs4all.nl/~naam/techniques.html]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[KREINDLER, D., LEVITT, A., WOOLRIDGE, N., AND LUMSDEN, C. J. 2003. A New Instrument for Mood Self-Report. Psychiatric Research, submitted.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 EMOCAP: Driving 3D Characters with Real Mood Dynamics Nicholas Woolridge* David Kreindler Charles Lumsden 
BMC Division Dept. of Psychiatry Dept. of Medicine University of Toronto University of Toronto University 
of Toronto Abstract Motion capture, or mocap, is now a standard method by which animators impress the 
trademark fluidity of an actor s movement on their digital characters. This animation sketch presents 
EMOCAP, a new method that extends the idea of mocap to the capture of psychological data defining complex 
human emotional states as they change over time. Although emotion capture is, like mocap, of general 
relevance to character animation, here we report the method and its first applications to visualizing 
long term patterns of psychiatric mood behavior. 1 Introduction In North America alone, more than 20 
million people each year endure the disabling impact of mood disorders. We are following 20 normal and 
20 rapidly cycling bipolar subjects for a period of 18 months using a newly designed, 19-item emotion 
assessment questionnaire [Kreindler et al. 2003]. Each item in the questionnaire runs as a visual analog 
scale on a PalmOS® enabled cellular telephone (Kyocera pdQ1900) and activates twice per day on the Bell 
Mobility (Canada) network. The subject places a mark on each scale at a position reflecting their current 
experienced intensity of an emotion, such as sadness or fatigue or euphoria. The pixel position of the 
mark on the line captures the datum for that emotion scale. 2 EMOCAP In Action In our current study, 
the basic unit of EMOCAP data is therefore a 19-dimensional vector in which each component is a response 
datum for one of the 19 emotion scales. A succession of vectors  Figure 1. PR, NPR: Neutral, mania, 
fatigue, depression. over time defines the subject s temporal signature of affect change. For our animations 
(Cinema 4D XL), morph targets for regional face muscle action were established according to the Facial 
Action Coding System (FACS) [Ekman and Friesen 1978]. The CD4XL nodes for each FACS slider were linked 
to compose *e-mail: n.woolridge@utoronto.ca e-mail: david.kreindler@utoronto.ca e-mail, correspondence: 
charles.lumsden@utoronto.ca sliders for a neutral-to-full intensity FACS facial expression (e.g Saddest 
ever ) on each scale. Because of their clinical importance, we have focused to date on four of our scales: 
depression, mania (euphoria), fatigue, and overall bipolar mood. An animation keyframe thus corresponds 
to the patient s emotion data vector at one unique time in their record. A keyframe sets automatically 
for every time point. Photorealistic (PR) and nonphotorealistic (NPR) [Hooimeijer 2001] renders are generated. 
The animations reveal time-lapsed emotion flow across the human face otherwise taking days, weeks, or 
months to appear. When visualized on these time scales, differences in the emotional lives of normal 
and manic depressive people are strikingly evident:  Figure 2. EMOCAP data (243 days), and 4-scale blend. 
 Normal emotion change is mostly rapid expressive chatter of comparatively low amplitude. In the bipolar 
faces, however, a more pronounced temporal phrasing of emotion change becomes visible, with the larger 
changes occurring on many time scales. Readability of the flow peaks at moderate nonphotorealism. Intriguingly, 
blending the effects of multiple scales in one face enriches the rendered expression, rather than washing 
or canceling out the effects of each scale. EMOCAP approaches may therefore scale relevantly from simple 
univariate measures to intricate emotion blends that endow digital animated characters with more realistic 
patterns of mood and feeling. We thank the Bell University Labs Program, University of Toronto, for support. 
  References ECKMAN, P., AND FRIESEN, W. V. 1978. Facial Action Coding System. Consulting Psychologists 
Press. HOOIMEIJER, B. 2001. An Alternative Cartoon Render Technique. http://www.xs4all.nl/~naam/techniques.html 
KREINDLER, D., LEVITT, A., WOOLRIDGE, N., AND LUMSDEN, C. J. 2003. A New Instrument for Mood Self-Report. 
Psychiatric Research, submitted. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965504</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Visualizing horn evolution by morphing high-resolution X-ray CT images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965504</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965504</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653969</person_id>
				<author_profile_id><![CDATA[81100023462]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wendy]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Hodges]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Riverside]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653957</person_id>
				<author_profile_id><![CDATA[81100516617]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Theodore]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Garland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California at Riverside]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653942</person_id>
				<author_profile_id><![CDATA[81100612383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Reuben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reyes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Texas at Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653959</person_id>
				<author_profile_id><![CDATA[81100148713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Timothy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rowe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Texas at Austin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CONROY, G. C. AND VANNIER, M. W. 1984. Noninvasive three-dimensional computer imaging of matrix-filled fossil skulls by high-resolution computed tomography. Science, 226, 1236--1239.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>241075</ref_obj_id>
				<ref_obj_pid>241020</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DECARLO, D. AND GALLIER, J. 1996. Topological evolution of surfaces. Graphics Interface, 194--203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[GARLAND, T., JR., MARTIN, K. L. M., AND D&#205;AZ-URIARTE, R. 1997. Reconstructing ancestral trait values using squared-change parsimony: plasma osmolarity at the origin of amniotes. In Amniote Origins: Completing the Transition to Land, Academic Press, San Diego. S. Sumida and K. Martin, Eds, 425--501.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[JERMANN, T. M., OPITZ, J. G., STACKHOUSE, J. AND BENNER, S. A. 1995. Reconstructing the evolutionary history of the artiodactyl ribonuclease superfamily. Nature, 374, 57--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing Horn Evolution by Morphing High-resolution X-ray CT Images Wendy L. Hodges* and Theodore 
Garland, Jr. Reuben Reyes and Timothy Rowe University of California at Riverside University of Texas 
at Austin 1 Introduction Reconstructing the evolutionary history of diverse species is abasic goal of 
systematic biology and essential to comparativebiology. Phylogenies representing this history are constructedfrom 
analyses of molecular or morphological data and used astools for understanding the evolution of complex 
traits. Objectivesof this research are to create visualization tools that dynamicallyreconstruct horn 
morphology in three-dimensions (3D) for hornedlizards and show how changes occur along a phylogeny viametamorphosis 
(morphing). These objectives will be met byincorporating results from phylogenetic analyses with ancestraltrait 
reconstructions applied to 3-D images generated from a high­resolution x-ray computed tomography (CT) 
scanner. Ancestralreconstruction algorithms are incorporated with nonlinear morphing to accommodate different 
rates of change along andbetween lineages. Application of 3-D morphing to systematicsand ancestral reconstruction 
is a novel approach in visualizationmethods and opens a new realm of collaborative learning anddiscovery. 
We believe these tools will provide a greaterunderstanding of how complex traits have evolved by allowingscientists 
to peer into the past, to see what ancestral states looklike, and to visualize changes through time. 
Keywords: 3-D morphing, morphology, control points,phylogeny, x-ray computed tomography 2 Scientific 
Application CT scanning has been available for medical purposes since the 1970s, and volumetric, three-dimensional 
visualization programs are now prominent tools for treating complex problems in medicine. More recently, 
CT scanning has been applied to fossils and biological specimens (Fig. 1), which allows visualization 
of interior, as well as exterior, 3-D geometry without destroying a specimen (Conroy and Vannier, 1984). 
Triangulated images generated from CT data contain over 700,000 facets in lizard skulls alone and can 
be used to select control points with X, Y, and Z coordinates corresponding to entire homologousfeatures 
(e.g. Fig. 1, temporal horns) comparable between speciesinstead of using single measured traits (e.g. 
horn length). In combination with known relationships of species in the form ofphylogenetic trees, traits 
from known specimens can be comparedthrough time. The phylogeny, combined with mathematical andstatistical 
techniques to reconstruct ancestral traits for taxa understudy, can generate innovative and creative 
research in biology tounderstand underlying conditions responsible for the rich diversityof species on 
earth (Garland et al. 1997; Jermann et al. 1995).  3 Nonlinear Morphing and Visualizing Change Morphing 
is a dynamic process thatallows visualization of transformational changes from one form to another in 
real time (DeCarloand Gallier 1996).Morphing was applied to 3-D CT data to visualize morphologicalchanges 
in two lizardspecies (Fig. 2). Inorder to accommodate different evolutionary rates, we can use atable 
of time curves to morph sets of traits between two end-stateforms. This table ties control points on 
the morphology ofdifferent species corresponding to homologous traits and allowsthe researcher to set 
different time functions obtained from phylogenetic information to produce nonlinear morphing acrossthe 
model. Different rates of change set for control pointsproduces a more accurate model of ancestral forms. 
In addition tostipulating different time functions, different mathematical modelscan be applied during 
morphing to provide a window into howdifferent models affect the reconstruction process or to test hypotheses 
about how structures change.  4 Conclusions This research integrates visualization tools for 3-D morphing 
withmethods to reconstruct phylogenetic relationships and ancestraltraits to produce novel analyses in 
systematic and comparativebiology. A general model of horn evolution could extend well intoevolution 
of skull ornamentation throughout the animal kingdom.These tools will not only be important for understanding 
evolutionof complex traits, but they will enable any scientist investigatingchanges to existing structures 
arising from mathematicallymodeled phenomena will be able to use these tools. 5 References CONROY, 
G. C. AND VANNIER, M. W. 1984. Noninvasive three­dimensional computer imaging of matrix-filled fossil 
skulls byhigh-resolution computed tomography. Science, 226, 1236-1239. DECARLO, D. AND GALLIER, J. 1996. 
Topological evolution of surfaces. Graphics Interface, 194-203. GARLAND, T., JR., MARTIN, K. L. M., AND 
DÍAZ-URIARTE, R. 1997. Reconstructing ancestral trait values using squared-changeparsimony: plasma osmolarity 
at the origin of amniotes. InAmniote Origins: Completing the Transition to Land, Academic Press, San 
Diego. S. Sumida and K. Martin, Eds, 425-501. JERMANN, T. M., OPITZ, J. G., STACKHOUSE, J. AND BENNER, 
S. A. 1995. Reconstructing the evolutionary history of the artiodactylribonuclease superfamily. Nature, 
374, 57-59. *e-mail: wendyh@citrus.ucr.edu e-mail: reyes@tacc.utexas.edu e-mail: rowe@mail.utexas.edu 
e-mail: garland@citrus.ucr.edu Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965502</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Emissive clipping planes for volume rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965502</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965502</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31024919</person_id>
				<author_profile_id><![CDATA[81100051809]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hardenbergh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TeraRecon, Inc]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14226280</person_id>
				<author_profile_id><![CDATA[81100659281]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TeraRecon, Inc]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>602114</ref_obj_id>
				<ref_obj_pid>602099</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[KNISS, J., PREMOZE, S., HANSEN, C., EBERT, D. 2002. Interactive translucent volume rendering and procedural modeling. In Proceedings of the conference on Visualization '02, IEEE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>641510</ref_obj_id>
				<ref_obj_pid>641480</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WU, Y., BHATIA, V., LAUER, H., SEILER, L. 2003. Shear-image ray casting volume rendering. In Proceedings of the 2003 symposium on Interactive 3D graphics, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Emissive Clipping Planes for Volume Rendering Jan Hardenbergh Yin Wu TeraRecon, Inc  Introduction 
 Volume rendering has become an essential tool in medical imaging. Lighting allows the doctor to understand 
the shape of the organs and blood vessels and spot abnormalities. Cutting through the volume allows one 
to see the tissues in relation to one another. However, when homogeneous materials are cut and illuminated, 
the result is speckled with bright and dark splotches. By rendering a thin slice with only emissive illumination, 
the tissue in the clip plane is revealed as well as the structure of the organs below. The result is 
a useful diagnostic image. In volume data, greater differences in tissue density yield larger estimated 
gradients. By making the less dense tissue transparent we display the surfaces that are implicitly defined 
in the data. By using the gradients as surface normals in the Blinn-Phong illumination model, the surfaces 
facing the light get brighter; the back facing surfaces do not contribute and create dark areas, which 
are only exposed when clipping. These dark areas are replaced with material color in the emissive clip 
plane. In homogeneous material, neighboring voxels have similar values. However, there is some noise 
due to formations smaller than the voxel spacing. Gradients estimated between these voxels are small 
and have random orientations. When these gradients are used for lighting, the result is extremely distracting. 
Kniss recognized this problem and proposed using an alternate illumination model for low gradient samples. 
Our solution is less general, but works with existing special purpose hardware[Wu] and maintains interactive 
frame rates. It is also applicable to volume rendering algorithms using precomputed gradients on commodity 
graphics hardware such as the ATI Radeon or NVIDIA GeForce. e-mail: { jch , wu }@terarecon.com Images 
rendered with the TeraRecon VolumePro 1000 [Wu].  Exposition In our implementation, we render two times. 
First, we render the volume behind the clip plane with lighting turned on. Second, we render the thin 
emissive slice with no diffuse or specular reflection and maximum emissive illumination. The additional 
render is so thin that it does not decrease performance. This technique works best when there is no partially 
translucent homogeneous material. In medical imaging is to common to render the less dense material as 
transparent and transition relatively quickly to totally opaque. We did consider and implement gradient 
modification at the clip surface. This will solve the problems and does help understand the orientation 
of the cut. However, it does not help see the material in the plane, which is the goal in our case. It 
is an added computational expense with no gain in image quality. The emissive clip plane provides a pragmatic 
solution for two illumination problems when cutting through a volume: random lighting in homogeneous 
materials and dark areas caused by back facing surfaces. The result is an image that both shows a clean 
cut surface and the illuminated structures below it. References KNISS, J., PREMOZE, S., HANSEN, C., 
EBERT, D. 2002. Interactive translucent volume rendering and procedural modeling. In Proceedings of the 
conference on Visualization 02, IEEE Press. WU, Y., BHATIA, V., LAUER, H., SEILER, L. 2003. Shear-image 
ray casting volume rendering. In Proceedings of the 2003 symposium on Interactive 3D graphics, ACM Press. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965501</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[The digital space shuttle, 3D graphics, and knowledge management]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965501</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965501</url>
		<abstract>
			<par><![CDATA[The Digital Shuttle is a knowledge management project that seeks to define symbiotic relationships between 3D graphics and formal knowledge representations (ontologies). 3D graphics provides geometric and visual content, in 2D and 3D CAD forms, and the capability to display systems knowledge. Because the data is so heterogeneous, and the interrelated data structures are complex, 3D graphics combined with ontologies provides mechanisms for navigating the data and visualizing relationships.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653895</person_id>
				<author_profile_id><![CDATA[81332501356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Julian]]></first_name>
				<middle_name><![CDATA[E]]></middle_name>
				<last_name><![CDATA[G&#243;mez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Research Institute for Advanced Computer Science, NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653934</person_id>
				<author_profile_id><![CDATA[81406592155]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Keller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NASA Ames Research Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Digital Space Shuttle, 3D Graphics, and Knowledge Management  Julian E Gómez* Research Institute 
for Advanced Computer Science NASA Ames Research Center Paul J. Keller** NASA Ames Research Center  
  Abstract The Digital Shuttle is a knowledge management project that seeks to define symbiotic relationships 
between 3D graphics and formal knowledge representations (ontologies). 3D graphics provides geometric 
and visual content, in 2D and 3D CAD forms, and the capability to display systems knowledge. Because 
the data is so heterogeneous, and the interrelated data structures are complex, 3D graphics combined 
with ontologies provides mechanisms for navigating the data and visualizing relationships. Introduction 
 Digital Shuttle will allow creation of a virtual Space Shuttle Orbiter model, based on intelligent Engineering 
Objects (EOs) and immersive display methodologies. The goal is to create semi-standalone encapsulated 
forms of knowledge of the Orbiter, with embedded graphical representations, providing accessibility through 
maintenance of relationship information external to a given application. This improves not just the 
Product Lifecycle Management of the system but also overall safety, as relationship information is made 
explicit as it is used within the lifecycle. Since the Space Transportation System (STS) was designed 
before modern 3D CAD, largely there is no 3D data. Generating this information is an essential step for 
visualization of the Orbiter and its systems, and allows sustaining engineering and support activities 
to take better advantage of current tools. The EOs represent knowledge collections of entities within 
the Orbiter, and are defined in an ontological framework. The knowledge collection includes legacy data 
and documents such as drawings, mishap reports, materials requirements, stress profiles, etc., as well 
as data yet to be generated, such as the CAD models. Ontologies also include systems knowledge, e.g. 
a brake on the landing gear is connected to both the hydraulic system (for actuation) and the electrical 
system (for status/fault indicators). Exposition Digital Shuttle data is severely heterogeneous, meaning 
that there are many different types of primitive data beyond strings, dates, and numbers. 3D graphics 
will be used for content, display and interaction, and navigation. To some extent, the 3D data forms 
a basic unit of information within the knowledge base, as a peer to, for example, strings. The CAD models 
currently being built are a good example of data heterogeneity. Drawings are 2D graphics, which are being 
laboriously converted to 3D. However, the drawings are annotated, and these notes form a critical part 
of the knowledge embodied in the drawings. Thus the 3D data must have some way of incorporating the annotations 
as essential structured data, but in a form that is both machine and human usable, such as RDF. Conventional 
3D display techniques, including stereo, provide different levels of image quality. Special purpose hardware, 
such as domes or head mounted displays, provide more immersive image generation. 3D audio is now available 
on all computer systems, and haptic devices are cheap enough to be relatively ubiquitous. These will 
augment the user experience so that it will be much more immersive. The addition of 3D audio and the 
haptic devices is expected to significantly differentiate Digital Shuttle from 3D systems that have come 
before it. A typical deskside computer currently provides enough power to drive it, so the average NASA 
engineer can expect to have access without a noticeable expenditure. More involved experiences may require 
labs or centers with larger investments.          In addition to content and display, 3D graphics 
is essential for dealing with the information architecture within Digital Shuttle. To begin with, the 
common concept of drill down becomes extended to drill in any direction and then on to drill to another 
dimension or context . Whenever a user is working in any part of the system, it should be possible to 
quickly access or shift to another, probably related, part of the system. An example might be while looking 
at the CAD of a brake, to move over to the hydraulic system logical layout, and then to the stress profile 
display for the brake, and then to an animated display of that stress profile over time. In a larger 
sense, the Orbiter contains a number of complex interacting systems. To develop ontologies requires some 
sophisticated tools; to make the ontologies co-exist with alternate data representations, such as 3D 
CAD, requires sophisticated tools with graphics. Again, the user should be able to switch context. Working 
with the ontologies presents a problem by itself. Since they will use RDF, there is a question of how 
to work with the information structure of a dataset with the complexity of the Digital Shuttle. In this 
case, the ontologies present yet another part of the overall visualization problem in Digital Shuttle, 
which means that 3D can be used to navigate and work with the ontology data structures. In this way the 
ontologies present both data to be visualized and information structure to be visualized, and in both 
cases 3D provides a key mechanism for the solution. -------------------------------------------- * email: 
jgomez@mail.arc.nasa.gov ** email: Paul.J.Keller@nasa.gov 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965505</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Designer-critiqued comparison of 2D vector visualization methods]]></title>
		<subtitle><![CDATA[a pilot study]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965505</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965505</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653846</person_id>
				<author_profile_id><![CDATA[81100096002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cullen]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Jackson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24016921</person_id>
				<author_profile_id><![CDATA[81100403423]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Acevedo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P62493</person_id>
				<author_profile_id><![CDATA[81100589961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Laidlaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653873</person_id>
				<author_profile_id><![CDATA[81367594145]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fritz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Drury]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[R.I.S.D.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P412624</person_id>
				<author_profile_id><![CDATA[81100382211]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Eileen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vote]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P412613</person_id>
				<author_profile_id><![CDATA[81100317632]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keefe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>601693</ref_obj_id>
				<ref_obj_pid>601671</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LAIDLAW, D. H., KIRBY, R. M., DAVIDSON, J. S., MILLER, T. S., DA SILVA, M., WARREN, W. H., AND TARR, M. J. 2001. Quantitative Comparative Evaluation of 2D Vector Field Visualization Methods. In Proceedings of IEEE Visualization 2001, 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Designer-critiqued Comparison of 2D Vector Visualization Methods: A Pilot Study Cullen D. Jackson, 
Daniel Acevedo, David Laidlaw* Fritz Drury Eileen Vote, Daniel Keefe* Brown University R.I.S.D. Brown 
University {cj, daf, dhl}@cs.brown.edu fdrury@risd.edu {evote, dfk}@cs.brown.edu  Figure 1. The six 
visualization methods the designer critiqued. The large image shows a full screen shot. The other six 
are details from each method (clockwise from top-left: JIT, LIC, LIT, OSTR, GRID, GSTR). The circles 
represent an advection task used in a previous study [Laidlaw et al. 2001]. Observers were asked to indicate 
where a particle in the flow, starting at the small concentric circle, would intersect the large concentric 
circle; the other circle represents the correct intersection point. 1 Introduction Evaluation of scientific 
visualization methods is typically either anecdotal, with feedback from scientific users, or quantitative, 
with performance measured on simple abstract tasks performed by relatively naïve users. While scientific 
users can provide domain­specific feedback, neither population is typically trained to provide visual-design 
feedback. The current pilot study examines how expert graphic design knowledge can provide a fast and 
robust visualization evaluation methodology, one that assesses scientific visualizations for their scientific 
value while also improving the design and composition of the visualizations. Since graphic designers, 
particularly illustrators, are trained to judge how well visual designs convey specific pieces of information, 
we believe they can evaluate scientific visualizations for how well they fulfill design goals based both 
on the scientific task represented and the actual visual design. 2 Our Approach In a previous user study 
[Laidlaw et al. 2001], we quantitatively evaluated several 2D vector field visualizations. We proposed 
three tasks to understand the flow in a bounded 2D vector field. For each task, we measured the accuracy 
and execution time for each observer. We found that particular visualization techniques were more suited 
to certain tasks than others. We will have designers grade scientific visualization methods based on 
their subjective estimates of user performance for certain tasks, and give verbal feedback (i.e., critique) 
on the effectiveness of each method for fulfilling these tasks. We hypothesize that designers will rank 
the methods similarly to objectively measured task performance. We also believe that the critiques will 
enable us to understand why methods work well and to synthesize better visualization techniques. This 
will enable us to identify which elements of these methods work best for the given tasks. * Department 
of Computer Science, Brown University, Providence, RI 02912 Department of Illustration, Rhode Island 
School of Design, Providence, RI 02903  Copyright held by the author GSTR JIT LIC LIT OSTR GRID Accuracy 
Designer A- D B- B+ A C- Prev. Study 1.3% 4.5% 19% 3.5% 0.9% 4% Time Designer A- D B- B+ A C- Prev. Study 
3.5 s 3.5 s 5.0 s 3.5 s 3 s 3.5 s Table 1. Scores given for the six visualization methods from designer 
critique and the previous user study [Laidlaw et al. 2001] for the advection task (see Figure 1). Accuracy 
for the user study is given as error rate and time for task completion is in seconds.  3 Pilot Study 
One designer gave grades to the visualization methods as an estimate of user performance for an advection 
task on two data sets with each method. He first ran through the previous study as training to understand 
the advection task. We then presented him two sets of images like the ones in Figure 1. We recorded his 
comments about the visualizations during his critique. He thought that the JIT method was the worst of 
the six techniques because its visual elements were too small. He stated that the OSTR method was the 
best, partly because it was not too busy in terms of the density of the visual elements. He also stated 
that this method gave the least 3D sense, which was good since this could confuse the interpretation 
of the data. The designer also commented on the other four visualization methods. We show the results 
of this critique along with the results from the previous user study in Table 1. 4 Discussion A visual 
design professional analyzed the perceptual and compositional characteristics of several visualization 
methods with respect to certain scientific goals. We found that his subjective estimates of user performance 
were similar to previous quantitative performance measures. We also recorded his comments concerning 
the elements of each method. We conclude that using this type of evaluation will give us a knowledge 
base to generate new and improved visualizations quickly and with better chances of success. Acknowledgements 
This work was partially supported by a National Science Foundation ITR grant (CCR-0086065). References 
LAIDLAW, D. H., KIRBY, R. M., DAVIDSON, J. S., MILLER, T. S., DA SILVA, M., WARREN, W. H., AND TARR, 
M. J. 2001. Quantitative Comparative Evaluation of 2D Vector Field Visualization Methods. In Proceedings 
of IEEE Visualization 2001, 143-150.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965506</section_id>
		<sort_key>23</sort_key>
		<section_seq_no>23</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Textures]]></section_title>
		<section_page_from>23</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P590095</person_id>
				<author_profile_id><![CDATA[81320488260]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ioana]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Boier-Martin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM T.J. Watson Research Center]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965509</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Spectral texturing for real-time applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965509</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965509</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653851</person_id>
				<author_profile_id><![CDATA[81100228621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Max Planck Institute for Biological Cybernetics, T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258882</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DEBONET, J. S. 1997. Multiresolution sampling procedure for analysis and synthesis of texture images. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, T. Whitted, Ed., Computer Graphics Proceedings, Annual Conference Series, ACM, 361--368.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Spectral Texturing for Real-Time Applications ubingen, Germany 1 Introduction In this sketch we present 
a new method for rendering large-scale, high-resolution, non-repetitive textures in real-time using multi 
layer texturing. The basic idea of spectral texturing is to construct the .nal texture by multiple texture 
layers where each layer pro­vides a certain range of the spectrum of the texture s spatial fre­quencies. 
Alpha channels are used to introduce statistical depen­dencies between the frequency bands. This approach 
extends a method called detail texturing, which does not use alpha channels to model higher statistical 
properties of the resulting texture. Other approaches to generate textures with speci.ed statistical 
properties, like [DeBonet 1997], are not suitable for real-time use and would require storage of the 
generated texture. Spectral texturing is very easy to implement, runs on all contem­porary 3d graphics 
cards, and is especially suitable for naturalistic textures in real-time applications which are viewed 
from a large range of distances.  2 Method Figure 1 shows a groundplane with a .ve-layer spectral texture 
(top). The texture of all .ve layers is the same low-pass .ltered ran­dom noise texture tile of 512*512 
pixels (bottom left), which tiles seamlessly. The texture contains a textured transparency channel (bottom 
right), which has in this case been generated by applying an emboss .lter to the color texture. The layer 
with the smallest tiles is rendered .rst. On top of this the next-larger scaled layer is rendered using 
alpha blending, and so on. The transparency map of each layer de.nes where the pre­viously rendered layers 
(containing higher frequency components of the texture) will be visible. It can, for example, make the 
ampli­tude of higher-frequency content depend on the phase of the texture layer s color map, as in Figure 
1. In this example there are 64x64 tiles per km2 at the smallest scale; at the largest scale the texture 
tile is stretched over 10x10 km. This results in a virtual texture size of 3276802 pixels (a texture 
of 10x10 km at a resolution of approximately 3x3 cm per pixel). In texture memory, such a texture would 
take up 300 GB. As a spectral texture, it uses just 1 MB, but rendering needs .ve textured layers. Both 
mip-mapping and bilinear .ltering are used to assure smoothness of the texture at all magni.cation levels. 
The textures themselves are low-pass .ltered to such a degree that they do not produce artifacts when 
magni.ed. As the different layers of the spectral texture are magni.ed differently at the same distance, 
they also go into mip-mapping and bilinear interpolation at different distances. This results in a texture 
which has .ne detail at a large range of distances without producing aliasing artifacts. The examples 
shown were all generated by manually adjusting the texture parameters. Currently we are working on a 
method to analyze a given image for its statistical properties, and to use these properties to generate 
texture tiles and mixing parameters for a similar-looking spectral texture. For the analysis a method 
similar to the one shown in [DeBonet 1997] could be used. * e-mail: daniel.berger@tuebingen.mpg.de  
 Figure 2: More examples of real-time spectral textures  3 Conclusion Spectral texturing can be used 
to simulate different sorts of clouds, gravel, stone, concrete, mud, vegetation, and other naturalistic 
tex­tures with random and fractal characteristics in real-time. Therefore this approach should be especially 
advantageous for 3d computer games and VR environments. See also: http://www.tuebingen.mpg.de/.berger/spectraltexturing/ 
 References DEBONET, J. S. 1997. Multiresolution sampling procedure for analysis and synthesis of texture 
images. In Proceedings of SIGGRAPH 1997, ACM Press / ACM SIGGRAPH, T. Whit­ted, Ed., Computer Graphics 
Proceedings, Annual Conference Series, ACM, 361 368. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965510</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Fast texture synthesis on arbitrary meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965510</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965510</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P590123</person_id>
				<author_profile_id><![CDATA[81100532138]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois, Urbana-Champaign]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15035731</person_id>
				<author_profile_id><![CDATA[81100542239]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kriegman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383296</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[EFROS, A. A., AND FREEMAN, W. T. 2001. Image quilting for texture synthesis and transfer. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, ACM Press, 341--346.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>543017</ref_obj_id>
				<ref_obj_pid>543015</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LEUNG, T., AND MALIK, J. 2001. Representing and recognizing the visual appearance of materials using 3d textons. International Journal of Computer Vision 43(1), 29--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast Texture Synthesis on Arbitrary Meshes Sebastian Magda University of Illinois Urbana-Champaign 
 1 Introduction Fast and high quality texture synthesis on surfaces with little user intervention is 
still dif.cult to achieve. The algorithm presented here brings us closer to that goal by generating high-quality 
tex­tures on arbitrary meshes in a matter of seconds. The result of this process is a mapping of every 
triangle in a mesh to the original tex­ture sample with no need for additional texture memory. The whole 
process is fully automatic, yet still user controllable. It also places no special restrictions on the 
mesh or on the texture, and the original mesh is not modi.ed in any way.  2 Exposition The basic idea 
behind the algorithm is to separate texture genera­tion into two independent phases: texture preprocessing 
and texture synthesis. While texture preprocessing is relatively slow, it only has to be done once per 
texture. At the same time, texture preprocessing makes the actual texture synthesis process fast. The 
goal of texture preprocessing is to identify sets of pixels in the sample texture whose neighborhoods 
have similar appearance. One way to accomplish this is to characterize the neighborhood of a pixel as 
a 2-D texton [Leung and Malik 2001]. By clustering textons, we can achieve a low-dimensional representation 
of neigh­borhoods. Each pixel in the texture is then assigned a texton label, so that pixels with similar 
neighborhoods are characterized by the same texton. The process of synthesis is an extension of the generic 
texture quilting algorithm [Efros and Freeman 2001]. While all existing 2-D texture quilting methods 
use square quilting patches for their simplicity, this does not apply to arbitrary 2-D manifolds in a 
3-D space. Since the basic 3-D primitive in computer graphics is a tri­angle, it makes sense to use triangles 
as our quilting blocks. The dif.culty comes from the fact that, unlike .xed size square patches in 2-D 
texture quilting, Triangles in a typical mesh have different shapes and sizes. The texture orientation 
will also be different from one triangle to another. We extend the basic quilting approach to handle 
arbitrarily shaped patches. The synthesis process starts from a randomly selected triangle, and the texture 
is grown on the sur­face from that initial face. The texture orientation on the mesh is de.ned by a surface 
tangential vector .eld. At each step, we try to select a triangular patch from our sample texture that 
best matches along edges with all textured neighbors in the mesh. We do that by actually matching textons 
lying on the already-textured edges. Since textons already characterize the local neighborhood, we es­sentially 
reduce the problem to a 1D string comparison and avoid direct neighborhood comparisons. Additional optimizations 
result in a fast algorithm that can synthesize texture on a mesh in a frac­tion of the time required 
by any existing algorithms [Turk 2001; Wei and Levoy 2001; Praun et al. 2000; Soler et al. 2002]. Exam­ples 
shown in .g. 1 took less than 5 seconds to synthesize (each mesh has between 5000 and 8200 faces). The 
synthesis quality is similar to the basic 2-D texture quilting algorithm. We additionally apply texture 
blending to smoothly blend across triangle edges. David Kriegman University of California San Diego 
 Figure 1: Examples of texture synthesis results for various meshes and texture samples. 3 Conclusion 
 We introduce a fast, patch-based method for automatically texturing triangular meshes. The algorithm 
is ideal for texturing a large set of objects (a forest scene containing many trees is a good example), 
or for interactive mesh modeling. Since the algorithm essentially performs texture mapping, it is also 
very memory ef.cient, and the rendering process is fast. All we need to store in the texture memory is 
simply the original texture sample. Finally, we are currently ex­tending the underlying synthesis method 
to other classes of texture data besides color texture images, particularly bidirectional texture functions 
(BTFs). This will allow the surfaces to be rendered with textures whose appearance varies with changes 
of viewpoint and lighting. References EFROS, A. A., AND FREEMAN, W. T. 2001. Image quilting for texture 
synthesis and transfer. In Proceedings of the 28th annual conference on Computer graphics and interactive 
tech­niques, ACM Press, 341 346. LEUNG, T., AND MALIK, J. 2001. Representing and recognizing the visual 
appearance of materials using 3d textons. Interna­tional Journal of Computer Vision 43(1), 29 44. Copyright 
held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965508</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[TensorTextures]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965508</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965508</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P557098</person_id>
				<author_profile_id><![CDATA[81100256929]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[Alex O.]]></middle_name>
				<last_name><![CDATA[Vasilescu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Toronto]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14109166</person_id>
				<author_profile_id><![CDATA[81100294834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Demetri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Terzopoulos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ACKNOWLEDGEMENT: Jared Silver provided invaluable CG modeling assistance.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>300778</ref_obj_id>
				<ref_obj_pid>300776</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DANA, K. J., VAN GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, J. J. 1999. Reflectance and texture of real-world surfaces. ACM Trans. Graphics 18, 1, 1--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882341</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HAN, J., AND PERLIN, K. 2003. Measuring bidirectional texture reflectance with a kaleidoscope. In ACM Trans. Graphics 22, in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383320</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[MALZBENDER, T., GELB, D., AND WOLTERS, H. 2001. Polynomial texture maps. In Proceedings of ACM SIGGRAPH 2001, 519--528.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566634</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[TONG, X., ZHANG, J., LIU, L., WANG, X., GUO, B., AND SHUM, H.-Y. 2002. Synthesis of bidirectional texture functions on arbitrary surfaces. ACM Trans. Graphics 21, 3, 665--672.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649173</ref_obj_id>
				<ref_obj_pid>645315</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[VASILESCU, M. A. O., AND TERZOPOULOS, D. 2002. Multilinear analysis of image ensembles: TensorFaces. In Proc. Euro. Conf. on Computer Vision, 447--460.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 TensorTextures M. Alex O. Vasilescu.,.and Demetri Terzopoulos.,. .Dept. of Computer Science, University 
of Toronto .Media Research Lab, New York University Figure 1: Frames from the Treasure Chest animation. 
The coin collection in the box is a TensorTexture mapped on a planar surface. Seen from different viewpoints 
(images 1 3) and under different illuminations (3 5), the TensorTexture appears to have considerable 
3D relief. 1. Introduction TensorTextures is a new image-based texture mapping technique. It is a generative 
model that, from a sparse set of example images, learns the interaction between viewpoint, illumination, 
and geom­etry, which determines surface appearance, including complex de­tails such as self-occlusion 
and self-shadowing. As Fig. 1 illus­trates, although the coins appear to have considerable 3D relief 
un­der continuously varying viewpoint and illumination, the collection of coins in the box is actually 
a synthesized TensorTexture mapped onto a planar surface. TensorTextures stems from a recently proposed 
multilinear (i.e., tensor) algebra approach to the analysis of image ensem­bles [Vasilescu and Terzopoulos 
2002]. Here, we apply the ap­proach to ensembles of images of textured surfaces. Given such an ensemble, 
our algorithm learns an approximation to the bidirec­tional texture function (BTF) [Dana et al. 1999], 
which describes the appearance of the textured surface under various viewpoints and illuminations. A 
generalization of the well-known BRDF, the BTF is a function of six variables (x,y,(v,v,(i,i), where 
(x,y)are pixel coordinates, ((v,v)is the viewpoint direction, and ((i,) i is the illumination direction. 
Unlike the method of [Malzbender et al. 2001] our technique es­timates the complete BTF, including variations 
not only in illumi­nation, but also in viewpoint. Unlike the BTF synthesis method of [Tong et al. 2002], 
our technique is purely image based it avoids the nontrivial problem of 3D geometric microstructure estimation. 
 2. TensorTextures Algorithm The TensorTextures algorithm starts by organizing an ensemble of texture 
images acquired under different viewpoints and illumina­tions as 3-mode array, or data tensor ., which 
indexes the im­ages according to pixel (x,y), viewpoint ((v,v), and illumination ((i,i)indices. Applying 
a multilinear generalization of the conventional matrix singular value decomposition (SVD), known as 
an N-mode SVD [Vasilescu and Terzopoulos 2002], with N=3, the data tensor .is then decomposed into the 
(3-mode) product of a core tensor (which is analogous to the SVD eigenvalue matrix) and three or­thogonal 
matrices. The columns of these matrices are eigenvectors that encode the observed variation in the texture 
image ensemble across the viewpoint, illumination, and pixel modes of .. Our TensorTextures algorithm 
represents the BTF as a multilin­ear combination of basis vectors that are expressed as the product 
 Copyright held by the author Variation in Illumination Variation in Illumination   Figure 2: TensorTexture 
bases for coins (left) and corn (right). of the core tensor and the pixel orthogonal matrix. Fig. 2 illustrates 
TensorTexture bases for the coins texture and a corn texture. Our algorithm can quickly synthesize textures 
for arbitrary viewpoints and illuminations as a prod­uct of the TensorTexture basis with interpolated 
viewpoint and illumination representations. 3. Results We have employed our TensorTextures algorithm 
to synthesize (from 777 sample images) the coins and corn TensorTextures featured in the Treasure Chest 
(Fig. 1) and Scare­crow (Fig. 3) animations.  References ACKNOWLEDGEMENT: Jared Silver provided invaluable 
CG modeling assistance. DANA, K. J., VAN GINNEKEN, B., NAYAR, S. K., AND KOENDERINK, J. J. 1999. Re.ectance 
and texture of real-world surfaces. ACM Trans. Graphics 18, 1, 1 34. HAN, J., AND PERLIN, K. 2003. Measuring 
bidirectional texture re.ectance with a kaleidoscope. In ACM Trans. Graphics 22, in press. MALZBENDER,T., 
GELB, D., AND WOLTERS, H. 2001. Polynomial texture maps. In Proceedings of ACM SIGGRAPH 2001, 519 528. 
TONG, X., ZHANG, J., LIU, L., WANG, X., GUO, B., AND SHUM, H.-Y. 2002. Syn­ thesis of bidirectional texture 
functions on arbitrary surfaces. ACM Trans. Graphics 21, 3, 665 672. VASILESCU, M.A.O., AND TERZOPOULOS, 
D. 2002. Multilinear analysis of image ensembles: TensorFaces. In Proc. Euro. Conf. on Computer Vision, 
447 460.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965507</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Texture synthesis from multiple sources]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965507</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965507</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14165448</person_id>
				<author_profile_id><![CDATA[81452594229]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Li-Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>345009</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[WEI, L.-Y., AND LEVOY, M. 2000. Fast texture synthesis using tree-structured vector quantization. In Proceedings of ACM SIGGRAPH 2000, 479--488.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>935082</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WEI, L.-Y. 2002. Texture Synthesis by Fixed Neighborhood Searching. PhD thesis, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Texture Synthesis from Multiple Sources Li-Yi Wei* Stanford University 1 Introduction Recent advances 
in texture synthesis have made it possible to syn­thesize textures based on examples. However, most of 
these al­gorithms can take only a single texture as the input and output a similar homogeneous texture. 
This is insuf.cient for textures that have progressively variant patterns or combined visual characteris­tics 
from several different sources. We present a new algorithm for synthesizing textures from mul­tiple sources. 
We demonstrate two specializations of our algorithm that are particularly useful: generating texture 
mixtures from differ­ent inputs and synthesizing solid textures from multiple 2D views.  2 Algorithm 
Our algorithm is extended from [Wei and Levoy 2000]. In the following, we describe the major extentions 
of our algorithm over [Wei and Levoy 2000] for multiple-source synthesis. The input of the algorithm 
consists of several texture sources. Those sources can be different views of a solid texture, or differ­ent 
patterns for a texture mixture. Each source is associated with a weight image which has the same size 
as the output. Those weights are user selected parameters specifying how the input sources should be 
mixed together. In addition, each source is asso­ciated with a neighborhood parameter, de.ning the size 
and shape of the neighborhood used during the search process for this spe­ci.c source. This is necessary 
for texture mixture synthesis where different sources have different texton sizes, or solid texture synthe­sis 
where each view has different neighborhoods oriented with the corresponding viewing directions. In [Wei 
and Levoy 2000], each output pixel is determined so that the local similarity between the input and output 
textures is pre­served as much as possible. We would like to achieve the same goal for multi-source texture 
synthesis. However, since we now have more than one input textures, we have to pick the output pixel 
value that preserves local similarity simultaneously with all the in­put sources as much as possible. 
Mathematically, for each output sample p we would like to .nd a set of input pixels {pi} so that the 
following error function is minimized: E(p, {pi})=wi × ( p - pi 2 + Ni(p)- Ni(pi) 2) (1) i where index 
i runs through all the input textures, p and each pi are the output and matching input pixels, and Ni(p), 
Ni(pi)are their neighborhoods. Note that we use different neighborhoods Ni for different sources. The 
error function is computed as a weighted sum of the L2 norm between {p, Ni(p)} and {pi,Ni(pi)}, and the 
weights {wi} specify the relative importance of the input textures. To minimize the error function E(p, 
{pi}), we need to deter­mine the values p and {pi} so that the sum on the right hand side of Equation 
1 is minimized. When only one source is present, we can directly minimize Equation 1 as follows: simply 
choose the pi such that Ni(p)- Ni(pi) 2 is minimal, and set p to be equal to pi. This is exactly the 
algorithm presented in [Wei and Levoy *e-mail: liyiwei@graphics.stanford.edu 2000]. However, when multiple 
sources are present, we cannot solve Equation 1 directly. Instead we use an iterative procedure, alternatively 
setting the values of {pi} and p while gradually de­creasing E(p, {pi}). At the beginning of each iteration, 
we .x the value p and choose {pi} so that each individual error term p - pi 2 + Ni(p)- Ni(pi) 2 is minimized. 
We then keep {pi}.xed, and set p as the weighted average of {pi}. It can be easily proven that both of 
these steps do not increase E(p, {pi}). The pro­cess can be stopped when E(p, {pi})remains the same after 
two consecutive iterations, but experimentally we have found that 1 to 4 iterations are suf.cient. We 
now describe how to apply our algorithm to synthesize solid textures and texture mixtures. To synthesize 
texture mixtures, we use the weight images to control how each source effects the mix­ture result. For 
example, by assigning equal weighting to all the inputs, the result will be a uniform mixture of the 
input textures. However, by spatially varying the weight images, we can achieve special effects such 
as one texture gradually transforming to an­other (Figure 1). To synthesize solid textures from multiple 
views, we simply specify several images consisting of different views of a hypothesized solid texture. 
Each view is associated with a neigh­borhood oriented with respect to the speci.c viewing direction. 
Fig­ure 2 demonstrates several synthesis results. For further details and results, we refer the reader 
to [Wei 2002][Chapter 7].  Figure 1: Texture mixture results. For each row of images, the two input 
textures are shown on the left, and the corresponding synthesis results, with equal weighting and ramp 
weighting, are shown on the right.  Figure 2: Solid texture synthesis results. For each pair of images, 
the original texture is shown on the left, and the corresponding synthesis result, a sphere of solid 
texture, is shown on the right. References WEI, L.-Y., AND LEVOY, M. 2000. Fast texture synthesis using 
tree-structured vector quantization. In Proceedings of ACM SIGGRAPH 2000, 479 488. WEI, L.-Y. 2002. Texture 
Synthesis by Fixed Neighborhood Searching. PhD thesis, Stanford University. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965511</section_id>
		<sort_key>24</sort_key>
		<section_seq_no>24</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Archeological reconstruction]]></section_title>
		<section_page_from>24</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP311527300</person_id>
				<author_profile_id><![CDATA[81100561258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yiorgos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chrysanthou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cyrpus]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965514</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Reconstructing a colossus of Ramesses II from laser scan data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965514</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965514</url>
		<abstract>
			<par><![CDATA[We present a new method to solve a 3D 'jigsaw puzzle', building a colossus from scans of its fragmentary ruins in Thebes, Egypt.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653904</person_id>
				<author_profile_id><![CDATA[81100506574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653834</person_id>
				<author_profile_id><![CDATA[81100312576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Casondra]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sobieralski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24007327</person_id>
				<author_profile_id><![CDATA[81100018898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[&#201;cole normale sup&#233;rieure, INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CURLESS, B., LEVOY, M. 1996. A volumetric method for building complex models from range images. In Proceedings of SIGGRAPH 1996, ACM Press/ACM SIGGRAPH, 303--312]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Reconstructing a Colossus of Ramesses II from Laser Scan Data Kevin Cain* Casondra Sobieralski Philippe 
Martinez INSIGHT INSIGHT École normale supérieure Abstract We present a new method to solve a 3D jigsaw 
puzzle , building a colossus from scans of its fragmentary ruins in Thebes, Egypt. 1 Field Work in Egypt 
While one of the largest colossi ever built by ancient Egyptian hands, a colossus of Ramesses II now 
lies in a ruinous state at the site of the Ramesseum in Thebes, Egypt. Under the site direction of Dr. 
Christian Leblanc (CNRS), our team assembled a detailed digital reconstruction of the fallen statue for 
use in evaluating pending physical restoration work on this important monument. Of the many hundreds 
of extant fragments, we selected 90 for detailed documentation. Our method relied on active optical triangulation 
scanning to derive 3D models as shown below:  Figure 2. Exploded view (left), false color model (right). 
*e-mail: kevin@insightdigital.org e-mail: casondra@insightdigital.org e-mail: pmartine@ens.fr, philippe@insightdigital.org 
INSIGHT 2 Process, Results and Future Work Once the individual fragments of the colossus had been modeled, 
our team began to assemble the jigsaw pieces under the direction of the French Egyptologist Dr. Philippe 
Martinez using software kindly provided to our team by Dr. Levoy at Stanford University. Figure 3. The 
assembled 3D model of the colossus. In order to correctly deduce the spatial relationship between fragments, 
we scanned the Northern Memnon colossus in Thebes to use as a 3D reference template for our reconstruction. 
The proportions of our reconstruction above reflect this template data.  Figure 4. Comparison between 
actual and digital fragments. In many cases we were able to digitally rejoin destroyed pieces of a larger 
whole, as seen above where two fragments are joined along a break. The void regions in our model will 
be in filled in our future work, and this colossus model will be rendered into its proper relation with 
the Ramesseum, which was also documented with optical triangulation during our field work. References 
CURLESS, B., LEVOY, M. 1996. A volumetric method for building complex models from range images. In Proceedings 
of SIGGRAPH 1996, ACM Press / ACM SIGGRAPH, 303-312 Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965512</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A high fidelity reconstruction of ancient Egypt]]></title>
		<subtitle><![CDATA[the temple of Kalabsha]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965512</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965512</url>
		<abstract>
			<par><![CDATA[Accurate light simulation is of highest importance in archaeological reconstructions when you want to investigate how a site might have looked like in the past. Failure to use the highest fidelity means there is a very real danger of misrepresenting the past. This sketch describes the high fidelity reconstruction and realistic lighting simulation of an ancient Egyptian temple.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[ancient Egypt]]></kw>
			<kw><![CDATA[high fidelity graphics]]></kw>
			<kw><![CDATA[virtual archaeology]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP15030142</person_id>
				<author_profile_id><![CDATA[81100357979]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Veronica]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sundstedt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023542</person_id>
				<author_profile_id><![CDATA[81100086839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chalmers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol, United Kingdom]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24007327</person_id>
				<author_profile_id><![CDATA[81100018898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Normale Superieure, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CHALMERS, A., AND DEVLIN, K. 2002. Recreating the Past ACM SIGGRAPH 2002 Course #27 notes, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SIEGLER, K. G. AND ROMBOCK, U. 1970, Kalabsha: Architektur und Baugeschichte des Tempels. Gebr. Mann Verlag, Berlin.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286090</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[WARD LARSON, G. AND SHAKESPEARE, R. 1998, Rendering with RADIANCE: The art and science of lighting]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[WRIGHT, G. R. H., 1972 Kalabsha: The preserving of the temple. Gebr. Mann Verlag, Berlin.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A High Fidelity Reconstruction of Ancient Egypt: The Temple of Kalabsha Veronica Sundstedt* Alan Chalmers 
 Philippe Martinez University of Bristol, UK University of Bristol Ecole Normale Superieure Linköping 
University, Sweden United Kingdom France Abstract Accurate light simulation is of highest importance 
in archaeological reconstructions when you want to investigate how a site might have looked like in the 
past. Failure to use the highest fidelity means there is a very real danger of misrepresenting the past. 
This sketch describes the high fidelity reconstruction and realistic lighting simulation of an ancient 
Egyptian temple. CR Categories and Subject Descriptions: I.3.7 [Computer Graphics]: Three-Dimensional 
Graphics and Realism -Animation, color, shading, shadowing and texture; I.4.8 [Image Processing and Computer 
Vision]: Scene Analysis -Time-varying imagery. Keywords: high fidelity graphics, virtual archaeology, 
ancient Egypt 1 Introduction The ancient Egyptian temple of Kalabsha dates back to 30 BC. In 1963 the 
temple was moved to a new site in order to save it from the rising waters of the Lake Nasser. Computer 
graphics in collaboration with Egyptologists makes it possible to recreate the temple, place it back 
to its original location and orientation, and illuminate it, as it may have appeared some 2000 years 
ago. 2 Process and Methodology The computer model was accurately created in Maya based on the detailed 
measurements made in 1963 [Siegler and Rombock 1970; Wright 1972]. The textures were gathered on site 
by taking photographs with and without a green card. The spectral values of the green card are known 
and this allowed us to obtain light-free textures from the photographs. This was important since we introduced 
our own simulated light in the reconstructions [Chalmers and Devlin 2002]. The sun was a key feature 
in ancient Egyptian religion. The position and orientation of the temple would thus have been carefully 
chosen when it was built. Knowing the original location and orientation of the temple before it was moved 
allows us to place it back as it was in the past prior to carrying out the lighting simulations. Furthermore, 
the location of the sun was repositioned as it would have appeared in 30 BC. The model was rendered with 
Radiance to ensure a high fidelity result [Ward Larson and Shakespeare 1998]. In addition to daylight 
simulations, Egyptologists wanted to investigate the perception of the interior of the temple as it was 
lit in ancient times. From analysis of lamps found in temples, it is clear that the interiors of temples 
would have been illuminated by sesame oil. The spectral properties of sesame oil flame were measured 
and lamps reconstructed based on examples in the Cairo museum. In addition to providing high fidelity 
lighting simulations, it is also possible, on the computer, to restore parts of the temple, which have 
been damaged over the passage of time. *e-mail: veronica@cs.bris.ac.uk e-mail: alan.chalmers@bris.ac.uk 
e-mail: pmartine@ens.fr  3 Results An animation was created showing how sunlight would have affected 
the perception of the temple over a ten-hour period in January 30 BC. Figure 1, shows a single frame 
from this animation. A comparison of this animation and the video footage taken at the site in January 
2003, allows the Egyptologists to compare just how the new location and orientation of the temple, in 
addition to the damage, has indeed altered the perception of the temple, figure 2. The interior of the 
temple was also modelled in detail. This allows the Egyptologists to see how the hieroglyphics would 
have been perceived in the past, not as they are seen today, unpainted (the paint having long since faded) 
and under modern lighting, but brightly painted under the illumination of sesame oil lamps.  Figure 
2: The same scene at 9am, January 2003.  References CHALMERS, A., AND DEVLIN, K. 2002. Recreating the 
Past ACM SIGGRAPH 2002 Course #27 notes, July. SIEGLER, K. G. AND ROMBOCK, U. 1970, Kalabsha: Architektur 
und Baugeschichte des Tempels. Gebr. Mann Verlag, Berlin. WARD LARSON, G. AND SHAKESPEARE, R. 1998, 
Rendering with RADIANCE: The art and science of lighting WRIGHT, G.R.H., 1972 Kalabsha: The preserving 
of the temple. Gebr. Mann Verlag, Berlin. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965513</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Assembling the sculptures of the Parthenon]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965513</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965513</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14121467</person_id>
				<author_profile_id><![CDATA[81100336707]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jessi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stumpfel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43124877</person_id>
				<author_profile_id><![CDATA[81100346087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tchou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43117244</person_id>
				<author_profile_id><![CDATA[81100179328]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hawkins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP40023545</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43127786</person_id>
				<author_profile_id><![CDATA[81452612087]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18002178</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653832</person_id>
				<author_profile_id><![CDATA[81371593065]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Emerson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24007327</person_id>
				<author_profile_id><![CDATA[81100018898]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ecole Normale Superieure]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653962</person_id>
				<author_profile_id><![CDATA[81100508738]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Tomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lochman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Basel Skulpturhalle]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>940008</ref_obj_id>
				<ref_obj_pid>939836</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CIGNONI, P., ROCCHINI C., MONTANI, C., SCOPINGNO, R. 2003 (to appear). External Memory Management and Simplification of Huge Meshes IEEE Trans. On Visualization and Computer Graphics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344849</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M., ET AL. 2000. The Digital Michelangelo Project: 3D Scanning of Large Statues. In Proceedings of SIGGRAPH 2000, 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[TCHOU, C. 2002. Image-Based Models: Geometry and Reflectace Acquisition Systems. MS thesis, University of California at Berkeley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Assembling the Sculptures of the Parthenon Jessi Stumpfel, Chris Tchou, Tim Hawkins, Paul Debevec, Jonathan 
Cohen, Andrew Jones, Brian Emerson, University of Southern California, Institute for Creative Technologies 
Philippe Martinez, Ecole Normale Superieure Tomas Lochman, Basel Skulpturhalle  Although the Parthenon 
has stood on the Athenian Acropolis for nearly 2,500 years, its sculptural decorations have been scattered 
to museums around the world. Many of its sculptures have been damaged or lost. Fortunately, most of the 
decoration survives through drawings, descriptions, and casts. A component of our Parthenon Project has 
been to assemble digital models of the sculptures and virtually reunite them with the Parthenon. This 
sketch details our effort to digitally record the Parthenon sculpture collection in the Basel Skulpturhalle 
museum, which exhibits plaster casts of almost all of the existing pediments, metopes, and frieze. Our 
techniques have been designed to work as quickly as possible and at low cost. Since the Parthenon sculptures 
are numerous, we designed a scanning system optimized for speed at 1mm accuracy rather than for maximal 
accuracy as in [Levoy et al. 2000]. To address this we built a custom structured light scanning system 
using a Pulnix TM-1040 black and white CCD camera, a Proxima Ultralight x350 DLP projector, a simple 
mounting system, a large checkerboard calibration object, and a portable computer. The system allowed 
us to capture 3D scans at 1024x1024 resolution at less than 30 seconds per scan. Using new algorithms 
for depth estimation from structured lighting [Tchou 2002], we achieved a depth resolution of better 
than 0.2mm at each pixel. Reflectance information from the casts was recorded in color by projecting 
red, green, and blue light onto the casts with the video projector. Because of the efficiency of the 
system, we were able to scan over 440 linear feet of frieze, 52 metopes, and both pediments in just five 
days with a team of four people, totaling nearly 2,200 individual scans. Our calibration process involved 
scanning a flat checkerboard grid pattern in several orientations, allowing us to solve for the distortion 
characteristics and the relative orientations of the camera and projector. We developed methods for extracting 
these calibrations using HDRShop and Matlab, and then streamlined the process into a single Windows application. 
Given such calibration parameters and a scan of a sculpture, the 3D structure of the visible surface 
is triangulated. We found it was faster to remove undesirable geometry in the 2D camera image space before 
the scans were converted to 3D meshes. For this we developed a program that allows the user to view the 
scanned surface normals and mask out specific regions. To fill geometry holes caused by occlusions in 
the scan images, each sculpture was scanned from multiple angles. Because scans were captured from different 
vantage points, they needed to be aligned to each other. In many cases, such as the frieze, the sculpture 
pieces were arranged lengthwise in the museum, allowing us to take the scans at regularly spaced intervals. 
By deriving the relationships between the first few meshes, an initial estimate of relative alignment 
could be propagated to the rest of the scans in that section of frieze, and then refined using the Iterative 
Closest Point (ICP) algorithm. We used MeshAlign 2.0 written by the Visual Computing Group at the Istituto 
di Scienza e Tecnolgia dell Informazione-C.N.R to perform both the initial point-based alignment and 
ICP. We also used their surface reconstruction program [Cignoni et al. 2003] to merge our aligned meshes. 
Our scanned models have allowed us to combine the sculptures with models of the Parthenon from several 
eras and to create virtual renderings using both global illumination rendering and subsurface scattering 
techniques. The sculpture database forms the basis of the next phase of the project: to produce restored 
models appropriate for the original Parthenon based on archeological research. This project provides 
a nuts and bolts example of how advanced technologies can be applied in archaeological contexts quickly 
and at low cost.  References CIGNONI, P., ROCCHINI C., MONTANI, C., SCOPINGNO, R. 2003 (to appear). 
External Memory Management and Simplification of Huge Meshes IEEE Trans. On Visualization and Computer 
Graphics. LEVOY, M., ET AL. 2000. The Digital Michelangelo Project: 3D Scanning of Large Statues. In 
Proceedings of SIGGRAPH 2000, 15 22. TCHOU, C. 2002. Image-Based Models: Geometry and Reflectace Acquisition 
Systems. MS thesis, University of California at Berkeley. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965515</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Improved speed and accuracy for archaeological site scanning]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965515</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965515</url>
		<abstract>
			<par><![CDATA[During viewpoint framing for long-range laser scanning, nearly all current scan control software assumes a uniform bounding box selection (parametric UxV) within an XYZ world. Here we suggest a new system of scanner control that does not make this assumption, but instead uses active parsing of incoming points to enable automated, "subdivided" scan viewpoint framing.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653904</person_id>
				<author_profile_id><![CDATA[81100506574]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INSIGHT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24018369</person_id>
				<author_profile_id><![CDATA[81100471528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Min]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Geometry Systems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653972</person_id>
				<author_profile_id><![CDATA[81100331634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Youda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[He]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Geometry Systems, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DURAN, Z., TOZ, G. 2002. Documentation and analysis of cultural heritage by means of photogrametric methods and GIS, Istanbul Technical University, Division of Photogrammetry]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Improved Speed and Accuracy for Archaeological Site Scanning Kevin Cain* Min Yu Youda He INSIGHT Geometry 
Systems, Inc. Geometry Systems, Inc. Abstract During viewpoint framing for long-range laser scanning, 
nearly all current scan control software assumes a uniform bounding box selection (parametric UxV) within 
an XYZ world. Here we suggest a new system of scanner control that does not make this assumption, but 
instead uses active parsing of incoming points to enable automated, subdivided scan viewpoint framing. 
 1 Unique Challenges in Large-Scale Scanning Because access to archaeological sites is often limited, 
it can be difficult to scan large sites at relatively high resolution. In the case of our laser scanning 
at the Ramesseum in Egypt (below), our goal was to establish detailed architectural plan, section, and 
elevation views of a site many hundreds of meters in dimension. Figure 1. The Ramesseum, Thebes, Egypt. 
Current survey techniques can yield apprx. 1:1000 digital maps [Duran, Toz]. However, to resolve the 
complex plan at the Ramesseum, we estimate a resolution of 10mm would be ideal. Even through our team 
was able to acquire points rapidly (~1000 points/sec), it was still not practical to scan the entire 
site at this resolution, since this would require approximately 250 days. 2 Increasing Detail via Progressive 
Refinement We propose a refining technique to 1) detect edges during the scanning process, and 2) mark 
these border areas for rescanning at a higher resolution specified by the user. This is particularly 
appropriate for archaeological sites such as the Ramesseum, where the large-scale structure of the site 
is composed of rectilinear elements (as shown above). Ours is a poly-resolution approach that aims to 
concentrate detail where it is critical for establishing relevant architectural details (i.e., corners) 
while allowing less critical surfaces (walls) to be scanned more rapidly. *e-mail: kevin@insightdigital.org 
e-mail: min@geometrysystems.com e-mail: youda@geometrysystems.com In the scene below, an initial synthetic 
scan pass is shown in red. Note that the actual spacing between scanned points on the target surface 
(PIV, or Point Interval Value) increases with the angle (.) and distance (d) of the laser head relative 
to the scanscan scanned surface. Since the movement of a scan head is typically monotonic, as shown in 
Fig. 2, the target PIV for the higher resolution scan must take into account .That is, to maintain scan. 
the target PIV, the rotation increment of the scanner must decrement as . increments. Since scan point 
confidence falls scan with increased ., a natural limit to . can be set and the scanscan program instructed 
to flag low-confidence points for user review. Figure 2. Schematic plan view of a sample scene. During 
the initial (red) scan, our proposed method evaluates the change in dfrom point to point. When the .d 
> Depth scan scan Threshold (a user specified value), a corner has been detected and a detailed scan 
is launched (shown above in green) to determine a more accurate position for this edge, which in turn 
allows for greater precision in view registration via ICP. The Depth Threshold can be set by the user 
so that minor changes in distance (for instance, the uneven bricks in the wall shown at left) do not 
trigger an automated scan refinement pass. The following is pseudo code for a single scan line: While 
UVcurrent > UVend_of_line > Depth Threshold If .dscan . Reset laser to UVprev . Frame UVprev -> UVcurrent 
. Scan framed region using PIVhighres > UserLimit, flag these points . If .scan as low confidence for 
the user . end (If) Else increment UVcurrent to next scan point  end (If)  end (While)  End Of Algorithm. 
 References DURAN, Z., TOZ, G. 2002. Documentation and analysis of cultural heritage by means of photogrametric 
methods and GIS, Istanbul Technical University, Division of Photogrammetry Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965516</section_id>
		<sort_key>25</sort_key>
		<section_seq_no>25</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Simulation in production]]></section_title>
		<section_page_from>25</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39028923</person_id>
				<author_profile_id><![CDATA[81322489561]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Juan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Buhler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[PDI/DreamWorks]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965517</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Rule-based smoke- and explosion dynamics in a full CG environment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965517</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965517</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35032017</person_id>
				<author_profile_id><![CDATA[81100604837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trojansky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CA Scanline Production GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653871</person_id>
				<author_profile_id><![CDATA[81100253963]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CA Scanline Production GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rule-Based Smoke-And Explosion Dynamics in a Full CG Environment Stephan Trojansky*, Florian Hu CA 
Scanline Production GmbH 1 Introduction This animation sketch presents the development of the Rule-Based 
Simulation technique that was used to calculate the smoke-, fire-, and explosion-dynamics for Check-in 
To Disaster (shown in the Electronic Theatre 2002). The sketch also elaborates the creation of the landscape 
based on rules and the dynamic and shading interactions between airplane, landscape, the broken chunks 
and the mentioned volumetric elements. Future prospects of new possibilities by combining these techniques 
with specially developed realtime Navier-Stokes simulation are specified. When Scanline and Cebas Computer 
first introduced this technique at Siggraph 2001, we already realised, that this could be a unique way 
of creating revolutionary visual effects picturing natural phenomena. But we never dreamed of being able 
to handle the complexity of a full CG emergency landing completely by rules thanks to this technique 
we managed 55 shots in a timespan of 10 weeks with about 6 animators to do the work. 2 The Idea of Rule-Based 
Simulation In contrast to classical keyframe animation, rules are defined for the behaviour of single 
simulation elements. This behaviour can be applied to millions of others with maximum control. By interlinking 
multiple rules you can create complex dependencies between all types of elements. 3 Rule-Based Smoke- 
and Fire Dynamics In advance we did a research for available Smoke- and Fire Systems, but we came to 
the conclusion that no system was fast enough for our purpose; and even more important, no system was 
suitable for a VFX production where the result has to be a mixture of the physical reality and the directors 
specific imagination. With our Rule-Based approach we achieved a realistic behaviour and look, but maintained 
full control over the simulation. We wanted to rule -not to be ruled by the simulation software . We 
were able to define the amount of kerosene, the direction, speed, length and size of the explosions with 
only a few parameters. The system was, in combination with our advanced volumetracing engine, very efficient. 
Our workflow was never blocked by `overnight´ simulation time.  4 Rule-Based Volumetracing Our simulation 
was supposed to work for all imaginable camera movements and it needed full selfshadowing, shadowcasting 
and raytracing abilities. Therefore we decided for volumetracing. To get an extremely close connection 
between the simulation and the rendering, we created rules that can directly access every parameter of 
the volume engine at every single point in space. That way quality and speed of the volumetracer can 
be increased multiple times, especially in complex situations.  Visualisation of the simulation data. 
The ground is generated view­dependant. 5 Navier-Stokes based rule-simulation At the moment Scanline 
is developing its own Navier-Stokes based Realtime 3D-Fire-Simulation. New possibilities arising from 
the combination of rule-systems with Navier-Stokes Simulations are exemplified with current animations. 
 Images from our 3D Realtime Navier-Stokes solver for fire. *e-mail: troja@scanline.de. Copyright held 
by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965518</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Rule-based crowds]]></title>
		<subtitle><![CDATA[generation, animation, cloth and rendering of 15.000 unique human characters]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965518</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965518</url>
		<abstract>
			<par><![CDATA[Scanline is currently developing the proprietary crowd-simulation tool "Sheep". It is designed to generate mass scenes with 15.000 different characters, animate them by an unlimited amount of rules, supply presimulated cloth and render a whole shot in a single layer taking less than 45 minutes. This will disclose completely new dimensions for the realization of Visual Effects with mass sequences.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35032017</person_id>
				<author_profile_id><![CDATA[81100604837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trojansky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CA Scanline Production GmbH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309704800</person_id>
				<author_profile_id><![CDATA[81539007056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P362247</person_id>
				<author_profile_id><![CDATA[81100547169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kolja]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[K&#228;hler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14202675</person_id>
				<author_profile_id><![CDATA[81314491744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christoph]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sprenger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653953</person_id>
				<author_profile_id><![CDATA[81100302237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stapel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rule-Based Crowds: Generation, Animation, Cloth and Rendering of 15.000 Unique Human Characters Stephan 
Trojansky*, CA Scanline Production GmbH Abstract Scanline is currently developing the proprietary crowd-simulation 
tool Sheep . It is designed to generate mass scenes with 15.000 different characters, animate them by 
an unlimited amount of rules, supply presimulated cloth and render a whole shot in a single layer taking 
less than 45 minutes. This will disclose completely new dimensions for the realization of Visual Effects 
with mass sequences. 1 Introduction: Sheep is a further development of the proprietary Rule-Based technology. 
Exemplified by the movie Hero of the Gladiators , which is going to be finished in June this year, this 
sketch elucidates work in progress using the following techniques: -Data base supported generation of 
an unlimited amount of different human characters, defined by rules -Rule-Based control of animation 
in realtime to achieve half autonomous crowd-behaviour -Use of presimulated cloth and its automated adaptation 
to any kind of human characters in realtime (to avoid shotbased simulation of cloth) -Raytracing of the 
mass scenes from infinite quantities of unique polygons by application of dynamic raytracing acceleration 
trees. Generation of the polygons by Sheep during the rendering process. Rendering of 15.000 characters 
in 2k resolution at an average time of 45 minutes per frame User interface of Sheep 2 Generation: All 
models and textures needed for the generation of a character are stored in data bases. The distribution 
of different characters within the arena can be either achieved by randomization, or determined freely 
in a very simple way: Attributes like gender, size, age, weight, headgear or haircut are associated with 
colours and can be arranged simply by painting in the site plan of the arena. To supervise the process, 
the characters can be assembled and displayed in the viewport at any time. Collaborators: Florian Hu, 
Kolja Kähler, Christoph Sprenger, Stephan Stapel 3 Animation: Directing the characters is achieved by 
the following techniques. Similar as in the generation process, all kinds of movements can be initiated 
through different colours of textures. Dark areas for instance could stand for restrained movements, 
bright ones for an ecstatically cheering crowd. Resulting reactions of characters to the moving of others 
can be triggered automatically (e.g. people giving way to a person leaving his seat). A certain kind 
of behaviour can be moved through the audience by shifting a defined object over the areas that should 
be affected. The behaviour of the crowd can be displayed in realtime at any time in terms of skeleton 
wireframes. Viewport display and the rendered layout 4 Cloth: As each character has its own physique 
and idiosyncratic body movements, a shotbased simulation of cloth for 15.000 characters would result 
in tremendous time for simulation. For that reason Sheep takes presimulated cloth from data bases and 
adapts them to the various physiques and movements by special transformations. Thereby the characters 
can be displayed and rendered with cloth at any time. 5 Raytracing: During a rendering of one frame 
an average of 9 gigabyte respectively 150 million unique polygons accumulates. To cope with this amount 
of data, a dynamic raytracing acceleration tree has been developed for finalRender. This does not request 
the geometric data of the character until rays hit his boundingbox. As soon as this is the case, Sheep 
compiles the character from the data base and provides it to the raytracer. By intelligent caching characters 
that are no longer required are removed from the acceleration tree, so the volume of the needed memory 
remains constant. As this procedure is applicable to all raytracing invocations, shadows and real Global-Illumination 
can be rendered as well. In addition, the whole shot can be rendered in one single layer. *e-mail: troja@scanline.de 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965519</section_id>
		<sort_key>26</sort_key>
		<section_seq_no>26</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[GPU/image-based modeling & rendering]]></section_title>
		<section_page_from>26</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14194811</person_id>
				<author_profile_id><![CDATA[81100561258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yiorgos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chrysanthou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cyprus]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965520</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Generating subdivision curves with L-systems on a GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965520</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965520</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP39030455</person_id>
				<author_profile_id><![CDATA[81100163820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Radom&#237;r]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#283;ch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SGI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14162862</person_id>
				<author_profile_id><![CDATA[81100465812]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Przemyslaw]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Prusinkiewicz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Calgary]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>114525</ref_obj_id>
				<ref_obj_pid>114524</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ORTIZ, L., PINTER, R., AND PINTER, S. 1991. An array language for data parallelism: Definition, compilation, and applications. The Journal of Supercomputing, 5, 7--29.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[PRUSINKIEWICZ, P., SAMAVATI, F., SMITH, C., AND KARWOWSKI, R. 2003. L-system description of subdivision curves. International Journal of Shape Modeling. To appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generating subdivision curves with L-systems on a GPU Radom´ir M.ech* SGI The introduction of .oating-point 
pixel shaders has initiated a trend of moving algorithms from CPUs to graphics cards. The .rst algorithms 
were in the rendering domain, but recently we have witnessed increased interest in modeling algorithms 
as well. In this sketch we focus on generating subdivision curves, speci.ed in terms of L-systems. Since 
L-systems can express many modeling algorithms in a compact way and are parallel in nature, they are 
an attractive paradigm for programming a GPU. Method Subdivision curves can be described using context-sensitive 
para­metric L-systems [Prusinkiewicz et al. 2003]. Control points of the curve are stored as symbols 
in the initial string, with parameters specifying point locations1 . L-system productions are used to 
re­place each point with new points according to a given subdivision scheme. For example, Chaikin subdivision 
of a closed curve is cap­tured by a single production, 13 31 P (vl ) <P (v) >P (vr ) -P ( vl + v)P ( 
v + vr ), 44 44 which replaces one point, the strict predecessor, with two new points, forming the successor. 
The location of each new point is an af.ne combination of the locations v, vl and vr of the strict pre­decessor 
point and its context (neighbors). An L-system in which each symbol is replaced by k symbols is easy 
to implement on graphics hardware that supports .oating­point pixel/fragment shaders2 (Figure 1). We 
store the initial string in one line of a texture. If one line is not enough, we modify the neighbor 
selection process in order to store the string in a 2D tex­ture. The symbol of each point is in the alpha 
channel, and the co­ordinates are in the RGB channels. Given a string of length n, we draw a line of 
length kn into a P-buffer, off-screen memory located on the graphics card. A fragment program reads texel 
values at po­sitions (i/k - 1)%n, i/k, and (i/k + 1)%n, and sets the value of pixel i in the P-buffer 
as de.ned for the i%k-th point of the produc­tion successor. In the next iteration of the algorithm, 
we treat this P-buffer as the input texture and use another P-buffer as the output. Finally, we read 
the .nal string using glReadPixels, and render the vertices. In the near future the drivers will support 
rendering into a vertex array, which will make it possible to avoid the readback. To generate open subdivision 
curves, we need an L-system that handles the endpoints [Prusinkiewicz et al. 2003]. Each row of this 
texture stores information on the context of all productions with the same strict predecessor. Fragment 
program 1 .nds the match­ing production for each point in the predecessor string, and outputs the successor 
length l and the production index s. This raises two problems: to .nd a production for each symbol, and 
to position the successor in the output string. We .nd the production using a pre­decessor texture (Figure 
2). This takes up to M steps, where M is the maximum number of productions with the same strict prede­cessor. 
To determine the position of each successor in the output string, fragment program 2 (not shown in Figure 
2) simulates the * rmech@sgi.com pwp@cpsc.ucalgary.ca 1We make here a distinction between the location 
of a point (three coor­dinates) and its position in the string (an index value). 2Our implementation 
and description are based on the ATI Radeon 9700 card (http://www.ati.com/developer). Przemyslaw Prusinkiewicz 
University of Calgary Figure 1: An L-system on GPU: closed subdivision curve. Figure 2: An L-system 
on GPU: open subdivision curve. scan-add operation [Ortiz et al. 1991]. It sums the lengths of all successors 
to the left of a given one in llog2 (n)J steps. These sums are read with glReadPixels and used to create 
a set of line segments, each starting at the pixel given by a sum. The symbols and af.ne combination 
coef.cients for the l successor points of the production with index s are stored in columns s to s + 
l - 1 of the successor texture. This texture is accessed by fragment program 3 (Figure 2), which uses 
the stored values to compute the af.ne combination of the predecessor point and its neighbors. The .nal 
string is read with glReadPixels, and the vertices are rendered as in the closed curve case. Conclusions 
We created a set of fragment programs on ATI s Radeon 9700 that implement L-systems capable of generating 
subdivision curves. Compared to a CPU implementation, the GPU implementation is faster when many curves 
are evaluated simultaneously. An intriguing problem for further research is an extension of this work 
to subdivision surfaces. References ORTIZ, L., PINTER, R., AND PINTER, S. 1991. An array language for 
data parallelism: De.nition, compilation, and applications. The Journal of Supercomputing, 5, 7 29. PRUSINKIEWICZ, 
P., SAMAVATI, F., SMITH, C., AND KARWOWSKI, R. 2003. L-system description of subdivision curves. International 
Journal of Shape Modeling. To appear. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965522</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Automated reconstruction of building facades for virtual walk-thrus]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965522</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965522</url>
		<abstract>
			<par><![CDATA[In this paper, we present a fast, automated approach to generating a highly detailed, textured 3D building facade model. This model is acquired at the ground level by driving a vehicle equipped with laser scanners and a digital camera under normal traffic conditions on public roads, and then processed offline. We evaluate our approach on a large data set acquired in downtown Berkeley.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15023813</person_id>
				<author_profile_id><![CDATA[81100134502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frueh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15029040</person_id>
				<author_profile_id><![CDATA[81100321235]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Avideh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zakhor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[FRUEH, C., AND ZAKHOR, A. 2001(a). Fast 3D model generation in urban environments. In Multisensor Fusion and Integration for Intelligent Systems, Baden-Baden, Germany, p. 165--170]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FRUEH, C., AND ZAKHOR, A. 2001(b). 3D model generation of cities using aerial photographs and ground level laser scans. In Computer Vision and Pattern Recognition, Hawaii, USA, p. II-31-8, vol. 2.2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[FRUEH, C., AND ZAKHOR, A. 2002. Data Processing Algorithms for Generating Textured 3D Building Facade Meshes From Laser Scans and Camera Images. In 3D Processing, Visualization and Transmission 2002, Padua, Italy, p. 834--847]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Automated Reconstruction of Building Facades for Virtual Walk-thrus   Christian Frueh and Avideh Zakhor* 
Video and Image Processing Lab University of California, Berkeley   Abstract In this paper, we present 
a fast, automated approach to generating a highly detailed, textured 3D building facade model. This model 
is acquired at the ground level by driving a vehicle equipped with laser scanners and a digital camera 
under normal traffic conditions on public roads, and then processed offline. We evaluate our approach 
on a large data set acquired in downtown Berkeley. 1 Introduction Three-dimensional models of urban 
environments are useful in a variety of applications such as gaming, urban planning, and virtual heritage 
conservation. Models reconstructed from airborne data such as aerial images or airborne laser scans generally 
lack information about the facades or details at the ground level; in addition, they are typically created 
via a labor-intensive process. Previous attempts to create ground-based models use 3D scanners, requiring 
a slow stop-and-go mode for the data acquisition, and hence are limited to a few buildings. Recently, 
we proposed a method that is capable of rapidly acquiring 3D geometry and texture data of an entire city 
at the ground level [Frueh and Zakhor, 2001a]. 2 Data Acquisition and Model Generation Using a combination 
of two fast 2D laser scanners and digital cameras mounted on a truck, as shown in Figure 1, we acquire 
data continuously while driving at normal speeds on public roads, rather than in a slow stop-and-go fashion. 
One 2D scanner is mounted vertically and scans the buildings and street scenery as the truck drives by. 
The other one is mounted horizontally and is used to determine the vehicle s pose via scan-to-scan matching 
[Frueh and Zakhor, 2001a], and to globally register the facades with respect to an airborne edge map 
by means of Monte-Carlo Localization [Frueh and Zakhor, 2001b].   Figure 1. Acquisition vehicle and 
setup. After a series of automated processing steps including removal of foreground objects and filling 
holes [Frueh and Zakhor, 2002], we reconstruct the 3D geometry of the facades from the vertical laser 
scans and texture map them using the camera images. Then, different levels of detail are generated and 
organized in a scene graph in order to enable interactive rendering. _____________________________________________________ 
*e-mail: frueh@eecs.berkeley.edu, avz@eecs.berkeley.edu 3 Results We have applied the proposed algorithms 
on a large data set acquired during two measurement drives in Berkeley, with a total length of 24.3 kilometers 
traversed in 78 minutes. A total of 332575 vertical and horizontal scans, consisting of 85 million scan 
points, along with 19200 images, are captured during those two drives. Using our automated processing 
steps, we generate facade models for 12 street blocks in downtown Berkeley as shown Figures 2 and 3. 
The total acquisition time for this model is only 25 minutes; this is the time that it took to drive 
the total of 8 kilometers around the blocks under city traffic conditions.  Figure 2. Facade models 
for downtown Berkeley.   Figure 3. Two close-up views of the Berkeley Public Library. Our proposed 
approach to city modeling is not only automated, but also fast from a computational viewpoint: For the 
twelve downtown blocks shown, the total time for the fully automated processing from the raw data to 
the final model is 4 hours and 45 minutes on a 2 GHz Pentium-4 PC. Since the complexity of all developed 
algorithms is linear in path length, our method is scalable and applicable to large environments.  References 
 FRUEH, C., AND ZAKHOR, A. 2001(a). Fast 3D model generation in urban environments. In Multisensor Fusion 
and Integration for Intelligent Systems, Baden-Baden, Germany, p. 165-170 FRUEH, C., AND ZAKHOR, A. 2001(b). 
3D model generation of cities using aerial photographs and ground level laser scans. In Computer Vision 
and Pattern Recognition, Hawaii, USA, p. II-31-8, vol.2. 2 FRUEH, C., AND ZAKHOR, A. 2002. Data Processing 
Algorithms for Generating Textured 3D Building Facade Meshes From Laser Scans and Camera Images. In 3D 
Processing, Visualization and Transmission 2002, Padua, Italy, p. 834 847 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965523</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Interactive rendering with view-dependent geometry and texture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965523</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965523</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653886</person_id>
				<author_profile_id><![CDATA[81309487513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jan-Friso]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Evers-Senne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Christian-Albrechts-University of Kiel, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39036726</person_id>
				<author_profile_id><![CDATA[81452616003]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Reinhard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Christian-Albrechts-University of Kiel, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383309</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BUEHLER, C., BOSSE, M., MCMILLAN, L., GORTLER, S. J., AND COHEN, M. F. 2001. Unstructured lumigraph rendering. In SIGGRAPH 2001, Computer Graphics Proceedings, ACM Press/ACM SIGGRAPH, E. Fiume, Ed., 425--432.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[EVERS-SENNE, J.-F., AND KOCH, R. 2003. Image based interactive rendering with view dependent geometry. In Proc. Eu-rographics 2003, Computer Graphics Forum, Eurographics Association, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Rendering with View-dependent Geometry and Texture Jan-Friso Evers-Senne, Reinhard Koch* 
Institute of Computer Science and Applied Mathematics, Christian-Albrechts-University of Kiel, Germany 
 Introduction In this work we present a novel approach for image based rendering (IBR) of complex real 
scenes that have been recorded with freely moving hand-operated cameras. The images are automatically 
cal­ibrated and 3D scene depth maps are computed for each real view. To render a new virtual view, the 
depth maps of the nearest real views are fused in a scalable fashion to obtain a locally consistent 3D 
model on the .y. This geometrical representation is based on triangles and can be textured with the images 
corresponding to the depth maps using hardware-accelerated techniques. When using IBR techniques for 
complex real outdoor scenes, new views are generated from several hundred or even thousands of images. 
For interpolating between images the geometrical struc­ture of the scene has to be taken into account 
to avoid artefacts. Standard approaches like Light.eld or Lumigraph suffer from the missing geometry, 
others rely on a given model [1]. When using image sequences of uncalibrated handheld cameras, it is 
nearly im­possible to build a globally consistent 3D model automatically. The proposed system relies 
on locally consistent 3D models which are valid for a particular viewpoint. So for each novel view the 
local model will be rebuilt to assure consistency. This process is per­formed at interactive frame rates. 
Scene Modeling After recording the image sequences of the desired scene with a handheld camera, the 
.rst step is to estimate the camera parame­ters for each image. To avoid any markers or beacons in the 
scene we use advanced structure-from-motion algorithms to obtain the in­trinsic and extrinsic camera 
parameters combined in a projection matrix P for each image. After calibration, a multiview stereo ap­proach 
is used to generate a dense depth map for each image. This triple of image, projection matrix, and depth 
map is called a real view or real camera. For IBR, depth and color information from several real cameras 
has to be processed in real-time. It is not possible to use every pixel from the depth map, therefore 
the depth information of each real camera is prepared in an of.ine step beforehand. The depth map is 
subsampled in a regular grid of typically 200x160 samples per view. For each sample a median .lter removes 
measurement outliers and smoothes the depth values. All sample points are then stored in a LOD pyramid. 
The corresponding RGB image is stored as precompressed texture. This preprocessing results in a space­and 
time-optimized representation of the scene model and several hundred to thousends of views can be stored 
in memory or loaded on demand in realtime. Interactive Rendering To render a novel view with IBR one 
has to decide which real cam­eras are used for interpolation. For each camera, several criteria like 
.eld-of-view, distance to the virtual camera and viewing direction are evaluated and combined into a 
cost function w.r.t. the rendering quality. Then the cameras are ranked to maximise rendering qual­ity. 
Depending on the complexity of the scene, the position, and .eld-of-view of the virtual camera, the best 
n cameras are selected. The ranked cameras and their associated depth samples are now used to interpolate 
novel views. Since the novel view may cover a .eld of view that is larger than any real camera view, 
we have to fuse views from different cameras into one locally consistent im­age. For interpolation of 
the virtual view, a triangular surface mesh *e-mail: evers@mip.informatik.uni-kiel.de, rk@informatik.uni-kiel.de 
 is placed in the image plane of the virtual camera. The spacing of this mesh can be scaled to the complexity 
of the scene. By project­ing the samples from all best n camera onto the virtual camera and comparing 
the projected points to the mesh vertices, the 3D infor­mation from the visible samples is assigned to 
the coordinates of the warping surface. For each triangle we store the best real views. Finally, hardware-accelerated 
multitexturing from the best views is used to for rendering. For details see [2].  Figure 1: Parking 
lot scene. Top: One of the 214 original images and the corresponding depth map. Bottom: Novel viewpoints 
ren­dered from the parking lot scene. Results The system has been tested with a large variety of complex 
indoor and outdoor scenes. Figure 1 shows some rendering results. The scalability of our rendering system 
can be used to trade between quality and performance. When choosing only a few cameras for interpolation, 
selecting a moderate number of samples and using a coarse grid, the quality is similar to standard light.eld 
rendering, with frame rates around 60-100 fps. For the highest quality ren­dering of complex scenes the 
warping mesh will be .ner and much more samples per camera will be used to enhance the resolution warping 
surface. This leads to non-interactive frame rates which can be used for off-line rendering. Using on-demand 
loading of compressed textures and LOD samples, large scenes consisting of thousends of images can be 
handled on recent PC hardware. Conclusion We have presented a novel IBR system that is capable to render 
new views of very complex real 3D scenes that have been acquired by simply moving a handheld uncalibrated 
camera through the scene. The work extends previous work on the unstructured lumigraph. Acknowledgements: 
This work has been supported by Euro­pean Project IST-2000-28436 ORIGAMI. References [1]BUEHLER, C., 
BOSSE, M., MCMILLAN, L., GORTLER, S. J., AND COHEN, M. F. 2001. Unstructured lumigraph rendering. In 
SIGGRAPH 2001, Computer Graphics Proceedings, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., 425 432. [2]EVERS-SENNE, 
J.-F., AND KOCH, R. 2003. Image based in­teractive rendering with view dependent geometry. In Proc. Eu­rographics 
2003, Computer Graphics Forum, Eurographics As­sociation, to appear. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965521</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Particle system collision detection using graphics hardware]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965521</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965521</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653855</person_id>
				<author_profile_id><![CDATA[81100305521]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knott]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42053155</person_id>
				<author_profile_id><![CDATA[81322508645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kees]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van den Doel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P67322</person_id>
				<author_profile_id><![CDATA[81100642559]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Dinesh]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Pai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383274</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LINDHOLM, E., KILGARD, M. J., AND MORETON, H. 2001. A user-programmable vertex engine. In Proc. of SIGGRAPH 2001, 149--158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801167</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[REEVES, W. T. 1983. Particle systems - a technique for modeling a class of fuzzy objects. In Proc. of SIGGRAPH 83, 359--376.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383322</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[VAN DEN DOEL, K., KRY, P. G., AND PAI, D. K. 2001. FoleyAutomatic: Physically-based sound effects for interactive simulation and animation. In Proc. of SIGGRAPH 2001, 537--544.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Particle System Collision Detection using Graphics Hardware Dave Knott Kees van den Doel Dinesh K. 
Pai University of British Columbia University of British Columbia University of British Columbia and 
Radical Entertainment and Rutgers, The State University of New Jersey 1 Introduction We demonstrate 
a technique for performing collision detection be­tween dynamically simulated particle primitives and 
groups of im­plicitly de.ned objects. We describe how this collision detection can be performed using 
a programmable vertex engine. We use the particle collision detection in the context of visual simulation, 
which requires no CPU intervention. We also provide a method for reporting collision information back 
to the CPU for further processing, using a construct called the impact map. 2 Particle Simulation We 
simulate particles[Reeves 1983] whose behaviour is described by closed-form equations of motion. A particle 
s de.ning parame­ters are speci.ed once and never change. This allows us to create simulations in which 
the properties of particles are computed on graphics hardware using vertex programs/shaders[Lindholm 
et al. 2001]. For purely graphical applications, the computer s CPU need never participate, except to 
pass data to the graphics accelerator at each frame of the animation. The motion of particles is completely 
determined by initial con­ditions, with the exception of impacts with (and possible response to) interfering 
bodies. Instantaneous particle position is given by a parametric equation p = g(t), where t is global 
simulation time. Particles may impact with colliders, surfaces de.ned implicitly by some equation, f 
(p)= 0, where p is a point on the surface. The intersection of a particle s motion with the surface is 
then described by h(t)= f (g(t)) = 0, which implicitly gives the colli­sion time, t. Each collider may 
also have associated constraints, c(p) > 0. These are useful in detecting collisions with bounded planar 
sections, or objects with holes, for example. We can currently test for collisions between planar sections 
and particles obeying second-order dynamics. We can also test for col­lisions between .rst-order particles 
and quadric surfaces. Figure 1: Examples of collision  3 Collision Feedback The impact map is a two-dimensional 
representation of the loca­tions on colliders at which particles have impacted. At each sim­ulation step, 
a GPU vertex program computes whether or not each particle has impacted a collider at some time between 
the previ­ous and current frames of the animation. If a collision is detected, then a two-dimensional 
representation of the exact impact location is calculated. A point primitive is drawn into the impact 
map at that contact e-mail: {knott|kvdoel|pai}@cs.ubc.ca location. The colour channels of the impact 
map can be used to store information about collisions, such as impact energy.  Figure 2: Collision with 
response and the associated impact map Applications such as audio simulation require collision informa­tion 
at rates much higher than those achievable with graphics hard­ware. The closed-form nature of the simulation 
allows us to achieve super-resolution results. Exact collision time is reported in the im­pact map, which 
is rendered slightly ahead of the visual simulation. For static or precomputable collision information, 
we can save CPU cycles by storing the data in texture memory. Collision data is reported in the impact 
map via a dependent texture look-up. 4 Results We have implemented an audio-visual simulation of hail 
falling on an outdoor scene. Hail is simulated as particles with motion de.ned by second-order dynamics. 
We perform collision detection between each hail particle and a variety of objects, as shown in Figure 
3. Figure 3: Hailstone particles colliding with outdoor objects We use collision feedback for two purposes. 
For visual simu­lation, a decal texture is rendered wherever a hailstone impacts with a collider object 
in the scene. We also drive an audio simula­tion that uses modal synthesis to generate sound[van den 
Doel et al. 2001] when hail strikes colliders. Future directions include using the dependent texture 
look-up to report sound synthesis parameters via the impact map. References LINDHOLM, E., KILGARD, M. 
J., AND MORETON, H. 2001. A user­programmable vertex engine. In Proc. of SIGGRAPH 2001, 149 158. REEVES, 
W. T. 1983. Particle systems -a technique for modeling a class of fuzzy objects. In Proc. of SIGGRAPH 
83, 359 376. VAN DEN DOEL, K., KRY, P. G., AND PAI, D. K. 2001. FoleyAutomatic: Physically-based sound 
effects for interactive simulation and animation. In Proc. of SIGGRAPH 2001, 537 544. Copyright held 
by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965524</section_id>
		<sort_key>27</sort_key>
		<section_seq_no>27</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Devices]]></section_title>
		<section_page_from>27</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39045197</person_id>
				<author_profile_id><![CDATA[81100487435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carolina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz-Neira]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Iowa State University]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965525</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3-D tele-direction interface using video projector]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965525</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965525</url>
		<abstract>
			<par><![CDATA[We developed a direction system for assisting the work in a real world from a distant site. At first, the 3-D shape of the object is measured and sent to the distant PC. A supervisor at the distant site can observe the CG of the object and draw annotation figures on it. The figures of the direction message are projected onto the object using projectors. The worker is free from any wearing equipment, ex. HMD, and multi projectors avoid the problem of occlusion by the worker body.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP37027540</person_id>
				<author_profile_id><![CDATA[81100434645]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shinsaku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hiura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653903</person_id>
				<author_profile_id><![CDATA[81100156605]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tojo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309759600</person_id>
				<author_profile_id><![CDATA[81545834456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Seiji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inokuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>732300</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[RASKER, R., WELCH, AND G., LOW, K-L. 2001. Shader Lamps: Animating Real Objects with Image Based Illumination, In Proceedings of Eurographics Workshop on Rendering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HEDBERG, S. R. 1996. Virtual reality at Boeing: pushing the envelope, VIRTUAL REALITY special report, 51--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[CAUDELL, T. P. AND MIZELL, D. W. 1992. Augmented reality: an application of heads-up display technology to manual manufacturing processes, IEEE Computer Society Press, Vol. 2, 659--669.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>142980</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[KUROKAWA, H. 1992. Spatial Workspace Collaboration: A SharedView Video Support System for Remote Collaboration Capability, Proc. of CHI'92, 533--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3-D Tele-direction Interface using Video Projector  Shinsaku Hiura    Kenji Tojo    Seiji 
Inokuchi Graduate School of Engineering Science, Osaka University  Abstract We developed a direction 
system for assisting the work in a real world from a distant site. At first, the 3-D shape of the object 
is measured and sent to the distant PC. A supervisor at the distant site can observe the CG of the object 
and draw annotation figures on it. The figures of the direction message are projected onto the object 
using projectors. The worker is free from any wearing equipment, ex. HMD, and multi projectors avoid 
the problem of occlusion by the worker body. 1 Introduction Techniques of the augmented reality are 
not restricted to the device between eyes and the object, because controlling the incident light which 
illuminates the object also changes its appearance. The advantages of this method are as follows: (1) 
The overlaid image does not move along with the motion of the viewpoint, (2) the workers do not need 
to wear any equipment, (3) multiple users can see. Since we valued these advantages, we developed a tele-direction 
system using projectors. We used multiple projectors to avoid the problem of occlusion of the projected 
light. Contrary to using HMD, the projection based MR can not show an image floating in the air, but 
in practice it makes no difference because of the sophisticated visual function of human. 2 Overview 
of tele-direction system The construction of the projector based tele-direction system is shown in figure 
1. Several sets of a camera, projector and PC are placed at the work space. Each set of a camera, projector 
and PC forms a rangefinder based on active triangular measurement method. At first, the 3-D shape and 
texture of the object at the work space is measured using the cameras and projectors. The data is sent 
to the PC at the distant site and shown as a CG on a display. The director can observe the object from 
any viewpoint by rotating the displayed CG model using GUI. To input some directions, the director must 
simply draw figures on the CG at any time as shown in figure 2(a). The drawn figures are projected onto 
CG model of the object work space distant site projectors cameras the object instantly and precisely 
as shown in figure 2(b). Since we used four projectors and cameras, the whole shape of the object can 
be measured and the director can draw figures on any surface. One of the most common intent to use a 
tele-direction system is to specify the position and the alignment of the object. Contrary to HMD, our 
method is said to have a disadvantage of impossibility of showing figures floating in the air. However, 
in experiment we confirmed it is not a matter because we can easily understand the skewed figures projected 
on the object as shown in figure 3. At first, the director traces the edges on a CG model. The traced 
figures are transformed to the sequence of 3-D points immediately, and it can be moved with certain displacement 
using GUI. The projected figures are skewed if the object is not planar, but the worker can easily move 
the object as intended.  References RASKER, R., WELCH, AND G., LOW, K-L. 2001. Shader Lamps: Animating 
Real Objects with Image Based Illumination, In Proceedings of Eurographics Workshop on Rendering. HEDBERG, 
S. R. 1996. Virtual reality at Boeing: pushing the envelope, VIRTUAL REALITY special report, 51-55. CAUDELL, 
T.P. AND MIZELL, D.W. 1992. Augmented reality: an application of heads-up display technology to manual 
manufacturing processes, IEEE Computer Society Press, Vol.2, 659-669. KUROKAWA, H. 1992. Spatial Workspace 
Collaboration: A SharedView Video Support System for Remote Collaboration Capability, Proc. of CHI 92, 
533-540. Figure 2. Drawn figures(a) and projected result(b). worker director direction messages  Figure 
1. System Overview. e-mail:shinsaku@sys.es.osaka-u.ac.jp http://www-sens.sys.es.osaka-u.ac.jp/users/shinsaku/index-e.html 
Figure 3. Specifying displacement of the object. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965526</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[The reality helmet]]></title>
		<subtitle><![CDATA[a wearable interactive experience]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965526</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965526</url>
		<abstract>
			<par><![CDATA[The Reality Helmet is an <i>interactive experience</i>, in which the user's vision and hearing is shielded off from the world. Video and sound is nevertheless recorded by the Reality Helmet, but through computer processing it presents sound to the user as vision, and likewise, vision is turned into a soundscape. The result is a form of <i>artificial synesthesia</i>. Other than as an appreciated art installation, which seems to make people calm and reflective, the Reality Helmet is used to explore relationships between the wearer's sense of <i>presence</i> and the kind of <i>realism</i> provided by the interactive environment.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14019643</person_id>
				<author_profile_id><![CDATA[81100018490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fallman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tools for Creativity Studio, Ume&#229;]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653899</person_id>
				<author_profile_id><![CDATA[81100062954]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kalle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jalkanen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tools for Creativity Studio, Ume&#229;]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653878</person_id>
				<author_profile_id><![CDATA[81100190235]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[L&#246;rstad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sonic Studio, Pite&#229;, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653891</person_id>
				<author_profile_id><![CDATA[81546314256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Waterworth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tools for Creativity Studio, Ume&#229;]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653892</person_id>
				<author_profile_id><![CDATA[81100418710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Westling]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tools for Creativity Studio, Ume&#229;]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>240808</ref_obj_id>
				<ref_obj_pid>240806</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DAVIES, C. AND HARRISON, J. 1996. Osmose: Towards Broadening the Aesthetics of Virtual Reality, ACM SIGGRAPH Computer Graphics, Vol. 30, No. 4, p. 25--28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Reality Helmet: A Wearable Interactive Experience Daniel Fallman*, Kalle Jalkanen*, Henrik Lörstad 
, John Waterworth*, and Jonas Westling* Interactive Institute, *Tools for Creativity Studio, Umeå, and 
Sonic Studio, Piteå, Sweden Abstract The Reality Helmet is an interactive experience, in which the user 
s vision and hearing is shielded off from the world. Video and sound is nevertheless recorded by the 
Reality Helmet, but through computer processing it presents sound to the user as vision, and likewise, 
vision is turned into a soundscape. The result is a form of artificial synesthesia. Other than as an 
appreciated art installation, which seems to make people calm and reflective, the Reality Helmet is used 
to explore relationships between the wearer's sense of presence and the kind of realism provided by the 
interactive environment. 1 Introduction The Reality Helmet is a wearable computer developed with the 
purpose of providing its users with altered interactive experiences of reality, (or artificial synesthesia) 
a form of art in which users are actively involved in creating their own, individual experiences through 
the use of digital technology. It physically consists of a custom-made helmet that the user wears, and 
computational equipment placed in a custom built backpack, which allows a high degree of mobility for 
its user. Figure 1. Physical Setup of the Reality Helmet. On the helmet, there are mounted a digital 
video camera and stereo microphones. On its inside, perceived by the user only, are a pair of small visual 
displays and headphones. As shown in Figure 1, the eyes and ears are completely covered when wearing 
the Reality Helmet, and users become audiovisually shielded, while their other senses are not interfered 
with. Thus, while immersed in a personal virtual environment, users still have the benefit of spatial 
freedom, which separates the Reality Helmet from many other virtual environments (cf. VR Caves). Through 
computer processing, the Reality Helmet alters the user s perceptual experience by providing a real-time 
visualization of the auditory environment in which the user moves, and likewise, a landscape of sound 
generated from the digital video input. Hence, the user sees what she would normally hear, and hears 
what she would normally see. * e-mails: {daniel.fallman; kalle.jalkanen; john.waterworth; henrik.lorstad; 
jonas.westling}@tii.se. web: http://www.tii.se Other than as an appreciated installation of art, seemingly 
making users calm and reflective, the Reality Helmet is being used to examine some pertinent research 
questions. One such area is the concept of presence, in which our prototype is used to challenge the 
argument that presence requires a high degree of realism , something which is often sought in VR. This 
work resonates with those who primarily seek to provide users with extraordinary experiences, rather 
than realism [Davies and Harrison, 1996].  2 Implementation The software consists of two applications 
running simultaneously on a laptop, under the Linux operating system, placed in the backpack. Sound analysis 
is based on the open source project Specgram. A Fast Fourier Transform allows a real-time frequency analysis 
of the sound in stereo, which feeds data to a visualization presented to the user. The user has the experience 
of slowly traveling through a tunnel, whose walls reflect the sound environment in which the user is 
but cannot hear.  Figure 2. Sound visualization (left); Image analysis (right). Auditorily, the user 
is presented with a soundscape based on recognition of change in the video input. A calm and ambient 
background sound is played when changes occur in the image as a whole. In addition, sound effects of 
different kinds are added in real-time for every object recognized by image analysis. Top left is the 
current frame, Fi, from the camera. Top right image is a merge, Mi, of previous frames. Mi = Fi-1 a + 
Mi-1 (1-a), where a .[0,1]. Applying this gives the image a certain motion. Bottom left is the image 
when a threshold operation is applied to the absolute difference of Fi and Mi. Every pixel with a value 
above threshold is turned into white, and every pixel with a value below the threshold is turned black. 
Bottom right is an image where a sound source has been connected to a blob . A blob is a set of interconnected 
pixels, i.e. pixels touching other pixels in four directions. The largest blobs are singled out and their 
center points calculated. The size of any given blob is used to determine the volume of the sound source 
which gets associated to it. The position of the center point in the horizontal direction determines 
the position of that particular sound, which may change in real­time and appear to the user as moving 
around in the soundscape.  References DAVIES, C. AND HARRISON, J. 1996. Osmose: Towards Broad­ening 
the Aesthetics of Virtual Reality, ACM SIGGRAPH Computer Graphics, Vol. 30, No. 4, p. 25 28. Copyright 
held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965528</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Designing collaborative games on handheld computers]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965528</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965528</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P142470</person_id>
				<author_profile_id><![CDATA[81100454095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Johan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanneblad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Future Applications Lab, Viktoria Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14060940</person_id>
				<author_profile_id><![CDATA[81100144729]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Lars]]></first_name>
				<middle_name><![CDATA[Erik]]></middle_name>
				<last_name><![CDATA[Holmquist]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Future Applications Lab, Viktoria Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Designing Collaborative Games on Handheld Computers Johan Sanneblad* and Lars Erik Holmquist Future 
Applications Lab, Viktoria Institute 1 Introduction The availability of handheld computers with wireless 
networking, such as Pocket PCs, makes it possible to create new types of interactive games that simply 
could not be done on stationary PCs. For instance, what if in the middle of a game of PacMan your game 
character had to move to the screen of another player s device to successfully complete the game?! Although 
gaming devices such as the Cybiko and Nintendo s Gameboy allow for competitive play via radio link or 
cable, there has not yet been a single commercial game released for handheld computers that can be played 
synchronously by two or more players over a wireless network connection. In a university course, we let 
students create networked games for handheld computers. Their brief was to design games where it would 
be necessary to collaborate to win. For instance, one game explored the possibility of combining the 
displays of handheld computers to form a larger play area, while another assigned each player a specific 
role, so that the players have to coordinate their actions to succeed. 2 Two Collaborative Games PacMan 
Must Die (Figure 1) is a game for two or more players. In a reversal of the classic arcade-game PacMan, 
each player controls a ghost trying to re-capture the dots stolen by PacMan in an earlier adventure. 
To win the game, the player must collect all the dots matching the color of his or her ghost. But some 
of these dots are positioned on the displays of other players devices! The player can enter another person 
s handheld computers by using doors at the edges of the map. When a player has entered the display of 
another computer she of course has to look at the other user s display to control her ghost. There are 
risks involved in entering the display of another player for instance, the other person might run away! 
(see Figure 2) To be successful, a player should make sure to enter the display of another user s handheld 
only if the other player is prepared to collaborate perhaps by in turn entering the display of the first 
player s device.  Figure 1. PacMan Must Die runs on two or more displays simultaneously. Earth Defenders 
(Figure 3) is played with two to five players. The goal of the game is to defend the earth from an alien 
invasion. Each player in the game is given a unique role: Combat Strategy, Star Fighter (two players), 
Orbital Defense, and Opposition *e-mail: johans@viktoria.se e-mail: leh@viktoria.se  Commander. Each 
of these roles involves different responsibilities and limitations. For instance, Combat Strategy cannot 
engage the enemy by herself, but must coordinate player 2-4 by notifying them where and when to engage 
the enemy. The Orbital Defense player cannot move her unit, and will have to request support from other 
players when necessary. The Opposition Commander plays a different role: the enemy, who can position 
new enemy ships at regular intervals on the map. The players defending Earth must continuously discuss 
their plans and give verbal commands to each other but they must be cautious not to let the player controlling 
the enemy forces listen in and get an unfair advantage! Figure 3. Earth Defenders: (a) Combat Strategy, 
(b) Star Fighter, (c) Orbital Defense, and (d) Opposition Commander. 3Implementation and Evaluation 
These two games were implemented by five M. Sc. students during a period of five weeks, using Pocket 
PC handheld computers equipped with Wireless LAN expansion cards. The games use our graphics platform 
GapiDraw (www.gapidraw.com) and our networking platform OpenTrek (www.opentrek.com), both specifically 
designed to create collaborative games on handheld computers. The games can be freely downloaded from 
www.cafetrek.com. A preliminary evaluation was performed at a local high school. Six handheld computers 
were given to students at a café. The students quickly grasped the game concepts, and also played the 
games differently based on experience. Beginners who played the game PacMan Must Die like an ordinary 
video game would quickly find their ghost captured by another player and lose. When players became more 
experienced they started to develop extensive collaborative strategies. It was obvious that all students 
found the games fun and enjoyable. Based on our experience, we believe that this type of collaborative 
games holds great promise. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965527</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Direct interaction based on a two-point laser pointer technique]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965527</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965527</url>
		<abstract>
			<par><![CDATA[The proposed laser pointer technique uses two spots on a screen for interaction with virtual environments and presentations. The spots are produced by splitting an infrared laser beam.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P645092</person_id>
				<author_profile_id><![CDATA[81100034302]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sergey]]></first_name>
				<middle_name><![CDATA[V.]]></middle_name>
				<last_name><![CDATA[Matveyev]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMK, Fraunhofer, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14188184</person_id>
				<author_profile_id><![CDATA[81100542022]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[G&#246;bel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMK, Fraunhofer, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[FROLOV P., MATVEYEV S., G&#214;BEL M. AND KLIMENKO S. 2002. Using Kalman Filter for Natural Hand Tremor Smoothing during the Interaction with the Projection Screen, in Workshop Proceedings VEonPC'2002, pp. 94--101, Protvino, Russia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MATVEYEV S., G&#214;BEL M., FROLOV P. 2003. Laser Pointer Interaction with Hand Tremor Elimination, Proceedings of HCI'2003, Greece (accepted)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Direct Interaction based on a Two-Point Laser Pointer Technique Sergey V. Matveyev IMK, Fraunhofer, 
Germany sergey.matveyev@imk.fraunhofer.de  Abstract The proposed laser pointer technique uses two spots 
on a screen for interaction with virtual environments and presentations. The spots are produced by splitting 
an infrared laser beam. 1 Introduction The proposed two-point interaction technique realizes direct interaction 
and control in virtual environments and presentations. The main principles that the laser pointer (LP) 
interaction technique is built on are the receiving of a position of a laser spot on the projection screen 
and the interpretation of its behaviour (e.g. movement, switching on or off). There are various problems 
associated with the standard use of the LP interaction technique. The use of a direct method for controlling 
the application on the projection screen based on the LP technique is natural at long distances. However, 
difficulty to choose small objects on the screen because of the magnification of natural hand tremors 
is the most important problem that influences the use of the LP method. The second problem is the interference 
of the visible laser spot with objects on the screen that have the same color as the spot. Lastly, the 
LP can only position the laser spot on the screen visually. Thus, the LP has two degrees of freedom (DoF) 
 movement in the x-and y-direction. It does not have an additional DoF like rotation or a mouse-button-like 
function. This sketch discusses ways of overcoming these problems.  2 Method The device for interaction 
is constructed on the basis of an infra­red (IR) laser which is invisible to the human eye and therefore 
does not interfere with the visible channels of a presentation. The problem of interference with like 
colors is overcome because there can be no coincidence of the color of the IR and the visible objects 
on the screen. The basic idea of our LP method is the reception of an additional point on the screen 
created by splitting the IR laser beam into two separate beams with a small angle of divergence. The 
coordinates of the laser spots received from the IR camera are interpreted as the position of the pointer 
(often represented on the screen by a cursor). Interaction is implemented not with the laser spots on 
the screen directly but with their mathematical model. The model is based on the mathematical description 
of a point movement on the screen and on the application of mathematical methods of the data prediction 
and smoothing [1]. The smoothing of the data allows a user to fix the cursor even on very small objects 
since it reduces the effect of cursor trembling on the screen [2]. The beam parallel to the optical axis 
of the laser is the basic beam used for the cursor position, and the procedures for choosing objects, 
performing manipulations and navigation are carried out Martin Göbel IMK, Fraunhofer, Germany martin.goebel@imk.fraunhofer.de 
 concerning this basic beam. The second beam is auxiliary and is used to bring in the additional degrees 
of freedom for controlling an application. Two points received on the display system from two laser beams 
with a definite angle of divergence increase the DoF of the device to four; versus the two DoF of the 
standard device with one laser point. The scheme of the interaction in this case is presented in Figure 
1:  The coordinates (xp, yp) of the basic point are used to choose and to move an object. The interval 
(d) contains the information about distance changes from the device location to the projection screen 
and is used for movement to/from the object; as the laser pointer is moved forward (d) becomes smaller, 
and this is interpreted as a forward movement. The angle (.) is the angle between the straight line passing 
through the points and the abscissa axis of the screen s coordinate system. It contains information about 
the device s orientation relative to its optical axis and is used to rotate the chosen object or the 
whole environment relative to the z­coordinate axis.  Acknowledgments Jeremy Eccles (IMK, Fraunhofer, 
Germany) for assistance and fruitful discussion. References [1] FROLOV P., MATVEYEV S., GÖBEL M. AND 
KLIMENKO S. 2002. Using Kalman Filter for Natural Hand Tremor Smoothing during the Interaction with the 
Projection Screen, in Workshop Proceedings VEonPC 2002, pp. 94-101, Protvino, Russia. [2] MATVEYEV S., 
GÖBEL M., FROLOV P. 2003. Laser Pointer Interaction with Hand Tremor Elimination, Proceedings of HCI'2003, 
Greece (accepted) Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965529</section_id>
		<sort_key>28</sort_key>
		<section_seq_no>28</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Performative projects]]></section_title>
		<section_page_from>28</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653955</person_id>
				<author_profile_id><![CDATA[81100035576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derrick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vicarious Visions]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965530</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Forming the dots]]></title>
		<subtitle><![CDATA[live optical motion capture animation dance]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965530</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965530</url>
		<abstract>
			<par><![CDATA[<i>Bob's Palace</i> is a multi-media dance piece for nine dancers, one video operator, two camera technicians, one motion capture animation artist/designer and one composer/musician. The subject matter of the dance revolves around panic, anxiety and everyday fears. It premiered at the Krannert Center for the Performing Arts, Urbana, Illinois, on February 4<sup>th</sup>, 2003.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653914</person_id>
				<author_profile_id><![CDATA[81392599436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Luc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vanier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Illinois]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36024301</person_id>
				<author_profile_id><![CDATA[81100122490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaczmarski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beckman Institute, Integrated Systems Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653908</person_id>
				<author_profile_id><![CDATA[81100287532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lance]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Beckman Institute, Integrated Systems Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Forming the Dots: Live Optical Motion Capture Animation Dance Luc Vanier Hank Kaczmarski Lance Chong 
University of Illinois Beckman Institute Beckman Institute Dance Department Integrated Systems Lab Integrated 
Systems Lab vanier@uiuc.edu hank@isl.uiuc.edu lchong@ncsa.uiuc.edu Abstract Bob s Palace is a multi-media 
dance piece for nine dancers, one video operator, two camera technicians, one motion capture animation 
artist/designer and one composer/musician. The subject matter of the dance revolves around panic, anxiety 
and everyday fears. It premiered at the Krannert Center for the Performing Arts, Urbana, Illinois, on 
February 4th, 2003. The system used in Bob's Palace starts with a ten digital camera visible red optical 
motion capture (mocap) system manufactured by Motion Analysis Corporation of Santa Rosa, California (http://www.motionanalysis.com). 
The camera array is connected to 48 volt DC power and sends capture data to the central computer over 
a gigabit TCP/IP network switch. This combined (and hopefully stable) dataset is sent over a second gigabit 
network to a second pc with enhanced memory (1GB) and workstation-class graphics capabilities (3D Labs 
Wildcat 4210 graphics engine) running a commercial real-time animation program, Filmbox (www.kaydara.com). 
The high resolution progressive scan computer graphics output from the Filmbox computer is then sent 
into a Sony DSC-1024HD scan converter which outputs NTSC video compatible with the rest of the system's 
video capabilities. We started to work on this project during the summer of 2002 when The Beckman Institute 
s Integrated Systems Laboratory was looking for a dancer to help set up their new infrared motion capture 
lab. The first step in this process was to start experimenting with various marker settings. Reflective 
markers are set on the body so that they can be recognized as triangles by the motion capture software, 
which then communicates with the animation software that attaches an animated figure (avatar) to the 
various geometric shapes. Once the marker settings were stable, we started designing various avatars. 
It is presently very difficult for a moving avatar to look lifelike in real time. In the movie industry, 
most imperfections are usually corrected during post production. Designing the body involves creating 
a virtual skeleton. This skeleton does not respond to dance movement in subtle ways. Contractions or 
undulations of the spine became lost  Copyright held by the author  on the avatar, as did stretched 
knees. It was often difficult for us to link the two bodies together in a way that was acceptable to 
a dancer or choreographer. But in trying to do so, Lance came up with our beloved distorted avatars. 
Even at its most consistent, the system portrayed an inherent instability. This suggested a sense of 
fear or panic to the choreographer. Luc started to design the movement in relationship to the various 
avatars. The multi-dimensionality of the project permits all aspects of art and technology to come together 
and shares the various areas of expertise of the staff at the Beckman Institute, Krannert Center for 
the Performing Arts, and the Dance Department that would not have been available in the past: costumes, 
flooring for dance, understanding the body in movement, etc. In the future, a performance designed optical 
retroreflective motion sensing system or possibly even a camera-based vision recognition system (not 
needing markers) might be more readily available to the dance world. Until that time, collaboration is 
the only way.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965533</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Screen]]></title>
		<subtitle><![CDATA[bodily interaction with text in immersive VR]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965533</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965533</url>
		<abstract>
			<par><![CDATA[<i>Bodily interaction with text</i> creates new reading experiences by involving the body of the reader. This has been explored in video installation art, but without immersive 3D. Using a virtual reality environment (the Brown University Cave) we are able to make the bodily experience more direct. In our initial piece, <i>Screen</i>, the reader produces three different textual experiences from the same body of text, two of which differ significantly based on how her body is employed to "play" the piece.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653893</person_id>
				<author_profile_id><![CDATA[81100114214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Joshua]]></first_name>
				<middle_name><![CDATA[J]]></middle_name>
				<last_name><![CDATA[Carroll]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31035724</person_id>
				<author_profile_id><![CDATA[81545976856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Coover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653949</person_id>
				<author_profile_id><![CDATA[81100458834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shawn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Greenlee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653821</person_id>
				<author_profile_id><![CDATA[81100104250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McClain]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31047440</person_id>
				<author_profile_id><![CDATA[81100554401]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wardrip-Fruin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[PENNY, S., SMITH, J., AND BERNHARDT, A. 1999. Traces: Wireless full body tracking in the CAVE. In Ninth International Conference on Artificial Reality and Telexistence (ICAT'99).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[UTTERBACK, C. 2003. Unusual Positions-embodied interaction with symbolic spaces. In First Person. Wardrip-Fruin, N. and Harrigan, P., eds. MIT Press, in press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286804</ref_obj_id>
				<ref_obj_pid>286498</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[WHITE, T. AND SMALL, D. 1998. An Interactive Poetic Garden. In Extended Abstracts of CHI'98. ACM. 335--6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen: Bodily Interaction with Text in Immersive VR Joshua J Carroll* Robert Coover Shawn Greenlee 
Andrew McClain** Noah Wardrip-Fruin Brown University Brown University Brown University Brown University 
Brown University Abstract Bodily interaction with text creates new reading experiences byinvolving the 
body of the reader. This has been explored in videoinstallation art, but without immersive 3D. Using 
a virtual realityenvironment (the Brown University Cave) we are able to make thebodily experience more 
direct. In our initial piece, Screen, the reader produces three different textual experiences from the 
samebody of text, two of which differ significantly based on how herbody is employed to play the piece. 
1Bodily Interaction Video installation art such as Camille Utterback and RomyAchituv's Text Rain (SIGGRAPH 
Gallery, 2000) enables readersto interact with text via an image of their bodies [Utterback 2003].In 
pieces such as David Small and Tom White's Stream of Consciousness (SIGGRAPH Gallery, 1998) readers may 
use theirbodies to alter the text itself, but the interface's connection to the body is even more indirect 
(a blue glow represents the hand)[White and Small 1998]. Virtual reality systems seem to hold thepromise 
of more direct bodily interaction. However, head­mounted display systems actually cut users off from 
their bodies they are like blindfolds with televisions inside. VR systems likethe CAVE, on the other 
hand, enable the creation of virtual experiences that still allow the user to be grounded by thepresence 
of his body. Artworks like Simon Penny's Traces have taken advantage of this to allow for the direct 
use of the body asthe VR interface creating nearly-athletic experiences in thenormally passive VR environment 
[Penny et al. 1999]. Suchprojects, however, have not yet explored the possibilities forinteraction with 
text. We use Brown's Cave (an IBM-sponsoredimmersive display with three walls and a floor) to enable 
directbodily interaction with text. Our work is carried out as part ofongoing research in spatial hypertext 
writing. 2 Screen Screen creates three reading experiences. The first is relativelytraditional, projecting 
text at the distance of the screens that makeup the walls, producing the illusion of a normal video installation.This 
is important because (a) it reinforces the location of thewalls, which is necessary for the second stage 
of reading and (b)this relatively traditional reading experience creates a point ofreference for the 
experiences that follow. Once the initial text (a meditation on memory as a virtualexperience, and on 
memory s instabilities) has been displayed, thesecond stage of reading begins. At the outset of this 
stage a wordpeels from one of the walls and flies toward the reader. When aword peels it is accompanied 
by an appropriately-positionedripping sound as well as the sound of a word being read. If thereader does 
nothing, the word circles around her. Soon anotherword peels, and then another, at an increasing pace, 
flockingaround the reader (figure 1). *Joshua_J_Carroll@brown.edu Robert_Coover@brown.edu Shawn_Greenlee@brown.edu 
**Andrew_McClain@alumni.brown.edu nwf@brown.edu The reader can intervene in this process by striking 
words withher hand (tracked with either a glove or wand). This body­involved process of reading the words 
that fly at you, of readingthe flock of words around you, of reading individual words whilestriking them 
with your hand is the second stage of reading. Figure 1. Reader with Peeled Words. (Michelle Higa) The 
third stage of reading overlaps with the second. It is what isproduced on the walls through the bodily 
interactions of thesecond stage. When a flying word is struck it flies back toward awall. If it is the 
only word off the wall it will return to the space itleft empty. However, if more than one word is off 
the wall then ahit word may return to a different space. Further, words can breakinto syllables that 
find spaces between (often abutting) words inplace on the walls. The second stage continues until the 
majorityof words are off the wall. Thus it lasts longer the more actively thereader plays the text (in 
a sense of play, perhaps, between thatof the computer game and that of the musical instrument). As thepace 
increases, prolonging the experience requires progressivelygreater physical exertion. Finally, being 
overwhelmed by words isinevitable, no matter how much the reader exerts himself (and nomatter how much 
prior practice he has playing the piece). In our initial evaluations, Screen produces an experience new 
tothose familiar with both related video installation and VR art. It is a new type of relation to language, 
and a new confrontation withtext. In its development, McClain proposed words flocking aroundand overwhelming 
a reader. Wardrip-Fruin proposed text peelingfrom walls and being struck back with the hand. Greenleedesigned 
the sound and implemented the system. McClain beganthe implementation of the graphics and interaction 
system, whichCarroll completed and improved. Wardrip-Fruin and Cooverwrote the text. Interaction design 
was shared by all five authors. References PENNY, S., SMITH, J., AND BERNHARDT, A. 1999. Traces: Wireless 
full body tracking in the CAVE. In Ninth International Conference on Artificial Reality and Telexistence 
(ICAT'99). UTTERBACK, C. 2003. Unusual Positions embodied interaction with symbolic spaces. In First 
Person. Wardrip-Fruin, N. and Harrigan, P., eds. MIT Press, in press. WHITE, T. AND SMALL, D. 1998. An 
Interactive Poetic Garden. In Extended Abstracts of CHI´98. ACM. 335-6. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965531</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Virtual performance and collaboration with improvisational dance]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965531</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965531</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653968</person_id>
				<author_profile_id><![CDATA[81100394531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[Scott]]></middle_name>
				<last_name><![CDATA[Meador]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653866</person_id>
				<author_profile_id><![CDATA[81100550312]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Kurt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653905</person_id>
				<author_profile_id><![CDATA[81100468887]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[O'Neal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Performance and Collaboration with Improvisational Dance W. Scott Meador Eric M. Kurt Kevin 
R. O Neal Assistant Professor Graduate Student Graduate Student Computer Graphics Technology Computer 
Graphics Technology Computer Graphics Technology Purdue University Purdue University Purdue University 
wsmeador@tech.purdue.edu emkurt@tech.purdue.edu kroneal@tech.purdue.edu Purdue University s departments 
of Computer Graphics Technology, and Visual and Performing Arts, have collaborated on a project to create 
a performance that incorporates live dance, and computer graphics imagery that is being generated by 
both live performances and existing real-time cg and video elements. This group includes professors from 
each area, several graduate students in computer graphics, and undergraduate dancers and computer graphics 
students. We have been working together to not only develop a performance, but a way of combining the 
very different workflows of each area. Research In the fall of 2002 this group worked on a project to 
develop a small Black Box performance which used one dancer, and one virtual character that interacted 
throughout the performance. We used this production as a test bed for the larger goals we had for the 
spring of 2003. During this we worked out small issues of collaboration styles and tried to understand 
where the future of this project lay. We had many successes in our fall presentation. We created a performance 
in which one dancer was able to communicate and pass emotions through a real time virtual character. 
At the same time, we also worked with real time choreography. We worked one on one with the director 
during the performance to allow her to direct the virtual character to adjust to the improvisational 
nature of the dance piece. She was able to do this through directing the computer operator using FilmBox 
as a medium. Preproduction Our work at the end of 2002 and the beginning of 2003 was the pre-production 
of a much larger performance than in fall 2002. Our goal was to create a performance that utilizes five 
dancers, one virtual character driven by a dancer in a motion capture suit, three different projection 
screens of video and cg virtual elements, as well as many other computer graphics segments run by several 
computer students. This performance was slated for the beginning of May 2003. Production We have met 
many challenges inherent in combining the work of two very different disciplines. One of the first issues 
we dealt with is the combination of the workflows used regularly in each department. Directors and Choreographers 
that are accustomed to working well within the limitations and peculiarities of traditional performance 
spaces may be unfamiliar, and in some cases, perhaps even uncomfortable, with the expanse of possibilities 
(and limitations) offered by the incorporation of the relatively new medium of computer graphics. This 
works the same way with computer graphics students who don t fully grasp the technical and perceived 
limitations of the modern stage. Our next stepping stone is found in the variation between the two "schools" 
familiarity with the "performance frame". People used to working in a live performance space think of 
the performance frame in terms of the live, viewing, audience-­which is typically on one side of the 
stage (though, of course, there are often variations on this). Whereas people used to working in virtual 
3D environments think of the performance frame in terms of the camera--and in most cases, take for granted 
the absence of gravity and 100% usage of 6-degrees of camera freedom. Besides the combination of the 
differences in normal operating habits, we had to tackle the technical issues of combining our work together. 
We have had to create a working space for planning, creation, and practice of this piece. Due to the 
large amount of hardware used to produce this type of show, it created a complicated issue of defining 
a permanent space, and the needs associated with this. The issues of hardware brought us into our next 
stepping stone, which was combining the hardware required to push computer graphics in real-time to a 
display with the normal technology of a stage. An example of this is the use of multiple displays in 
our stage design. To control the output of multiple displays, we dedicated a computer for each screen. 
Kaydara s FilmBox was set up so that all instances can be controlled by a single set of triggers. The 
computers were networked with Analogus Gypsy 3.0 wired motion capture suit so that data is available 
to all of them. In addition, a server/client relationship is created between the instances of FilmBox 
so that a single device (such as a keyboard or joystick) can control the cameras and characters across 
the network. Continuing Work Our future goals for this project are to continue with production of work 
that incorporates both bodies of knowledge. We hope to answer some of the questions about combining the 
different working environments of each area into an integrated workflow so that production of future 
shows becomes easier. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965532</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Multi-user performance of Commedia dell' Arte in 3D]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965532</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965532</url>
		<abstract>
			<par><![CDATA[Commedia is a medieval form of highly improvisational theatre from Italy in which archetypal characters like Harlequin the sly trickster, or Columbine the tough smart wench, were combined with satire, acrobatics, and low-ball farce into a style that transformed and shaped the writings of Shakespeare, Moliere, and The Loony Toons.In Gilligan's Island, Gilligan's bumbling servant combined with Skippers overweight bravado, against Professors ungrounded wisdom and Mr. Howells miserly riches all vying for the attentions of innocent Marianne and vivacious Ginger typify the modern retellings of Commedia characters and scenarios. Buggs Bunny is Harlequin drawn to life, while Lara Croft is Columbine in perfect 3D.We believe that bringing improvised Commedia to Multi-User 3D Performance will add to both disciplines. It will expand Commedia to the limits of imagination while it will give Multi-User 3D Performance a set of time tested techniques to explore improvisation.Our purpose is to create an application in Macromedia Director that allows actor-driven 3D characters to be controlled through motion and other media capture devices. Imagine <i>Toy Story</i> where the ending changes every night, while the characters play with the audience, and you begin to get the idea.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[commedia del arte]]></kw>
			<kw><![CDATA[improvised computer puppetry]]></kw>
			<kw><![CDATA[multi-user performance]]></kw>
			<kw><![CDATA[real-time character animation]]></kw>
			<kw><![CDATA[web 3D]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14184449</person_id>
				<author_profile_id><![CDATA[81100531227]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Wilson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[3D Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653960</person_id>
				<author_profile_id><![CDATA[81100371073]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Todd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holoubek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Physical Computing Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653826</person_id>
				<author_profile_id><![CDATA[81100611592]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poochareon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Graduate Student]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ALLEN PARTRIDGE. 2001. REAL-TIME INTERACTIVE 3D GAMES]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>559678</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[PAUL CANTANESE. 2001. DIRECTOR'S THIRD DIMENSION, FUNDAMENTALS OF 3D PROGRAMMING IN DIRECTOR 8.5]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[JEAN-MARK GAUTIER. 2001. CREATING INTERACTIVE 3D CHARACTERS AND THEIR WORLDS]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[PIERRE LOUIS DUCHARTRE. 1966. THE ITALIAN COMEDY]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multi-user Performance of Commedia dell Arte in 3D  Michael D. Wilson* 3D Research ITP @ NYU Todd Holoubek 
Physical Computing Research ITP @ NYU Ann Poochareon Graduate Student ITP @ NYU  Abstract Commedia 
is a medieval form of highly improvisational theatre from Italy in which archetypal characters like Harlequin 
the sly trickster, or Columbine the tough smart wench, were combined with satire, acrobatics, and low-ball 
farce into a style that transformed and shaped the writings of Shakespeare, Moliere, and The Loony Toons. 
In Gilligan s Island, Gilligan s bumbling servant combined with Skippers overweight bravado, against 
Professors ungrounded wisdom and Mr. Howells miserly riches all vying for the attentions of innocent 
Marianne and vivacious Ginger typify the modern retellings of Commedia characters and scenarios. Buggs 
Bunny is Harlequin drawn to life, while Lara Croft is Columbine in perfect 3D. We believe that bringing 
improvised Commedia to Multi-User 3D Performance will add to both disciplines. It will expand Commedia 
to the limits of imagination while it will give Multi-User 3D Performance a set of time tested techniques 
to explore improvisation. Our purpose is to create an application in Macromedia Director that allows 
actor-driven 3D characters to be controlled through motion and other media capture devices. Imagine Toy 
Story where the ending changes every night, while the characters play with the audience, and you begin 
to get the idea.         FIG 1: Harlequin; Once and Future  Keywords: real-time character animation, 
multi-user performance, web 3D, improvised computer puppetry, commedia del arte -------------------------------------------- 
*e-mail: mike.wilson@nyu.edu e-mail: todd.holoubek@nyu.edu e-mail: ann@miserychick.net 1 Introduction 
 The goal of our application is to create a platform for providing a synthesis of improvised live theatre 
in the traditional form and character of Commedia dell Arte with the total freedom from physics and geopolitical 
reality that multi-user 3D provides. 2 Exposition While the use of motion capture for computer character 
animation is not a new subject, and has been explored by everyone from MIT to Mat the Ghost ["Character 
Motion Systems", SIGGRAPH 94: Course 9], the combination of actor controlled puppets with the tumbling 
freeform style of commedia adds a new element to the concept similar to the improvised machinima currently 
being done with the QuakeII engine mods [http://www.machinima.org/]. Scenario: You arrive a few minutes 
late to see what appears to be an animated feature in which two exaggerated characters are engaged in 
a love scene. Suddenly the male character looks right at you and says Hey Buddy, do ya mind? I m trying 
to get some action here. Sit down already! You stare in shock at the screen amidst ripples of laughter 
as the female slaps the young man in the face and says, That s for not paying attention to me. Macromedia 
s Director is our application of choice because it is a widely available multi-user and audio streaming 
server with a web based 3D engine in a robust and stable system. In addition, it is an extremely easy 
application to bring in serial information for implementing new animation puppetry devices. For this 
SIGGRAPH we will be creating a proof of concept performance space in which live audio is blended with 
cued pre-built animation bits or acrobatics (lazzi in Commedia) and real time character puppetry in improvised 
scenarios. 3 Conclusion Multi-user 3D is an application with millions of fans that in the world of 
Massively Multiplayer Online Games is growing exponentially [Computer Graphics World, November 2002]. 
Commedia dell Arte is a philosophy and form of live performance with over 500 years of history. Combined 
with computer character puppetry, we believe these forms have the potential to create a whole greater 
than the parts. References ALLEN PARTRIDGE. 2001. REAL-TIME INTERACTIVE 3D GAMES PAUL CANTANESE . 
2001. DIRECTOR S THIRD DIMENSION, FUNDAMENTALS OF 3D PROGRAMMING IN DIRECTOR 8.5 JEAN-MARK GAUTIER. 
2001. CREATING INTERACTIVE 3D CHARACTERS AND THEIR WORLDS PIERRE LOUIS DUCHARTRE . 1966. THE ITALIAN 
COMEDY 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965534</section_id>
		<sort_key>29</sort_key>
		<section_seq_no>29</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Production workflow]]></section_title>
		<section_page_from>29</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP24022392</person_id>
				<author_profile_id><![CDATA[82358662857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Darren]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hendler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965536</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Effective asset management for episodic television and features]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965536</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965536</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653819</person_id>
				<author_profile_id><![CDATA[81335496600]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Allan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rempel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653861</person_id>
				<author_profile_id><![CDATA[81100539392]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dean]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Broadland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653951</person_id>
				<author_profile_id><![CDATA[81100192934]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sonja]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Struben]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39049652</person_id>
				<author_profile_id><![CDATA[81100584369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[F.]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Fracchia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mainframe Entertainment Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Effective Asset Management for Episodic Television and Features Allan Rempel Dean Broadland Mainframe 
Entertainment Inc. Mainframe Entertainment Inc. allan@reboot.com dean@reboot.com 1 Introduction All projects 
require that the materials necessary for their construction be organized, easily accessible, and safely 
stored. These requirements become more pronounced when multiple people work on a project simultaneously. 
CGI animation projects specifically have asset management needs particular to the industry. Yet there 
has been relatively little attention given to this topic in the literature. This sketch shows how a new 
asset management tool called ShotTracker2, developed at Mainframe Entertainment Inc, was essential to 
the efficient completion of the direct-to-video feature Barbie as Rapunzel. 2 Motivation The demands 
of TV animation require that Mainframe have an efficient and streamlined animation pipeline, with a minimum 
of overhead. In the past, informal personal communication was used to transfer responsibility for a shot 
from one person to another. However, as Mainframe moved toward the development of feature-length CGI 
productions, more structure became necessary to keep track of data and responsibility for shots, and 
to be able to ascertain the overall status of the production. Initially, that structure took the form 
of "pink slips" - pink pieces of paper physically passed from one person to another to denote the transfer 
of responsibility for a shot. The pink slip would contain the necessary information for the next person 
to continue the shot. However, this system had its problems most notably being the misplacement of pink 
slips. After investigating a number of commercially available options and finding them not entirely suitable 
for our environment, ShotTracker2 was developed. 3 Features ShotTracker2 allows animators to submit 
and retrieve their shot data to and from a secure centralized repository. It allows only those to whom 
a shot is assigned to submit data for that shot, and Sonja Struben F. David Fracchia Mainframe Entertainment 
Inc. Mainframe Entertainment Inc. sstruben@acm.org drdude@reboot.com provides mechanisms for assigning 
a shot between department supervisors and between supervisors and crew members. It provides automatic 
versioning of assets by separating out and keeping track of the data from each submit request. Notes 
can be added to a shot at any time. In addition, an auto-render facility creates quarter-size images 
at submit time so that scenes can later be visually browsed. ShotTracker2 is able to process a variety 
of types of data associated with a particular shot, including Maya scene data as well as data associated 
with Grin3, Mainframe's proprietary lip­sync animation system. As Softimage is our primary animation 
system, ShotTracker2 is optimized to handle 3D and XSI data. In particular, it checks Softimage scenes 
for validity and weeds out extraneous elements. It is also efficient in its use of disk space in that 
recurring elements such as texture files are stored in another repository and referred to by shots, thus 
reducing redundancy. ShotTracker2 currently runs under both Irix and Linux. 4 Results Anecdotal reports 
from the crew of Barbie as Rapunzel were overwhelmingly positive. Operationally, workflow improvements 
translated to increased productivity and a noticeable reduction in overhead cost. ShotTracker2 supports 
the need for increased structure and organization for large-scale projects, and also shows that an asset 
management system need not be extremely complex or expensive to be effective. Had it been more complex, 
it might not have yielded the same workflow improvements. Asset management is a vital part of any animation 
production. It reduces the dependence of the success of the project on the discipline of the project 
members, which will ultimately result in a smoother, more efficient, and less costly production. The 
success of ShotTracker2 continues with our current feature Barbie of Swan Lake (Figure 1). It has also 
been modified for use in episodic TV series production and is currently being tested on selected episodes 
of Hot Wheels: Highway 35 and Spider-Man: The Animated Series, where the scale is measured in weeks. 
 Figure 1. ShotTracker2 (main screen). Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965535</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Home grown CGI]]></title>
		<subtitle><![CDATA[the cultivation of "Henry's Garden"]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965535</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965535</url>
		<abstract>
			<par><![CDATA[Can a home-based production beset with software incompatibilities, corrupt hard drives, earthquakes and teething puppies make it to the big screen? The creators of "Henry's Garden" air their dirty laundry in this candid look at the making of an independent, animated short film.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP31037276</person_id>
				<author_profile_id><![CDATA[81100325450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kevin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Geiger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simplistic Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653923</person_id>
				<author_profile_id><![CDATA[81100162705]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Moon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simplistic Pictures]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Home Grown CGI: The Cultivation of Henry s Garden  Kevin Geiger Simplistic Pictures kevin@simplisticpictures.com 
 Abstract Can a home-based production beset with software incompatibilities, corrupt hard drives, earthquakes 
and teething puppies make it to the big screen? The creators of Henry s Garden air their dirty laundry 
in this candid look at the making of an independent, animated short film. 1 Into The Rabbit Hole What 
happens when two animation industry veterans embark upon production of an independent, animated short 
film without the benefit of custom tools, a sys admin or a decent render queue? How many computers can 
you fit into your bedroom, and what are the pros and cons of running ethernet through the kitchen? When 
Moon Seun and Kevin Geiger began production of Henry s Garden in January 2000, they thought they knew 
what they were getting into. See the artists cling to their creative vision (and their sanity) over the 
next 2½ years while their home studio grows from a lowly iMac to a network of more than 6 PCs, their 
savings shrink, and their dog happily chews through cable after cable as the film almost doubles in length. 
 2 The Art Of The Deal From first draft to film out, producing Henry s Garden required resources and 
resourcefulness. Access to hardware, software and post-production services was achieved by coupling out-of-pocket 
expenses with a few key professional favors. Crafting arrangements of mutual benefit was critical to 
the realization of the project.   Moon Seun Simplistic Pictures moon@simplisticpictures.com   3 
No Matter Where You Go, There You Are One of the more interesting aspects of Henry s Garden was the 
extent to which the project served as a microcosm of large-scale studio production in ways both intentional 
and unanticipated. By design, Henry s Garden was structured to emulate the development and production 
process of a typical animation studio, from script and storyboard through animatic and work reel to final 
color. Ironically, the film also echoed the studio process in terms of creative disputes, technological 
disruptions, looming deadlines, nerve-wracking test screenings and story revisions. In its own modest 
way, the creation Henry s Garden sheds light upon the considerations and factors that comprise the inherent 
price of admission for an animated theatrical production.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965537</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Production-grade scene translation pipelines]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965537</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965537</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP42049024</person_id>
				<author_profile_id><![CDATA[81320492686]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maskit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP24009175</person_id>
				<author_profile_id><![CDATA[81320489804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chamberlain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Production-Grade Scene Translation Pipelines Daniel Maskit and Chamberlain Fong* Digital Domain 1 Introduction 
Today s increasingly complex computer effects and animation fre­quently require the ability to create 
and edit scene elements in more than one 3D package. At Digital Domain we have built a set of translation 
tools that enables artists to quickly and accurately move entire scenes from one package to another. 
This enables our artists and technical directors to construct each portion of a scene in the preferred 
tool. Moving geometry from one package to another is quite simple, and moving data from a 3D package 
to a renderer is a solved prob­lem using such tools as Pixar s MTOR. Our goal, however, is to allow the 
movement of entire scenes from one animation and mod­eling package to another while preserving animation, 
object names, camera and light settings, etc. This type of translation has tradition­ally been performed 
through the use of a standardized intermediate format, such as iges or Inventor, but we have found such 
approaches to be cumbersome and slow. In addition, even with a standard inter­mediate format we have 
found ourselves developing custom tools for each translation task. There are commercial products available, 
such as Okino s PolyTrans, but they only address a subset of the problems we need to solve. The tools 
that we have created per­form all of the tasks we require, and are fully integrated into our production 
pipeline. Figure 1: Scene from Adidas Mechanical Legs in Maya 2 Approach Our approach uses compiler 
techniques to analyze scenes, with the generated code being a script in the language of the target package 
that will build the scene when run. This approach is coupled with custom-written tools to allow for import 
and export of package­native geometry .les such as Houdini s BGEO .les. There are complications in the 
translation process due to different packages having different de.nitions of standard geometries. For 
example, Maya uses a counter-clockwise ordering of polygon vertices, while Houdini s ordering is clockwise. 
A more complex discrepancy is the NURBs representation between Maya and Houdini. While we have working 
tools for several packages, this presentation will pri­marily discuss our Maya to Houdini and Houdini 
to Maya transla­tion tools. The major compilation challenge posed by these packages is the difference 
in their approach to geometry hierarchy. Maya uses a * e-mail: danielm@d2.com, cfong@d2.com Directed 
Acyclic Graph (DAG) structure, and encapsulates relation­ships among objects within parenting relationships. 
Houdini uses a network-based structure which will support parenting, but will also support cross-referencing 
among networks. The Houdini Object space, which supports parenting similar to that found in the Maya 
DAG, contains lights and cameras, as well as procedural geometry subnetworks (SOPs). In the export from 
Maya to Houdini some portion of the DAG is placed in the Object space, with the rest of the geometry 
being placed in SOP networks. The user can control how this division occurs. While it is possible to 
recreate the entire Maya DAG within the Houdini Object space, this is a signi.cant departure from the 
Houdini scene philosophy. Figure 2: Houdini Exports with Full and Pruned Export Trees To reconcile the 
differences in how the tools construct geometry, the translation pipeline establishes a set of simple 
rules for the con­tents of SOP networks in Houdini. These rules result in constraints on the ability 
to edit portions of the exported scene, but provide for a cleanly de.ned reversible transformation from 
one package to the other. Data within a SOP network is treated as either rigid geometry with a single 
set of transformations, or as deforming geometry. An­imation of rigid objects is fairly simply handled 
through a mixture of Houdini chan .les and scripted keyframing. None of the packages we work with have 
signi.cant common­alities in how they create deforming geometry. We have adopted a strategy of using 
geometry blending. For the initial frame of an an­imation full de.nitions of geometry are generated. 
For subsequent frames we export point cloud .les containing the current position of all points in the 
geometry. Using either existing mechanisms (such as the Houdini blend SOP), or using custom-written code 
(such as a Maya deformer), we interpolate between the original de.nition and the current point cloud 
to duplicate the deformation. Support for sub-frame sampling allows us to generate accurate motion blur. 
3 Conclusions While the goal of a universal scene graph is an admirable one, in practice the dif.culty 
lies not in representing the data during the transformation, but in reconciling differences among packages. 
Our approach is to integrate the translation code with the packages, to use native scene graph management 
tools for our frontend, and to directly generate the native code for our target to recreate the scene. 
This signi.cantly reduces our execution times as we do not need to massage data to insert and retrieve 
it from an intermediate format. These tools have been used in production on a number of com­mercials 
(including Adidas Mechanical Legs ), music videos, and feature .lms (including Daredevil ), and are now 
a fully integrated component of our production pipeline. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965538</section_id>
		<sort_key>30</sort_key>
		<section_seq_no>30</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Simulating nature]]></section_title>
		<section_page_from>30</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP311466200</person_id>
				<author_profile_id><![CDATA[82658659657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Neill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965540</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Rendering plant leaves faithfully]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965540</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965540</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653929</person_id>
				<author_profile_id><![CDATA[81100189415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Franzke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Dresden University of Technology, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP15021799</person_id>
				<author_profile_id><![CDATA[81100092246]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deussen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Constance, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BARANOSKI, G., AND ROKNE, J. 2001. Efficiently simulating scattering of light by leaves. The Visual Computer (September).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[GOVAERTS, Y., JAQUEMOUD, S., AND ANS S. USTIN, M. V. 1996. Three dimensional radiation transfer modeling in a dicotyledon leaf. Applied Optics 35, 6585--6598.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[HOSGOOD, B., JACQUEMOUD, S., ANDREOLI, G., VERDEBOUT, J., PEDRINI, G., AND SCHMUCK, G. 1995. Leaf optical properties experiment 93. Tech. rep., Joint Research Center, European Comission, Institute for remote sensing applications, EUR 16095 EN.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H., MARSCHNER, S., LEVOY, M., AND HANRAHAN, P. 2001. A practical model for subsurface light transport. In Proceedings of SIGGRAPH 2001, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[VOGELMANN, C. 1993. Plant tissue optics. Annu. Rev. Plant Physiol. Plant Mol. Biol. 44, 231--251.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[WOLLEY, J. 1971. Reflection and transmittance of light by leaves. Plant Physiology 47, 656--662.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering Plant Leaves Faithfully Oliver Franzke Dresden University of Technology, Germany We present 
a practical approach for an improved representation of translucency in plants. This approach is based 
on a set of spe­cially obtained textures that are combined with a biologically moti­vated optical model 
for leaves. Figure 1: A plant model illuminated from different sides. Using the proposed method the 
translucency is rendered correctly A leaf is usually modelled by four layers with different optical properties 
[Vogelmann 1993]. The upper and lower epidermis cover the leaf, in the interior a layer of elongated 
palisade parenchyma is arranged densely in parallel to the incident radiation (cf. Figure 2). (a) (b) 
Figure 2: (a) Leaf interior b) scattering of light in an optical layer The light passes through these 
cells into the spongy layer of the leaf. The elliptical cells, which are interspersed with intercellu­lar 
air spaces, cause internal scattering of the incident radiation and thereby distribute the light within 
the leaf. This process is important for an optimal absorption of the light by the leaf s mesophyll. In 
several papers the spectral re.ection of leaves was measured [Hos­good et al. 1995; Wolley 1971]. Also, 
some attempts have been made to simulate this behaviour [Govaerts et al. 1996]. However, the rendering 
of some leaves still needs about 95 minutes on an SGI R10000 (cf. [Baranoski and Rokne 2001]). On the 
other hand, ef­.cient methods for rendering subsurface scattering were proposed by Jensen et al. [Jensen 
et al. 2001]. In our ray tracing approach the leaf is represented by a set of textures obtained from 
real leaves. The leaf is modelled by three layers with two internal and two external borders. The re.ection 
of upper and lower side is modelled by a re.ection scan of the corre­sponding plant leaves. The leafs 
extinction is obtained by a through light scan and serves also as a source for a thickness map. During 
ray tracing, the ray is refracted at each border, optical ex­periments show that mostly forward scattering 
takes place in plant leaves, hence backscattering can be neglected. Similarly to Jensen *see http://www.inf.uni-konstanz.de/ 
deussen Copyright held by the author Oliver Deussen* University of Constance, Germany et al. [Jensen 
et al. 2001] the ray is traced through the scattering me­dia of the interior of the leaf and the single 
scatter term is computed. Light intensity is reduced exponentially according to the extinction map. The 
multi scatter term is approximated by a small ambient factor -optical experiments justify this for thin 
leaves. The internal structure of the leaf (the spongy layer) is represented by the textures and the 
corresponding re.ection as well as transmission functions. For ef.ciency reasons we omitted modelling 
the interior geomet­rically. Using our technique we are able to render medium sized plants in high resolution 
within a few minutes. The accompanying video demonstrates that the results are suitable for animations. 
In the future we will extend our model to larger plants and try to im­plement a good approximation in 
hardware using vertex and pixel shaders. Figure 3: A small bush rendered with translucency. References 
BARANOSKI, G., AND ROKNE, J. 2001. Ef.ciently simulating scattering of light by leaves. The Visual Computer 
(September). GOVAERTS, Y., JAQUEMOUD, S., AND ANS S. USTIN, M. V. 1996. Three dimen­sional radiation 
transfer modeling in a dicotyledon leaf. Applied Optics 35, 6585 6598. HOSGOOD, B., JACQUEMOUD, S., ANDREOLI, 
G., VERDEBOUT, J., PEDRINI, G., AND SCHMUCK, G. 1995. Leaf optical properties experiment 93. Tech. rep., 
Joint Research Center, European Comission, Institute for remote sensing applications, EUR 16095 EN. JENSEN, 
H., MARSCHNER, S., LEVOY, M., AND HANRAHAN, P. 2001. A practical model for subsurface light transport. 
In Proceedings of SIGGRAPH 2001, 511 518. VOGELMANN, C. 1993. Plant tissue optics. Annu. Rev. Plant Physiol. 
Plant Mol. Biol. 44, 231 251. WOLLEY, J. 1971. Re.ection and transmittance of light by leaves. Plant 
Physiology 47, 656 662.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965539</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Realistic and fast cloud rendering in computer games]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965539</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965539</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653927</person_id>
				<author_profile_id><![CDATA[81100212369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Niniane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Microsoft Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[HARRIS, M., AND LASTRA, A. 2001. Real-time cloud rendering. In Computer Graphics Forum, Blackwell Publishers, vol. 20, 76--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SCHAUFLER, G. 1995. Dynamically generated imposters. In Modeling Virtual Worlds - Distributed Graphics, MVD Workshop, 129--136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965541</section_id>
		<sort_key>31</sort_key>
		<section_seq_no>31</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Virtual reality displays]]></section_title>
		<section_page_from>31</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP40029075</person_id>
				<author_profile_id><![CDATA[81100615391]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Slater]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965542</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Simulating spatial assumptions]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965542</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965542</url>
		<abstract>
			<par><![CDATA[This sketch presents a new method for assessing 'functional realism' across a range of applications. The basic premise is that an individual's prior experience will influence how he or she perceives, comprehends and remembers new information in a scene. 36 participants across three conditions of varied rendering quality of the same space were exposed to the computer graphics environment and completed an object-based memory recognition task. The preliminary experiments presented here could have significant implications while identifying areas of an interactive computer graphics scene that require higher quality of rendering as well as areas where lower fidelity could be adequate, based on this premise.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P348897</person_id>
				<author_profile_id><![CDATA[81100567689]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katerina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14075354</person_id>
				<author_profile_id><![CDATA[81406595069]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robinson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Sussex, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BREWER, W. F. & TREYENS, J. C. (1981). Role of Schemata in Memory for Places. Cognitive Psychology, 13, 207--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>942443</ref_obj_id>
				<ref_obj_pid>942439</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MANIA, K., TROSCIANKO, T., HAWKES, R., CHALMERS, A. (2003). Fidelity Metrics for VE Simulations based on Spatial Memory Awareness States. Presence, Teleoperators and Virtual Environments, 12(3), MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulating Spatial Assumptions Katerina Mania Andrew Robinson University of Sussex, UK Abstract This 
sketch presents a new method for assessing functional realism across a range of applications. The basic 
premise is that an individual s prior experience will influence how he or she perceives, comprehends 
and remembers new information in a scene. 36 participants across three conditions of varied rendering 
quality of the same space were exposed to the computer graphics environment and completed an object-based 
memory recognition task. The preliminary experiments presented here could have significant implications 
while identifying areas of an interactive computer graphics scene that require higher quality of rendering 
as well as areas where lower fidelity could be adequate, based on this premise. Methodology Being in 
a certain place (say, an office) results in mentally representing spatial elements that humans perceive 
there, depending on prior experience. These representations could be described as assumptions or spatial 
hypotheses than humans adopt after a very short exposure to a space. If we were able to exploit existing 
research on how these assumptions or schemata are formed in real life and then be able to simulate those 
assumptions in a VE scenario, we would have a powerful new measure of functional realism (same information 
is transmitted in real and artificial scenes). This measure, discussed here, sets out to match the capabilities 
of a Virtual Environment (VE) to the requirements of the human perceptual and motor systems rather than, 
for instance, focusing on tracking eye gaze as certain commonly employed techniques do. The first experimental 
results will be presented which hold promise and can lead to an efficient use of graphics resources, 
via a rendering engine which renders only those objects to a high level which require detailed inspection, 
and vice versa. This approach could be applied across a range of application fields and in situations 
when there is no overt task which is capable of being learned and assessed in a quantitative manner, 
ultimately offering a generic solution based on scene context rather on the performance of a task. An 
individual s prior experience will influence how he or she perceives, comprehends and remembers new information. 
Schemata are knowledge structures based on past experience. When participants are exposed to a large 
amount of information in a scene, cognitive psychologists have suggested that schemata are used to guide 
the search for information in memory (Brewer &#38; Treyens). An activated schema could aid retrieval 
of spatial information which is present in a specific space and also trigger the apparent recall of non-present 
(inferred) spatial information after exposure. This research initially explores the relationship between 
object-based memory recognition performance and schema in question, in a computer graphic scene rendered 
in varying levels of rendering quality (radiosity). e-mail: k.mania@sussex.ac.uk 36 participants across 
three conditions of varied rendering quality of the same space were exposed to the VE (Figure 1) and 
completed a memory recognition task related to the objects in the room, which was a typical academic 
s office. The VE was displayed on a Head Mounted Display (HMD) in stereo. The objects in the scene felt 
under five distinct categories: High-schema expectancy objects present (computer, desk, etc.), High-schema 
objects that are absent (keyboard, telephone, etc.), Non-schema present (skull, etc.), Non-schema absent 
(wrench, etc.) and Frame objects (floor, ceiling, lights, walls, etc). They were then asked to indicate 
on a scale of 1 to 5 how strongly they believed each object on the list was present in the environment 
to which they had been exposed, with 1 being positively not present and 5 being positively present (76 
objects in total).  Figure 1: Experimental room (high quality) Experimental results are going to be 
detailed during the presentation. It could be concluded that gross quality of the rendering (render less) 
was adequate for schema related spatial information to be recalled, compared to non-schema objects that 
require detailed inspection (render more) of a photorealistic scene. Rendering computation, therefore, 
for applications that require a high level of spatial awareness (simulation, games, etc.) could be associated 
with the relation of each object in a scene to the scene s context or schema, no matter what the task 
is. Future work will include further experimentation and a psychophysical validation of an object-based 
rendering engine based on experimental results.  References BREWER, W.F. &#38; TREYENS, J.C. (1981). 
Role of Schemata in Memory for Places. Cognitive Psychology, 13, 207-230. MANIA, K., TROSCIANKO, T., 
HAWKES, R., CHALMERS, A. (2003). Fidelity Metrics for VE Simulations based on Spatial Memory Awareness 
States. Presence, Teleoperators and Virtual Environments, 12(3), MIT Press. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965543</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A tile-based 3D frame using a reconfigurable display matrix]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965543</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965543</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653853</person_id>
				<author_profile_id><![CDATA[81100142684]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daqing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P247806</person_id>
				<author_profile_id><![CDATA[81100536039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Roger]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Crawfis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>732300</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[R. Raskar, G. Welch, K. Low, and D. Bandyopadhyay (2001). "Shader Lamps: Animating Real Objects With Image-Based Illumination," Rendering Techniques 2001, Proceedings of the Eurographics Workshop in London, United Kingdom. S. J. Gortler and K. Myszkowski. University College London (UCL), London, England, Springer, New York: 89--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Tile-Based 3D Frame Using A Reconfigurable Display Matrix Daqing Xue Roger Crawfis The Ohio State 
University {xue, crawfis}@cis.ohio-state.edu 1 Introduction We develop a tile-based 3D frame with a 
reconfigurable display matrix to reproduce the scene. Our display matrix can be configured into many 
scenes without building the exact geometric models as in Raskar et al. s shader lamp [1]. By adjusting 
the tile positions, we can reproduce different scenes on the display matrix or add interactivity by varying 
the image depth buffer in real time. 2 System Overview Figure 1 shows the diagram of our 3D frame system. 
The scene image is divided into the same number of the tiles as the display matrix, pre-warped and projected 
from the left projector onto the non-planar display surface of the matrix. The displacement of each tile 
is determined by the average depth of the pixels that project onto it. Since the display surface is a 
discontinuous manifold, shadows occur due to the illumination from the left projector and may be visible 
to the user in the walking area. The right projector is used to illuminate or hide these shadow regions. 
The user at the center of the walking area sees a geometrically correct scene image on the non-planar 
display wall. Users, viewing the display off center, see a geometric approximation to the 3D model with 
per-pixel illumination. To change the display surface interactively, we have developed a robotic display 
matrix. Figure 2 shows our matrix with a 16×8 set of display tiles. Each tile is driven by an inexpensive 
servo motor. We can adjust the tiles displacement to create different scenes as well as provide interactivity 
or dynamic scene support by modifying the tiles displacement in real time. 3 Texture Image Warping 
Each tile is mapped with its counterpart in the projected scene view with image or texture blocks of 
equal size initially. The display tile is then extruded according to the average depth of the pixels 
in their image tile. This changes the projected texture and thus the average depth. We iteratively adjust 
the tile s displacement, the mapped texture, and its corresponding depth until we have a good warping. 
Then all tiles on the display wall are rendered to the left and right projectors, respectively, and mapped 
with the proper texture tiles created from the above step. For the right projector, we use the tile geometry 
to calculate the projected shadows from the left projector. We use this shadow mask to restrict the energy 
projected from the right projector. Finally, these two warped images are projected from the left and 
right projectors, respectively, to reproduce the virtual environment on the scene-shaped display wall. 
 4 Application We have applied our system to render a torus scene, a dynamic scene of a bouncing teapot, 
and some landscape scenes. Figure 3 shows a picture of the torus scene on the display matrix taken from 
the center in the walking area. The landscape scene and the bouncing teapot scene can be visited at http://www.cis.ohio­state.edu/~xue/research/siggraph03-sketch. 
 References [ 1] R. Raskar, G. Welch, K. Low, and D. Bandyopadhyay (2001). Shader Lamps: Animating Real 
Objects With Image-Based Illumination, Rendering Techniques 2001, Proceedings of the Eurographics Workshop 
in London, United Kingdom. S. J. Gortler and K. Myszkowski. University College London (UCL), London, 
England, Springer, NewYork: 89-102. tile  Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965545</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Video-based virtual environments]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965545</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965545</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P327774</person_id>
				<author_profile_id><![CDATA[81100304105]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pintaric]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31026754</person_id>
				<author_profile_id><![CDATA[81100090389]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Albert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rizzo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39053030</person_id>
				<author_profile_id><![CDATA[81100662479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ulrich]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Neumann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>376408</ref_obj_id>
				<ref_obj_pid>354384</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ulrich Neumann, Thomas Pintaric, and Albert Rizzo. Immersive Panoramic Video. In Proceedings of the 8th ACM International Conference on Multimedia, pp. 493--494.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237263</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Smith, A. R., and Blinn, J. F. Blue screen matting. In Proceedings of ACM SIGGRAPH 96, pp. 259--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Albert Rizzo, Ulrich Neumann, Thomas Pintaric, and Martin Norden. Issues for Application Development Using Immersive HMD 360 Degree Panoramic Video Environments. In Usability Evaluation and Interface Design, Smith, M. J., Salvendy, G., Harris, D. & Koubek, R. J. (Eds.) Vol. 1, pp. 792--796.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rebecca Allen and Eitan Mendelowitz. Coexistence. In Proceedings of CAST01 // living in mixed realities, pp. 299--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VIDEO-BASED VIRTUAL ENVIRONMENTS Thomas Pintaric*, Albert Rizzo, Ulrich Neumann University of Southern 
California In this sketch we present a system for rapidly developing and deploying semi-interactive, 
video-based Virtual Environments. 1. INTRODUCTION In a previous paper [1] we described a procedure for 
acquiring high­resolution, omni-directional video through an array of cameras, mounted above a pentagonal 
mirror pyramid. Expanding upon our results, we have devised a software framework to perform interactive 
compositing of panoramic-video backgrounds with real-time computer-generated graphics and studio recordings 
of actors, who were previously shot against a blue screen. Combined with simple scripting capabilities, 
our technique makes it possible to rapidly construct photo-realistic Virtual Reality applications. The 
resulting VR environments are presented to users through a set of head-tracked display glasses, and can 
be driven by an off-the-shelf personal computer with a graphics processor that supports programmable 
shading. 2. SYSTEM DESCRIPTION We place a frame-synchronized camera array above a pyramid-shaped arrangement 
of planar mirrors to record multiple video streams (at 30 fps) that share a common center of projection 
[1]. Our software corrects for lens distortion using a static camera calibration model and seamlessly 
combines the single camera views into a high-resolution 360° x 75° (approx.) cylindrical video stream, 
which later becomes the backdrop to our Virtual Environments. The resulting 360° footage is split into 
sub­streams, or video-tiles , each of which covers an equal fraction of the total field of view. This 
is necessary to allow for selective decoding of a user s region of interest, based on her viewing direction. 
Subsequently, every video-tile gets compressed using non-temporal (or sparsely­temporal1) video compression 
methods (such as Motion-JPEG), which yield better interactive performance by minimizing the delay induced 
from switching between sub-streams at playback-time. Scripted dialogs and non-verbal interaction, performed 
by real-life actors, form a key-element of our video-based environments. For each actor, we first capture 
a limited set of base-posture cycles consisting of repeating video loops. Subsequently, we record scripted 
sequences such that they can seamlessly be triggered at specific branching points, whereby real-time 
video-feedback assists actors to find the correct starting and ending-pose. For all of the above steps 
we use a blue screen and reproduce lighting conditions that roughly match the panoramic background footage. 
The resulting video clips are stored separately from the multiplexed background-tiles, since they require 
fast random-access. Our player software selectively decodes only the currently visible video-elements 
and texture-maps the video onto proxy-geometry to perform perspective correction. All scripted sequences 
are drawn on billboards in front of the cylindrically projected panoramic background footage using fragment 
shading for real-time masking and spill suppression, as described by Vlahos and summarized in [2]. Altogether, 
our system is able to maintain consistently high frame-rates above 60 Hz, with the video rendered at 
30 Hz. * now at Vienna University of Technology By sparsely-temporal, we mean utilizing relatively short 
key-frame distances of, at most, two to three frames. 3. APPLICATIONS We currently use video-based Virtual 
Environments in a study on the assessment and therapeutic treatment of social anxiety disorders [3], 
where realistic rendition of human interaction is a crucial element for evoking a sense of presence within 
the simulated surroundings. Our technology has also recently been used to create an interactive mixed­reality 
arts installation [4]. 4. ACKNOWLEDGEMENTS This research was funded in part by the Integrated Media Systems 
Center, a National Science Foundation Engineering Research Center, Cooperative Agreement No. EEC- 9529152. 
5. REFERENCES [1] Ulrich Neumann, Thomas Pintaric, and Albert Rizzo. Immersive Panoramic Video. In Proceedings 
of the 8th ACM International Conference on Multimedia, pp. 493 494. [2] Smith, A. R., and Blinn, J. F. 
Blue screen matting. In Proceedings of ACM SIGGRAPH 96, pp. 259 268. [3] Albert Rizzo, Ulrich Neumann, 
Thomas Pintaric, and Martin Norden. Issues for Application Development Using Immersive HMD 360 Degree 
Panoramic Video Environments. In Usability Evaluation and Interface Design, Smith, M.J., Salvendy, G., 
Harris, D. &#38; Koubek, R.J. (Eds.) Vol. 1, pp. 792 796. [4] Rebecca Allen and Eitan Mendelowitz. Coexistence. 
In Proceedings of CAST01 // living in mixed realities, pp. 299 302. We place a frame-synchronized 5-camera 
array above a planar mirror pyramid to capture omni-directional video at 30 fps. Our software seamlessly 
combines the separate camera views into a single high-resolution 360-degree panoramic video stream. 
We script interactive dialogs and shoot actors in a studio against a green screen. The resulting video 
clips are then composited into the panoramic background footage in real-time. Transparency masks are 
computed in real-time using the fragment shader as chroma keyer. The final VR environment is explored 
through a set of head-tracked display glasses. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965544</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Omnidirectional stereo surround for panoramic virtual environments]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965544</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965544</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14183008</person_id>
				<author_profile_id><![CDATA[81100527067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Simon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer IMK, Sankt Augustin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14063284</person_id>
				<author_profile_id><![CDATA[81100152956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steffi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Beckhaus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer IMK, Sankt Augustin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>166134</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CRUZ-NEIRA, C., SANDIN, D. J., AND DE FANTI, T. A., 1993. Surroundscreen projection-based virtual reality: The Design and Implementation of the CAVE. Proc. SIGGRAPH '93: 135--142.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[OETTERMANN, S., 1997. The Panorama: History of a Mass Medium. New York: Zone Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PELEG, S. AND BEN-EZRA, M., 1999. Stereo panorama with a single camera. CVPR '99: 395--401.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280871</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[RADEMACHER, P. AND BISHOP, G., 1998. Multiple-Center-of-Projection Images. Proc. SIGGRAPH '98: 199--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>826579</ref_obj_id>
				<ref_obj_pid>826030</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[SIMON, A., AND G&#214;BEL, M., 2002. The i-Cone#8482; - A Panoramic Display System for Virtual Environments. Proc. Pacific Graphics '02: 3--7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Omnidirectional Stereo Surround for Panoramic Virtual Environments  Andreas Simon  Steffi Beckhaus 
Fraunhofer IMK, Sankt Augustin, Germany, {simon,beckhaus}@imk.fhg.de By combining the advantages of 
19th century panorama paintings featuring a high quality image, full integration of the observer in a 
multi-user environment, exploration and browsing independent from others with projection based virtual 
environment display technology featuring dynamic, interactive content and real-time rendering of stereoscopic 
3D images we create an interactive multi-user omnidirectional stereo display system. This allows for 
new paradigms of group interaction and storytelling in virtual en-vironments. 1 Introduction Panorama 
paintings (from the Greek pan all and hórama see) of the 19th century were a remarkable invention, delivering 
a sense of place and high quality suspension of disbelief [OETTERMANN 1997]. These paintings were large 
installations, completely im-mersing the viewer, shutting him off from distracting real world artifacts, 
except for other visitors and props that continue the im-age into the foreground. Up to 150 visitors, 
standing on a raised platform, were free to walk around and browse the image, looking in any direction. 
Group experiences in today's immersive virtual environments like the CAVE [CRUZ-NEIRA ET AL. 1993] suffer 
from a lack of inter-active components and the lack of independent observation possi-bilities for the 
individual participant. The undistorted view in con-ventional perspective stereo images is bound to a 
preferred view-ing direction, selected by the application or the viewing direction of a single head tracked 
guide. All interaction with the content is also done by this guide. A viewer looking at 90° to the preferred 
viewing direction perceives no stereo [Fig.2a]; looking in the opposite direction produces an autoscopic 
image with left- and right-eye images swapped. This results in a narrow 120º stereo-scopic field of view 
with one focus of attention and leaves the group of participants as being passive spectators. But large-scale 
panoramic displays, then and now, are designed to engage large groups of par 2 Omnidirectional Stereo 
We have developed a new omnidirectional stereo surround per-spective suitable for 360º panoramic curved 
screen displays for virtual environments. This technique is appropriate for real-time rendering and allows 
multiple participants to share one contiguous virtual environment and to experience true stereoscopic 
vision in every direction without head tracking. Omnidirectional stereo pro-jection is a multiperspective 
technique [RADEMACHER AND BISHOP 1998], based on circular projection stereo [PELEG AND BEN-EZRA 1999]. 
Perspective projection forms the image by rays converging in a single viewpoint [Fig.2a]. In circular 
projection stereo, the image is formed by rays tangential to a viewing circle. The left-eye image is 
formed by clockwise tangential rays; the right-eye image is formed by counterclockwise tangential rays 
[Fig.2b]. This produces a stereo pair where the baseline is always perpen-dicular to the viewing direction, 
resulting in correct omnidirec-tional stereo. For real-time rendering, we approximate the circular projection 
stereo image by rendering connected slices of perspective stereo images. The number of slices is determined 
by the approximation error of the sliced perspective image compared to true circular projection. Our 
i-Cone panoramic curved screen display [SIMON AND GÖBEL 2001] uses four stereo channels for a combined 
240° field of view and 6000 pixel horizontal resolution [Fig.1].  Fig. 1. The 240º i-Cone Panoramic 
Display In this configuration, only four slices per channel (15º per slice) are required to produce less 
than half a pixel error between adja-cent slices. Because of view frustum culling, there is only moder-ate 
per slice overhead for transform limited rendering of wide (relative to polygon size) slices. Overhead 
for fill limited scenes is negligible. With four slices per channel, the total overhead for rendering 
on an SGI Onyx2 for a typical scene is less than 50%.  Fig. 2. Perspective (a) vs. Circular (b) Projection 
3 Applications With the omnidirectional stereo perspective, it is now possible to use, display permitting, 
the full 360º viewing circle as in the 19th century panorama. This allows for new paradigms of storytelling 
and user interaction. These are for example: surround scenarios with multiple points of interest, multiple 
story threads concurrently displayed - then, decisions on the story line can be based on the highest 
interest (position, view direction) in a thread among participants -, multiple users can simultaneously 
play several instances of a game. We have implemented an interactive music game with multiple concurrent 
sound generation stations distributed over the display.  At the same time, all participants can communicate 
with each other, move, look around, and browse the scene independently. References CRUZ-NEIRA, C., SANDIN, 
D. J., AND DEFANTI, T. A., 1993. Sur-roundscreen projection-based virtual reality: The Design and Im-plementation 
of the CAVE. Proc. SIGGRAPH '93: 135 142. OETTERMANN, S., 1997. The Panorama: History of a Mass Me-dium. 
New York: Zone Books. PELEG, S. AND BEN-EZRA, M., 1999. Stereo panorama with a sin-gle camera. CVPR '99: 
395-401. RADEMACHER, P. AND BISHOP, G., 1998. Multiple-Center-of-Projection Images. Proc. SIGGRAPH '98: 
199-206. SIMON, A., AND GÖBEL, M., 2002. The i-Cone A Panoramic Display System for Virtual Environments. 
Proc. Pacific Graphics '02: 3-7. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965546</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Interactive light field display from a cluster of projectors]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965546</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965546</url>
		<abstract>
			<par><![CDATA[We are developing a novel display system that physically realizes a sampling of the light field emitted by a three-dimensional scene. An array of projectors, each with a two-dimensional framebuffer, populates the 4D space of the light field. The view of the scene is simultaneously correct for all head positions within a volume. This eliminates the need for head tracking, produces binocular disparities without the need for glasses, and supports any number of viewers. Input to our display could be streamed from a light field sensor [1], or can be efficiently rendered in parallel using a cluster of standard computer graphics pipelines. For static scenes, no run-time rendering is necessary. We demonstrate the feasibility of the approach using a prototype cluster of projectors.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653920</person_id>
				<author_profile_id><![CDATA[81421597761]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steele]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP43118703</person_id>
				<author_profile_id><![CDATA[81452607405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jaynes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Kentucky]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LEVOY, M. AND HANRAHAN, P. Light Field Rendering. In Proceedings of SIGGRAPH 1996, ACM Press, Computer Graphics Proceedings, Annual Conference Series, ACM, pp. 31--42, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218490</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LUCENTE, M. AND GALYEAN, T. Rendering Interactive Holographic Images. In Proceedings of SIGGRAPH 1996, ACM Press, Computer Graphics Proceedings, Annual Conference Series, ACM, pp. 387--394.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Light Field Display from a Cluster of Projectors Matt Steele and Christopher Jaynes Dept. 
of Computer Science, University of Kentucky Abstract We are developing a novel display system that physically 
realizesa sampling of the light field emitted by a three-dimensional scene.An array of projectors, each 
with a two-dimensional framebuffer,populates the 4D space of the light field. The view of the scene is 
simultaneously correct for all head positions within a volume.This eliminates the need for head tracking, 
produces binoculardisparities without the need for glasses, and supports any numberof viewers. Input 
to our display could be streamed from a lightfield sensor [1], or can be efficiently rendered in parallel 
using acluster of standard computer graphics pipelines. For static scenes, no run-time rendering is necessary. 
We demonstrate the feasibility of the approach using a prototype cluster of projectors. 1 Introduction 
This work introduces a novel approach to interactive display thatis inspired by recent advances in capturing 
and rendering the lightfield [1]. The Light Field Display uses an array of digital lightprojectors to 
simulate the 4D light field emitted by a three­dimensional scene. Advantages of the approach over other 
interactive and immersive three-dimensional displays are significant. Tracking the user s head position 
is not required,binocular disparities are realized without additional hardware, andany number of viewers 
are supported. For static scenes, the light­field can be pre-computed and loaded onto the display without 
theneed for run-time rendering. Existing techniques, such as onesbased on holography [2] or lenticular 
images, are similar, although we are unaware of anyone previously using our methodology that emphasizes 
direct production of the light filedusing loosely configured projector clusters. Our current work focuses 
on 1) a demonstration of the feasibilityof the method including automatic and scalable calibration oflarge 
clusters of projectors and 2) low-cost approaches to that willeventually achieve high-resolution display. 
The reader is encouraged to visit webpage-removed for videos and more detail. 2 Technical Sketch The 
prototype system uses a 5x4 array of projectors that generate a static Light Field (see Figure 1a). Projectors 
are looselypositioned and oriented so that their frustums overlap in thevolume where the light field 
is to be constructed (Figure 1b,bottom). Light incident on any given point in the viewing volumeincludes 
rays originating from a single pixel in each projector sframe buffer. A user in the volume, looking back 
at the array,sees each projector s lens as the color of the framebuffer (pixel)ray that intersects the 
nodal point of her or his eye. As the usermoves to a different position, different rays from each projector,and 
therefore different framebuffer pixels, illuminate the eye.These images are 2D samples of the Light Field 
corresponding tothe three-dimensional scene being visualized. The contents of each projector s framebuffer 
needed to realize amodel s light field is determined via a projective map. We use anautomated, camera-based 
calibration technique to estimate theprojection matrix for each projector. These matrices map eachprojector 
ray into the given Light Field and are used to compute, in parallel, each projectors framebuffer contents 
(or directionmap) with a standard z-buffering render. (a) (b) Figure 1: (a) The 5x4 Light-Field Display. 
(b) Slices of the light field scattered by a planar surface. Top left clockwsise, the plane is moved 
away from projectors, at near distance, projector direction maps are visible, other offsets show rays 
converging and forming images of a three-dimensional scene.  Initial Results A three-dimensional scene 
consisting of two textured planes wasused to demonstrate feasibility (Figure 2a). A camera was movedto 
several locations in the light field to observe the three­dimensional model from that view (Figure 2b-d). 
Users also interact with the display by wearing sunglasses and interactivelylooking into, and moving 
through, the light field. (a) (b) (c) (d) Figure 2: Initial results. (a) A rendering of the test scene. 
(b-d) Three different views of the light field. (b) Looking over the front blue plane (c), A perspective 
view of the font plane and its green dot, and (d) looking below front plane to see a red cross on a white 
field. Our prototype display has a resolution of only 5x4, limiting thequality of the sampled field. 
Adding more projectors will help butis not cost effective. Improvements can be made by trading offdirectional 
resolution in each framebuffer to gain the effect of agreater spatial resolution. We expect to accomplish 
this usinglarge lens array that replicate each projector nodal point intopotentially hundreds of samples. 
 References [1] LEVOY, M. AND HANRAHAN, P.. Light Field Rendering. In Proceedings of SIGGRAPH 1996, ACM 
Press, Computer Graphics Proceedings, Annual Conference Series, ACM, pp. 31­42, 1996. [2] LUCENTE, M. 
AND GALYEAN, T. Rendering Interactive Holographic Images. . In Proceedings of SIGGRAPH 1996, ACM Press, 
Computer Graphics Proceedings, Annual ConferenceSeries, ACM, pp. 387-394. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965547</section_id>
		<sort_key>32</sort_key>
		<section_seq_no>32</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Physical interfaces]]></section_title>
		<section_page_from>32</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39051099</person_id>
				<author_profile_id><![CDATA[81100619792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Linda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lauro-Lazin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pratt Institute]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965550</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[The Sun Dagger Interactive]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965550</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965550</url>
		<abstract>
			<par><![CDATA[The Sun Dagger Interactive is a real-time computer simulation of the ancient site on Fajada Butte in Chaco Canyon, New Mexico, and is now part of a permanent exhibit on cultural astronomy at the Adler Planetarium in Chicago. The exhibit opened on March 20, 2002.The Sun Dagger construct is dated over a thousand years old and is believed to mark both the solar year and the nineteen-year lunar cycle. This new computer application allows viewers to explore the ancient site that is no longer accessible to the public.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14191420</person_id>
				<author_profile_id><![CDATA[81335496510]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Price]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland Baltimore County]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[SOFAER, A., AND SINCLAIR, R. M., 1983. Astronomical markings at Three Sites on Fajada Butte. Astronomy and Ceremony in the Prehistoric Southwest, Maxwell Museum of Anthropology, Anthropological Papers, No. 2, edited by John B. Calrson and W. James Judge, Albuquerque, NM: University of New Mexico, 1983.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SOFAER, A., ZINSER, V., AND SINCLAIR, R. M., 1979. A Unique Solar Marking Construct. Science, 19 October 1979, Volume 206, Number 4416, pp. 283--291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Sun Dagger Interactive Alan Price* University of Maryland Baltimore County Abstract The Sun Dagger 
Interactive is a real-time computer simulation ofthe ancient site on Fajada Butte in Chaco Canyon, New 
Mexico,and is now part of a permanent exhibit on cultural astronomy atthe Adler Planetarium in Chicago. 
The exhibit opened on March20, 2002. The Sun Dagger construct is dated over a thousand years old andis 
believed to mark both the solar year and the nineteen-year lunarcycle. This new computer application 
allows viewers to explorethe ancient site that is no longer accessible to the public. 1 Background and 
History Fajada Butte stands prominently in the south entrance of ChacoCanyon in New Mexico, rising 135 
meters above the valley floorto an elevation of 2018 meters. Near the top of this isolated butte,three 
large stone slabs collimate sunlight in vertical patterns oflight onto two spiral petroglyphs carved 
on the cliff behind them.The light illuminates the spirals each day near noon in a changingpattern throughout 
the year; marking the solstices and equinoxeswith particular images. Near noon at summer solstice, a 
narrowvertical form of light moves downward through the center of thelarger spiral. At the equinoxes 
and winter solstice, correspondingforms of light mark the spirals. The first modern observations at the 
site were made on June 29 1977 by Anna Sofaer of The Solstice Project, who then initiatedthe studies 
of the patterns of light on the spirals during the Sun sannual cycle. The computer simulation here shows 
how thecombination of the light patterns and the spirals were used by theancient people of Chaco to mark 
accurately the time of thesolstices and equinoxes. In the 1980 s possible vandalism occurred that caused 
one of thestone slabs to shift, resulting in shadow markings that no longeroperate in the way they had 
for a thousand years. The nationalpark service has since closed down access to the top of the butte. 
*e-mail: price@umbc.edu 2Technology and Process The interactive program displays on screen a 3D recreation 
of thestone slabs, petroglyphs and cliff wall making up the Sun Daggerconstruct. The geometry defining 
the stone slabs was derived fromphotogrammetry of the site produced on glass plate negatives in1979. 
A polygonal mesh was constructed for each stone slabbased on the point cloud data provide by the original 
combinationof the plates. A small area of the cliff wall behind the stones wasalso measured, but the 
rest of the cliff wall rising overhead wasmodeled roughly based on photographic reference. Astronomically 
accurate positioning of the sun s light casts realtime shadows of the stone slabs onto the petroglyph 
to recreate theactual events marked by the ancient calendar system. The viewercan explore these relationships 
by interacting with several gaugeson the screen. The user can drag an icon representing the Sunalong 
a slider and see the changing light and angle of the Sun sposition in real time. Navigation allows the 
viewer to move intoand around the cliff wall behind the three stone slabs. Figure 1. Screen capture 
of the Sun Dagger Interactive. References SOFAER, A., AND SINCLAIR, R. M., 1983. Astronomical markings 
at Three Sites on Fajada Butte. Astronomy and Ceremony in the Prehistoric Southwest, Maxwell Museum of 
Anthropology, Anthropological Papers, No. 2, edited by John B. Calrson and W. James Judge, Albuquerque, 
NM: University of New Mexico, 1983. SOFAER, A., ZINSER, V., AND SINCLAIR, R. M., 1979. A Unique Solar 
Marking Construct. Science, 19 October 1979, Volume 206, Number 4416, pp. 283-291. Copyright held by 
the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965551</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Virtual sculpting with a pressure sensitive pen]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965551</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965551</url>
		<abstract>
			<par><![CDATA[We present a new user interface for the virtual sculpting system. In our system, the user operates a chisel with a pressure sensitive pen to carve a workpiece in the virtual space. The user can control the depth of carving and the angle of the virtual chisel to the surface of the workpiece dynamically like the real wood carving.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP54031692</person_id>
				<author_profile_id><![CDATA[81547432556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizuno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653849</person_id>
				<author_profile_id><![CDATA[81350596422]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daigo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP36027705</person_id>
				<author_profile_id><![CDATA[81100385802]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Minoru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP54030286</person_id>
				<author_profile_id><![CDATA[81350590408]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jun-ichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Toriwaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP309473600</person_id>
				<author_profile_id><![CDATA[81545528656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toyohashi University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[MIZUNO, S., OKADA, M. AND TORIWAKI, J. 1999. An Interactive Designing System with Virtual Sculpting and Virtual Woodcut Printing, Computer Graphics Forum, 18(3), C183-C193, C409.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>842946</ref_obj_id>
				<ref_obj_pid>839291</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MIZUNO, S., OKADA, M., TORIWAKI, J. AND YAMAMOTO, S. 2002. Improvement of the Virtual Printing Scheme for Synthesizing Ukiyo-e, IN Proceedings of ICPR2002, Vol. 3, 1043--1046.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[MASSIE, T. H. AND SALISBURY, J. K. 1994. The Phantom Haptic Interface: A Device for Probing Virtual Objects, IN Proceedings of ASME International Mechanical Engineering Congress and Exhibition, 295--301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Sculpting with a Pressure Sensitive Pen Shinji Mizuno*1, Daigo Kobayashi*1, Minoru Okada*2, 
Jun-ichiro Toriwaki*3, and Shinji Yamamoto*1 *1 Toyohashi University of Technology,  *2 Waseda University, 
 *3 Chukyo University mizuno@cc.tut.ac.jp Abstract We present a new user interface for the virtual sculpting 
system. In our system, the user operates a chisel with a pressure sensitive pen to carve a workpiece 
in the virtual space. The user can control the depth of carving and the angle of the virtual chisel to 
the surface of the workpiece dynamically like the real wood carving. 1. Introduction We have developed 
the virtual sculpting system: an interactive model­ing technique to form a solid object with curved surfaces 
by carving a workpiece in the 3D space [Mizuno et al. 1999]. This system is useful to create artistic 
virtual sculptures or woodblocks for virtual Ukiyo-e printing [Mizuno et al. 2002]. In the former system, 
the user operated a chisel with a mouse, and the user must input the carving depth and the carving angle 
to the surface with the keyboard before carving. It was useful for industrial products, but it was different 
from real carv­ing operation and not so useful for artistic works. A 3D mouse or a pressure sensitive 
device are useful to solve such problems. SensAble s FreeForm system uses PHANToM [Massie and Salisbury 
1994] to make complex forms only by hand. We consider that a pressure sensitive pen and a display are 
most suitable for virtual wood carving with chis­els in our system, and develop a new user interface. 
The pressure of carving operation is converted to parameters of the carving depth and the carving angle 
dynamically like the carving in the real world. 2. Virtual Sculpting An original workpiece and several 
chisels are prepared in the system. The original workpiece has a simple shape with a CSG expression. 
Each virtual chisel has an ellipsoidal, cubic, or cylindrical shape. The user carves a virtual workpiece 
displayed on the screen directly: re­moving or attaching pieces of chisels from or to the workpiece, 
and then the workpiece is deformed at once. To deform the workpiece in real time, a list of intersecting 
points along with each viewing line on the screen is generated. The lists are revised each time when 
each carving operation is done or when the viewpoint is changed. Doing carving operations repeatedly, 
the user can form a complex shaped solid object. 3. The User Interface with a Pressure Sensitive Pen 
The position of a virtual chisel in the virtual 3D space is decided when the user drags a mouse on the 
virtual workpiece displayed in the screen (Fig. 1). One carving operation is done with one moving. The 
size of tangent plane pressure of carving p   Copyright held by the author the chisel, the point on 
the surface of the workpiece, and the rotation on the normal vector at the point can be obtained from 
the position and the direction of dragging. We develop a new user interface using a pressure sensitive 
pen and a display. Using them, the user can give the values of the carving depth: d, and the carving 
angle to the surface: . dynamically. The user drags on the display with the pen to carve the virtual 
workpiece. This is similar to that with a mouse. The pressure applied by the user is transferred to the 
carving depth and the carving angle to the surface (Fig. 2). The carving depth d is based on the aver­age 
of the pressure: pa , in one carving operation. d = r {1- (p- p) / p} , zmax amax rz: the height of the 
virtual chisel, pmax: the maximum sensitive pressure. The carving angle to the surface . is based on 
the average of the first half pressure: pf, and the latter half: pl , in the carving operation. .= tan- 
1{(d- d) / l}, d= r {1- (p- p) / p} , lf{f, l} zmax {f, l}max l: the length of the carving operation 
in the virtual space. To carve deeply, the user operates the pen with high pressure. The transition of 
the pressure during one carving operation decides the carv­ing angle to the surface. 4. Experiment and 
Conclusion We implemented the virtual sculpting system with a pressure sensitive pen and a display: Wacom 
PL-500 (Fig. 3). In this system, the user can create a virtual 3D object only with the pen as if carving 
it in the real world. The system became more useful for beginners of computer graphics, and sculptures 
and woodblock prints artists can use their carv­ing techniques in the system. We are developing a virtual 
printing system with the pressure sensitive pen. We are going to use our system for digital archives 
of sculptures and prints such as Japanese Ukiyo-e. This work is supported in part by the Hori Information 
Science Promo­tion Foundation.  References MIZUNO, S., OKADA, M. AND TORIWAKI, J. 1999. An Interactive 
Design­ing System with Virtual Sculpting and Virtual Woodcut Printing, Com­puter Graphics Forum, 18(3), 
C183-C193, C409. MIZUNO, S., OKADA, M., TORIWAKI, J. AND YAMAMOTO, S. 2002. Improve­ment of the Virtual 
Printing Scheme for Synthesizing Ukiyo-e, IN Pro­ceedings of ICPR2002, Vol. 3, 1043-1046. MASSIE, T. 
H. AND SALISBURY, J. K. 1994. The Phantom Haptic Interface: A Device for Probing Virtual Objects, IN 
Proceedings of ASME Inter­national Mechanical Engineering Congress and Exhibition, 295-301. dragging 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965549</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Canopy climb]]></title>
		<subtitle><![CDATA[a rope interface]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965549</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965549</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP61023255</person_id>
				<author_profile_id><![CDATA[81366590839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Winslow]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burleson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31046142</person_id>
				<author_profile_id><![CDATA[81100525482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ted]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[web.media.mit.edu/~win/tree.htm]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Resnick, Mitch, <u>Rethinking Learning in the Digital Age</u>. In The Global Information Technology Report: Readiness for the Networked World, edited by G. Kirkman. Oxford University Press. 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Isen, Alice; Some Neuropsychologyical Factors in Positive Affect, Social Behavior, and Cognition; <u>Handbook of Positive Psychology</u> by C. R. Snyder & S. Lopez 2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Semper, Robert. Science Museums as Environments for learning, Physics Today, vol 43, no. 11, pp. 50--56, Nov. 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Canopy Climb: A Rope Interface Winslow Burleson, Ted Selker MIT Media Lab Supporting Learning in New 
Ways Canopy Climb is a museum-style interactive, a tangible user interface, which physically couples 
a rope to a scroll mouse. It is a new interface to explore a projected rainforest tree. The tree is simply 
a Java Script Website, therefore users can substitute their own imagery, sound, or video. [1] Users have 
created journeys to the bottom of the ocean and to outer space. Mitchel Resnick states that, new technologies 
have the potential to fundamentally transform how and what people learn . while [they] make a learning 
revolution possible, they certainly do not guarantee it. [2] We set out to develop a tool to enable learning 
in new ways. This interface has been shown to several hundred visitors at the MIT Media Lab and has been 
installed in the Marie Selby Botanical Gardens and the New England Aquarium Activities Center. In a user 
study experiment visitors found the rope interface more natural and engaging, believed they would learn 
more and used it for twice as long as a scroll mouse interface to the same content. These results indicate 
that novel physical and virtual interfaces can have a significant impact on engagement and other elements 
of learning: peer interaction, positive affect and beliefs about learning. [3] Figure 1: Projected Rainforest 
Tree with Rope. Interface &#38; Construction Kit The user pulls on a rope which runs over the scroll 
mouse wheel causing the projected interface to move up and down. When they step on the log the scroll 
mouse button is clicked presenting a web page about the animals that live in that layer of the tree. 
Stepping off the log returns them to the tree for further exploration. We have posted a construction 
kit online describing how to make a Log and load vertical interface. We have been able to set up the 
system in less than 15 minutes. All one must do is arrange to hang the rope and secure the log to the 
floor, plug in the scroll mouse, project the website, and invite users to enjoy the interface. -------------------------------------------- 
*e-mail: win@media.mit.edu , verp@alum.mit.edu . User Experiences A second-grade class loved the canopy 
climb exhibit. A large number of children crowded around telling each other where to climb and what animals 
they wanted to see, pulling on the ropes and clicking. Clearly the exhibit provided an exciting way for 
the kids to explore the rainforest environment; their teachers had to drag them away from the exhibit. 
The rope interface fosters peer interaction, which has been shown to be beneficial in the learning process, 
particularly in museums. [4] The extended continuous vertical design of the rope interface is also useful 
for accessibility as it can be reached at any height. Users ranging from children who are two feet tall 
on up to individuals who use wheel chairs to tall adults all have interacted with the rope interface 
at a height that is ideally suited for them.  Figure 3: Images from Online Construction Kit.  References 
1. web.media.mit.edu/~win/tree.htm 2. Resnick, Mitch, Rethinking Learning in the Digital Age. In The 
Global Information Technology Report: Readiness for the Networked World, edited by G. Kirkman. Oxford 
University Press. 2002. 3. Isen, Alice; Some Neuropsychologyical Factors in Positive Affect, Social 
Behavior, and Cognition; Handbook of Positive Psychology by C.R. Snyder &#38; S. Lopez 2002 4. Semper, 
Robert. Science Museums as Environments for learning, Physics Today, vol 43, no. 11, pp.50-56, Nov. 1990. 
 Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965548</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[&#230;ther]]></title>
		<subtitle><![CDATA[reading with tactile vision]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965548</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965548</url>
		<abstract>
			<par><![CDATA[<i>&#230;ther</i> is an interactive, haptic surface for computer mediated visual information that allows for a physical experience of textual analogous to the visual experience of whitespace in a poem.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653867</person_id>
				<author_profile_id><![CDATA[81100033827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Conrad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lecturer, University of Maryland Baltimore County]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CALVINO, I., trans. WEAVER, W. 1986. Under the Jaguar Sun. Harvest, New York]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ONG, WALTER J. 1962. Orality and Literacy, Rutledge, London.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 æther: reading with tactile vision Erik Conrad Lecturer, University of Maryland Baltimore County Abstract 
æther is an interactive, haptic surface for computer mediated visual information that allows for a physical 
experience of textual analogous to the visual experience of whitespace in a poem. 1 Introduction Technologies 
are not mere exterior aids, but also interior trans­formations of consciousness, and never more than 
when they affect the word. 1 Walter Ong æther is an experiment in how new technologies of representa­tion 
affect human perception. It acts as a haptic surface for com­puter mediated visual information to enable 
tangible experiences common to painting and sculpture, but rare in digital media. Unlike Text Rain by 
Camille Utterback and Romy Achituv (SIG-GRAPH Art Gallery 2000), physical interaction with the words 
provides information about the formal structure of an entire text, or text selection. The experience 
is more than the ability to visu­ally manipulate intangible words; it allows one to feel out a shape 
that and entire text embodies. For example, touching a narrative thread, one can see it emerge from a 
sea of text and trace its path throughout the entire narrative. Similar to research done by the Research 
in Experimental Documents (RED) group at Xerox Parc, eXperiments in the Future of Reading (XFR), æther 
shows how physical form and content can be used to create rich reading environments. As an alternative 
interface to the printed word, it has the potential to shed light about the ways in which people read 
and thus, think. Unlike XFR, its goal is pri­marily expressive to combine body knowledge and visual knowl­edge 
to allow readers/writers to think about how reading and writing can inhabit multi-dimensional spaces, 
how texts feel , and how they can be navigated by touch. 2 Exposition The desire to visualize narrative 
structure was inspired, in part, by Italo Calvino s novel, If on a Winter s Night a Traveler. Housed 
within its covers are the beginnings to ten different nov­els, each with different characters and written 
in different styles. The novel s brilliance lies in its frame. The frame, allows the picture to exist, 
isolating it from the rest, but at the same time it recalls and somehow stands for everything that remains 
out of the picture. 2 æther began as attempt at visualizing the frame, but in a larger sense, it is a 
study in the phenomenology of vision. Learning to read is often the only visual training that one seriously 
under­takes in their lifetime. Other visual methodologies are tacitly learned and internalized with repeated 
exposure. Learned vision affects the way that people think that they see, and thus the ways in which 
they are capable of seeing. By immersing the experi­ence of reading into a more tactile visual space, 
different ways of seeing are allowed to dynamically combine and separate, giving rise to new forms of 
visual expression and reception. The ways of seeing that æther tries to illicit are those common to painting 
and sculpture. Contemplating a painting is not purely visual, it comprises underlying impressions as 
well. The power of works like Joseph Turner's Snowstorm (Figure 2) does not lie in an ability to be a 
transparent window into a scene, but in its physicality as a tactile surface united with form and color. 
æther creates a space for the sensory exploration of the frame, by pro­viding a surface for textual narrative 
through the interactive visu­alization of narrative structures. Figure 2. Joseph Turner, Snowstorm, 
1842, Tate Britain, Clore Gallery for the Turner Collection, London  References CALVINO, I., trans. 
WEAVER, W. 1986. Under the Jaguar Sun. Harvest, New York ONG, WALTER J. 1962. Orality and Literacy, Rutledge, 
London. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965552</section_id>
		<sort_key>33</sort_key>
		<section_seq_no>33</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Street tech]]></section_title>
		<section_page_from>33</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP42061526</person_id>
				<author_profile_id><![CDATA[81341497557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tribe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rhizome.org]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965553</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[WATCH]]></title>
		<subtitle><![CDATA[summary data in spatial context]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965553</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965553</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653940</person_id>
				<author_profile_id><![CDATA[81100266005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rebecca]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ASYMPTOTE. 1998. Virtual New York Stock Exchange.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BUREAU OF INVERSE TECHNOLOGY. 2002. Feral Robot {FRE}]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WATCH: Summary Data in Spatial Context Rebecca Ross Center for Advanced Technology*, New York University 
 Introduction WATCH is a wristwatch that cross-references the wearer's position with a small set of environmental, 
economic and social statistics about their location within the United States. The idea for this project 
emerged after spending some time on Staten Island's Fresh Kills Landfill, now a completely treeless span 
of brown dirt, and noting the US Geological Survey's perfectly accurate classification of the land as 
Deciduous Forest, based on rainfall. WATCH provides a means for wearers to immediately juxtapose the 
way in which they experience the world through statistical summaries with an understanding they derive 
in person with eyes, ears and hands. Rationale The purpose of this project is not to discount the usefulness 
of statistical summary data nor is it to give wearers any particular opportunities to dispute the veracity 
of data, though these are both potential readings, depending on the inclinations of wearers. More specifically, 
the goal is to wrestle with the benefits and limits of quantitative summaries of human experience by 
restoring their relationship in space with the circumstances they describe. This is especially important 
within the context of a culture that relies heavily on statistical summary in decision-making. Still 
from video sketch at http://cat.nyu.edu/~rebecca/watch How WATCH works WATCH uses GPS, Global Positioning 
System, to pinpoint its latitude and longitude whenever it has a clear relationship to positioning satellites 
(usually outdoors). A database stored on WATCH supports a conversion from latitude/longitude to US Postal 
Service zip code. A second database contains location specific data for the entire country keyed by zip 
code. 32 megabytes of storage will accommodate overhead plus approximately 10 pieces of moderately precise 
numeric data for every zip code within the United States. *719 Broadway, 12th Floor, New York, NY 10003 
rebecca@cat.nyu.edu, http://cat.nyu.edu/~rebecca The data categories are constant across space each data 
category, such as median family income, will be available throughout the country. The user interface 
to this data set is extremely straightforward. Access to data associated with particular zip codes is 
offered through travel of any kind, such as walking or driving. A single button supports simple vertical 
navigation through the categories of data as well as an interface for setting the time. One category 
of data is visible on the watch face at any given time. WATCH Data An initial data set is being compiled 
from various sources including the 2000 US Census, web sites of corporations including McDonald's and 
non-profit organizations such as scorecard.org this is a draft of the set of ten statistics which would 
be displayed on WATCH: Avg. Household Income HS Graduation Rate Carbon Dioxide Level Average Jan. Temperature 
Drug Overdoses / Annum Annual Rainfall AIDS cases / 1000 % Institutionalized Number of McDonalds % School 
Lunch Continuing Research A prototype of WATCH is being built on a Pocket PC handheld computer attached 
via a serial compact flash card to a self­powered GPS unit. We are investigating the use of alternative 
methods for sensing location, such as GSM positioning or triangulation using known locations of nearby 
wireless transmitters. The specific technology used is not as relevant to this research as its application. 
There already exist products on the market that incorporate GPS within a wristwatch at reasonable cost. 
The next step for this research will be to consider means of maintaining and varying the data set over 
time. Storing data on removable media and providing infrastructure for syncing with a network would provide 
opportunities for matching data sets to particular interests (e.g. historical, environmental or sociological) 
as well as ensuring relative currency. We are particularly interested in the possibility of providing 
WATCH as an interface to decentralized data sets, managed over the network by wide­scale communities 
collaboratively. References ASYMPTOTE. 1998. Virtual New York Stock Exchange. BUREAU OF INVERSE TECHNOLOGY. 
2002. Feral Robot [FRE] Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965556</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Augmented reality disk jockey]]></title>
		<subtitle><![CDATA[AR/DJ]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965556</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965556</url>
		<abstract>
			<par><![CDATA[This technical sketch describes an application for 3D sound using intuitive augmented reality interface. The AR/DJ was designed for the use in todays clubs, allowing a DJ to use the capabilities of 3D sound in such an environment, with the demands of being easy to use, enabling collaborative use and generating accurate and inter- active 3D sound.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3D-Sound]]></kw>
			<kw><![CDATA[DJ]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.2.6</cat_node>
				<descriptor>Integrated environments</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>PP28017538</person_id>
				<author_profile_id><![CDATA[81328490619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stampfl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Imagination Computer Services GmbH - Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Augmented Reality Disk Jockey -AR/DJ Philipp Stamp.* Imagination Computer Services GmbH -Research Abstract 
This technical sketch describes an application for 3D sound using intuitive augmented reality interface. 
The AR/DJ was designed for the use in todays clubs, allowing a DJ to use the capabilities of 3D sound 
in such an environment, with the demands of being easy to use, enabling collaborative use and generating 
accurate and inter­active 3D sound. CR Categories: D.2.6 [SOFTWARE ENGINEERING]: Program­ming Environments 
Integrated environments Keywords: DJ, 3D-Sound, Augmented Reality 1 Introduction The AR/DJ (Augmented 
Reality Disk Jockey) is a tool for 3D music creation in clubs and discotheques. Additionally to regular 
sound mixing features, it allows the DJ to play dozens of sound samples that can be placed anywhere in 
3D space in the club. This offers a totally new dimension for music creation, as he can not only use 
different rhythms and light to create a club atmosphere but he can also play with the third dimension 
(position) of each sound on the dance .oor. The DJ is also able to directly access common effects like 
compression, .anger, reverb and many more for each sound source, which are calculated in real time. We 
use augmented reality to place and visualize sounds (music) in 3D space. A 3D model of the dance .oor 
is used as reference, to enable an easier positioning and better orientation in the room. 2 Exposition 
The AR/DJ system consists of two parts. The sound engine (called 3deSoundBox) is able to load and render 
sound samples facilitat­ing various effects. The Augmented Reality interface (called DJ-Vision) allows 
the DJ to visualizes sound sources in a virtual 3D model of the dance .oor. We use an Intersense pen 
tracker as po­sitioning device for the DJ and a keyboard to switch between dif­ferent sound samples and 
directly access the effects. With this pen the DJ can drag and drop sound sources within the 3D model 
of the dance .oor. This method offers an easy to use and save way to place sounds in the room. Within 
the model he can see the sounds which are directly related to the positions in the real world. He is 
also able to move sounds very fast and with very small hand move­ment, avoiding fatigue of the DJs arms. 
A fact that is really impor­tant if the system is used for a longer period. The sound engine is called 
3deSoundBox. This is an external device, that does the load­ing of the sound samples and the distribution 
of the different sound sources onto the speakers according their position in space. The 3deSoundBox can 
calculate an output for up to 7.1 speakers via dig­ital or analog output. One of these outputs can easily 
be connected to common club sound systems. Each output has to be connected to a dedicated speaker set, 
ideally one in front, one on the left, one on the right, one in the middle left, one in the middle right, 
one in *e-mail: phil@imagination.at the back left and one in the back right of the dance .oor. Another 
nice feature, is that the 3deSoundBoxes are stack able. That means more of these SoundBoxes can work 
together to generate more out­put channels. We use this feature to produce two versions of the same virtual 
space for the DJ. One space is 1:1 related to the real dance .oor. Within this virtual space a direct 
manipulation such as correcting the position, effects, amplitudes of sounds are directly rendered on 
the dance .oors sound system. The other virtual space is an exact copy of the geometry but might contain 
a different set of sounds for pre-listening and pre-mixing. It is mainly used to en­able a possibility 
of 3D-sound pre-mixing which can be put on air by simply fading back and forth between these two spaces. 
The AR/DJ also allows two DJs working with the system at the same time, both using a pen. One DJ could 
move sound sources in the live environment, while the other one is pre-mixing some effects, or they both 
work together within one environment at the same time. Figure 1: Using 2 SoundBoxes, one for pre-mixing, 
one for live mixing. 3 Conclusion The AR/DJ offers a number of new possibilities in the .eld of music 
composition in combination with three dimensional and multichan­nel sounds. Our simple and intuitive 
augmented reality interface offers an accurate visualization combined with an easy positioning tool and 
even offers the possibility of a collaborate use. The AR/DJ can also be used for sound artists to create 
highly dynamic sound spaces as well as for athmo-sound creation for movies. It offers a good mix between 
the standard usage of multiple sound samples like looping, mixing and effecting as well as advanced features 
like the 3D positioning in conjunction with a good usability and a sim­ple and intuitive handling. Our 
general aim is to facilitate access to the 3D sound technology by generating utilities that enable an 
easy use for everybody in our every days life. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965555</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The Bass-Station]]></title>
		<subtitle><![CDATA[a community based information space]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965555</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965555</url>
		<abstract>
			<par><![CDATA[The Bass-Station is a novel mobile device that allows people to easily share their artistic digital belongings with their immediate physical community. Building on currently available technology, incorporated into an everyday object, we create an information space that affords people of an artistic and collaborative community the ability to share and exchange their personal media. The Bass-Station facilitates communication and sharing between people in a local community, making use of technology as a medium for developing relationships and new artistic forms.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653817</person_id>
				<author_profile_id><![CDATA[81100336086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ahmi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wolf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653917</person_id>
				<author_profile_id><![CDATA[81100013067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Argo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[New York University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[GIBSON, J. 1979. The Ecological Approach to Visual Perception. New York: Houghton Mifflin, 1979.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[POCKETCALCULATORSHOW.COM 2003. The Boombox Museum. http://pocketcalculatorshow.com/boombox.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[REDSTR&#214;M, J., et al. 1995. Ad Hoc Information Spaces. http://dahlberg.se/pub/adhoc-final.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[WEISER, M., BROWN, J. S. 1996. The Coming Age of Calm Tech. http://www.ubiq.com/hypertext/weiser/acmfuture2endnote.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Bass-Station: A Community Based Information Space Ahmi Wolf and Mark Argo Interactive Telecommunications 
Program - New York University {aw596, ma848}@nyu.edu Abstract The Bass-Station is a novel mobile device 
that allows people toeasily share their artistic digital belongings with their immediatephysical community. 
Building on currently available technology,incorporated into an everyday object, we create an information 
space that affords people of an artistic and collaborative community the ability to share and exchange 
their personal media.The Bass-Station facilitates communication and sharing betweenpeople in a local 
community, making use of technology as amedium for developing relationships and new artistic forms. 1 
Introduction In the early 1980 s the ghettoblaster was introduced into the urbanculture and instantly 
became a symbol of mobility, fresh artisticcollaboration, and spontaneous community. This new technology, 
-no longer tethered to a specific environment was an essentialfactor in the development of new communities 
and relationshipsthat spawned revolutionary art forms. Twenty years later, we find ourselves surrounded 
by technologiesthat afford us the ability to be mobile, communicate, and interactwith other people like 
never before. These technologies make themost out of a temporally and geographically displaced communityallowing 
us to communicate and share information with people across the globe. While our technologies bridge displacedcommunities 
they create an unnecessary gap between those thatare in our immediate vicinity. This gap is most noticeable 
whenwe try to share our digital artifacts with those that are physically near us. In order to promote 
the development of relationships, potentialcollaborations, and new artistic endeavors, we see it essential 
to be able to easily share our digital belongings with people in ourimmediate community. Additionally, 
mobility and wirelesscommunications enables the creation of spontaneous communitiesallowing peoples artworks 
to proliferate across territories evolving new styles and forms. The Bass-Station creates a mobile information 
space that enablesthe exchange of digital media and contains services for thedissemination of community 
information. Embodied in a familiarform that represents many aspects of community-based interaction -the 
boombox - we integrate today s technologies to allow peopleto share interests, learn about others, exchange 
digital artworks,and listen to music. By facilitating such activities, the Bass-Station endeavors towards 
the development of artistic relationships, collaborations, and perhaps new art forms. 2 Exposition 
The Bass-Station is comprised of an original 1980 s boombox thatimbeds off the shelf hardware, freely 
available software, andcustom built elements. The heart of the system is a single boardcomputer with 
a wireless network interface. A large hard-driveprovides storage for the community s media, a custom 
built mp3 decoder interfaces with the stereo s speakers, and varioussoftware services, some custom built, 
are integrated into thesystem. The Bass-Station will be fully mobile, being able to bepowered from rechargeable 
batteries that fit into the ghettoblaster.  The single board computer functions as a wireless access-pointwhich 
is not itself associated to any other network. It runs all thenecessary services allowing wireless clients 
in range to associatewith it. The fact that the Bass-Station uses networkingtechnologies yet does not 
provide Internet access is a uniquefeature. In this manner a local community-based informationspace is 
created. The Bass-Station hosts a variety of services such as audio playback via the stereo s speakers, 
file sharing, personal filestorage, a chat room, and a discussion forum. All of these servicesare accessed 
via a web browser - the main logical interface to thedevice. These are the services that allow people 
in the communityto easily share their artistic media and personal artifacts. The physical interface of 
the Bass-Station contains a collection ofknobs, buttons, and LED displays. Some of these knobs andbuttons 
are controls for volume and song playback. The LEDdisplays give visual feedback for activity such as 
current networkload or amount of available disk space. The Bass-Station will be introduced into our academic 
communityin mid to late April and will begin to function as a physical part ofour social and academic 
environment.  References GIBSON, J. 1979. The Ecological Approach to Visual Perception.New York: Houghton 
Mifflin, 1979. POCKETCALCULATORSHOW.COM 2003. The Boombox Museum. http://pocketcalculatorshow.com/boombox. 
REDSTRÖM, J., et al. 1995. Ad Hoc Information Spaces.http://dahlberg.se/pub/adhoc-final.pdf. WEISER, 
M., BROWN, J.S. 1996. The Coming Age of Calm Tech.http://www.ubiq.com/hypertext/weiser/acmfuture2endnote.htm. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965554</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[The ViscousDisplay]]></title>
		<subtitle><![CDATA[adaptive transient interfaces in public space]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965554</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965554</url>
		<abstract>
			<par><![CDATA[The ViscousDisplay explores the exchange of social information through transient public interfaces. Shaped by principles of 'underground public art', the ViscousDisplay is conceived as a novel mobile communication medium, where messages can be shared in public spaces. Inspired by biological learning systems; the ViscousDisplay learns gestural motions and colors that form along traces of a participant's movements and maps this information onto a flexible display. Because it is made up of inexpensive materials, the ViscousDisplay is also a disposable artifact which may be collected in public spaces. It combines multi-modal sensing, learning algorithms, and a pliable silicone display.]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA['underground public art']]></kw>
			<kw><![CDATA[gesture recognition]]></kw>
			<kw><![CDATA[interpersonal communication]]></kw>
			<kw><![CDATA[learning algorithm]]></kw>
			<kw><![CDATA[tangible user interface]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653910</person_id>
				<author_profile_id><![CDATA[81100609080]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lily]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirvanee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT, Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP30028746</person_id>
				<author_profile_id><![CDATA[81100369938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Downie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT, Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>728869</ref_obj_id>
				<ref_obj_pid>647592</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Benbasat, A. Y. and Paradiso, J. A ., "An Inertial Measurement Framework for Gesture Recognition and Applications" LNAI 2298, Springer-Verlag, 2002. pp. 9--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fritzke, B., "A Growing Neural Gas Network Learns Topologies." Advances in Neural Information Processing Systems 7, 1995, ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lefebvre, H., The Production of Space, trans. by Donald Nicholson-Smith, Oxford: Blackwell, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sticker Shock: Artists' Stickers; Exhibition at Institute for Contemporary Art, University of Pennsylvania, January 15-March 7, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The ViscousDisplay: Adaptive Transient Interfaces in Public Space Lily Shirvanee* . Marc Downie MIT, 
Media Lab MIT, Media Lab  Abstract The ViscousDisplay explores the exchange of social informationthrough 
transient public interfaces. Shaped by principles of underground public art , the ViscousDisplay is conceived 
as anovel mobile communication medium, where messages can beshared in public spaces. Inspired by biological 
learning systems;the ViscousDisplay learns gestural motions and colors that formalong traces of a participant 
s movements and maps this information onto a flexible display. Because it is made up ofinexpensive materials, 
the ViscousDisplay is also a disposableartifact which may be collected in public spaces. It combinesmulti-modal 
sensing, learning algorithms, and a pliable siliconedisplay. Keywords Interpersonal communication, tangible 
user interface, learningalgorithm, gesture recognition, underground public art . 1 Introduction: Social 
Histories In The Production of Space, Henri Lefebvre describes space as a social phenomenon where history 
accounts for the interrelationships of spaces and their links with social practice [3]. He argues that 
the production of space is grounded ininherent conditions, where traces of social existence are forever 
creating our histories and our perception of space. The ViscousDisplay is an interactive device with 
which we areexploring this exchange of social information through transientpublic interfaces. Shaped 
by principles of so-called undergroundpublic art [4], the ViscousDisplay attempts to similarly occupyand 
engage shared spaces. When left in public spaces,ViscousDisplays are conceived as a communication medium 
between people sharing these spaces, where messages and signalsspanning the continuum between private 
and shared informationcan be sited. 2 Implementation: Environmental Traces We have developed ViscousDisplays 
that are embedded withlearning algorithms which sample gestural motions and colorsforming along the traces 
of a participant s movements. Abstractedvisual and gestural information are taken from her/hisenvironment 
and mapped onto its flexible, viscous display. As the user samples colors from her/his environment with 
an attachedcamera, the ViscousDisplay s dynamically responsive, fabric-likeinterface unfolds visually 
represented messages. After an initialtraining process, predictable signals develop from unpredictableenvironments 
enabled by the adaptive and temporal behavior ofthe computational system. The evolving characteristic 
of these *e-mail: lilys@media.mit.edu e-mail: marcd@media.mit.edu embedded learning algorithms permits 
the robust transmission of complex environmental messages to this fairly simple computational device. 
The malleable, sticky quality of the ViscousDisplay enablesparticipants to attach it to many objects 
in the environment. Whenused in public spaces, this interface can become a tactile transmitter of social 
histories: a publicly retrievable dialogue,engaging users across an informational space to relay and 
retrievesymbolic messages of another s experience. 3 Physical and Computational Design The LED based 
display is made of a copper mesh and flexiblesilicone encasement to enable sensing, folding, and manipulationof 
this malleable display. Small surface mount LEDs are fixed to acopper mesh to create a full-colored diffuse 
display. Color andgesture information is captured by three photo diodes and a compact inertial measurement 
unit [1] that are placed inside anoptically clear spherical object, woven via flexible wires to thecenter 
of one side of the display. The training procedure is facilitated by an atomic gesture analysisalgorithm, 
based on a neural gas model [2], that measures gesturalmotion and color information sampled by the participant. 
Thealgorithmic processes perpetually receive sensing data, learngestural paths, and adapt to the environmental 
information offeredby the user. 4 Influences and Related Work The ViscousDisplay was inspired by the 
transient and socialqualities of underground public art and iconic sticker art [4]that attempt to encode 
shared spaces and engage a publicdialogue. References 1.Benbasat, A.Y. and Paradiso, J.A., An Inertial 
Measurement Framework for Gesture Recognition and Applications LNAI 2298, Springer-Verlag, 2002. pp. 
9-20. 2.Fritzke, B., A Growing Neural Gas Network Learns Topologies. Advances in Neural Information Processing 
Systems 7, 1995, ACM Press. 3.Lefebvre, H., The Production of Space, trans. by Donald Nicholson-Smith, 
Oxford: Blackwell, 1991. 4.Sticker Shock: Artists' Stickers; Exhibition at Institute for Contemporary 
Art, University of Pennsylvania, January 15­ March7,1999. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965557</section_id>
		<sort_key>34</sort_key>
		<section_seq_no>34</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Fluids & level sets]]></section_title>
		<section_page_from>34</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP14034439</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Trinity College Dublin]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965560</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A fast polymesh to level set algorithm]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965560</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965560</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653877</person_id>
				<author_profile_id><![CDATA[81100392996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[F&#228;lt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027386</person_id>
				<author_profile_id><![CDATA[81100308346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[BREEN, D. E., MAUCH, S., WHITAKER, R. T., AND MAO, J. 2001. 3d metamorphosis between different types of geometric models. Proceedings of Eurographics 2001, 36--48.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[DAVIS, J., MARSCHNER, S., GARR, M., AND LEVOY, M. 2002. Filling holes in complex surfaces using volumetric diffusion. First International Symposium on 3D Data Processing, Visualization, Transmission.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[SETHIAN, J. A. 1996. A fast marching level set method for monotonically advancing fronts. Proc. Nat. Acad. Sci 94, 4, 159--1595.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Fast Polymesh to Level Set Algorithm Henrik F¨Douglas Roble alt * Norrk¨oping Visualization and Interaction 
Studio, Digital Domain Link¨oping University, Sweden 1 Introduction Level sets are a powerful tool which 
can be used in a wide variety of applications, including .uid simulation. In order to transition from 
a polymesh paradigm, a rapid method of creating level sets from polygonal models is necessary. We present 
a new approach to rapidly transform triangular polymeshes to level sets (signed dis­tance .elds) on a 
three dimensional grid. 2 Method A grid point in a level set contains the distance to the closest point 
on the surface of all objects. A naive approach to computing this distance is to compare each grid point 
with every polygonal sur­face. Additionally, inside/outside checks are required to determine the sign 
of the distance. [Breen et al. 2001] developed a scan­conversion technique that relied on a Voronoi space 
partition based on the features of the polygonal objects. We developed a scheme that uses a simple bounding 
box and space partitioning cone. Computation of values within a narrow band around the surface consists 
of two steps: pre-processing of geometric information and distance splatting. Further, we provide the 
option to produce completely populated signed distance .elds using a high order Fast Marching Method 
[Sethian 1996]. Our approach is to minimize the number of triangles that are compared with each grid 
point by building bounding geometries for each triangle in a pre-processing step. Because this is a local 
approach, it is sensitive to errors in the geometry. Self-intersecting objects and incorrect normals 
will cause artifacts. However holes can be .lled using Volumetric Diffusion [Davis et al. 2002].  Figure 
1: 2D Illustration of bounding cones (dashed lines). 2.1 Pre-Processing of Geometrical Information First, 
for each triangle, we construct an axis aligned bounding box and expand it by the width of the narrow-band. 
Problems arise in the area around the corners of the object. Grid points near this area will .nd that 
the closest point on the object is at the corner, which is shared by many triangles. Each of these triangles 
may return a different value for the inside/outside test and ambiguities arise. To solve this, a second, 
cone-like bounding volume is created. See Figure 1. It is de.ned by three planes, where each plane is 
*e-mail: henfa459@student.liu.se e-mail: doug@d2.com de.ned by the edge of the triangle and the average 
normal of the edge s shared triangles. These cones are unambiguous and faster to compute than a Voronoi 
diagram. 2.2 Distance Splatting Algorithm All grid-points in the sub-grid de.ned by the bounding box 
are tra­versed. For each grid-point, the we check if the point is inside the cone. If it is inside, we 
minimize a parametric distance function to .nd the distance between the grid-point and the triangle. 
The sign of the distance is determined by looking at the normal of the trian­gle. Since, in some cases, 
the bounding volumes of the triangles are overlapping, some grid-points get their distance values calcu­lated 
multiple times. The algorithm always chooses the distance values with smallest magnitude. It ignores 
distances that are larger than the width of the narrow band. Figure 2: Original mesh (left) and level 
set reconstruction (right).  3 Example and Evaluation The algorithm is conceptually simple and easy 
to implement. In our experience, the time performance of producing a narrow-banded level set behaves 
almost linearly with respect to number of trian­gles. It is currently used in Digital Domain s proprietary 
.uid sim­ulation tool and has proven to be robust and fast. References BREEN, D. E., MAUCH, S., WHITAKER, 
R. T., AND MAO, J. 2001. 3d metamorphosis between different types of geometric models. Proceedings of 
Eurographics 2001, 36 48. DAVIS, J., MARSCHNER, S., GARR, M., AND LEVOY, M. 2002. Filling holes in complex 
surfaces using volumetric diffusion. First International Symposium on 3D Data Processing, Visual­ization, 
Transmission. SETHIAN, J. A. 1996. A fast marching level set method for mono­tonically advancing fronts. 
Proc. Nat. Acad. Sci 94, 4, 159 1595. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965561</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A unified approach for modeling complex occlusions in fluid simulations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965561</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965561</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14187903</person_id>
				<author_profile_id><![CDATA[81320490566]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Houston]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Frantic Films]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653840</person_id>
				<author_profile_id><![CDATA[81332490465]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Frantic Films]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653919</person_id>
				<author_profile_id><![CDATA[81100003111]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wiebe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Frantic Films]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566645</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ENRIGHT, D., MARSCHNER, S. AND FEDKIW, R. Animation and Rendering of Complex Water Surfaces. ACM Trans. On Graphics (SIGGRAPH 2002 Proceedings) 21, 736--744, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N. AND FEDKIW, R. Practical Animation of Liquids. SIGGRAPH 2001 Annual Conference, 12--22, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>244315</ref_obj_id>
				<ref_obj_pid>244304</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND METAXAS, D. Realistic Animation of Liquids. Graphical Models and Image Processing 58, 471--483, 1996.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Unified Approach for Modeling Complex Occlusions in Fluid Simulations Ben Houston Chris Bond Mark 
Wiebe Frantic Films Frantic Films Frantic Films bhouston@franticfilms.com cbond@franticfilms.com mwiebe@franticfilms.com 
 1 Introduction A few recent papers, [Enright 2002; Foster 2001; Foster 1996], have described the development 
of a fluid simulation method useful for computer graphics. We have implemented a system based on these 
papers, and propose an extension that augments and simplifies the handling of occlusions (impermeable 
solid objects). In the referenced papers, occlusions are treated differently based on whether or not 
they are moving, and discussion of how to deal with complex occlusions efficiently is minimal. The technique 
we have developed unifies the treatment of static and dynamic objects, and is able to better represent 
fluid-occlusion interactions at the low simulation resolutions used. 2 Unified Occlusion Representation 
In order to unify all of the occlusions affecting the simulation, we represent their geometry using a 
level set instead of relying on a polygon mesh directly (as was done in [Enright 2002; Foster 2001; Foster 
1996].) A level set alone can only capture the instantaneous geometry of the occlusions, thus it cannot 
model the effects of the occlusion velocities and slip conditions. We store this information in two addition 
fields a vector field for the velocities and a scalar field for the slip conditions. The unified occlusions 
representation, .(x), is thus composed of a level set, fx (), occlusion velocity field, u(x), and slip 
condition field, a(x) such that: .()={() () ()ux ,ax } x fx , The global occlusions representation is 
the union of all the individual occlusions. For our purposes the union operator is defined as: .. ()x 
if f()x = () A AB.AUB (). f x x = ..B ()x otherwise This union operator, since it is both associative 
and distributive, allows considerable flexibility in the calculation and partial caching of the global 
occlusions model.  3 Creating Occlusions Representations The three fields that represent occlusions 
in our approach often need to be generated from explicit geometry representations. The following method 
is used to compute the set of fields for one occlusion. 3.1 Level Set. Of the three fields, the generation 
of the level set is the most complex. Our method, which is both efficient and robust, consists of the 
following steps: (1) Explicit geometry is converted into a structure that allows for efficient calculation 
of ray intersections. (2) Rays are cast along a number of axis directions, recording data indicating 
whether the center of a voxel is inside or outside, and an initial distance from the surface. This data 
is then resolved into a rough distance function. (3) Discontinuities near the zero-level set (which can 
result from imperfections such as gaps in the geometry) are removed. (4) The level set is then smoothed 
via a standard re-initialization algorithm. 3.2 Velocity Field. Our calculation of the velocity field 
is straightforward. Each occlusion is associated with a motion path expressible as a function mapping 
time t to a transformation matrix, T(t). The velocity field is obtained by taking the product of the 
position and the component-wise derivative of this motion path: u(x,t)= (T (t))x. In the case of a deforming 
object, a more sophisticated approach must be taken. 3.3 Slip Field. We use a single value for the slip 
condition of each occlusion, so the slip field is a constant.  4 Incorporation into Fluid Simulation 
The occlusions are relevant to two major aspects of the fluid solver, the mass conservation calculation 
and the advection velocity field. 4.1 Mass Conservation. Incompressibility is enforced by solving the 
equation .·u =.2p for pressure, p. During the calculation of .·u , vector occlusion velocities are used 
instead of fluid velocities in regions interior to the occlusions. The occlusions are incorporated into 
the Poisson matrix (which represents the Laplacian operator) by omitting cells that lie completely within 
the occlusions, and using von Neumann boundary conditions for the remaining faces whose centers lie within 
the occlusions. 4.2 Advection Velocity Field. The advection velocity field, which is used to advect 
both the level set and the fluid velocity field, must accurately account for the occlusions. This is 
achieved via our new method of constrained velocity extrapolation. This is similar to the method described 
in [Enright 2002] for extrapolating fluid velocities into the air, the main distinction however, besides 
the fact that we are extrapolating into occlusions rather than the air, is that constraints are imposed 
on the velocities after extrapolation. These constraints reflect the effects of three space varying aspects 
of the occlusions: the slip coefficient, the surface orientation, and the velocity. It is convenient 
to calculate the resulting constrained velocity as the sum of two orthogonal components one parallel 
to the occlusion normal and the other perpendicular: .proj un ·u =n ·u n f of oo o u|| =. u .=ortho no 
(u f (1 -2ao )+u oao ) .projnuo otherwise o Where uf is the unconstrained extrapolated fluid velocity, 
uo is the occlusion velocity, ao is the occlusion slip coefficient, and no is the normalized gradient 
of the occlusions level set. The slip coefficient parameterizes the range of boundary conditions from 
full-slip to no-slip along the range [0,1]. (It should be noted that while we do here suggest a full 
parameterization of the slip coefficient we have only thoroughly tested the full-slip extreme were a=0.) 
 5 Results The results are promising. Our unified approach to modeling occlusions has significantly 
decreased the difficulty of specifying complex dynamic scenes. Incorporating the occlusions into the 
fluid solver via constrained velocity extrapolation, because it models the subtleties of occlusions, 
result in more realistic and precise fluid behavior in the presence of objects. Figure 1, courtesy of 
Jason Cobill, illustrates the accuracy at which occlusions, in this case a scan of a human head, can 
be incorporated into the fluid simulation. Figure 2, courtesy of Conrad Dueck, is a still from an animation 
consists of a series of cups scooping up fluid from a rectangular container. Figure 3, courtesy of Chris 
Pember, shows the rendering of a simple scene via photon mapping. Figure 1 Figure 2 Figure 3  References 
ENRIGHT, D., MARSCHNER, S. AND FEDKIW, R. Animation and Rendering of Complex Water Surfaces. ACM Trans. 
On Graphics (SIGGRAPH 2002 Proceedings) 21, 736-744, 2002. FOSTER, N. AND FEDKIW, R. Practical Animation 
of Liquids. SIGGRAPH 2001 Annual Conference, 12-22, 2001. FOSTER, N., AND METAXAS, D. Realistic Animation 
of Liquids. Graphical Models and Image Processing 58, 471-483, 1996. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965562</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[The cubic interpolated level set method for realistic fluid animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965562</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965562</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653901</person_id>
				<author_profile_id><![CDATA[81100134394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp., Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP45027179</person_id>
				<author_profile_id><![CDATA[81343507482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Heihachi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Information Systems Corp., Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P22866</person_id>
				<author_profile_id><![CDATA[81100183033]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunimatsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Corp., Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566645</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[D. Enright and S. Marschner and R. Fedkiw. Animation and Rendering of Complex Water Surfaces. Proceedings of SIGGRAPH2002]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>136618</ref_obj_id>
				<ref_obj_pid>136599</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[J. U. Brackbill, D. B. Kothe, and C. Zemach. A continuum method for modeling surface tension. J. Comp. Phys., 100:335--354, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[T. Yabe. Unified Solver CIP for Solid, Liquid and Gas. Comp. Fluid Dynamics Review 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[T. Aoki and S. Nishita and K. Sakurai. Interpolated differential operator (IDO) scheme and application to level set method. Comp. Fluid Dynamics JOURNAL, pp. 406--417, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The cubic Interpolated level set method for realistic fluid animation Ken Tanaka Heihachi Ueki Atsushi 
Kunimatsu Toshiba Corp.,Japan Toshiba Information Systems Corp.,Japan Toshiba Corp.,Japan ken3.tanaka@toshiba.co.jp 
heihachi.ueki@glb.toshiba.co.jp atsushi.kunimatsu@toshiba.co.jp 1 Introduction In this paper, we present 
a method for simulating natural motion of fluid with complex surfaces, bubbles and surface tension. D. 
Enright et al. proposed the particle level set method [1] and they succeeded in creating difficult scenes 
such as water being poured into a glass. Their method has the ability to model complex surfaces of fluids, 
and improve conservation of fluid volume by using mass-less particles and implicit surfaces. However, 
their results lack the effect of bubbles and surface tension (and the motion of water is a little sticky 
due to low precision of velocity field solution). These effects are essential for realistic fluid animations. 
Although we do not use any particles, our system uses 3rd order implicit surface to model the shape of 
fluid and employs highly accurate schemes to evolve implicit surface. With these methods, we achieved 
a practical level of conservation of volume and realistic motions with bubbles and surface tension. 
2 Cubic interpolated level set function We use Cubic Interpolated Level Set Function (CI-LSF) to represent 
the shape of fluids and solids. This is a cubic interpolated polynomial of level set functions defined 
in each local cube domain and has characteristic of signed distance function. Therefore, we can define 
the shape of fluids and solids as 3rd order implicit surfaces. Figure 1 shows the initial shape of fluid 
defined by CI-LSF and its collapse. Figure 1: (mesh size : 90x90x90)  3 Two phase flow model We employed 
the variable density model to simulate the effect of bubbles. This method treats gas and liquid as two 
different fluids with two different density values. As mentioned above, we use CI-LSF . to represent 
the shape of fluids (liquid or gas). And we convert . into approximate Heaviside function Ha (.) and 
finally we can get fluid density by .g + (.l -.g )Ha (.). Here .l , .g means density of liquid and gas 
respectively. For this method, it is unnecessary to set any boundary conditions at the interfaces. Furthermore, 
we can easily calculate mean curvature from . , and so surface tension can be treated as body force by 
using the Continuum Surface Force (CSF) model [2].  4 Interface tracking We solve the advection equation 
for time evolution of . . We employed the Cubic Interpolated Propagation (CIP) method [3] to solve this 
equation. As is well known, however, the advection process ruins the characteristic of the signed distance 
function of . , and therefore, we have to perform a reinitialization process. In this process, we use 
the Interpolated Differential Operator (IDO) method [4]. Both the CIP and IDO methods use cubic interpolated 
polynomial in each local cube domain by using the information of derivatives of. . In addition to the 
advection equation E , we need to solve its derivative equations to calculate derivative values of . 
. In order to get 64 coefficient complete cubic polynormial in local cube domain, we need 7 derivative 
values of . . But for efficiency, we use incomplete 32 coefficient cubic polynomials by using only 3 
derivative values such as . , x .y , .z . Therefore, we only have to solve the corresponding 4 equations 
E , E , Ey , Ez . x  5 Velocity advection For realistic fluid animation, it is important to solve the 
momentum portion of the Navier-Stokes equations accurately. The most influential part of the equations 
is treatment of the non­linear advection terms. In this part, we use the CIP/IDO method, which is a 3rd 
order scheme. Although there are some cases in which a 1st order semi-Lagrangian scheme is used in this 
part, it is difficult to model a low kinematic viscosity fluid such as water with 1st order scheme. 
6 Example Figure 2 shows water being poured into a glass with bubbles. Figure 2: (mesh size : 40x40x80) 
 7 Conclusion We use cubic interpolated level set function to represent the shape of fluids and solids. 
And we employed the CIP and IDO methods to solve the equations of time evolution of . and velocity field. 
The combination of these methods enables simulation of realistic fluid motion with bubbles and surface 
tension. 8 References [1] D.Enright and S.Marschner and R.Fedkiw. Animation and Rendering of Complex 
Water Surfaces. Proceedings of SIGGRAPH2002 [2] J.U.Brackbill, D.B.Kothe, and C.Zemach. A continuum method 
for modeling surface tension. J. Comp. Phys., 100:335 354, 1992. [3] T.Yabe. Unified Solver CIP for Solid, 
Liquid and Gas. Comp. Fluid Dynamics Review 1998. [4] T.Aoki and S.Nishita and K.Sakurai. Interpolated 
differential operator (IDO) scheme and application to level set method. Comp. Fluid Dynamics JOURNAL, 
pp.406-417, 2001. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965558</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Fluid simulation interaction techniques]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965558</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965558</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP24017316</person_id>
				<author_profile_id><![CDATA[81100426615]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Magnus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wrenninge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027387</person_id>
				<author_profile_id><![CDATA[81100308346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383261</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[FOSTER, N., AND FEDKIW, R. 2001. Practical animations of liquids. In SIGGRAPH 2001, Computer Graphics Proceedings, ACM Press / ACM SIGGRAPH, E. Fiume, Ed., 23--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fluid Simulation Interaction Techniques Magnus Wrenninge * Doug Roble Norrk¨oping Visualization and 
Interaction Studio Digital Domain Link¨ oping University, Sweden 1 Introduction Level set surfaces are 
well suited for representing the complex sur­face of a liquid in a ¤uid simulation. At Digital Domain 
we have developed a ¤uid simulation package that represents all objects, not just the liquid, as particle 
corrected level sets and velocity £elds. This framework enables easy experimentation with all aspects 
of the simulation, including adding new ways of interacting with the liquid. This presentation will illustrate 
the power of the framework with the following new concepts: moving, deforming objects repre­sented as 
level sets; level set sources and drains; a generalized way of manipulating the velocity £eld, based 
on an image compositing paradigm.  Figure 1: Moving, deforming object, represented as a level set, in­teracting 
with a liquid. 2 Moving, Deforming Objects In [Foster and Fedkiw 2001], the authors describe using level 
sets to represent the liquid and a voxelized representation for static ob­jects in a ¤uid simulation. 
However, they leave moving objects in their polygonal representation. We implemented a very similar sys­tem, 
but could not resist the temptation to represent all objects as level sets. There are many bene£ts to 
a level set representation, the gradient/normal at any point is easy to compute, inside/outside tests 
are trivial and CSG operations are natural and simple. There are some challenges to this approach. First, 
a robust, fast method to convert from a polygonal object to a level set represen­tation is needed. We 
discuss this method in a separate presenta­tion. Second, for moving, deforming objects, a corresponding 
vec­tor £eld is needed to represent the typically non-uniform velocity of each voxel in the object. By 
using a windowed radial basis function, we are able to interpolate the non-uniformly distributed velocities 
present at vertices on the surface of the object throughout the voxel space of the object. Finally, in 
order to satisfy the CFL condition * e-mail: magwr867@student.liu.se e-mail: doug@d2.com of the ¤uid 
simulation, the internal time step is likely much £ner than the 30 frames per second object animation. 
The brute force way to address this problem is to simply ask the animation package to create a sub-frame 
instance of the geometry and convert that to a level set and velocity £eld, this produces a large number 
of £les. We have been experimenting with using the velocity £eld associ­ated with the level set at a 
frame of the animation to move the level set forward to produce a sub-frame level set. 3 Direct Manipulation 
of Velocity Fields When dealing with user interaction, methods of using 3D splines to alter the motion 
of the liquid have been described. When we looked at what other motion primitives might be needed, we 
found that any number could be conceived, and that a generalized layer separating the engine from speci£c 
interaction elements was needed. Our goal was to allow artists to develop custom interac­tion elements, 
without having to alter the simulation engine at all. We found that by accepting vector £elds as inputs, 
the simulation could be driven both kinematically and dynamically through veloc­ity and force-£elds, 
respectively. And by adding an alpha channel, the artists could composite the many velocity £elds together. 
 4 Sources, Drains and Water Recorders Sources and drains provide a convenient way of introducing or 
re­moving water from the ¤uid simulation. Level sets represent the shape of the source or drain and velocity 
£elds control how water direction and speed of the water. This uniform level set / velocity £eld framework 
makes integrat­ing moving objects, sources and drains is conceptually simple. Us­ing the trivial CSG 
operations provided by level sets, the moving objects can be unioned with the static objects and their 
velocities merged into one velocity £eld for all objects. Likewise, the source (or drain) level sets 
can be unioned (or differenced) with the exist­ing liquid in the simulation to produce (or destroy) liquid. 
Finally, we have implemented a water recorder that combines many of the previous techniques. An arbitrarily 
shaped area that water will be ¤owing through is speci£ed. As the simulation pro­gresses, the water and 
velocities inside this area are written to sep­arate level set and velocity £eld £les. These £les can 
be used as animated sources to run a smaller section of the simulation again or at a higher resolution. 
These level sets can even be used as a moving, deforming object in another ¤uid simulation, providing 
a cheap way to simulate a much lighter liquid ¤owing over a much heavier liquid. References FOSTER, 
N., AND FEDKIW, R. 2001. Practical animations of liquids. In SIGGRAPH 2001, Computer Graphics Proceedings, 
ACM Press / ACM SIGGRAPH, E. Fiume, Ed., 23 30. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965559</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Fluids with extreme viscosity]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965559</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965559</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653877</person_id>
				<author_profile_id><![CDATA[81100392996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[F&#228;lt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP35027386</person_id>
				<author_profile_id><![CDATA[81100308346]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Roble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Domain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>545289</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CARLSON, M., MUCHA, P., III, B. V. H., AND TURK, G. 2002. Melting and flowing. ACM SIGGRAPH Symposium on Computer Animation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ENRIGHT, D., AND FEDKIW, R. 2002. Robust treatment of interfaces for fluid flow and computer graphics. 9th Int. Conf. on Hyperbolic Problems Theory, Numerics, Applications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fluids With Extreme Viscosity Henrik F¨Douglas Roble alt * Norrk¨oping Visualization and Interaction 
Studio, Digital Domain Link¨oping University, Sweden 1 Introduction Special care must be taken when simulating 
.uids with varying and high viscosities. We have developed a technique that builds on pre­vious work 
in this .eld [Carlson et al. 2002], but does not suffer from any unwanted arti.cial dampening of velocities. 
In fact our method lets us introduce arti.cial dampening to give the impression of a semi-solid .uid. 
This is of great importance when producing effects like mud, clay, melting and solidifying.  2 Method 
In our simulator, the Navier-Stokes equations are solved with a semi-Lagrangian integration scheme common 
in the computer graphics community. We represent the air/.uid interface with a par­ticle level set and 
use a ghost .uid velocity extrapolation method [Enright and Fedkiw 2002]. At obstacle boundaries we enforce 
the Dirichlet and Neuman condition. Following [Carlson et al. 2002] the diffusion term is separated and 
a linear system is formed for each dimension of the velocity, AUdif f = U Where A = I - D and I is the 
identity matrix and D is our diffu­sion operator. U is a row vector of velocity components inside the 
.uid before the diffusion. Udif f is the velocities after the operation. For each velocity component, 
(i, j, k), inside the .uid we setup a row in A of the form Lt A(i, j,k)= -· t2 ··· .i, j,k- 1 ··· .i, 
j- 1 ··· .i- 1 ··· di, j,k ··· .i+ 1 ··· .i, j+ 1 ··· .i, j,k+ 1 ··· 2 ,k 2 , j,k 2 , j,k 2 ,k 22 where 
t is the voxel width, . is the viscosity and d is the diag­onal coef.cient which is equal to one minus 
the sum of the other coef.cients of the row. The example row above corresponds to a velocity component 
where all neighboring components are inside the .uid. If a neighboring component is outside the .uid 
it will not have an entry in the row, and subsequently not in d (or in A at all), making the system sparse 
and symmetric. When simulating extreme viscosities, we have found that in­creasing the d entries in A 
that represent voxels next to obstacle boundaries gives faster damping. In other words, increasing d 
will increase the the in.uence of the velocity at that voxel on the so­lution of the linear system. This 
is integrated into our system by setting viscosities inside the obstacles and adding that, as we add 
ordinary coef.cients, to d when next to an obstacle boundary. Set­ting viscosity inside obstacles to 
zero will exclude this effect. The .nal expression for d is then di, j,k = 1 -Lt · [ . (.i± 1 )+ . (.i± 
1 )] 2 , j± 12 ,k± 12 , j± 21 ,k± 1 t22 2 fluid obstacle *e-mail: henfa459@student.liu.se e-mail: doug@d2.com 
 Figure 1: Benchmarking test with viscosities of (left to right) 10, 100, 1000, 10 000 and 100 000. 
We then solve for Udif f by using the conjugate gradient method and a diagonal pre-conditioner. 3 Example 
and Evaluation Using the described method we can simulate .uids with varying viscosities. Extremely high 
viscosity will result in a solid-like .uid. The method proves robust and does not introduce any unrealistic 
behavior, such as freezing in free .ight, see Figure 1. Future work will be integration of moving obstacles 
and improving the treatment of the .uid/air interface. References CARLSON, M., MUCHA, P., III, B. V. 
H., AND TURK, G. 2002. Melting and .owing. ACM SIGGRAPH Symposium on Com­puter Animation. ENRIGHT, D., 
AND FEDKIW, R. 2002. Robust treatment of in­terfaces for .uid .ow and computer graphics. 9th Int. Conf. 
on Hyperbolic Problems Theory, Numerics, Applications. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965563</section_id>
		<sort_key>35</sort_key>
		<section_seq_no>35</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling & collision]]></section_title>
		<section_page_from>35</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P653955</person_id>
				<author_profile_id><![CDATA[81100035576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derrick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vicarious Visions, Inc.]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965567</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Grooming furry surfaces of arbitrary topology]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965567</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965567</url>
		<abstract>
			<par><![CDATA[The Moving Picture Company has developed a custom fur solution which employs a novel interpolation technique driven by Maya shading networks, allowing the artist to intuitively groom furry creatures regardless of surface topology or type.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14203672</person_id>
				<author_profile_id><![CDATA[81538876556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Harvey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Moving Picture Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ENGLE R. ET AL. 2002. Stuart Little 2: Let the Feathers Fly. SIGGRAPH 02 Course Notes.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Grooming Furry Surfaces of Arbitrary Topology Andrew Harvey The Moving Picture Company Abstract The 
Moving Picture Company has developed a custom fur solution which employs a novel interpolation technique 
driven by Maya shading networks, allowing the artist to intuitively groom furry creatures regardless 
of surface topology or type. 1 Introduction For two commercials advertising Seriously Strong Cheddar, 
a mixture of digital creature effects and live action was used to incarnate a criminal mastermind with 
the body of a man and the head of a mouse. We decided that a custom fur system allowing us to develop 
in direct response to our artists needs was worth the up-front investment in time. The most pressing 
issue was the grooming method. A guide curve approach where each hair is a blend of nearby control curves 
was favored. This method has been used with great success in the past, perhaps most famously in the Stuart 
Little films by Sony Imageworks [Engle et al. 2002]. One drawback though was that such systems often 
required large numbers of curves typically hundreds. Therefore the first challenge was to improve their 
expressive power. Another challenge was to avoid imposing restrictions on the underlying surface geometry 
available to the artist. A Maya plug-in was therefore developed, together with a Renderman backend. 
 2 Hair Interpolation Since we cannot make any assumptions about either the topology or the parameterization 
of the surface, our interpolation scheme relies only on the surface points and their normals. The technique 
is thus decoupled from the adverse effects of warping and pinching that may be evident within the surface 
s rest pose. The animated surface is associated with a topologically identical reference surface that 
remains static. First, we shape each individual hair on the reference geometry. We then anchor the resulting 
curve shape onto the animated surface. An analogy may be made with the traditional use of reference geometry 
for texturing. In both cases, swimming artifacts are eliminated, ensuring inter-frame coherence. Guide 
curves are arranged in orbit around the stationary reference surface. Each guide curve is associated 
with the closest surface point Pg and is directed along the accompanying normal Ng. Fundamentally, there 
must exist a one-to-one relationship between points on the reference surface and points on the animated 
surface. This is dependent on the type of geometry; although UVs are a natural choice for NURBS surfaces, 
barycentric coordinates may be favored for triangle meshes. Crucially, the technique imposes no restriction 
on the form of the relationship, only that one exists. andrew-h@moving-picture.com  Given an arbitrary 
point P on the reference surface with normal N, we calculate the contribution of a nearby guide curve 
using the following steps. We perform these in the object space of the reference surface: 1. Calculate 
the shortest rotation that would align Ng with N. 2. Apply the rotation to the guide curve about its 
base. 3. Root the new curve at P, then transform into the local surface frame established by P, N, and 
the corresponding tangents.  The influence of each guide curve is spread over a hemisphere of surface 
normals centered at Ng. Furthermore, the region of influence may be shaped using an influence map. After 
blending the result with contributions from the other guide curves, we place the new hair on the animated 
surface.  3 Maps and Shading Networks Influence maps, along with other surface-varying quantities such 
as color, density and clumping, are controlled using Maya shading networks. Both procedural and painted 
textures may be applied using conventional tools commonly employed by Maya artists. There is no hard 
limit on the number of quantities allowed, and additional controls exist to introduce randomness. These 
values, sampled at the root of each hair on the reference surface, may then be passed into a Renderman 
shader at render time.  References ENGLE R. ET AL. 2002. Stuart Little 2: Let the Feathers Fly. SIGGRAPH 
02 Course Notes. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965566</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Wire modeling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965566</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965566</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14196652</person_id>
				<author_profile_id><![CDATA[81320492516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Esan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mandal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visualization Sciences Program, Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14054606</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visualization Sciences Program, Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP42051680</person_id>
				<author_profile_id><![CDATA[81339530055]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vinod]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Srinivasan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Visualization Sciences Program, Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[http://www-viz.tamu.edu/faculty/ergun/topology]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wire Modeling Esan Mandal, Ergun Akleman*and Vinod Srinivasan Visualization Sciences Program Texas A&#38;M 
University Modeling shapes that are made from wires and solids with large number of holes is an interesting 
research problem in computer graphics. These shapes are common in art, such as Escher s drawings of rind 
shapes, and the intricate nested carved sculptures of the Far East. Figure 1 is such an ex­ample of the 
highly decorative architectural embelishments used widely in Indian architecture. Note that this door 
is made from stone and has a visible thickness.  Figure 1: An example of a solid shape with large number 
of holes. Figure 2: Wired caricature of Humphrey Bogart. Unfortunately, most existing modeling programs 
do not allow the users to easily create such beautiful structures, which we simply call wired shapes. 
Many modeling pro­grams allow to render wireframes that can be used to give an illusion of wires, but 
wireframes are not 3d meshes hence *Corresponding Author. e-mail: ergun@viz.tamu.edu. lack the depth 
and richness of a 3d model. Most existing programs also support texture mapping with transparency. Since 
the transparent texture maps can be very .exible, they are commonly used in computer graphics. However, 
again they lack depth and do not automatically respond to the 3D environment that they are created in, 
i.e. in terms of lighting (specularity, shadows etc.) There exist biological visualiza­tion programs 
that can create a 3D wire look. However, their application is strictly restricted to protein modeling 
and they are not easily used for artistic purposes. For our purposes, we extended capabilities of our 
existing topological mesh modeler to create wired shapes (For more information on topological mesh modeling 
see [1]). This sim­ple extension allows us to replace each edge of a given mesh with a 3D pipe . Moreover, 
our system guarantees that the pipes are connected and the resulting shape can be physi­cally constructed. 
We have developed a variety of remeshing algorithms to create artistically interesting mesh structures 
from any give mesh. Our remeshing algorithms include not only well­known remeshing schemes (i.e. subdivision 
schemes such as vertex-insertion, corner-cutting, and simplex; and classical polyhedral operators such 
as dual, stellation, and trunca­tion), but also the new schemes we have developed. One example of the 
new schemes is honeycomb algorithm that changes majority of the faces to hexagons and creates a hon­eycomb 
look. We can create a wide variety of mesh struc­tures by applying several combinations of remeshing 
schemes (For an example see Figure 3). Figure 3: Wired rabbit. References [1] http://www-viz.tamu.edu/faculty/ergun/topology 
Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965565</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[3D modeling of trees from freehand sketches]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965565</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965565</url>
		<abstract>
			<par><![CDATA[We will present a user interface for quickly and easily modeling three-dimensional (3D) botanical trees from freehand sketches. The system generates 3D geometry from a two-dimensional (2D) sketch based on the assumption that trees spread their branches uniformly. Several editing operations are implemented, such as adding, cutting, and erasing branches. Our system also predicts, and generates, branches automatically. The user can continue editing a 3D tree by selecting the predicted branches. The interface of our system allows novices to design reasonably natural-looking trees quickly and interactively as compared with the interfaces of rule-based systems designed for expert users (e.g., L-systems).]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653915</person_id>
				<author_profile_id><![CDATA[81100546396]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18001697</person_id>
				<author_profile_id><![CDATA[81100444444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>263525</ref_obj_id>
				<ref_obj_pid>263407</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[IGARASHI, T., MATSUOKA, S., KAWACHIYA S., AND TANAKA, H. 1997. Interactive Beautification: A Technique for Rapid Geometric Design. In Proceedings of UIST '97, 105--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218427</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[WEBER, J. AND PENN, J. 1995. Creation and Rendering of Realistic Trees. In Proceedings of ACM SIGGRAPH 95, 119--128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Modeling of Trees from Freehand Sketches Makoto Okabe* Takeo Igarashi Tokyo University, Japan Tokyo 
University, Japan Abstract We will present a user interface for quickly and easily modeling three-dimensional 
(3D) botanical trees from freehand sketches. The system generates 3D geometry from a two-dimensional 
(2D) sketch based on the assumption that trees spread their branches uniformly. Several editing operations 
are implemented, such as adding, cutting, and erasing branches. Our system also predicts, and generates, 
branches automatically. The user can continue editing a 3D tree by selecting the predicted branches. 
The interface of our system allows novices to design reasonably natural-looking trees quickly and interactively 
as compared with the interfaces of rule-based systems designed for expert users (e.g., L-systems). 1. 
Modeling Process Figure 1 shows the entire modeling process using our system. The user begins to model 
a 3D tree by sketching a simple 2D tree by dragging the mouse (a). A pair of strokes represents a trunk 
or a branch. When the user rotates the model with the right button, the system generates a 3D tree (b). 
Branches are spread uniformly in 3D space as each pair of neighboring branches makes an approximately 
equal angle. We implemented strokes for editing 3D tree models. A stroke intersecting a branch once cuts 
it. A stroke intersecting a branch twice erases it. The user can also add a branch to the existing model. 
The difference between (b) and (c) is that the upper two branches have been erased and a small branch 
has been added to the bottom-right branch. Sketching is a very intuitive operation for novices, but it 
is extremely cumbersome to sketch all the branches on a tree. We therefore implemented a prediction function 
inspired by [Igarashi et al. 1997]. Our system automatically generates predicted branches around a clicked-on 
branch by scaling and copy-pasting the existing child branches. For example, by clicking on the trunk 
in (c), predicted branches are generated around the trunk as in (d). The prediction uses the information 
on the two branches that are already on the trunk. Our system implements eight kinds of prediction patterns 
[Weber and Penn 1995], such as conical , spherical , and so on. For example, in (d), branches are predicted 
using the conical pattern. If the user clicks on the trunk again, the system performs an alternative 
prediction using a different pattern. Alternatively, the user can select some of the predicted branches 
by clicking on them or drawing a circle around them, and the selected branches then become part of the 
existing 3D tree model (e). The user can also generate predicted branches around another branch with 
a child branch. In (f), the user clicks on the bottom­right branch and selects all the predicted branches 
(g). By clicking on a branch twice, the user can propagate the same branching style for all other branches 
belonging to the same generation; (h) shows the result of clicking on the bottom-right branch twice. 
The user completes the operation by selecting the predicted branches (i). The user can also add planar 
polygons representing leaves to terminal branches (j). *e-mail: makoto21@ui.is.s.u-tokyo.ac.jp e-mail: 
takeo@ui.is.s.u-tokyo.ac.jp 2. Results It took approximately one minute to model the tree shown in Figure 
1. Figure 2 shows example trees designed by the author using our system. We assigned appropriate texture 
images to leaf polygons. We spent approximately 30 to 90 minutes to design each of these models. We also 
asked test users to create original tree models, and examples are shown in Figure 3. They spent approximately 
5 to 20 minutes to design each of these models. This sketch presents our system for modeling 3D trees. 
Our system is interactive and intuitive, and our experience shows that novices can use it to model 3D 
trees freely and creatively. (a) (b) (c) (d) (e) selecting stroke  (j) (i) (h) stroke (g) (f) Figure 
1. The modeling process using our system  Figure 2. 3D trees designed by the author Figure 3. 3D trees 
designed by test users References IGARASHI, T., MATSUOKA, S., KAWACHIYA S., AND TANAKA, H. 1997. Interactive 
Beautification: A Technique for Rapid Geometric Design. In Proceedings of UIST 97, 105-114. WEBER, J. 
AND PENN, J. 1995. Creation and Rendering of Realistic Trees. In Proceedings of ACM SIGGRAPH 95, 119-128. 
Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965564</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[A procedural approach to modeling impact damage]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965564</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965564</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP28017550</person_id>
				<author_profile_id><![CDATA[81100652943]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eberle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653930</person_id>
				<author_profile_id><![CDATA[81100553312]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[Strunk]]></middle_name>
				<last_name><![CDATA[Havok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653945</person_id>
				<author_profile_id><![CDATA[81100465448]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ronan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[O'Sullivan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>586195</ref_obj_id>
				<ref_obj_pid>586191</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[ANDREWS, L. 2001. A Template for the nearest Neighbor problem. C++ Users Journal. 19, 11, p. 40--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SI, H. 2000. Tetgen. A 3D Delaunay tetrahedral Mesh Generator, v.1.2 Users Manual. Technical Report. WeirStrass Institute for Applied Analysis and Stochastics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1242200</ref_obj_id>
				<ref_obj_pid>1242073</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[RAGHAVACHARY, S. 2002. Fracture generation on polygonal meshes using Voronoi polygons. In Conference Abstracts and Applications 2002, ACM Press, ACM SIGGRAPH. p. 187]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[SMITH, J., WITKEN, A., AND BARAFF, D. 2000. Fast and Controllable Simulation of the Shattering of Brittle Objects. Graphics Interface p. 27--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Procedural Approach to Modeling Impact Damage David Eberle* Oliver Strunk Ronan O Sullivan Havok 
1 Introduction In recent years, the demand for destruction effects in movies and games has sparked research 
into the physically based modeling of deformation and fracture. While these approaches have produced 
some quality results, many are difficult to implement and require large computational resources. In this 
sketch, we present a fast and simple non-physical alternative for creating a subset of impact damage 
effects on brittle objects. 2 Initial Construction Our technique has been developed using the rigid 
body dynamics portion of the Havok 2.0 SDK. The initial shape of our rigid body is decomposed into a 
number of tetrahedral elements using the open source project Tetgen [Si 2000]. The decomposition of our 
geometry into tetrahedra is not a prerequisite for our technique and alternative methods such as those 
described in [Raghavachary 2002] can easily be substituted. We then construct a dual graph of the tetrahedralization 
using the center of mass of each tetrahedron in local space for the vertex positions and the face relative 
neighbors to construct the edges. The mass, center of mass and inertia tensor for each tetrahedron is 
is stored and associated with the vertex for later use. The vertex positions are stored in a nearest 
neighbor data structure as described in [Andrews 2002]. Our material parameters define the material hardness 
in each of the standard basis directions. The strength of an edge is determined by a projection of the 
hardness parameters onto the edge direction. This heuristic enables us to model simple anisotropic behavior. 
A callback function is then registered with our simulator and is associated with our initial rigid body 
before the simulation starts. This function is later called when a contact point has been generated between 
this body and another in the system. 3 Impact Modeling Inside the callback function, a frame is constructed 
from the relative velocity direction and a tangent to the contact surface. Using this frame an ellipsoidal 
region of impact is constructed. The scale and eccentricity of this ellipsoid are controlled by function 
of material parameters and the collision impulse magnitude. The contact point is then transformed into 
local space where an in-ellipsoid query is made with the aid of the nearest neighbor structure. For each 
incident edge of a vertex inside, a value of damage is computed using functions of distance from the 
contact point and the edge direction. Edges that are more perpendicular to the impact velocity direction 
receive more damage than those that are more parallel as a heuristic to artificially model the transmission 
of energy and breaking of bonds within the material. The damage value is then subtracted from the edge 
strength. -------------------------------------------- *e-mail: daveeberle@earthlink.net e-mail: oliver.strunk@havok.com 
 e-mail: rjosullip@hotmail.com Figure 1: Left: Wall being struck by a projectile. Right: The dual graph 
displaying the active and inactive edges. 4 Creating Fragments When the strength of an edge falls below 
zero it is marked as being inactive. A union-find algorithm is then performed using the active edges 
in the graph to determine the partition of elements. Each set of elements returned is used to create 
a new rigid body. Using the mass, center of mass and inertia tensor values initially stored in the associated 
vertices, these parameters can be easily computed for the new rigid bodies. To compute the linear and 
angular velocity for the fragments we solve the normal equations of the linear system described in [Smith 
et al 2000]. If a fragment consists of enough elements the callback function can be attached to facilitate 
further breakage. Finally the original rigid body is removed from the system and the new bodies are added. 
 5 Conclusion The technique presented simulates a very limited range of breakage effects due to its non-physical 
approach. Damage is local to the region of impact and crack formation is completely ignored. Despite 
these limitations in contrast with more physical approaches we believe this technique offers a simple 
and fast solution to modeling a useful subset of damage effects. References ANDREWS, L. 2001. A Template 
for the nearest Neighbor problem. C++ Users Journal. 19, 11, p. 40-49. SI, H. 2000. Tetgen. A 3D Delaunay 
tetrahedral Mesh Generator, v.1.2 Users Manual. Technical Report. WeirStrass Institute for Applied Analysis 
and Stochastics. RAGHAVACHARY, S. 2002. Fracture generation on polygonal meshes using Voronoi polygons. 
In Conference Abstracts and Applications 2002, ACM Press, ACM SIGGRAPH. p.187 SMITH, J., WITKEN, A., 
AND BARAFF, D. 2000. Fast and Controllable Simulation of the Shattering of Brittle Objects. Graphics 
Interface p. 27 34. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965568</section_id>
		<sort_key>36</sort_key>
		<section_seq_no>36</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Faces]]></section_title>
		<section_page_from>36</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39036514</person_id>
				<author_profile_id><![CDATA[81100296840]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ronen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barzel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965569</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Digital face cloning]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965569</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965569</url>
		<abstract>
			<par><![CDATA[This sketch describes the process and the technology used in the creation of a digital clone of a human face for a story in the November 2002 issue of National Geographic on skin[Swerdlow 2002].]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14219695</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[JENSEN, H. W., MARSCHNER, S., LEVOY, M., AND HANRAHAN, P. 2001. A practical model for subsurface light transport. Proceedings of SIGGRAPH '2001, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74361</ref_obj_id>
				<ref_obj_pid>74333</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[KAJIYA, J., AND KAY, T. 1989. Rendering fur with three dimensional textures. Proceedings of SIGGRAPH 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[SWERDLOW, J. 2002. Unmasking skin. National Geographic, 36--63.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Face Cloning Henrik Wann Jensen* University of California, San Diego  (a) 3D mesh (close-up 
of nostril) (b) Color data (c) Diffuse rendering (d) Oily layer (e) Subsurface scattering (f) Final 
result Figure 1: These images illustrate the steps in recreating a human face using 3D scanning (top 
row) and a skin BSSRDF model (bottom row). Abstract This sketch describes the process and the technology 
used in the creation of a digital clone of a human face for a story in the Novem­ber 2002 issue of National 
Geographic on skin[Swerdlow 2002]. Digital Face Cloning In the Spring of 2002 National Geographic contacted 
me to hear if I would be interested in rendering a human face for a story on skin. The challenge would 
be to reproduce the appearance of a face given a 3D model. Scanning: The .rst challenge was the acquisition 
of an accurate 3D model of a face. Since the face would be rendered as a close-up spread over two pages 
the face model had to be very accurate, with enough resolution to capture .ne details such as pores. 
Unfortu­nately, it is not possible with existing techniques to scan skin with this accuracy due to subsurface 
scattering which blurs the light as it interacts with the skin. To achieve suf.cient accuracy a company 
(Arius3D) has developed a technique where they create a cast of a human face. The cast is .exible enough 
to represent pores and suf.ciently opaque that it can be scanned with very high resolution (100µm). With 
this technology, they were able to produce a face model with 13 million triangles. A slightly simpli.ed 
version is shown in Figure 1a -the smallest triangles in this image are repre­sentative for the detail 
in the full model. In addition, photographs of the model were aligned with the 3D model to obtain a rough 
es­timate of the skin color (Figure 1b). A white diffuse rendering of the geometry is shown in Figure1c. 
Rendering: To accurately render a human face it is necessary to take into account the two key elements 
of light interaction with human skin: subsurface scattering and light re.ection of the oily layer at 
the surface the skin. Since the scanned model did not have any information about oil, the .rst task was 
to create a program that made it possible to interactively adjust the oil layer density over the *e-mail: 
henrik@cs.ucsd.edu Copyright held by the author face. A rendering of the .nal oily layer (using a Torrance 
Sparrow model) is shown in Figure 1d. A full simulation of subsurface scattering requires a BSSRDF model 
[Jensen et al. 2001] as well as scattering and absorption pa­rameters for the skin. These parameters 
were not available and in­stead the skin parameters measured in [Jensen et al. 2001] were used as a starting 
point. Furthermore, a simple heuristic was devel­oped to locally adjust the scattering parameters slightly 
based on the color information from the scanning process. Using the BSS-RDF model we obtained the image 
in Figure 1e, which shows the subsurface scattering component of the skin. To obtain the .nal face image 
it was necessary to add facial hair to the model. Most important was the lack of eyebrows. The eye­brows 
and eyelashes were modeled by Steve Worley using his com­mercial Sasquatch program (www.worley.com), 
and rendered using the Kajiya model for hair [Kajiya and Kay 1989]. Note, that the rendering of the .nal 
face model had to be done in roughly one week. The .nal combined result is shown in Figure 1f, and this 
image appeared in the 2002 November issue of National Geographic [Swerdlow 2002], and was used on the 
cover of the Dutch edition. For the same article, National Geographic had the human subject photographed 
under roughly the same lighting con­ditions as in the simulation in order to compare with the results 
of the computer generated skin. Unfortunately, the subject used a sig­ni.cant amount of makeup for these 
closeup photographs, which almost eliminated the presence of subsurface scattering in her skin. Future 
challenges includes a more detailed skin model as well as the ability to animate the face. References 
JENSEN, H. W., MARSCHNER, S., LEVOY, M., AND HANRAHAN, P. 2001. A practical model for subsurface light 
transport. Proceedings of SIGGRAPH 2001, 511 518. KAJIYA, J., AND KAY, T. 1989. Rendering fur with three 
dimensional textures. Proceedings of SIGGRAPH 1989. SWERDLOW, J. 2002. Unmasking skin. National Geographic, 
36 63. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965571</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Learning controls for blend shape based realistic facial animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965571</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965571</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP35032581</person_id>
				<author_profile_id><![CDATA[81100641728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pushkar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14224261</person_id>
				<author_profile_id><![CDATA[81100654479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wen]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Tien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14026403</person_id>
				<author_profile_id><![CDATA[81100041821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mathieu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Desbrun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39023832</person_id>
				<author_profile_id><![CDATA[81100026067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pighin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>872921</ref_obj_id>
				<ref_obj_pid>872743</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[DEBUNNE, G., DESBRUN, M., CANI, M.-P., AND BARR, A. 2000. Adaptive simulation of soft bodies in real-time. In Proceedings of Computer Animation 2000, 15--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[NOCEDAL, J., AND WRIGHT, S. 1999. Numerical Optimization. Springer Verlag, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>907312</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PARKE, F. 1974. A parametric model for human faces. PhD thesis, University of Utah, Salt Lake City, Utah. UTEC-CSc-75-047.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Learning Controls for Blend Shape Based Realistic Facial Animation Pushkar Joshi* Wen C. Tien Mathieu 
Desbrun Frederic Pighin University of Southern California 1 Introduction Since Parke s pioneering work 
[Parke 1974], blend shape anima­tion as been a method of choice for facial animation. A blend shape model 
de.nes the face as a convex linear combination of sample ex­pressions: the blend shapes. A speci.c facial 
expression is then de­termined by specifying the contribution of each blend shape. There has been little 
work on how to specify these contributions; in this work, we investigate how we can automatically segment 
the lin­ear models in multiple, meaningful regions by applying a simple physical model to the blend shapes. 
The regions extracted, directly dependent on the idiosyncracies of the input expressions, de.ne a set 
of parameters that can be manipulated to control the face. We demonstrate the relevance of these parameters 
within two animation techniques. 2Segmentation Instead of having the user going through a long and tedious 
segmen­tation of the face by hand, we perform an automatic, data-driven segmentation. The key observation 
is that, for linear blend shape models, only regions with little deformation can safely be blended. Therefore, 
drawing on a simplest physical model using linear elas­ticity [Debunne et al. 2000], we propose to compute 
the Laplacian of the displacement .eld d between vertex s current position and its position on the neutral 
face. This second-order operator .d, null for any rigid deformation and large for big deformation, helps 
to segment the face into disjoint regions of similar amount of defor­mation, as explained next. We compute 
the discrete laplacian value at every vertex of every non-neutral (i.e., expressive) blend shape, and 
take the magnitude of the resulting vectors. This provides us with a deformation map for each expression. 
We gather these maps into a single deformation map M by computing for each vertex independently its maximum 
deformation value across all expressions. A fast segmentation can now be performed by simply splitting 
this map in regions of low deformation, and regions of high deformation. These regions, au­tomatically 
extracted, correspond to groups of vertices that gener­ally undergo similar deformation: locally, each 
region deforms in a quasi-rigid way. Notice that any idiosyncracy present in the input expressions (asymmetry 
of the smile for instance) will automati­cally be picked up by our segmentation algorithm. 3 Applications 
We showcase the validity of the parametrization by using it in two applications: the mapping of recorded 
facial motions onto the set of blendshapes and the interactive design of facial keyframes. Motion Mapping 
To map recorded facial data on a set of blend shapes, we assume that the motion (or the per-frame position) 
of a motion marker can be expressed as a linear combination of the cor­responding points in the blend 
shapes. This gives a system of linear equation for each motion marker. By using an iterative quadratic 
programming solver [Nocedal and Wright 1999], we obtain the op­timal values of the blending weights. 
Instead of solving the above *e-mail: ppj@usc.edu Figure 1: Various steps in an interactive face manipulation: 
start­ing from a neutral expression, the user can pick the corner of the mouth and very naturally create 
a smiling expression for instance (notice that all the wrinkles associated to this new facial pose are 
present, as expected). system for the entire model, we solve for each region created using our automatic 
segmentation process. To better .t the geometry to the motions, we then use radial basis functions to 
interpolate the displacement .eld created by the markers motion to the entire face geometry. The technique 
results in an animation that faithfully fol­lows the motion and also shows the surface deformations modelled 
in the blend shapes. Keyframe Animation We have designed a user interface that al­lows direct manipulation 
of a face model within the constraints of the blend shape. The user can directly grab a location on the 
face with the mouth pointer and pull it in an arbitrary direction. The system then maps the mouse motion 
onto the blend shapes: the contribution of the blend shapes are adjusted so that the resulting deformation 
best matches this motion. The deformations are esti­mated using a constrained least square solver and 
are easily com­puted in real-time. Damping is necessary to avoid instabilities in the areas of the face 
where little motion is expressed within the blend shapes. To allow editing at different level of details, 
we build a hi­erarchy of regions. Large regions allow gross modi.cation of the face whereas small regions 
allow local changes. The interface is very intuitive: complex facial expressions can be rapidly designed 
without any knowledge of the underlying control mechanisms.  References DEBUNNE, G., DESBRUN, M., CANI, 
M.-P., AND BARR, A. 2000. Adap­ tive simulation of soft bodies in real-time. In Proceedings of Computer 
Animation 2000, 15 20. NOCEDAL, J., AND WRIGHT, S. 1999. Numerical Optimization. Springer Verlag, New 
York. PARKE, F. 1974. A parametric model for human faces. PhD thesis, Univer­sity of Utah, Salt Lake 
City, Utah. UTEC-CSc-75-047. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965570</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Wrinkle generation model for 3D facial expression]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965570</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965570</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653879</person_id>
				<author_profile_id><![CDATA[81100522424]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP17013278</person_id>
				<author_profile_id><![CDATA[81320489695]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Etsuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Genda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kyushu Institute of Design]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>212551</ref_obj_id>
				<ref_obj_pid>212531</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. WU, N. M. THALMANN AND D. THALMANN. 1994. A Plastic-visco-elastic Model for Wrinkles in Facial Animation and Skin Aging. Proc. Pacific Graphics '94, 201--213.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[D. TERZOPOULOS AND K. WATERS. 1990. Physically-Based Facial Modeling, Analysis, and Animation. Jounal of Visualization and Computer Animation 1, 73--80.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[P. VOLINO AND N. M. THALMANN. 1999. Fast Generation Wrinkles on Animated Surfaces. Proc. WSCG '99.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wrinkle Generation Model for 3D Facial Expression Hiroshi Kono Etsuo Genda Graduate School of Kyushu 
Institute of Design Kyushu Institute of Design kono164@mac.com genda@kyushu-id.ac.jp 1 Introduction 
Facial lines generated by expression changes, or expressive wrinkles, are an important factor in interpreting 
the meaning of facial expressions, and can be used for realistic facial animation. Facial wrinkles formed 
according to skin texture and age are always present regardless of facial motion. They can be reproduced 
effectively by bump texture mapping. However, because they are directly affected by the movement of facial 
muscles, expressive wrinkles change constantly, and cannot be reproduced with the same technique. Therefore, 
we propose an expressive wrinkle generation model based on a simple algorithm and anatomic point of view. 
 2 Related Works Previous major researches on expressive wrinkles include studies on physical simulation 
models with multiple layers by Wu et al. [1994] and by Terzopoulous et al. [1990], and a study on the 
speed of facial wrinkle generation by Violino et al. [1999]. Since these methods revealed the difficulties 
in intuitive control such as adjusting size and position, we have developed a simple method to easily 
model facial wrinkles by controlling the movement of virtual muscles inspired by the motion of bellows. 
This method enables the reproduction of individual facial lines synchronized with facial movements. 
3 How To Generate Wrinkles Our model is built by simplifying anatomical structures into three parts: 
skelton skeleton, skin and virtual muscle, as shown in Fig. 1. Assuming that when virtual contraction 
and expansion of virtual muscle muscle changes the shape of the skin, the skeleton remains stationary, 
and skin wrinkles are generated with facial expressions as shown in Fig. 2. Wrinkle generation can be 
achieved by changing the shape of virtual Fig. 1. The structure of muscle, and we decide that the basic 
our model. unit for such wrinkle generation is the virtual muscle between wrinkles. wrinkles wrinkles 
generated is defined as the "wrinkle point"; the midpoint of two wrinkle points is defined as the "bulge 
point"; and the line connecting each wrinkle point is defined as the "muscular line." muscular line 
 wrinkle point wrinkle point bulge point Next, multiple numbers of these basic units are joined and placed 
on a skeleton as shown in Fig. 3. The muscular line is described as a curve along the shape of the skeleton 
r = r (t). On Curve r, nth wrinkle point Wn and bulge point Bn are placed. When Wn moves along r and 
parameter t changes from u to u', Wn-1 moves from v to v' (u, u', and v are known). Now we introduce 
parameter S, which shows the ratio of the muscular line length between Wn and Wn-1 to the entire curve 
length. When Parameter S is always static regardless of the movement of Wn, parameter v' is determined 
by the following equation and we can get the position of Wn-1 after movement: v' u' v dr dr drdt = dt 
-S dt dt dt dt u' u . Thus, by animating Wn on the muscular line, the positions of wrinkle points are 
changed. Bulge points move in the direction of normal to curve r for the changed distance between wrinkle 
points and the surface of the skin protrudes (Fig.4). Fig. 3. Wrinkle points are placed Fig. 4. After 
animating Wn. along a skelton.  4 Result Results reveal that the proposed facial line generation method 
can satisfactorily increase the realism of facial animation through a simple protrusion movement of the 
virtual muscle. Intuitive facial animation is made possible by using the virtual muscle mimicking the 
size and position of real facial expression muscles. Thus, the model's capability of generating facial 
lines of various size and position by controlling wrinkle points and bulge points has been shown. In 
the future, we intend to enhance reality in animation by applying such as a spring model in virtual muscle 
movement.  Reference Y. WU, N. M. THALMANN AND D. THALMANN. 1994. A Plastic­visco-elastic Model for 
Wrinkles in Facial Animation and Skin Aging. Proc. Pacific Graphics '94, 201-213. D. TERZOPOULOS AND 
K. WATERS. 1990. Physically-Based Facial Modeling, Analysis, and Animation. Jounal of Visualization and 
Computer Animation 1, 73-80. P. VOLINO AND N. M. THALMANN. 1999. Fast Generation Wrinkles on Animated 
Surfaces. Proc. WSCG '99. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965572</section_id>
		<sort_key>37</sort_key>
		<section_seq_no>37</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Narrative translations]]></section_title>
		<section_page_from>37</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>PP39051099</person_id>
				<author_profile_id><![CDATA[81100619792]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Linda]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lauro-Lazin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pratt Institute]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965576</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A 3D studio production system with immersive actor feedback]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965576</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965576</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14165339</person_id>
				<author_profile_id><![CDATA[81100474052]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[O.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BBC R&D, Kingswood Warren, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14191396</person_id>
				<author_profile_id><![CDATA[81100551006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Price]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BBC R&D, Kingswood Warren, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14145034</person_id>
				<author_profile_id><![CDATA[81440598790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[G.]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Thomas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[BBC R&D, Kingswood Warren, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[GRAU, O., AND THOMAS, G. A. 2002. Use of image-based 3d modelling techniques in broadcast applications. In 2002 Tyrrhenian International Workshop on Digital Communications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[ORIGAMI. project web-site. http://www-dsp.elet.polimi.it/origami/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ROSENTHAL, S., GRIFFIN, D., AND SANDERS, M. 2001. Real-time compter graphics for on-set visualization: "a.i." and "the mummy returns". In Siggraph 2001, Sketches and Applications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A 3D studio production system with immersive actor feedback O. Grau, M. Price, G.A.Thomas* BBC R&#38;D, 
Kingswood Warren,UK 1 Introduction TV-and .lm productions are making increasing use of 3D special effects. 
Actors involved in these kind of programmes are usually .lmed in a studio equipped with chroma-keying 
facilities. Any vir­tual scene components are added later in post-production. For the director, camera 
operators and actors there is a strong need to have an on-set pre-visualisation of the composited scene. 
This has been addressed in the past by using virtual studio techniques to get the composited scene on 
a studio monitor [Rosenthal et al. 2001]. Un­fortunately the actor still has the problem of looking into 
empty space. Hence synchronisation, particularly the eye-lines with a moving virtual object is a major 
problem. This sketch describes a new system that combines the ability to capture dynamic scenes, based 
on a multi-camera system in a chroma-key environment, with a view-dependent projection for ac­tor feedback. 
It is built upon previous work we did on the devel­opment of components for virtual studios [Grau and 
Thomas 2002] and is part of the EU-funded ORIGAMI project [origami]. 2 System overview The production 
system described here is designed to work in a nor­mal studio environment. For the immersive actor feedback 
front projection onto a special retro-re.ective cloth is used, which does not interfere with the chroma-keying 
facility. The .xed studio cap­ture cameras are equipped with a ring of blue LEDs around the lenses. This 
light is re.ected back to the camera and the cloth ap­pears saturated blue as depicted in .g. 1 (left). 
 Figure 1: Camera view (left) and director s view(right). The images of the .xed capture cameras (currently 
up to 12) are digitised and stored at full frame rate for later off-line process­ing. At the same time 
a 3D model of the scene is computed using chroma-keying and a visual hull computation in real-time. The 
real­time 3D model is then used for on-set visualisation (director s view .g. 1 right) in a virtual set. 
Since the entire virtual scene is three­dimensional, the virtual camera can be moved to any position, 
even outside of the real studio space. The immersive actor feedback system works in parallel to the real-time 
modelling system. It is based on standard data projec­tors driven from rendering PCs. The head position 
of one actor is derived from the captured 3D model and used to synthesise a view­dependent image of the 
virtual scene. *e-mail: oliver.grau|marc.price|graham.thomas@rd.bbc.co.uk Figure 2: Actor s view. A 
particular problem with front projection is that there can be light projected onto the actor which would 
be visible in the .nal programme. This is addressed in our system and solved by creating a mask derived 
from the real-time 3D shape of the actor. The system is implemented as a distributed system and is very 
scalable. The components are connected using inexpensive stan­dard IT-network technology. In addition 
to the .xed cameras a mo­bile studio camera equipped with a real-time camera-tracker can be used for 
close-ups or as a master camera. 3 Experimental results The system was recently used in a demo production. 
The entrance hall of a museum was digitised by one of the ORIGAMI partners and was then used as a virtual 
set. This scenario was the basis for pre-visualisation of the entire production, including actors mod­elled 
by our system and additional virtual objects. The production system has shown a lot of potential as a 
valuable tool. In particular it enables the actor to keep eye-contact with vir­tual objects. At the same 
time, the director can monitor the work with a composite view allowing better informed decisions about 
camera views and positions of both real and virtual actors. Acknowledgements Virtual backgrounds used 
in the pictures were provided by Politec­nico di Milano and Christian-Albrechts-University Kiel. References 
GRAU, O., AND THOMAS, G. A. 2002. Use of image-based 3d modelling techniques in broadcast applications. 
In 2002 Tyrrhe­nian International Workshop on Digital Communications. ORIGAMI. project web-site. http://www-dsp.elet.polimi.it/origami/. 
ROSENTHAL, S., GRIFFIN, D., AND SANDERS, M. 2001. Real­time compter graphics for on-set visualization: 
a.i. and the mummy returns . In Siggraph 2001, Sketches and Applications. Copyright held by the author 
 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965573</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Little red]]></title>
		<subtitle><![CDATA[storytelling in mixed reality]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965573</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965573</url>
		<abstract>
			<par><![CDATA[This proposal discusses utilizing Mixed Reality (MR) systems as an interface for storytelling, using a picture book as a metaphor. "Little Red" implements methods of re-creating literary work in 3D using MR techniques and a head-mounted display (HMD).]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P653963</person_id>
				<author_profile_id><![CDATA[81100345175]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoki]]></first_name>
				<middle_name><![CDATA["Issac"]]></middle_name>
				<last_name><![CDATA[Saso]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653902</person_id>
				<author_profile_id><![CDATA[81100439232]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39041138</person_id>
				<author_profile_id><![CDATA[81335492043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inakage]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[KATO, H., BILLINGHURST, M., POUPYREV, I., IMAMOTO, K., AND TACHIBANA, K. 2000. Virtual Object Manipulation on a Table-Top AR Environment. In Proceedings of ISAR 2000, Oct 5th-6th, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618818</ref_obj_id>
				<ref_obj_pid>616070</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BILLINGHURST, M., KATO, H., AND POUPYREV, I. 2001. The Magic Book: Moving Seamlessly between Reality and Virtual Reality. IEEE Computer Graphics and Application, May 2001, Vol. 21, No. 3, 2--4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Little Red: Storytelling in Mixed Reality Tomoki Issac Saso Kenji Iguchi Masa Inakage Keio University 
Keio University Keio University tom@imgl.sfc.keio.ac.jp needle@imgl.sfc.keio.ac.jp inakage@imgl.sfc.keio.ac.jp 
 Abstract This proposal discusses utilizing Mixed Reality (MR) systems as an interface for storytelling, 
using a picture book as a metaphor. Little Red implements methods of re-creating literary work in 3D 
using MR techniques and a head-mounted display (HMD). 1 Introduction Little Red is an exploration in 
new methods of storytelling. It is a children s pop-up book that implements techniques of re­creating 
literary work in 3D using MR technology. The user can read the book wearing an HMD, and enjoy the story 
being told through a three-dimensional field. In order to explore new possibilities of storytelling beyond 
what can be done with conventional books, the Little Red project interprets the physical form of the 
book itself as the interface. Therefore, actions such as closing the book, turning the page, and touching 
the page can be used as metaphoric ways to interact with the story. Also, presenting the story to the 
user by simultaneously utilizing both rendered 3D graphics and printed illustrations enable forms of 
expression that would have been otherwise impossible. Augmented Reality and MR are entering the practical 
stage. Vision based Augmented Reality, which uses markers placed in real space to synchronize the coordinate 
axis between real space and virtual space, can be expected to be applied to the publishing field, as 
devices and markers for implementation can be obtained inexpensively. We decided that a book-reading 
style was suited for MR storytelling, since the book, as an object, affords to the user many familiar 
actions (such as turning and page flipping) that can be used as devices for interaction with the MR scene. 
Similar systems that use a book as an interface exists, such as Mark Bilinghurst s Magic Book . Unlike 
it, however, we have focused on the storytelling aspect and further utilization of the book metaphor 
for the human-computer interface. We believe that establishing a new means of storytelling, one where 
the user views a 3D scene emerging amidst real space, and interferes with the story using their own hands, 
will expand the realms of storytelling. Figure 1. Little Red MR.  2 Exposition Little Red is intended 
to be experienced with an HMD. The system looks like a picture book, except with only the scene terrain 
depicted on the pages. With an HMD on, the 2D terrain features work in conjunction with the superimposed 
3D animation to present the story to the user in one coherent whole. The story begins when the user puts 
on the HMD and opens the book. Dialogue and narratives are delivered aurally. Each page contains a scene 
in Little Red Riding Hood s travel from the starting point, her house, to the destination, her grandmother 
s house. Since both the book (in real space) and computer-generated graphics (in virtual space) holds 
three-dimensional information, the user can freely rotate or take a closer look at the book, and the 
system reacts accordingly. Progression of the story in conventional books consists of turning the page, 
but this is not the case with Little Red . The story instead develops along with time, similarly to a 
movie. In Little Red , turning the page amounts to the act of switching the segment of the world currently 
being viewed. For instance, if the user flipped to the page directly after Little Red Riding Hood s current 
location, the user would actually be able to see the wolf running to get ahead of her. Events can be 
happening at multiple locations simultaneously, so the user can quickly flip through pages and experience 
more of the story than they were previously possible with conventional books. The storyline basically 
adheres to the original text of the Little Red Riding Hood story, but the story can fork at certain points, 
in accordance to the user s actions. For example, depending on the user, Little Red Riding Hood could 
reach her grandmother s house before the wolf does, instead of after. The story and ending can change 
drastically depending on what actions the user may take. Little Red can be represented as a publication 
of the next generation, and at the same time, an intuitive and tangible interface for enjoying interactive 
movies.  References KATO, H., BILLINGHURST, M., POUPYREV, I., IMAMOTO, K., AND TACHIBANA, K. 2000. Virtual 
Object Manipulation on a Table-Top AR Environment. In Proceedings of ISAR 2000, Oct 5th­6th, 2000. BILLINGHURST, 
M., KATO, H., AND POUPYREV, I. 2001. The Magic Book: Moving Seamlessly between Reality and Virtual Reality. 
IEEE Computer Graphics and Application, May 2001, Vol. 21, No. 3, 2-4. Copyright held by the author 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965575</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Textable movie]]></title>
		<subtitle><![CDATA[improvising with a personal movie database]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965575</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965575</url>
		<abstract>
			<par><![CDATA[This sketch presents a new approach to improvising movies according to the inter-relationship between personal videos and the story of an experience. <i>Textable Movie</i> is a graphical interface that invites a storyteller of any age to compose and visualize movies, images and sound environments while writing a story; the system self-selects and self-edits movies in real time based on textual input from the teller. <i>Textable Movie</i> aims to exalt the imagination of its authors (writer, and film-maker) by immersing them in real time, in a co-constructed narration.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P344897</person_id>
				<author_profile_id><![CDATA[81452607629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Catherine]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaucelle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Lab Europe, Story Networks Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P98573</person_id>
				<author_profile_id><![CDATA[81100068833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Glorianna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davenport]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab/Media Lab Europe, Interactive Cinema Group/Story, Networks Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P345115</person_id>
				<author_profile_id><![CDATA[81100596381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tristan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jehan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab/Media Lab Europe, Hyperinstruments Group/Story, Networks Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[PROUST, M. 1913. &#192; la recherche du temps perdu. Du c&#244;t&#233; de chez Swann. Remembrance of Things Past, 1922.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>760364</ref_obj_id>
				<ref_obj_pid>647458</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LIEBERMAN, H, and LIU, H, May 2002. Adaptive Linking between Text and Photos Using Common Sense Reasoning, Conference on Adaptive Hypermedia and Adaptive Web Systems, Malaga, Spain.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ASAKAWA, T. et al., To appear in the Proceedings of the World Conference on Educational Multimedia, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Textable Movie: improvising with a personal movie database Catherine Vaucelle * Glorianna Davenport 
 Tristan Jehan Media Lab Europe MIT Media Lab/Media Lab Europe MIT Media Lab/Media Lab Europe Story Networks 
Group Interactive Cinema Group/Story Hyperinstruments Group/Story Networks Group Networks Group Abstract 
This sketch presents a new approach to improvising moviesaccording to the inter-relationship between 
personal videos andthe story of an experience. Textable Movie is a graphical interfacethat invites a 
storyteller of any age to compose and visualizemovies, images and sound environments while writing a 
story; thesystem self-selects and self-edits movies in real time based ontextual input from the teller. 
Textable Movie aims to exalt the imagination of its authors (writer, and film-maker) by immersingthem 
in real time, in a co-constructed narration. 1 Introduction When Marcel Proust writes about having tea 
and cookies, he isinspired by having the experience himself, which brings backmemories to his mind. This 
is well known as the "Madeleine of Proust" phenomenon [Proust, 1913]. With Textable Movie, we would like 
to recreate this same phenomenon, by presentinginstantly to the users, videos from their own footage. 
Byimmersion into their own memories, they could become engagedinto telling rich, and passionate stories, 
based on past experience.It is inspired by a previous work that annotates images in order toretrieve 
them within a specific context [Lieberman, and Liu,2002] and it extends the concept to the making of 
movies with anumber of automatic functions. Video editing tools usuallyprovide the user with many parametric 
functions for creatingmovies, but do so with a certain constraint on spontaneity. With Textable Movie, 
the videos could stimulate the author's imagination while the experience is recalled. Even though noempirical 
data has been collected at this time, we could hypothesize that pictures may lead the fantasy of the 
user duringthis improvisation. 2 Description Textable Movie retrieves movie segments and sounds in a 
specified database, from analyzing textual input. It loads andplays them in real time while the story 
is being typed.Consequently, a novel movie gets created and generated in a verytransparent, and easy 
manner. The simplicity of use, andimmediate response could help focusing on the storytelling, ratherthan 
on the concern of technical editing. The system can easily beconnected to any personal movie database, 
and simply requires atext file with a series of descriptive keywords for each clip. Forexample, the following 
short keyword sequence [forest.mov,forest nature tree wood leaves Yosemite;] could describe a personal 
10-second video clip of the Yosemite park forest, called"forest.mov". The personal labeling is important 
as it allows theuser to give the medium his/her own meaning. The current versionalso features a series 
of simple commands, which add instantmanipulations of the movie being played. *e-mail: cati@media.mit.edu 
e-mail: gid@media.mit.edu e-mail: tristan@media.mit.edu These commands are typed directly in the text, 
and include:[closeup] to zoom in the frame, [faster] and [slower] to change thespeed rate, [loop] and 
[palindrome] to loop in a normal or palindrome fashion, and [spring], [summer], [fall], and [winter],to 
alter the overall coloration of the image. By deleting thecommand, the effect disappears (see Figure 
1). Figure 1. Examples of interactions with Textable Movie 3 Discussion and Future work Textable Movie 
has been presented to, and experimented by, manytellers at an open house of the Media Lab Europe in Dublin, 
andhas engaged them in various tales, although using the samefootage. The direct, and instant relationship 
between text andmovie seems to be quite effective, surprising, aesthetic, inspiring,and fun. The core 
engine of Textable Movie has already been used in otherapplications, e.g., a system that retrieves, and 
displays images,from analyzing text messages sent by cellular phone. In its futureversion, based on the 
online intercultural training research [Asakawa, et al, to appear], our system will be networked, andused 
as a multimedia tool to reflect onto someone else's culture. The communities (currently, adolescents 
from Dublin, Boston,and Sao Paulo) will share their movies, and will learn about theother's own perception 
of their environment.  References PROUST, M. 1913. À la recherche du temps perdu. Du côté de chez Swann. 
Remembrance of Things Past, 1922. LIEBERMAN, H, and LIU, H, May 2002. Adaptive Linking between Text and 
Photos Using Common Sense Reasoning, Conference on Adaptive Hypermedia and Adaptive Web Systems, Malaga, 
Spain. ASAKAWA, T. et al., To appear in the Proceedings of the World Conference on Educational Multimedia, 
2003. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965574</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Office voodoo]]></title>
		<subtitle><![CDATA[a real-time editing engine for an algorithmic sitcom]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965574</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965574</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP14209592</person_id>
				<author_profile_id><![CDATA[81100606713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lew]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Media Lab Europe, Story Networks Group]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>935346</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[AGAMANOLIS, S. 2001. Chapter 5: Viper. In Isis, Viper, Cabbage: New tools and strategies for designing responsive media, PhD thesis, Massachusetts Institute of Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>303028</ref_obj_id>
				<ref_obj_pid>302979</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[JOHNSON, M. P., WILSON, A., KLINE, C., BLUMBERG, B., BOBICK, A. 1999. Sympathetic Interfaces: Using a Plush Toy to Direct Synthetic Characters. In Proceedings of CHI 99.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[MANOVICH, L. 2001. The New Temporality: The Loop as a Narrative Engine, The Language of New Media. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Office Voodoo: a real-time editing engine for an algorithmic sitcom Michael Lew Media Lab Europe Story 
Networks Group lew@media.mit.edu Office Voodoo is an interactive film installation using exclusively 
live action footage and running on a real-time, shot-based editing engine that fluidly assembles the 
film as it is being watched, while respecting the conventions of continuity editing. Each character in 
the film is represented by a physical voodoo doll. As viewers manipulate these dolls, they affect the 
emotions of the people on screen. They can also call the people in the film using their phones.  Real-time 
editing engine In the tradition of lens-based, non-synthetic cinema, new forms of film for computational 
media are becoming possible as footage is freed from its inherently linear celluloid substrate : they 
are non­deterministic and algorithmic films [Manovich 2001]. But trying to achieve generativity and extended 
interactivity in the purely filmic genre is especially difficult, because the previously shot material 
is finite ; solving this problem comes about defining techniques by which footage can be recombined in 
different ways in real time. The approach taken in Office Voodoo is to use time-based multiplexing on 
a shot-based granularity level, i.e. continuity editing as it has been practiced since Griffith. It relies 
extensively on the creative use of automated editing [Agamanolis 2001], where the task of the interactive 
film editor is no longer to define univocally where cuts happen, but instead to script the editing rules 
and define the constraints and freedom that the viewer has in the exploration of the movie. The real-time 
editing engine works by assembling streams in parallel for each of the different characters (for example 
shot and reverse shot in our case) and cutting back and forth on a semi­random basis between these streams. 
It does so while avoiding jump cuts and insuring sound continuity, accomplishing L-cuts by simulating 
look-aheads in the streams. Shots that make up the streams are selected from an annotated clip database 
in real-time so that they match the desired turn of the story. This process makes use of loops, repetition, 
and footage repurposing : a single shot is re-used in multiple contexts, in a successful application 
of the Kulashov effect. Emotion-driven narrative Office Voodoo is an interactive sitcom featuring Frank 
and Nancy, two bored 30-something officemates, condemned to spend their lives in an office, in the vein 
of a Sartrian huis clos. The piece explores the influence of emotions as initial conditions in any social 
interaction; set as a satirical simulation of office life, it works as a social laboratory where it is 
like injecting drugs into the protagonists and seeing what would happen... if Frank is cranky and Nancy 
is depressed ? If Frank is lethargic and Nancy is flirtatious ? The underlying narrative structure of 
this infinite film is emotion­and character-driven : each character has an emotional state (euphoric, 
depressed, agitated, mellow..) that is represented as x-y coordinates in a two-dimensional valence/arousal 
emotional space as used by Shlosberg (1952). As a viewer shakes and squeezes a doll, he's changing the 
corresponding character's emotional state, thereby navigating this space. Therefore each particular experience 
of the film can be described as a trajectory across these emotional spaces. Finally, for certain emotional 
combinations of the two characters, pre-edited stories are triggered, starting a short non-interactive 
sequence. The semantics used to annotate the shots are essentially based on emotional state, type of 
shot and character. Interface design To control the film, custom tangible interfaces were designed : 
voodoo dolls representing the characters in the film [Johnson et al. 1999]. Force-sensitive resistors 
and accelerometers placed in the dolls measure activity, while vibrating motors and flashing LEDs provide 
visual and haptic feedback. Microcontrollers perform simple signal processing on the inputs and then 
relay the data to the real-time editing engine on the main CPU. The other interface is the phone : viewers 
have the office numbers of Frank and Nancy and can call them at any time. A two-line voice server picks 
up the calls and signals the editing engine. Phones and voodoo dolls were conceived as very credible, 
intuitive interfaces bridging the real world and the fictional world. As the installation was shown in 
festivals, we were pleased to notice that they actually reinforce the suspension of disbelief and accentuate 
the illusion of life of the characters in the film. By their nature, they also allow continuous intervention 
of the viewers, at any time during the film. Office Voodoo demonstrates that it is possible to craft 
compelling interactive narratives that only rely on live action footage, thus benefitting of the aesthetics 
of cinematography and unsynthesizable subtlety of real actors. Figure 1. The installation Figure 2. 
Frank &#38; Nancy Acknowledgements I would like to thank Glorianna Davenport as well as all the members 
of the Story Networks Group and of the Human Connectedness Group at Media Lab Europe. References AGAMANOLIS, 
S. 2001. Chapter 5 : Viper. In Isis, Viper, Cabbage: New tools and strategies for designing responsive 
media, PhD thesis, Massachusetts Institute of Technology. JOHNSON, M. P., WILSON, A., KLINE, C., BLUMBERG, 
B., BOBICK, A. 1999. Sympathetic Interfaces: Using a Plush Toy to Direct Synthetic Characters. In Proceedings 
of CHI 99. MANOVICH, L. 2001. The New Temporality: The Loop as a Narrative Engine, The Language of New 
Media. MIT Press. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
	<section>
		<section_id>965577</section_id>
		<sort_key>38</sort_key>
		<section_seq_no>38</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Motion capture]]></section_title>
		<section_page_from>38</section_page_from>
		<chair_editor>
			<ch_ed>
				<person_id>P571380</person_id>
				<author_profile_id><![CDATA[81100210477]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Neill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol]]></affiliation>
				<role><![CDATA[Session Chair]]></role>
				<email_address><![CDATA[]]></email_address>
			</ch_ed>
		</chair_editor>
	<article_rec>
		<article_id>965578</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Complex character animation that combines kinematic and dynamic control]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965578</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965578</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P641166</person_id>
				<author_profile_id><![CDATA[81100431086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shapiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14131760</person_id>
				<author_profile_id><![CDATA[81100370834]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Petros]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Faloutsos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383287</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[FALOUTSOS, P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for physics-based character animation. In Proceedings of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 251--260.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218414</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O'BRIEN, J. F. 1995. Animating human athletics. In Proceedings of SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, 71--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. ACM Transactions on Graphics 21, 3 (July), 473--482.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566604</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[LI, Y., WANG, T., AND SHUM, H.-Y. 2002. Motion texture: A two-level statistical model for character motion synthesis. ACM Transactions on Graphics 21, 3 (July), 465--472.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Complex Character Animation That Combines Kinematic and Dynamic Control Ari Shapiro* Petros Faloutsos 
University of California, Los Angeles University of California, Los Angeles 1 Motivation We implement 
a framework for developing interactive characters by combining kinematic animation with physical simulation. 
The combination of animation techniques allows the characters to ex­ploit the advantages of each technique. 
For example, characters can perform natural-looking kinematic gaits and react dynamically to unexpected 
situations. Kinematic techniques such as those based on motion capture data can create very natural-looking 
animation. However, motion cap­ture based techniques are not suitable for modeling the complex interactions 
between dynamically interacting characters. Physical simulation, on the other hand, is well suited for 
such tasks. Our work develops kinematic and dynamic controllers for interactive character animation, 
leveraging the advantages of both classes of techniques. 2 Kinematic and Dynamic Control We build upon 
our previous work on dynamic controller composi­tion [Faloutsos et al. 2001]. Within our framework, controllers 
are now black boxes that can encapsulate any kind of animation tech­nique. For example, they can be based 
on the direct application of motion capture, elaborate motion graph structures [Kovar et al. 2002; Li 
et al. 2002] or dynamic controllers [Hodgins et al. 1995; Faloutsos et al. 2001]. Our system can switch 
between kinematic animation and physi­cal simulation for any number of characters in the environment 
and determines which effects will be placed on which characters. For example, it may be desirable to 
fully animate one character hitting another by motion captured data and only simulate the impact of the 
force of the hit on the second character. In other instances, all characters can be dynamically simulated 
[Faloutsos et al. 2001]. The characters can react to unexpected circumstances through the use of dynamic 
controllers which are automatically activated based on the state of the character and the environment. 
For example, a character falling down will automatically put its hands down to cushion the impact. Our 
system properly matches the positions and the velocities for the degrees of freedom of the characters 
when it switches between kinematic animation and dynamic simulation. Our characters can react autonomously 
to changes in their envi­ronment and to user interaction as well as follow user instructions. The power 
of combining kinematic and dynamic control is demon­strated by the image sequence in Figure 1. Our system 
is capable of integrating current and future kine­matic and dynamic techniques since it imposes no restrictions 
on the structure of individual controllers. We are currently working on incorporating the motion graph 
technique [Kovar et al. 2002] as an individual controller into our system. * ashapiro@cs.ucla.edu pfal@cs.ucla.edu 
 Figure 1: Kinematically animated kick and dynamically simulated reaction and interaction. The motion 
of the character on the right is dynamically simulated. Images are in raster order. References FALOUTSOS, 
P., VAN DE PANNE, M., AND TERZOPOULOS, D. 2001. Composable controllers for physics-based character an­imation. 
In Proceedings of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 251 260. 
HODGINS, J. K., WOOTEN, W. L., BROGAN, D. C., AND O BRIEN, J. F. 1995. Animating human athletics. In 
Proceed­ings of SIGGRAPH 95, Computer Graphics Proceedings, Annual Conference Series, 71 78. KOVAR, L., 
GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. ACM Transactions on Graphics 21, 3 (July), 473 482. 
LI, Y., WANG, T., AND SHUM, H.-Y. 2002. Motion texture: A two-level statistical model for character motion 
synthesis. ACM Transactions on Graphics 21, 3 (July), 465 472. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965580</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Cloth motion capture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965580</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965580</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP15020811</person_id>
				<author_profile_id><![CDATA[81544189656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pritchard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14221002</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[GUSKOV, I. 2002. Efficient tracking of regular patterns on nonrigid geometry. In 16th International Conference on Pattern Recognition, vol. 2, 1057--1060.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851523</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[LOWE, D. 1999. Object recognition from local scale-invariant features. In International Conference on Computer Vision, 1150--1157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[PRITCHARD, D., AND HEIDRICH, W. 2003. Cloth motion capture. In Proceedings of Eurographics 2003, forthcoming.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cloth Motion Capture David Pritchard and Wolfgang Heidrich* University of British Columbia 1 Introduction 
Motion capture systems do not have enough degrees of freedom to capture moving cloth. Our system recovers 
the geometry and parameterisation of a sheet of moving cloth by using stereo corre­spondence and an extension 
of the scale-invariant feature transform (SIFT) introduced by Lowe [1999]. Previous work on cloth capture 
[Guskov 2002] required a checkerboard pattern to be printed on the cloth, and recovered much fewer features. 
 2 Approach A stereo vision system is used to capture a video sequence of mov­ing cloth, and produce 
a recti.ed disparity map for each frame. Us­ing a multiscale algorithm, we .ll holes and interpolate 
to generate a smoothly-varying .oating-point disparity map. To parameterise the surface, we use SIFT 
to detect scale and rotation-invariant features in one of the captured stereo images. Features are also 
detected in a scanned reference image of the .at­tened cloth. We use a novel seed-and-grow algorithm 
to match fea­tures in the stereo image to features in the reference image. We .rst .nd a seed feature 
in the captured image, and .nd the best matching reference feature. Each SIFT feature has an associ­ated 
vector, representing a coarse sampling of the local image gradi­ent. The Euclidean distance between SIFT 
feature vectors provides a reasonable metric for evaluating the quality of a match. From the matched 
seed feature, we grow outwards, .nding matches for adjacent features. We repeat this with multiple seeds 
until all fea­tures have been either matched or rejected as unmatchable. Cloth strongly resists stretching 
and compression, and our algorithm uses this as a constraint on the matching process, limiting stretch 
to a maximum of 10%. To do this, we consider pairs of matched fea­tures, and examine both the distance 
between reference features and the 3D geodesic distance between the matching captured features. Following 
matching, we use a voting scheme to verify the cor­rectness of matches. This helps to eliminate both 
bad seeds, and incorrect matches of individual features. The matching process allows us to establish 
the parametric co­ordinates of every feature. Features are sparse, so parameters must be interpolated 
if a dense parameterisation is desired. We perform an interpolation to yield a dense, regularly sampled 
parametric map corresponding directly to the disparity map, To reduce distortion, we ensure constant 
spacing of isoparametric curves without requir­ing perpendicularity of the curves. This .ts with the 
nature of cloth, which resists stretch but allows some shearing. 3 Results and Conclusions In Figure 
1 we show some results. In this 1024 × 768 captured image, 2532 features were successfully matched. Over 
the two sec­ond sequence of twenty images, an average of 2103 features were matched per frame. Our approach 
requires a distinct, non-repeating pattern to be printed on the cloth. SIFT s multiresolution nature 
allows features to be detected in the presence of motion blur, allowing even fast * e-mail: {drpritch,heidrich}@cs.ubc.ca 
 Figure 1: Top row: captured image; hole-.lled, smoothed disparity map. Bottom row: parameterised geometry 
with checkered texture; feature density in reference image (from blue=1 to red=35, with grey=0) movements 
to be captured. Our method will extend to calibrated camera systems with any number of cameras. Further 
details can be found in our paper [Pritchard and Heidrich 2003]. References GUSKOV, I. 2002. Ef.cient 
tracking of regular patterns on non­rigid geometry. In 16th International Conference on Pattern Recognition, 
vol. 2, 1057 1060. LOWE, D. 1999. Object recognition from local scale-invariant fea­tures. In International 
Conference on Computer Vision, 1150 1157. PRITCHARD, D., AND HEIDRICH, W. 2003. Cloth motion capture. 
In Proceedings of Eurographics 2003, forthcoming. Copyright held by the author  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>965579</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-27-2003</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[How to capture absolute human skeletal posture]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/965400.965579</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=965579</url>
		<abstract>
			<par><![CDATA[Commercially available motion capture products give us fairly precise movements of human body segments but do not measure enough information to define skeletal posture in its entirety. This sketch describes how to obtain the complete posture of skeletal structure with the help of marker locations relative to bones that are derived from MRI data sets.]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>PP43116754</person_id>
				<author_profile_id><![CDATA[81100138187]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwasawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ATR]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P653907</person_id>
				<author_profile_id><![CDATA[81100576835]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kiyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seikei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39025768</person_id>
				<author_profile_id><![CDATA[81100070056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nagoya University, ATR]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP18000233</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seikei University, ATR]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[LEVA, P. DE. 1996. Joint Center Longitudinal Positions Computed from a Selected Subset of Chandler's Data. Journal of Biomechanics, 29, 1231--1233.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[SPARGER, C. 1972. Anatomy and Ballet; 5th Edition. Theatre Arts Books.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 How to Capture Absolute Human Skeletal Posture Shoichiro Iwasawa* Kiyoshi Kojima ATR Seikei University 
 Abstract Commercially available motion capture products give us fairly precise movements of human body 
segments but do not measure enough information to define skeletal posture in its entirety. This sketch 
describes how to obtain the complete posture of skeletal structure with the help of marker locations 
relative to bones that are derived from MRI data sets. 1 Introduction Today, ordinary motion capture 
(mocap) products do not have the ability to measure the actual posture of an underlying skeleton. They 
only measure moment-to-moment displacements from the movements of markers attached to the skin surface 
or clothing. Each joint angle is calculated from these displacements based on simplified bone structure. 
In a typical post-processing of commercial mocap products, a user has to determine the correspondence 
between skeleton structure and marker position at an arbitrary initial posture. A more anatomically correct 
way to determine skeletal posture is based on certain types of knowledge such as the relative locations 
of surface landmarks and the hip joint s center location. In the field of biomechanics, various estimation 
methods [Leva 1996] were introduced based on the geometric relation of the bone and a landmark on the 
anatomical skin s surface. However, this landmark can only be observed by skilled professionals. The 
method is based on statistical analysis and depends on various factors such as race, age and gender. 
Furthermore, each individual is necessarily different from the statistical results if the estimation 
method is employed and individual characteristics are completely ignored. In addition, an estimation 
method for Europeans is obviously not suitable for Asians. As described above, the absolute posture of 
a skeleton cannot be determined through an ordinary mocap process. Such stringent determination is not 
so important in the entertainment industries because acceptable motions need not be scientifically realistic. 
On the other hand, capturing traditional motion like classical ballet requires highly detailed and precise 
posture for educational and archiving purposes. *mailto:shoichiro.iwasawa@acm.org mailto:mase@itc.nagoya-u.ac.jp 
mailto:shigeo@ee.seikei.ac.jp  Copyright held by the author Kenji Mase Shigeo Morishima Nagoya University 
Seikei University ATR ATR 2 Our Work Human motion of ballet dance is our primary target. Ballet dance 
includes extreme postures like keeping a foot tip higher than eye height [Sparger 1972]. That is why 
we chose ballet dance as a primary target motion. Highly reflective ball-like markers are attached to 
the dancer s skin surface. These markers are filled with oil for the reason described below. We have 
to observe the interior of a human body and markers on the surface at the same time. Computer tomography 
(CT) seems an appropriate solution for such a requirement. It has the capability to acquire images of 
the whole body all at once. However, a CT scanner works with X-ray, which can cause serious health problems. 
Therefore, magnetic resonance imaging (MRI) might be considered as an alternative. To use MRI, we focus 
on small volumes such as the shoulder or hip joint so that the scan area is strictly limited compared 
to CT. Mocap markers are specially designed for this purpose. Oil inside a marker can be easily identified 
in a MRI image as shown in Figure 1. Once MRI data sets of the target dancer are acquired, bone segments 
are then semi-automatically extracted. At specific bone segments with a few markers, the centroid of 
each marker is located by hand and represented relative to local coordinates on the attached bone segment. 
Describing a bone-to-marker transformation is most essential. We then perform a mocap operation on ballet 
dancing. After that, each marker is labeled, and its position is recovered in three-dimensional space 
by the conventional method. The 3D positions of markers and the bone­to-marker mapping description help 
to locate bones. Figure 2 illustrates recovered skeletal posture from a capture session of ballet. Figure 
2. Recovered Skeleton Structures. This research was supported in part by the Telecommunications Advancement 
Organization of Japan.  References LEVA, P. DE. 1996. Joint Center Longitudinal Positions Computed 
from a Selected Subset of Chandler s Data. Journal of Biomechanics, 29, 1231-1233. SPARGER, C. 1972. 
Anatomy and Ballet; 5th Edition. Theatre Arts Books. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>2003</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	</section>
</content>
</proceeding>
