<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/05/2012</start_date>
		<end_date>08/09/2012</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Los Angeles]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2343483</proc_id>
	<acronym>SIGGRAPH '12</acronym>
	<proc_desc>ACM SIGGRAPH 2012 Courses</proc_desc>
	<conference_number>2012</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-1678-1</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2012</copyright_year>
	<publication_date>08-05-2012</publication_date>
	<pages>1998</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>In SIGGRAPH 2012 Courses, attendees learn from the experts in the field and gain inside knowledge that is critical to career advancement. Courses are short (1.5 hours) or half-day (3.25 hours) structured sessions that often include elements of interactive demonstration, performance, or other imaginative approaches to teaching.</p> <p>The spectrum of Courses ranges from an introduction to the foundations of computer graphics and interactive techniques for those new to the field to advanced instruction on the most current techniques and topics. Courses include core curricula taught by invited instructors as well as Courses selected from juried proposals.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>2343484</article_id>
		<sort_key>10</sort_key>
		<display_label>Article No.</display_label>
		<pages>82</pages>
		<display_no>1</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Applying color theory to digital media and visualization]]></title>
		<page_from>1</page_from>
		<page_to>82</page_to>
		<doi_number>10.1145/2343483.2343484</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343484</url>
		<abstract>
			<par><![CDATA[<p>This course reviews the visual impact of specific color combinations and provides practical suggestions on digital color mixing for visualization.</p> <p>The course begins by describing the red, green, Blue (RGB) color model of color computer monitors (additive color) and the cyan, magenta, yellow and key black (CMYK) color model of printers and other output devices (subtractive color) and how to move between these two color models and other color models. Then it reviews color gamut, explaining that a color gamut and color model combine to define a color space, describes types of color systems such as the Munsell color system and the Pantone color-matching system, and introduces the color wheel and how to work with it to create digital media and visualizations.</p> <p>Other topics include: digital color application illustrated by digital media and visualization examples inspired by artistic movements such as Pointillism, Impressionism, Fauvism, Cubism, and Color Field Painting; concepts introduced by Johannes Itten for successful color combinations and Josef Albers for understanding the interaction of color; and how color theories and tools are applied actual visualization projects.</p> <p>The course concludes with a hands-on workshop session where attendees can work with online color scheme tools to build visualization color maps followed by a review of additional reading on applying color theory.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738878</person_id>
				<author_profile_id><![CDATA[81350590594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theresa-Marie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rhyne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
  IN THIS TUTORIAL, WE HIGHLIGHT 5 TOPICS: Additive and Subtractive Color Models. Defining Color Gamut, 
Spaces and Systems. Selected Artistic Movements related to Color Theory.  Case Studies pertaining to 
Colorizing Visualizations.  Hands on Workshop using Online Color Tools theresamarierhyne@gmail.com 2 
 FIRST, LET S START OUR TUTORIAL WITH A REVIEW OF ADDITIVE AND SUBTRACTIVE COLOR MODELS: Red, Green 
Blue (RGB) - adding colors with light as seen on our color display monitors and with digital cameras. 
 Cyan, Magenta, Yellow and Key Black (CMYK) - subtracting colors with ink as seen from high resolution 
printouts and with our local printing devices.  Red, Yellow and Blue (RYB) - subtracting colors with 
paint when we use a real paintbrush, crayons or makers.  theresamarierhyne@gmail.com RED, GREEN AND 
BLUE (RGB) -THE ADDITIVE COLOR MODEL OF LIGHTS:  Red, Green and Blue lights showing secondary colors. 
Open Source Image available at WIkipedia and created by en:User: Bb3dxv, see: http://en.wikipedia.org/wiki/File:RGB_illumination.jpg. 
 theresamarierhyne@gmail.com CYAN, MAGENTA,YELLOW AND KEY BLACK (CMYK) -THE SUBTRACTIVE COLOR MODEL 
OF PRINTING:  Layers of simulated glass show how semi-transparent Cyan, Magenta and Yellow colors combine 
on paper. Open Source Image available at WIkipedia and created by Mirsad Todorovac, see: http://en.wikipedia.org/wiki/File:Color­subtractive-mixing-cropped.png. 
 5 theresamarierhyne@gmail.com VISUALLY SUMMARIZING COLOR MODELS:  RGB adds with lights. RYB subtracts 
to mix paints. CMYK subtracts for printing. theresamarierhyne@gmail.com SOMETIMES OTHER COLOR MODELS 
ARE DEVELOPED TO SUPPORT SPECIFIC OUTPUT DEVICES:  Hexachrome (CMYKOG) - six color printing process 
once used: Cyan, Magenta, Yellow, Key Black, Orange and Green. theresamarierhyne@gmail.com  NEXT, WE 
DEFINE SOME COLOR TERMINOLOGY: Color Model + Color Gamut = Color Space. International Commission on 
Illumination Color Space (CIE XYZ). Munsell Color System. Pantone Color Matching  Web Colors: Hex Triplets 
 Hue, Saturation, Value (HSV) The Color Wheel and Color Schemes theresamarierhyne@gmail.com COLOR GAMUT: 
THE SUBSET OF COLORS THAT CAN BE ACCURATELY REPRESENTED IN A GIVEN CIRCUMSTANCE.  The Color Gamut of 
a typical computer monitor. The grayed out portion represents the entire color range available. Open 
Source Image available at WIkipedia and created by Hankwang, see: http://en.wikipedia.org/ wiki/File:CIExy1931_srgb_gamut.png. 
 theresamarierhyne@gmail.com 10 COLOR MODEL + COLOR GAMUT = COLOR SPACE.  RGB MODEL + COLOR GAMUT = 
COLOR SPACE Comparison of the color spectrum (shown as the large oval in the back) with RGB color spaces. 
This image shows that an Epson 2200 printer can produce colors outside sRGB and Adobe RGB color spaces. 
Open Source Image available at WIkipedia and created by Jeff Schewe, see: http://en.wikipedia.org/wiki/File:Colorspace.png. 
 theresamarierhyne@gmail.com COMPARISON OF RGB &#38; CMYK COLOR SPACES  Comparison of the RGB and CMYK 
colors models. This image depicts the differences between how colors appear on a color monitor (RGB) 
compared to how the colors reproduce in the CMYK print process. Public Domain Image available at WIkipedia 
and created by Annette Shacklett, see: http://en.wikipedia.org/wiki/ File:RGB_and_CMYK_comparison.png. 
 theresamarierhyne@gmail.com SOME REAL WORLD COMPARISONS OF COLOR SPACES.  CIBACHROME (ILFOCHROME) 
PHOTOGRAPH VERSUS PAPER COLLAGE Comparison of imagery from two color collage studies. The original collages 
were made by cut and pasting colorful papers together. Later a 35mm slide was taken of the paper collage 
and a Cibachrome (Ilfochrome) color print was created from the slide image. Notice the differences between 
the final colors of the paper imagery and the photograph imagery. Imagery created by Theresa-Marie Rhyne. 
 theresamarierhyne@gmail.com SOME REAL WORLD COMPARISONS OF COLOR SPACES.  COMPARISON OF COLOR PRINTOUTS 
AND COLOR PHOTOGRAPHS Here, we show two examples that compare an original color printout of a digital 
image with a photograph of a 35mm slide of the same digital image. Imagery created by Theresa-Marie Rhyne. 
 theresamarierhyne@gmail.com   ADDITIONAL RESOURCES ON COLOR MANAGEMENT: Real World Color Management, 
2nd Edition by Bruce Fraser, Chris Murphy and Fred Bunting (http://www.colorremedies.com/realworldcolor/). 
Real World Image Sharpening with Adobe Photoshop, Camera Raw, and Lightroom, 2nd Edition by Bruce Fraser 
and Jeff Schewe (http://www.adobepress.com/bookstore/product.asp?isbn=0321637550).  Color Management 
Web Site: (http://www.colormanagement.com/). theresamarierhyne@gmail.com CIE XYZ COLOR SPACE: FROM 
THE INTERNATIONAL COMMISSION ON ILLUMINATION  The CIE XYZ color space is a based on experimental perception 
studies conducted by W. David Wright and John Guild in the 1920s. The CIE 1931 XYZ color space,shown 
above, is designed for matching calibrated displays or printers. Open Source Image available at WIkipedia 
and created by Paulschou, see: http://en.wikipedia.org/w/index.php? title=File:Chromaticity_diagram_full.pdf&#38;page=1. 
 16 theresamarierhyne@gmail.com UPDATES TO CIE XYZ COLOR SPACE: CIE 1931 was limited in expressing 
lightness, purity and dominant wavelength between colors. In 1976, CIE evolved two systems to address 
uniform color spacing: CIELUV and CIELAB. CIE LUV is designed as a simple to compute transformation 
of CIE XYZ 1931 to address perceptual uniformity, L, u &#38; v are calculated from chromaticity coordinates. 
 CIE LUV is well suited for computer graphics, for more mathematics details see: Computer Graphics: Principles 
and Practice in C (2nd Edition) J. Foley, A. van Dam, S. Feiner, and J. Hughes. With CIE LAB (CIE Lab), 
L = lightness coordinate, a = red/green coordinate, and b = yellow/blue coordinate. For more specifics 
on CIE LAB, see Gernot Hoffmann s discussion at: http://www.fho-emden.de/~hoffmann/cielab03022003.pdf. 
CIE LAB is closely related to the Munsell Color System. theresamarierhyne@gmail.com MUNSELL COLOR SYSTEM: 
A HUE, VALUE AND CHROMA COLOR SPACE  Hue: 5 principal hues of Red, Yellow, Green, Blue and Purple with 
5 intermediate hues halfway between each principal. Each of these 10 steps is divided into 10 sub-steps 
to yield 100 hues with integer values.  Value: black (value 0) at the bottom to white (value 10) at 
the top.  Chroma: measured radially from the center of each slice.  Lower chroma value is less pure, 
more washed out like a pastel.    MUNSELL COLOR SYSTEM: A HUE, VALUE AND CHROMA COLOR SPACE  Open 
Source Image available at WIkipedia and created by Jacobolus, see: http:// en.wikipedia.org/wiki/File:Munsell­system.svg 
Open Source Image available at WIkipedia and created by SharkD, see: http:// commons.wikimedia.org/wiki/File:Munsell_1929_color_solid.png. 
 19 theresamarierhyne@gmail.com PANTONE COLOR MATCHING SYSTEM: USED FOR STANDARDIZING COLORS  1,114 
colors specified by their allocated number such as PMS 130 . Colors based on 15 pigments (13 base color 
pigments along with black &#38; white) that are mixed in specified amounts.  Aids in standardizing 
colors in the CMYK color printing process. CMYK printing effectively reproduces a special subset of 
Pantone colors.  Pantone online: www.pantone.com  Open Source Image available at WIkipedia and created 
by my PANTONE app : http://www.pantone.com/ Parhamr, see: http://en.wikipedia.org/wiki/ pages/pantone/pantone.aspx?pg=20696 
File:PantoneFormulaGuide-solidMatte-2005edition.png 20 theresamarierhyne@gmail.com  WEB COLORS: HEX 
TRIPLETS  A hex triplet: the 6 digit, 3-byte hexadecimal number used in HTML, CSS, SVG and other web 
focused applications to represent colors.  A byte: a number in the range 00 to FF (hexadecimal notation) 
or 0 to 255 in decimal notation.  The bytes represent red, green, and blue components of color.  Byte 
1: red value Byte 2: green value Byte 3: blue value If any one of the 3 color values is less than 10 
hex (or 16 decimal), it must be represented with a 0 so that the triplet always has 6 digits.  Image 
from the World Wide Web Consortium (W3C) s CSS Color Module Level 3 Recommendation (standard) -07 June 
2011, (http://www.w3.org/TR/css3-color/#svg-color) "Copyright &#38;#169; 07 June 2011 World Wide Web 
Consortium, (Massachusetts Institute of Technology, European Research Consortium for Informatics and 
Mathematics, Keio University). All Rights Reserved. http:// www.w3.org/Consortium/Legal/2002/copyright-documents-20021231" 
theresamarierhyne@gmail.com THE COLOR WHEEL: ARRANGING COLORS HUES AROUND A CIRCLE  Primary Colors: 
set of colors combined to make a useful range of colors. RGB, CMY, and RYB are the most popular sets 
of primary colors.  Secondary Colors: produced by mixing primary colors. For RGB: Yellow, Cyan, Magenta 
 For CMY: Blue, Green, Red For RYB: Orange, Green, Purple  Complementary Colors: colors opposite each 
other on the color wheel.   Public Domain Image available at WIkipedia and created by J. Arthur H. 
Hatt for The Colorist in 1908. See: http:// en.wikipedia.org/wiki/File:RGV_color_wheel_1908.png. USING 
THE COLOR WHEEL TO BUILD COLOR SCHEMES  Monochromatic: Different tints or shades of one color. Analogous: 
colors adjacent to each other on the Color Wheel.  Split Analogous: a main color and two colors one 
space away from it on each side of the Color Wheel.  Complementary: colors opposite each other on the 
Color Wheel.  Split Complementary: a main color and two colors on each side of its complementary color. 
 Triadic: 3 colors equally spaced on the Color Wheel.  Tetradic: Any 4 colors with a logical relationship 
on the Color Wheel such as 2 complementary pairs.  Others to be discussed in our Hands on Session. Public 
Domain Image available at WIkipedia and created by J. Arthur H. Hatt for The Colorist in 1908. See: http://en.wikipedia.org/wiki/ 
File:RGV_color_wheel_1908.png.    24 theresamarierhyne@gmail.com  NOW, LET S EXAMINE SELECTED ARTISTIC 
MOVEMENTS PERTAINING TO COLOR THEORY: Pointillism (as well as Impressionism, Divisionism and Ben Day 
Dots). Fauvism. Color Field Painting. Bauhaus teachings of Color Theory. theresamarierhyne@gmail.com 
POINTILLISM: BUILDING AN IMAGE FROM SEPARATE DOTS OF PAINT  Technique of painting developed by George 
Seurat in 1886. Image on right is from his La Parade de Cirque painting. Relies on the eye and mind 
of the viewer to compose the color dots into a broader range of tones.  Pointillism is an outgrowth 
of the Impressionism art movement. Impressionism paintings noted for visible brush strokes &#38; emphasis 
on light.  Divisionism is a variant of Pointillism that   focuses on color theory. Public Domain 
Image available at WIkipedia (expired Copyright) , see: http://en.wikipedia.org/wiki/File:Seurat­La_Parade_detail.jpg 
  MANY OTHER METHODS ARE ANALOGOUS TO POINTILLISM  The CMYK printing process used by color printers 
is dot based.  Television and Computer Monitors use a pointillist method to represent image with the 
RGB color model.  Ben-Day Dots printing process uses colored dots that are widely spaced, closely spaced 
or overlapping to create optical illusions.  The illustrator Benjamin Day developed the method that 
has been used in comic books. The artist, Roy Lichtenstein, enlarged and exaggerated the method.  Image 
created by Theresa-Marie Rhyne to study Divisionism and Ben Day dots. See: http:// web.me.com/tmrhyne/Theresa-Marie_Rhynes_Viewpoint/Blog/Entries/ 
2009/12/3_Exploring_Divisionism_in_Computer_Graphics.html. FAUVISM: STRONG COLOR EMPHASIZED OVER REPRESENTATIONAL 
OR REALISM  Short lived art movement from 1905 to 1907. Leading artists were Henri Matisse and Andre 
Derain. Image on right is Matisse s Woman with a Hat oil painting created in 1905.  The grouping of 
artists were called Les Fauves or The Wild Beasts .  Painterly qualities of wild brush work and saturated 
color. The subject matter was simplified and sometimes abstract.  Public Domain Image in the USA available 
at WIkipedia (image created before 1923) , see: http://en.wikipedia.org/ wiki/File:Matisse-Woman-with-a-Hat.jpg. 
 COLOR FIELD PAINTING: LARGE AMOUNTS OF FLAT SOLID COLOR SPREAD ACROSS A CANVAS Abstract Expressionism 
art movement that emerged during the 1940s and continued into the 1950s and 1960s. Leading artists were 
/ are Kenneth Noland, Gene Davis, Ellsworth Kelly, Helen Frankenthaler, Anne Truitt, Jack Bush, and Frank 
Stella. Image on right is Frank Stella s Ragga II painting created in 1970. Large canvas areas of pure 
color that created areas of unbroken surface and a flat picture plane. Fair Use Image in the USA taken 
by Theresa-Marie Rhyne. Ragga II painting and Copyright by Frank Stella, 1970. Dimensions: 120 by 300 
inches. In the collection of the North Carolina Museum of Still an active abstract painting style today. 
Art, see: http://collection.ncartmuseum.org/collection11/view/ objects/asitem/People$0040185/0? t:state:flow=57b04465-42fb-4180-ab3b-b7063019d013. 
 NEXT, WE HIGHLIGHT SELECTED BAUHAUS TEACHINGS OF COLOR THEORY:  The Bauhaus: School in Germany with 
pioneering approaches to teaching design. School operated from 1919 to 1933. Many instructors continued 
developing their teachings after 1933. Paul Klee: The Diaries of Paul Klee 1898 -1918 . Wassily Kandinsky: 
Concerning the Spiritual in Art .  Johannes Itten: The Art of Color: the subjective experience and objective 
rationale of color .  Josef Albers: The Interaction of Color . theresamarierhyne@gmail.com  SPECIFIC 
FOCUS ON JOSEF ALBERS:  German born American artist &#38; educator. Taught at the Bauhaus, eventually 
immigrating to the USA to teach at Black Mountain College &#38; Yale University.  Created color studies 
entitled Homage to the Square for a 25 year period starting ~1950.  In 1963, published The Interaction 
of Color detailing his color theories.  Fair Use Image in the USA taken by Theresa-Marie Rhyne. Homage 
to the Square color studies created by Josef Albers and on display at the North Carolina Museum of Art. 
Copyright by the Josef &#38; Anni Albers estate &#38; foundation. http://collection.ncartmuseum.org/collection11/view/ 
objects/asitem/People$0040176/4;jsessionid=8A427C773907338158799E73A053A5C7? t:state:flow=276f0018-1877-46d0-9c16-6cb0ad65949d. 
 theresamarierhyne@gmail.com CASE STUDY #1: COLORIZING HOUSEHOLD BROADBAND AVAILABILITY  Visualization 
based on Household Broadband Availability data for the 100 Counties in the State of North Carolina from 
the years of 2002 through 2007. Information and data provided by the e-NC Authority. theresamarierhyne@gmail.com 
  COLORBREWER S COLOR SCHEME CONCEPTS:  Sequential Schemes: optimized for ordered data from low to 
high.  Diverging Schemes: places equal emphasis on mid-range critical values as well as extreme values. 
 Qualitative Schemes: does not imply magnitude differences and suited for representing nominal or categorial 
data.  The ColorBrewer tool was conceptualized with color schemes by Cynthia A. Brewer with interface 
design and software development by Mark Harrower and others (both in the Department of Geography at Pennsylvania 
State University). See: (http://colorbrewer2.org/). theresamarierhyne@gmail.com NOW, LET S USE ADOBE 
S KULER TOOL TO ANALYZE THE COLORS IN OUR BROADBAND AVAILABILITY VISUALIZATION:  Adobe s Kuler tool 
allows us to analyze the colors in a JPEG image. We can save the resulting color palettes for future 
work. See: http://kuler.adobe.com/. 35 theresamarierhyne@gmail.com  CASE STUDY #2: COLORIZING A HURRICANE 
 Visualization based on a Hurricane Katrina model run at 2 kilometer grid resolution using the Weather 
Research Forecast (WRF) model. The animation shows rain isosurfaces, with the purple areas being locations 
of heaviest rainfall. Dark blue areas are land masses. 37 theresamarierhyne@gmail.com  HERE, WE HIGHLIGHT 
3 TOPICS: Applying Color Theory to a time series animation of Hurricane Katrina. Using Adobe s Kuler 
tool to analyze an existing Color Scheme. Working with the Color Brewer tool to build the Tropical Storm 
Animation Color Scheme. theresamarierhyne@gmail.com FIRST, LET S USE ADOBE S KULER TOOL TO ANALYZE THE 
COLORS IN OUR HURRICANE VISUALIZATION:  Adobe s Kuler tool allows us to analyze the colors in a JPEG 
image. We can save the resulting color palettes for future work. See: http://kuler.adobe.com/. 39 theresamarierhyne@gmail.com 
  KEY ELEMENTS OF COLOR MAP DESIGN : Establish Color Maps based on flow of Animation Sequences rather 
than Static Image Displays. ColorBrewer tool helps to Mock-Up Color Maps. From the Mock-Up develop the 
Final Color Maps with the visualization &#38; animation tool (VisIt). theresamarierhyne@gmail.com FRAME 
FROM ANIMATION SEQUENCE SHOWING COLOR MAPS:  Visualization programming by Steve Chall with Colorization 
executed by Theresa-Marie Rhyne at RENCI@NCSU. Created in the VisIt open source visualization tool from 
weather model data based on Hurricane Katrina, see: (http:www.llnl.gov/ VisIt). 43 theresamarierhyne@gmail.com 
A COLOR SCHEME ANALYSIS OF OUR HURRICANE  Using Color Scheme Designer, we see that our hurricane colors 
form an analogous color scheme of Magenta, Purple and Blue. Our wind vectors, in Orange, from a complementary 
color scheme to our Blue ocean background. theresamarierhyne@gmail.com SNAPSHOT OF ANIMATION SEQUENCE 
EVOLVING IN A NON-LINEAR EDITING SYSTEM:  Time series based on Hurricane Katrina model run at a 2 kilometer 
grid resolution using the Weather Research Forecast (WRF) model. The animation shows rain isosurfaces, 
with the purple areas being locations of heaviest rainfall. Dark blue areas are land masses. theresamarierhyne@gmail.com 
CASE STUDY #3: VISUALIZING CORRELATION IN MOLECULAR  BIOLOGICAL DATA  Using tiled scatter plot displays 
with complementary color schemes, coordinated patterns of correlation and anti­correlation are visible. 
 Reference: Visualizing Global Correlation in Large-Scale Molecular Biological Data , A.N.M. Imroz Choudhury, 
Kristin Potter, Theresa-Marie Rhyne, Yarden Livnat, Chris R. Johnson, and Olry Alter, Scientific Computing 
and Imaging Institute, University of Utah, a poster presentation at BioVis 2011. theresamarierhyne@gmail.com 
 HERE, WE HIGHLIGHT 2 TOPICS: Selecting Complementary Color Schemes with Adobe s Kuler Tool. Verifying 
Color Blindness Concerns with Vischeck. theresamarierhyne@gmail.com  TESTING COMPLEMENTARY COLORS SCHEMES: 
 Complementary Schemes: Colors opposite each other on the Color Wheel  Creates high contrast to designate 
correlation and anti­correlation variables.  Here, we show traditional color maps used in biological 
research.  Adobe s Kuler tool is used to  help create color schemes, see: (http://kuler.adobe.com/). 
 theresamarierhyne@gmail.com  MUTED COLOR SCHEMES TO ADDRESS COLOR BLIND ISSUES: 3 types of Color 
Blindness: Deuteranope, Protanope, Tritanope. Use Vischeck simulations to evaluate results. (http://www.vischeck.com) 
 Here, we show results for the red-green color scheme. Deuteranope and Protanope are most sensitive to 
these color schemes.  theresamarierhyne@gmail.com  MUTED COLOR SCHEMES TO ADDRESS COLOR BLIND ISSUES: 
 3 types of Color Blindness: Deuteranope, Protanope, Tritanope. Use Vischeck simulations to evaluate 
results. (http://www.vischeck.com) Here, we show results for the blue - yellow color scheme. Tritanope 
is most sensitive to these color schemes.  theresamarierhyne@gmail.com  ALTERNATIVE COLOR SCHEME: 
BLUE-RED COLORS  3 types of Color Blindness: Deuteranope, Protanope, Tritanope. Use Vischeck simulations 
to evaluate results. (http://www.vischeck.com) Here, we show results for the blue - red color scheme. 
 3 types of Color Blindness: Deuteranope, Protanope, Tritanope.  theresamarierhyne@gmail.com  CONTRASTING 
COLOR SCHEME: GREEN-PURPLE COLORS  Use Vischeck simulations to evaluate results. (http://www.vischeck.com) 
 Here, we show results for the green - purple color scheme. theresamarierhyne@gmail.com  SELECTED COMPLEMENTARY 
COLORS SCHEME:  See: (http://kuler.adobe.com/). theresamarierhyne@gmail.com CASE STUDY #4: COLORIZING 
A SUPERNOVA  Visualization based on astrophysics data of a supernova shock wave. The computational model 
was executed on a high performance computer and visualized with CEI s Ensight Visualization software. 
(http://www.ensight.com)  theresamarierhyne@gmail.com  HERE, WE HIGHLIGHT 2 TOPICS:  Building the 
analogous and complementary color schemes with Color Scheme Designer.  Using Adobe s Kuler tool to analyze 
an existing Color Scheme.  theresamarierhyne@gmail.com   BUILDING ANALOGOUS COLORS: Analogous Colors 
are color adjacent to each other on the Color Wheel. Color Scheme Designer has an Analogic option to 
help select analogous colors. theresamarierhyne@gmail.com  BUILDING COMPLEMENTARY COLORS: Complementary 
Colors are color across each other on the Color Wheel. Color Scheme Designer has an Complement option 
to help select complementary colors. theresamarierhyne@gmail.com COMBINING ANALOGOUS &#38; COMPLEMENTARY 
COLOR SCHEMES  We use the analogous color scheme of Yellow, Green and Blue to colorize our Super Nova 
object. Next, we use Color Scheme designer to find the complementary color to Yellow. Our intent is to 
emphasize the ring of data values surrounding the Super Nova with this complementary color. Color Scheme 
Designer assists us in showing the complementary color to be Blue-Purple. theresamarierhyne@gmail.com 
NOW, LET S USE ADOBE S KULER TOOL TO ANALYZE THE COLORS IN OUR SUPERNOVA VISUALIZATION:  Adobe s Kuler 
tool allows us to analyze the colors in a JPEG image. We can save the resulting color palettes for future 
work. See: http://kuler.adobe.com/. 59 theresamarierhyne@gmail.com  HANDS ON WORKSHOP: USING ONLINE 
COLOR TOOLS Adobe s Kuler Tool: http://kuler.adobe.com/ Color Scheme Designer: http://colorschemedesigner.com/ 
Colorbrewer: http://colorbrewer2.org/ ColorSchemer Touch: http://www.colorschemer.com/touch_info.php 
 theresamarierhyne@gmail.com HANDS ON WORKSHOP: A DATA VISUALIZATION EXAMPLE  Adobe s Kuler Tool: http://kuler.adobe.com/ 
Color Scheme Designer: http://colorschemedesigner.com/ Colorbrewer: http://colorbrewer2.org/ ColorSchemer 
Touch: http://www.colorschemer.com/touch_info.php theresamarierhyne@gmail.com  USING ADOBE S KULER 
TOOL: http://kuler.adobe.com/ Register &#38; Establish an account with Adobe  Login to Adobe Kuler. 
 Select Create Option.  Move 5 sensors to select colors.  Establish Color Palette.  Name Color Palette 
under Title .  Save Color Palette.  theresamarierhyne@gmail.com VISUAL RESULTS WITH OUR JPEG EXAMPLE: 
 Adobe s Kuler tool allows us to analyze the colors in a JPEG image. We can save the resulting color 
palettes for future work. See: http://kuler.adobe.com/. 64 theresamarierhyne@gmail.com  USING COLOR 
SCHEME DESIGNER: http://colorschemedesigner.com/ Use Browser to go to Color Scheme Designer Site.  
Select primary Color Scheme for Project. (Our Example uses a Tetrad Scheme)  Establish Color Options 
(In our Example: Orange, Yellow, Blue &#38; Green )  Select Color List to view selected Colors.  Establish 
&#38; Save Color Palette.  theresamarierhyne@gmail.com  USING COLORBREWER: http://colorbrewer2.org/ 
 Use Browser to go to Colorbrewer Site. (Requires Flash Plugin)  Select number of data classes . (Our 
Example uses 4 data classes)  Establish nature of your data . (In our Example: Diverging )  Select 
Color Scheme &#38; view selected Colors.  Pick a Color System &#38; Save Color Palette.  68 theresamarierhyne@gmail.com 
  USING COLORSCHEMER TOUCH: http://www.colorschemer.com/touch_info.php  Select ColorSchemer App on 
your iPhone/iPod Touch/iPad. (This is a free app from the iTunes online store.)  Select + Option to 
view Create Palette Color Wheel Screen.  Select PhotoSchemer to begin Photo Import &#38; Select Photos 
icon.  Import Photo from Photo Albums into Create Palette .  Brush over Photo to select colors &#38; 
Create Palette .  Establish &#38; Save Color Palette.  theresamarierhyne@gmail.com VISUAL RESULTS 
WITH OUR JPEG EXAMPLE:  ColorSchemer Touch allows us to analyze the colors in a JPEG image / Photo on 
our mobile device. We can save the resulting color palettes for future work. See: http://www.colorschemer.com/touch_info.php. 
 71 theresamarierhyne@gmail.com  USING ADOBE S KULER TOOL: http://kuler.adobe.com/ Register &#38; 
Establish an account with Adobe  Login to Adobe Kuler.  Select Create Option.  Move 5 sensors to select 
colors.  Establish Color Palette.  Name Color Palette under Title .  Save Color Palette.  theresamarierhyne@gmail.com 
 VISUAL RESULTS WITH OUR JPEG EXAMPLE:  Adobe s Kuler tool allows us to analyze the colors in a JPEG 
image. We can save the resulting color palettes for future work. See: http://kuler.adobe.com/. 74 theresamarierhyne@gmail.com 
  USING COLOR SCHEME DESIGNER: http://colorschemedesigner.com/ Use Browser to go to Color Scheme Designer 
Site.  Select primary Color Scheme for Project. (Our Example uses an Analogous Scheme)  Establish Color 
Options (In our Example: Orange, Red &#38; Pink )  Select Color List to view selected Colors.  Establish 
&#38; Save Color Palette.  theresamarierhyne@gmail.com VISUAL RESULTS WITH OUR JPEG EXAMPLE:  The 
Color Scheme Designer tool allows us to select a color scheme. We can save the resulting color palette 
for future work. See: http://colorschemedesigner.com/. 77 theresamarierhyne@gmail.com  USING COLORSCHEMER 
TOUCH: http://www.colorschemer.com/touch_info.php  Select ColorSchemer App on your iPhone/iPod Touch/iPad. 
(This is a free app from the iTunes online store.)  Select + Option to view Create Palette Color Wheel 
Screen.  Select PhotoSchemer to begin Photo Import &#38; Select Photos icon.  Import Photo from Photo 
Albums into Create Palette .  Brush over Photo to select colors &#38; Create Palette .  Establish &#38; 
Save Color Palette.  theresamarierhyne@gmail.com VISUAL RESULTS WITH OUR JPEG EXAMPLE:  ColorSchemer 
Touch allows us to analyze the colors in a JPEG image / Photo on our mobile device. We can save the resulting 
color palettes for future work. See: http://www.colorschemer.com/touch_info.php. 79 theresamarierhyne@gmail.com 
  IN SUMMARY, WE HIGHLIGHTED 5 TOPICS IN OUR COURSE: Additive and Subtractive Color Models. Defining 
Color Gamut, Spaces and Systems. Selected Artistic Movements related to Color Theory.  Case Studies 
pertaining to Colorizing Visualizations.  Hands on Workshop using Online Color Tools. theresamarierhyne@gmail.com 
80  OTHER RESOURCES ON COLOR THEORY: A Field Guide to Digital Color by Maureen Stone (http://www.stonesc.com/book/index.htm) 
The Interaction of Color by Josef Albers (http://yalepress.yale.edu/yupbooks/book.asp?isbn=9780300115956) 
My recently published Book Chapter: (http://theresamarierhyne.com/Theresa-Marie_Rhynes_Viewpoint/ Color_Theory_Book_Chapter.html) 
 theresamarierhyne@gmail.com  ACKNOWLEDGEMENTS: Thanks for to the ACM SIGGRAPH 2012 Courses Reviewers 
&#38; Committee for accepting my proposal. Thanks to all who attended this session during ACM SIGGRAPH 
2012 in Los Angeles, California.  Thanks to many colleagues in visualization &#38; digital media who 
have helped me learn so very much over many years.  with much gratitude &#38; appreciation... Theresa-Marie 
Rhyne theresamarierhyne@gmail.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343485</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>109</pages>
		<display_no>2</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Introduction to modern OpenGL programming]]></title>
		<page_from>1</page_from>
		<page_to>109</page_to>
		<doi_number>10.1145/2343483.2343485</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343485</url>
		<abstract>
			<par><![CDATA[<p>OpenGL is the most widely available library for creating interactive computer graphics applications across all of the major computer operating systems. Its applications range from creating systems for scientific visualization to computer-aided design, interactive gaming, and entertainment, and with each new version, its capabilities reveal the most up-to-date features of modern graphics hardware. This course provides an accelerated introduction to programming OpenGL, emphasizing current methods for using the library. While there have been numerous courses on OpenGL in the past, the recent sequence of revisions to the API, culminating in OpenGL version 4.2, provide a wealth of new functionality and features for creation of ever-richer content.</p> <p>In recent years, OpenGL has undergone numerous updates that have fundamentally changed how programmers interact with the application programming interface (API) and the skills required for being an effective OpenGL programmer. The most notable of those changes are the introduction of shader-based rendering, which has expanded to subsume almost all functionality in OpenGL, and the depracation of immediate-mode functions. Course attendees are introduced to each of the shader stages in OpenGL version 4.2, along with methods for specifying data to be used in rendering with OpenGL.</p> <p>The course begins with an overview of the complete OpenGL pipeline, introducing all the latest shader stages. Then it focuses on the shader-based pipeline, which requires an application to provide both a vertex shader and a fragment shader. It also includes an summary of key graphics concepts: the synthetic-camera model, transformations, viewing, and lighting.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Shading</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.2.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007</concept_id>
				<concept_desc>CCS->Software and its engineering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Documentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P3738879</person_id>
				<author_profile_id><![CDATA[81100366265]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Edward]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Angel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of New Mexico]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738880</person_id>
				<author_profile_id><![CDATA[81322506343]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dave]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shreiner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ARM, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
   The focus of the course is on programming with a fully shader-based OpenGL. Thus we will developing 
applications from scratch emphasizing the differences between the older immediate mode (pre 3.1) versions 
of OpenGL and the present versions (3.1-4.3). The code we develop can be ported easily to OpenGL ES 2.0 
and WebGL.  OpenGL is a library of function calls for doing computer graphics. With it, you can create 
interactive applications that render high-quality color images composed of 3D geometric objects and images. 
Additionally, the OpenGL API is window and operating system independent. That means that the part of 
your application that draws can be platform independent. However, in order for OpenGL to be able to render, 
it needs a window to draw into. Generally, this is controlled by the windowing system on whatever platform 
you are working on.  While OpenGL has been around for close to 20 years, a lot of changes have occurred 
in that time. This course concentrates on the latest versions of OpenGL. In these modern versions of 
OpenGL (which we defined as versions starting with version 3.1), OpenGL applications are shader based. 
In fact most of this course will discuss shaders and the operations they support. If you re familiar 
with previous versions of OpenGL, or other rasterization-based graphics pipelines that may have included 
fixed-function processing, we won t be covering those techniques since these functions have been deprecated. 
Instead, we ll concentrate on showing how we can implement those techniques on a modern, shader-based 
graphics pipeline. In this modern world of OpenGL, all applications will need to provide shaders, and 
as such, providing some perspective on how the pipeline evolved and its phases will be illustrative. 
We ll discuss this next.   The initial version of OpenGL was announced in July of 1994. That version 
of OpenGL implemented what s called a fixed-function pipeline, which means that all of the operations 
that OpenGL supported were fully-defined, and an application could only modify their operation by changing 
a set of input values (like colors or positions). The other point of a fixed-function pipeline is that 
the order of operations was always the same that is, you can t reorder the sequence operations occur. 
This pipeline was the basis of many versions of OpenGL and expanded in many ways, and is still available 
for use. However, modern GPUs and their features have diverged from this pipeline, and support of these 
previous versions of OpenGL are for supporting current applications. If you re developing a new application, 
we strongly recommend using the techniques that we ll discuss. Those techniques can be more flexible, 
and will likely preform better than using one of these early versions of OpenGL since they can take advantage 
of the capabilities of recent Graphics Processing Units (GPUs).  While many features and improvements 
were added into the fixed-function OpenGL pipeline, designs of GPUs were exposing more features than 
could be added into OpenGL. To allow applications to gain access to these new GPU features, OpenGL version 
2.0 officially added programmable shaders into the graphics pipeline. This version of the pipeline allowed 
an application to create small programs, called shaders, that were responsible for implementing the features 
required by the application. In the 2.0 version of the pipeline, two programmable stages were made available: 
 vertex shading enabled the application full control over manipulation of the 3D geometry provided by 
the application  fragment shading provided the application capabilities for shading pixels (the terms 
classically used for determining a pixel s color).  OpenGL 2.0 also fully supported OpenGL 1.X s pipeline, 
allowing the application to use both version of the pipeline: fixed-function, and programmable.  Until 
OpenGL 3.0, features have only been added (but never removed) from OpenGL, providing a lot of application 
backwards compatibility (up to the use of extensions). OpenGL version 3.0 introduced the mechanisms for 
removing features from OpenGL, called the deprecation model. It defines how the OpenGL design committee 
(the OpenGL Architecture Review Board (ARB) of the Khronos Group) will advertise of which and how functionality 
is removed from OpenGL. You might ask: why remove features from OpenGL? Over the 15 years to OpenGL 3.0, 
GPU features and capabilities expanded and some of the methods used in older versions of OpenGL were 
not as efficient as modern methods. While removing them could break support for older applications, it 
also simplified and optimized the GPUs allowing better performance. Within an OpenGL application, OpenGL 
uses an opaque data structure called a context, which OpenGL uses to store shaders and other data. Contexts 
come in two flavors: full contexts expose all the features of the current version of OpenGL, including 
features that are marked deprecated.  forward-compatible contexts enable only the features that will 
be available in the next version of OpenGL (i.e., deprecated features pretend to be removed), which can 
help developers make sure their applications work with future version of OpenGL.  Forward-compatible 
contexts are available in OpenGL versions from 3.1 onwards.  OpenGL version 3.1 was the first version 
to remove deprecated features, and break backwards compatibility with previous versions of OpenGL. The 
features removed from included the old-style fixed-function pipeline, among other lesser features. One 
major refinement introduced in 3.1 was requiring all data to be placed in GPU-resident buffer objects, 
which help reduce the impacts of various computer system architecture limitations related to GPUs. While 
many features were removed from OpenGL 3.1, the OpenGL ARB realized that to make it easy for application 
developers to transition their products, they introduced an OpenGL extensions, GL_ARB_compatibility, 
that allowed access to the removed features.  Until OpenGL 3.2, the number of shader stages in the OpenGL 
pipeline remained the same, with only vertex and fragment shaders being supported. OpenGL version 3.2 
added a new shader stage called geometry shading which allows the modification (and generation) of geometry 
within the OpenGL pipeline.  In order to make it easier for developers to choose the set of features 
they want to use in their application, OpenGL 3.2 also introduced profiles which allow further selection 
of OpenGL contexts. The core profile is the modern, trimmed-down version of OpenGL that includes the 
latest features. You can request a core profile for a Full or Forward-compatible profile. Conversely, 
you could request a compatible profile, which includes all functionality (supported by the OpenGL driver 
on your system) in all versions of OpenGL up to, and including, the version you ve requested.  The OpenGL 
4.X pipeline added another pair of shaders (which work in tandem, so we consider it a single stage) for 
supporting dynamic tessellation in the GPU. Tessellation control and tessellation evaluation shaders 
were added to OpenGL version 4.0. The current version of OpenGL is 4.3, which includes some additional 
features over the 4.0 pipeline, but no new shading stages.  WebGL is becoming increasingly more important 
now that it is supported by most browsers, including Mozilla and Chrome. Besides the advantage of being 
able to run without recompilation across platforms, it can easily be integrated with other Web applications 
and make use of a variety of portable packages available over the Web.   To begin, let us introduce 
a simplified model of the OpenGL pipeline. Generally speaking, data flows from your application through 
the GPU to generate an image in the frame buffer. Your application will provide vertices, which are collections 
of data that are composed to form geometric objects, to the OpenGL pipeline. The vertex processing stage 
uses a vertex shader to process each vertex, doing any computations necessary to determine where in the 
frame buffer each piece of geometry should go. The other shading stages we mentioned tessellation and 
geometry shading are also used for vertex processing, but we re trying to keep this simple. After all 
the vertices for a piece of geometry are processed, the rasterizer determines which pixels in the frame 
buffer are affected by the geometry, and for each pixel, the fragment processing stage is employed, where 
the fragment shader runs to determine the final color of the pixel. In your OpenGL applications, you 
ll usually need to do the following tasks: specify the vertices for your geometry  load vertex and 
fragment shaders (and other shaders, if you re using them as well)  issue your geometry to engage the 
OpenGL pipeline for processing Of course, OpenGL is capable of many other operations as well, many of 
which are outside of the  scope of this introductory course. We have included references at the end 
of the notes for your further research and development.  You ll find that a few techniques for programming 
with modern OpenGL goes a long way. In fact, most programs in terms of OpenGL activity are very repetitive. 
Differences usually occur in how objects are rendered, and that s mostly handled in your shaders. There 
four steps you ll use for rendering a geometric object are as follows: 1. First, you ll load and create 
OpenGL shader programs from shader source programs you create 2. Next, you will need to load the data 
for your objects into OpenGL s memory. You do this by creating buffer objects and loading data into them. 
 3. Continuing, OpenGL needs to be told how to interpret the data in your buffer objects and associate 
that data with variables that you ll use in your shaders. We call this shader plumbing. 4. Finally, 
with your data initialized and shaders set up, you ll render your objects  We ll expand on those steps 
more through the course, but you ll find that most applications will merely iterate through those steps. 
 While OpenGL will take care of filling the pixels in your application s output window or image, it 
has no mechanisms for creating that rendering surface. Instead, OpenGL relies on the native windowing 
system of your operating system to create a window, and make it available for OpenGL to render into. 
For each windowing system (like Microsoft Windows, or the X Window System on Linux [and other Unixes]), 
there s a binding library that lets mediates between OpenGL and the native windowing system. Since each 
windowing system has different semantics for creating windows and binding OpenGL to them, discussing 
each one is outside of the scope of this course. Instead, we use an open-source library named freeglut 
that abstracts each windowing system s specifics into a simple library. freeglut is a derivative of an 
older implementation called GLUT, and we ll use those names interchangeably. GLUT will help us in creating 
windows, dealing with user input and input devices, and other window-system activities. You can find 
out more about freeglut at its website: http://freeglut.sourceforge.net BothGLUTandfreeglut usedeprecatedfunctionsandshouldnotworkwithacore 
profile.OnealternativeisGLFWwhichrunsonWindows,LinuxandMacOSX.  Just like window systems, operating 
systems have different ways of working with libraries. In some cases, the library you link your application 
exposes different functions than the library you execute your program with. Microsoft Windows is a notable 
example where you compile your application with a .lib library, but use a .dll at runtime for finding 
function definitions. As such, your application would generally need to use operating-system specific 
methods to access functions. In general, this is troublesome and a lot of work. Fortunately, another 
open-source library comes to our aid, GLEW, the OpenGL Extension Wrangler library. It removes all the 
complexity of accessing OpenGL functions, and working with OpenGL extensions. We use GLEW in our examples 
to simplify the code. You can find details about GLEW at its website: http://glew.sourceforge.net  In 
OpenGL, as in other graphics libraries, objects in the scene are composed of geometric primitives, which 
themselves are described by vertices. A vertex in modern OpenGL is a collection of data values associated 
with a location in space. Those data values might include colors, reflection information for lighting, 
or additional coordinates for use in texture mapping. Locations can be specified on 2, 3 or 4 dimensions 
but are stored in 4 dimensional homogeneous coordinates. Vertices must be organized in OpenGL server-side 
objects called vertex buffer objects (also known asVBOs), which need to contain all of the vertex information 
for all of the primitives that you want to draw at one time. VBOs can store vertex information in almost 
any format (i.e., an array-of­structures (AoS) each containing a single vertex s information, or a structure-of-arrays 
(SoA) where all of the same type of data for a vertex is stored in a contiguous array, and the structure 
stores arrays for each attribute that a vertex can have). The data within a VBO needs to be contiguous 
in memory, but doesn t need to be tightly packed (i.e., data elements may be separated by any number 
of bytes, as long as the number of bytes between attributes is consistent). VBOs are further required 
to be stored in vertex array objects (known as VAOs). Since it may be the case that numerous VBOs are 
associated with a single object, VAOs simplify the management of the collection of VBOs.  To form 3D 
geometric objects, you need to decompose them into geometric primitives that OpenGL can draw. OpenGL 
only knows how to draw three things: points, lines, and triangles, but can use collections of the same 
type of primitive to optimize rendering. OpenGL  Primi3ve   Descrip3on Total Ver3ces for n Primi3ves 
  GL_POINTS Rendersinglepoint  pervertex(pointsmaybelargerthansinglepixel)   n GL_LINES Connect  eachpairofverAceswithasinglelinesegment. 
2n GL_LINE_STRIP Connect  eachsuccessivevertextothepreviousonewithlinesegment. n+1 GL_LINE_LOOP Connect 
 allverAcesinloopoflinesegments. n GL_TRIANGLES RendertriangleforeachtripleofverAces. 3n Rendertrianglefromthe.rst 
 threeverAcesinthelist,andthencreate GL_TRIANGLE_STRIPn+2 newtrianglewiththelast  tworenderedverAces,andthenewvertex. 
Createtrianglesbyusingthe.rst   GL_TRIANGLE_FAN vertexinthelist,andpairsofn+2successiveverAces.    
 The next few slides will introduce our first example program, one which simply displays a cube with 
different colors at each vertex. We aim for simplicity in this example, focusing on the OpenGL techniques, 
and not on optimal performance.  In order to simplify our application development, we define a few types 
and constants to make our code more readable and organized. Our cube, like any other cube, has six square 
faces, each of which we ll draw as two triangles. In order to sizes memory arrays to hold the necessary 
vertex data, we define the constant NumVertices. AddiAonally,aswe llseeinour.rst  shader,theOpenGLshadinglanguage,GLSL,hasbuilt-­-intypecalledvec4,whichrepresentsvectoroffour.oaAng-­-point 
 values.Wede.neC++  classforourapplicaAonthat  hasthesamesemanAcsasthat  GLSLtype.AddiAonally,tologicallyassociatetypeforourdata 
 withwhat  weintendtodowithit,weleverageC++  typedefstocreatealiasesforcolorsandposiAons.  In order 
to provide data for OpenGL to use, we need to stage it so that we can load it into the VBOs that our 
application will use. In your applications, you might load these data from a file, or generate them on 
the fly. For each vertex, we want to use two bits of data vertex attributes in OpenGL speak to help 
process each vertex to draw the cube. In our case, each vertex has a position in space, and an associated 
color. To store those values for later use in our VBOs, we create two arrays to hold the per vertex data. 
Note that we can organize our data in other ways such as with a single array with interleaved positions 
and colors.  In our example we ll copy the coordinates of our cube model into a VBO for OpenGL to use. 
Here we set up an array of eight coordinates for the corners of a unit cube centered at the origin. You 
may be asking yourself: Why do we have four coordinates for 3D data? The answer is that in computer graphics, 
it s often useful to include a fourth coordinate to represent three-dimensional coordinates, as it allows 
numerous mathematical techniques that are common operations in graphics to be done in the same way. In 
fact, this four-dimensional coordinate has a proper name, a homogenous coordinate. We could also use 
a point3 type, i.e. point2(-0.5, -0.5, 0.5) which will be stored in 4 dimensions on the GPU.  Just like 
our positional data, we ll set up a matching set of colors for each of the model s vertices, which we 
ll later copy into our VBO. Here we set up eight RGBA colors. In OpenGL, colors are processed in the 
pipeline as floating-point values in the range [0.0, 1.0]. Your input data can take any for; for example, 
image data from a digital photograph usually has values between [0, 255]. OpenGL will (if you request 
it), automatically convert those values into [0.0, 1.0], a process called normalizing values.  As our 
cube is constructed from square cube faces, we create a small function, quad(), which takes the indices 
into the original vertex color and position arrays, and copies the data into the VBO staging arrays. 
If you were to use this method (and we ll see better ways in a moment), you would need to remember to 
reset the Index value between setting up your VBO arrays.  Here we complete the generation of our cube 
s VBO data by specifying the six faces using index values into our original vertex_positions and vertex_colors 
arrays. It s worth noting that the order that we choose our vertex indices is important, as it will affect 
something called backface culling later. We ll see later that instead of creating the cube by copying 
lots of data, we can use our original vertex data along with just the indices we passed into quad() here 
to accomplish the same effect. That technique is very common, and something you ll use a lot. We chose 
this to introduce the technique in this manner to simplify the OpenGL concepts for loading VBO data. 
 Similarly to VBOs, vertex array objects (VAOs) encapsulate all of the VBO data for an object. This 
allows much easier switching of data when rendering multiple objects (provided the data s been set up 
in multiple VAOs). The process for initializing a VAO is similar to that of a VBO, except a little less 
involved. 1. First, generate a name VAO name by calling glGenVertexArrays() 2. Next,maketheVAO current 
bycallingglBindVertexArray().Similartowhat  wasdescribedforVBOs,you llcallthiseveryAmeyouwant  touseorupdatetheVBOscontainedwithinthisVAO. 
  The above sequence calls shows how to create and bind a VAO. Since all geometric data in OpenGL must 
be stored in VAOs, you ll use this code idiom often.  While we ve talked a lot about VBOs, we haven 
t detailed how one goes about creating them. Vertex buffer objects, like all (memory) objects in OpenGL 
(as compared to geometric objects) are created in the same way, using the same set of functions. In fact, 
you ll see that the pattern of calls we make here are similar to other sequences of calls for doing other 
OpenGL operations. In the case of vertex buffer objects, you ll do the following sequence of function 
calls: 1. Generate a buffer s name by calling glGenBuffers()   2. Next, you ll make that buffer the 
current buffer, which means it s the selected buffer for reading or writing data values by calling glBindBuffer(),withtypeofGL_ARRAY_BUFFER.Therearedi.erent 
 typesofbu.erobjects,withanarraybu.erbeingtheoneusedforstoringgeometricdata. 3. To initialize a buffer, 
you ll call glBufferData(), which will copy data from your application into the GPU s memory. You would 
do the same operation if you also wanted to update data in the buffer 4. Finally, when it comes time 
to render using the data in the buffer, you ll once again call glBindVertexArray() to make it and its 
VBOs current again. In fact, if you have multiple objects, each with their own VAO, you ll likely call 
glBindVertexArray() once per frame for each object.   The above sequence of calls illustrates generating, 
binding, and initializing a VBO with data. In this example, we use a technique permitting data to be 
loaded into two steps, which we need as our data values are in two separate arrays. It s noteworthy to 
look at the glBufferData() call; in this call, we basically have OpenGL allocate an array sized to our 
needs (the combined size of our point and color arrays), but don t transfer any data with the call, which 
is specified with the NULL value. This is akin to calling malloc()  to create a buffer of uninitialized 
data. We later load that array with our calls to glBufferSubData(), which allows us to replace a subsection 
of our array. This technique is also useful if you need to update data inside of a VBO at some point 
in the execution of your application.  The final step in preparing you data for processing by OpenGL 
(i.e., sending it down for rendering) is to specify which vertex attributes you d like issued to the 
graphics pipeline. While this might seem superfluous, it allows you to specify multiple collections of 
data, and choose which ones you d like to use at any given time. Each of the attributes that we enable 
must be associated with an in variable of the currently bound vertex shader. You retrieve vertex attribute 
locations was retrieved from the compiled shader by calling glGetAttribLocation().We discuss this call 
in the shader section.  To complete the plumbing of associating our vertex data with variables in our 
shader programs, you need to tell OpenGL where in our buffer object to find the vertex data, and which 
shader variable to pass the data to when we draw. The above code snippet shows that process for our two 
data sources. In our shaders (which we ll discuss in a moment), we have two variables: vPosition, and 
vColor, which we will associate with the data values in our VBOs that we copied form our vertex_positions 
and vertex_colors arrays. The calls to glGetAttribLocation() will return a compiler-generated index which 
we need to use to complete the connection from our data to the shader inputs. We also need to turn the 
valve on our data by enabling its attribute array by calling glEnableVertexAttribArray() with the selected 
attribute location. This is the most flexible approach to this process, but depending on your OpenGL 
version, you may be able to use the layout construct, which allows you to specify the attribute location, 
as compared to having to retrieve it after compiling and linking your shaders. We ll discuss that in 
our shader section later in the course. BUFFER_OFFSET is a simple macro defined to make the code more 
readable #define BUFFER_OFFSET( offset ) ((GLvoid*) (offset)) In order to initiate the rendering of primitives, 
you need to issue a drawing routine. While there are many routines for this in OpenGL, we ll discuss 
the most fundamental ones. The simplest routine is glDrawArrays(), to which you specify what type of 
graphics primitive you want to draw (e.g., here we re rending a triangle strip), which vertex in the 
enabled vertex attribute arrays to start with, and how many vertices to send.  This is the simplest 
way of rendering geometry in OpenGL Version 3.1. You merely need to store you vertex data in sequence, 
and then glDrawArrays() takes care of the rest. However, in some cases, this won t be the most memory 
efficient method of doing things. Many geometric objects share vertices between geometric primitives, 
and with this method, you need to replicate the data once for each vertex. We ll see a more flexible, 
in terms of memory storage and access in the next slides.   As with any programming language, GLSL 
has types for variables. However, it includes vector-, and matrix-based types to simplify the operations 
that occur often in computer graphics. In addition to numerical types, other types like texture samplers 
are used to enable other OpenGL operations. We ll discuss texture samplers in the texture mapping section. 
 The vector and matrix classes of GLSL are first-class types, with arithmetic and logical operations 
well defined. This helps simplify your code, and prevent errors. Note in the above example, overloading 
ensures that both a*m and m*a are defined although they will not in general produce the same result. 
 For GLSL s vector types, you ll find that often you may also want to access components within the vector, 
as well as operate on all of the vector s components at the same time. To support that, vectors and matrices 
(which are really a vector of vectors), support normal C vector accessing using the square-bracket notation 
(e.g., [i] ), with zero-based indexing. Additionally, vectors (but not matrices) support swizzling, which 
provides a very powerful method for accessing and manipulating vector components. Swizzles allow components 
within a vector to be accessed by name. For example, the first element in a vector element 0 can also 
be referenced by the names x , s , and r . Why all the names to clarify their usage. If you re working 
with a color, for example, it may be clearer in the code to use r to represent the red channel, as compared 
to x , which make more sense as the x-positional coordinate In addition to types, GLSL has numerous qualifiers 
to describe a variable usage. The most common of those are:  in qualifiers that indicate the shader 
variable will receive data flowing into the shader, either from the application, or the previous shader 
stage. out qualifier which tag a variable as data output where data will flow to the next shader stage, 
or to the framebuffer  uniform qualifiers for accessing data that doesn t change across a draw operation 
  Like the C language, GLSL supports all of the logical flow control statements you re used to.  GLSL 
also provides a rich library of functions supporting common operations. While pretty much every vector- 
and matrix-related function available you can think of, along with the most common mathematical functions 
are built into GLSL, there s no support for operations like reading files or printing values. Shaders 
are really data-flow engines with data coming in, being processed, and sent on for further processing. 
 Fundamental to shader processing are a couple of built-in GLSL variable which are the terminus for 
operations. In particular, vertex data, which can be processed by up to for shader stages in OpenGL are 
all ended by setting a positional value into the built-in variable, gl_Position. Similarly, the output 
of a fragment shader (in version 3.1 of OpenGL) is set by writing values into the built-in variable gl_FragColor. 
Later versions of OpenGL allow fragment shaders to output to other variables of the user s designation 
as well.  Here s the simple vertex shader we use in our cube rendering example. It accepts two vertex 
attributes as input: the vertex s position and color, and does very little processing on them; in fact, 
it merely copies the input into some output variables (with gl_Position being implicitly declared). The 
results of each vertex shader execution are passed further down the OpenGL pipeline, and ultimately end 
their processing in the fragment shader.  Here s the associated fragment shader that we use in our cube 
example. While this shader is as simple as they come merely setting the fragment s color to the input 
color passed in, there s been a lot of processing to this point. In particular, every fragment that s 
shaded was generated by the rasterizer, which is a built-in, non-programmable (i.e., you don t write 
a shader to control its operation). What s magical about this process is that if the colors across the 
geometric primitive (for multi-vertex primitives: lines and triangles) is not the same, the rasterizer 
will interpolate those colors across the primitive, passing each iterated value into our color variable. 
 Shaders need to be compiled in order to be used in your program. As compared to C programs, the compiler 
and linker are implemented in the OpenGL driver, and accessible through function calls from within your 
program. The diagram illustrates the steps required to compile and link each type of shader into your 
shader program. A program can contain either a vertex shader (which replaces the fixed-function vertex 
processing), a fragment shader (which replaces the fragment coloring stages), or both. If a shader isn 
t present for a particular stage, the fixed-function part of the pipeline is used in its place. Just 
a with regular programs, a syntax error from the compilation stage, or a missing symbol from the linker 
stage could prevent the successful generation of an executable program. There are routines for verifying 
the results of the compilation and link stages of the compilation process, but are not shown here. Instead, 
we ve provided a routine that makes this process much simpler, as demonstrated on the next slide.  To 
simplify our lives, we created a routine that simplifies loading, compiling, and linking shaders: InitShaders(). 
It implements the shader compilation and linking process shown on the previous slide. It also does full 
error checking, and will terminate your program if there s an error at some stage in the process (production 
applications might choose a less terminal solution to the problem, but it s useful in the classroom). 
InitShaders() accepts two parameters, each a filename to be loaded as source for the vertex and fragment 
shader stages, respectively. The value returned from InitShaders() will be a valid GLSL program id that 
you can pass into glUseProgram().  OpenGL shaders, depending on which stage their associated with, process 
different types of data. Some data for a shader changes for each shader invocation. For example, each 
time a vertex shader executes, it s presented with new data for a single vertex; likewise for fragment, 
and the other shader stages in the pipeline. The number of executions of a particular shader rely on 
how much data was associated with the draw call that started the pipeline if you call glDrawArrays() 
specifiying 100 vertices, your vertex shader will be called 100 times, each time with a different vertex. 
Other data that a shader may use in processing may be constant across a draw call, or even all the drawing 
calls for a frame. GLSL calls those uniform varialbes, since their value is uniform across the execution 
of all shaders for a single draw call. Each of the shader s input data variables (ins and uniforms) needs 
to be connected to a data source in the application. We ve already seen glGetAttribLocation() for retrieving 
information for connecting vertex data in a VBO to shader variable. You will also use the same process 
for uniform variables, as we ll describe shortly.  Once you know the names of variables in a shader 
 whether they re attributes or uniforms you can determine their location using one of the glGet*Location() 
calls. If you don t know the variables in a shader (if, for instance, you re writing a library that accepts 
shaders), you can find out all of the shader variables by using the glGetActiveAttrib() function.  
You ve already seen how one associates values with attributes by calling glVertexAttribPointer(). To 
specify a uniform s value, we use one of the glUniform*() functions. For setting a vector type, you ll 
use one of the glUniform*() variants, and for matrices you ll use a glUniformMatrix *() form.  You ll 
find that many OpenGL programs look very similar, particularly simple examples as we re showing in class. 
Above we demonstrate the basic initialization code for our examples. In our main() routine, you can see 
our use of the freeglut and GLEW libraries. The main() has a number of tasks: Initialize and open a 
window  Initialize the buffers and parameters by calling init()  Specify the callback functions for 
events  Enter an infinite event loop  Although callbacks aren t required by OpenGL, it is the standard 
method for developing interactive applications.  A display callback is required by freeglut. It is invoked 
whenever OpenGL determines a window has to be redrawn, i.e. when a window is first opened or the contents 
of a window are changed. In our example we use a keyboard callback to end the program.  We begin delving 
into shader specifics by first taking a look at vertex shaders. As you ve probably arrived at, vertex 
shaders are used to process vertices, and have the required responsibility of specifying the vertex s 
position in clip coordinates. This process usually involves numerous vertex transformations, which we 
ll discuss next. Additionally, a vertex shader may be responsible for determine additional information 
about a vertex for use by the rasterizer, including specifying colors. To begin our discussion of vertex 
transformations, we ll first describe the synthetic camera model.   This model has become know as the 
synthetic camera model. Note that both the objects to be viewed and the camera are three-dimensional 
while the resulting image is two dimensional.  The processing required for converting a vertex from 
3D or 4D space into a 2D window coordinate is done by the transform stage of the graphics pipeline. The 
operations in that stage are illustrated above. The purple boxes represent a matrix multiplication operation. 
In graphics, all of our matrices are 4×4 matrices (they re homogenous, hence the reason for homogenous 
coordinates). When we want to draw an geometric object, like a chair for instance, we first determine 
all of the vertices that we want to associate with the chair. Next, we determine how those vertices should 
be grouped to form geometric primitives, and the order we re going to send them to the graphics subsystem. 
This process is called modeling. Quite often, we ll model an object in its own little 3D coordinate system. 
When we want to add that object into the scene we re developing, we need to determine its world coordinates. 
We do this by specifying a modeling transformation, which tells the system how to move from one coordinate 
system to another. Modeling transformations, in combination with viewing transforms, which dictate where 
the viewing frustum is in world coordinates, are the first transformation that a vertex goes through. 
Next, the projection transform is applied which maps the vertex into another space called clip coordinates, 
which is where clipping occurs. After clipping, we divide by the w value of the vertex, which is modified 
by projection. This division operation is what allows the farther-objects-being-smaller activity. The 
transformed, clipped coordinates are then mapped into the window.  Note that human vision and a camera 
lens have cone-shaped viewing volumes. OpenGL (and almost all computer graphics APIs) describe a pyramid-shaped 
viewing volume. Therefore, the computer will see differently from the natural viewpoints, especially 
along the edges of viewing volumes. This is particularly pronounced for wide-angle fish-eye camera lenses. 
 By using 4×4 matrices, OpenGL can represent all geometric transformations using one matrix format. 
Perspective projections and translations require the 4th row and column. Otherwise, these operations 
would require an vector-addition operation, in addition to the matrix multiplication. While OpenGL specifies 
matrices in column-major order, this is often confusing for C programmers who are used to row-major ordering 
for two-dimensional arrays. OpenGL provides routines for loading both column- and row-major matrices. 
However, for standard OpenGL transformations, there are functions that automatically generate the matrices 
for you, so you don t generally need to be concerned about this until you start doing more advanced operations. 
For operations other than perspective projection, the fourth row is always (0, 0, 0, 1) which leaves 
the w-coordinate unchanged..  Another essential part of the graphics processing is setting up how much 
of the world we can see. We construct a viewing frustum, which defines the chunk of 3-space that we can 
see. There are two types of views: a perspective view, which you re familiar with as it s how your eye 
works, is used togenerate frames that match your view of reality things farther from your appear smaller. 
This is the type of view used for video games, simulations, and most graphics applications in general. 
The other view, orthographic, is used principally for engineering and design situations, where relative 
lengths and angles need to be preserved. For a perspective, we locate the eye at the apex of the frustum 
pyramid. We can see any objects which are between the two planes perpendicular to eye (they re called 
the near and far clippingplanes, respectively). Any vertices between near and far, and inside the four 
planes that connect them will be rendered. Otherwise, those vertices are clipped out and discarded. In 
some cases a primitive will be entirely outside of the view, and the system will discard it for that 
frame. Other primitives might intersect the frustum, which we clip such that the part of them that s 
outside is discarded and we create new vertices for the modified primitive. While the system can easily 
determine which primitive are inside the frustum, it s wasteful of system bandwidth to have lots of primitives 
discarded in this manner. We utilize a technique named culling to determine exactly which primitives 
need to be sent to the graphics processor, and send only thoseprimitives to maximize its efficiency. 
 In OpenGL, the default viewing frusta are always configured in the same manner, which defines the orientation 
of our clip coordinates. Specifically, clip coordinates are defined with the eye located at the origin, 
looking down the z axis. From there, we define two distances: our near and far clip distances, which 
specify the location of our near and far clipping planes. The viewing volume is then completely by specifying 
the positions of the enclosing planes that are parallel to the view direction .  LookAt() generates 
a viewing matrix based on several points. LookAt() provides natrual semantics for modeling flight application, 
but care must be taken to avoid degenerate numerical situations, where the generated viewing matrix is 
undefined. An alternative is to specify a sequence of rotations and translations that are concatenated 
with an initial identity matrix. Note: that the name modelview matrix is appropriate since moving objects 
in the model front of the camera is equivalent to moving the camera to view a set of objects.  Here 
we show the construction of a translation matrix. Translations really move coordinate systems, and not 
individual objects.  Here we show the construction of a scale matrix, which is used to change the shape 
of space, but not move it (or more precisely, the origin). The above illustration has a translation to 
show how space was modified, but a simple scale matrix will not include such a translation.  Here we 
show the effects of a rotation matrix on space. Once again, a translation has been applied in the image 
to make it easier to see the rotation s affect.  Here s an example vertex shader for rotating our cube. 
We generate the matrices in the shader (as compared to in the application), based on the input angle 
theta. It s useful to note that we can vectorize numerous computations. For example, we can generate 
a vectors of sines and cosines for the input angle, which we ll use in further computations.  Completing 
our shader, we compose two of three rotation matrices (one around each axis). In generating our matrices, 
we use one of the many matrix constructor functions (in this case, specifying the 16 individual elements). 
It s important to note in this case, that our matrices are column-major, so we need to take care in the 
placement of the values in the constructor.  We complete our shader here by generating the last rotation 
matrix, and ) and then use the composition of those matrices to transform the input vertex position. 
We also pass-thru the color values by assigning the input color to an output variable.  Finally, we 
merely need to supply the angle values into our shader through our uniform plumbing. In this case, we 
track each of the axes rotation angle, and store them in a vec3 that matches the angle declaration in 
the shader. We also keep track of the uniform s location so we can easily update its value.   Lighting 
is an important technique in computer graphics. Without lighting, objects tend to look like they are 
made out of plastic. OpenGL divides lighting into three parts: material properties, light properties 
and global lighting parameters. Lighting is available in both RGBA mode and color index mode. RGBA is 
more flexible and less restrictive than color index mode lighting.  OpenGL can use the shade at one 
vertex to shade an entire polygon (constant shading) or interpolate the shades at the vertices across 
the polygon (smooth shading), the default. The original lighting model that was supported in hardware 
and OpenGL was due to Phong and later modified by Blinn. Although the lighting functions have been deprecated, 
the model can easily be implemented in shaders and forms the basis for other models.  The orientation 
of a surface is specified by the normal at each point. For a flat polygon the normal is constant over 
the polygon. Because normals are specified by the application program and can be changed between the 
specification of vertices, when we shade a polygon it can appear to be curved.  OpenGL lighting is based 
on the Phong lighting model. At each vertex in the primitive, a color is computed using that primitives 
material properties along with the light settings. The color for the vertex is computed by adding four 
computed colors for the final vertex color. The four contributors to the vertex color are: Ambient is 
color of the object from all the undirected light in a scene.  Diffuse is the base color of the object 
under current lighting. There must be a light shining on the object to get a diffuse contribution.  
Specular is the contribution of the shiny highlights on the object.  Emission is the contribution added 
in if the object emits light (i.e., glows)   The lighting normal tells OpenGL how the object reflects 
light around a vertex. If you imagine that there is a small mirror at the vertex, the lighting normal 
describes how the mirror is oriented, and consequently how light is reflected.  Material properties 
describe the color and surface properties of a material (dull, shiny, etc). The properties described 
above are components of the Phong lighting model, a simple model that yields reasonable results with 
little computation. Each of the material components would be passed into a vertex shader, for example, 
to be used in the lighting computation along with the vertex s position and lighting normal.  Here we 
declare numerous variables that we ll use in computing a color using a simple lighting model. All of 
the uniform values are passed in from the application and describe the material and light properties 
being rendered.  In the initial parts of our shader, we generate numerous vector quantities to be used 
in our lighting computation. pos represents the vertex s position in eye coordinates  L represents 
the vector from the vertex to the light  E represents the eye vector, which is the vector from the vertex 
s eye-space position to the origin  H is the half vector which is the normalized vector half-way between 
the light and eye vectors  N is the transformed vertex normal Note that all of these quantities are 
vec3 s, since we re dealing with vectors, as compared to  homogenous coordinates. When we need to convert 
form a homogenous coordinate to a vector, we use a vector swizzle to extract the components we need. 
 Here we complete our lighting computation. The Phong model, which this shader is based on, uses various 
material properties as we described before. Likewise, each light can contribute to those same properties. 
The combination of the material and light properties are represented as our product variables in this 
shader. The products are merely the component-wise products of the light and objects same material propreties. 
These values are computed in the application and passed into the shader. In the Phong model, each material 
product is attenuated by the magnitude of the various vector products. Starting with the most influential 
component of lighting, the diffuse color, we use the dot product of the lighting normal and light vector, 
clamping the value if the dot product is negative (which physically means the light s behind the object). 
We continue by computing the specular component, which is computed as the dot product of the normal and 
the half-vector raised to the shininess value. Finally, if the light is behind the object, we correct 
the specular contribution. Finally, we compose the final vertex color as the sum of the computed ambient, 
diffuse, and specular colors, and update the transformed vertex position.   The final shading stage 
that OpenGL supports is fragment shading which allows an application per­pixel-location control over 
the color that may be written to that location. Fragments, which are on their way to the framebuffer, 
but still need to do some pass some additional processing to become pixels. However, the computational 
power available in shading fragments is a great asset to generating images. In a fragment shader, you 
can compute lighting values similar to what we just discussed in vertex shading per fragment, which 
gives much better results, or add bump mapping, which provides the illusion of greater surface detail. 
Likewise, we ll apply texture maps, which allow us to increase the detail for our models without increasing 
the geometric complexity.  As an example of what we can do in a fragment shader, consider using our 
lighting model, but for every pixel, as compared to at the vertex level. Doing fragment lighting provides 
much better visual result, but using almost identical shader code (except you need to move it from your 
vertex shader into your fragment shader). The only trick required is that we need to have the rasterizer 
provide us updated normal values for each fragment. However, that s just like iterating a color, so there 
s almost nothing to it.  We ll now analyze a few case studies from different applications.  The first 
simple application we ll look at is rendering height fields, as you might do when rendering terrain in 
an outdoor game or flight simulator.  We d first like to render a wire-frame version of our mesh, which 
we ll draw a individual line loops. To begin, we build our data set by sampling the function f for a 
particular time across the domain of points. From there, we build our array of points to render. Once 
we have our data and have loaded into our VBOs we render it by drawing the individual wireframe quadrilaterals. 
There are many ways to render a wireframe surface like this give some thought of other methods.   
 Here s a rendering of the mesh we just generated.  While the wireframe version is of some interest, 
we can create better looking meshes by adding a few more effects. We ll begin by creating a solid mesh 
by converting each wireframe quadrilateral into a solid quad composed of two separate triangles. Turns 
out with our pervious set of points, we can merely changed our glDrawArrays()call or more specifically, 
the geometric primitive type to render a solid surface. However, if we don t do some additional modification 
of one of our shaders, we ll get a large back blob. To produce a more useful rendering, we ll add lighting 
computations into our vertex shader, computing a lighting color for each vertex, which will be passed 
to the fragment shader.  Details of lighting model are not important to here. The model includes the 
standard modified Phong diffuse and specular terms without distance. Note that we do the lighting in 
eye coordinates and therefore must compute the eye position in this frame. All the light and material 
properties are set in the application and are available through the OpenGL state.   Here s a rendering 
of our shaded, solid mesh.   Textures are images that can be thought of as continuous and be one, two, 
three, or four dimensional. By convention, the coordinates of the image are s, t, r and q. Thus for the 
two dimensional image above, a point in the image is given by its (s, t) values with (0, 0) in the lower-left 
corner and (1, 1) in the top-right corner. A texture map for a two-dimensional geometric object in (x, 
y, z) world coordinates maps a point in (s, t) space to a corresponding point on the screen.  The advantage 
of texture mapping is that visual detail is in the image, not in the geometry. Thus, the complexity of 
an image does not affect the geometric pipeline (transformations, clipping) in OpenGL. Texture is added 
during rasterization where the geometric and pixel pipelines meet.  In the simplest approach, we must 
perform these three steps. Textures reside in texture memory. When we assign an image to a texture it 
is copied from processor memory to texture memory where pixels are formatted differently. Texture coordinates 
are actually part of the state as are other vertex attributes such as color and normals. As with colors, 
OpenGL interpolates texture inside geometric objects. Because textures are really discrete and of limited 
extent, texture mapping is subject to aliasing errors that can be controlled through filtering. Texture 
memory is a limited resource and having only a single active texture can lead to inefficient code.  
The general steps to enable texturing are listed above. Some steps are optional, and due to the number 
of combinations, complete coverage of the topic is outside the scope of this course. Here we use the 
texture object approach. Using texture objects may enable your OpenGL implementation to make some optimizations 
behind the scenes. As with any other OpenGL state, texture mapping requires that glEnable() be called. 
The tokens for texturing are: GL_TEXTURE_1D - one dimensional texturing GL_TEXTURE_2D - two dimensional 
texturing GL_TEXTURE_3D - three dimensional texturing 2D texturing is the most commonly used. 1D texturing 
is useful for applying contours to objects ( like altitude contours to mountains ). 3D texturing is useful 
for volume rendering.  The first step in creating texture objects is to have OpenGL reserve some indices 
for your objects. glGenTextures() will request n texture ids and return those values back to you in texIds. 
To begin defining a texture object, you call glBindTexture() with the id of the object you want to create. 
The target is one of GL_TEXTURE_{123}D(). All texturing calls become part of the object until the next 
glBindTexture() is called. To have OpenGL use a particular texture object, call glBindTexture() with 
the target and id of the object you want to be active. To delete texture objects, use glDeleteTextures( 
n, *texIds ), where texIds is an array of texture object identifiers to be deleted.  After creating 
a texture object, you ll need to bind to it to initialize or use the texture stored in the object. This 
operation is very similar to what you ve seen when working with VAOs and VBOs.  Specifying the texels 
for a texture is done using the glTexImage{123}D() call. This will transfer the texels in CPU memory 
to OpenGL, where they will be processed and converted into an internal format. The level parameter is 
used for defining how OpenGL should use this image when mapping texels to pixels. Generally, you ll set 
the level to 0, unless you are using a texturing technique called mipmapping, which we will discuss in 
the next section.  When you want to map a texture onto a geometric primitive, you need to provide texture 
coordinates. Valid texture coordinates are between 0 and 1, for each texture dimension, and usually manifest 
in shaders as vertex attributes. We ll see how to deal with texture coordinates outside the range [0, 
1] in a moment.  Just like vertex attributes were associated with data in the application, so too with 
textures. In particular, you access a texture defined in your application using a texture sampler in 
your shader. The type of the sampler needs to match the type of the associated texture. For example, 
you would use a sampler2D to work with a two-dimensional texture created with glTexImage2D( GL_TEXTURE_2D, 
 ); Within the shader, you use the texture() function to retrieve data values from the texture associated 
with your sampler. To the texture() function, you pass the sampler as well as the texture coordinates 
where you want to pull the data from. Note: the overloaded texture() method was added into GLSL version 
3.30. Prior to that release, there were special texture functions for each type of texture sampler (e.g., 
there was a texture2D() call for use with the sampler2D).  Similar to our first cube example, if we 
want to texture our cube, we need to provide texture coordinates for use in our shaders. Following our 
previous example, we merely add an additional vertex attribute that contains our texture coordinates. 
We do this for each of our vertices. We will also need to update VBOs and shaders to take this new attribute 
into account.  The code snippet above demonstrates procedurally generating a two 64 × 64 texture maps. 
 The above OpenGL commands completely specify a texture object. The code creates a texture id by calling 
glGenTextures(). It then binds the texture using glBindTexture() to open the object for use, and loading 
in the texture by calling glTexImage2D(). After that, numerous sampler characteristics are set, including 
the texture wrap modes, and texel filtering.  In order to apply textures to our geometry, we need to 
modify both the vertex shader and the pixel shader. Above, we add some simple logic to pass-thru the 
texture coordinates from an attribute into data for the rasterizer.  Continuing to update our shaders, 
we add some simple code to modify our fragment shader to include sampling a texture. How the texture 
is sampled (e.g., coordinate wrap modes, texel filtering, etc.) is configured in the application using 
the glTexParameter*() call.   All the above books except Angel and Shreiner, Interactive Computer Graphics 
(Addison-Wesley), are in the Addison-Wesley Professional series of OpenGL books. Books on WebGL are just 
starting to appear.   Many example programs, a C++ matrix-vector package and the InitShader function 
are under the Book Support tab at www.cs.unm.edu/~angel
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343486</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>62</pages>
		<display_no>3</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Example-based color image manipulation and enhancement]]></title>
		<page_from>1</page_from>
		<page_to>62</page_to>
		<doi_number>10.1145/2343483.2343486</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343486</url>
		<abstract>
			<par><![CDATA[<p>Color transfer was introduced about 10 years ago as a technique to make rendered images look more natural by adjusting their color content on the basis of an example image. Now, color transfer is not a single algorithm but a range of methods and techniques that aim to make one image look more like another, and the technique now has applications far beyond its humble beginnings.</p> <p>This course helps color-transfer researchers understand where there may be further opportunities for algorithmic improvements, and it helps practitioners in creative industries such as photography, movies, and games understand how to make the most of these algorithms. The course begins with an overview of current techniques, and then presents many examples and comparisons that show when these algorithms are expected to produce their best results in still images and videos. It also shows how to choose appropriate examples to steer the results and reviews applications of color transfer that include making night-time images from day-time images, color correcting stereo pairs, and color matching photographs to stitch pre-processing to panoramas.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738881</person_id>
				<author_profile_id><![CDATA[81464672320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tania]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pouli]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738882</person_id>
				<author_profile_id><![CDATA[81100331006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Erik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reinhard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
      1D vs 3D 3 x 1D problems Each colour channel is manipulated separately 19 1D vs 3D 3 x 1D 
problems Each colour channel is manipulated separately 1 x 3D problem The image is treated as a 3D dataset 
 3 easier problems BUT...  The choice of colour space matters  Channel cross-talk  20 1D vs 3D 1 
x 3D problem A lot more data The image is treated as More complex a 3D dataset algorithms Often requires 
optimization to reduce artefacts 21 Example Methods 22 Previous work 3x1D problems Reinhard et al. 
2001 Linear shifting and scaling Xiao &#38; Ma 2009 Full histogram matching Color Transfer in Laß Convert 
source  and target  to Laß Is It Linear shifting and scaling using mean and standard deviation: I0 
= Is . µs I00 t I0 = s = I00 Io + µt Each channel manipulated separately Reinhard et al, 'Color Transfer 
between Images', IEEE Computer Graphics and Applications 21(5), pp. 34-41, 2001 26          
                  Contrast Adjustment += Pre-adjustment Post-adjustment 113 HDR Examples 
Source HDR Target LDR (linear) 114   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343487</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>159</pages>
		<display_no>4</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Computational displays]]></title>
		<subtitle><![CDATA[combining optical fabrication, computational processing, and perceptual tricks to build the displays of the future]]></subtitle>
		<page_from>1</page_from>
		<page_to>159</page_to>
		<doi_number>10.1145/2343483.2343487</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343487</url>
		<abstract>
			<par><![CDATA[<p>This course provides the first comprehensive overview of computational displays for the graphics community. These display architectures employ co-design of optical elements, efficient computational processing, computationally tractable models for human perception, and advanced mathematical analysis.</p> <p>The course reviews all aspects of computational displays in detail, from concept introduction to a variety of example displays that exploit joint design of optical components and computational processing for applications such as high-dynamic-range and wide-color-gamut display, extended depth-of-field projection, and high-dimensional information display for computer-vision applications. In particular, the course focuses on how high-speed displays, multiple stacked LCDs, and directional backlighting combined with advanced mathematical analysis and efficient computational processing provide the foundations of 3D displays of the future. It also reviews psycho-physiological aspects that are of importance for display design and demonstrates how perceptually driven computational displays can enhance the capability of current technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738883</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738884</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738885</person_id>
				<author_profile_id><![CDATA[81100022708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Diego]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gutierrez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Zaragoza]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738886</person_id>
				<author_profile_id><![CDATA[81442601918]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Displays: Combining Optical Fabrication, Computational Processing, and Perceptual Tricks 
to Build the Displays of the Future  SIGGRAPH 2012 Course Sunday, 5 August 2012, 2:00-3:30 pm Los Angeles 
Convention Center -Room 408B  Gordon Wetzstein Douglas Lanman MIT Media Lab MIT Media Lab gordonw@media.mit.edu 
dlanman@media.mit.edu Diego Gutierrez Matthew Hirsch Universidad de Zaragoza MIT Media Lab diegog@unizar.es 
mhirsch@media.mit.edu  Abstract With the invention of integral imaging and parallax barriers in the 
begin­ning of the 20th century, glasses-free 3D displays have become feasible. Only today more than a 
century later glasses-free 3D displays are .­nally emerging in the consumer market. The technologies 
being employed in current-generation devices, however, are fundamentally the same as what was invented 
100 years ago. With rapid advances in optical fab­rication, digital processing power, and computational 
models for human perception, a new generation of display technology is emerging: compu­tational displays 
exploring the co-design of optical elements and compu­tational processing while taking particular characteristics 
of the human vi­sual system into account. This technology does not only encompass 3D displays, but also 
next-generation projection systems, high dynamic range displays, perceptually-driven devices, and computational 
probes. This course serves as an introduction to the emerging .eld of compu­tational displays. The pedagogical 
goal of this course is to provide the au­dience with the tools necessary to expand their research endeavors 
by pro­viding step-by-step instructions on all aspects of computational displays: display optics, mathematical 
analysis, ef.cient computational processing, computational perception, and, most importantly, the effective 
combina­tion of all these aspects. Speci.cally, we will discuss a wide variety of different applications 
and hardware setups of computational displays, in­cluding high dynamic range displays, advanced projection 
systems as well as glasses-free 3D display. The latter example, computational light .eld displays, will 
be discussed in detail. In the course presentation, supple­mentary notes, and an accompanying website, 
we will provide source code that drives various display incarnations at real-time framerates, detailed 
instructions on how to fabricate novel displays from off-the-shelf compo­nents, and intuitive mathematical 
analyses that will make it easy for re­searchers with various backgrounds to get started in the emerging 
.eld of computational displays. We believe that computational display technology is one of the hottest 
topics in the graphics community today; with this course we will make it accessible for a diverse audience. 
While the popular, introductory-level courses Build Your Own 3D Displays and Build Your Own Glasses-free 
3D Display , previously taught at SIGGRAPH and SIG-GRAPH ASIA, discussed conventional 3D displays invented 
in the past, this course introduces what we believe to be the future of display tech­nology. We will 
only brie.y review conventional technology and focus on practical and intuitive demonstrations of how 
an interdisciplinary ap­ 1 proach to display design encompassing optics, perception, computation, and 
mathematical analysis can overcome the limitations for a variety of applications. We will discuss all 
aspects of computational displays in detail. Specif­ically, we begin by introducing the concept and discussing 
a variety of example displays that exploit the joint-design of optical components and computational processing 
for applications such as high dynamic range im­age and wide color gamut display, extended depth of .eld 
projection, and high-dimensional information display for computer vision applications. We will then proceed 
to discussing state-of-the-art computational light .eld displays in detail. In particular, we will focus 
on how high-speed displays, multiple stacked LCDs, and directional backlighting combined with ad­vanced 
mathematical analysis and ef.cient computational processing pro­vide the foundations of 3D displays of 
the future. Finally, we will review psycho-physiological aspects that are of importance for display design 
and demonstrate how perceptually-driven computational displays can enhance the capability of current 
technology. Prerequisites For this intermediate-level course, some familiarity with Matlab, C/C++, OpenGL, 
as well as a general understanding of linear algebra and Fourier analysis is assumed, although the course 
also functions as a brief, applica­tion-driven introduction to each of these tools. 2  Speaker Biographies 
 Gordon Wetzstein MIT Media Lab gordonw@media.mit.edu http://web.media.mit.edu/ gordonw Gordon Wetzstein 
is a Postdoctoral Associate at the MIT Media Lab. His research interests include light .eld and high 
dynamic range dis­plays, projector-camera systems, computational optics, computational photography, computer 
vision, computer graphics, and augmented reality. Gordon received a Diplom in Media System Science with 
Honors from the Bauhaus-University Weimar in 2006 and a Ph.D. in Computer Science at the University of 
British Columbia in 2011. His doctoral dissertation focuses on computational light modulation for image 
acquisition and dis­play. He is co-chairing the .rst workshop on Computational Cameras and Displays at 
CVPR 2012, is serving in the general submissions committee at SIGGRAPH 2012, has served on the program 
committees of IEEE ProCams 2007 and IEEE ISMAR 2010, won a Laval Virtual Award in 2005 for his work on 
projector-camera systems, and a best paper award for Hand-Held Schlieren Photography with Light Field 
Probes at the International Conference on Computational Photography in 2011, introducing light .eld probes 
as computational displays for computer vision and .uid mechanics applications. Douglas Lanman MIT Media 
Lab dlanman@media.mit.edu http://web.media.mit.edu/ dlanman Douglas Lanman is a Postdoctoral Associate 
at the MIT Media Lab. His research is focused on computational photography and displays, including light 
.eld capture, automultiscopic 3D displays, and active illumination for 3D reconstruction. He received 
a B.S. in Applied Physics with Honors from Caltech in 2002 and M.S. and Ph.D. degrees in Electrical Engineering 
from Brown University in 2006 and 2010, respectively. Prior to joining MIT and Brown, he was an Assistant 
Research Staff Member at MIT Lincoln Laboratory from 2002 to 2005. Douglas has worked as an intern at 
Intel, Los Alamos National Laboratory, INRIA Rh one-Alpes, 3 Mitsubishi Electric Research Laboratories 
(MERL), and the MIT Media Lab. He presented the Build Your Own Glasses-free 3D Display course at SIGGRAPH 
2011, the Build Your Own 3D Scanner course at SIGGRAPH 2009 and SIGGRAPH Asia 2009 and the Build Your 
Own 3D Display course at SIGGRAPH 2010 and SIGGRAPH Asia 2010. Matthew Hirsch MIT Media Lab mhirsch@media.mit.edu 
http://web.media.mit.edu/ mhirsch Matthew Hirsch is a Ph.D. student at the MIT Media Lab. His re­search 
focuses on imaging devices that enable new understanding and interaction scenarios. He works with Henry 
Holtzman and Ramesh Raskar in the Information Ecology and Camera Culture groups, respectively. Matthew 
graduated from Tufts University in 2004 with a B.S. in Computer Engineering. He worked as an Imaging 
Engineer at Analogic Corp. from 2004 to 2007, where he designed threat detection algorithms for computed 
tomography security scanners. He presented the Build Your Own Glasses-free 3D Display course at SIGGRAPH 
2011, and the Build Your Own 3D Display course at SIGGRAPH 2010 and SIGGRAPH Asia 2010. Diego Gutierrez 
Universidad de Zaragoza diego@unizar.es giga.cps.unizar.es/ diegog/ Diego Gutierrez is a tenured Associate 
Professor at the Universidad de Zaragoza, in Spain, where he leads the Graphics and Imaging Lab. His 
research interests include applied perception in graphics and visualization, global illumination and 
computational photography. Since 2006, he has already presented eight courses at both SIGGRAPH conferences. 
He s currently Papers Chair for EGSR 2012, and has previously chaired other in­ternational conferences 
like APGV 2011. He has served on many Program Committees, including SIGGRAPH, SIGGRAPH Asia and Eurographics, 
and is also an Associate Editor of three journals (IEEE Computer Graphics &#38; Applications, ACM Transactions 
on Applied Perception and Computers &#38; Graphics). 4  Course Outline 3 minutes: Introduction and 
Overview Gordon Wetzstein This part will introduce the speakers, present a motivation of the course, 
and outline the individual parts. 22 minutes: Computational Displays as a Next-generation Technology 
Gordon Wetzstein This part will introduce the emerging .eld of computational displays. We will discuss 
the fundamental building blocks of computational displays: optical components, computational processing 
as well as the human visual system. This part will also serve as an overview of computational displays, 
such as adaptive coded aperture projec­tion, high dynamic range displays, and emerging projection systems. 
In addition to displays intended for the human visual system, we also plan to provide an overview of 
computational probes: high­dimensional displays targeted toward computer vision applications rather than 
the human visual system. 35 minutes: Computational Light Field Displays -Hardware Architec­tures, Fabrication, 
Content Generation and Optimization Douglas Lanman and Matthew Hirsch The combination of numerical optimization, 
display fabrication, and ef.cient computational processing provides the foundation of fu­ture glasses-free 
3D display design. This part will present the lat­est light .eld display designs exploiting high-speed 
LCDs as well as stacked layers of light-attenuating and polarization-rotating LCDs. We will present detailed 
instructions on how to build arbitrary com­binations of high-speed see-through LCD panels and refractive 
op­tical elements from off-the-shelf parts. In addition, we will pro­vide source code and instructions 
for driving these with ef.cient GPU-based implementations of the most important algorithms: to­mographic 
light .eld synthesis, non-negative matrix factorizations as well as non-negative tensor factorizations. 
Furthermore, this part will discuss how important display characteristics, such as depth of .eld, .eld 
of view, and contrast, are theoretically analyzed. This analysis along with hardware and software-related 
implementation details will be presented as step-by-step instructions so as to provide 5 other researchers 
with intuitive tools that facilitate them to get started in this exciting new .eld and build their own 
computational light .eld displays. 20 minutes: Perceptually-driven Computational Displays Diego Gutierrez 
This part will review aspects of the human visual system that are of particular importance for designing 
displays. In particular, we will discuss sensitivity to contrast, spatial frequencies, stereo disparities 
and other depth cues as well as temporally-multiplexed signals. The goal of this part is to emphasize 
how the limitations of the human visual system can be exploited to enhance the perceived capabilities 
of computational displays. 10 minutes: Summary and Q &#38; A All This part will summarize how computational 
displays are chang­ing current display architectures by exploiting the co-design of dis­play optics and 
computational processing targeted toward human observers. We will outline future directions of this emerging 
.eld and allow for suf.cient time to answer questions and stimulate dis­cussions. 6  This part of the 
course is meant to give an overview of computational displays. Rather that focusing on a few different 
approaches, the next 20 minutes will be more of a fast-forward of much of the research in the area that 
has been conducted within the last decade or so. Let s start out with a technology that most people 
have at home: a television. Within the last few years, most TVs that you buy in the store today have 
local dimming or micro dimming integrated. That is an approach to creating high contrast imagines by 
combining a low­resolution LED-based backlight with a high-resolution LCD. The underlying technology 
was invented in 2004 and presented here at Siggraph for the first time. HDR displays tackle the problem 
of conventional LCDs having a limited contrast by replacing old-school CFL backlights with an array of 
programmable LEDs. This provides programmable rear-illumination that can be locally dimmed or even turned 
off, while illuminating the LCD with full brightness in other image parts. The necessary pre-computation, 
usually carried out on the device in real-time, decomposes a target HDR image into a low-resolution but 
high contrast pattern displayed on the LED array and a corresponding high-resolution LCD image that adds 
sharp image details and colors on top of the LEDs. The concept allowing for a significant increase in 
contrast for these displays is dual modulation. By using two layers of displays that act in a multiplicative 
fashion, in this case an LCD and a LED array, the overall contrast of the display is increased. Similar 
ideas have also been applied to increasing the contrast of static prints or other hardcopies. For this 
purpose, a projector can be used to illuminate the print, an e-reader, x-ray transparencies, or any other 
type of low-contrast display. As long as the projector is registered with the secondary display, it can 
just illuminate it with the exact image shown on the hardcopy to increase its dynamic range as seen in 
these examples on the top. Oliver Bimber also explored the concept of dual modulation for microscopy. 
The optical design is more involved than for simple printouts, but the idea is the same: a camera observes 
a specimen and the optics are built so that a programmable light source illuminates it so as to optically 
enhance the observed contrast. With live camera feedback, the projected images can also be adjusted to 
allow for dynamic content such as live specimen. Dual modulation has the potential to increase the dynamic 
range of a variety of other displays as well. As seen in this schematic, the dynamic range of projectors 
can be extended through dual modulation. What we see is the design of an HDR projector that basically 
consists of a light source on the left, a conventional reflective or transmissive spatial light modulator 
for each color channel in the center, and an additional modulator on the right. While the latter only 
allows for the modulation of the luminance channel, the dynamic range for displayed luminance values 
is increased as the blacklevel is decreased. Please note that the human visual system is most sensitive 
to contrast for luminance perception and not very sensitive to chrominance contrast. In effect, the optical 
projector design enhances the capabilities of the device in a perceptually optimal manner. Exploiting 
the limitations of human perception for display optics design and the corresponding computational processing 
is the spirit of computational displays. A somewhat more sophisticated approach to high dynamic range 
projection was recently presented at Siggraph Asia. While the previous HDR projector blocks a lot of 
the light inside the device to achieve a lower backlevel, this projector recycles excessive background 
light in dark image areas. Using an analog micro-mirror array in the optical path, excessive light is 
steered to other image areas and basically increases the maximum image brightness there. Light re-allocation 
or recycling in projectors is an idea that not only increases the contrast of the devices but also reduces 
the heat and cooling power consumption because the produced light is steered out of the physical enclosure 
rather than dumping it inside. This particular project is a great example of how a similar functionality, 
in this case high dynamic range imaging, can require very different optical designs and corresponding 
processing depending on whether it s a projector or a TV. In one case dual modulation may be a great 
idea because one can mostly control where light is being emitted whereas in a projector one usually does 
not have that luxury, so reallocation may be a much better option. In a much broader sense, projectors 
have been used for unconventional applications for more than a decade. The Office of the Future is probably 
one of the seminal papers envisioning seamless integration of multi-projector systems into our daily 
workspaces. Even now, more than a decade later, fully-immersive teleconferencing systems and spatial 
interfaces allowing us to augment the world with virtual information are still an active area of research. 
Many of the practical problems associated with multi-projector systems, such as photometric and geometric 
calibration, however, are solved. Textbooks, such as that by Aditi Majumder cover the topic exhaustively. 
 Another well-known use of computational imaging is structured illumination. The joint design of projected 
optical codes and computational reconstruction of the underlying data has been a standard technique in 
computer visions for years. Usually, these approaches encode a one-to­one mapping between projector and 
camera pixels which allows for diffuse geometry acquisition when the devices are calibrated. Spatial 
augmented reality, such as shader lamps and radiometric compensation, allow projectors to manipulate 
the appearance of objects turning brick walls into planar, white canvases or plain white objects into 
colorful miniatures of the real world. These approaches require the geometry and reflectance of the surfaces 
to be known and registered to the projectors; the computational pre-distortion of displayed images can 
then easily be performed using standard projective texture mapping or other forms of image distortion. 
 While most projector camera systems make strong assumptions on the imaged scene, such as Lambertian-ness, 
inverse light transport with applications to radiometric compensation and synthetic relighting has been 
explored as well. The involved illumination patterns and their decoding are more involved than for diffuse 
scenes, but the general idea of compensating for optical effects using computational pre-processing is 
the same as simple structured illumination. Light transport does not always have to be inverted, it 
can also be transposed. Pradeep Sen and colleagues have shown that the transpose of the light transport 
matrix can be useful for generating dual images showing the scene from the point of view of a projector 
illuminated by a light source at the point of view of a camera. This allows for novel view generation, 
even unveiling parts of the scene that were only visible by the projector and never by the camera. Relighting 
a complex scene with novel illumination patters, such as seen in these images, is another application. 
 Arrays of projectors, here simulated with a single device illuminating an array of mirrors, in combination 
with random illumination patterns can create a large synthetic aperture projector. As is the case for 
cameras, large apertures for projectors create a very shallow depth of field. In this particular application, 
individual depth slices of the scene can selectively be illuminated such as seen for the David statue 
on the right. While a shallow depth of field is sometimes desirable, when projecting on complex screens 
an increase in depth of field is actually required to guarantee focused image projection on all parts 
of the screen. Multiple overlapping projectors, each adjusted at a different focal length can be used 
to extend the depth of field of a single, virtual projector. Using a camera in the loop, the surface 
geometry can be scanned and a composite image from all devices computed that allows for the minimal amount 
of overall defocus in the system. An alternative, single device approach to extended depth of field 
projection has been proposed by us a few years ago. We replace the circular aperture of a projector with 
programmable liquid crystal array to build a coded aperture projector. The purpose of this device is 
an extended depth of field. We achieve this by jointly optimizing the display image and aperture pattern 
taking the contrast sensitivity of the human visual system into account. On the right you can see an 
image that has an increasing frequency on the x-axis and an increasing contrast on the y-axis. The underlying 
pattern should be a linear gradient in both axes, but looking at it we actually see the a curve that 
visualizes our frequency-dependent contrast detection threshold. Here are some results, the focused 
image on the left, an optically defocused projection in the center right, and the compensated image in 
the center. Corresponding close-ups are shown on the right. The aperture codes seen in the insets are 
computed so as to preserve the image frequencies that are most important for a human observer. Given 
this pattern, the projected image is computed by deconvolving it with that pattern as a blur kernel. 
 Let us also look at a completely different family of displays: head or eyeworn devices. With google 
goggles and project glass, these ideas finally start to emerge in the consumer market. The field of augmented 
reality, however, has been exploring the potential of head-mounted displays both optical see-through 
and video-based for decades. I would just like to highlight one display that has recently been proposed, 
which is not really an AR approach. We envision sunglasses, car windshields, and other commonly used 
see­through screens to have spatial light modulators integrated. Sunglasses of the future will be able 
to dim the environment light where it is actually bright, while preserving the visibility of shadows 
and other low-light parts of the scene. The underlying physical mechanisms require all-optical image 
processing by selectively blocking light that reaches the observer s eyes. This can be done by integrating 
a small camera into the sunglasses, processing the recorded video stream, and computing a modulation 
patterns for the see-through screen. We demonstrate applications to contrast manipulation, which can 
be used as optical tonemapping. Furthermore, we can optically highlight specific objects of interest 
by dimming the other parts of the scene. The human visual system pre-attentively processed this kind 
of information high level visual processing is not required. We also show that colors of the observed 
scene can be modulated, allowing for color de-metamerization or even recoloring objects to enhance the 
vision of color-deficient viewers. Lighting sensitive displays have also been an active area of investigation. 
Shree Nayar proposed such a display in 2004. The presented virtual content reacts to the environment 
illumination and, in this example, can be lit by a real light source. In a way, the displays acts as 
a window into a virtual world that is a seamless extension of the physical world with light interacting 
between the two. The underlying technology is actually rather simple: a wide field of view camera is 
integrated in the display frame and captures an environment map. This allows for real-time relighting 
of the content. Recently, Microsoft Research and Samsung have introduced PixelSense or sensors in pixels 
as part of Microsoft Surface 2.0. Thus is basically a big multi-touch LCD screen. What s special about 
is is that it has, just as regular LCDs, three subpixels for the individual color channels but in addition 
this screen has a fourth subpixel that acts as a sensor. Combined with infrared background illumination, 
an unprecedented resolution of the touch interaction can be captured. Please note that the captured images 
are only in focus when the fingers actually touch the surface. Years before pixel sense was announced 
by Microsoft, Matthew Hirsch and Douglas Lanman published a very related research project - the Bidi 
screen. Here, a screen is envision that also acts as a camera but captures a 4D light field rather than 
a 2D image as the Surface 2.0 does. This capability is achieved by placing the light sensitive elements 
at a slight offset from the actual LCD pixels. The LCD switches, at a very high refresh rate, between 
standard image display and a mask-pattern that allows the underlying sensor cells to capture the light 
field. These images show the prototype Bidi screen that implemented the concept using a camera and a 
diffuser behind the screen, hence it s a litter bigger. One application for such a depth-sensing screen 
is hovering gestural interaction - multi-touch in 3D. Another application is light-sensitive image display, 
just like Shree Nayar s original idea. In this implementation, however, the screen acts as the light 
sensing device and captures the 4D incident light field which allows for much more accurate relighting 
as compared to a conventional 2D camera. Another light sensitive display is the 6D display. This is 
a passive display which shows objects that are rear-illuminated by the real world. It uses a lenslet 
array that is flipped around and converts the incident 4D illumination light field into an interlaced 
2D pattern on a transparency; the latter encodes images showing a scene under exactly these lighting 
conditions. Combining the ideas of capturing and displaying 4D light fields with a single display surface 
results in the 8D display, which was recently built by Matthew Hirsch. Here, the viewer looks sees 3D 
objects without having to wear glasses. By capturing the incident light field, these objects can also 
be lit by physical illumination. Shadows are cast from the real world onto virtual objects in the most 
natural manner. While we will discuss glasses-free 3D displays in more detail in the second part of 
our course, I would just like to highlight an unconventional example of fabricating 3D displays. Scratch 
holography was explored in online tutorials by William Beaty in the 90s and what he basically did was 
using a compass to scratch many circles into the surface of a surface. When illuminated by a distant 
light source, such as the sun, the surface then creates specular highlights that create a most convincing 
impression of a 3D object floating around the screen. You can move around the screen it supports motion 
parallax and binocular disparity. A computational approach to scratch holography was proposed by Regg 
et al. in 2010. The authors decomposed a given 3D object into scratches and then automatically fabricated 
the surface with an engraving machine. While the results have a relatively low-resolution, this is a 
perfect example of a computational display that combines fabrication and computational pre­processing. 
 The idea of computational materials has been rather popular in the last few years. Weyrich and colleagues 
proposed a computational approach to fabricating micro-geometry so as to achieve a predefined reflectance 
behavior of the material. Custom reflections can, for instance, be teapots as seen in the top row here. 
A dynamic BRDF display was proposed by Matthias Hullin and presented at Siggraph emerging technologies 
last year in Vancouver. In case you did not have a chance to see it there, it s basically a small water 
tank with programmable actuators on the side. These are moved in a way to create wave-patterns that overlay 
and create a desired BRDF. In the bottom center, you see a checkerboard reflected in the water tank without 
any waves, whereas the bottom shows the same scene with wave patterns that create a custom reflectance 
of the surface. Spatially-varying reflectance can also be printed , as proposed by Wojciech Matusik 
in 2009. Here, a 2D multi-material desktop printer mixes different materials to achieve a custom reflectance 
of the printed patterns. Printed reflectance properties range from purely reflective to diffuse and anything 
in between. But not only 2D materials can be mixed to create a spatially varying reflectance, also 3D 
printers can facilitate new display capabilities by mixing different materials. Objet s Connex 500, a 
rather expensive 3D printer, actually has the capability mix two different print materials in addition 
to the support material. By combining those in clever ways, an approximation to arbitrary, spatially-varying 
subsurface scattering can be created on printed 3D objects, such as the bunnies and printed marble slabs 
on the right. On this note, I would like to point you to Neri Oxman s work at the MIT Media Lab. She 
has been exploring computational materials for design, art, and architecture for a number of years. So 
please see the website of her mediated matter group for more information, if you are interested in this 
topic: http://www.media.mit.edu/research/groups/mediated-matter Not only can materials with custom reflection 
properties be fabricated, similar concepts also apply to transmissive displays. The folks at Disney Research 
in Zurich have been actively working on that topic and proposed an approach to milling the surface of 
a refractive piece of plexiglas so that it creates caustics that form an image, such as Lena seen in 
the top. Rather than using a refractive surface, multiple stacked layers of light attenuators can also 
be used. Spaced by clear acrylic sheets, multiple inkjet-printed transparences contain pre­computed patterns 
that create different shadow images depending on the incident angle of a distant light source. Displays 
can also be used as probes that encode visual information for computer vision applications rather than 
for a human observer. Last year, we introduced light field probes for visualizing and reconstructing 
transparent refractive objects. For this purpose, a light field display consisting of a lenslet array 
with a high-frequency pattern is placed behind the transparent object of interest. When observed without 
the object from the central position, the probe just looks white. As the object refracts light, the incident 
angle of observed light rays on the background probe change and different ray angles on the background 
are color coded. Here s an example. The glass unicorn is almost transparent when observed in front of 
a uniform light source. On the right, we see the probe without an object. But when placing the object 
in front of the probe, the angles and magnitudes of refracted light rays are optically coded in color 
and saturation. This information can directly be used to reconstruct the surface of refractive objects. 
As only a single image is required, even dynamic surfaces such as wavy water can be acquired and reconstructed. 
 Computational displays are nor only fun but also have important applications in global health. A final 
family of computational displays for ophthalmological applications has been proposed over the last few 
years. In this example, we see how a smart phone display can be converted into a 4D light field display 
using an inexpensive clip on. By asking the user to interactively align a few patterns, this device has 
the capability of measuring the refractive errors of the observer. The display acts as the inverse of 
a Shack-Hartmann sensor that is often used in astronomical imaging to capture an incident wavefront. 
In this application, the user basically changes the patterns to align in some form in the perceived image, 
but the displayed pattern itself is predistorted so as to compensate for the refractive errors of the 
eye.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343488</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>29</pages>
		<display_no>5</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Virtual texturing in software and hardware]]></title>
		<page_from>1</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/2343483.2343488</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343488</url>
		<abstract>
			<par><![CDATA[<p>This course introduces partially resident textures (PRTs), a new GPU feature for virtual texturing, and contrasts them with software-based methods of virtual texturing. PRTs are available in the Southern Islands (Radeon HD 7xxx) family of graphics processors.</p> <p>The basic idea of virtual texturing is simple: instead of maintaining a separate texture for each object rendered on the screen, all textures are stored in a "virtual texture". The size of the virtual texture is on the order of billions of texels, and each object is assigned unique virtual-texture coordinates from the virtual texture. When used in a shader, the virtual-texture coordinates are translated into physical-texture coordinates, which are used to access the physical texture that contains the working set of all required tiles.</p> <p>Existing approaches implement the entire virtual texturing algorithm in software. The software is required to update the page table (another texture used for translating virtual texture coordinates to physical ones), perform address translation, and deal with hardware differences when it comes to supported texture types, formats, and filtering modes. The first part of the course outlines this process and discusses difficulties encountered when this technology is deployed in RAGE. PRTs eliminate the need for maintaining the page table and address translation and provide support for all texture types, formats, and filtering modes. The second part of the course describes the hardware architecture as it relates to PRTs. The third part of the course introduce the new AMD sparse-texture OpenGL extension that exposes PRTs to software applications. The course includes several PRT use cases, a technical demo, and a summary of the strengths and weaknesses of PRT technology.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738887</person_id>
				<author_profile_id><![CDATA[81335495702]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Juraj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Obert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738888</person_id>
				<author_profile_id><![CDATA[81504687994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[J.]]></first_name>
				<middle_name><![CDATA[M. P.]]></middle_name>
				<last_name><![CDATA[van Waveren]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ID Software]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738889</person_id>
				<author_profile_id><![CDATA[81474698573]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sellers]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Micro Devices]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Virtual Texturing in Software and Hardware SIGGRAPH 2012 Courses Los Angeles, August 5-9 2012 Juraj 
Obert Advanced Micro Devices J.M.P. van Waveren ID Software Graham Sellers Advanced Micro Devices Abstract 
The objective of this course is to introduce Partially Resident Textures (PRTs), a new GPU feature for 
virtual texturing, and contrast them with traditional, software-based, meth­ods used for virtual texturing. 
PRTs are currently available in the Southern Islands (Radeon HD 7xxx) family of graphics processors. 
Software-based virtual texturing methods have been used in computer games since 2007. The technology 
was popularized by John Carmack (ID Software) and ID Software s im­plementation of virtual texturing 
is termed Megatexture. The .rst title featuring virtual texturing was Enemy Territory: Quake Wars (Splash 
Damage), followed by other titles such as Brink (Splash Damage) or RAGE (ID Software). The basic idea 
of virtual texturing is simple instead of maintaining a separate texture for each object rendered on 
the screen, all textures are stored in a massive virtual texture . The size of the virtual texture is 
in the order of billions of texels and each object is assigned unique virtual texture coordinates from 
the virtual texture. When used in a shader, the virtual texture coordinates are translated into physical 
texture coordinates, which are used to access the physical texture that contains the working set of all 
required tiles. Existing approaches implement the entire virtual texturing algorithm in software. The 
software is required to update the page table (yet another texture used for translating virtual texture 
coordinates into physical ones), perform address translation (by dependent texture lookups in a shader) 
and deal with hardware di.erences when it comes to supported texture types, formats and .ltering modes. 
The .rst part of the course will outline this process and discuss di.culties encountered when deploying 
this technology in RAGE. Partially Resident Textures are a new hardware technology that provides direct 
hardware support for virtual texturing. PRTs eliminate the need for maintaining the page table (the hardware 
does it) and address translation (the hardware does it) as well as provide support for all texture types/formats 
and .ltering modes (again the hardware does it all). The second part of the course will describe the 
hardware architecture as it relates to PRTs. The third part of the course will introduce the new AMD 
sparse texture OpenGL exten­sion that exposes PRTs to software applications. We will present several 
PRT use cases, a tech demo and discuss strengths and weaknesses of this technology. The course will conclude 
with a discussion of the limitations present in today s PRT hardware and future plans for PRT development. 
About the Authors Juraj Obert Advanced Micro Devices juraj.obert@amd.com Juraj Obert is a software engineer 
in the OpenGL driver team at AMD, where he focuses on performance tuning, feature development and software 
architecture. He has over 10 years of experience in graphics programming and holds a PhD in Computer 
Science from the University of Central Florida. His research work was previously published at both SIGGRAPH 
and Eurographics conferences. J.M.P. van Waveren ID Software mrelusive@idsoftware.com J.M.P. van Waveren 
studied computer science at Delft University of Technology in the Netherlands. He has been developing 
technology for computer games for over a decade and has been involved in the research for, and development 
of various triple-A game titles such as: Quake III Arena, Return to Castle Wolfenstein, DOOM III and 
RAGE.  Graham Sellers Advanced Micro Devices graham.sellers@amd.com Graham Sellers is the manager of 
the OpenGL driver team at AMD. He represents AMD at the OpenGL ARB and Khronos Group and is responsible 
for the design and implementation of new features in AMD s OpenGL implementation, including extensions 
and new versions of the OpenGL API. He is the author of or a contributor to over 20 OpenGL extensions, 
many of which are now part of the core API speci.cation. He is listed as a contributor to OpenGL from 
version 3.2 onwards. He is also a co-author of the OpenGL SuperBible and the upcoming edition of the 
OpenGL Programming Guide. He holds a Masters degree in Engineering from the University of Southampton, 
UK. Course Outline 10 minutes: Introduction Juraj Obert 1. Introduction to the course and its goals, 
course overview and introducing all speakers 2. Introduction to virtual texturing  25 minutes: Challenges 
of Software Virtual Texturing J.M.P. van Waveren 1. Virtual texturing in RAGE 2. Di.erent virtual to 
physical translations 3. Texture .ltering 4. Feedback rendering  25 30 minutes: Hardware Virtual Texturing 
 Partially Resident Textures Juraj Obert 1. Hardware architecture 2. Driver support (OpenGL, DX) 3. 
OpenGL AMD sparse texture extension (Part 1)  20 minutes: Demo &#38; Future development Graham Sellers 
1. OpenGL AMD sparse texture extension (Part 2) 2. Tech demo 3. Future development  5 10 minutes: 
Conclusion and Discussion Juraj Obert, J.M.P. van Waveren, Graham Sellers Contents 1 Introduction 5 
1.1 VirtualTexturinginSoftware .......................... 5 1.1.1 VirtualTextures ............................. 
5 1.1.2 TextureCoordinates ........................... 6 1.1.3 Rendering................................. 
6 1.2 VirtualTexturinginHardware.......................... 6 1.2.1 PartiallyResidentTextures ....................... 
7 2 Challenges in Software Virtual Texturing 9 2.1 AddressTranslation................................ 
10 2.2 TextureFiltering ................................. 13 2.3 FeedbackRendering................................ 
15 3 Hardware Virtual Texturing Partially Resident Textures 17 3.1 VirtualMemory .................................. 
17 3.2 Page/TileResidencyInformation ........................ 22 4 PRT Use Cases &#38; Future Development 
23 4.1 VeryLargeTextureArrays ............................ 23 4.2 IncompleteMip-mapChains ........................... 
24 4.3 TrulySparseTextures .............................. 25 4.4 Current Limitations and Thoughts on 
the Future . . . . . . . . . . . . . . . 25 A Code Samples 27 Chapter 1  Introduction These course notes 
describe software and hardware virtual texturing methods available to graphics developers in early 2012. 
This course is divided into 3 main parts Virtual Texturing in Software, Virtual Texturing in Hardware, 
and Demo/Future Development. The objective of the course it to familiarize the reader with existing software 
techniques, introduce (in detail) Partially Resident Textures (a new hardware technique for virtual texturing) 
and provide a comparison between hardware and software techniques. At the end of the course, the reader 
should have enough knowledge to understand the strengths and weaknesses of all approaches. 1.1 Virtual 
Texturing in Software 1.1.1 Virtual Textures Virtual texturing refers to a texturing technique in which 
multiple object textures are stored in one massive texture called the virtual texture. The size of the 
virtual texture generally exceeds the available storage space1 in modern GPUs (RAGE used virtual textures 
that contain 128k × 128k texels). When rendering using a virtual texture, only parts of it are made resident 
on the GPU and accessed from shaders. For the purpose of this course, the regions of the texture resident 
in GPU memory at any given time are termed the working set. In order to allow applications to selectively 
upload texture regions to the GPU, the virtual texture is subdivided into virtual tiles (pages). The 
working set is the set of tiles resident in the GPU memory and it always is a subset of the set of all 
tiles in the entire virtual texture. 1The terms GPU memory and storage space will be used interchangeably 
throughout this course. Both refer to the memory the GPU is able to access and render from. For high-performance 
texturing purposes, local GPU memory is used almost exclusively by most applications. 1.1.2 Texture 
Coordinates Surfaces of objects referencing the virtual texture are parameterized using virtual texture 
coordinates. Virtual texture coordinates refer to texels in the virtual texture. At rendering time, virtual 
texture coordinates are translated to physical texture coordinates which refer to texels in the working 
set (i.e. the virtual tiles that are resident in the GPU memory at the time the shader is invoked). In 
GPU terms, the working set is stored in the GPU memory as a physical texture. The physical texture is 
smaller than the virtual one and serves as a container for tiles that need to be accessible by the current 
draw call. Since the texture coordinate space of the virtual texture is di.erent from the texture coordinate 
space of the physical texture, a mapping must exist between virtual and physical texture coordinates. 
This mapping is stored in another texture termed the page table and the conversion of virtual texture 
coordinates to physical texture coordinates is termed virtual­to-physical address translation. 1.1.3 
Rendering When rendering with virtual textures, the address translation is typically performed inside 
a shader by a lookup into the page table texture. The input to the lookup is a set of virtual texture 
coordinates and the result of the lookup is a set of physical texture coordinates. Once obtained, the 
physical textures coordinates are used to fetch texture data from the physical texture. The entire pipeline 
is depicted in Figure 1.1. Notice that the rendering pass is preceded by a feedback pass that determines 
which texture pages are required to be resident for the next frame. The feedback pass is traditionally 
implemented on the GPU and requires a readback operation that transfers the IDs of required pages to 
the CPU. In the analysis pass, the application determines which pages need to be uploaded and which are 
already resident based on what it knows about the last rendered frame. The non-resident pages that are 
required for the next frame are then uploaded to the GPU and the page table is updated. Finally, the 
frame is rendered using the virtual texture.  1.2 Virtual Texturing in Hardware While sounding fairly 
easy in theory, implementing virtual texturing in software comes with a great deal of issues. The issues 
encountered when developing RAGE will be described in detail in Chapter 2. For now, let us mention some 
just some of them: How to deal with di.erent texture .ltering modes across virtual tiles? (nearest, 
linear, aniso, etc.)  How to deal with mipmapped textures?  How to deal with di.erent texture formats? 
 How to deal with special texture types? (such as cubemaps)  How to deal with multiple platforms? 
 While almost all of these issues can be resolved on the software side to a quite high level of satisfaction, 
dealing with all of them requires a non-negligible amount of e.ort and should not be required on modern 
GPUs. We will discuss these issues in detail in the next chapter. 1.2.1 Partially Resident Textures In 
order to diminish the amount e.ort game developers need to put into the software imple­mentation of virtual 
texturing, independent hardware vendors are now moving toward sup­porting virtual texturing directly 
in hardware. AMD s Partially Resident Textures (PRTs) represent the .rst implementation of virtual texturing 
directly in hardware. Compared to existing implementations of virtual texturing in software, PRTs provide 
 several advantages, among which are: Support for all texture types, formats and .ltering modes  Elimination 
of dependent texture fetches during rendering  Support for mipmapped textures (all types)  Access to 
tile residency information directly from shaders  Support for massive texture size  In Chapter 3, we 
will describe the hardware support for PRTs as it exists in the Southern Islands family of AMD GPUs (Radeon 
HD 7xxx Series). We will then discuss how the PRT functionality is exposed to developers (OpenGL AMD 
sparse texture extension) and provide examples in the form of screenshots and sample source code. The 
rest of this document is organized as follows. In Chapter 2, we describe the imple­mentation of software 
virtual texture as used in RAGE. We discuss the main challenges en­countered when working with software 
virtual textures. In Chapter 3, we introduce Partially Resident Textures and give an overview of the 
hardware architecture that supports them. We discuss how PRTs eliminate many of problems inherent to 
software virtual textures. Finally, in Chapter 4, we propose several PRT use cases, describe the AMD 
sparse texture OpenGL extension and provide outlook on future hardware development. Chapter 2  Challenges 
in Software Virtual Texturing Modern simulations increasingly require the display of very large, uniquely 
textured worlds at interactive rates. In large outdoor environments and also high detail indoor environments, 
like those displayed in the computer game RAGE (see Figure 2.1), the unique texture detail requires signi.cant 
storage and bandwidth. Virtual textures reduce the cost of unique texture data by providing a sparse 
representation which does not require all of the data to be present for rendering while leaving the majority 
of the texture data in highly compressed form on secondary storage. Virtual textures not only provide 
for a reduction of memory requirements but also im­proved rendering performance through a reduction in 
both graphics driver and GPU state changes because many surfaces can use a single virtual texture without 
the need for per sur­face texture selection. Several practical examples are discussed to emphasize the 
challenges of implementing virtual textures in software and viable solutions are presented. These include 
solutions for address translation, texture .ltering, compression, caching, and streaming. 2.1 Address 
Translation A virtual texture is divided into small pages that are loaded into a pool of resident physical 
pages as required for rendering. These small pages are square blocks of texels, typically on the order 
of 128 × 128. The pool with physical pages is a fully resident texture that is logically subdivided into 
such square blocks of texels. While a virtual texture can be very large (say a million pages) and is 
never fully resident in video memory, the texture that holds the pool of physical pages is fully resident 
but much smaller (typically only 4096 × 4096 texels or 1024 pages). Virtual texture pages are mapped 
to physical texture pages, and during rendering virtual addresses need to be translated to physical ones. 
In its simplest form, the virtual to physical translation is equivalent to .nding the de­sired level 
of detail (LOD) by using the virtual texture address to walk the quad-tree that represents the mip hierarchy 
of the currently resident texture pages. Every node in the quad-tree provides a scale and bias that will 
convert a virtual address inside a virtual page to a physical address inside a physical page. The scale 
is the ratio between the size of the virtual mip level and the size of the physical texture. The bias 
is the o.set to the physical page in the physical texture, minus the scaled o.set to the virtual page 
in the virtual mip level. While walking the quad-tree to .nd the desired LOD at a given virtual address, 
either the scale and bias for the desired LOD are found, or the quad-tree terminates early and the scale 
and bias at the .nal node are used for the address translation. In the latter case, the address translation 
falls back to a page from a coarser mip level because the texture page for the desired .ner mip level 
is not yet available in pool of physical pages. Figure 2.2 shows an overview of the virtual to physical 
address translation and the calculation of the scale and bias. Instead of using a quad-tree data structure, 
the virtual to physical translation can be implemented in various di.erent ways. These di.erent virtual 
to physical translations all implement a trade between: 1. Cost of virtual to physical translation during 
rendering 2. Page table memory requirements 3. Cost of page table updates  For instance, the quad-tree 
data structure with currently resident texture pages allows for a minimal memory page table, but it has 
the worst case access latency because it requires a dependent read for each .ner level of detail accessed. 
A straightforward approach to implementing the virtual to physical translation is looking up the scale 
and bias in a mip-mapped FP32x4 texture with one texel per virtual page. In e.ect this mip-mapped texture 
stores the complete quad-tree data structure with a node for every virtual texture page whether it is 
resident or not. A regular lookup into this page table texture allows the texture hardware to be used 
to compute the nearest texel of the nearest mip level that corresponds to the virtual texture page for 
the desired LOD at a given virtual address. The texture lookup is biased with the base-two logarithm 
of the page width to account for the size di.erence between the virtual texture and the page table texture. 
The scale and bias retrieved from this page table texture can be used directly to map a virtual address 
to a physical one. A texel of this texture will store a scale and bias for a texture page from a coarser 
mip if the desired .ner mip is not yet available in the pool of physical pages. Even though this results 
in one of the simplest implementations the page table tends to be rather large (21.33 MB for a virtual 
texture with 1024 × 1024 virtual pages) and FP32x4 texture lookups may be costly on some hardware. To 
reduce the memory requirements, the page table can be split into two textures. The .rst texture is mip-mapped 
with one texel per virtual page. Once again a regular lookup into this texture allows the texture hardware 
to be used to compute the nearest texel of the nearest mip level that corresponds to the virtual texture 
page for the desired LOD at a given virtual address. Instead of storing a scale and bias, each 2-byte 
texel of this texture contains the (x,y) coordinates of the physical page to be used for the virtual 
page. A texel of this texture will point to a physical page from a coarser mip if the desired .ner mip 
is not yet available. The second texture is a non-mip-mapped FP32x4 texture with one texel per physical 
page. A texel from this texture contains the scale and bias (ST-scale, S-bias, T-bias) necessary to map 
a virtual texture coordinate to a physical texture coordinate for that page. This approach saves memory 
by storing the .oating-point scale and bias in a much smaller (typically 32 × 32) texture with one texel 
per physical page, while allowing access to this texture at the cost of a fully resident much larger 
(typically 1024 × 1024) texture with only two bytes per virtual page. This memory optimization costs 
the latency of a dependent texture read, but it saves 8x the memory compared to storing the .oating-point 
scale and bias in a texture with one texel per virtual page. Instead of storing a scale and bias in textures, 
the virtual to physical mapping can also be calculated in a fragment program based on the coordinates 
and mip level of a physical page. The coordinates and mip level of a physical page can be stored in a 
single mip-mapped texture which avoids the latency of a dependent texture read to fetch a scale and bias. 
Here also, a regular lookup into this texture allows the texture hardware to be used to compute the nearest 
texel of the nearest mip level that corresponds to the virtual texture page for the desired LOD at a 
given virtual address. The coordinates of the physical page retrieved from this texture are used to calculate 
the o.set to the top-left corner of the physical page in the physical texture. To calculate the complete 
physical address, the o.set within the virtual page needs to be scaled and added to the top-left corner 
of the physical page. This o.set within the virtual page needs to be calculated for the correct mip level 
which is done by .rst multiplying the virtual texture coordinates with the width in pages of the physical 
page s mip level ( virtual pages wide / 2mip ) and then applying the frac() function. The fraction is 
scaled into the correct range before being added to the physical page o.set. The physical page coordinates 
and the mip level can be stored in an 8-bit per component RGBA texture. It is also possible to store 
the physical page coordinates in the 5-bit components, and the mip level in the 6 bit component of a 
5:6:5: RGB texture. However, this limits the size of the physical page texture and the number of mip 
levels of the virtual texture. On DirectX9 class hardware there are also various di.erences in the way 
these texture components are made available as .oating-point values in a fragment program which signi.cantly 
complicates the calculation of the virtual to physical translation. The virtual to physical translation 
using a mip-mapped page table texture is a lot faster than using a quad-tree structure with a dependent 
read for each .ner level of detail accessed. However, compared to storing only the quad-tree, page table 
updates are much more expen­sive when using a mip-mapped page table texture. Consider when the .rst page 
of a virtual texture is mapped: the entire page table texture must be populated with a single texel value. 
When the next .ner page is mapped in, one quarter of the texels must be updated, and so on. Fortunately 
large page table updates happen infrequently. Instead of using a quad-tree page table or a page table 
texture, a hash table can be used to provide a middle ground between access latency, memory footprint 
and compute. Only resident texture pages are stored in the hash table. A virtual page is found in the 
hash table with a hash key calculated from the mip level and (x,y) coordinates of the page. A good hash 
key function in combination with a small hash table typically results in very few collisions, allowing 
the lookup of most pages with a single memory access. The spatial index of a page in the virtual texture 
quad-tree modulo the hash table size can be used as a hash key. However, better results are achieved 
if the (x,y) coordinates of a virtual page are .rst remapped within the mip level such that pages that 
are close to each other in the quad-tree do not map to the same hash table entry. The hash table does 
not provide an automatic mechanism to fall back to a texture page from a coarser mip if a desired .ner 
mip is not yet available in the pool of physical pages. Instead, when the desired page is not found in 
the hash table, the hash key for the next coarser page will have to be calculated in an attempt to fetch 
the next coarser page from the hash table. If the next coarser page is also not resident this process 
will have to be repeated until a valid page is found. On average, when most desired pages are resident, 
the hash table access latency is much better than a quad-tree page table. However, in the worst case, 
when few or no pages are resident, multiple hash keys have to be calculated and multiple memory accesses 
are required. 2.2 Texture Filtering One of the unfortunate complexities of software virtual textures 
is that the texture unit, being unaware of the actual texture pages, cannot .lter across page boundaries. 
Instead of using the .lter hardware, it is too costly to implement texture .ltering completely in a fragment 
program. In order to support hardware bi-linear .ltering, each physical texture page must have a border 
of texels around it. Implementations of software virtual textures are also not able to transparently 
support tri-linear .ltering. A straightforward way to allow for hardware accelerated tri-linear .ltering 
is to store one mip level for the texture with physical pages but this comes at the expense of a 25% 
increase in memory footprint and an increase in compute and bandwidth to create and upload this mip level 
for every physical page that is updated. Another way to implement tri-linear .ltering is to access two 
virtual pages during rendering, determining the LOD fraction between them and computing the weighted 
average. Even with a mip-mapped page table texture implementation, the cost of a single virtual to physical 
translation carries signi.cant overhead. Hardware accelerated anisotropic .ltering can be supported if 
the page border is wider than 1 texel. For instance, a 4-texel border is used around each physical page. 
The 4-texel border maps well to the 4x4 block size of a DXT compressed physical page texture and allows 
for reasonable quality anisotropic .ltering with the maximum anisotropy set to 4. Ideally, the virtual 
texture coordinate is used to compute the anisotropic footprint and TXD (tex2Dgrad()) is used for fetching 
into physical pages. This requires scaling the deriva­tives to factor in the di.erent scales that texture 
coordinates will have when they come from di.erent mip levels. The scale factor is the ratio between 
the size of the virtual mip level and the size of the physical texture. When using a virtual to physical 
translation with one or more mapping textures, this scale factor has to be stored separately using an 
additional texture component. No additional data needs to be stored when the virtual to physical mapping 
is calculated in the fragment. Calculating and scaling the derivatives adds fragment program complexity 
and on most hardware TXD is more expensive which may make this solution unattractive from a perfor­mance 
standpoint. Instead, hardware accelerated anisotropy on the physical texture coordi­nate with implicitly 
computed derivatives can be used. This results in erroneous footprints for quad-fragments that cross 
virtual page boundaries because the physical texture space is discontinuous at page boundaries. Texture 
pages that are adjacent in virtual texture space do not necessarily map to physical pages that are next 
to each other, let alone close to each other. However, even though for virtual page crossings the derivatives 
may become arbitrar­ily large with an arbitrary sign, the anisotropic footprint is still bounded to a 
single physical texture page because the maximum anisotropy is equal to the border width. The erroneous 
footprints at page boundaries are a reasonable performance vs. quality trade-o. on most hardware. The 
quality is surprisingly good and the erroneous footprints are only noticeable under signi.cant magni.cation. 
Normally, when fetching texture data from a mip-mapped texture the anisotropic foot­print is sampled 
using texels from multiple mip levels. Even when an additional mip level is provided for the physical 
texture to allow tri-linear .ltering, the page table is point-sampled using a regular texture lookup 
unaware of the anisotropic texture fetch that follows. As such the anisotropic texture fetch typically 
ends up sampling a physical texture page from a mip level that is too coarse to provide useful texture 
detail. To provide additional texture detail for an anisotropic texture fetch, the page table lookup 
can be biased with the negative base-two logarithm of the maximum anisotropy. This allows the anisotropic 
texture fetch to work with additional texture detail on surfaces at an oblique angle to the viewer where 
the sampled footprint is maximized (anisotropic). However, this can cause noticeable shim­mering or aliasing 
on surfaces that are orthogonal to the view direction where the sampled footprint is minimal (isotropic). 
To improve the quality, the bias of the page table texture lookup can be dynamically adjusted based on 
the anisotropy. The calculation of the bias is shown below. const float minAnisoBias = -2; // -log2( 
maxAniso ) float2 dx = ddx( virtCoords.xy ); float2 dy = ddy( virtCoords.xy ); float px = dot( dx, dx 
); float py = dot( dy, dy ); float maxLod = 0.5 * log2( max( px, py ) ); // log2(sqrt()) = 0.5*log2() 
float minLod = 0.5 * log2( min( px, py ) ); float anisoBias = max( minLod -maxLod, minAnisoBias ); Obviously 
calculating the LOD bias adds computational complexity to the fragment program. It is interesting to 
note however, that compared to using a constant LOD bias some performance is gained back due to better 
texture cache usage. For surfaces that are mostly orthogonal to the view direction, the dynamic LOD bias 
causes a mip level to be selected where the texture samples are closer to each other. Nevertheless selecting 
a mip level based on the anisotropic footprint is often unattractive from a performance standpoint because 
of the additional fragment program complexity. Instead, using a maximum anisotropy of 4 and a page table 
texture lookup biased with a constant negative 2 typically results in a reasonable trade between quality 
and performance where surfaces at an oblique angle to the viewer are signi.cantly sharper while minimal 
shimmering or aliasing appears on surfaces orthogonal to the view direction. 2.3 Feedback Rendering 
While a sparse representation makes it possible to render with a partially resident texture, feedback 
is necessary for determining which parts of the texture need to be resident. Texture feedback needs to 
be rendered to a separate bu.er to store the virtual page coordinates (x,y), desired mip level, and virtual 
texture ID (to allow multiple virtual textures). This information is then used to pull in the texture 
pages needed to render the scene. The feedback can be rendered in a separate rendering pass or to an 
additional render target during an existing rendering pass. An advantage of rendering the feedback is 
that the feedback is properly depth tested, so the virtual texture pipeline is not stressed with requests 
for texture pages that are ultimately invisible. When a separate rendering pass is used it is .ne for 
the feedback to be rendered at a signi.cantly lower resolution (say 10x smaller). Only the texture coordinates 
and not the actual texture data are used in the feedback rendering pass which means that alpha tested 
surfaces are considered completely opaque. To properly pull in texture data that is visible through an 
alpha tested surface any such surfaces that are not completely opaque could be rendered randomly every 
so many frames to the feedback bu.er. Similarly, when a surface uses multiple virtual texture sources, 
these sources could be alternated every render frame such that over time all the necessary texture data 
is pulled in. Unfortunately, this has the tendency to destabilize the virtual texture pipeline because 
a di.erent set of texture pages is requested every frame even when the scene does not change. The pages 
that are requested one frame may end up replacing the pages that were requested for the same surface 
the previous frame. As a result, the system may never stabilize and pages may be continuously replaced 
without ever pulling in the highest detail texture data necessary for rendering the scene. When a surface 
uses multiple virtual texture sources the solution is to alternate the di.erent texture sources in screen 
space, where every other pixel of the feedback bu.er pulls texture data from a di.erent source. When 
rendering from two di.erent sources, this results in a simple checkerboard pattern but more complex patterns 
can be used when rendering from more than two sources. This approach may increase the chance of undersampling 
the feedback when a surface is very small and covers very few pixels, but this turns out not to be a 
problem in practice. A similar approach can be used for alpha tested surfaces where every other pixel 
of the feedback bu.er covered by an alpha tested surfaces is considered either fully transparent or fully 
opaque. When a simple checkerboard pattern is used to alternate between fully transparent and fully opaque, 
not all the texture data may be pulled in for a scene with multiple alpha tested surfaces stacked on 
top of each other. Using more complex patterns and a di.erent pattern per alpha tested surface can alleviate 
this problem. This is similar to rendering with screen-door transparency or alpha-to-coverage. The results 
of the feedback rendering pass are analyzed in a separate process. This process could stall and wait 
for the feedback render but it is typically .ne to use a frame old data and incur a frame of latency. 
The feedback analysis walks the screen bu.er and condenses the page information into a list with unique 
pages. E.ectively, the feedback analysis creates the quad-tree with all the pages that need to be resident 
to properly render the current scene. The analysis process sorts the pages on priority. First, the priority 
is set such that the farther away the desired mip level is from the actual resident mip level, the higher 
the priority. Second, the priority increases as the number of samples for a particular page increases 
in the feedback bu.er. The virtual texture system uses the sorted pages to maintain residency of already 
resident visible pages and to .rst stream in the non-resident pages that will most improve the visual 
quality of the currently rendered scene. Chapter 3  Hardware Virtual Texturing Partially Resident Textures 
Partially Resident Textures provide direct hardware support for the majority of all tasks present in 
the virtual texturing pipeline. Application developers are no longer required to deal with managing of 
the page table, address translation and/or .guring out which texture types need to be supported. The 
responsibility of managing the virtual nature of a texture is moved toward the hardware and the driver. 
Partially Resident Textures are supported in all AMD Radeon HD 7xxx GPUs. The functionality is exposed 
to application developers via the AMD sparse texture OpenGL ex­tension and an upcoming DX extension (exact 
details are not known at the moment). PRT support in hardware relies on 3 core components: HW Virtual 
Memory subsystem  Page Residency information propagation  Driver stack support for e.cient mapping/unmapping 
 3.1 Virtual Memory Memory addresses used to fetch texture data on Radeon 7xxx GPUs are virtual. When 
a shader attempts to fetch a texel from a texture using UV coordinates, a dedicated GPU block .rst computes 
the virtual memory address of the texel (or a block of texels if .ltering is used). The address computation 
depends on the texture type, format, UV coordinate values, desired mipmap level, o.set, internal texture 
tiling, etc. The virtual memory address is then fed to the virtual memory subsystem. The VM subsystem 
performs the virtual-to-physical address translation and then initiates a read operation from the physical 
memory. When the read completes, the texel data is returned to the shader. The virtual-to-physical address 
translation inside the VM subsystem leverages a dedi­cated hardware page table. This is in stark contrast 
to software virtual texturing techniques in which the page table is just another texture managed by the 
application. A dedicated hardware page table provides several bene.ts when compared to a software one. 
Uni.ed format When dealing with software (texture) page tables, the application must decide on its size, 
format, etc. None of this is required with hardware page tables as they have a uni.ed format and support 
all texture types, formats and sizes. The advantage of this is that applications can use di.erent formats 
for di.erent purposes, without having to rewrite their address encoding schemes. The only downside of 
the uni.ed format is that texture tiles might have di.erent dimen­sions based on what type/format they 
are using. In the current hardware, the page size is .xed to 64kB, which means that a 32-bit RGBA8 texture 
will have tile dimensions of 128 × 128 texels. PRT tile dimensions for uncompressed 2D textures are listed 
in Table 3.1. Texture BPP PRT Tile Width PRT Tile Height 128 64 64 64 128 64 32 128 128 16 256 128 8 
256 256 Figure 3.1: PRT tile dimensions for uncompressed 2D textures.  Filtering Hardware page tables 
provide support for all texture .ltering modes. With software page tables, certain .ltering modes (e.g. 
trilinear or anisotropic .ltering) are very di.cult to implement in a robust fashion. As discussed in 
the previous chapter, this is because the physical texture coordinates are not contiguous across page 
boundaries. The hardware is not aware of any page boundaries and therefore cannot .lter across pages. 
On the other hand, PRT-enabled hardware supports .ltering across page boundaries without issues. Two-level 
structure Software page tables used in virtual texturing are traditionally only one-level (in the virtual 
memory terminology, they only contain the PTEs). This impacts their sizes and does not allow for any 
kind of compression. Hardware page tables can be two-level (they contain both PDEs and PTEs), which decreases 
their memory footprint if the virtual address space is only sparsely populated. Address translation performance 
Sampling from a virtual texture with software page tables amounts to a texture fetch using virtual texture 
coordinates followed by another texture fetch using physical texture coordinates. Both of these require 
roundtrips between the shader and memory, which can incur signi.cant performance penalties should cache 
trashing occur. With hardware page tables, the address translation happens as a part of the texture fetch 
that uses virtual texture coordinates directly, diminishing the bandwidth requirements by 50% in the 
general case. Simpli.ed programming When dealing with software page tables, the application at­tempting 
to map/unmap a page needs to take care of the page table updates. Hardware page tables are programmed 
by the SW driver stack when the client application commits/clears individual texture tiles. The application 
is only required to specify which parts of the texture should be resident for the upcoming commands. 
Caching e.ciency Hardware page tables can take advantage of special HW caches to speed up lookups and 
the virtual-to-physical address translation. This is not possible with software page tables, as they 
go down the traditional texture fetch hardware path. Consider the following fragment shader that performs 
sampling using a software (texture) page table: uniform sampler2D samplerPageTable; // page table uniform 
sampler2D samplerPhysTexture; // physical texture in vec4 virtUV; // virtual texture coordinates out 
vec4 color; // output color vec2 getPhysUV(vec4 pte); // translation function void main() { vec4 pte 
= texture(samplerPageTable, virtUV.xy); // 1 vec2 physUV = getPhysUV(pte); // 2 color = texture(samplerPhysTexture, 
physUV.xy); // 3 } Figure 3.2 illustrates what happens inside the hardware during software-based virtual­to-physical 
address translation operation. In the .rst texture fetch invocation (line 1), the virtual texture coordinates 
are used to look up the PTE (page table entry). The PTE is an application-speci.c data structure stored 
in the page table texture. In the next step (line 2), the PTE is converted to physical texture coordinates, 
again by an application-speci.c function that deals with the encoding scheme, .ltering, formats, etc. 
Finally (line 3), the physical texture coordinates are used to fetch the texture data. From the hardware 
s point of view, both texture fetches (lines 1 and 3) are pretty much identical except that they are 
accessing di.erent textures. One important detail to notice is that both texture fetches are dependent, 
i.e. the second one cannot be launched before the .rst one completes. Dependent texture fetches are 
generally not a good approach when high-performance is desirable. Now consider a di.erent fragment shader 
that takes advantage of a PRT-enabled texture fetch (the sparseTexture() texture sampling instruction 
is the new instruction introduced in the AMD sparse texture OpenGL extension details in Chapter 4): 
uniform sampler2D samplerVirtTexture; in vec4 virtUV; out vec4 color; // // virtual output texture color 
coordinates void main() { // sparse texture fetch int code = sparseTexture(samplerVirtTexture, virtUV.xy, 
color); } The shader no longer uses two dependent texture fetches, but instead, the virtual-to­physical 
address translation is performed directly in hardware based on the virtual texture coordinates passed 
to the sampling function. The hardware function is depicted in Figure 3.3.  3.2 Page/Tile Residency 
Information The previous section introduced the concept of hardware page tables and discussed their advantages 
in the context of fetching data from pages that we know are resident in GPU memory. However, a completely 
di.erent class of algorithms can be based on the idea of determining page residency information at runtime. 
Page fault is a virtual memory event that occurs when the client attempts to translate a virtual address 
that does not have an entry in the page table (i.e. the page is not mapped to any physical address). 
In virtual texturing systems, it is very convenient when a shader is able to determine page residency 
status in an e.cient manner. Querying page residency status from software page tables is straightforward 
 the shader performs a texture fetch from the page table texture (as in Figure 3.2) and then tests the 
resulting address for validity (an application speci.c value can be stored in the page table to indicate 
unmapped access). The cost of the texture fetch is equal to the cost of any other texture fetch. With 
PRTs, the hardware directly supports propagating of the page residency information from the page table 
to the shader core. In other words, if the shader attempts to read from an unmapped virtual address, 
the hardware will report failure without having to perform a read from the texture memory. The return 
code is referred to as a NACK in the rest of this document. The sequence of events is illustrated in 
Figure 3.4. Chapter 4  PRT Use Cases &#38; Future Development In the .rst part of this chapter, we 
cover some use cases of PRTs in the real world and demonstrate techniques that may be implemented using 
the PRT feature. Use cases are enumerated below. In a second part of this chapter, we address some current 
limitations of the approach implemented in AMD s hardware, and some thoughts on future directions. 4.1 
Very Large Texture Arrays First, we discuss the use of very large texture arrays as an application managed 
cache of textures that may be used to virtually eliminate texture binds in a real-time application. Under 
this scheme, one, very large texture array is allocated for each class of texture (say, di.use albedo, 
specular coe.cients, normal maps, etc.). These array textures are bound and left bound for the lifetime 
of the application. Each material in a scene is assigned a slice of the array. On current hardware, we 
are able to support more than 8,000 slices in a single texture array, allowing more than 8,000 unique 
materials to be represented in a single array. Of course, a moderate sized array texture (of the order 
of 2K × 2K texels) with 8,000 slices consumes more than 10s of gigabytes of address space and so it is 
impossible to ensure that all of the texture data is resident at all times. However, assuming that the 
live data set for a single rendering call can be made resident (i.e., it .ts in GPU memory), several 
advantages arise from using texture arrays with sparse textures. The .rst of these is that the layout 
of textures in memory is consistent between materials. All materials have access to their di.use albedo, 
specular coe.cient and normal map textures if they have them. For those materials that do not have some 
of those com­ponents, then those slices of the array may be left non-resident without consuming precious 
physical memory only virtual memory is reserved. This simpli.es shader development as it allows texture 
layout to be declared boiler-plate style. The second, and perhaps more important aspect to this approach 
is that the texture array can be considered an application-controlled cache. When a material is about 
to be rendered, the application must ensure that the relevant slices of the appropriate texture arrays 
are present in GPU memory. However, there is no need to bind new textures as all of the texture data 
is actually part of the same set of array textures. As materials are rendered, new slices of the array 
are uploaded to the GPU as needed and then left resident. If the same slice is needed again, then it 
is already resident and no texture upload or rebind operation is necessary. If, during texture upload, 
an out-of-memory error is detected, slices that are no longer needed may be discarded and a new attempt 
to make pages resident made. If, during the rendering of a single frame, all textures needed .t into 
GPU memory, then nothing is discarded, everything remains resident and no paging is necessary on the 
next frame. Thus, the subsequent frame may be rendered with no texture binds at all. Once the need for 
binding textures between draw commands is eliminated, several com­mon optimizations found in modern realtime 
graphics engines become redundant. For ex­ample, engines often sort or bin geometry in order to reduce 
state changes. As a change in texture is no longer considered a state change, this sorting becomes less 
important. As another example, large, complex models consisting of surfaces with many materials are often 
broken into several smaller parts for rendering. As all of the texture data for these parts can now be 
made resident simultaneously, this can be avoided by simply attaching a per­chunk material ID to what 
would previously have been separate drawing commands. Other graphics features such as instancing become 
more applicable here. 4.2 Incomplete Mip-map Chains A second technique that becomes possible with sparse 
textures is the use of incomplete mip­map chains. These may be used for procedurally generated textures 
or streaming texture data from networks, optical drives or other slow media. Under such circumstances, 
a minimum level of detail is made resident before scene rendering begins. This level can be chosen by 
the developer, but due to artifacts of the PRT implementation, is likely to be a minimum of 64KB per 
texture. This data may, perhaps, be kept closer to the engine in the form of a decompressed base-level 
texture set, or a set of texture data that is downloaded .rst. During rendering, a record is made of 
which textures are actually necessary during scene traversal. This could be done on the CPU based on 
some simple CPU-based rendering, through GPU assisted techniques such as occlusion queries, or entirely 
on the GPU by writing texture access data into images in GPU memory. The application then periodically 
examines the list of live textures and brings them into GPU memory on demand. On the shader side, an 
attempt is made to fetch the textures that are required to render the scene. If the necessary textures 
are not resident in GPU memory, a signal is returned to the shader to indicate so, and the shader begins 
traversing the mip-map pyramid until a resident texel is found. Because the application made all of the 
lowest resolution mip-map levels resident during initialization, it is guaranteed that some reasonable 
texture data is found during this pyramid walk. Over the next few frames, texture data becomes resident 
 either by loading it from the slow resource, or by generating it on the .y using the CPU or even the 
GPU itself. Non-resident textures are displayed as blurry, downsampled versions of their higher resolution 
counterparts at .rst, and over the course of one or more frames, become sharper. Because no physical 
address space is required for non-resident texture data, the largest resolution layer of the mip-map 
pyramid need not even exist if it is known a-priori that it will never be accessed by the texture. The 
same algorithm follows, though; traverse the mip-map pyramid, starting from the desired LoD until a resident 
texel is found. 4.3 Truly Sparse Textures Truly sparse textures are another excellent use case for PRT. 
For example, consider a tra­ditional texture atlas. In general, tools must .nd a balance between tightly 
packing atlas components in order to conserve empty space, and leaving enough space between those com­ponents 
to avoid bleeding during the generation of the mip-map chain. With PRT, this is not as necessary. Large 
regions of unused space may be left empty between components of a texture atlas. Any 64KB chunk of texture 
can be ignored as it will not be allocated in physical storage. Large, irregular shapes may be created 
in the atlas without worrying about .lling the voids in the convex or even hollow outlines. Sparsity 
is even more relevant in 3D and volumetric data-sets. A 3D scan of a large volume can often consume many 
gigabytes of storage, but contain large homogeneous regions and even voids. By using a PRT to store these 
types of texture, larger volumes that would previously have been impossible to ren­der without complex 
shader driven page tables may be simply treated as large contiguous textures. Those regions that are 
completely empty may be left entirely un-allocated. For those regions where lower frequency or even single-valued 
data is acceptable, the very lowest level of the 3D mip-map pyramid may be used. Use in ray-marching 
or slice-based rendering algorithms of these apparently complete data sets is then trivial. 4.4 Current 
Limitations and Thoughts on the Future The PRT feature we are shipping in hardware is certainly very 
powerful, but does not address all the wants or needs of the current SVT community. In particular, the 
maximum texture size has not changed -it is 16K × 16K × 8K texels. The limit lies in the precision of 
the representation of texture coordinates with enough sub-texel resolution for artifact-free linear sampling. 
To some degree, this may be easy to lift, but we are seeing requests from developers to go as high as 
1M × 1M or more in a single texture. This presents signi.cant architectural challenges and may or may 
not be feasible in the near term. It is also easy to see that with large textures and high precision 
texel formats, we start to exhaust even the virtual address space of the GPU. The largest possible texture 
is 16K × 16K × 8K × 16 bytes per texel. This amounts to 32 terabytes of linear address space. This far 
exceeds the addressable space available to the GPU, irrespective or residency. Furthermore, as it is 
backed by the virtual memory subsystem, page table entries need to be allocated for those pages referenced 
by sparse textures. The approximate overhead of the page tables for a virtual allocation on current-generation 
hardware is 0.02% of the virtual allocation size. This does not seem like much and for traditional uses 
of virtual memory, it is not. However, when we consider ideas such as allocation of a single texture 
which consumes a terabyte of virtual address space, this overhead is 20GB much larger than will .t into 
the GPU s physical memory. To address this, we need to consider approaches such as non-resident page 
tables and page table compression. There are several use cases for PRT that seem reasonable but that 
come with subtle complexities that prevent their clean implementation. One such complexity is in the 
use of PRTs as renderable surfaces. Currently, we support rendering to PRTs as color surfaces. Writes 
to un-mapped regions of the surface are simply dropped. However, supporting PRTs as depth or stencil 
bu.ers becomes complex. For example, what is the expected behavior of performing depth or stencil testing 
against a non-resident portion of the depth or stencil bu.er? Also, supporting rendering to MSAA surfaces 
is not well supported. Because of the way compression works for multisampled surfaces, it is possible 
for a single pixel in a color surface to be both resident and non-resident simultaneously, depending 
on how many edges cut that pixel. For this reason, we do not expose depth, stencil or MSAA surfaces as 
renderable on current generation hardware. The operating system is another component in the virtual memory 
subsystem which must be considered. Under our current architecture, a single virtual allocation may be 
backed by multiple physical allocations. Our driver stack is responsible for virtual address space allocations 
whereas the operating system is responsible for the allocation of physical address space. The driver 
informs the operating system how much physical memory is available and the operating system creates allocations 
from these pools. During rendering, the operating system can ask the driver to page physical allocations 
in and out of the GPU memory. The driver does this using DMA and updates the page tables to keep GPU 
virtual addresses pointing at the right place. During rendering, the driver tells the operating system 
which allocations are referenced by the application at any given point in the submission stream and the 
operating system responds by issuing paging requests to make sure they are resident. When there is a 
1-to-1 (or even a many-to-1) correspondence between virtual and physical allocations, this works well. 
However, when a large texture is slowly made resident over time, the list of physical allocations referenced 
by a single large virtual allocation can become very long. This presents some performance challenges 
that real-world use will likely show us in the near term and will need to be addressed.  Appendix A 
Code Samples To query PRT tile dimensions for a give texture format/type: GLint sizeX = 0; GLint sizeY 
= 0; GLint sizeZ = 0; glGetInternalformativ(GL_TEXTURE_2D, GL_RGBA8, GL_VIRTUAL_PAGE_SIZE_X_AMD, 1, &#38;sizeX); 
glGetInternalformativ(GL_TEXTURE_2D, GL_RGBA8, GL_VIRTUAL_PAGE_SIZE_Y_AMD, 1, &#38;sizeY); glGetInternalformativ(GL_TEXTURE_2D, 
GL_RGBA8, GL_VIRTUAL_PAGE_SIZE_Z_AMD, 1, &#38;sizeZ); To create a 2D partially resident texture with 
5 × 10 tiles: GLuint prtTexture = 0; glGenTextures(1, &#38;prtTexture); glBindMultiTexture(GL_TEXTURE0, 
GL_TEXTURE_2D, prtTexture); glTexStorageSparseAMD(GL_TEXTURE_2D, GL_RGBA8, sizeX * 5, sizeY * 10, 1, 
0, GL_TEXTURE_STORAGE_SPARSE_BIT_AMD); To map a 2 × 1 PRT region at o.set (0, 0) in mipmap level 0: 
 glBindMultiTexture(GL_TEXTURE0, GL_TEXTURE_2D, prtTexture); glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, 
sizeX * 2, sizeY * 1, GL_RGBA, GL_UNSIGNED_BYTE, data); To unmap the same PRT region: glBindMultiTexture(GL_TEXTURE0, 
GL_TEXTURE_2D, prtTexture); glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, sizeX * 2, sizeY * 1, GL_RGBA, GL_UNSIGNED_BYTE, 
NULL); To check for tile residency inside a fragment shader: uniform sampler2D sampler; in vec4 colorVert; 
in vec4 texCoordVert; out vec4 fragmentColor; void main() { vec4 outColor = vec4(1.0, 1.0, 1.0, 1.0); 
int code = sparseTexture(sampler, texCoordVert.xy, outColor); if (sparseTexelResident(code)) { // data 
present fragmentColor = vec4(outColor.rgb, 1.0); } else { // NACK fragmentColor = vec4(1.0, 0.0, 0.0, 
1.0); } } 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343489</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>469</pages>
		<display_no>6</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[State of the art in photon density estimation]]></title>
		<page_from>1</page_from>
		<page_to>469</page_to>
		<doi_number>10.1145/2343483.2343489</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343489</url>
		<abstract>
			<par><![CDATA[<p>Photon-density estimation techniques are a popular choice for simulating light transport in scenes with complicated geometry and materials. This class of algorithms can be used to accurately simulate inter-reflections, caustics, color bleeding, scattering in participating media, and subsurface scattering. Since its introduction, photon-density estimation has been significantly extended in computer graphics with the introduction of: specialized techniques that intelligently modify the positions or bandwidths to reduce visual error using a small number of photons, approaches that eliminate error completely in the limit, and methods that use higher-order samples and queries to reduce error in participating media.</p> <p>This two-part course explains how to implement all these latest advances in photon-density estimation. It begins with a short introduction using classical photon mapping, but the remainder of the course provides new, hands-on explanations of the latest developments in this area by experts in each technique. Attendees gain concrete and practical understanding of the latest developments in photon-density-estimation techniques that have not been presented before in SIGGRAPH courses.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738890</person_id>
				<author_profile_id><![CDATA[81320490282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hachisuka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aarhus University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738893</person_id>
				<author_profile_id><![CDATA[81100389194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wojciech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarosz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738894</person_id>
				<author_profile_id><![CDATA[81556261956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Guillaume]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bouchard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#233; Claude Bernard Lyon, CNRS, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738895</person_id>
				<author_profile_id><![CDATA[81504682634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Per]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Christensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738896</person_id>
				<author_profile_id><![CDATA[81335490578]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jeppe]]></first_name>
				<middle_name><![CDATA[Revall]]></middle_name>
				<last_name><![CDATA[Frisvad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Denmark]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738897</person_id>
				<author_profile_id><![CDATA[81448598202]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Wenzel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jakob]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738898</person_id>
				<author_profile_id><![CDATA[81100640205]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Henrik]]></first_name>
				<middle_name><![CDATA[Wann]]></middle_name>
				<last_name><![CDATA[Jensen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC San Diego]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738899</person_id>
				<author_profile_id><![CDATA[81466648514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaschalk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738900</person_id>
				<author_profile_id><![CDATA[81474662121]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Claude]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Knaus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bern]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738891</person_id>
				<author_profile_id><![CDATA[81100351513]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selle]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738892</person_id>
				<author_profile_id><![CDATA[81414603221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Ben]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spencer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Swansea University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[{Arv86} James Arvo. Backward ray tracing. In <i>In ACM SIGGRAPH '86 Course Notes, Developments in Ray Tracing</i>, pages 259--263, 1986. xi]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[{CB04} Per H. Christensen and Dana Batali. An irradiance atlas for global illumination in complex production scenes. In Alexander Keller and Henrik Wann Jensen, editors, <i>Proceedings of the 15th Eurographics Workshop on Rendering Techniques, Norkping, Sweden, June 21--23, 2004</i>, pages 133--142. Eurographics Association, 2004. xv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[{DBB06} Philip Dutr&#233;, Philippe Bekaert, and Kavita Bala. <i>Advanced Global Illumination</i>. AK Peters, Ltd., second edition, 2006. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618487</ref_obj_id>
				<ref_obj_pid>1661412</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[{HJ09} Toshiya Hachisuka and Henrik Jensen. Stochastic progressive photon mapping. In <i>SIGGRAPH Asia '09: ACM SIGGRAPH Asia 2009 papers</i>, pages 1--8, New York, NY, USA, 2009. ACM. xiii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[{HOJ08} Toshiya Hachisuka, Shinji Ogaki, and Henrik Jensen. Progressive photon mapping. <i>ACM Transactions on Graphics (SIGGRAPH Asia Proceedings)</i>, 27(5):Article 130, 2008. xii, xiii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[{HTF01} Trevor Hastie, Robert Tibshirani, and Jerome Friedman. <i>The Elements of Statistical Learning</i>. Springer Series in Statistics. Springer New York Inc., New York, NY, USA, 2001. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[{Ige99} Homan Igehy. Tracing ray differentials. In <i>Proceedings of the 26th annual conference on Computer graphics and interactive techniques</i>, SIGGRAPH '99, pages 179--186, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[{JC98} Henrik Jensen and Per Christensen. Efficient simulation of light transport in scences with participating media using photon maps. In <i>Computer Graphics (Proceedings of SIGGRAPH 98)</i>, pages 311--320, New York, NY, USA, 1998. ACM Press. xiv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[{Jen96} Henrik Jensen. Global illumination using photon maps. In Xavier Pueyo and Peter Schr&#246;der, editors, <i>Rendering Techniques '96</i>, Eurographics, pages 21--30. Springer-Verlag Wien New York, 1996. xi]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1899409</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[{JNSJ11} Wojciech Jarosz, Derek Nowrouzezahrai, Iman Sadeghi, and Henrik Wann Jensen. A comprehensive theory of volumetric radiance estimation using photon points and beams. <i>ACM Transactions on Graphics (Presented at ACM SIGGRAPH 2011)</i>, 30(1):5:1--5:19, January 2011. xiv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024215</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[{JNT&#60;sup&#62;+&#60;/sup&#62;11} Wojciech Jarosz, Derek Nowrouzezahrai, Robert Thomas, Peter-Pike Sloan, and Matthias Zwicker. Progressive photon beams. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH Asia 2011)</i>, 30(6), December 2011. xiv, xv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[{JRJ11} Wenzel Jakob, Christian Regg, and Wojciech Jarosz. Progressive expectation--maximization for hierarchical volumetric photon mapping. <i>Computer Graphics Forum (Proceedings of EGSR 2011)</i>, 30(4), June 2011. xiv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[{JZJ08} Wojciech Jarosz, Matthias Zwicker, and Henrik Wann Jensen. The beam radiance estimate for volumetric photon mapping. <i>Computer Graphics Forum (Proceedings of Eurographics 2008)</i>, 27(2):557--566, April 2008. xiv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1966404</ref_obj_id>
				<ref_obj_pid>1966394</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[{KZ11} Claude Knaus and Matthias Zwicker. Progressive photon mapping: A probabilistic approach. <i>ACM Trans. Graph</i>., 30:25:1--25:13, May 2011. xiii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[{LURM02} M. Lastra, C. Urena, J. Revelles, and R. Montes. A particle-path based method for monte carlo density estimation. In <i>Rendering Techniques 2002 (Proceedings of the Thirteenth Eurographics Workshop on Rendering)</i>, June 2002. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[{Lux12} Luxrender. http://www.luxrender.net,http://www.luxrender.net/wiki/New_in_0-9, 2012. xv]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964924</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[{NJS&#60;sup&#62;+&#60;/sup&#62;11} Derek Nowrouzezahrai, Jared Johnson, Andrew Selle, Dylan Lacewell, Michael Kaschalk, and Wojciech Jarosz. A programmable system for artistic volumetric lighting. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2011)</i>, 30(4):29:1--29:8, August 2011. xv, xvi]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321293</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[{SFES07} Lars Schj&#248;th, Jeppe Revall Frisvad, Kenny Erleben, and Jon Sporring. Photon differentials. In <i>Proceedings of the 5th international conference on Computer graphics and interactive techniques in Australia and Southeast Asia</i>, GRAPHITE '07, pages 179--186, New York, NY, USA, 2007. ACM. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[{SFES10} Lars Schj&#248;th, Jeppe Revall Frisvad, Kenny Erleben, and Jon Sporring. Temporal photon differentials. In <i>GRAPP'10</i>, pages 54--61, 2010. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[{SJ09} Ben Spencer and Mark W. Jones. Into the blue: Better caustics through photon relaxation. <i>Eurographics 2009, Computer Graphics Forum</i>, 28(2):319--328, March 2009. xii]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[{SWH&#60;sup&#62;+&#60;/sup&#62;95} Peter Shirley, Bretton Wade, Philip Hubbard, David Zareski, Bruce Walter, and Donald Greenberg. Global illumination via density estimation. <i>Rendering Techniques 95 (Proceedings of Eurographics Workshop on Rendering 95)</i>, pages 219--230, 1995. xi]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[{Whi80} Turner Whitted. An improved illumination model for shaded display. <i>Commun. ACM</i>, 23(6):343--349, 1980. xi]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[{Fau92} Henri Faure. Good permutations for extreme discrepancy. <i>Journal of Number Theory</i>, (42):47--56, 1992.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[{HP02} Heinrich Hey and Werner Purgathofer. Advanced radiance estimation for photon map global illumination. <i>Computer Graphics Forum: Proceedings of Eurographics '02</i>, 21(3):541--545, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[{JC95} Henrik Wann Jensen and Niels J&#248;rgen Christensen. Photon maps in bidirectional Monte Carlo ray tracing of complex objects. <i>Computers and Graphics</i>, 19(2):215--224, 1995.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[{Jen96} Henrik Wann Jensen. Global illumination using photon maps. In <i>Proceedings of the Eurographics workshop on Rendering techniques '96</i>, pages 21--30, London, UK, 1996. Springer-Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Henrik Wann Jensen. <i>Realistic Image Synthesis using Photon Mapping</i>. A. K. Peters, Ltd., 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[{Sch03} Roland Schregle. Bias compensation for photon maps. <i>Computer Graphics Forum</i>, 22(4):729--742, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321293</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[{SFES07} Lars Schj&#248;th, Jeppe Revall Frisvad, Kenny Erleben, and Jon Sporring. Photon differentials. In <i>GRAPHITE '07</i>, pages 179--186, New York, NY, USA, 2007. ACM Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[{Sil86} B. W. Silverman. <i>Density Estimation for Statistics and Data Analysis</i>. Springer, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[{SJ09} Ben Spencer and Mark W. Jones. Into the blue: Better caustics through photon relaxation. <i>Eurographics 2009, Computer Graphics Forum</i>, 28(2):319--328, March 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[{SOS08} Lars Schj&#248;th, Ole Fogh Olsen, and Jon Sporring. Diffusion based photon mapping. <i>Computer Graphics Forum</i>, 27(8):2114--2127, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[{Uli88} R. A. Ulichney. Dithering with blue noise. <i>Proceeding of the IEEE</i>, 76(1):56--79, 1988.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., Laur, D. M., Fong, J., Wooten, W. L., and Batali, D. 2003. Ray differentials and multiresolution geometry caching for distribution ray tracing in complex scenes. In <i>Proceedings of Eurographics 2003</i>, Blackwell Publishing Inc., Computer Graphics Forum, 543--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97895</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. S. 1990. Adaptive radiosity textures for bidirectional ray tracing. In <i>Computer Graphics</i>, ACM Siggraph Conference proceedings, 145--154.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Igehy, H. 1999. Tracing ray differential. In <i>Siggraph 1999, Computer Graphics Proceedings</i>, Addison Wesley Longman, Los Angeles, A. Rockwood, Ed., 179--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Christensen, N. J. 1995. Photon maps in bidirectional monte carlo ray tracing of complex objects. <i>Computers &amp; Graphics 19</i>, 2 (Mar), 215--224.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 1996. <i>The Photon Map in Global Illumination</i>. PhD thesis, Technical University of Denmark, Lyngby.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 2001. <i>Realistic image synthesis using photon mapping</i>. A. K. Peters, Ltd., Natick, MA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731974</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Myszkowski, K. 1997. Lighting reconstruction using fast and adaptive density estimation techniques. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques '97</i>, Springer-Verlag, London, UK, 251--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Nicodemus, F. E., Richmond, J. C., Hsia, J. J., Ginsberg, I. W., and Limperis, T. 1977. Geometrical considerations and nomenclature for reflectance. Tech. rep., National Bureau of Standards (US), Oct.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>225296</ref_obj_id>
				<ref_obj_pid>225294</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Redner, R. A., Lee, M. E., and Uselton, S. P. 1995. Smooth b-spline illumination maps for bidirectional ray tracing. <i>ACM Trans. Graph. 14</i>, 4, 337--362.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Schj&#248;th, L., Olsen, O. F., and Sporring, J. 2006. Diffusion based photon mapping. In <i>International conference on Computer Graphics -- Theory and Applications</i>, INSTICC Press, Set&#250;bal, Portugal, 168--175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Schregle, R. 2003. Bias compensation for photon maps. <i>Computer Graphics Forum 22</i>, 4, 729--742.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Shirley, P., Wade, B., Hubbard, P. M., Zareski, D., Walter, B., and Greenberg, D. P. 1995. Global Illumination via Density Estimation. In <i>Proceedings of the Sixth Eurographics Workshop on Rendering</i>, Springer-Verlag, New York, NY, 219--230.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Silverman, B. 1986. <i>Density Estimation for Statistics and Data Analysis</i>. Monographs on Statistics and Applied Probability. Chapman and Hall, London-New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Simonoff, J. S. 1996. <i>Smoothing Methods in Statistics</i>. Springer Series in Statistics. Springer-Verlag, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732120</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Suykens, F., and Willems, Y. D. 2000. Density control for photon maps. In <i>Proceedings of the 11th Eurographics Workshop on Rendering</i>, Springer-Verlag, London, UK, 23--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732286</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Suykens, F., and Willems, Y. D. 2001. Path differentials and applications. In <i>Proceedings of the 12th Eurographics Workshop on Rendering</i>, Springer-Verlag, London, UK, 257--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>256158</ref_obj_id>
				<ref_obj_pid>256157</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Hubbard, P. M., Shirley, P., and Greenberg, D. P. 1997. Global illumination using local linear density estimation. <i>ACM Trans. Graph. 16</i>, 3, 217--259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927407</ref_obj_id>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Walter, B. 1998. <i>Density estimation techniques for global illumination</i>. PhD thesis, Cornell University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., and Batali, D. 2004. An irradiance atlas for global illumination in complex production scenes. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, 133--141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073330</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Cline, D., Talbot, J., and Egbert, P. 2005. Energy redistribution path tracing. <i>ACM Trans. Graph. (SIGGRAPH Proceedings) 24</i>, 3, 1186--1195.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Dutr&#233;, P., Lafortune, E., and Willems, Y. 1993. Monte carlo light tracing with direct computation of pixel intensities. In <i>Proceedings of Compugraphics '93</i>, 128--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1196364</ref_obj_id>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Dutr&#233;, P., Bekaert, P., and Bala, K. 2006. <i>Advanced Global Illumination (2nd edition)</i>. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383662</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Fradin, D., Meneveaux, D., and Horna, S. 2005. Out of core photon-mapping for large buildings. In <i>Proceedings of Eurographics Symposium on Rendering 2005</i>, Eurographics.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Havran, V., Herzog, R., and Seidel, H.-P. 2005. Fast final gathering via reverse photon mapping. <i>Computer Graphics Forum (Proceedings of Eurographics 2005) 24</i>, 3 (September).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Herzog, R., Havran, V., Kinuwaki, S., Myszkowski, K., and Seidel, H.-P. 2007. Global illumination using photon ray splatting. In <i>Eurographics 2007</i>, Blackwell, vol. 26 of <i>Computer Graphics Forum</i>, 503--513.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1103920</ref_obj_id>
				<ref_obj_pid>1103900</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Suykens, F., Christensen, P., and Kato, T. 2004. A practical guide to global illumination using ray tracing and photon mapping. In <i>SIGGRAPH '04: ACM SIGGRAPH 2004 Course Notes</i>, ACM, New York, NY, USA, 20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 1996. Global illumination using photon maps. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques '96</i>, Springer-Verlag, London, UK, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 2001. <i>Realistic Image Synthesis Using Photon Mapping</i>. A. K. Peters, Ltd., Natick, MA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T. 1986. The rendering equation. <i>Computer Graphics (SIGGRAPH Proceedings) 20</i>, 4, 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Kelemen, C., Szirmay-Kalos, L., Antal, G., and Csonka, F. 2002. A simple and robust mutation strategy for the metropolis light transport algorithm. <i>Computer Graphics Forum (Eurographics) 21</i>, 3, 531--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P., and Willems, Y. D. 1993. Bi-directional path tracing. In <i>Proceedings of Third International Conference on Computational Graphics and Visualization Techniques (Compugraphics '93)</i>, H. P. Santo, Ed., 145--153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383884</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Lai, Y.-C., Fan, S. H., Chenney, S., and Dyer, C. 2007. Photorealistic image rendering with population monte carlo energy redistribution. In <i>Proceedings of the Rendering Techniques (EGSR)</i>, Eurographics Association, Grenoble, France, 287--295.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Schregle, R. 2003. Bias compensation for photon maps. In <i>Computer Graphics Forum 22, 4 (2003), C792--C742</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Suykens, F., and Willems, Y. D. 2000. Adaptive filtering for progressive monte carlo image rendering. In <i>Eighth International Conference in Central Europe on Computer Graphics, Visualization and Interactive Digital Media (WSCG 2000)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218498</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. 1995. Optimally combining sampling techniques for monte carlo rendering. In <i>Computer Graphics (SIGGRAPH Proceedings)</i>, 419--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. 1997. Metropolis light transport. In <i>Computer Graphics (SIGGRAPH Proceedings)</i>, 65--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Veach, E. 1998. <i>Robust monte carlo methods for light transport simulation</i>. PhD thesis, Stanford, CA, USA. Adviser-Leonidas J. Guibas.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581915</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Cammarano, M., and Jensen, H. W. 2002. Time dependent photon mapping. In <i>Rendering Techniques</i>, Eurographics Association, S. Gibson and P. E. Debevec, Eds., vol. 28 of <i>ACM International Conference Proceeding Series</i>, 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073330</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Cline, D., Talbot, J., and Egbert, P. 2005. Energy redistribution path tracing. <i>ACM Trans. Graph. (SIGGRAPH Proceedings) 24</i>, 3, 1186--1195.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808590</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., Porter, T., and Carpenter, L. 1984. Distributed ray tracing. In <i>Computer Graphics (SIGGRAPH Proceedings)</i>, vol. 3(18), 137--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1196364</ref_obj_id>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Dutr&#233;, P., Bekaert, P., and Bala, K. 2006. <i>Advanced Global Illumination (2nd edition)</i>. A K Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409083</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., Ogaki, S., and Jensen, H. W. 2008. Progressive photon mapping. <i>ACM Transactions on Graphics (SIGGRAPH Asia Proceedings) 27</i>, 5, Article 130.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Jarosz, W., Zwicker, M., and Jensen, H. W. 2008. The beam radiance estimate for volumetric photon mapping. <i>Comput. Graph. Forum 27</i>, 2, 557--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 1996. Global illumination using photon maps. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques '96</i>, Springer-Verlag, London, UK, 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T. 1986. The rendering equation. <i>Computer Graphics (SIGGRAPH Proceedings) 20</i>, 4, 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P., and Willems, Y. D. 1993. Bi-directional path tracing. In <i>Proceedings of Third International Conference on Computational Graphics and Visualization Techniques (Compugraphics '93)</i>, H. P. Santo, Ed., 145--153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Silverman, B. 1986. <i>Density Estimation for Statistics and Data Analysis</i>. Mongraphs on Statistics and Applied Probability. Chapman and Hall, New York, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218498</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. 1995. Optimally combining sampling techniques for monte carlo rendering. In <i>Computer Graphics (SIGGRAPH Proceedings)</i>, 419--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. 1997. Metropolis light transport. In <i>Computer Graphics (SIGGRAPH Proceedings)</i>, 65--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1202956</ref_obj_id>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Wasserman, L. 2006. <i>All of Nonparametric Statistics (Springer Texts in Statistics)</i>. Springer-Verlag New York, Inc., Secaucus, NJ, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[{BMP77} Breiman L., Meisel W., Purcell E.: Variable kernel estimates of multi-variate densities. <i>Technometrics 19</i>, 2 (May 1977), 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1101430</ref_obj_id>
				<ref_obj_pid>1101389</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[{BPPP05} Boudet A., Pitot P., Pratmarty D., Paulin M.: Photon splatting for participating media. In <i>GRAPHITE '05: Proceedings of the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia</i> (New York, NY, USA, 2005), ACM Press, pp. 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[{Cha60} Chandrasekar S.: <i>Radiative Transfer</i>. Dover Publications, New York, 1960.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581915</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[{CJ02} Cammarano M., Jensen H. W.: Time dependent photon mapping. In <i>EGRW '02: Proceedings of the 13th Eurographics workshop on Rendering</i> (Aire-la-Ville, Switzerland, Switzerland, 2002), Eurographics Association, pp. 135--144.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[{CPP*05} Cerezo E., P&#233;rez F., Pueyo X., Seron F. J., cois X. Sillion F.: A survey on participating media rendering techniques. <i>The Visual Computer 21</i>, 5 (June 2005), 303--328.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H. W., Christensen P. H.: Efficient simulation of light transport in scences with participating media using photon maps. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 1998), ACM Press, pp. 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[{JMLH01} Jensen H. W., Marschner S. R., Levoy M., Hanrahan P.: A practical model for subsurface light transport. In <i>SIGGRAPH '01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 2001), ACM Press, pp. 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[{JZJ08} Jarosz W., Zwicker M., Jensen H. W.: <i>The Beam Radiance Estimate for Volumetric Photon Mapping</i>. Tech. rep., University of California, San Diego, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[{Kaj86} Kajiya J. T.: The rendering equation. In <i>SIGGRAPH '86: Proceedings of the 13th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 1986), ACM Press, pp. 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[{KH84} Kajiya J. T., Herzen B. P. V.: Ray tracing volume densities. In <i>SIGGRAPH '84: Proceedings of the 11th annual conference on Computer graphics and interactive techniques</i> (New York, NY, USA, 1984), ACM Press, pp. 165--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[{LW96} Lafortune E. P., Willems Y. D.: Rendering Participating Media with Bidirectional Path Tracing. In <i>Proceedings of the eurographics workshop on Rendering techniques '96</i> (London, UK, 1996), Springer-Verlag, pp. 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383583</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[{PARN04} Premoze S., Ashikhmin M., Ramamoorthi R., Nayar S. K.: Practical rendering of multiple scattering effects in participating media. In <i>Rendering Techniques</i> (2004), pp. 363--373.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732117</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[{PKK00} Pauly M., Kollig T., Keller A.: Metropolis light transport for participating media. In <i>Proceedings of the Eurographics Workshop on Rendering Techniques 2000</i> (London, UK, 2000), Springer-Verlag, pp. 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[{PM93} Pattanaik S. N., Mudur S. P.: Computation of global illumination in a participating medium by Monte Carlo simulation. <i>The Journal of Visualization and Computer Animation 4</i>, 3 (July--Sept. 1993), 133--152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37436</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[{RT87} Rushmeier H. E., Torrance K. E.: The zonal method for calculating light intensities in the presence of a participating medium. <i>Computer Graphics (SIGGRAPH '87 Proceedings) 21</i>, 4 (July 1987), 293--302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[{Sil86} Silverman B.: <i>Density Estimation for Statistics and Data Analysis</i>. Chapman and Hall, New York, NY, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[{SRNN05} Sun B., Ramamoorthi R., Narasimhan S. G., Nayar S. K.: A practical analytic single scattering model for real-time rendering. <i>ACM Trans. Graph. 24</i>, 3 (2005), 1040--1049.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[{Sta95} Stam J.: Multiple Scattering as a Diffusion Process. In <i>Rendering Techniques '95 (Proceedings of the Sixth Eurographics Workshop on Rendering)</i> (New York, NY, 1995), Hanrahan P. M., Purgathofer W., (Eds.), Springer-Verlag, pp. 41--50.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[{Vea98} Veach E.: <i>Robust Monte Carlo Methods for Light Transport Simulation</i>. PhD thesis, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[{WMG*07} Wald I., Mark W. R., G&#252;nther J., Boulos S., Ize T., Hunt W., Parker S. G., Shirley P.: State of the art in ray tracing animated scenes. In <i>STAR Proceedings of Eurographics 2007</i> (Prague, Czech Republic, September 2007), Eurographics Association, pp. 0--0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383551</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[{CB04} Christensen P. H., Batali D.: An irradiance atlas for global illumination in complex production scenes. In <i>Rendering Techniques</i> (June 2004), pp. 133--142. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[{Cha60} Chandrasekhar S.: <i>Radiative Transfer</i>. Dover Publications, New York, 1960. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[{CPP*05} Cerezo E., P&#233;rez F., Pueyo X., Seron F. J., Sillion F. X.: A survey on participating media rendering techniques. <i>The Visual Computer 21</i>, 5 (2005). 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1196364</ref_obj_id>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[{DBB06} Dutr&#233; P., Bala K., Bekaert P.: <i>Advanced global illumination</i>. AK Peters Ltd, 2006. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[{DLR*77} Dempster A., Laird N., Rubin D., et al.: Maximum likelihood from incomplete data via the em algorithm. <i>Journal of the Royal Statistical Society. 39</i>, 1 (1977), 1--38. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1026038</ref_obj_id>
				<ref_obj_pid>1025128</ref_obj_pid>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[{DYN04} Dobashi Y., Yamamoto T., Nishita T.: Radiosity for point-sampled geometry. In <i>12th Pacific Conference on Computer Graphics and Applications</i> (Oct. 2004), pp. 152--159. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[{GNN10} Garcia V., Nielsen F., Nock R.: Hierarchical Gaussian mixture model. 2, 6, 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[{GR05} Goldberger J., Roweis S.: Hierarchical clustering of a mixture model. <i>Advances in Neural Information Processing Systems 17</i>, 505--512 (2005), 2--4. 2, 7]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1081501</ref_obj_id>
				<ref_obj_pid>1081432</ref_obj_pid>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[{HE03} Hopf M., Ertl T.: Hierarchical splatting of scattered data. In <i>Proceedings of VIS</i> (2003). 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1602032</ref_obj_id>
				<ref_obj_pid>1602022</ref_obj_pid>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[{HG09} Hasan B. A. S., Gan J.: Sequential EM for unsupervised adaptive Gaussian mixture model based classifier. In <i>Machine Learning and Data Mining in Pattern Recognition</i>, vol. 5632. 2009, pp. 96--106. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866170</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[{HJJ10} Hachisuka T., Jarosz W., Jensen H. W.: A progressive error estimation framework for photon density estimation. <i>ACM Transactions on Graphics. 29</i> (December 2010). 10]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1018059</ref_obj_id>
				<ref_obj_pid>1018014</ref_obj_pid>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[{HLE04} Hopf M., Luttenberger M., Ertl T.: Hierarchical splatting of scattered 4d data. <i>IEEE Computer Graphics and Applications 24</i> (July 2004), 64--72. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409083</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[{HOJ08} Hachisuka T., Ogaki S., Jensen H. W.: Progressive photon mapping. <i>ACM Transactions on Graphics 27</i>, 5 (Dec. 2008), 130:1--130:8. 2, 10]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122740</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[{HSA91} Hanrahan P., Salzman D., Aupperle L.: A rapid hierarchical radiosity algorithm. In <i>Computer Graphics (Proceedings of SIGGRAPH 91)</i> (July 1991), pp. 197--206. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276412</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[{HSRG07} Han C., Sun B., Ramamoorthi R., Grinspun E.: Frequency domain normal map filtering. <i>ACM Transactions on Graphics. 26</i>, 3 (2007). 2, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H. W., Christensen P. H.: Efficient simulation of light transport in scenes with participating media using photon maps. In <i>Proceedings of SIGGRAPH</i>. (July 1998). 1, 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1899409</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[{JNSJ11} Jarosz W., Nowrouzezahrai D., Sadeghi I., Jensen H. W.: A comprehensive theory of volumetric radiance estimation using photon points and beams. <i>ACM Transactions on Graphics 30</i>, 1 (Jan. 2011), 5:1--5:19. 3, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401137</ref_obj_id>
				<ref_obj_pid>1401132</ref_obj_pid>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[{JZJ08} Jarosz W., Zwicker M., Jensen H. W.: The beam radiance estimate for volumetric photon mapping. <i>Computer Graphics Forum 27</i>, 2 (Apr. 2008). 1, 3]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122748</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[{LH91} Laur D., Hanrahan P.: Hierarchical splatting: A progressive refinement algorithm for volume rendering. In <i>Proceedings of SIGGRAPH</i>. (July 1991), pp. 285--288. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[{LW93} Lafortune E. P., Willems Y. D.: Bi-directional path tracing. In <i>Compugraphics</i> (1993). 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[{LW96} Lafortune E. P., Willems Y. D.: Rendering participating media with bidirectional path tracing. In <i>Eurographics Rendering Workshop</i> (June 1996), pp. 91--100. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360636</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[{LZT*08} Lehtinen J., Zwicker M., Turquin E., Kontkanen J., Durand F., Sillion F. X., Aila T.: A meshless hierarchical representation for light transport. <i>ACM Transactions on Graphics 27</i>, 3 (Aug. 2008), 37:1--37:9. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>308679</ref_obj_id>
				<ref_obj_pid>308574</ref_obj_pid>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[{NH98} Neal R., Hinton G.: A view of the EM algorithm that justifies incremental, sparse, and other variants. <i>Learning in graphical models 89</i> (1998), 355--368. 2, 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[{PJJ*11} Papas M., Jarosz W., Jakob W., Rusinkiewicz S., Matusik W., Weyrich T.: Goal-based caustics. <i>Computer Graphics Forum (Proceedings of Eurographics '11) 30</i>, 2 (June 2011). 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732117</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[{PKK00} Pauly M., Kollig T., Keller A.: Metropolis light transport for participating media. In <i>Eurographics Workshop on Rendering</i> (June 2000), pp. 11--22. 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344940</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[{RL00} Rusinkiewicz S., Levoy M.: Qsplat: A multiresolution point rendering system for large meshes. In <i>Proceedings of ACM SIGGRAPH</i>. (July 2000), pp. 343--352. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321293</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[{SFES07} Schj&#248;th L., Frisvad J. R., Erleben K., Sporring J.: Photon differentials. In <i>GRAPHITE</i>. (2007), pp. 179--186. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[{Sil86} Silverman B. W.: <i>Density Estimation for Statistics and Data Analysis</i>. Monographs on Statistics and Applied Probability. Chapman and Hall, New York, NY, 1986. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614311</ref_obj_id>
				<ref_obj_pid>614259</ref_obj_pid>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[{Sil95} Sillion F. X.: A unified hierarchical algorithm for global illumination with scattering volumes and object clusters. <i>IEEE Transactions on Visualization and Computer Graphics 1</i>, 3 (1995). 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477371</ref_obj_id>
				<ref_obj_pid>1477065</ref_obj_pid>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[{SJ09a} Spencer B., Jones M. W.: Hierarchical photon mapping. <i>IEEE Transactions on Visualization and Computer Graphics 15</i>, 1 (Jan./Feb. 2009), 49--61. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[{SJ09b} Spencer B., Jones M. W.: Into the blue: Better caustics through photon relaxation. <i>Computer Graphics Forum 28</i>, 2 (Apr. 2009), 319--328. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[{SSO08} Schj&#248;th L., Sporring J., Olsen O. F.: Diffusion based photon mapping. <i>Computer Graphics Forum 27</i>, 8 (Dec. 2008), 2114--2127. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383670</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[{TLQ*05} Tan P., Lin S., Quan L., Guo B., Shum H.-Y.: Multiresolution reflectance filtering. In <i>Proceedings Eurographics Symposium on Rendering 2005</i> (2005), pp. 111--116. 2, 9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[{VG94} Veach E., Guibas L.: Bidirectional estimators for light transport. In <i>Eurographics Workshop on Rendering</i> (1994). 1]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1164979</ref_obj_id>
				<ref_obj_pid>1164973</ref_obj_pid>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[{VNV06} Verbeek J. J., Nunnink J. R., Vlassis N.: Accelerated EM-based clustering of large data sets. <i>Data Mining and Knowledge Discovery 13</i>, 3 (November 2006), 291--307. 2, 4]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141997</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[{WABG06} Walter B., Arbree A., Bala K., Greenberg D. P.: Multidimensional lightcuts. <i>ACM Transactions on Graphics. 25</i>, 3 (2006). 1, 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[{WBKP08} Walter B., Bala K., Kulkarni M., Pingali K.: Fast agglomerative clustering for rendering. In <i>IEEE Symposium on Interactive Ray Tracing</i> (August 2008), pp. 81--86. 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073318</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[{WFA*05} Walter B., Fernandez S., Arbree A., Bala K., Donikian M., Greenberg D. P.: Lightcuts: a scalable approach to illumination. <i>ACM Transactions on Graphics. 24</i>, 3 (Aug. 2005). 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338560</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[{ZHG*07} Zhou K., Hou Q., Gong M., Snyder J., Guo B., Shum H.-Y.: Fogshop: Real-time design and rendering of inhomogeneous, single-scattering media. In <i>Pacific Graphics</i>. (2007), pp. 116--125. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614526</ref_obj_id>
				<ref_obj_pid>614287</ref_obj_pid>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[{ZPvBG02} Zwicker M., Pfister H., van Baar J., Gross M.: EWA splatting. <i>IEEE Transactions on Visualization and Computer Graphics 8</i>, 3 (July/Sept. 2002), 223--238. 2]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360635</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[{ZRL*08} Zhou K., Ren Z., Lin S., Bao H., Guo B., Shum H.-Y.: Real-time smoke rendering using compensated ray marching. <i>ACM Transactions on Graphics. 27</i>, 3 (2008). 2, 6]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>555371</ref_obj_id>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[Apodaca, A. A. and Gritz, L. 1999. <i>Advanced RenderMan: Creating CGI for Motion Pictures</i>. Morgan Kaufmann, San Francisco, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97886</ref_obj_id>
				<ref_obj_pid>97879</ref_obj_pid>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[Arvo, J. and Kirk, D. 1990. Particle transport and image synthesis. In <i>SIGGRAPH '90: Proceedings of the 17th annual conference on Computer graphics and interactive techniques</i>. ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1101430</ref_obj_id>
				<ref_obj_pid>1101389</ref_obj_pid>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[Boudet, A., Pitot, P., Pratmarty, D., and Paulin, M. 2005. Photon splatting for participating media. In <i>Proceedings of the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia</i>. GRAPHITE '05. ACM, New York, NY, USA, 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[Cerezo, E., P&#233;rez, F., Pueyo, X., Seron, F. J., and Sillion, F. X. 2005. A survey on participating media rendering techniques. <i>The Visual Computer 21</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[Chandrasekar, S. 1960. <i>Radiative Transfer</i>. Dover Publications, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[Coleman, W. 1968. Mathematical verification of a certain Monte Carlo sampling technique and applications of the technique to radiation transport problems. <i>Nuclear Science and Engineering</i> 32, 76--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1089523</ref_obj_id>
				<ref_obj_pid>1089508</ref_obj_pid>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[Ernst, M., Akenine-M&#246;ller, T., and Jensen, H. W. 2005. Interactive rendering of caustics using interpolated warped volumes. In <i>Proceedings of Graphics Interface</i>. Waterloo, Ontario, Canada, 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383601</ref_obj_id>
				<ref_obj_pid>2383586</ref_obj_pid>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[Fabianowski, B. and Dingliana, J. 2009. Interactive global photon mapping. <i>Computer Graphics Forum (Proceedings of EGSR 2009) 28</i>, 4, 1151--1159.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383660</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[Havran, V., Bittner, J., Herzog, R., and Seidel, H.-P. 2005. Ray Maps for Global Illumination. In <i>16th EG Workshop on Rendering</i>. 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618489</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[Ha&#353;an, M., K&#345;iv&#225;nek, J., Walter, B., and Bala, K. 2009. Virtual spherical lights for many-light rendering of glossy scenes. <i>Transactions on Graphics (Proceedings of SIGGRAPH Asia 2009) 28</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808588</ref_obj_id>
				<ref_obj_pid>964965</ref_obj_pid>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[Heckbert, Paul, S. and Hanrahan, P. 1984. Beam tracing polygonal objects. In <i>SIGGRAPH</i>. ACM, New York, USA, 119--127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[Herzog, R., Havran, V., Kinuwaki, S., Myszkowski, K., and Seidel, H.-P. 2007. Global illumination using photon ray splatting. In <i>Computer Graphics Forum (Proceedings of Eurographics 2007)</i>. Blackwell, Czech Republic, 503--513.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[Igehy, H. 1999. Tracing ray differentials. In <i>SIGGRAPH</i>. ACM Press/Addison-Wesley Publishing Co., New York, USA, 179--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>883440</ref_obj_id>
				<ref_obj_pid>882473</ref_obj_pid>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[Iwasaki, K., Nishita, T., and Dobashi, Y. 2001. Efficient rendering of optical effects within water using graphics hardware. In <i>Pacific Graphics</i>. IEEE Computer Society, Washington, DC, USA, 374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[Jarosz, W., Zwicker, M., and Jensen, H. W. 2008. The Beam Radiance Estimate for Volumetric Photon Mapping. <i>Computer Graphics Forum (Proceedings of Eurographics 2008) 27</i>, 2 (Apr.), 557--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 2001. <i>Realistic Image Synthesis Using Photon Mapping</i>. A. K. Peters, Ltd., Natick, MA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. and Christensen, P. H. 1998. Efficient Simulation of Light Transport in Scences with Participating Media Using Photon Maps. In <i>SIGGRAPH</i>. ACM Press, New York, USA, 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T. 1986. The rendering equation. In <i>SIGGRAPH</i>. ACM Press, New York, USA, 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258769</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[Keller, A. 1997. Instant radiosity. In <i>SIGGRAPH</i>. Computer Graphics Proceedings, Annual Conference Series. 49--56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383935</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[Kr&#252;ger, J., B&#252;rger, K., and Westermann, R. 2006. Interactive screen-space accurate photon tracing on GPUs. In <i>Rendering Techniques</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P. and Willems, Y. D. 1993. Bi-directional path tracing. In <i>Compugraphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P. and Willems, Y. D. 1996. Rendering participating media with bidirectional path tracing. In <i>EG Rendering Workshop</i>. 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[Lastra, M., Ure&#241;a, C., Revelles, J., and Montes, R. 2002. A Particle-Path based Method for Monte Carlo Density Estimation. In <i>13th EG Workshop on Rendering</i>. EG Association, 7--14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T. and Veach, E. 2000. Deep shadow maps. In <i>SIGGRAPH</i>. ACM Press, New York, NY, USA, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>87856</ref_obj_id>
				<ref_obj_pid>87851</ref_obj_pid>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[MacDonald, D. J. and Booth, K. S. 1990. Heuristics for ray tracingusing space subdivision. <i>The Visual Computer 6</i>, 3, 153--166.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>134082</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. and Hanrahan, P. 1992. Illumination from curved reflectors. In <i>SIGGRAPH</i>. ACM, New York, USA, 283--291.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141995</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[Moon, J. T. and Marschner, S. R. 2006. Simulating multiple scattering in hair using a photon mapping approach. In <i>Transactions on Graphics (Proceedings of SIGGRAPH 2006)</i>. ACM, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[Nakamaru, K. and Ohno, Y. 2002. Ray tracing for curves primitive. In <i>WSCG</i>. 311--316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Nishita, T., Miyawaki, Y., and Nakamae, E. 1987. A shading model for atmospheric scattering considering luminous intensity distribution of light sources. In <i>SIGGRAPH</i>. ACM, New York, USA, 303--310.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192261</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[Nishita, T. and Nakamae, E. 1994. Method of displaying optical effects within water using accumulation buffer. In <i>SIGGRAPH</i>. ACM, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732117</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[Pauly, M., Kollig, T., and Keller, A. 2000. Metropolis light transport for participating media. In <i>11th EG Workshop on Rendering</i>. 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[Pegoraro, V. and Parker, S. G. 2009. An Analytical Solution to Single Scattering in Homogeneous Participating Media. <i>Computer Graphics Forum (Proceedings of Eurographics 2009) 28</i>, 2, 329--335.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975275</ref_obj_id>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[Pharr, M. and Humphreys, G. 2004. <i>Physically Based Rendering: From Theory to Implementation</i>. Morgan Kaufmann, San Francisco, USA, Chapter 14, 641--644.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[Raab, M., Seibert, D., and Keller, A. 2008. Unbiased global illumination with participating media. In <i>Monte Carlo and Quasi-Monte Carlo Methods 2006</i>. Springer, 591--606.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[Schj&#248;th, L. 2009. Anisotropic density estimation in global illumination: a journey through time and space. Ph.D. thesis, University of Copenhagen.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321293</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA[Schj&#248;th, L., Frisvad, J. R., Erleben, K., and Sporring, J. 2007. Photon differentials. In <i>GRAPHITE</i>. ACM, New York, USA, 179--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>181</ref_seq_no>
				<ref_text><![CDATA[Schj&#248;th, L., Olsen, O. F., and Sporring, J. 2006. Diffusion based photon mapping. In <i>GRAPP 2006: Proceedings of the First International Conference on Computer Graphics Theory and Applications, Set&#250;bal, Portugal, February 25--28, 2006</i>. 168--175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>182</ref_seq_no>
				<ref_text><![CDATA[Schj&#248;th, L., Sporring, J., and Olsen, O. F. 2008. Diffusion based photon mapping. <i>Computer Graphics Forum (Proceedings of Eurographics 2008) 27</i>, 8 (Dec.), 2114--2127.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>183</ref_seq_no>
				<ref_text><![CDATA[Silverman, B. 1986. <i>Density Estimation for Statistics and Data Analysis</i>. Monographs on Statistics and Applied Probability. Chapman and Hall, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>184</ref_seq_no>
				<ref_text><![CDATA[Sun, B., Ramamoorthi, R., Narasimhan, S. G., and Nayar, S. K. 2005. A practical analytic single scattering model for real time rendering. <i>Transactions on Graphics (Proceedings of SIGGRAPH 2005) 24</i>, 3, 1040--1049.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>185</ref_seq_no>
				<ref_text><![CDATA[Suykens, F. and Willems, Y. D. 2000. Adaptive filtering for progressive monte carlo image rendering. In <i>Vis. and Interactive Digital Media</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732286</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>186</ref_seq_no>
				<ref_text><![CDATA[Suykens, F. and Willems, Y. D. 2001. Path differentials and applications. In <i>Rendering Techniques</i>. Springer-Verlag, London, UK, 257--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>187</ref_seq_no>
				<ref_text><![CDATA[Veach, E. 1997. Robust Monte Carlo methods for light transport simulation. Ph.D. thesis, Stanford, CA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>188</ref_seq_no>
				<ref_text><![CDATA[Veach, E. and Guibas, L. 1994. Bidirectional estimators for light transport. In <i>Fifth Eurographics Workshop on Rendering</i>. 147--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141997</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>189</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Arbree, A., Bala, K., and Greenberg, D. P. 2006. Multidimensional lightcuts. <i>Transactions on Graphics (Proceedings of SIGGRAPH 2006) 25</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531398</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>190</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Zhao, S., Holzschuch, N., and Bala, K. 2009. Single scattering in refractive media with triangle mesh boundaries. <i>Transactions on Graphics (Proceedings of SIGGRAPH 2009) 28</i>, 3 (Aug.).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320551</ref_obj_id>
				<ref_obj_pid>2319031</ref_obj_pid>
				<ref_seq_no>191</ref_seq_no>
				<ref_text><![CDATA[Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. 2004. Image quality assessment: From error visibility to structural similarity. <i>IEEE transactions on image processing 13</i>, 4, 600--612.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97920</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>192</ref_seq_no>
				<ref_text><![CDATA[Watt, M. 1990. Light-water interaction using backward beam tracing. In <i>SIGGRAPH</i>. ACM, New York, USA, 377--385.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>193</ref_seq_no>
				<ref_text><![CDATA[Zinke, A. and Weber, A. 2006. Efficient ray based global illumination using photon maps. In <i>Vision, Modeling, and Visualization 2006 (VMV 2006)</i>. Akademische Verlagsgesellschaft Aka GmbH, Berlin, 113--120.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>194</ref_seq_no>
				<ref_text><![CDATA[Akenine-M&#246;ller, T., Haines, E., and Hoffman, N. 2008. <i>Real-Time Rendering 3rd Edition</i>. A. K. Peters, Ltd., Natick, MA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>195</ref_seq_no>
				<ref_text><![CDATA[Chandrasekar, S. 1960. <i>Radiative Transfer</i>. Dover Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944752</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>196</ref_seq_no>
				<ref_text><![CDATA[Chen, J., Baran, I., Durand, F., and Jarosz, W. 2011. Real-time volumetric shadows using 1D min-max mipmaps. In <i>Symposium on Interactive 3D Graphics and Games</i>, 39--46.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730823</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>197</ref_seq_no>
				<ref_text><![CDATA[Engelhardt, T., and Dachsbacher, C. 2010. Epipolar sampling for shadows and crepuscular rays in participating media with single scattering. In <i>Symposium on Interactive 3D Graphics and Games</i>, 119--125.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>198</ref_seq_no>
				<ref_text><![CDATA[Engelhardt, T., Nov&#225;k, J., and Dachsbacher, C. 2010. Instant multiple scattering for interactive rendering of heterogeneous participating media. Tech. rep., Karlsruhe Institut of Technology, Dec.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618487</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>199</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., and Jensen, H. W. 2009. Stochastic progressive photon mapping. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2009) 28</i>, 5 (Dec.), 141:1--141:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409083</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>200</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., Ogaki, S., and Jensen, H. W. 2008. Progressive photon mapping. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2008) 27</i>, 5 (Dec.), 130:1--130:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866170</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>201</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, T., Jarosz, W., and Jensen, H. W. 2010. A progressive error estimation framework for photon density estimation. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2010) 29</i>, 6 (Dec.), 144:1--144:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383660</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>202</ref_seq_no>
				<ref_text><![CDATA[Havran, V., Bittner, J., Herzog, R., and Seidel, H.-P. 2005. Ray maps for global illumination. In <i>Rendering Techniques 2005: (Proceedings of the Eurographics Symposium on Rendering)</i>, 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>203</ref_seq_no>
				<ref_text><![CDATA[Herzog, R., Havran, V., Kinuwaki, S., Myszkowski, K., and Seidel, H.-P. 2007. Global illumination using photon ray splatting. <i>Computer Graphics Forum (Proceedings of Eurographics 2007) 26</i>, 3 (Sept.), 503--513.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730822</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>204</ref_seq_no>
				<ref_text><![CDATA[Hu, W., Dong, Z., Ihrke, I., Grosch, T., Yuan, G., and Seidel, H.-P. 2010. Interactive volume caustics in single-scattering media. In <i>Symposium on Interactive 3D Graphics and Games</i>, 109--117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>205</ref_seq_no>
				<ref_text><![CDATA[Jarosz, W., Zwicker, M., and Jensen, H. W. 2008. The beam radiance estimate for volumetric photon mapping. <i>Computer Graphics Forum (Proceedings of Eurographics 2008) 27</i>, 2 (Apr.), 557--566.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1899409</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>206</ref_seq_no>
				<ref_text><![CDATA[Jarosz, W., Nowrouzezahrai, D., Sadeghi, I., and Jensen, H. W. 2011. A comprehensive theory of volumetric radiance estimation using photon points and beams. <i>ACM Transactions on Graphics (Presented at SIGGRAPH 2011) 30</i>, 1 (Jan.), 5:1--5:19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>207</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Christensen, P. H. 1998. Efficient simulation of light transport in scenes with participating media using photon maps. In <i>SIGGRAPH '98</i>, 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>208</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W. 2001. <i>Realistic Image Synthesis Using Photon Mapping</i>. A. K. Peters, Ltd., Natick, MA, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>209</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T. 1986. The rendering equation. In <i>Computer Graphics (Proceedings of SIGGRAPH 86)</i>, 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1966404</ref_obj_id>
				<ref_obj_pid>1966394</ref_obj_pid>
				<ref_seq_no>210</ref_seq_no>
				<ref_text><![CDATA[Knaus, C., and Zwicker, M. 2011. Progressive photon mapping: A probabilistic approach. <i>ACM Transactions on Graphics (Presented at SIGGRAPH 2011) 30</i>, 3 (May), 25:1--25:13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383935</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>211</ref_seq_no>
				<ref_text><![CDATA[Kr&#252;ger, J., B&#252;rger, K., and Westermann, R. 2006. Interactive screen-space accurate photon tracing on gpus. In <i>Rendering Techniques 2006 (Proceedings of the Eurographics Workshop on Rendering)</i>, 319--330.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>212</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P., and Willems, Y. D. 1993. Bi-directional path tracing. In <i>Compugraphics</i>, 145--153.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275468</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>213</ref_seq_no>
				<ref_text><![CDATA[Lafortune, E. P., and Willems, Y. D. 1996. Rendering participating media with bidirectional path tracing. In <i>Eurographics Rendering Workshop 1996</i>, 91--100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>214</ref_seq_no>
				<ref_text><![CDATA[Lastra, M., Ure&#241;a, C., Revelles, J., and Montes, R. 2002. A particle-path based method for monte carlo density estimation. In <i>Rendering Techniques (Proceeeings of the Eurographics Workshop on Rendering)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944753</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>215</ref_seq_no>
				<ref_text><![CDATA[Liktor, G., and Dachsbacher, C. 2011. Real-time volume caustics with adaptive beam tracing. In <i>Symposium on Interactive 3D Graphics and Games</i>, 47--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344958</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>216</ref_seq_no>
				<ref_text><![CDATA[Lokovic, T., and Veach, E. 2000. Deep shadow maps. In <i>SIGGRAPH</i>, 385--392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1572783</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>217</ref_seq_no>
				<ref_text><![CDATA[McGuire, M., and Luebke, D. 2009. Hardware-accelerated global illumination by image space photon mapping. In <i>Proceedings of High Performance Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778803</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>218</ref_seq_no>
				<ref_text><![CDATA[Parker, S. G., Bigler, J., Dietrich, A., Friedrich, H., Hoberock, J., Luebke, D., McAllister, D., McGuire, M., Morley, K., Robison, A., and Stich, M. 2010. Optix: A general purpose ray tracing engine. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2010) 29</i>, 4 (July), 66:1--66:13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732117</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>219</ref_seq_no>
				<ref_text><![CDATA[Pauly, M., Kollig, T., and Keller, A. 2000. Metropolis light transport for participating media. In <i>Rendering Techniques 2000 (Proceedings of the Eurographics Workshop on Rendering)</i>, 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>220</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 2001. Noise hardware. In <i>Realtime Shading, ACM SIGGRAPH Course Notes</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>221</ref_seq_no>
				<ref_text><![CDATA[Raab, M., Seibert, D., and Keller, A. 2008. Unbiased global illumination with participating media. In <i>Monte Carlo and Quasi-Monte Carlo Methods 2006</i>. Springer, 591--606.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1321293</ref_obj_id>
				<ref_obj_pid>1321261</ref_obj_pid>
				<ref_seq_no>222</ref_seq_no>
				<ref_text><![CDATA[Schj&#248;th, L., Frisvad, J. R., Erleben, K., and Sporring, J. 2007. Photon differentials. In <i>GRAPHITE</i>, ACM, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>223</ref_seq_no>
				<ref_text><![CDATA[Silverman, B. 1986. <i>Density Estimation for Statistics and Data Analysis</i>. Monographs on Statistics and Applied Probability. Chapman and Hall, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778791</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>224</ref_seq_no>
				<ref_text><![CDATA[Sun, X., Zhou, K., Lin, S., and Guo, B. 2010. Line space gathering for single scattering in large scenes. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2010) 29</i>, 4 (July), 54:1--54:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>225</ref_seq_no>
				<ref_text><![CDATA[Szirmay-Kalos, L., T&#243;th, B., and Magdics, M. 2011. Free path sampling in high resolution inhomogeneous participating media. <i>Computer Graphics Forum 30</i>, 1, 85--97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>226</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. 1994. Bidirectional estimators for light transport. In <i>Photorealistic Rendering Techniques (Proceedings of the Eurographics Workshop on Rendering)</i>, 147--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>227</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. J. 1997. Metropolis light transport. In <i>Proceedings of SIGGRAPH 97</i>, 65--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531398</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>228</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Zhao, S., Holzschuch, N., and Bala, K. 2009. Single scattering in refractive media with triangle mesh boundaries. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2009) 28</i>, 3 (July), 92:1--92:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>229</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1978. Casting curved shadows on curved surfaces. <i>Computer Graphics (Proceedings of SIGGRAPH 78) 12</i> (Aug.), 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>230</ref_seq_no>
				<ref_text><![CDATA[Woodcock, E., Murphy, T., Hemmings, P., and T. C., L. 1965. Techniques used in the GEM code for Monte Carlo neutronics calculations in reactors and other systems of complex geometry. In <i>Applications of Computing Methods to Reactor Problems</i>, Argonne National Laboratory.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866199</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>231</ref_seq_no>
				<ref_text><![CDATA[Yue, Y., Iwasaki, K., Chen, B.-Y., Dobashi, Y., and Nishita, T. 2010. Unbiased, adaptive stochastic sampling for rendering inhomogeneous participating media. <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2010) 29</i> (Dec.), 177:1--177:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>555371</ref_obj_id>
				<ref_seq_no>232</ref_seq_no>
				<ref_text><![CDATA[{AG00} Apodaca A., Gritz L.: <i>Advanced RenderMan --- Creating CGI for Motion Pictures</i>. Morgan Kaufmann, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566652</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>233</ref_seq_no>
				<ref_text><![CDATA[{BD02} Benson D., Davis J.: Octree textures. In <i>ACM Transactions on Graphics, Proc. SIGGRAPH 02</i> (2002), pp. 785--790.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>340504</ref_obj_id>
				<ref_obj_pid>340501</ref_obj_pid>
				<ref_seq_no>234</ref_seq_no>
				<ref_text><![CDATA[{Chr99} Christensen P.: Faster photon map global illumination. <i>Journal of Graphics Tools 4</i>, 3 (1999), 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>235</ref_seq_no>
				<ref_text><![CDATA[{CLF*03} Christensen P., Laur D., Fong J., Wooten W., Batali D.: Ray differentials and multiresolution geometry caching for distribution ray tracing in complex scenes. In <i>Computer Graphics Forum, Proc. Eurographics 03</i> (2003), pp. 543--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566649</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>236</ref_seq_no>
				<ref_text><![CDATA[{DGPR02} DeBry D., Gibbs J., Petty D., Robins N.: Painting and rendering textures on unparameterized models. In <i>ACM Transactions on Graphics, Proc. SIGGRAPH 02</i> (2002), pp. 763--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>237</ref_seq_no>
				<ref_text><![CDATA[{DP01} Disney Enterprises, Inc., Pixar Animation Studios: Monsters, Inc., 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>311555</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>238</ref_seq_no>
				<ref_text><![CDATA[{Ige99} Igehy H.: Tracing ray differentials. In <i>Computer Graphics, Proc. SIGGRAPH 99</i> (1999), pp. 179--186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566619</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>239</ref_seq_no>
				<ref_text><![CDATA[{JB02} Jensen H., Buhler J.: A rapid hierarchical rendering technique for translucent materials. In <i>ACM Transactions on Graphics, Proc. SIGGRAPH 02</i> (2002), pp. 576--581.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>240</ref_seq_no>
				<ref_text><![CDATA[{JC98} Jensen H., Christensen P.: Efficient simulation of light transport in scenes with participating media using photon maps. In <i>Computer Graphics, Proc. SIGGRAPH 98</i> (1998), pp. 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>275461</ref_obj_id>
				<ref_obj_pid>275458</ref_obj_pid>
				<ref_seq_no>241</ref_seq_no>
				<ref_text><![CDATA[{Jen96} Jensen H.: Global illumination using photon maps. In <i>Rendering Techniques '96, Proc. 7th Eurographics Workshop on Rendering</i> (1996), Springer-Verlag, pp. 21--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>500844</ref_obj_id>
				<ref_seq_no>242</ref_seq_no>
				<ref_text><![CDATA[{Jen01} Jensen H.: <i>Realistic Image Synthesis using Photon Mapping</i>. A. K. Peters, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>243</ref_seq_no>
				<ref_text><![CDATA[{KW00} Keller A., Wald I.: Efficient importance sampling techniques for the photon map. In <i>Proc. 5th Fall Workshop on Vision, Modeling, and Visualization</i> (2000), IEEE, pp. 271--279.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>244</ref_seq_no>
				<ref_text><![CDATA[{LC03} Larsen B., Christensen N.: Optimizing photon mapping using multiple photon maps for irradiance estimates. In <i>Proc. 11th Int. Conf. in Central Europe on Computer Graphics, Visualization and Computer Vision (WSCG)</i> (2003), University of West Bohemia.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>122748</ref_obj_id>
				<ref_obj_pid>122718</ref_obj_pid>
				<ref_seq_no>245</ref_seq_no>
				<ref_text><![CDATA[{LH91} Laur D., Hanrahan P.: Hierarchical splatting: a progressive refinement algorithm for volume rendering. In <i>Computer Graphics, Proc. SIGGRAPH 91</i> (1991), pp. 285--288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>91449</ref_obj_id>
				<ref_obj_pid>91385</ref_obj_pid>
				<ref_seq_no>246</ref_seq_no>
				<ref_text><![CDATA[{LW90} Levoy M., Whitaker R.: Gaze-directed volume rendering. In <i>Computer Graphics, Proc. Symposium on Interactive 3D Graphics</i> (1990), pp. 217--223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731974</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>247</ref_seq_no>
				<ref_text><![CDATA[{Mys97} Myszkowski K.: Lighting reconstruction using fast and adaptive density estimation techniques. In <i>Rendering Techniques '97, Proc. 8th Eurographics Workshop on Rendering</i> (1997), Springer-Verlag, pp. 251--262.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>614392</ref_obj_id>
				<ref_obj_pid>614269</ref_obj_pid>
				<ref_seq_no>248</ref_seq_no>
				<ref_text><![CDATA[{Ney98} Neyret F.: Modeling, animating, and rendering complex scenes using volumetric textures. <i>IEEE Transactions on Visualization and Computer Graphics 4</i>, 1 (1998), 55--70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325246</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>249</ref_seq_no>
				<ref_text><![CDATA[{Pea85} Peachey D.: Solid texturing of complex surfaces. In <i>Computer Graphics, Proc. SIGGRAPH 85</i> (1985), pp. 279--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>250</ref_seq_no>
				<ref_text><![CDATA[{Pea90} Peachey D.: Texture on demand. Pixar technical memo 217 (unpublished manuscript), 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>325247</ref_obj_id>
				<ref_obj_pid>325334</ref_obj_pid>
				<ref_seq_no>251</ref_seq_no>
				<ref_text><![CDATA[{Per85} Perlin K.: An image synthesizer. In <i>Computer Graphics, Proc. SIGGRAPH 85</i> (1985), pp. 287--296.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>252</ref_seq_no>
				<ref_text><![CDATA[{Pix04} Pixar Animation Studios: Ambient occlusion, image-based illumination, and global illumination. PhotoRealistic RenderMan Application Note #35, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258791</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>253</ref_seq_no>
				<ref_text><![CDATA[{PKGH97} Pharr M., Kolb C., Gershbein R., Hanrahan P.: Rendering complex scenes with memory-coherent ray tracing. In <i>Computer Graphics, Proc. SIGGRAPH 97</i> (1997), pp. 101--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>254</ref_seq_no>
				<ref_text><![CDATA[{PP98} Peter I., Pietrek G.: Importance driven construction of photon maps. In <i>Rendering Techniques '98, Proc. 9th Eurographics Workshop on Rendering</i> (1998), Springer-Verlag, pp. 269--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344936</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>255</ref_seq_no>
				<ref_text><![CDATA[{PZvBG00} Pfister H., Zwicker M., van Baar J., Gross M.: Surfels: surface elements as rendering primitives. In <i>Computer Graphics, Proc. SIGGRAPH 00</i> (2000), pp. 335--342.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732120</ref_obj_id>
				<ref_obj_pid>647652</ref_obj_pid>
				<ref_seq_no>256</ref_seq_no>
				<ref_text><![CDATA[{SW00} Suykens F., Willems Y.: Density control for photon maps. In <i>Rendering Techniques '00, Proc. 11th Eurographics Workshop on Rendering</i> (2000), Springer-Verlag, pp. 11--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>732286</ref_obj_id>
				<ref_obj_pid>647653</ref_obj_pid>
				<ref_seq_no>257</ref_seq_no>
				<ref_text><![CDATA[{SW01} Suykens F., Willems Y.: Path differentials and applications. In <i>Rendering Techniques '01, Proc. 12th Eurographics Workshop on Rendering</i> (2001), Springer-Verlag, pp. 257--268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>731965</ref_obj_id>
				<ref_obj_pid>647651</ref_obj_pid>
				<ref_seq_no>258</ref_seq_no>
				<ref_text><![CDATA[{TWFP97} Tobler R., Wilkie A., Feda M., Purgathofer W.: A hierarchical subdivision algorithm for stochastic radiosity methods. In <i>Rendering Techniques '97, Proc. 8th Eurographics Workshop on Rendering</i> (1997), Springer-Verlag, pp. 193--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>259</ref_seq_no>
				<ref_text><![CDATA[{WH92} Ward G., Heckbert P.: Irradiance gradients. In <i>Proc. 3rd Eurographics Workshop on Rendering</i> (1992), pp. 85--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>256158</ref_obj_id>
				<ref_obj_pid>256157</ref_obj_pid>
				<ref_seq_no>260</ref_seq_no>
				<ref_text><![CDATA[{WHSG97} Walter B., Hubbard P., Shirley P., Greenberg D.: Global illumination using local linear density estimation. <i>ACM Transactions on Graphics 16</i>, 3 (1997), 217--259.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801126</ref_obj_id>
				<ref_obj_pid>800059</ref_obj_pid>
				<ref_seq_no>261</ref_seq_no>
				<ref_text><![CDATA[{Wil83} Williams L.: Pyramidal parametrics. In Computer Graphics, Proc. <i>SIGGRAPH 83</i> (1983), pp. 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>378490</ref_obj_id>
				<ref_obj_pid>54852</ref_obj_pid>
				<ref_seq_no>262</ref_seq_no>
				<ref_text><![CDATA[{WRC88} Ward G., Rubinstein F., Clear R.: A ray tracing solution for diffuse interreflection. In <i>Computer Graphics, Proc. SIGGRAPH 88</i> (1988), pp. 85--92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>263</ref_seq_no>
				<ref_text><![CDATA[{WS98} Ward Larson G., Shakespeare R.: <i>Rendering with Radiance</i>. Morgan Kaufmann, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218068</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>264</ref_seq_no>
				<ref_text><![CDATA[Angelidis, A., Neyret, F., Singh, K., and Nowrouzezahrai, D. 2006. A controllable, fast and stable basis for vortex based smoke simulation. In <i>SCA</i>, Eurographics Association, 25--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>265</ref_seq_no>
				<ref_text><![CDATA[Chandrasekar, S. 1960. <i>Radiative Transfer</i>. Dover Publications, New York, New York.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>266</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. H., 2008. Point-based approximate color bleeding. Pixar Technical Memo 08-01, July.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37414</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>267</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., Carpenter, L., and Catmull, E. 1987. The reyes image rendering architecture. In <i>Computer Graphics (Proceedings of SIGGRAPH 87)</i>, 95--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383260</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>268</ref_seq_no>
				<ref_text><![CDATA[Fedkiw, R., Stam, J., and Jensen, H. W. 2001. Visual simulation of smoke. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Computer Graphics Proceedings, Annual Conference Series, 15--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>269</ref_seq_no>
				<ref_text><![CDATA[Gilland, J. 2009. <i>Elemental Magic: The Art of Special Effects Animation</i>. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276436</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>270</ref_seq_no>
				<ref_text><![CDATA[Hong, J.-M., Shinar, T., and Fedkiw, R. 2007. Wrinkled flames and cellular patterns. <i>ACM Transactions on Graphics 26</i>, 3 (July), 47:1--47:6.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1899409</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>271</ref_seq_no>
				<ref_text><![CDATA[Jarosz, W., Nowrouzezahrai, D., Sadeghi, I., and Jensen, H. W. 2011. A comprehensive theory of volumetric radiance estimation using photon points and beams. <i>ACM Transactions on Graphics 30</i>, 1 (Jan.), 5:1--5:19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280925</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>272</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., and Christensen, P. H. 1998. Efficient simulation of light transport in scenes with participating media using photon maps. In <i>Proceedings of SIGGRAPH 98</i>, Computer Graphics Proceedings, Annual Conference Series, 311--320.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>273</ref_seq_no>
				<ref_text><![CDATA[Kajiya, J. T. 1986. The rendering equation. In <i>Computer Graphics (Proceedings of SIGGRAPH 86)</i>, 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531332</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>274</ref_seq_no>
				<ref_text><![CDATA[Kerr, W. B., and Pellacini, F. 2009. Toward evaluating lighting design interface paradigms for novice users. <i>ACM Transactions on Graphics 28</i>, 3 (July), 26:1--26:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778772</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>275</ref_seq_no>
				<ref_text><![CDATA[Kerr, W. B., and Pellacini, F. 2010. Toward evaluating material design interface paradigms for novice users. <i>ACM Transactions on Graphics 29</i>, 4 (July), 35:1--35:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>276</ref_seq_no>
				<ref_text><![CDATA[Kerr, W. B., Pellacini, F., and Denning, J. D. 2010. Bendylights: Artistic control of direct illumination by curving light rays. <i>Computer Graphics Forum 29</i>, 4, 1451--1459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>277</ref_seq_no>
				<ref_text><![CDATA[K&#345;iv&#225;nek, J., Fajardo, M., Christensen, P. H., Tabellion, E., Bunnell, M., Larsson, D., and Kaplanyan, A. 2010. Global illumination across industries. In <i>SIGGRAPH Courses</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015744</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>278</ref_seq_no>
				<ref_text><![CDATA[McNamara, A., Treuille, A., Popovi&#263;, Z., and Stam, J. 2004. Fluid control using the adjoint method. <i>ACM Transactions on Graphics 23</i>, 3 (Aug.), 449--456.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>279</ref_seq_no>
				<ref_text><![CDATA[Obert, J., K&#345;iv&#225;nek, J., Pellacini, F., S&#253;kora, D., and Pattanaik, S. N. 2008. iCheat: A representation for artistic control of indirect cinematic lighting. <i>Computer Graphics Forum 27</i>, 4, 1217--1223.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>280</ref_seq_no>
				<ref_text><![CDATA[Obert, J., Pellacini, F., and Pattanaik, S. N. 2010. Visibility editing for all-frequency shadow design. <i>Comput. Graph. Forum 29</i>, 4, 1441--1449.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1243983</ref_obj_id>
				<ref_obj_pid>1243980</ref_obj_pid>
				<ref_seq_no>281</ref_seq_no>
				<ref_text><![CDATA[Pellacini, F., Battaglia, F., Morley, R. K., and Finkelstein, A. 2007. Lighting with paint. <i>ACM Transactions on Graphics 26</i>, 2 (June), 9:1--9:14.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778771</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>282</ref_seq_no>
				<ref_text><![CDATA[Pellacini, F. 2010. envylight: An interface for editing natural illumination. <i>ACM Transactions on Graphics 29</i>, 4 (July), 34:1--34:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778793</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>283</ref_seq_no>
				<ref_text><![CDATA[Sadeghi, I., Pritchett, H., Jensen, H. W., and Tamstorf, R. 2010. An artist friendly hair shading system. <i>ACM Transactions on Graphics 29</i>, 4 (July), 56:1--56:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778794</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>284</ref_seq_no>
				<ref_text><![CDATA[Schmid, J., Sumner, R. W., Bowles, H., and Gross, M. 2010. Programmable motion effects. <i>ACM Transactions on Graphics 29</i>, 4 (July), 57:1--57:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987666</ref_obj_id>
				<ref_obj_pid>987657</ref_obj_pid>
				<ref_seq_no>285</ref_seq_no>
				<ref_text><![CDATA[Selle, A., Mohr, A., and Chenney, S. 2004. Cartoon rendering of smoke animations. In <i>Non-photorealistic Animation and Rendering</i>, ACM, 57--60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531337</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>286</ref_seq_no>
				<ref_text><![CDATA[Song, Y., Tong, X., Pellacini, F., and Peers, P. 2009. Subedit: A representation for editing measured heterogeneous sub-surface scattering. <i>ACM Transactions on Graphics 28</i>, 3 (July), 31:1--31:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015748</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>287</ref_seq_no>
				<ref_text><![CDATA[Tabellion, E., and Lamorlette, A. 2004. An approximate global illumination system for computer generated films. <i>ACM Transactions on Graphics 23</i>, 3 (Aug.), 469--476.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>288</ref_seq_no>
				<ref_text><![CDATA[Tessendorf, J., and Kowalski, M. 2010. Resolution independent volumes. In <i>ACM SIGGRAPH 2010 Courses</i>, SIGGRAPH, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882337</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>289</ref_seq_no>
				<ref_text><![CDATA[Treuille, A., McNamara, A., Popovi&#263;, Z., and Stam, J. 2003. Keyframe control of smoke simulations. <i>ACM Transactions on Graphics 22</i>, 3 (July), 716--723.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531398</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>290</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Zhao, S., Holzschuch, N., and Bala, K. 2009. Single scattering in refractive media with triangle mesh boundaries. <i>ACM Transactions on Graphics 28</i>, 3 (July), 92:1--92:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>291</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1978. Casting curved shadows on curved surfaces. In <i>Computer Graphics (Proceedings of SIGGRAPH 78)</i>, 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2012 State of the Art in Photon Density Estimation Course Notes Toshiya Hachisuka Wojciech 
Jarosz Aarhus University Disney Research Z¨urich Guillaume Bouchard Per Christensen Universit´e Claude 
Bernard Lyon 1, CNRS, France Pixar Jeppe Revall Frisvad Wenzel Jakob Technical University of Denmark 
Cornell University Henrik Wann Jensen Michael Kaschalk UC San Diego Walt Disney Animation Studios Claude 
Knaus Andrew Selle University of Bern Walt Disney Animation Studios Ben Spencer Swansea University Abstract 
 Photon density estimation techniques are a popular choice for simulating light transport in scenes with 
complicated geometry and materials. This class of algorithms can be used to accurately sim­ulate inter-re.ections, 
caustics, color bleeding, scattering in participating media and subsurface scattering. Since its introduction, 
photon density estimation has been signi.cantly extended in computer graphics with the introduction of: 
specialized techniques that intelligently modify the positions or bandwidths to reduce visual error using 
a small number of photons, approaches which eliminate error completely in the limit, and methods that 
use higher-order samples and queries to reduce error in participating media. This course provides the 
necessary insight to implement all these latest advances in photon density estimation. The course starts 
out with a short intro­duction to photon density estimation using classical photon mapping, but the remainder 
of the two-part course provides new, hands-on explanations of the latest developments in this area by 
the experts behind each technique. The course will also cover recent applications of these techniques 
within industry. The course will give the audience concrete and practical understanding of the latest 
developments in photon density estimation techniques that have not been presented in prior SIGGRAPH courses. 
i   Contents Abstract.............................................. i TableofContents ......................................... 
ii AbouttheOrganizers ...................................... iv AbouttheLecturers ....................................... 
v CourseSyllabus .......................................... viii Introduction ............................................ 
x I Course Materials 1 1 Regular Photon Density Estimation 2 1.1 PhotonMappingBasics .................................. 
3 1.2 PhotonRelaxation ..................................... 4 1.3 PhotonDi.erentials .................................... 
25 2 Progressive Photon Density Estimation 37 2.1 ProgressivePhotonMapping&#38;Extensions ....................... 
38 2.2 Probabilistic Formulation of Photon Density Estimation . . . . . . . . . . . . . . . . 123 3 
Photon Density Estimation in Participating Media 124 3.1 ParticipatingMediaBasics ................................ 
125 3.2 ProgressiveExpectation Maximization .......................... 208 3.3 FromPhotonstoBeams .................................. 
252 4 Photon Density Estimation in Industry 299 4.1 Photon Beams in Disney s Tangled . . ... ... . ... 
. ... ... . ... ... . 300 ® 4.2 Photon Mapping in Pixar s RenderMan R. . ... . ... ... . ... ... . ... 
. 301 4.3 ProgressivePhotonMappinginLuxRender ....................... 302 II Supplemental Materials 
321 5 Regular Photon Density Estimation 322 5.1 Supplemental Material for Photon Mapping Basics . . . 
. . . . . . . . . . . . . . . . 323 5.2 SupplementalMaterialforPhotonRelaxation ...................... 
324 5.3 SupplementalMaterialforPhotonDi.erentials ..................... 333 ii 6 Progressive Photon 
Density Estimation 342 6.1 Supplemental Material for Progressive Photon Mapping . . . . . . . . . . . 
. . . . . 343 6.2 Supplemental Material for Extensions of Progressive Photon Mapping . . . . . . . . 
351 7 Photon Density Estimation in Participating Media 360 7.1 Supplemental Material for Beam Radiance 
Estimation . . . . . . . . . . . . . . . . . 361 7.2 Supplemental Material for Progressive Expectation 
Maximization . . . . . . . . . . . 372 7.3 SupplementalMaterialforPhotonBeams ........................ 
384 7.4 Supplemental Material for Progressive Photon Beams . . . . . . . . . . . . . . . . . 404 8 Photon 
Density Estimation in Industry 416 8.1 Supplemental Material for Photon Mapping in ® RenderMan R. ... 
. ... ... . ... ... . ... . ... ... . ... . ... ... 417 8.2 Supplemental Material for Photon Beams in 
Disney s Tangled ... . ... ... . . 442 iii About the Organizers Toshiya Hachisuka <toshiya@cs.au.dk> 
 Aarhus University http://cs.au.dk/~toshiya/ Toshiya Hachisuka is an Assistant Professor in the Department 
of Computer Science at Aarhus University. His main research interests are the development of general 
light transport simulation algorithms and the intersection of computational statistics and realistic 
image synthesis. He has published multiple work on those topics including a new formulation of photon 
density estimation and a multidimensional adaptive sampling framework for ray tracing. He received his 
Ph.D. in Computer Science from University of California, San Diego in 2011 and B.Eng. from the University 
of Tokyo in 2006. Wojciech Jarosz <wjarosz@disneyresearch.com> Disney Research Z¨urich http://zurich.disneyresearch.com/~wjarosz 
 Wojciech Jarosz is a Research Scientist in charge of the rendering group at Disney Research Z¨urich, 
and an adjunct lecturer at ETH Z¨urich. Prior to joining Disney, Wojciech obtained his Ph.D. (2008) and 
M.S. (2005) in computer graphics from UC San Diego, and his B.S. (2003) in computer science from the 
University of Illinois, Urbana-Champaign. Wojciech s main research interest is realistic image synthesis 
and his publications explore practical applications in a variety of areas in computer graphics including: 
participating media; complex illumination and materials; global illumination; Monte Carlo methods and 
e.cient sampling; and high-dynamic range imaging. His work in these areas has been incorporated into 
production ren­dering systems and used in the making of feature .lms, including Disney s Tangled (2010). 
iv  About the Lecturers Guillaume Bouchard <guillaume.bouchard@liris.cnrs.fr> Universit´e Claude Bernard 
Lyon 1, CNRS, France Guillaume Bouchard is a French Ph.D. student studying physically based rendering 
of scenes dom­inated by specular light paths. He works in the R3AM team of the LIRIS laboratory, which 
is based in Lyon. The team s specialty is realistic and real-time rendering. He is also a luxrender core 
developer since 2010. He has been mostly involved in the implementation of progressive pho­ton density 
estimation methods. He obtained his M.S. in 2009 from the UCBL where he taught computer science from 
the B.S. to the M.S. level. He also has an Ing. from INSA Lyon. Per H. Christensen <per.christensen@acm.org> 
 Pixar http://www.seanet.com/~myandper/per.htm Per Christensen is a senior software developer in Pixar 
s RenderMan group in Seattle. His main research interests are e.cient ray tracing and global illumination 
in very complex scenes. He received an M.Sc. degree in electrical engineering from the Technical University 
of Denmark and a Ph.D. in computer science from the University of Washington in Seattle. Before joining 
Pixar, he worked at Mental Images in Berlin and at Square USA in Honolulu. His movie credits include 
Finding Nemo, The Incredibles, Cars, Ratatouille, Wall-E, Up, Toy Story 3, and Cars 2. He has received 
an Academy Award for his contributions to the development of point-based global illumination and ambient 
occlusion. Jeppe Revall Frisvad <jrf@imm.dtu.dk> Technical University of Denmark http://www2.imm.dtu.dk/~jrf/ 
 Jeppe Revall Frisvad is an Associate Professor at the Technical University of Denmark (DTU). He received 
a M.Sc.(Eng.) degree in Applied Mathematics (2004) and a Ph.D. degree in Computer Graphics (2008) from 
DTU. His research interests are mainly material appearance modeling, re­alistic rendering, and light 
scattering. Since 2007, he has taught several postgraduate courses in rendering and also freshman courses 
in mathematics and programming. Wenzel Jakob <wenzel@cs.cornell.edu> Cornell University http://www.cs.cornell.edu/~wenzel/ 
 Wenzel Jakob is a 4th year Ph.D. student at Cornell University advised by Dr. Steve Marschner. His research 
interests focus on robust bidirectional light transport algorithms, volumetric light transport and material 
appearance modeling. He is the designer of Mitsuba, a research-oriented rendering system. v Henrik Wann 
Jensen <henrik@cs.ucsd.edu> University of California, San Diego http://graphics.ucsd.edu/~henrik Henrik 
Wann Jensen is a Professor at UC San Diego where he teaches computer graphics. His main research interest 
is in the area of global illumination and appearance modeling. He received his M.Sc. degree and his Ph.D. 
degree from the Technical University of Denmark. He is the author of Realistic Image Synthesis using 
Photon Mapping , AK Peters 2001. Michael Kaschalk <michael.kaschalk@disneyanimation.com> Walt Disney 
Animation Studios Born in Johnstown, PA and a graduate of the Art Institute of Pittsburgh, E.ects Supervisor, 
Michael Kaschalk started at Walt Disney Animation Studios in January 1997 on Fantasia 2000. Kaschalk 
is credited on such animated titles as Meet the Robinsons, Chicken Little, Home on the Range, Atlantis, 
Tarzan, and Fantasia 2000. He currently leads a team of FX artists creating cutting edge visual FX that 
add depth and dimension to such recent box o.ce successes as Tangled. In 2008 Kaschalk was nominated 
for a VES award for Best Visual E.ects on Bolt. Claude Knaus <knaus@iam.unibe.ch> University of Bern 
http://www.cgg.unibe.ch/staff/claude-knaus Claude Knaus is a research assistant at the University of 
Bern. His research interest is rendering. In his previous life, he was a software engineer and architect 
at various companies, including SGI and IBM. He obtained his M.S. in computer science from ETH Zurich 
in 1998. Andrew Selle <andrew.selle@disneyanimation.com> Walt Disney Animation Studios http://physbam.stanford.edu/~aselle/ 
 Andrew Selle graduated with a Ph.D. in Computer Science from Stanford in June 2008, where his advisor 
was Ron Fedkiw. He is currently employed at Walt Disney Animation Studios in Burbank, CA. Before this 
he was in graduate school and also consulted at Industrial Light + Magic and Intel Corporation. Andrew 
received his undergraduate degree in Computer Science and Mathematics at the University of Wisconsin 
Madison. He has been credited on several movies: Poseidon (2006), Pirates of the Caribbean: At World 
s End (2007), Evan Almighty (2007), The Princess and the Frog (2009), Tangled (2010), Winnie the Pooh 
(2011). Andrew s interest is primarily physical simulation in special e.ects, speci.cally .uid and deformable 
solid e.ects. Ben Spencer <csbenjamin@swansea.ac.uk> Swansea University http://cs.swan.ac.uk/~csbenjamin/ 
 vi Ben Spencer received B.S. and Ph.D. degrees in computer science from Swansea University in the UK 
where he studied methods of realistic rendering and global illumination. He now works as a full­time 
postdoctorate researcher. His research interests focus on the development of realistic rendering algorithms 
with particular emphasis on the photon mapping framework and its performance under challenging lighting 
conditions. vii  Course Syllabus Photon mapping techniques have progressed signi.cantly in the last 
few years. The .rst part will include a brief introduction to light transport and the core ideas behind 
photon mapping. The remainder of the time will focus on recent advances in photon density estimation 
for surfaces. The second part will start with a brief introduction to participating media, followed by 
in-depth presentations on recent developments in volumetric photon mapping. The last portion of the course 
will cover the uses of photon density estimation in industry. These topics are shown below: A detailed 
breakdown in provided in Figure 1. Each slot is 5 min. viii  Figure 1: Session timing breakdown for 
the course. ix  Introduction Many .elds require visually accurate recreations of the real world. For 
example, computer-generated images used for product design need to be as visually close as possible to 
the real world correspon­dences. Architects need accurate ways to pre-visualize the appearance of buildings 
under di.erent lighting con.gurations before construction. Movies, advertisements, and video games also 
need believable computer-generated images. Computer simulation of how light interacts with virtual shapes 
and materials, light transport simulation, is a natural approach to achieve this result. Figure 2: Photon 
density estimation can be used to simulate light transport in a complex architec­tural model. The sun 
is the only light source and most of the light in the image is due to indirect bounces of light. Among 
many di.erent approaches of light transport simulation, one of the most popular is photon density estimation.The 
basic algorithm is composed of two passes. The .rst pass distributes packets of light energy, called 
photons, using a light transport simulation starting from light sources, and x the second pass computes 
illumination at locations within the scene by estimating the local photon density. This class of methods 
poses light transport as a density estimation problem. Figure 2 shows one example of light transport 
simulation using photon density estimation. Since its introduction to computer graphics, photon density 
estimation techniques have been used in many practical applications. At the same time, the original techniques 
have been signi.cantly expanded, especially in the past several years. Early Photon Density Estimation 
Methods The earliest application of photon density estimation was Arvo s backward ray tracing method 
[Arv86]1 . Arvo s approach .rst distributes packets of light energy across the scene using photon tracing. 
It then computes the local density using the histogram method, by counting the number of photons in discrete 
bins on surfaces, to estimate the lighting intensity. (a) (b) (c) Figure 3: Rendered images with early 
photon density estimation techniques: (a) Caustics rendered by backward ray tracing [Arv86]. (b) Direct 
visualization of photon density estimation using photon mapping. (c) Rendered image using .nal gathering 
and the photon map in (b) [Jen96]. Since then, photon density estimation has attracted great interests 
from the research community due to its generality. For example, Shirley et al. [SWH+95] applied a photon 
density estimation method to compute a view-independent solution of light transport simulation on a triangle 
mesh. Perhaps one of the most successful photon density estimation algorithms is photon mapping [Jen96]. 
Photon mapping uses kNN density estimation and was the .rst to use photon density estimation for view-independent 
rendering. Photon mapping also popularized the concept of the two pass approach to photon density estimation. 
Examples from these early photon density estimation techniques are shown in Figure 3. 1The word backward 
comes from the fact that traditional ray tracing [Whi80] traces rays from the eye, whereas backward ray 
tracing traces rays from light sources. xi Improved Photon Density Estimation Methods Initial applications 
of photon density estimation used standard techniques such as kNN kernel density estimation and the histogram 
method (a review of existing density estimation methods is available in the book by Hastie et al [HTF01]). 
More recent techniques have proposed novel density estimation methods customized for photon density estimation. 
Figure 4 shows some examples of such improvements. One such technique is a density estimator of segments 
of light transport paths, rather than the endpoints of segments (i.e., photons), which reduces bias in 
photon density estimation [LURM02]. Schjøth et al. applied the concept of ray di.erentials [Ige99] to 
lights transport paths for pho­ tons [SFES07]. The idea is to compute an approximated footprint of a 
photon by considering a photon path as a beam of light with .nite thickness. The estimated footprints 
are used to determine appropriate bandwidths for photon density estimation. This concept was later extended 
to handle the temporal footprints of photons [SFES10].  (a) (b) Figure 4: Results of improved photon 
density estimation methods: (a) Images rendered with (right) and without (left) photon di.erentials [SFES07]. 
(b) Images rendered with (right) and without (left) photon relaxation [SJ09]. For particular types of 
light transport paths such as caustics, noise in the photon distribution leads directly to artifacts 
in the rendered image. For example, even if photons are distributed according to a uniform distribution, 
with a .nite number of photons, the randomness of photon locations makes the estimated density deviate 
from a constant intensity. Photon relaxation [SJ09] signi.cantly reduces such artifacts by moving photons 
into a target distribution with lower discrepancy. Progressive Photon Density Estimation One fundamental 
limitation in the standard two-pass photon density estimation algorithm is that we need to store all 
photons before we perform the density estimation pass. Since all photons have to be stored, this limits 
the quality of of the .nal result based on the amount of available storage. In contrast, for light transport 
simulation algorithms based on Monte Carlo path integration [DBB06], the quality is only limited by the 
computation time. Hachisuka et al. [HOJ08] introduced a new framework of density estimation, called progressive 
 xii density estimation, which completely removes this limitation. In progressive density estimation, 
the result is progressively re.ned as new photons samples are generated and the density estimate converges 
to the correct solution without storing photons. The quality of the progressive density estimate is only 
limited by the computation time, just like Monte Carlo path integration; however, unlike Monte Carlo 
path integration, progressive photon density estimation is robust to specular­di.use-specular light transport 
(Figure 5). The basic progressive density estimation was extended to incorporate distribution ray tracing 
ef­fects such as depth of .eld and motion blur [HJ09]. Knaus and Zwicker later showed a di.erent formulation 
of progressive density estimation [KZ11]. This new formulation is easier to implement  Figure 5: Light 
transport simulation of a glass lamp using di.erent rendering methods [HOJ08].  (a) (b) Figure 6: Complex 
light transport simulation with distribution ray tracing e.ects: (a) Alarm clocks illuminated by a desk 
lamp with depth of .eld [HJ09]. (b) Rendering of diamonds with dispersions and depth of .eld [KZ11]. 
 xiii  (a) (b) Figure 7: Rendered images of participating media using photon density estimation: (a) 
Comparison of original volumetric photon mapping (top) and the bream radiance estimate (bottom) [JZJ08]. 
 (b) The result of .tting 4K Gaussians using progressive EM [JRJ11]. than the original formulation, 
using conventional photon density estimation as a black box, and also supports distribution ray tracing 
e.ects. Figure 6 highlights some results from those papers. Rendering Participating Media using Photon 
Density Estimation Photon density estimation is not just for solving light transport between surfaces. 
Volumetric photon mapping [JC98] is an extension of the original photon mapping method which can handle 
light transport within participating media such as smoke and fog. One recent improvement on this topic 
is the beam radiance estimate [JZJ08]. The idea is to replace the costly ray march­ing process in volumetric 
photon mapping by a single density estimation of photons along a line of sight (i.e., the beam of the 
eye path). Later, Jakob et al. [JRJ11] proposed an accelerated expectation maximization technique to 
.t a compact gaussian mixture model to the underlying photon distribution. This results in a practical 
algorithm for photon density estimation in partic­ipating media which incorporates elements of photon 
relaxation and level-of-detail within a single framework. Figure 7 shows some results from those papers. 
Jarosz et al. introduced a generalized theory of volumetric density estimation using points or beams 
[JNSJ11]. In particular, they introduced the concept of photon beams which replaces pho­ tons by segments 
of photon paths within participating media and signi.cantly improves the quality of photon density estimation 
for participating media. Recently, the concept of photon beam was combined with progressive photon density 
estimation [JNT+11]. Like progressive photon mapping, the algorithm is robust to specular-di.use-specular 
light transport and provably converges to the correct solution (Figure 8). Rendering Systems using Photon 
Density Estimation The combination of original photon mapping and .nal gathering has been widely used 
in many di.erent commercial rendering systems. The basic approach is to use the result of photon mapping 
as a rough solution of global illumination, and re.ne this solution by another step that performs xiv 
 Figure 8: Rendered images using progressive photon beams for both homogeneous and heteroge­neous media 
[JNT+11]. We generate a sequence of independent render passes (middle) where we progressively reduce 
the photon beam radii. additional gathering of radiance by tracing one-bounce rays called a .nal gathering 
step. This ® approach is used within Pixar s Photorealistic RenderMan Rsoftware to incorporate global 
illu­mination in movie production [CB04]. Recent versions of the software include improvements in photon 
mapping tailored to more general global illumination and participating media rendering. Recent developments 
in photon density estimation have also started making their ways into other rendering systems. LuxRender 
[Lux12], one of the open source rendering systems with a large user base, is currently incorporating 
progressive photon mapping as a new feature for e.ciently rendering scenes with glass, water, and mirrors. 
The photon beams method was also recently used for the production of the Disney s animated .lm Tangled 
with extensions to allow artist-driven design of volumetric e.ects [NJS+11] (Figure 9). xv  Figure 
9: The photon beams algorithm was customized to author artistic volumetric e.ects for the movie Tangled. 
The technique s ability to produce curving light beams was used to match the organic artistic style of 
the .lm [NJS+11]. Copyright &#38;#169;cDisney Enterprises, Inc. xvi  Bibliography [Arv86] James Arvo. 
Backward ray tracing. In In ACM SIGGRAPH 86 Course Notes, Devel­ opments in Ray Tracing, pages 259 263, 
1986. xi [CB04] Per H. Christensen and Dana Batali. An irradiance atlas for global illumination in complex 
production scenes. In Alexander Keller and Henrik Wann Jensen, editors, Proceedings of the 15th Eurographics 
Workshop on Rendering Techniques, Norkping, Sweden, June 21-23, 2004, pages 133 142. Eurographics Association, 
2004. xv [DBB06] Philip Dutr´e, Philippe Bekaert, and Kavita Bala. Peters, Ltd., second edition, 2006. 
xii Advanced Global Illumination. AK [HJ09] Toshiya Hachisuka and Henrik Jensen. Stochastic progressive 
photon mapping. In SIGGRAPH Asia 09: ACM SIGGRAPH Asia 2009 papers, pages 1 8, New York, NY, USA, 2009. 
ACM. xiii [HOJ08] Toshiya Hachisuka, Shinji Ogaki, and Henrik Jensen. Progressive photon mapping. ACM 
Transactions on Graphics (SIGGRAPH Asia Proceedings), 27(5):Article 130, 2008. xii, xiii [HTF01] Trevor 
Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer Series 
in Statistics. Springer New York Inc., New York, NY, USA, 2001. xii [Ige99] Homan Igehy. Tracing ray 
di.erentials. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, 
SIGGRAPH 99, pages 179 186, New York, NY, USA, 1999. ACM Press/Addison-Wesley Publishing Co. xii [JC98] 
Henrik Jensen and Per Christensen. E.cient simulation of light transport in scences with participating 
media using photon maps. In Computer Graphics (Proceedings of SIGGRAPH 98), pages 311 320, New York, 
NY, USA, 1998. ACM Press. xiv [Jen96] Henrik Jensen. Global illumination using photon maps. In Xavier 
Pueyo and Pe­ ter Schr¨oder, editors, Rendering Techniques 96, Eurographics, pages 21 30. Springer-Verlag 
Wien New York, 1996. xi [JNSJ11] Wojciech Jarosz, Derek Nowrouzezahrai, Iman Sadeghi, and Henrik Wann 
Jensen. A comprehensive theory of volumetric radiance estimation using photon points and beams. xvii 
 ACM Transactions on Graphics (Presented at ACM SIGGRAPH 2011), 30(1):5:1 5:19, January 2011. xiv [JNT+11] 
Wojciech Jarosz, Derek Nowrouzezahrai, Robert Thomas, Peter-Pike Sloan, and Matthias Zwicker. Progressive 
photon beams. ACM Transactions on Graphics (Pro­ ceedings of ACM SIGGRAPH Asia 2011), 30(6), December 
2011. xiv, xv [JRJ11] Wenzel Jakob, Christian Regg, and Wojciech Jarosz. Progressive expectation maximization 
for hierarchical volumetric photon mapping. Computer Graphics Forum (Proceedings of EGSR 2011), 30(4), 
June 2011. xiv [JZJ08] Wojciech Jarosz, Matthias Zwicker, and Henrik Wann Jensen. The beam radiance 
estimate for volumetric photon mapping. Computer Graphics Forum (Proceedings of Eurographics 2008), 27(2):557 
566, April 2008. xiv [KZ11] Claude Knaus and Matthias Zwicker. Progressive photon mapping: A probabilistic 
 approach. ACM Trans. Graph., 30:25:1 25:13, May 2011. xiii [LURM02] M. Lastra, C. Urena, J. Revelles, 
and R. Montes. A particle-path based method for monte carlo density estimation. In Rendering Techniques 
2002 (Proceedings of the Thirteenth Eurographics Workshop on Rendering), June 2002. xii [Lux12] Luxrender. 
http://www.luxrender.net,http://www.luxrender.net/wiki/New_in_ 0-9, 2012. xv [NJS+11] Derek Nowrouzezahrai, 
Jared Johnson, Andrew Selle, Dylan Lacewell, Michael Kaschalk, and Wojciech Jarosz. A programmable system 
for artistic volumetric lighting. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2011), 30(4):29:1 
29:8, August 2011. xv, xvi [SFES07] Lars Schjøth, Jeppe Revall Frisvad, Kenny Erleben, and Jon Sporring. 
Photon di.er­entials. In Proceedings of the 5th international conference on Computer graphics and interactive 
techniques in Australia and Southeast Asia, GRAPHITE 07, pages 179 186, New York, NY, USA, 2007. ACM. 
xii [SFES10] Lars Schjøth, Jeppe Revall Frisvad, Kenny Erleben, and Jon Sporring. Temporal photon di.erentials. 
In GRAPP 10, pages 54 61, 2010. xii [SJ09] Ben Spencer and Mark W. Jones. Into the blue: Better caustics 
through photon re­laxation. Eurographics 2009, Computer Graphics Forum, 28(2):319 328, March 2009. xii 
 [SWH+95] Peter Shirley, Bretton Wade, Philip Hubbard, David Zareski, Bruce Walter, and Don­ald Greenberg. 
Global illumination via density estimation. Rendering Techniques 95 (Proceedings of Eurographics Workshop 
on Rendering 95), pages 219 230, 1995. xi [Whi80] Turner Whitted. An improved illumination model for 
shaded display. Commun. ACM, 23(6):343 349, 1980. xi xviii  Course Materials 1  Regular Photon Density 
Estimation 2 Photon Mapping Basics Photon Relaxation            Photon Di.erentials  
      Progressive Photon Density Estimation 37 Progressive Photon Mapping &#38; Extensions 38 
                                           Probabilistic Formulation 
of Photon Density Estimation 123  Photon Density Estimation in Participat­ing Media 124 Participating 
Media Basics 125                                          
 Progressive Expectation Maximization 208                       From Photons 
to Beams 252                         Photon Density Estimation in Industry 
 299 Photon Beams in Disney s Tangled 300 &#38;#169; Photon Mapping in Pixar s RenderMan R 301  
Progressive Photon Mapping in LuxRender 302           Supplemental Materials 321  Regular 
Photon Density Estimation 322 Supplemental Material for Photon Mapping Basics 323  Supplemental Material 
for Photon Relaxation 324      Supplemental Material for Photon Di.erentials 333      Progressive 
Photon Density Estimation 342 Supplemental Material for Progressive Photon Mapping 343     Supplemental 
Material for Extensions of Progressive Photon Mapping 351      Photon Density Estimation in Participat­ing 
Media 360 Supplemental Material for Beam Radiance Estimation 361       Supplemental Material 
for Progressive Expectation Maximization 372       Supplemental Material for Photon Beams 384 
           Supplemental Material for Progressive Photon Beams 404        Photon Density 
Estimation in Industry 416 Supplemental Material for Photon Mapping in &#38;#169; RenderMan R 417 
             Supplemental Material for Photon Beams in Disney s Tangled 442      
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343490</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>217</pages>
		<display_no>7</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Optimizing realistic rendering with many-light methods]]></title>
		<page_from>1</page_from>
		<page_to>217</page_to>
		<doi_number>10.1145/2343483.2343490</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343490</url>
		<abstract>
			<par><![CDATA[<p>With recent improvements in hardware performance, there has been an increased demand in various industries, including game development, film production, and architectural visualization, for realistic image rendering with global illumination (GI). But current algorithms can not fulfill the strict speed and quality requirements of modern applications. Many-light rendering solves this problem. By reducing light-transport simulation to rendering the scene with many light sources, the many-light formulation offers a unified view of the global-illumination scene. Unlike other GI algorithms, the quality-speed trade-off in the many-light methods produces artifact-free images in a fraction of a second while converging to the full GI solution over time.</p> <p>This course presents a coherent summary of the state of the art in many-light rendering. It covers the basic many-light formulation and recent work on its use for computing global illumination in real time, on improving scalability with a large number of lights, on using the many-light method as a basis for a full GI solution, and on rendering participating media. The course focuses on the clarity of the underlying mathematical concepts as well as on the practical aspects of the individual algorithms. One segment of the course is devoted to the practical considerations of using many-light methods in the Autodesk Cloud Rendering service.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738901</person_id>
				<author_profile_id><![CDATA[81100294023]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaroslav]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Krivanek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Charles University in Prague]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738902</person_id>
				<author_profile_id><![CDATA[81314494073]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Milos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hasan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738903</person_id>
				<author_profile_id><![CDATA[81100118944]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arbree]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Autodesk, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738904</person_id>
				<author_profile_id><![CDATA[81487650764]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Carsten]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dachsbacher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Karlsruher Institut f&#252;r Technologie]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738905</person_id>
				<author_profile_id><![CDATA[81504682709]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NVIDIA Advanced Rendering Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738906</person_id>
				<author_profile_id><![CDATA[81100010986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Walter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Optimizing Realistic Rendering with Many-Light Methods SIGGRAPH 2012 Course Course Notes This is a 
preliminary version. The .nal version is available from http://cgg.mff.cuni.cz/ jaroslav/papers/mlcourse2012. 
 Organizers Jaroslav K.anek riv´ Charles University, Prague Milo.s Ha.san UC Berkeley Lecturers Adam 
Arbree Autodesk, Inc. Carsten Dachsbacher Karlsruhe Institute of Technology Alexander Keller NVIDIA ARC 
GmbH Bruce Walter Cornell University Abstract With the recent improvements in hardware performance, there 
has been an increased demand in various industries, including game development, .lm production, or archi­tectural 
visualization, for realistic image rendering with global illumination. However, the inability of the 
state of the art algorithms to meet the strict speed and quality re­quirements fosters more research 
in this area. Many-light rendering, a class of methods derived from the Instant Radiosity algorithm proposed 
by Keller [1997], has received much attention in recent years. By reducing light transport simulation 
to rendering the scene with many light sources, the many-light formulation offers a uni.ed view of the 
global illumination problem. Unlike with other GI algorithms, the quality-speed trade­off in the many-light 
methods (in terms of the number of lights) is able to produce artifact-free images in a fraction of a 
second while converging to the full GI solution over time. This formulation is therefore potentially 
able to cater to all of the aforemen­tioned industries. The primary goal of this course is to present 
a coherent summary of the state of the art in many-light rendering. The course covers the basic many-light 
formulation, as well as the recent work on its use for computing global illumination in real-time, on 
improving scalability with a large number of lights, on using many-lights as a basis for a full GI solution, 
and also on rendering participating media. The course will focus on the clarity of the underlying mathematical 
concepts as well as on the practical aspects of the individual algorithms. An important part of the course 
will be devoted to the practical considerations necessary for the use of many-light methods in the Autodesk 
360 Rendering service. Intended audience Industry professionals and researchers interested in recent 
advances in realistic render­ing with global illumination. Software developers and managers looking for 
the right global illumination solution for their application will also bene.t from the course. Prerequisites 
Familiarity with rendering and with concepts of global illumination computation is expected. Level of 
dif.culty Intermediate Syllabus 1. Introduction (K.anek) riv´ (5 min) 2. Instant Radiosity -principles 
and practice (Keller) (30 min)  Light transport simulation using many point light sources  Path space 
partitioning  Consistent generation of light paths  3. Handling dif.cult paths (Ha.san, K.anek) riv´ 
(30 min) Kollig-Keller method for dealing with singularities and its limitations  Virtual spherical 
lights  Improved VPL distribution and local VPLs  4. Scalability with many lights I (Walter) (25 min) 
 Lightcuts and Multidimensional Lightcuts Break (15 min) 4. Scalability with many lights II (Ha.san) 
(20 min)  Matrix row-column sampling, Matrix Slice Sampling  Visibility clustering  5. Real-time many-light 
rendering (Dachsbacher) (35 min)  Many-light generation with Re.ective Shadow Maps  Fast rendering 
with many lights via Imperfect Shadow Maps and Micro-Rendering  Approximate bias compensation for high-quality 
rendering of surface and volume lighting  6. Many-lights methods in Autodesk 360 Rendering (Arbree) 
(30 min)  What is Autodesk Cloud Rendering?  Our many-lights rendering algorithm  Advantages of a 
many-light solution  Discussion of results  7. Conclusions / Q &#38; A (all) (5 min)  Course presenter 
information Jaroslav K.anek riv ´Charles University, Prague jaroslav.krivanek@mff.cuni.cz Jaroslav is 
an assistant professor at Charles University in Prague. Prior to this ap­pointment, he was a Marie Curie 
post-doctoral research fellow at the Cornell Univer­sity Program of Computer Graphics, and a junior researcher 
and assistant professor at Czech Technical University in Prague. Jaroslav received his Ph.D. from IRISA/INRIA 
Rennes and the Czech Technical University (joint degree) in 2005. In 2003 and 2004 he was a research 
associate at the University of Central Florida. His primary research interest is realistic rendering 
and global illumination. Milo.san s Ha.UC Berkeley milos.hasan@gmail.com Milo.san is a post-doctoral 
researcher at the University of California, Berkeley. He s Ha.received his Ph.D. from Cornell University 
in 2009, after which he spent one year as a post-doctoral fellow at Harvard University. His primary research 
interest is in the area of light transport, including ef.cient global illumination algorithms, precomputed 
techniques, GPU-oriented algorithms, and applications to fabrication and visualization. Adam Arbree Autodesk, 
Inc. adam.arbree@autodesk.com Adam Arbree is a principle engineer developing rendering applications for 
Autodesk. He received his Ph.D. from Cornell University in 2009 and, in his dissertation, he cre­ated 
scalable many-lights rendering algorithm for subsurface scatting. For the last two years, he has been 
designing and building a physically accurate, many-lights rendering system for architectural visualization 
for Autodesk. The resulting product, Autodesk 360 Rendering, premiered in September 2011 as part Autodesks 
global launch of cloud services. Carsten Dachsbacher Karlsruhe Institute of Technology dachsbacher@kit.edu 
Carsten Dachsbacher is Full Professor for computer graphics at the Karlsruhe Insti­tute of Technology, 
Germany. Prior to joining KIT, he has been Assistant Professor at the University Stuttgart, and post-doctoral 
fellow at REVES/INRIA Sophia-Antipolis, France. He received a PhD from the University of Erlangen, Germany. 
His research in­cludes real-time computer graphics, (interactive) global illumination, GPU techniques, 
and visualization. He has published several articles at various conferences including SIGGRAPH and Eurographics. 
Carsten has been a tutorial speaker at SIGGRAPH, Eurographics, and the Game Developers Conference. Alexander 
Keller NVIDIA ARC GmbH keller.alexander@gmail.com Alexander Keller is a member of NVIDIA Research and 
leads advanced rendering research at NVIDIA ARC GmbH, Berlin. Before, he had been the Chief Scientist 
of mental images and had been responsible for research and the conception of future products and strategies 
including the design of the iray renderer. Prior to industry, he worked as a full professor for computer 
graphics and scienti.c computing at Ulm University, where he co-founded the UZWR (Ulmer Zentrum fur wissenschaftliches 
Rechnen). Alexander Keller holds a PhD in computer science, authored more than 21 patents, and published 
more than 40 papers mainly in the area of quasi-Monte Carlo methods and photorealistic image synthesis. 
Bruce Walter Cornell University bjw@graphics.cornell.edu Bruce Walter is a Research Associate at the 
Cornell Program of Computer Graphics. His current research interests are expanding the capabilities of 
physically-based ren­dering and global illumination algorithms with respect to robustness, scalability, 
and generality. He has published many related research papers at SIGGRAPH and else­where. He also served 
a project lead for trueSpace product at Caligari, and was a post-doc in iMAGIS laboratory in Grenoble, 
France, and earned a Ph.D. from Cornell and a B.A. from Williams college. Introduction  Jaroslav K.anek 
 riv´ This"course"covers a"group"of"global"illumination"algorithms known"as""many.light" methods","or""VPL.rendering"methods".""(VPL"="virtual"point"light) 
 Global"illumination"(GI)"is"indeed"one"of"the"most"important"aspects"of realistic"rendering" as"you"can"see"in"this"example."Global"illumination"fills"shadows"with"natural"light"and"adds" 
illumination"gradients"around"geometry"features,"that"the"human"eye"is"used"to"seeing"in" reality. Gl"is"extensively"used"in"architectural"visualization,"product"design,"and"in"the"movie" 
industry. Global"illumination"is"due"to"multiple"inter.reflections"of"lights, as"shown"in"the"example" 
above." Rendering"of"realistic"images"with"global"illumination"then"involves"simulating"these"inter. 
reflections. The"many.light"formulation,"covered"in"our"course,"provides"a"particularly"efficient" approach"to"simulating"the"light"inter.reflections."" 
These"methods"originate"from"the"Instant"Radiosity algorithm"proposed"by"Alexander" Keller."The"main"idea"is"to"approximate"indirect"illumination"by"a"number"so"called"Virtual 
Point"Lights,"or"VPLs." A"basic"VPL"rendering"algorithm"works"as"follows:""In"the"first"step,"the"VPLs"are"distributed" 
on"scene"surfaces"by"tracing"particles"from"light"sources."In"the"second"step,"the"color"of"a" pixel 
is computed by summing the contributions from all the VPLs to the surface point(s)ixl"isomputed"by"summing"the"conibutios"frm"all"the"VPLs"to"the"surfe"poins)" 
pe"ctrnoact(visible"through"that"pixel. In"other"words,"the"problem"of"computing"indirect"illumination"has"been"reduced"to"the" 
computation"of"direct"illumination"from"many.lights,"hence"the"name"of"the"methods. One"of"the"most"important"advantages"of"the"many.light"formulation"is"that"it"unifies"the" 
rendering"problem"to"computing"direct"illumination"form"a"(potentially"large)"number"of" lights,"the"VPLs. 
Indeed,"large"area"lights,"environment"maps,"and"indirect"illumination"are"seamlessly handled"in"the"same"way."All"that"is"needed"is"to"convert"the"illumination"to"a"set"of"VPLs. 
 The popularity"of"many.light"methods"is"also"due"to"their"wide"applicability,"from" approximate"interactive"rendering"to"high.fidelity"offline"rendering."This"is"due"to"the"fact" 
that"even"with"a"limited"number"of"VPLs,"the"generated"images,"though"incorrect,""provide" a"visually"plausible"approximation"of"the"correct"solution. 
 The"main"technical"issues that"need"to"be"addressed"when"using"many.light"methods"in" practice"are"listed"on"the"slide. 
First,"as"usual,"we"want"to"make"the"rendering"fast."This"involves"especially"accelerating"the" visibility"tests"required"to"compute"illumination"from"VPLs."Such"improvements"lead"to"a" 
constant"factor"speed.up. However,"the"many.light"methods"lend"themselves"to"asymptotic"speed"improvements,"i.e." 
the"rendering"time"can"grow"much"more"slowly"than"the"number"of"VPLs."A"prime"example" ofthis approach 
is the "this"apprach"is"theLightcuts ts algorithm algorithm. ofo"Lightcu Finally,"in"the"basic"form,"many.light"methods"suffer"from"some"approximations."Special" 
care"is"needed"to"make"these"methods"applicable"in"high.fidelity"rendering"applications. The"goal"of"our 
course"is"to"cover"all"of"these"issues.  The"material"will"be"presented"by"researchers who"were"originally"involved"with"the" 
development"of"the"various"many.light"methods. Instant Radiosity -principles and practice Alexander 
Keller Slides for this part of the course are provided in the .nal version available from http://cgg.mff.cuni.cz/ 
jaroslav/papers/mlcourse2012. Handling dif.cult paths  Milo.s Ha.riv´ san, Jaroslav K.anek  In this 
part of the course, I will discuss virtual spherical lights, a technique that can reduce the energy loss 
problems encountered with standard formulatons of virtual point lights.  One important applicaton is 
architectural or industrial previews. They ofen contain a signifcant fractons of glossy materials, which 
create interrefectons that cannot be neglected. These are somewhat extreme examples: their appearance 
is completely dominated by glossy inter-refectons. But they do present nice examples of scenes that will 
bring classic many-light algorithms to their knees. So let's say we run standard instant radiosity on 
this glossy Cornell box. If we do it naively, we quickly fnd that we have to play tricks which will remove 
most of the interestng interrefecton efects. These tricks are: consider only difuse VPLs and apply plenty 
of clamping. Let's look at these in more detail. Here is what a VPL emission functon should look like 
to give unbiased results: it is simply a combinaton of a difuse and glossy BRDF lobe corresponding to 
the incoming directon of the VPL, multplied by the cosine term. We can easily see the issue -if the surface 
is highly glossy, the BRDF lobe will functon as a )laser" light that will create unacceptable spikes 
in random places around the scene. This problem is usually avoided by not including the glossy contributon 
of the VPL, which is clearly suboptmal. However, this will not be sufcient. As we move the VPL locaton 
p and the shading point x closer and closer to the corner, the VPL contributon will go to infnity. This 
problem will be even worse if the BRDF at x is also glossy (remember, we only removed the glossy BRDF 
from the VPL locaton p). The common soluton to this is to clamp the VPL contributon to a user-specifed 
maximum value. This not only removes illuminaton, but introduces another tricky parameter to the user, 
which is difcult to set autmatcally. These two approximatons cause a large chunk of the indirect illuminaton 
to be missing. Clearly, neither clamping and using difuse-only VPLs can be neglected in our search for 
a be.er soluton. One may think that this problem only occurs in a contrived scene such as the Cornell 
box, where we made all surfaces glossy. This is not the case: in this kitchen scene, instant radiosity 
some important parts of light transport, which results in serious illuminaton loss on glossy surfaces, 
as you can see on the range hood or the counter. The illuminaton loss problem hasn't been extensively 
discussed in previous work with the excepton of the paper by Kollig and Keller, who propose to compensate 
for the missing energy by path tracing. Unfortunately, in glossy scenes, their compensaton methods can 
be nearly as expensive as path tracing the entre image. Our idea, on the other hand, is to prevent the 
illuminaton loss in the frst place, rather than trying to compensate for it. We achieve this by introducing 
a new type of light, the VSL, that overcomes some of the problems of VPLs. With this new type of light, 
we are able to render images very similar to the reference yet in much shorter tme. More specifcally, 
we well spread the light energy over the surfaces inside the sphere of radius r centered at the light 
positon p. And the contributon of the light will be computed as an integral over the solid angle subtended 
by the sphere. This can be seen as an analogy to photon mapping: A photon contributes to all surfaces 
within radius r, and an additonal fnal gather operator uses the "spla.ed" illuminaton to light other 
surfaces. Let's write down the formula for the contributon of such a light to the surface point x. We 
have the integraton over the solid angle. The integrand is a product of the following terms: the cosine 
weighted BRDF at the surface, next, the BRDF at the point y n the vicinity of the light locaton. Finally, 
we have an indicator term that is zero for all the directons that correspond to surface point y outside 
the sphere. We normalize the integraton by the expected surface area inside the sphere, pi*r"2, and multply 
by the light fux. To avoid this indicator term, we could defne the light contributon as an integral over 
a disk area. Unfortunately, doing that re-introduces the infamous 1/dist"2 term and produces bad results 
(we tried it). Unfortunately, this formulaton requires fnding the point y for all directons l inside 
the cone, which requires ray tracing. This is clearly not feasible. To produce a computatonally convenient 
approximaton to the previous formula, we make the following simplifying assumptons. We assume that the 
visibility, the surface normal and the BRDF are constant inside the sphere. And we take them from the 
light locaton p. With these assumptons, we can write a formula for the contributon of a VSL. The indicator 
functon on the right will be approximated by a cosine term -this approximaton tends to be correct as 
the distance between the light and the shading point increases. .ow we have arrived at a nice, clean 
formulaton of the VSL contributon: it becomes an integral over the conical solid angle given by the sphere 
radius, and the integrand will be the product of two BRDFs and two cosine terms: one each for the receiver 
and the light locaton. This formulaton is very practcal -no ray-tracing is necessary to compute the 
contributon, and all necessary parameters are local to either the shading point or the light. It stll 
does require Monte Carlo integraton, but this is purely numerical and can be done in a shader. We can 
use shadow maps for visibility if desired, as usual. As I mentoned, we can use stratfed Monte Carlo 
to compute the VSL integral. One possible issue, in case one or both of the BRDFs involved have a glossy 
component, is that uniform sampling of the solid angle cone will not be well adapted to the integrand 
and lead to noise. However, we can use multple importance sampling -a variaton of the same technique 
used in bidirectonal path tracing. In additon to cone sampling, we can importance-sample either of the 
two BRDFs, and combine these estmators using the classic balance heuristc. .ou can fnd the shader that 
computes the VSL integral online. Our implementaton uses the row-column sampling technique to reduce 
the number of VSLs, which I will also describe later, with shadow mapping for visibility. However, the 
VSL can be included into any many-light renderer, even lightcuts, as Adam Arbree will describe later. 
We generate about 2..,... VSLs and reduce them to 1.,... using MRCS. These numbers are higher than in 
difuse scenes -the VSL does not magically make glossy scenes as esy as difuse, but does make them tractable 
for many-light approaches. An important detail is se.ng the VSL radius: we fnd the k nearest virtual 
lights (say 1.) and set the radius as a multple of that. This makes the radii larger in areas where lights 
are sparse. Here's an example of the results we can get with VSLs. This scene is lit by mostly indirect 
light, through refecton from the shiny metallic surface on the right. There are several other highly 
glossy and anisotropic materials. The image computed using the classic approach of clamping and difuse 
VPLs looks clean but obviously dark, with some metals close to black. In constrast, the VSL image is 
quite close to the path-traced ground truth, with a bit of blurring. A similar result arises in this 
model of the Disney concert hall, lit by a sun-sky model. The walls of the building are purely glossy 
with no difuse component: the blue color comes from the sky and the brown is a refecton of the ground. 
Obviously, using difuse VPLs will not capture any illuminaton of the walls; on the other hand, with VSLs 
the image looks quite close to the reference, again with some blurring. This tableau consists of an 
anisotropic metallic plane with some ob.ects, and 3 strong directonal lights. This is a really bad case 
for clamping, which removes much of the refecton. On the other hand, VSLs capture it nicely, only slightly 
blurred. From the results it is apparent that the main limitaton of VSLs is bias in the form of blurring. 
However, this is a very predictable efect, and in many cases may be acceptable. It is also consistent 
similar to photon mapping -results get more correct as VSL number increases (and therefore VSL radii 
decrease). .aroslav will later decribe the local light technique, which improves upon this limitaton 
of VSLs, at the cost of somewhat higher complexity. In this part of the course, I will discuss various 
approaches for generatng VPLs where they are most needed for a given camera view. Let us start by reviewing 
the classic VPL rendering algorithm, instant radiosity. In the frst step, the VPLs are distributed on 
scene surfaces by tracing partcles from light sources. In the second step, the image is rendered by summing 
contributons from all the VPLs. In this part of the course, I will focus on various approaches to distributng 
the VPLs.  The need to develop alternate VPL distributon approaches follow from the fact that with the 
basic VPL tracing algorithm, the VPLs may end up in regions where they do not contribute signifcantly 
to the image. This will be the case especially in large environments where the camera is looking at a 
small porton of the scene. In additon, VPL distributons generated by the basic VPL tracing algorithm 
cannot be used to render local light inter­refectons. Here is an example of the inability of VPLs to 
reproduce local light inter-refectons. The number of VPLs along the edges is insufcient to render the 
local inter-refectons, resultng in artfacts in the form of light splotches. The usual way of dealing 
with these artfacts the clamping that we discussed previously, where we clamp limit the contributon of 
a single VPL to a prescribed maximum value. But this selectve energy removal can severely change material 
appearance, as you can see in the image on the right. A be.er soluton would be to ensure that more VPLs 
are generated in the visible areas along the edges.  So, the purpose is to get the VPLs where they are 
most needed. There has been a number of approaches proposed in the literature for this purpose and I 
will discuss some of them in the remaining part of my contributon. The simplest approach is to apply 
re.ecton sampling, where VPLs that do not signifcantly contribute to the image are re.ected. Second, 
we can use a more advanced sampling algorithm such as Metropolis sampling. And fnally, we can distribute 
the VPL by tracing paths from the camera instead of from light sources.  A form of re.ecton sampling 
is used for VPL distributon in the Autodesk 36. Rendering soluton that will be later described by Adam 
Arbree. So I will only briefy menton the approach presented by .eorgiev et al. in their .. 2.1. short 
paper.  The main idea is very simple: They use the exact same VPL tracing algorithm as in instant radiosity 
but they probabilistcally re.ect the VPLs if their image contributon is less than an average.  So our 
goal is to end up with N VPLs, each having some "average" total contributon to the image IYLv /N. The 
algorithm generates one VPL at a time. For each candidate VPL generated by the original VPL tracing algorithm, 
the total image contribution of the VPL, Phii is estimated. Then the VPL is accepted with a probability 
given by the ratio of the VPL contribution to the target contribution. If accepted, the VPL energy is 
boosted by dividing by the acceptance probability: Russian roulette in action. So how do we estmate 
the target "average" VPL contributon. A simple way to do it is to run a number of pilot VPLs and render 
a low-resoluton image. Another possibility is to use informaton form the previous frame, if rendering 
an animaton. To estmate the image contributon of a candidate VPL, we simply render a "low-res" image 
by picking only a couple of pixels. There is no need to be very precise -it's no use to spend much tme 
on estmatng the VPL contributons. The algorithm will produce correct results no ma.er how accurately 
the VPL contributon is estmated.   To conclude, VPL re.ecton sampling is cheap and simple and can help 
a lot. There's really no reason for not using it, especially in mostly difuse scenes. The problem is 
that it makes the "one-pixel image" assumpton -it will not help us to resolve the local inter-refecton 
problem.   Another opton is to distribute the VPLs by tracing paths from the camera instead of from 
the light sources. This approach is bound to produce VPLs in locatons important for the image to be rendered, 
but it also has some issues: First, we need to explicitly connect these VPLs to the light sources so 
that they can form complete light transport paths. Second, computng the VPL intensity involves the evaluaton 
of the probability density of generatng the partcular VPL positon. The probability calculaton is signifcantly 
more complex (and costly) when the VPLs are generated from the camera. On the following slides, I will 
discuss the method proposed by Davidovi. et al. in our SI..RAPH Asia 2.1. paper. We use the idea of 
separatng the light transport into the clamped, global component, and the local component, as previously 
discussed by Alexander and Milos. The global component accounts for the long-distance light transfer, 
while the local component corresponds to the short-range inter-refectons, and indirect glossy highlights. 
We take advantage of the specifc structure of each of the two components to design a soluton for each 
of them that is substantally more efcient than a general .I soluton. Specifcally, we handle as much energy 
as possible in the global component which leaves only the local inter-refectons for the local component, 
which we handle by the so called local VPLs that are distributed by tracing paths from the camera. Let 
us start with the overview of the standard Kollig . Keller compensaton. They frst trace a ray from the 
hit point, and connect the target to global light to obtain the intensity. This path tracing result is 
then used in place of the clamped away energy. This is a lot of efort for compensatng .ust a single pixel. 
 Our idea is to create a VPL at the intersecton of the camera path. This way, the cost of tracing that 
path is amortzed by le.ng it contribute not .ust to the pixel that generated the VPL, but also to its 
neighboring pixels. With this basic idea, let us take a closer look at what is necessary to actually 
make it work.  The frst important thing we have to compute is the probability density of the generated 
local VPL. We cannot simply use the probability with which it has been generated, but we have to sum 
the probabilites over all the pixels in the tle it contributes to. The reason is that all the neighboring 
pixel could potentally have generated the VPL at this partcular locaton. The second important thing is 
that if we used a fxed tle grid, the boundaries would be fairly visible. To break this coherence, we 
.i.er the tles for each VPLs. .ow we come to the part that would simply not be possible without spli.ng 
the light transport. To compute the full probability density for our light, we would normally need to 
compute visibility to all pixels, which would be prohibitvely expensive. However, we do have the global 
component that contains most of the energy and handles most of the shadows. The local lights can then 
have their visibility approximated because they only handle local inter-refectons. We approximate it 
by .ust one visibility sample, which is actually the ray that generated the light in the frst place. 
 So the key insight here, the light transport split made tle visibility approximaton possible. The second 
thing the split allows us to do is re.ectng lights. .ot all local lights actually have contributon to 
the tle they belong to, and we can fat out re.ect these, giving us another 2-4x speedup.  So, for the 
overview of the whole process. We generate local lights Re.ect lights with zero contributon Connect the 
surviving local lights to global lights Have them contribute to a tle And afer repeatng this about 2. 
million tmes, we get the fnal local soluton. .ow we have the result of the local soluton. We simply 
add the result of the global soluton and obtain the fnal indirect illuminaton soluton. The global soluton 
can be computed by any VPL method, for example Lightcuts. In the original paper we used used a visibility 
clustering algorithm.  Here we see that the local lights nicely capture this highlight from metal stool 
leg or the refecton of the paper towel on the metal back wall, something that cannot be done with the 
"globally distributed" VSLs. However, this scene also shows some of the limitatons the method has. There 
is a loss of defniton of the shadows behind the bo.les. This is caused by the fact that the local lights 
on the ke.le in the front contain too much energy. Pushing more energy into the global component could 
resolve this problem. One detail we did not menton is that in some of the scenes we stll need slight 
clamping even on the Local VPLs, causing some darkening here. This can be solved by interpretng the Local 
VPLs as Local VSLs.  To conclude, distributng VPLs by tracing paths from the camera is very useful for 
resolving local inter-refectons. They are best used in con.uncton with a separate "global" soluton which 
can take care of the smooth, long distance light transport in the scene.  Scalability with many lights 
II Milo.s Ha.san  In this part of the course, I will talk about several techniques that improve the 
scalability of many-light methods with the number of virtual lights. In other words we have reduced our 
rendering problem to computing the connections between many VPLs and many receivers, or gather points 
. Of course, one way to do it are the lightcuts algorithms that Bruce described, which are very reliable 
ad high-quality. I will introduce alternative techniques like row-column sampling and visibility clustering, 
which can have some other advantages. For example, one may be able ot use shadow maps instead of ray 
casting for visibility checking, which tends to be faster in most cases. No bounds on shders are required, 
which can lead to simpler implementations. There may also be advantages in highly occluded environments, 
where lightcuts will conservatively evaluate illumination assuming full visibility. To get started in 
describing the algorithms, we first need to interpret the many light problem as a matrix of light-pixel 
interactions. This means that each element of the matrix is the contribution of a single light to a single 
pixel. So, the columns of the matrix are really images rendered with a single point light. The rows represent 
contributions of all the lights to a particular pixel. In this setting, the ideal image we would like 
to render is equal to the sum of the columns of the matrix. Or, to put it differently, the color of 
each pixel is equal to the sum of the matrix row corresponding to that pixel. It is important to note 
that we re not given the matrix data, we just have an oracle a function that can evaluate the elements 
on demand. Our goal is to compute the sum without evaluating most of the elements. How is this even possible? 
 The trick is that the matrix is highly structured. Here is an example with a Cornell box. with a single 
direct light, and Numerically, the matrix often close to low-rank: its columns can often be approximated 
by linear combinations of other columns. Therefore, we can get away with computing only a very small 
subset of the elements, and still gather enough information to render an accurate image. Of course, 
the low ran assumption is not always valid here is an example of a really bad case, where the light 
s contributions are linearly independent. Fortunately, this does not usually happen in practice. So, 
we want to sample a subset of the matrix elements, but which ones should we choose? If we sample complete 
rows and columns, we can use GPU shadow mapping as our visibility algorithm. This way we can compute 
elements at very high rate. Futhermore, it is easier to reason about rows and columns, so even if we 
use a ray-tracer, it may still be an advantage to sample like this. One may wonder how to compute the 
row contributions using shadow mapping. After all, shadows normally come from the light! However, shadow 
mapping is simply a way to determine the visibility from a point to many other points at once. We can 
compute a cube shadow map at the VPL position, and determine the visibility of the surface samples, as 
usual. Or, we can compute the cube at the receiver position, and query the light positions against it. 
 OK, we know how to compute rows and columns. How do we design the rest of the algorithm based on them? 
We use an idea that combines exploration and exploitation. As a first step, we explore the structure 
of the matrix by computing a small, randomly chosen subset of rows. Next, we analyze the gathered information, 
and decide which columns to choose and their appropriate weights. The exploitation step then computes 
the selected columns, which are finally accumulated into an image. The only thing missing I the contents 
of the black box that analyzes the rows and chooses the columns. To understand this, let s take the 
rows that we computed in the exploration stage, And assemble them into this long, but not very tall 
reduced matrix. Now let s flip attention to the columns of this matrix, which we ll call reduced columns. 
 These can in fact be thought of as tiny images that are sub-sampled versions of the full columns.  
We will use a clustering approach to choosing columns: we cluster similar reduced columns, and we pick 
a representative in each cluster. Of course, we can arbitrarily re-shuffle the columns there s no such 
thing as neighboring columns . Note that this is actually an unbiased Monte Carlo estimator: each representative, 
if properly weighted, computed an inbiased estimate of its own cluster, and The accuracy of the algorithm 
is highly dependent on the quality of the clustering, so we should design it carefully. First, let s 
think about the reduced columns as vectors in a high-dimensional space. We re going to visualize these 
high-dimensional vectors as circles. The radius of each circle will correspond to the norm of the reduced 
column, or equivalently, to the brightness of the little image. The positions will correspond to the 
positions of normalized reduced columns in the high-dimensional space. With a bit of simplification, 
we can say that circles that are close to each other represent similar lights, and large circles represent 
lights with strong intensity.  We can prove that the following formula gives the optimal clustering 
that minimizes the expected error. So let s look at its meaning. We re minimizing the sum of costs of 
all clusters, Where the cost of a cluster is defined as the sum over all pairs of elements in the cluster 
of the product of norms and squared distance. This confirms the intuition that strong lights, or lights 
that are far from each other, should be in separate clusters. This is an NP-hard problem, but a good 
approximation can be found by the following divide &#38; conquer algorithm. We pick a plane with a random 
orientation. Remember this is in many dimensions, here I ll just visualize it as a line. Then we move 
the plane to its optimal position and split the points into two clusters. There are only n possibilities 
so we can check them all to find the best one. We then continue this process recursively on the cluster 
with the currently highest cost.  To summarize, here s the full algorithm: In the exploration stage, 
we evaluate some rows on the GPU, then focus on the reduced columns. We obtain a clustering through the 
techniques I just described, then pick representatives. Finally, we accumulate the corresponding columns 
with the correct weights into an image. Let s not lose the big picture: all of this is an abstraction 
that lets us increase the scalability of an underlying many-light algorithm. The temple is quite a large 
scene, not only in terms of the number of polygons, but also its spatial extent. In fact, only a tiny 
portion of the scene is in view. Most of the illumination in the scene is indirect or sky illumination, 
with the only pixels lit directly by the sun form the small bright patches on the right. The method can 
quite effectively pick the small subset of VPLs that captures the illumination. Many VPLs are either 
invisible or have a small contribution, and can be aggressively clustered. These scenes are designed 
to test the algorithm by complex incoherent geometry, and it works quite welll. Approaches based on interpolating 
illumination across coherent surfaces would have a difficult time rendering these images. However, our 
algorithm makes no assumptions about image-space coherence. In this sense, low-rank light transport can 
be a better approximation than smooth, low-frequency irradiance. This scene, the Grand Central station, 
is a bit of a difficult case. A unique feature of thescene are the omni-directional lights positioned 
in small recesses between stone blocks. These lights pose a problem since they violate the low-rank assumption. 
The columns corresponding to these lights are pretty much linearly independent. Therefore, a larger number 
of rows and columns will be required here, and some lights are still missing on the left side. An important 
question is whether the row sampling step of our algorithm is more useful than brute force. Indeed, instead 
of creating 100,000 lights and trying to pick a small subset of them, we could simply create a smaller 
number of lights and render them all. However, if we compare these images, we find that the simpler approach 
produces much more objectionable artifacts despite taking a bit longer. Here are difference images as 
well.  Let s now turn attention to extending row-column sampling to animation. Here is what we get by 
running row-column sampling per frame with 100 rows and 200 columns. There is noticeable flicker, which 
is not a surprise given that MRCS is a randomized algorithm with no knowledge of the time dimension. 
We could fix this by increasing the row and column numbers, but it would be nice to find an algorithm 
that takes advantage of temporal coherence. In row-column sampling, we had a different matrix in every 
frame. Let s concatenate them, and consider all of them at once as one large 3D array (or tensor) of 
lights contributing to pixels over frames. The amount of data is very large, and we need to avoid constructing 
the whole tensor at any point in the algorithm, similar to the matrix case.  The extension will be done 
as follows: We compute a set of rows in every frame, resulting in a set of horizontal slices of the tensor. 
Then we cluster the columns of this reduced tensor; I call this a rectangular clustering since every 
cluster is a cartesian product of a light subset and a frame subset. We can then use a similar representative 
selection and reconstruction as before, except here we have to be careful, because pixels move between 
frames. A pixel mapping trick similar to optical can be used to warp the representatives to avoid this 
problem. How do we find a rectangular clustering? This problem is again NP-hard, but we can adapt the 
divide-and-conquer as follows. We will start with all light-frame pairs in one cluster, and keep splitting 
the cluster with highest cost until we reach the desired number of clusters. We will try splitting the 
cluster in time and in lights, and pick the split that gives us the better objective function value. 
Now the question is, how to split in time and in lights; we can answer this by trying both splits and 
seeing which one decreases the objective function more.  Here is an example animation rendered with 
this method. On the left is the result, and on the right is reference with all VPLs, which took about 
9 times longer. And here is another example with a temple and a moving sun. This one is about 77x faster 
than the brute-force solution. Next, I will introduce an interesting variation of the row-column approach 
that is some cases more robust. The design of the algorithm is motivated by several observation on shading 
and visibility in glossy scenes. First, we observe that in glossy scenes we cannot easily pick a small 
subset lights to approximate the solution, because shading from the individual lights is quite different. 
In other words, we want to compute the shading from all the global VPLs. And that s actually doable because 
the GPU is extremely efficient at computing the shading. But we just cannot afford to evaluate a SM for 
each of those 200k lights! So the key insight is to separate shading from visibility, use ALL the light 
for shading, and only a small number of representative lights for visibility. Here s the overview of 
visibilty clustering. We start by distributing the global VPLs in the scene by tracing particles form 
light sources. After that, we sample a number of rows of the interaction matrix, which means that we 
pick a number of pixels and for each of them, we evaluate the shading and visibility for all the lights. 
That gives us the reduced shading and visibility matrices. The visibility clustering then analyzes these 
matrices and yields clusters of lights that will share the same shadow map. After that, we render all 
the VPLs with the shadow map of the representative light for each cluster. This yields the complete global 
solution. There s only one bit that remains to be explained here, and that s the visibility clustering 
algorithm. The goal of the visibility clustering algorithm is to group VPLs into clusters that will 
share the shadow map of its representative VPL. We use a data-driven approach where we analyze the row 
samples from the light interaction matrix. The clustering algorithms proceeds in a top-down fashion, 
splitting clusters with highest cost, which is defined as the L2 error of the matrix incurred by the 
visibility approximation. Here s a comparison of the result of our visibility clustering with MRCS in 
the same time for a simple scene of the happy Buddha statue reflected in a glossy plane. The MRCS result 
uses 10k representative lights (i.e. 10k shadow maps and 10k lights for shading), which leaves visible 
splotches in the image. The visibility clustering, in the other hand, uses only 5k shadow maps for visibility 
but all the 200k lights for shading, substantially improving the smoothness of the reflections. Another 
nice idea, published recently in the LightSlice paper, is to refine the original clustering within slices 
of the image. One problem is that each slice will only have one row sample, and one-dimensional data 
is not enough to run a clustering algorithm. This is addressed by using neighboring row samples to help. 
 This works very well - in fact, on this fairly complex scene, the algorithm seemsto outperform both 
lightcuts and row-column sampling. Another alternative approach I should mention uses no matrix formulations, 
but instead uses a simple visibility clustering idea that nevertheless works fine in simple scenes and 
can even achieve real-time performance. The idea is to use k-means clustering for visibility, and render 
full shading. One may think that this for highly variable VPL intensities, but for a single bounce, and 
one lightsource, one can easily make all VPLs same intensity. The approach also uses soft shadow maps, 
so it can get away with pretty small numbers of shadow maps. How to pick the number of rows and columns 
automatically? It would be nice to design a variation of the algorithm that alternates row and column 
sampling, since knowing some columns might tell us which rows to sample, but no one seeknows how to do 
it. It might also be useful to make the algorithm progressive, so the user can stop the evaluation when 
they are satisfied with the image. We are also interested in developing a temporal version of the technique 
that renders multiple frames at once. The problem is how to do it so that the representatives do not 
have to be kept in memory? Finally, there are some other approaches for matrix completion in the literature, 
but we tried them for rendering and they did not turn out to be better. However, there might be ways 
to improve them, they just haven t been studied systematically. Real-time many-light rendering Carsten 
Dachsbacher  let's"first"start with"single.bounce"indirect"illumination,"which"essentially"means"placing" 
VPLs"only"at"surfaces"directly"visible"from"the"light"sources these"surfaces"can"simply"be"rasterized,"actually"these"are"the"surfaces"one"would"store" 
in"a"shadow"map In"addition"to"the"depth"value"such"an"extended"shadow"map"-reflective"shadow"map". 
stores"the"world"space"position,"the"surface"normal"and"the"reflected"radiant"flux"for" every"pixel." 
The"world"space"position"could"be"reconstructed"from"light"source"positioning"and"the" depth"value". 
storing"it"is"just"a"trade.off"between"computation"overhead"and"memory" consumption. So"with"this"data"we"know"everything"about"the"surface"to"compute"single.bounce" 
indirect"lighting. Now"the"flux"defines"the"brightness"and"color"of"the"surface"and"the"normal"its"spatial" 
emission"characteristics. The"total"indirect"irradiance"at"a"surface"point"could"be"approximated"by"summing"up" 
the"illumination"due"to"all"pixel"lights. If we do"not"do"any additional"work,"and just"use the RSM"as 
is,"we cannot consider occlusion for the indirect light"sources and e.g."the floor is wrongly lit on"the 
left side of the box. The"classic"RSM"method performs a"gathering approach computing the sum of pixel 
light" contributions for each pixel in"the image by sampling the extended shadow map. As the"number"of"pixels"is"typically"too"large,"we"reduce"the"sum"to"a"restricted"number" 
of"a"few"hundred"samples,"concentrating"on"the"most"relevant"pixel"lights. In"general those pixel lights 
are relevant"which are close in"world space and of course they are also"close in"the shadow map projection 
-in"reverse this is of course not"true. .or this,"we precompute a"sampling pattern for selecting pixel 
lights which gets centered around the projection of the surface point to be lit. By this,"we reduce the 
number of samples to a"few hundred per"pixel. But"we"obviously"we"are"lacking"two"aspects:"mutliple 
bounces"of"indirect"light"and" shadowing of"indirect"light. The"first"problem"can"be"dealt"with"relatively"easily:"as"in"standard"random"walk. 
 we can"pick"a"random"pixel,"which"maps"to"a"direction,"in"every"such"RSM,"take"the" surface"location"and"orientation"from"there,"and"recursively"continue"rendering"RSMs. 
 Visibility"is"a"problem,"as"we"typically"have"a"large"number"of"VPLs"and"thus"shadow" maps.".ven"in"diffuse"scenes,".OOO"or"more"VPLs"is"common. 
 In"fact,"this"shadow"map"generation"is"the"bottleneck: Assuming"we"use".O.4"VPLs"and"a".OOk"triangle"3d.model. 
.irst,"vertex"processing"requires"to"do"3OOM"vertex"transforms"when"drawing"all" triangles"into"all"shadow"maps. 
Second,"drawing".OOk"tris"into"a".k"texel"depth"map"is"a"hundertfold overdraw. Let's"find"something"better". 
 What"we"propose"as"a"solution"is"based"on"the"observation,"that"low"quality"depth" maps"are"sufficient. 
This"is,"because"the"individual"contribution of"every"VPL"is only"small. Also"indirect"lighting"varies"smoothly 
in"most"scenes"or"at"least"in"mostly"diffuse"scenes. Our"contribution"is"to"allow"imperfection"when"creating"a"depth"maps"that"allow"for"a" 
much"more"efficient"generation. And"this"is"achieved"by"using"a"point.based"rendering"approach"which"is"typically"well" 
suited"when"you"need"LOD"and"approximate"rendering. The"main"steps"that"I"will"outline"here"are: .. 
Point.based"depth"map"generation .. A"pull.push"operation"to"fill"holes"from"point"rendering  So"the"goal"is"to"generate"as"many"depth"maps"as"possible,"but"each"of"them"is 
allowed" to"be"of"a"low"resolution,"say"3.x3."pixels. .sing"classic"depth"maps"for"this,"takes"hundreds"of"milliseconds"for"the"Sponza 
scene. We"use"point"rendering,"because"point representations"decouple"from"the"input" geometry,"and"they"can"easily"rendered"into"a"single"render"target"for"multiple"views. 
Also"LOD"for"points"is"very"simpler,"because"they"don't"require"connectivity. .3  .ere"you"can"see"a"comparison. 
 .4  As"surface.VPLs"emit"into"the"positive"hemisphere, we"chose"a"paraboloid parameterization"of"directions." 
Points"fir"this"well,"as"lines"map"to"curves"when"using"this"parameterization"which"can" cause"triangle"meshes"to"be"problematic. 
.urther"benefits of"paraboloid"shadow"maps"are:"single.pass.projection"for"hemisphere," relatively"low"distortion. 
 The"point.representation"is"precomputed, each"VPL"has"its"own"randomly"created"point" set"(to"avoid"banding)"of"about"8k"points. 
The"image"shows"one"VPL"and"its"point"set. .6  Instead"of"storing"an"x.y.z". position"for"every"point,"we"store"it"as"a"generalized" 
barycentric coordinate"instead,"that"is:"a"triangle"index"and"a"barycentric oord inside"this" triangle. 
By"doing"so,"the"points"can"naturally"follow"the"deformation"of"the"mesh"the" approximate.  While"this"is"a"classic"depth 
map,"drawing""a"low"number"of"points"leads"to"holes,"such" as"in"this"example. We"fill"those"holes"using"a"pull.push"step,"similar"to".rossman.Dally,"to"fill"holes. 
To"this"end,"we"build"a"pyramid"of"depth"values,"where"we"average"only"valid"depth" values"in"a"pull"step. 
In"the"subsequent"push"step,"every"undefined"pixel"a"every"level,"is"replaced"by"the" average"of"the"defined"pixels. 
 .8  .ere"we"show the"imperfect"shadow"of"an"individual"VPL. A depth"map"without"pull.push"will"have"light"leaks,"that"are"fixed 
by"pull.push. .owever,"pull.push"sometimes"miss.classifies"depth"values,"and"gives"false"positive or" 
false"negatives. Small"holes"might"be""real""holes"and"small"objects"could"be"isolated,"pixel.sized"floating" 
objects. If"such"miss.classification"happens,"the"only"negative"result"is,"that"such"objects"will"not" 
cast"shadow"or"let"light"through,"which"is"not"very"important,"because"they"are"small;" we"have"many"VPLs; 
and"such"errors"are"uncorrelated.  We do"this"pull.push"step"on"all"depth"maps"in"parallel. As"we"work"in"depth"map"space"now,"we"are"now"independent"of"the"geometric" 
complexity:"It"is"irrelevant"if"this"is"an"image"of"a"Cornell"Box"or"an"..."dragon:"it's"just" an"image. 
.O  Let"me"now"present"some"of"our"results,"which"range"from"diffuse"bounces"in"a"Cornell" box"to complex"scenes,"including"multiple"bounces,"arbitrary"local"area"lights,"natural" 
illumination"to"caustics. .ere"are"animated"meshes"inside"the"Cornell"box with"a"dynamic"direct"light. 
Most of"the"light"in"this"scene"is"indirect. Note,"how"the"animals"feet"cast"high.frequency"shadows,"whereas"the"animal"itself"casts" 
a"correct"soft"shadow. Also"note,"the"subtle"variations"in"shadow"color. Despite the"fundamental"changes,"in"indirect"lighting"there"is"no"flickering. 
 This"is"a"more"complex"scene,"where"a"cloth deformation"is"placed"inside"the"well" known"Sponza"model. 
To"achieve"sufficient"temporal"coherence,"we"need"...."VPLs in"this"example. Note,"how"the"bounced"light"color"changes"drastically 
when"the"cloth"is"moving. Note,"the"indirect"shadow from"the"columns. .3  In this"example,"we"did"not"use"global"illumination,"but"direct"natural"illumination"from" 
an"environment"map. Distant"light"sources"are"placed"on"an"environment"map,"with"an"orthographic"shadow" 
for"each. .4  In"a"similar"way,"we"can"generate"local"soft"shadows"from"complex"area"lights, with" varying"color. 
We discretize the"area"light"into"several"point"lights"and"compute"and"ISMs"for"each.  .inally,"we"are"not"limited"to"Lambertian 
VPLs. .ere,"we"use"a"Phong distribution, to"compute"one"glossy"bounce"with"full"indirect" visibility"which"is"usually"ignored. 
.6  ISMs"are based on"quickly creating shadow maps from a"point.based representation,"but" point.based 
rendering is not"necessarily of low quality or exhibiting holes. Typically points are organized in"a"hierarchy 
to allow for choosing the required level of detail on"the fly."The"classic"example is .splats where the 
refinement criterion is the size of a"point primitive"on"the screen. The"idea of micro.rendering"was"initially 
to do"final"gathering -we will"see how this is related to many.lights rendering in"a"minute -by creating 
many small renderings of the incident light"at many surface points. Once we have a"micro.rendering,"this 
small frame buffer with incident light,"we can concolve it with the BRD."to compute the reflected light. 
.8  It"essentially"uses"a".splat like"point"hierarchy"and"rendering"technique,"here"illustrated" in".D." 
The"micro.framebuffer is"on"the"left"and"we"consider"this"little"binary"tree"of"points" (circles)"on"the"right. 
This"tree"as"a".d"bouncing"volume"hierarchy,"looks"like"this. To"resolve"occlusions"correctly,"we"also"use"a"micro.depth.buffer. 
In"this"example,"the"white"and"blue"nodes"fall"into"a"single"pixel,"but"using"the"depth" buffer,"the"correct"ordering"canbe 
resolved. 3O  We"use"a"complete"binary"tree"to"store"the"geometry"into"a"texture. We"store"node"position"and"radius,"as"well"as"normal"cones"for"each"node. 
3.  Doing"all"this,"there"is"one"special"case"that"needs"consideration:"If"a"node"is"very"close" to"a"receiver"it"might"become"bigger"than"a"micro.pixel. 
We"tried"several"established"procedures"form"point"rendering,"including"the"pull.push" we"used"in"imperfect"shadow"maps,"but"the"most"accurate"and"reliable"method"was"to" 
raycast such"surfels as"small"disks. Please"note,"that"such"raytracing does"not"mean"we"have"to"find"an"intersection"for"the" 
entire"scene"in"some"directions."It"only"means,"that"we"need"to"find"the"intersection" point"of"one"direction"and"handful"of"disks. 
3.  Let"me"show"some"examples,"how"this"is"useful. Those"images"might"look"similar,"but"in"corners,"where".I"is"difficult,"prominently"for" 
VPLs,"they"are"different. Without"raycasting,"the"nearby"surface"has"holes,"that"can"be"filled"by"raycasting. 
 .or final"gathering, micro.rendering"is clearly better,"as it resolves visibility at surface locations,"thus 
capturing occlusions better. Since"we"store"a"point"hierarchy"(with"additional"information"such"as"color.BRD.," 
orientation)"we"have"all"information"to"compute"a"radiosity.like"energy"transfer." That"is,"from"direct"illumination,"we"can"compute"point.to.point"transfer,"also"multiple" 
iterations"and"thus"achieve"multiple"bounces"-just"as"an"excursus. 3.  More"interesting"for"many.lights"methods"are"two"things: 
.. we"can"render"high.quality"shadow"maps"quickly .. we"can"use"final"gathering"like"lighting"by"using"micro.rendering"to"resolve"visibility" 
from"a"surface"point"to"other"surfaces"and"thus"also"to"VPLs"residing"there. There"are"also"further"extensions"that"I'm"not"mentioning"today,"e.g."micro.rendering" 
can"also"account"for"the"BRD."and"warp"the"micro.buffer"essentially"to"do"importance" sampling. Now"that 
we""solved."the"problems"of"creating"and"using"VPLs,"I"would"like"to"address" the"issue"of"quality"or"correctness"a"bit"more,"speaking"about"bias"compensation"and" 
how"this"can"be"done"in"interactive"applications. 3.  well,"the"rendering"equation.  creation of random 
walks (here we simply assume that this can be done,"no matter"if using ray casting or rasterization) 
3.  creation of random walks (here we simply assume that this can be done,"no matter"if using ray casting 
or rasterization) 4O  creation of random walks (here we simply assume that this can be done,"no matter"if 
using ray casting or rasterization) 4.  we"assume"to"use"VPLs"to"approximate"indirect"illumination only 
-direct"illumination"is" typically"computed"using"dedicated"methods"with"some"shadow"mapping"variant. 
4.  Let's see what happens when computing the indirect light. here everything is fine, just"accumulate 
the contributions from all"VPLs. ."here it is different.  ."here it is different."as there's a"nearby 
VPL,"which -because of the geometry term ­will"create a"bright splotch. 4.  the"na.ve"solution"-as"we"have"seen"-is"to"clamp"the"VPLs'"contributions. 
 however,"bounding"the"geometry"term"introduces"a"systematic"error. In"fact,"we"remove"a"short"distance"light"transport 
 The"bias"is"visible"as"artificial"darkening"around"concave"features  4.  We"can"now"have"a"closer"look"what's"happening 
when"clamping. The"clamped"transport"operator"T b describes"VPL"lighting"with"clamped"contributions" 
and"is"shown"on"the"slide. We"can"now"define"another"transport"operator"(T rfor"residual"transport),"which" 
describes"just"this"amount"of"light"which"has"been"clamped"away. 4.  .ere you can see how these transport 
operators related to each other .O  The"standard way of compensating the bias -the error due"to clamped.away 
energy -is what we have already seen in"the previous parts. We have"shown"that"you"can"compute"the"bias"compensation"differently"(see"paper"for" 
details). The"reformulation"itself"does"not"tell"you"how"to"actually"compute"it,"but"it"was"done" having"in"mind"that"we"render"an"image"with"clamped"VPLs"first,"and"then"use"only"this" 
operation"to"recover"the"missing"energy. This"is the algorithm overview. .3  .4  .6  .8  In"order"to"speed 
up"the"rendering"we"use"a"hierarchical"screen"space"approach"inspired" by"multiresolution splatting"of"Nichols"and"Wyman,"please"see"the"paper"for"details. 
 6O  Compute"a"conservative"bounding"radius"outside"which,"none"of"the"pixels"can" contribute"the"to"compensation 
6.  If"possible: .term not"too"high,"otherwise"error No"discontinuity,"otherwise"data"in"..Buffer"inaccurate 
6.   6.  6.  The"hierarchical integration obviously reduces the computational demands,"while not" 
degrading the quality noticeably. The"number of samples (i.e."queried pixels)"has important impact on"the 
rendering performance and is shown here. 6.  These are some results showing the clamped image,"recoverd 
energy,"and final"image. .O  Multiple"iterations recover more energy,"but"the additional"energy drops 
exponentially because of the transport operator (the BRD."in"there respectively) These are some results 
showing the clamped image,"recoverd energy,"and final"image.  These are some results showing the clamped 
image,"recoverd energy,"and final"image. .3  Computing"the"illumination"due"to"VPLs"is"still"the"most"costly"part"of"the"algorithm,"i.e." 
bias"compensation"is"really"not"expensive"to"add"when"computing"it"in"screen"space. .4  Comparison to 
ray casting based bias compensation illustrating the impact of a"screen. space approximation of the scene's 
surfaces. screen.space approximation of the scene's surfaces means:"not"information about all" surfaces 
in"the scene and irregular sampling of the scene's surfaces. Surfaces seen under grazing angles are overestimated,"as 
a"pragmatic solution,"we bound their contributions. .6  And of"course"we"have"not"information"on"hidden"surfaces." 
Note"the"screen.space"approach"is"a"design"chose,"the"reformulated"compensation" would"work"with"other"representations"(e.g."finite"elements)"as"well. 
 .8  Many-lights methods in Autodesk 360 Rendering Adam Arbree This"final"section"of"the"course"discusses"the"use"of"many"lights"algorithms"in"our"cloud" 
rendering"service,"Autodesk®"360"Rendering. To"begin I"want"to"quickly"introduce"our"service"to"illustrate"the"challenges"that"many"lights" 
algorithms"help"to"us"address."Our"service"is"a"component"in"a"system"of"cloud"applications," called"Autodesk"360,"launched"last"March.""The"goal"of"the"system"is"to"provide"a"suite"of" 
tools,"accessible"everywhere"through"web"and"mobile"front"ends,"that"allows"users"to" store,"share"and"collaborate"on"projects"using"our"software.""Our"desktop"applications"are" 
integrating"with"this"cloud"system"to"enable"a"seamless"transition"between"local"work"and" the"cloud"resources. 
 .or"rendering"specifically, we"have"added"cloud"rendering"options,"mirroring"to"the" previous"desktop"rendering"options,"to"our"Autodesk®"Revit®"and"Autodesk"Auto.A.®" 
applications.".sers"can"use"these"cloud"rendering"tools"to"upload"and"render"their"models" remotely"in"the"background.".ompleted"renderings"become"available"in"our"render"gallery" 
website"where"they"can"be"viewed,"modified"and"re.rendered"with"a"list"of"advanced" features,"such"as"panorama"views." 
 In"addition our"service"provides"back.end"visualization"support"for"consumer"products"such" as"Autodesk®"Homestyler®,"a"tool"lets"users"build"and"render"their"own"interior"design" 
plans. .orking"back"from these"applications,"we"can"sketch"out"the"goals"of"our"cloud"rendering" system.".irst"we"focus"primarily"on"architectural"and"design"visualization."This"focus"is"both" 
challenging,"because"these"applications"demand"higher.quality,"predictive"simulations,"and" simplifying,"because"these"applications"use"a"reduced"palette"of"materials,"lights"and" 
geometry"that"are"generally"physically"modeled.".econd,"we"want"to"make"rendering"a"one. click"option"available"anywhere"in"any"product."This"allows"us"to"support"consumer" 
applications,"such"as"Homestyler,"and"challenges"us"to"make"rendering"simpler"and"easier" to use Third 
we need our renderer to scale Our users turn to the cloud to render their to"use."Third,"we"need"our"renderer"to"scale."Our"users"turn"to"the"cloud"to"render"their" 
largest"and"most"comple."scenes,"those"beyond"the"capacity"of"their"desktops,"and"they" e.pect"results"quickly."And,"finally"of"course,"we"need"the"renderer"to"be"efficient"since" 
Autodesk"bears"the"cost"of"the"cloud"compute"resources. Now, meeting"these"goals"can"be"consolidated"into"the"central"problem"of"our"cloud" 
rendering"application:"how"can"we"automatically,"efficiently"and"reliably"produce"a"large" number"of"physically.accurate"renderings"in"a"predictable"amount"of"time?".or"our" 
application,"the"solution"to"this"problem"was"largely"to"use"a"many"lights"rendering" algorithm."This"talk"discusses"how"we"implemented"many"lights"rendering"in"our"system" 
and"why"is"was"critical"to"our"success. This"talk"will"have"two"parts."The"first"part"discusses"the"rendering"algorithm"we"built"at" 
Autodesk"and"some"of"the"implementation"issues"we"addressed"when"developing"that" system."Then"the"second"part"discusses"the"advantages"a"many"lights"rendering"algorithm" 
brought"to"our"application."The"overall"point"of"this"second"discussion"is"that"many"lights" algorithms"have"proven"to"be"faster"and"are"fundamentally"more"scalable."Our"results"show" 
that"this"holds"true"across"a"very"wide"array"of"images"and"scenes."However,"since"raw" performance"has"been"discussed"at"length"in"this"course,"this"talk"will"focus"on"additional" 
consequences of that scalability .pecifically the goal of this talk is to describe how these consequences"of"that"scalability.".pecifically,"the"goal"of"this"talk"is"to"describe"how"these" 
algorithms"also"improve"the"reliability"and"predictability"our"rendering"system"and"how" their"advantages"have"been"critical"to"the"success"to"our"service."They"have"helped"us 
to" make"rendering"easier"for"novice"users,"to provide"more"consistent"results"for"our" customers"and"to"improve"the"quality"of"images"under"fi.ed"cost"constraints. 
 Our"service"uses Multidimensional"Lightcuts"to"compute"its"images.".e"implemented" virtual"spherical"lights"instead"of"VPLs"to"avoid"some"clamping"and"reduce"the"appearance" 
of"corner"darkening."Our"basic"implementation"is"essentially"identical"to"those"described" earlier"in"the"course."However,"in"implementing"these"algorithms,"we"needed"to"address" 
three"important"issues.".irst,"we"needed"to"add"some"form"of"bias"compensation"to"better" render"glossy"materials,"particularly"when"there"are"glossy"inter.reflections.".e"introduce" 
an"eye"ray"splitting"heuristic"solve"this"problem.".econd,"many"scenes"had"difficult"lighting" occlusion 
and we faced issues generating robust VPL distributions To generate more occlusion"and"we"faced"issues"generating"robust"VPL"distributions."To"generate"more" 
efficient"VPLs"sets,"we"added"a"VPL"targeting"method.".inally,"we"needed"support"for" directional"point"light"emission"so"we"created"a"simple"directional"VPL"type. 
 High gloss"materials"are"common"in"our"scenes.".e"found"that"clamping,"sufficient"to"avoid" noise,"could"significantly"effect"a"material.s"appearance"and"that"V.Ls"alone"could"not"solve" 
the"problem".see"the"image"on"the"left"above"and"note"the"missing"reflections"of"the"glossy" highlights"in"particular.."To"address"this"problem,"we"recursively"continue"our"eye"paths"for" 
glossy"materials. This"recursion"is identical"to"the"recursion"used"for"delta"specular"materials.".hen"an"eye" 
path"hits"a"sufficiently"glossy"surface,"instead"of"creating"a"gather"point"immediately,"we" sample"secondary"rays"and"continue."The"number"of"secondary"rays"and"the"decision"to" 
split"is"determined"by"a"heuristic.".nfortunately,"splitting"does"increase"cost"significantly;" requiring"an"increase"in"the"allowed"ma.imum"cut"size. 
 Our"second issue"was"large"scenes"with"significant occlusion"to"the"lighting."In"this" e.ample,"we"have"a"large"office"building"model."In"this"image"we"are"viewing"only"a"single" 
hallway"in"the"model.".ithout"modification"to"the"VPL"tracing"algorithm,"it"is"difficult"to" generate"a"sufficient"density"of"VPLs"in"this"region;"instead"VPLs"are"distributed"over"large" 
regions"of"the"model"out"of"frame."As"can"be"seen"on"the"left"side"of"this"image,"these"low" densities"result"in"a"negative,"darker"bias"and"banding"artifacts"on"the"ceiling."To"address" 
this"issue,"we"resample"the"VPLs"to"ensure"a"higher"density"near"the"camera."To"do"this,"we" use a VPL 
targeting method to redistribute these VPLs Empirically we have found that this use"a"VPL"targeting"method"to"redistribute"these"VPLs."Empirically"we"have"found"that"this" 
targeting"reduces"bias,"improves"quality"and"reduces"cost. Our"targeting"method"uses"image"importance"to"estimate"the"quality"of"a"potential"VPL." 
This"method"is"modeled"on"methods"for"photon"mapping"with"importance"discussed"by"Per" .hirstensen"..hristensen,"P.".Adjoints"and"Importance"in"Rendering:"An"Overview.."TV.G" 
..3.."2003..".""Our targeting"algorithm"splits"the"VPL"tracing"into"two"passes."In"the"first" pass,"importance"carrying"paths"are"generated"from"the"eye."E.actly"as"in"photon"mapping," 
the"resulting"set"of"intersections"..importons.."is"stored"and"placed"in"a"hierarchical" acceleration"structure.".uring"the"second"VPL"tracing"pass,"this"importon"map"is"queried"to" 
estimate the local importon density and we use Russian roulette to reject VPLs in low estimate"the"local"importon"density"and"we"use"Russian"roulette"to"reject"VPLs"in"low" 
density"regions." .inally,"many"of"Autodesk.s"products"support"directionally"emitting"point"lights"and"we"had" 
to"implement"a"VPL"type"that"could"represent"them.".e"found"that"the"omni.directional" VPL"described"in"the"original"Multidimensional"Lightcuts"paper"could"be"trivially"e.tended" 
to"model"directional"emission."The"only"issue"was"bounding"the"light.s"emission"function." To"do"this,"we"repurpose"the"cube"maps"that"bound"material"reflectance"also"bound" 
emittance. The"results"from our"first"few"months"\have"been"robust"and"promising.".e"produce" images"using"small"clusters."Globally"the"cluster"size"is"heterogeneous"but"on"average"we" 
allocate"64"cores"per"image"and"produce"a"megapi.el"in"1.0s."Every"day"we"render"several" thousand"images"and"we"should"produce"our"millionth"image"sometime"this"year."The" 
images"on"this"slides"are"typical"results"shared"by"our"customers. A"selection of"images"rendered"by"customers 
of"360"Rendering"and"shared"on"our" .acebook®"page".as"of"mid.May"2012.. In"the second"part"of"this"discussion,"I"want"to"highlight"the"advantages"of"the"many"lights" 
rendering"algorithm"in"our"application. .ndoubtedly, the"most"important"advantage"of"the"many"lights"system"is"its"performance." 
This"point"has"been"emphasized"throughout"this"course"but"I"want"to"reiterate"it"one"last" time."The"performance"advantages"these"algorithms"have"been"significant"and"meaningful" 
to"our"application"for"Autodesk."Not"only"has"the"performance"impressed"our"customers" but"it"has"significantly"reduced"our"costs."Every".P./second"our"renderer"saves"is"a" 
.P./second"we"don.t"have"to"buy"and"these"algorithms"save"a"lot"of"them."This"is" e.tremely"valuable."However,"since"the"performance"has"been"stressed"throughout"the" 
course I want to take the remainder of this talk to discuss several additional advantages of course,"I"want"to"take"the"remainder"of"this"talk"to"discuss"several"additional"advantages"of" 
these"algorithms. The"first"of"these is"that"many"lights"algorithms"are"easy"to"automate.".ou"can"leverage" 
their"scalability,"not"just"to"make"the"rendering"faster,"but"also"to"make"the"process"of" rendering"easier.".irst,"I"note"that"configuring"a"render"is"a"challenging"task."On"this"slide,"I" 
have"lined"up"the"render"options"available"in"Autodesk®"3.".tudio"Ma.®".there"are"a"lot"of" them..".or"an"e.pert"user,"the"comple.ity"of"these"controls"is"e.tremely"useful."They"can" 
tune"the"renderer"to"achieve"an"certain"artistic"look,"tailor"the"simulation"to"be"ma.imally" efficient"for"a"particular"scene"and"selectively"downgrade"the"computation"for"pre. 
visualization However for novice users these controls are confusing and moreover in our visualization."However,"for"novice"users"these"controls"are"confusing"and,"moreover"in"our" 
cloud"application,"they"are"somewhat"unnecessary.".sers"assume"that"in"the"cloud"there" are"enough"resources"to"compute"their"image"and"they"are"less"concerned"with" 
performance"tuning"options."Our"users"just"want"a"certain"predictive"quality"without"a"lot" of"effort. 
 .onveniently the"structure"and"scalability"of"many"lights"algorithms"make"this"easy"to" achieve."One"can"divide"a"many.lights"algorithm"into"two"components."The"VPL"tracing"and" 
the"eye"ray"generation"represent"a"sampling"component."The"evaluation"of"the"samples"to" generate"an"image"is"a"second"component."Largely"users"care"about"the"how"the"second" 
component"behaves."It"encapsulates"an"intuitive"quality/cost"tradeoff."However,"only" e.pert"users"understand"how"to"correctly"tune"the"first"sampling"component."Most"users" 
just"want"it"to"be"set".correctly.. .ith"many"lights,"we"can"do"e.actly"this."In"our"application,"we"can"permanently"set" 
conservative,".correct."VPL"and"eye"sampling"parameters"internally"in"the"renderer."This" hides"detailed"mathematical"information"about"our"algorithm"from"the"user."Instead"we"let" 
the"user"control"quality"and"time"by"choosing"from"a"predefined"set"of"quality"choices" controlling"the"error.bounds,"cut"size"and"sampling"rates."This"works"because"we"can"rely" 
on"the"fundamental"scalability"of"the"renderer"to"avoid"e.tra"work"if"our"sampling"is"a"little" too"conservative."This"lets"the"user"focus"on"more"intuitive"parts"of"their"request".see"our" 
render settings dialog. such as image size quality and format render"settings"dialog."such"as"image"size,"quality"and"format. 
 I can"demonstrate"that"this"works"well"in"practice."Here"we"look"at"the"relative"render"times" per"megapi.el"for"appro.imately".0,000"jobs"rendered"in"April"2012."The"results" 
demonstrate"two"important"features"of"a"many"lights"algorithm."One,"fi.ed"conservative" setting"reliably"produce"images"across"a"wide"range"of"scenes."Two,"the"algorithms"robustly" 
avoid"e.tra"work:"the"highest"quality"is,"on"average,"only"2."the"cost"of"the"baseline"despite" many"more"VPLs,"eye"samples"and"a"much"larger"ma.imum"cut"size. 
 The"third advantage"of"many"lights"technology"is"related"to"the"last."Our"users"are"building" models"for"a"diverse"set"of"purposes.".or"e.ample,"architects"and"engineers"use"our" 
software"to"make"products"or"a"buildings."Rendering"these"models"should"not"interfere" with"these"other"purposes"but"this"is"not"always"true."Making"a"rendering"can"require"the" 
user"to"annotate"daylight"portals,"create"sectioned"models".shown"here"bottom"left."or" enable/disable"lights."However,"the"scalability"of"many"lights"algorithm"bridges"this"render" 
model/design"model"gap."The"scalability"lets"our"system"robustly"support".design."size" models reducing 
user effort and encouraging them to use rendering more models,"reducing"user"effort"and"encouraging"them"to"use"rendering"more. 
 Additionally,"the"scalability"of"many"lights"algorithms"makes"rendering"costs"more"uniform" and"predictable."This"is"important"because"our"users"want"to"use"rendering"to"e.plore" 
visual"design"options:"lighting,"window"placements"and"building"layout,"for"e.ample."But" this"is"harder"to"do"if"there"is"a"huge"variation"in"the"cost"of"rendering"these"options."By" 
making"rendering"more"scalable"and"uniform,"many"lights"algorithms"allow"our"users"to" more"freely"render"intermediate"designs"and"enable"them"to"use"rendering"to"inform"their" 
designs"rather"than"just"to"visualization"the"final"choices. To"illustrate"this"point,"I"rendered"the"same"scene"four"times."In"the"left"column,"the" 
procedural"sun"and"sky"is"enabled"and"in"the"right"it"is"off."In"the"top"row,"the"model"has" been"sectioned"so"that"only"the"visible"region"is"used"for"simulation"while"in"the"lower"half" 
the"full"model"is"preserved".including"many"light"fi.tures"out"of"frame.."Of"course,"costs" increase"with"the"increased"geometry"and"lighting"but"they"do"so"slowly."The"increased" 
render"time"is"far"outweighed"by"the"time"the"user"saves"building"the"sectioned"model"or" disabling"light"fi.tures. 
 But"one"e.ample is"not"as"compelling"as"a"thousand"e.amples."I"can"continue"this" uniformity"argument"to"a"database"of"appro.imately"20K"images."Here"I"plot"the"relative" 
cost"of"adding"lighting"fi.tures"to"scenes."In"this"sample,"it"turns"out"that"sun/sky"models" have"the"lowest"cost"so"I"normalized"this"plot"to"show"the"relative"cost"of"rendering"models" 
with"only"sunlight"compared"to"those"with"both"sunlight"and"fi.tures."The"models"are" grouped"into"bins"of"100"fi.tures"each."The"leftmost"bin"includes"those"scenes"with"less" 
than"100"and"the"rightmost"is"scenes"with"more"than"1000."Overall"the"results"show" remarkable consistency 
in the rendering cost across the whole range This is the advantage remarkable"consistency"in"the"rendering"cost"across"the"whole"range."This 
is"the"advantage" of"a"many"lights"approach. .inally,"the"last"advantage"is"one"of"the"biggest:"the"performance"of"the"algorithm"at"low" 
quality."This"is"very"critical"for"our"cloud"application"because"we"need"to"offer"cheap,"fast" renderings,"useful"for"intermediate"feedback,"that"are"predictive"of"a"long.running,"high. 
quality"final"result. This"slide"compares"this"cost/quality"tradeoff"for"our"system".top."and"a"path"tracer"with" 
irradiance"caching".bottom.."The"images"on"the"right"are"final"quality"and"cost"10."more" than the images 
in the leftmost column .Note: columns are equal time comparisons than"the"images"in"the"leftmost"column.".Note:"columns"are"equal"time"comparisons" 
between"the"two"renderers;"and"the"underlying"material"system"for"the"two"renderers"is" not"the"same"so"the"teapot"and"table"appear"slightly"different.."Because"we"can"keep"the" 
VPL"sampling"rate"the"same"across"this"whole"set".VPL"cost"is"included"in"the"timings.,"the" many"lights"solution"tends"to"preserve"the"lighting"quality"and"appearance"across"the" 
whole"range."However,"the"path"traced"solution"becomes"negatively"biased"as"the" irradiance"cache"sampling"rate"and"density"become"low. 
 To"conclude this"talk,"I"return"to"the"problem"I"discussed"at"the"beginning."How"to"you" make"rendering"accurate,"fast,"automatic,"efficient"and"reliable?"Or"more"succinctly:"how" 
do"you"make"rendering"a"service?"That"is"our"goal"at"Autodesk. Many"lights"algorithms"begin to"make"these"type"of"service"a"reality."Our"case"study"at" 
Autodesk"demonstrates"that"many"lights"rendering"is"a"robust"and"well.studied"technology" ready"for"production."Moreover"we"have"demonstrated"that"the"technology"has"many" 
significant"advantages."It: .. Is"faster,"cheaper"and"more"efficient; .. Enables"automatic"render"configuration; 
.. Provides inherent amortization of costs across design and quality options; Provides"inherent"amortization"of"costs"across"design"and"quality"options;.. 
Makes"rendering"easier"for"novice"users;"and.. Produces"useful"preview"images"at"very"low"cost. Of"course,"the"work"is"never 
done."To"close"I"want"to"highlight"two"issues"that"would" considerably"improve"our"system.".oremost,"general"VPL"targeting"is"an"unsolved"problem." 
.undamentally,"we"need"methods"to"assess"the"error"of"a"particular"set"of"VPLs"and"to" generalize"the"targeting"process"to"reduce"this"error.".econd,"there"seems"to"be"room"for" 
further"performance"improvements"in"the"cut"refinement"process."One,"if"we"could" estimate"the"absolute"error"of"an"intermediate"cut,"we"could"begin"to"quantify"the"absolute" 
accuracy"of"many"lights"methods."Also"we"want"to"continue"to"investigate"whether" efficiency could be 
improved by altering the VPL trees representative selection or cut efficiency"could"be"improved"by"altering"the"VPL"trees,"representative"selection"or"cut" 
refinement"ordering. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343491</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>75</pages>
		<display_no>8</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Graphics programming for the web]]></title>
		<page_from>1</page_from>
		<page_to>75</page_to>
		<doi_number>10.1145/2343483.2343491</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343491</url>
		<abstract>
			<par><![CDATA[<p>With HTML5 and ever-improving browser performance, the web has emerged as an ideal platform for showcasing graphics applications. Several graphics applications that were once too slow to be written in anything but native code may now be fast enough to run as web apps.</p> <p>This course for developers who want to develop graphics applications for the web introduces the concepts of core web programming and the dominant graphics technologies that are supported by most modern browsers. It begins with a brief primer on general-purpose web programming (HTML parsing, CSS, and DOM- and render-tree construction), how Javascript can be used to generate dynamic web content, and how to improve performance and accelerate development time.</p> <p>The bulk of this course describes the web technologies specific to graphics:</p> <p>CSS3: transitions, animations, 3D transforms and the new CSS-shaders</p> <p>HTML5 Canvas 2D: path API, compositing, image editing, and animation</p> <p>SVG: overview of API, how it's different from HTML5 Canvas 2D</p> <p>WebGL: the key necessary steps, relation to OpenGL, performance hints</p> <p>WebCL: the formal specifications, what's implemented and what's to come</p> <p>For each topic, the course provides a significant number of code examples that illustrate the relevant graphics capabilities. Attendees are welcome to copy and paste our code snippets and execute them inside any modern web browser.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738907</person_id>
				<author_profile_id><![CDATA[81100641728]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pushkar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Motorola Mobility]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738908</person_id>
				<author_profile_id><![CDATA[81100388833]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mika&#235;l]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bourges-S&#233;venier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Motorola Mobility]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738909</person_id>
				<author_profile_id><![CDATA[81488655116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Russell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738910</person_id>
				<author_profile_id><![CDATA[81100494148]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Zhenyao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aarnio, T. and Bourges-Sevenier, M. WebCL Working Draft. <i>Khronos WebCL Working Group</i>. https://cvs.khronos.org/svn/repos/registry/trunk/public/webcl/spec/latest/index.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Munshi, A. OpenCL Specification 1.1. <i>Khronos OpenCL Working Group</i>. http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Marrin, C. WebGL Specification. <i>Khronos WebGL Working Group</i>. http://www.khronos.org/registry/webgl/specs/latest/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Munshi, A. and Leech, J. OpenGL ES 2.0.25. <i>Khronos Group</i>. http://www.khronos.org/registry/gles/specs/2.0/es_full_spec_2.0.25.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Simpson, R. J. The OpenGL ES Shading Language. <i>Khronos Group</i>. http://www.khronos.org/registry/gles/specs/2.0/GLSL_ES_Specification_1.0.17.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Herman, D. and Russell, K., eds. Typed Array Specification. <i>Khronos.org</i>. http://www.khronos.org/registry/typedarray/specs/latest/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[OpenCL 1.1 Reference Pages. OpenCL 1.1 Reference Pages. <i>Khronos.org</i>. http://www.khronos.org/registry/cl/sdk/1.1/docs/man/xhtml/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[NVidia OpenCL Programming Guide for the CUDA Architecture. 2012. <i>NVidia OpenCL Programming Guide for the CUDA Architecture</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[AMD Accelerated Parallel Processing OpenCL. 2011. <i>AMD Accelerated Parallel Processing OpenCL</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2046379</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gaster, B., Howes, L., Kaeli, D. R., Mistry, P., and Schaa, D. 2011. <i>Heterogeneous Computing with OpenCL</i>. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Scarpino, M. 2011. OpenCL in Action: How to Accelerate Graphics and Computations. Manning Publications.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2049883</ref_obj_id>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Munshi, A., Gaster, B., Mattson, T. G., Fung, J., and Ginsburg, D. 2011. <i>OpenCL Programming Guide</i>. Addison-Wesley Professional.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1841511</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kirk, D. and Hwu, W.-M. 2010. <i>Programming Massively Parallel Processors</i>. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hillis, W. D. and Steele, G. 1986. <i>Data parallel algorithms</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Motorola Mobility. Node-webcl, an implementation of Khronos WebCL specification using Node.JS. https://github.com/Motorola-Mobility/node-webcl]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Nokia Research. WebCL. http://webcl.nokiaresearch.com/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Samsung Research. WebCL prototype for WebKit. http://code.google.com/p/webcl/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Mozilla. FireFox WebCL branch. http://hg.mozilla.org/projects/webcl/]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>128874</ref_obj_id>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Cole, M. I. 1989. Algorithmic skeletons: structured management of parallel computation]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Gerstmann, D. 2009. <i>Advanced OpenCL</i>. Siggraph 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1863091</ref_obj_id>
				<ref_obj_pid>1863086</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[McCool, M. D. 2010. Structured parallel programming with deterministic patterns. <i>Proceedings of the 2nd USENIX conference on Hot topics in parallelism</i>, 5--5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Bordoloi, U. D. 2010. Optimization Techniques: Image Convolution. 1--25. http://developer.amd.com/zones/openclzone/events/assets/optimizations-imageconvolution1.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[BDT Nbody Tutorial. BDT Nbody Tutorial. <i>Brown Deer Technology</i>. http://www.browndeertechnology.com/docs/BDT_OpenCL_Tutorial_NBody-rev3.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[I&#241;igo Quilez. ShaderToy with Mandelbulb shader. http://www.iquilezles.org/apps/shadertoy/?p=mandelbulb]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Donnelly, W. GPU Gems - Chapter 8. Per-Pixel Displacement Mapping with Distance Functions. <i>developer.nvidia.com</i>. http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter08.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Mattson, T. G., Buck, I., Houston, M., and Gaster, B. 2009. OpenCL - A standard platform for programming heterogeneous parallel computers. <i>SC'09</i>. http://www.crc.nd.edu/~rich/SC09/docs/tut149/OpenCL-tut-sc09.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Feng, W.-C., Lin, H., Scogland, T., and Zhang, J. 2012. OpenCL and the 13 dwarfs: a work in progress.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Lefohn, A., Kniss, J., and Owens, J. D. Chapter 33. Implementing Efficient Parallel Data Structures on GPUs. In: <i>GPU Gems 2</i>. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Kr&#252;ger, J. and Westermann, R. Chapter 44. A GPU Framework for Solving Systems of Linear Equations. In: <i>GPU Gems 2</i>. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Porting CUDA Applications to OpenCL. Porting CUDA Applications to OpenCL. <i>developer.amd.com</i>. http://developer.amd.com/zones/OpenCLZone/programming/pages/portingcudatoopencl.aspx.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Hensley, J., Gerstmann, D., and Harada, T. OpenCL by Example. <i>SIGGRAPH Asia 2010</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[A Pattern Language for Parallel Programming. A Pattern Language for Parallel Programming. <i>parlab.eecs.berkeley.edu</i>. http://parlab.eecs.berkeley.edu/wiki/patterns/patterns.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 *UDSKLFV 3URJUDPPLQJ IRU WKH :HE 6LQJOH VHQWHQFH VXPPDU\ :H LQWURGXFH DQG GHPRQVWUDWH +70/ ZHE WHFKQRORJLHV 
WKDW HQDEOH JUDSKLFV GHYHORSHUV WR SURGXFH FRPSOH[ JHQHUDO SXUSRVH JUDSKLFV DSSOLFDWLRQV IRU WKH ZHE 
$EVWUDFW :LWK WKH DGYHQW RI +70/ DQG WKH HYHU LPSURYLQJ EURZVHU SHUIRUPDQFH WKH ZRUOG ZLGH ZHE KDV HPHUJHG 
DV DQ LGHDO SODWIRUP IRU VKRZFDVLQJ JUDSKLFV DSSOLFDWLRQV 6HYHUDO JUDSKLFV DSSOLFDWLRQV WKDW HDUOLHU 
PD\ KDYH EHHQ WRR VORZ WR EH ZULWWHQ LQ DQ\WKLQJ EXW QDWLYH FRGH PD\ QRZ EH IDVW HQRXJK WR UXQ DV ZHE 
DSSV 7KLV FRXUVH KRSHV WR KHOS WKRVH GHYHORSHUV ZKR ZLVK WR GHYHORS JUDSKLFV DSSOLFDWLRQV IRU WKH ZHE 
:H LQWURGXFH WKH GRPLQDQW JUDSKLFV WHFKQRORJLHV WKDW DUH DFFHVVLEOH YLD ZHE SURJUDPPLQJ RQ PRVW PRGHUQ 
EURZVHUV :H VWDUW ZLWK D TXLFN SULPHU RQ JHQHUDO SXUSRVH ZHE SURJUDPPLQJ :H LQWURGXFH +70/ SDUVLQJ &#38;66 
'20 DQG UHQGHU WUHH FRQVWUXFWLRQ DQG WKH XVH RI -DYDVFULSW IRU JHQHUDWLQJ G\QDPLF ZHE FRQWHQW 7KH EXON 
RI WKLV FRXUVH GHVFULEHV WKH ZHE WHFKQRORJLHV VSHFLILF WR JUDSKLFV +70/ &#38;DQYDV SDWK $3, LPDJH HGLWLQJ 
DQLPDWLRQ FRPSDULVRQ ZLWK 69* &#38;66 WUDQVLWLRQV DQLPDWLRQV ' WUDQVIRUPV DQG WKH QHZ FVV VKDGHUV :HE*/ 
JHWWLQJ VWDUWHG DFKLHYLQJ KLJK SHUIRUPDQFH DGYDQFHG ' WHFKQLTXHV :HE&#38;/ WKH IRUPDO VSHFLILFDWLRQV 
ZKDW¶V LPSOHPHQWHG DQG ZKDW¶V WR FRPH )RU HDFK WRSLF ZH SURYLGH D VLJQLILFDQW QXPEHU RI FRGH H[DPSOHV 
WKDW LOOXVWUDWH WKH UHOHYDQW JUDSKLFV FDSDELOLWLHV 7KH FRXUVH SDUWLFLSDQWV ZLOO VHH VHYHUDO LQWHUDFWLYH 
GHPRV GXULQJ WKH FRXUVH DQG ZLOO DOVR EH DEOH WR FRS\ DQG SDVWH RXU FRGH VQLSSHWV DQG H[HFXWH WKHP HDVLO\ 
LQVLGH DQ\ PRGHUQ ZHE EURZVHU :H GR QRW UHTXLUH WKH FRXUVH SDUWLFLSDQWV WR KDYH DQ\ NQRZOHGJH RI ZHE 
SURJUDPPLQJ *UDSKLFV H[SHUWLVH LV QRW UHTXLUHG RU DVVXPHG +RZHYHU ZH H[SHFW WKLV FRXUVH ZLOO EH PRVW 
XVHIXO WR SHRSOH DOUHDG\ IDPLOLDU ZLWK JUDSKLFV FRQFHSWV  2QOLQH &#38;RXUVH 5HSRVLWRU\ 7KH ODWHVW YHUVLRQ 
RI WKHVH QRWHV DQG RWKHU KHOSIXO UHVRXUFHV ZLOO EH KRVWHG DW KWWS ZZZ NKURQRV RUJ GHYHORSHUV OLEUDU\ 
 VLJJUDSK FRXUVH JUDSKLFV SURJUDPPLQJ IRU WKH ZHE 6DPSOH &#38;RXUVH 6FKHGXOH DP ,QWURGXFWLRQ   
        DP &#38;DQYDV 69*             DP &#38;66         DP :HE*/ 3DUW 
  DP EUHDN            DP :HE*/ 3DUW   DP :HE&#38;/             SP &#38;RXUVH 
(QG ZH ZLOO DOORW WLPH IRU TXHVWLRQV DW WKH HQG RI HDFK VHFWLRQ WDON ,QVWUXFWRUV 3XVKNDU -RVKL LV D 
JUDSKLFV UHVHDUFK HQJLQHHU DW 0RWRUROD 0RELOLW\ +LV UHVHDUFK IRFXVHV RQ JHRPHWULF PRGHOLQJ ZLWK DQ HPSKDVLV 
RQ FDVXDO PRGHOLQJ IRU QRYLFH XVHUV 3ULRU WR 0RWRUROD 0RELOLW\ KH ZDV D FRPSXWHU VFLHQWLVW DW WKH $GYDQFHG 
7HFKQRORJ\ /DEV DW $GREH ZKHUH KH GHYHORSHG WKH FRUH JHRPHWU\ HQJLQH IRU WKH 5HSRXVVp IHDWXUH RI $GREH 
3KRWRVKRS &#38;6 3XVKNDU KDV D 3K ' LQ FRPSXWHU VFLHQFH IURP WKH 8QLYHUVLW\ RI &#38;DOLIRUQLD %HUNHOH\ 
 0LNDHO %RXUJHV 6HYHQLHU LV D PXOWLPHGLD VRIWZDUH DUFKLWHFW DW 0RWRUROD 0RELOLW\ IRFXVLQJ RQ XVHU H[SHULHQFH 
DQG PXOWLFRUH DSSOLFDWLRQV RQ PRELOH GHYLFHV +H LV FR HGLWRU RI :HE&#38;/ VSHFLILFDWLRQ 3ULRU WR 0RWRUROD 
KH ZDV HGLWRU RI YDULRXV VWDQGDUGV VXFK DV 03(* ; ' 8 ' DQG WKHLU LPSOHPHQWDWLRQ LQ SURGXFWV RI $GREH 
6XQ L9$67 )UDQFH 7HOHFRP 0LNDHO KDV DQ 06 LQ PHFKDQLFDO DQG HOHFWULFDO HQJLQHHULQJ IURP (&#38;$0 /\RQ 
)UDQFH DQG D 06 LQ VLJQDO DQG LPDJH SURFHVVLQJ IURP 8QLYHUVLW\ 5HQQHV , )UDQFH .HQ 5XVVHOO LV D VRIWZDUH 
HQJLQHHU RQ WKH &#38;KURPH ZHE EURZVHU WHDP DW *RRJOH ,QF DQG LV FXUUHQWO\ VHUYLQJ DV WKH FKDLU RI WKH 
:HE*/ ZRUNLQJ JURXS DW .KURQRV .HQ KDV RYHU ILIWHHQ \HDUV RI ' JUDSKLFV SURJUDPPLQJ H[SHULHQFH +H KROGV 
D %DFKHORU RI 6FLHQFH LQ (OHFWULFDO (QJLQHHULQJ DQG &#38;RPSXWHU 6FLHQFH IURP WKH 0DVVDFKXVHWWV ,QVWLWXWH 
RI 7HFKQRORJ\ DQG D 0DVWHU RI 6FLHQFH LQ 0HGLD $UWV DQG 6FLHQFHV IURP WKH 0,7 0HGLD /DE =KHQ\DR 0R LV 
D VRIWZDUH HQJLQHHU RQ WKH &#38;KURPH ZHE EURZVHU WHDP DW *RRJOH ,QF =KHQ\DR HDUQHG KLV 3K ' IURP 8QLYHUVLW\ 
RI 6RXWKHUQ &#38;DOLIRUQLD GXULQJ ZKLFK KLV UHVHDUFK IRFXVHG RQ FRPSXWHU JUDSKLFV $IWHU JUDGXDWLRQ KH 
FRQWLQXHG KLV HQWKXVLDVP LQ ' JUDSKLFV IRU WKH SDVW  \HDUV ZRUNLQJ DW *RRJOH KLV PDLQ HIIRUW LV LPSOHPHQWLQJ 
DQG LPSURYLQJ :HE*/ LQ ZHENLW DQG &#38;KURPH ,QWURGXFWLRQ :HE SURJUDPPLQJ KDV HYROYHG IURP VPDOO WDVNV 
IRU G\QDPLF ZHE SDJH FRQWHQW WR IXOO DSSOLFDWLRQV IRU DFFRPSOLVKLQJ FRPSOH[ WDVNV 9DVW LPSURYHPHQWV LQ 
-DYDVFULSW SHUIRUPDQFH LQ DOO PDMRU EURZVHUV LQFOXGLQJ WKRVH IRU PRELOH SODWIRUPV KDYH PDGH LW SRVVLEOH 
IRU WKH ZHE WR HPHUJH DV D FRPPRQ FRPSXWLQJ SODWIRUP IRU PRVW KDUGZDUH DQG VRIWZDUH FRQILJXUDWLRQV :LWK 
WKH DGYHQW RI +70/ DQG &#38;66 DOO GRPLQDQW EURZVHUV SURYLGH D GUDZLQJ $3, FRPSOHWH ZLWK PDQLSXODWLRQ 
RI ZHE SDJH HOHPHQWV LQ ' VSDFH 7KURXJK :HE*/ PRVW EURZVHUV DOVR SURYLGH D GLUHFW LQWHUIDFH WR WKH JUDSKLFV 
KDUGZDUH RQ WKH FOLHQW FRPSXWHU 8SFRPLQJ :HE&#38;/ WHFKQRORJLHV PDNH LW HYHQ HDVLHU IRU SHRSOH WR FRQYHUW 
WKHLU PXOWLWKUHDGHG QDWLYH DSSOLFDWLRQV LQWR ZHE DSSOLFDWLRQV 7KHVH IDFWRUV PDNH WKH ZHE DQ HVSHFLDOO\ 
LGHDO SODWIRUP IRU VKRZFDVLQJ DGYDQFHG JUDSKLFV DSSOLFDWLRQV 7KLV FRXUVH LV WDUJHWHG WRZDUGV SURJUDPPHUV 
ZKR ZLVK WR ZULWH JHQHUDO SXUSRVH JUDSKLFV DSSOLFDWLRQV IRU WKH ZHE %\ ³JHQHUDO SXUSRVH´ ZH PHDQ DSSOLFDWLRQV 
WKDW PD\ FRQVXPH D ODUJH DPRXQW RI FRPSXWLQJ UHVRXUFHV DQG DUH W\SLFDOO\ ZULWWHQ LQ QDWLYH &#38; &#38; 
 FRGH E\ H[SHULHQFHG JUDSKLFV SURJUDPPHUV :H ZLVK WR KHOS WKRVH SURJUDPPHUV ZULWH VLPLODU DSSOLFDWLRQV 
IRU WKH ZHE E\ LQWURGXFLQJ JUDSKLFV ZHE SURJUDPPLQJ :H GR QRW UHTXLUH WKH FRXUVH SDUWLFLSDQWV WR KDYH 
DQ\ NQRZOHGJH RI ZHE SURJUDPPLQJ *UDSKLFV H[SHUWLVH LV QRW UHTXLUHG RU DVVXPHG KRZHYHU ZH H[SHFW WKLV 
FRXUVH ZLOO EH PRVW XVHIXO WR SHRSOH IDPLOLDU ZLWK JUDSKLFV FRQFHSWV $IWHU D TXLFN SULPHU RQ ZHE SURJUDPPLQJ 
ZH LQWURGXFH WKH +70/ FDQYDV HOHPHQW DQG GHVFULEH KRZ LW FDQ EH XVHG WR HQDEOH LQWHUDFWLYH GUDZLQJ WRROV 
LPDJH HGLWLQJ DQG LQWHUDFWLYH ' DQLPDWLRQV 1H[W ZH LQWURGXFH &#38;66 DQG VKRZ KRZ LW FDQ EH XVHG WR 
SURGXFH G\QDPLF ' XVHU LQWHUIDFHV ' LPDJH ILOWHUV DQG JHQHUDO SXUSRVH &#38;66 VKDGHUV 1H[W ZH VKRZ KRZ 
' JUDSKLFDO FRQWHQW FDQ EH GLVSOD\HG DQG PDQLSXODWHG YLD :HE*/ $ORQJ ZLWK WKH LQLWLDO VHWXS QHFHVVDU\ 
IRU DOO :HE*/ SURJUDPV ZH DOVR JLYH FRPPRQO\ DSSOLFDEOH KLQWV IRU LPSURYHG JUDSKLFV SHUIRUPDQFH )LQDOO\ 
ZH LQWURGXFH WKH QHZ :HE&#38;/ VSHFLILFDWLRQ WKDW EULQJV WKH SDUDOOHO FRPSXWDWLRQ RI 2SHQ&#38;/ WR ZHE 
SURJUDPV :HE 3URJUDPPLQJ 3ULPHU ,Q WKLV VHFWLRQ ZH LQWURGXFH WKH WHFKQRORJLHV WKDW DUH HVVHQWLDO IRU 
ZHE SURJUDPPLQJ 1RWH WKDW ZH GHVFULEH ZHE SURJUDPPLQJ DW D YHU\ KLJK OHYHO DQG WKLV VHFWLRQ LV LQWHQGHG 
IRU VRPHRQH FRPSOHWHO\ QHZ WR ZHE SURJUDPPLQJ 5HDGHUV IDPLOLDU ZLWK WKHVH WRSLFV FDQ VNLS DKHDG WR WKH 
QH[W VHFWLRQ ZKHUH ZH ZLOO VWDUW GLVFXVVLQJ ZHE SURJUDPPLQJ WRSLFV VSHFLILF WR JUDSKLFV +70/ DQG &#38;66 
0RVW ZHE FRQWHQW LV LQ WKH IRUP RI SODLQ WH[W +70/ +\SHU 7H[W 0DUNXS /DQJXDJH +70/ LV WKH ODQJXDJH XVHG 
WR PDUN XS FRQWHQW WR WKH GLVSOD\HG RQ WKH ZHE 7KH ZHE FRQWHQW LV PRVWO\ WH[W EXW FDQ DOVR LQFOXGH LPDJHV 
RU JHQHUDO ELQDU\ REMHFWV VXFK DV YLGHR 7KH PDUN XS LV GRQH E\ D FRQWHQW LGHQWLILHU DOVR NQRZQ DV D 
WDJ DQG XVXDOO\ LQVWUXFWV D ZHE EURZVHU KRZ WR GLVSOD\ WKH FRQWHQW )RU H[DPSOH DQ +70/ SDJH PD\ FRQWDLQ 
WDJV OLNH WKLV .. ......u....U... ... .....w.. 7KH ³VDPSOH WH[W´ LQ EHWZHHQ WKH VWDUW H J .. ......u....U. 
DQG VWRS H J .w.. WDJV ZLOO EH GLVSOD\HG XVLQJ WKH SUHVHQWDWLRQ VW\OH VSHFLILHG IRU WKH WDJ EROG LQ WKLV 
FDVH 7KH GLVSOD\ VW\OH IRU HDFK WDJ RI WKH GRFXPHQW LV JLYHQ WR WKH EURZVHU E\ WKH &#38;66 &#38;DVFDGLQJ 
6W\OH 6KHHW IRU WKDW GRFXPHQW 7KH H[DFW UXOHV IRU ZULWLQJ +70/ DQG &#38;66 DUH GHILQHG LQ WKH VSHFLILFDWLRQV 
SXEOLVKHG E\ WKH :RUOG :LGH :HE &#38;RQVRUWLXP : &#38;  $ EURZVHU SDUVHV WKH +70/ SDJH DQG DGGV FRQWHQW 
FRUUHVSRQGLQJ WR WKH WDJ DV D QHZ HOHPHQW RI WKH SDJH¶V GRFXPHQW REMHFW PRGHO '20 7KH '20 LV WKH SURJUDPPLQJ 
LQWHUIDFH IRU DFFHVVLQJ DQG PDQLSXODWLQJ WKH FRQWHQWV RI DQ +70/ SDJH 7KH '20 DOORZV XV WR DFFHVV WKLQJV 
OLNH WKH SURSHUWLHV RI D VSHFLILF HOHPHQW WKH QXPEHU RI D FHUWDLQ W\SH RI D WDJ HWF 7KH '20 HOHPHQWV 
IRUP D '20 WUHH WKDW JLYHV XV D KLHUDUFKLFDO RYHUYLHZ RI WKH FRQWHQWV RI WKH SDJH &#38;RQVLGHU WKH IROORZLQJ 
KWPO SDJH VRXUFH DQG LWV FRUUHVSRQGLQJ '20 WUHH .      .Sww . ww   .S.. .. ..... ..ww .. ... 
.. ...... .... ............s. ....S ....... s ........S ....s ..S ...s ....S ......s .....S .....s .....S.. 
.... ..... ....w... ... ........ ....w.. .w.... .w..... .w.. .. *LYHQ WKH '20 WUHH WKH EURZVHU SURGXFHV 
D UHQGHU WUHH D WUHH RI YLVXDO HOHPHQWV LQ WKH RUGHU WKDW WKH\ DUH SDLQWHG )RU WKH SXUSRVH RI WKLV FRXUVH 
WKLQN RI WKHVH YLVXDO HOHPHQWV DV UHFWDQJOHV RQH IRU HDFK +70/ EORFN WKDW WKH EURZVHU ZLOO SDLQW $V PHQWLRQHG 
EHIRUH WKH UXOHV XVHG E\ WKH EURZVHU IRU GLVSOD\LQJ WKH YLVXDO HOHPHQW H J WKH EDFNJURXQG FRORU RI WKH 
UHFWDQJOH DUH SURYLGHG E\ WKH SUHVHQWDWLRQ VW\OH DVVRFLDWHG ZLWK WKDW HOHPHQW 7KH VWDQGDUG SUDFWLFH 
LV WR QRW LQFOXGH WKH SUHVHQWDWLRQ VW\OH LQIRUPDWLRQ GLUHFWO\ LQ WKH +70/ WDJV DV ZDV GRQH DERYH EXW 
LQVWHDG WR GHVFULEH WKH SUHVHQWDWLRQ VW\OH IRU HDFK +70/ WDJ XVHG LQ WKH ZHE SDJH LQ D VHSDUDWH ILOH 
FDOOHG WKH VW\OHVKHHW 7KLV UHGXFHV WKH +70/ SDJH VL]H DQG PDNHV LW HDVLHU WR VSHFLI\ RU FKDQJH WKH VW\OH 
IRU DQ HQWLUH ZHEVLWH DOO ZHE SDJHV ZLOO UHIHU WR WKH VDPH VW\OH VKHHW  7KH VW\OHVKHHW WKDW GHILQHV 
WKH VW\OH IRU HYHU\ HOHPHQW RI WKH '20 WUHH LV JLYHQ E\ WKH &#38;DVFDGLQJ 6W\OH 6KHHW &#38;66 IRU WKDW 
GRFXPHQW )RU H[DPSOH DV GHILQHG E\ WKH : &#38; ER[ PRGHO KWWS ZZZ Z RUJ 75 &#38;66 ER[ KWPO HDFK UHFWDQJOH 
FRUUHVSRQGLQJ WR FHUWDLQ +70/ WDJV NQRZQ DV EORFN WDJV ZLOO EH VXUURXQGHG E\ DGGLWLRQDO UHFWDQJOHV QDPHO\ 
SDGGLQJ ERUGHU DQG PDUJLQ ,Q DGGLWLRQ WKH VW\OH FDQ FRQWDLQ LQIRUPDWLRQ OLNH IRQWV XVHG FRORU RI WKH 
WH[W VW\OH RI WKH WH[W DQG VR RQ 7KH WHUP ³FDVFDGLQJ´ LQ &#38;66 UHIHUV WR WKH RUGHU LQ ZKLFK SUHVHQWDWLRQ 
UXOHV DUH DSSOLHG 7KH SUHVHQWDWLRQ UXOH DSSOLHG WR DQ HOHPHQW RI WKH UHQGHU WUHH LV DXWRPDWLFDOO\ DSSOLHG 
WR LWV FKLOGUHQ LQ WKH UHQGHU WUHH XQOHVV WKH FKLOG QRGHV KDYH GLIIHUHQW SUHVHQWDWLRQ UXOHV VSHFLILHG 
LQ WKH &#38;66 $OO EURZVHUV ZLOO KDYH D GHIDXOW VW\OHVKHHW WKDW GHILQHV WKH SUHVHQWDWLRQ VW\OH IRU '20 
HOHPHQWV QRW LQFOXGHG LQ D SDUWLFXODU ZHE SDJH¶V VW\OHVKHHW &#38;66 ZLOO EH XVHIXO IRU XV ODWHU LQ WKLV 
WXWRULDO ZKHQ ZH GLVFXVV WKH QHZ JUDSKLFV VSHFLILF IHDWXUHV RI &#38;66 $IWHU WKH UHQGHU WUHH LV JHQHUDWHG 
PRVW EURZVHUV KDYH D VHSDUDWH ³OD\RXW´ SURFHVV WKDW DVVLJQV D SRVLWLRQ ; DQG < ORFDWLRQ ZLWKLQ WKH EURZVHU 
ZLQGRZ DQG ZLGWK DQG KHLJKW WR HDFK HOHPHQW RI WKH UHQGHU WUHH 7KLQN RI WKLV DV D SURFHVV RI SODFLQJ 
UHFWDQJOHV DW FHUWDLQ ORFDWLRQV ZLWKLQ WKH EURZVHU ZLQGRZ DQG DVVLJQLQJ D ZLGWK DQG KHLJKW WR HDFK UHFWDQJOHV 
$IWHU WKH OD\RXW SURFHVV WKH LQGLYLGXDO HOHPHQWV RI WKH UHQGHU WUHH DUH SDLQWHG LQ WKH EURZVHU ZLQGRZ 
LQ WKH RUGHU VSHFLILHG E\ WKH UHQGHU WUHH -DYDVFULSW -DYDVFULSW LV VFULSW ODQJXDJH XVHG WR SURJUDPPDWLFDOO\ 
L H G\QDPLFDOO\ DGG RU FKDQJH FRQWHQW IRU D ZHE SDJH RU FKDQJH WKH SURSHUWLHV RI H[LVWLQJ FRQWHQW )RU 
H[DPSOH LI \RX ZDQWHG WR FKDQJH WKH WH[W FRORU FRQWDLQHG LQ WKLV WDJ .... ... u... . .U ...... u.....s 
...U . . ... ... .w.... ZKHQ WKH XVHU¶V PRXVH SRLQWHU KRYHUHG RYHU WKH WH[W \RX FRXOG ZULWH D ³. ........´ 
KDQGOHU OLNH WKLV .... ... u... . .U . ......... u.... . .S... .. . . . ...... ...bS.....S....... ....SU. 
. ... ... .w.... ZKLFK ZLOO FKDQJH WKH ³6DPSOH 7H[W´ FRORU IURP UHG WR EOXH RQ PRXVH RYHU 7KURXJK -DYDVFULSW 
ZH FDQ DFFHVV WKH '20 IRU WKH ZHE SDJH E\ FDOOLQJ IXQFWLRQV OLNH ³.... . .S... .. . . . .´ 2YHU WKH 
\HDUV WKH VFRSH RI -DYDVFULSW KDV JURZQ VLJQLILFDQWO\ ,QVWHDG RI EHLQJ D ODQJXDJH RQO\ IRU PDNLQJ VPDOO 
FKDQJHV WR D PRVWO\ VWDWLF ZHE SDJH LW LV QRZ XVHG IRU SHUIRUPLQJ VLJQLILFDQW FDOFXODWLRQV RU EXLOGLQJ 
ODUJH ZHE IUDPHZRUNV 7KLV JURZWK LQ VFRSH LV GXH LQ ODUJH SDUW WR WKH KXJH LPSURYHPHQW LQ WKH SHUIRUPDQFH 
RI WKH -DYDVFULSW LQWHUSUHWHU LQ ZHE EURZVHUV D SHUIRUPDQFH LPSURYHPHQW RI URXJKO\  SHU \HDU 7KH LPSURYHPHQW 
LQ WKH -DYDVFULSW LQWHUSUHWHU SHUIRUPDQFH LV RQH RI WKH WKLQJV WKDW HQDEOHV XV WR ZULWH FRPSOH[ JUDSKLFV 
DSSOLFDWLRQV IRU WKH ZHE 1RWH WKDW PDQ\ WUDGLWLRQDO &#38; &#38; -DYD SURJUDPPHUV ILQG -DYDVFULSW WR 
EH IUXVWUDWLQJ -DYDVFULSW LV QRW D VXEVHW RI -DYD WKH QDPH LV IRU WKH PRVW SDUW D PLVQRPHU 6HYHUDO DVSHFWV 
WKDW PDQ\ SURJUDPPHUV WDNH IRU JUDQWHG LQ RWKHU SURJUDPPLQJ ODQJXDJHV OLNH VWURQJ W\SHV EORFN VFRSH IRU 
YDULDEOHV DUH QRW SUHVHQW LQ -DYDVFULSW -DYDVFULSW GRHV VXSSRUW VRPH SDUDGLJPV RI PRGHUQ SURJUDPPLQJ 
ODQJXDJHV OLNH REMHFW RULHQWHG GHVLJQ DQG LQKHULWDQFH EXW QRW LQ DQ LQWXLWLYH VWUDLJKW IRUZDUG PDQQHU 
0RUHRYHU EURZVHUV DUH H[WUHPHO\ HUURU WROHUDQW DQG -DYDVFULSW FRPSLODWLRQ HUURUV DUH QRW UHSRUWHG XQWLO 
WKH RIIHQGLQJ OLQH LV H[HFXWHG 7KHUHIRUH OHDUQLQJ -DYDVFULSW FDQ EH FKDOOHQJLQJ IRU PDQ\ VRIWZDUH GHYHORSHUV 
 7KDW EHLQJ VDLG WKH ZHE FRPPXQLW\ KDV EXLOW VHYHUDO XVHIXO WRROV IRU KHOSLQJ -DYDVFULSW SURJUDPPHUV 
+HUH DUH VRPH RI WKHP 7HVWLQJ IRU VXSSRUW RQ GLIIHUHQW SODWIRUPV ZKLFK EURZVHUV DQG SODWIRUPV VXSSRUW 
ZLWK IHDWXUH KWWS FDQLXVH FRP 7HVWLQJ FRGH SHUIRUPDQFH KWWS MVSHUI FRP LQFOXGHV WKRXVDQGV RI VDYHG 
WHVW FDVHV LQFOXGLQJ WKRVH IRU JUDSKLFV 7DON RQ ZULWLQJ KLJK SHUIRUPDQFH -DYDVFULSW KWWS ZZZ \XLEORJ 
FRP EORJ   YLGHR KSMV   &#38;DQYDV DQG &#38;66 3XVKNDU -RVKL 0RWRUROD 0RELOLW\ +70/ &#38;DQYDV 
'UDZLQJ 7RRO )LOO DQG 6WURNH 6W\OHV /D\RXW DQG 7UDQVIRUPDWLRQV ,PDJH (GLWLQJ $QLPDWLRQ &#38;RPSDULVRQ 
ZLWK 69* &#38;66 3ODQHV LQ 6SDFH $QLPDWLRQV ,PDJH )LOWHUV *HQHUDO 3XUSRVH &#38;66 6KDGHUV   +70/ &#38;DQYDV 
2QFH ZH DUH LQWURGXFHG WR +70/ &#38;66 DQG -DYDVFULSW ZH DUH UHDG\ WR OHDUQ DERXW WKH FDQYDV HOHPHQW 
WKDW ZDV LQWURGXFHG LQ +70/ 7KH FDQYDV HOHPHQW LV WKH HDVLHVW ZD\ WR REWDLQ DQ LQWHUDFWLYH GUDZLQJ VXUIDFH 
IRU D ZHE SDJH RQ D PRGHUQ EURZVHU DQG LV FXUUHQWO\ H[WHQVLYHO\ XVHG IRU FUHDWLQJ ZHE EDVHG JDPHV 6LPLODU 
WR RWKHU EORFN FRQWHQW WDJV OLNH GLY! RU S! WKH FDQYDV! WDJ LGHQWLILHV D UHFWDQJXODU UHJLRQ RI WKH EURZVHU 
ZLQGRZ 6WDQGDUG &#38;66 RSHUDWLRQV OLNH VHWWLQJ WKH ZLGWK KHLJKW EDFNJURXQG FRORU DQG SRVLWLRQ WKDW 
FDQ EH SHUIRUPHG RQ VWDQGDUG FRQWHQW WDJV OLNH GLY! FDQ DOVR EH SHUIRUPHG RQ WKH FDQYDV! WDJ ,Q FDVH 
WKH EURZVHU FDQQRW GLVSOD\ WKH FDQYDV L H ROGHU EURZVHUV ZH GLVSOD\ D IDOOEDFN PHVVDJH FRQWDLQHG ZLWKLQ 
WKH EHJLQQLQJ FDQYDV! DQG HQG FDQYDV! WDJV ... .. ...... ... ... .... ... .... ........... ............ 
.............. s ........S ....s...S ...s ..S ... .... .S.....s.  .. ...... .. .. . .... .... .w.. .... 
.w..... .w.. .. 7KH +70/ GRFXPHQW RQ WKH OHIW SURGXFHV D EOXH VTXDUH FDQYDV VKRZQ RQ WKH ULJKW 7KH FDQYDV 
LV RIIVHW E\  SL[HOV IURP WKH OHIW DQG SL[HOV IURP WKH WRS DV VSHFLILHG E\ LWV VW\OH LQ WKH FDQYDV! 
WDJ DERYH 1RWLFH WKDW ZLWK ROGHU EURZVHUV WKDW GR QRW VXSSRUW +70/ WKH WH[W ³)DOOEDFN FRQWHQW JRHV KHUH´ 
ZLOO EH GLVSOD\HG LQVWHDG RI WKH FDQYDV 8QOLNH WKH RWKHU +70/ WDJV WKH FDQYDV! WDJ RIIHUV D GUDZLQJ FRQWH[W 
WKDW FDQ DFFHVV DQG SDLQW WKH LQGLYLGXDO SL[HOV LQVLGH WKH FDQYDV 3HRSOH IDPLOLDU ZLWK 2SHQ*/ RU 'LUHFW; 
ZLOO EH IDPLOLDU ZLWK WKH QRWLRQ RI D GUDZLQJ FRQWH[W $ GUDZLQJ FRQWH[W LV HVVHQWLDOO\ WKH ³VXUIDFH´ 
RQ ZKLFK \RX FDQ GUDZ SDLQW \RXU SL[HOV 7KH VWDQGDUG PHWKRG IRU DFFHVVLQJ WKH GUDZLQJ FRQWH[W LV WKURXJK 
-DYDVFULSW :H KDYH DGGHG VRPH -DYDVFULSW WR WKH +70/ GRFXPHQW IURP HDUOLHU 7KLV VFULSW TXHULHV WKH '20 
DQG WKHQ FDOOV WKH ³JHW&#38;RQWH[W ´ IXQFWLRQ RI WKH FDQYDV REMHFW KWPO! KHDG! VFULSW W\SH DSSOLFDWLRQ 
MDYDVFULSW ! IXQFWLRQ GUDZ ^  YDU FDQYDV GRFXPHQW JHW(OHPHQW%\,G P\FDQYDV  LI FDQYDV JHW&#38;RQWH[W 
^ YDU FRQWH[W FDQYDV JHW&#38;RQWH[W G     LVVXH GUDZLQJ FRPPDQGV KHUH       ` ` VFULSW! 
 KHDG! ERG\ RQORDG GUDZ ! FDQYDV LG P\FDQYDV ZLGWK  KHLJKW  VW\OH SRVLWLRQ UHODWLYH OHIW S[ WRS 
 S[ EDFNJURXQG FRORU &#38;&#38;&#38;&#38;)) ! )DOOEDFN FRQWHQW JRHV KHUH FDQYDV! ERG\! KWPO! &#38;XUUHQWO\ 
WZR W\SHV RI FRQWH[WV DUH VXSSRUWHG D G FRQWH[W WKDW RIIHUV WKH DELOLW\ WR PDQLSXODWH WKH FDQYDV OLNH 
D ELWPDS DQG D :HE*/ FRQWH[W WKDW RIIHUV WKH DELOLW\ WR GUDZ LQ ' ,Q WKLV VHFWLRQ ZH ZLOO FRYHU WKH ' 
GUDZLQJ FRQWH[W DQG :HE*/ ZLOO EH FRYHUHG LQ D VHSDUDWH VHFWLRQ 7KH FRRUGLQDWH V\VWHP RI WKH ' FRQWH[W 
KDV LWV RULJLQ LQ WKH WRS OHIW FRUQHU RI WKH FRQWHQW RI WKH ER[ 7KH FRRUGLQDWHV RI DQ\ REMHFWV GUDZQ 
LQ WKLV FRQWH[W PXVW OLH LQ WKH UDQJH > FDQYDV ZLGWK@ DQG > FDQYDV KHLJKW@ $Q\ REMHFWV WKDW GR QRW 
OLH ZLWKLQ WKLV UDQJH ZLOO EH LJQRUHG DQG FOLSSHG 7KH +70/ FDQYDV XVHV WKH ³LPPHGLDWH´ PRGH RI GUDZLQJ 
WKH GUDZLQJ FRPPDQGV DUH H[HFXWHG LPPHGLDWHO\ DIWHU EHLQJ LVVXHG DQG WKH V\VWHP VDYHV QR LQIRUPDWLRQ 
DERXW ZKDW ZDV MXVW GUDZQ 7KH RQO\ VWDWH RI WKH FDQYDV VDYHG E\ WKH EURZVHU LV WKH FRORU RI WKH SL[HOV 
LQVLGH WKH FDQYDV /DWHU ZH FRQWUDVW WKLV ZLWK 69* ZKLFK XVHV WKH ³GHFODUDWLYH´ RU ³UHWDLQHG´ JUDSKLFV 
PRGH 8QOLNH WKH FDQYDV HOHPHQW HYHU\ 69* HOHPHQW FDQ EH UHIHUHQFHG WKURXJK WKH '20 DQG HGLWHG ODWHU RQ 
 ,Q WKH UHVW RI WKLV VHFWLRQ ZH ZLOO GHVFULEH VRPH RI WKH IXQFWLRQDOLW\ SRVVLEOH ZLWK WKH +70/ &#38;DQYDV 
$3, WKDW LV SDUWLFXODUO\ UHOHYDQW IRU JUDSKLFV GHYHORSHUV 'UDZLQJ 7RRO +70/ LQWURGXFHG D TXLWH SRZHUIXO 
$3, IRU GUDZLQJ DQG ILOOLQJ SDWKV 8VLQJ WKLV $3, ZH FDQ FRQVWUXFW D YHFWRU GHVLJQ RU VNHWFKLQJ ZHE DSSOLFDWLRQ 
6HH KWWS PXJWXJ FRP VNHWFKSDG IRU DQ H[DPSOH ,I \RX DUH IDPLOLDU ZLWK OHJDF\ GUDZLQJ $3,V OLNH [/LE 
RU YHFWRU GUDZLQJ $3,V OLNH 3RVW6FULSW \RX ZLOO UHFRJQL]H WKH IRUPDW RI WKH SDWK $3, 6LPLODU WR WKRVH 
$3,V ZH PLPLF WKH SHQ DQG SORWWHU LQWHUIDFH ZKHUH HYHU\ QHZ REMHFW LV GUDZQ E\ ILUVW OLIWLQJ DQG PRYLQJ 
WKH GUDZLQJ SHQ WR WKH VWDUW ORFDWLRQ DQG WKHQ WUDFLQJ DORQJ WKH SDWK WR EH GUDZQ FRQWH[W OLQH:LGWK 
  WKH QRVH SRO\OLQH FRQWH[W EHJLQ3DWK FRQWH[W PRYH7R   FRQWH[W OLQH7R   FRQWH[W OLQH7R   FRQWH[W 
VWURNH  FRQYHUW GHJ WR UDGLDQV YDU G 5 0DWK 3,   WKH H\HEURZ FLUFXODU DUFV FRQWH[W EHJLQ3DWK FRQWH[W 
DUF     G 5  G 5 FRQWH[W VWURNH FRQWH[W EHJLQ3DWK FRQWH[W DUF     G 5  G 5 FRQWH[W VWURNH 
    WKH H\HEDOO FLUFOHV FRQWH[W EHJLQ3DWK FRQWH[W DUF     0DWK 3, IDOVH FRQWH[W VWURNH FRQWH[W 
EHJLQ3DWK FRQWH[W DUF     0DWK 3, IDOVH FRQWH[W VWURNH  WKH ORZHU OLS FXELF %H]LHU FXUYH FRQWH[W 
PRYH7R   FRQWH[W EH]LHU&#38;XUYH7R        WKH XSSHU OLS TXDGUDWLF %H]LHU FXUYH FRQWH[W PRYH7R 
   FRQWH[W TXDGUDWLF&#38;XUYH7R     FRQWH[W VWURNH ,Q WKH ILJXUH DERYH WKH FRGH RQ WKH ULJKW SURGXFHV 
WKH OLQH GUDZLQJ RQ WKH OHIW IRU D [  SL[HO FDQYDV 7KLV H[DPSOH GHPRQVWUDWHV WKH DELOLW\ WR GUDZ SRO\OLQHV 
FLUFXODU DUFV LQFOXGLQJ IXOO FLUFOHV FXELF %H]LHU SDWKV DQG TXDGUDWLF %H]LHU SDWKV 1RWLFH WKH XVH RI 
WKH ³EHJLQ3DWK ´ IXQFWLRQ WR LQGLFDWH WKDW D QHZ SDWK LV EHLQJ GUDZQ IRU FDVHV ZKHUH FDOOLQJ ³PRYH7R 
´ ZRXOG EH PRUH FRPSOLFDWHG 3ULRU WR ³EHJLQ3DWK ´ ZH PXVW FDOO WKH ³VWURNH ´ IXQFWLRQ WR UHQGHU WKH SUHYLRXV 
SDWK 7KH SDWK $3, LQFOXGHV WZR JHRPHWULF IXQFWLRQV WKDW DUH FRPPRQO\ QHHGHG IRU JUDSKLFV WDVNV VR WKH\ 
DUH ZRUWK PHQWLRQLQJ KHUH  FRQWH[W EHJLQ3DWK FRQWH[W DUF    0DWK 3, IDOVH FRQWH[W VWURNH LI FRQWH[W 
LV3RLQW,Q3DWK PRXVH; PRXVH< ^ FRQWH[W ILOO6W\OH UHG  FRQWH[W ILOO ` 7UDFNLQJ ZKHQ WKH PRXVH SRLQWHU 
HQWHUV D SDWK E\ XVLQJ WKH LV3RLQW,Q3DWK IXQFWLRQ XVLQJ WKH FXUUHQW PRXVH SRLQWHU FRRUGLQDWHV ,I WKH 
PRXVH LV GHWHFWHG WR EH LQVLGH WKH SDWK ZH ILOO WKH SDWK ZLWK D VROLG UHG FRORU 7KH ³.. .. . .....r.b´ 
IXQFWLRQ UHWXUQV WUXH LI WKH LQSXW [ \ SRVLWLRQ LV LQVLGH WKH SDWK DVVXPLQJ D QRQ ]HUR ZLQGLQJ QXPEHU 
UXOH L H VDPH UXOH XVHG IRU ILOOLQJ WKH SDWK 7KLV FDQ EH XVHIXO IRU LQWHUVHFWLRQ WHVWLQJ HVSHFLDOO\ 
IRU FROOLVLRQ GHWHFWLRQ LQ JDPHV  FRQWH[W VDYH  EOXH FLUFOH FOLS PDVN FRQWH[W EHJLQ3DWK FRQWH[W DUF 
    0DWK 3, FRQWH[W VWURNH LI FRQWH[W LV3RLQW,Q3DWK PRXVH; PRXVH< ^ FRQWH[W FOLS `  JUHHQ FLUFOH 
JHWV FOLSSHG FRQWH[W EHJLQ3DWK FRQWH[W DUF    0DWK 3, FRQWH[W VWURNH6W\OH JUHHQ FRQWH[W VWURNH 
 FRQWH[W UHVWRUH  :KHQ WKH PRXVH SRLQWHU HQWHUV WKH EOXH SDWK ZH VSHFLI\ WKH GUDZLQJ FRQWH[W WR FOLS 
DOO VXEVHTXHQW GUDZLQJ FDOOV DJDLQVW WKH EOXH SDWK 7KH FOLSSLQJ FDQ EH WXUQHG RII E\ FDOOLQJ WKH FRQWH[W 
³UHVWRUH ´ IXQFWLRQ 7KH ³.....b´ PHWKRG LV XVHG WR LQGLFDWH WKDW RQO\ WKH SDUW RI WKH FDQYDV WKDW¶V 
LQVLGH WKH SDWK ZLOO EH UHQGHUHG WR WKH FDQYDV 0DNH VXUH WR LQFOXGH WKH ³.....b´ IXQFWLRQ SULRU WR FDOOLQJ 
WKH ³.....b´ PHWKRG VR WKH FOLSSLQJ FDQ EH WXUQHG RII E\ FDOOLQJ WKH FRUUHVSRQGLQJ ³........b ´ IXQFWLRQ 
)LOO DQG 6WURNH 6W\OHV :KDWHYHU VKDSH KDV EHHQ DGGHG WR WKH SDWK VR IDU ZLOO EH ILOOHG ZKHQ \RX LVVXH 
WKH ILOO FRPPDQG (YHQ RSHQ SDWKV FDQ EH ILOOHG ± IRU WKH SXUSRVH RI WKH ILOO WKH SDWK LV DVVXPHG WR 
EH FORVHG E\ FRQQHFWLQJ WKH ODVW SRLQW WR WKH ILUVW SRLQW 7KH ILOO UXOH IRU FRPSOH[ VHOI LQWHUVHFWLQJ 
 SDWKV LV WKH QRQ ]HUR ZLQGLQJ QXPEHU UXOH ± WKH UHJLRQ RI WKH SDWK WKDW KDV D QRQ ]HUR ZLQGLQJ QXPEHU 
LV ILOOHG 2EYLRXVO\ WKLV LV LQGHSHQGHQW RI WKH RULHQWDWLRQ RI WKH SDWK 7KH VKDSH FDQ EH ILOOHG ZLWK 
D VROLG FRORU D JUDGLHQW RU D SDWWHUQ WLOHG LPDJHV DV VKRZQ EHORZ FRQWH[W ILOO6W\OH EOXH YDU JUDGLHQW 
FRQWH[W FUHDWH5DGLDO*UDGLHQW Z K  Z K Z  JUDGLHQW DGG&#38;RORU6WRS  EOXH JUDGLHQW DGG&#38;RORU6WRS 
 UJED      FRQWH[W ILOO6W\OH JUDGLHQW YDU JUDGLHQW FRQWH[W FUHDWH/LQHDU*UDGLHQW Z K Z  K 
 JUDGLHQW DGG&#38;RORU6WRS ³EOXH JUDGLHQW DGG&#38;RORU6WRS  UJED      FRQWH[W ILOO6W\OH JUDGLHQW 
 YDU LPJ QHZ ,PDJH LPJ VUF VWDU MSJ YDU P\3DWWHUQ FRQWH[W FUHDWH3DWWHUQ LPJ UHSHDW FRQWH[W ILOO6W\OH 
 P\3DWWHUQ 7KH VDPH UXOHV WKDW DSSO\ IRU ³.... .....b´ DOVR DSSO\ IRU ³.... . .....b´ 7KDW LV WKH VWURNH 
UHJLRQ IRU D SDWK FDQ EH ILOOHG ZLWK D VROLG FRORU D JUDGLHQW UDGLDO RU OLQHDU RU D UHSHDWLQJ LPDJH 
SDWWHUQ /D\RXW DQG 7UDQVIRUPDWLRQV $Q\ VKDSH GUDZQ RQ WKH FDQYDV FDQ EH WUDQVIRUPHG LQ RUGHU WR SRVLWLRQ 
LW DQ\ZKHUH ZLWKLQ WKH FDQYDV FRRUGLQDWH VSDFH ,Q WKLV ZD\ WKH &#38;DQYDV $3, FDQ EH XVHG WR OD\RXW ' 
JUDSKLFDO HOHPHQWV RQ WKH VFUHHQ  /HW XV FRQVLGHU WKH WDVN RI W\SLQJ WKH OHWWHUV $ % &#38; ' DORQJ D 
]LJ]DJ SDWK DV VKRZQ LQ WKH ILJXUH DERYH :KLOH ZH FRXOG XVH WKH SDWK $3, WR GUDZ WKH OHWWHUV ZH¶OO WDNH 
WKH VLPSOHU RSWLRQ RI XVLQJ WKH WH[W DSL L H WKH µ.... ....b¶ RU µ.... . ....b¶ IXQFWLRQV EXLOW LQWR 
WKH ' GUDZLQJ FRQWH[W 7KH WUDQVIRUPDWLRQV DIIHFW WKH FRRUGLQDWH V\VWHP RI WKH GUDZLQJ FRQWH[W 6HH WKH 
FRGH EORFN DERYH IRU WKH OHWWHU D W\SLFDO FDOO WR WUDQVODWH DQG URWDWH WKH OHWWHU $ LQ RUGHU WR SODFH 
LW LQ WKH SURSHU SRVLWLRQ ([FHSW LQ IHZ FDVHV WKH RUGHU RI WKH WUDQVIRUPDWLRQV LV LPSRUWDQW )RU H[DPSOH 
 FDOOLQJ WKH URWDWH IXQFWLRQ EHIRUH WKH WUDQVODWH IXQFWLRQ ZRXOG QRW KDYH URWDWHG WKH OHWWHU LQ SODFH 
DV DERYH ([SHULHQFHG JUDSKLFV GHYHORSHUV ZLOO VHH WKH VLPLODULW\ EHWZHHQ WKLV WUDQVIRUPDWLRQ PRGHO DQG 
WKDW SUHVHQW LQ SUHYDOHQW JUDSKLFV $3,V OLNH 2SHQ*/ RU 'LUHFW; 6LPLODU WR WKRVH JUDSKLFV $3, ZH FDQ 
FRQFDWHQDWH WKH HQWLUH WUDQVIRUPDWLRQ DQG VSHFLI\ LW DV RQH KRPRJHQHRXV [ PDWUL[ YLD WKH ³... .... 
.b´ DQG ³... .. .... .b´ IXQFWLRQV LQ WKH &#38;DQYDV $3, %RWK IXQFWLRQV WDNH VL[ DUJXPHQWV WKH QXPEHU 
RI GHJUHHV RI IUHHGRP DYDLODEOH IRU ' WUDQVIRUPV 7KH GLIIHUHQFH EHWZHHQ WKH ³... .... .b´ DQG ³... .. 
.... .b´ IXQFWLRQ LV WKDW WKH IRUPHU FRQFDWHQDWHV WKH VSHFLILHG WUDQVIRUPDWLRQ WR WKH FXUUHQW WUDQVIRUPDWLRQ 
ZKLOH WKH ODWWHU VHWV WKH VSHFLILHG WUDQVIRUPDWLRQ DV WKH RQO\ WUDQVIRUPDWLRQ ZH ORVH WKH KLVWRU\ RI 
WKH SUHYLRXV WUDQVIRUPV  <RX PD\ KDYH QRWLFHG FDOOV WR ³.....b´ DQG ³........b ´ 7KH ³VDYH´ IXQFWLRQ 
SXVKHV WKH FXUUHQW GUDZLQJ FRQWH[W VWDWH LQFOXGLQJ WKH WUDQVIRUPDWLRQ DORQJ ZLWK VRPH RWKHU VWDWH YDULDEOHV 
RQWR D VWDFN 6XEVHTXHQW FDOOV WR FKDQJH WKH VWDWH H J YLD DGGLWLRQDO WUDQVIRUPDWLRQV ZLOO FRQFDWHQDWH 
WR WKH FXUUHQW VWDWH EXW WKH RULJLQDO VWDWH FDQ EH UHFRYHUHG E\ WKH ³........b´ IXQFWLRQ WKDW ZLOO SRS 
WKH WRS RI WKH VWDFN DQG VHW WKH FXUUHQW VWDWH WR WKH SRSSHG RII YDOXH $JDLQ H[SHULHQFHG JUDSKLFV GHYHORSHUV 
ZLOO QRWLFH WKH VLPLODULW\ EHWZHHQ ³.....b´ DQG .. ... ......b DQG ³........b´ DQG .. .. ......b LQ 2SHQ*/ 
 ,PDJH (GLWLQJ 7KH &#38;DQYDV $3, DOORZV UDQGRP DFFHVV WR WKH E\WH OHYHO 5*%$ YDOXHV RI WKH LQGLYLGXDO 
SL[HOV ZLWKLQ WKH FRRUGLQDWH VSDFH RI WKH FDQYDV 7KHUHIRUH ZH FDQ VHW WKH FRORUV RI DQ\ SDUW RI WKH FDQYDV 
RQ D SL[HO E\ SL[HO OHYHO :H FDQ DOVR ORDG DUELWUDU\ LPDJHV LQWR WKH FDQYDV DQG PDQLSXODWH WKHLU SL[HO 
YDOXHV *LYHQ WKHVH IXQFWLRQV ZH FDQ LPSOHPHQW D FRPSUHKHQVLYH VHW RI LPDJH HGLWLQJ IHDWXUHV XVLQJ WKH 
FDQYDV $3, +HUH DUH VRPH FRGH VQLSSHWV WKDW \RX ZLOO QHHG LQ RUGHU WR SHUIRUP DQ\ LPDJH HGLWLQJ IXQFWLRQDOLW\ 
/RDGLQJ DQG GLVSOD\LQJ LPDJHV ... . ... ..... . .. ....bS . ... .....S... . u.......... .........US 
. ... .....S. .... . .. .... .b . .. ....S.... ..... ... .....r.r.bS . ,Q WKH DERYH FRGH ZH ILUVW FUHDWH 
DQ LPDJH REMHFW DQG VSHFLI\ WKH SDWK WR WKH DFWXDO LPDJH ILOH $OVR ³FRQWH[W´ LV WKH VWDQGDUG &#38;DQYDV 
' FRQWH[W WKH ³ ´ LQ WKH GUDZ,PDJH FDOO VSHFLILHV WKH WRS OHIW FRUQHU RI WKH LPDJH DW WKH FDQYDV RULJLQ 
LQ WKLV FDVH DQG WKH LPDJH LV GUDZQ RQO\ ZKHQ LW LV IXOO\ ORDGHG L H LV LQ WKH RQORDG HYHQW KDQGOHU 
IRU WKH LPDJH REMHFW $FFHVV SL[HOV 1RZ WKDW WKH LPDJH LV GUDZQ RQ WKH FDQYDV ZH JHW D SRLQWHU WR LWV 
SL[HOV $W DQ\ WLPH ZH FDQ REWDLQ D ' DUUD\ RI SL[HOV WKDW FRQWDLQ WKH FXUUHQW FRORU YDOXHV RI WKH FDQYDV 
HOHPHQW E\ XVLQJ WKH ... ... ... IXQFWLRQ :H FDQ JHW DOO RU D VXEVHW RI WKH FDQYDV SL[HOV 7KH UHWXUQHG 
' DUUD\ RI SL[HOV LV RUGHUHG OHIW WR ULJKW IROORZHG E\ WRS WR ERWWRP ... . ... ... . .. ....S... ... 
.....r.r.. ........r.. .........bS &#38;KDQJLQJ SL[HOV 2QFH ZH KDYH DFFHVV WR WKH SL[HO DUUD\ ZH FDQ 
PRGLI\ LWV YDOXHV 7KH IROORZLQJ H[DPSOH FRQYHUWV WKH FRORUHG SL[HOV LQWR D JUD\VFDOH UHSUHVHQWDWLRQ ... 
...... . . ... ...S....S... .... . . .r . ......S.. ...S . . S . .. .b . ww.... . ... ... . ... .... 
. ... .... ... .... . ............S.b . ..............S..b . ..............S..bS ........ . . ....S 
 ........... . ....S  ........... . ....S . 1RWH WKDW HDFK SL[HO DFWXDOO\ WDNHV IRXU VSRWV LQ WKH 
SL[HO DUUD\ RQH IRU HDFK RI 5HG *UHHQ %OXH DQG $OSKD YDOXHV ZKLFK LV ZK\ ZH LQFUHPHQW RXU DUUD\ LWHUDWRU 
E\ 8SGDWLQJ WKH FDQYDV $IWHU PRGLI\LQJ WKH SL[HOV ZH QHHG WR XSGDWH WKH LPDJH GLVSOD\HG E\ WKH FDQYDV 
:H GR VR E\ UHSODFLQJ WKH FXUUHQW YDOXH RI WKH SL[HOV E\ WKH PRGLILHG YDOXH .. ....S... ... ..... ... 
...r .r .bS $V EHIRUH ZH FDQ SRVLWLRQ WKH QHZ SL[HOV DQ\ZKHUH ZLWKLQ WKH FDQYDV DQG LQ WKH H[DPSOH DERYH 
ZH KDYH SRVLWLRQHG LW DW WKH FDQYDV RULJLQ $ FRPPRQ LPDJH HGLWLQJ RSHUDWLRQ SHUIRUPHG XVLQJ WKH &#38;DQYDV 
$3, LV WKH LPSOHPHQWDWLRQ RI LPDJH ILOWHUV 6HH WKH IROORZLQJ OLQN IRU D GHPR RI VRPH &#38;DQYDV $3, LPDJH 
ILOWHUV LQFOXGLQJ VRPH FRQYROXWLRQ VKDUSHQ /DSODFH HWF ILOWHUV KWWS ZZZ KWPO URFNV FRP HQ WXWRULDOV 
FDQYDV LPDJHILOWHUV ,I \RXU LPDJH HGLWLQJ DSSOLFDWLRQ LV OLPLWHG WR ILOWHUV ZKHUH \RX ZLOO SHUIRUP WKH 
VDPH RSHUDWLRQ IRU HYHU\ SL[HO H J \RX ZLVK WR SHUIRUP D IL[HG VWHQFLO FRQYROXWLRQ RYHU WKH LPDJH ZH 
UHFRPPHQG \RX OLPLW WKH XVH RI +70/ &#38;DQYDV RQO\ IRU SURWRW\SLQJ DQG XVH :HE*/ RU &#38;66 ILOWHUV 
GHVFULEHG ODWHU IRU WKH DFWXDO UHOHDVH FRGH 7KH ODWWHU RSWLRQ ZLOO SUHYHQW WKH H[SHQVLYH -DYDVFULSW ORRS 
RYHU DOO WKH SL[HOV DQG FDQ LQVWHDG EH H[HFXWHG LQ SDUDOOHO DV *38 VKDGHUV $QLPDWLRQ 5HPHPEHU WKDW WKH 
+70/ FDQYDV RSHUDWHV LQ ³LPPHGLDWH´ PRGH DQG GRHV QRW VDYH DQ\ LQIRUPDWLRQ DERXW WKH REMHFWV GUDZQ RQ 
LW *LYHQ WKLV LPPHGLDWH PRGH DQLPDWLRQ LV SHUIRUPHG E\ PRGLI\LQJ WKH SRVLWLRQ RI WKH DQLPDWHG REMHFW 
DQG VLPSO\ UHGUDZLQJ WKH UHJLRQ RI WKH FDQYDV WKDW KDV FKDQJHG ,Q PRVW FDVHV WKH PRGLILHG UHJLRQ LQFOXGHV 
WKH HQWLUH FDQYDV KWWS ERPRPR FRP KWWS ZZZ EOREVDOODG VH 3HUIRUPDQFH ,PSURYHPHQW 7UDGLWLRQDOO\ WKH 
FRGH IRU DQLPDWLRQ XVLQJ -DYDVFULSW XWLOL]HV D WLPHU ZKHUH VRPH FRGH IRU XSGDWLQJ WKH SRVLWLRQ RI HOHPHQWV 
LQ WKH EURZVHU LV LQYRNHG DW UHJXODU LQWHUYDOV XVLQJ WKH ³... . ............. .r... ...... .......b´ 
IXQFWLRQ FDOO 7KH SUREOHP ZLWK WKDW DSSURDFK LV WKDW FRPSOH[ DQLPDWLRQV FDQ VORZ GRZQ WKH EURZVHU DQG 
SURGXFH DQ XQGHVLUDEOH XVHU H[SHULHQFH 7KH QHZ SUHIHUUHG PHWKRG LV WR XVH WKH QHZ ........ . .... ... 
. IXQFWLRQ WKDW LQGLFDWHV WKDW ZH ZLVK WR DQLPDWH WKH FRQWHQWV RI WKH EURZVHU ZLQGRZ DW +] LGHDOO\ 
RU DV IDVW DV SRVVLEOH IRU WKH EURZVHU LI QRW +] 7KLV DOORZV WKH EURZVHU WR RSWLPL]H FRGH IRU XV DQG 
DOVR SUHYHQWV WKH KRVW FRPSXWHU IURP EHLQJ XQQHFHVVDULO\ VORZHG GRZQ GXH WR RXU DQLPDWLRQ 7KHUHIRUH 
ZH FDQ XVH WKH IROORZLQJ FRGH VWUXFWXUH RULJLQDOO\ IURP KWWS SDXOLULVK FRP UHTXHVWDQLPDWLRQIUDPH IRU 
VPDUW DQLPDWLQJ ww.... . ... ....... ....... . .... .. . .. .... ....... ...S....... . .... .. . . .. 
...S... .. ...... . .... .. . YY .. ...S .. ...... . .... .. . YY .. ...S . ...... . .... .. .S ww. 
.. . ... ....... . .... .. . .. .... . ... .. ... ....... .. .... . . ... . ...... .b . ww....... .... 
.. .... .. ...... ... .. ... .. .. . .. ...S....... . .... .. ... . ... . ...r .. ...bS ww....... ... 
. .... .. ... .. ...... .... ... ... ... . . . ... ... ... .b .... . ....bS . 7KH IROORZLQJ OLQN GHVFULEHV 
WKH XVH RI ....... . .... .. . DQG RWKHU KLQWV IRU LPSURYLQJ SHUIRUPDQFH RQ +70/ &#38;DQYDV KWWS ZZZ 
KWPO URFNV FRP HQ WXWRULDOV FDQYDV SHUIRUPDQFH  &#38;RPSDULVRQ ZLWK 69* 6RPH UHDGHUV ZLOO KDYH REVHUYHG 
WKH VLPLODULW\ EHWZHHQ WKH SDWK GUDZLQJ DQG ILOOLQJ FDSDELOLWLHV RI WKH +70/ &#38;DQYDV DQG WKRVH RI 
WKH 6FDODEOH 9HFWRU *UDSKLFV 69* VSHFLILFDWLRQ FRPPRQO\ LPSOHPHQWHG RQ PRGHUQ ZHE EURZVHUV ,Q PDQ\ DVSHFWV 
WKH +70/ &#38;DQYDV LV VLPLODU WR 69* ,Q WKLV VXE VHFWLRQ ZH SRLQW RXW WKH GLIIHUHQFHV EHWZHHQ WKH WZR 
HVSHFLDOO\ WKRVH WKDW DUH UHOHYDQW IRU JUDSKLFV SURJUDPPHUV 7KH PDLQ GLIIHUHQFH EHWZHHQ WKH +70/ &#38;DQYDV 
DQG 69* LV WKHLU UHQGHULQJ PRGH +70/ &#38;DQYDV XVHV LPPHGLDWH PRGH ZKLOH 69* XVHV GHFODUDWLYH PRGH 
%\ XVLQJ GHFODUDWLYH PRGH ZH FDQ DFFHVV LQGLYLGXDO 69* HOHPHQWV WKURXJK WKH '20 DQG PRGLI\ WKHLU SURSHUWLHV 
VXFK DV SRVLWLRQ FRORU YLVLELOLW\ ZLWKRXW QHHGLQJ WR UH GUDZ WKH GUDZLQJ DUHD 7KH UH GUDZ LV KDQGOHG 
E\ WKH EURZVHU ,Q WKLV VHQVH DQ 69* HOHPHQW LV VLPLODU WR DQ +70/ HOHPHQW  ZH FDQ FKDQJH WKH LQGLYLGXDO 
HOHPHQW SURSHUWLHV G\QDPLFDOO\ DQG WKH ZHE SDJH LV UH GUDZQ DXWRPDWLFDOO\ 7KH GHFODUDWLYH PRGH RI 69* 
DOVR LQFOXGHV DQRWKHU IHDWXUH JURXSLQJ 9HFWRU DUWZRUN FDQ EH FRPELQHG LQWR RQH JURXSV VHYHUDO RI ZKLFK 
FDQ EH IXUWKHU FRPELQHG LQWR DQRWKHU JURXS DQG VR RQ 6XFK D KLHUDUFKLFDO RUJDQL]DWLRQ RU DUWZRUN LQ VHPDQWLFDOO\ 
UHOHYDQW JURXSV DOORZV XV WR VSHFLI\ UHJLRQV RI LQIOXHQFH RI ORFDO WUDQVIRUPDWLRQV XVHIXO IRU DGGLQJ 
GHWDLOV WR H[LVWLQJ DQLPDWHG YHFWRU DUWZRUN IRU H[DPSOH  $QRWKHU GLIIHUHQFH EHWZHHQ 69* DQG +70/ &#38;DQYDV 
LV WKH XVH RI ILOWHU HIIHFWV LQ 69* :KLOH 69* GRHV QRW RIIHU GLUHFW E\WH OHYHO DFFHVV WR SL[HOV OLNH 
+70/ &#38;DQYDV GRHV D IL[HG VHW RI LPDJH ILOWHUV FDQ EH DSSOLHG GLUHFWO\ WR WKH 69* HOHPHQWV ZLWKRXW 
QHHGLQJ WR ZULWH WKHP LQ VORZHU -DYDVFULSW 6HH WKLV VLWH IRU DQ H[DPSOH KWWS LH PLFURVRIW FRP WHVWGULYH 
*UDSKLFV KDQGV RQ FVV KDQGV RQBVYJ ILOWHU HIIHFWV KWP :H VKDOO H[SORUH 69* ILOWHU HIIHFWV LQ PRUH GHWDLO 
LQ WKH QH[W VHFWLRQ RQ &#38;66 ZKHQ ZH GHVFULEH &#38;66 VKDGHUV &#38;66 5HPHPEHU WKDW WKH DIWHU EXLOGLQJ 
WKH '20 WUHH WKH EURZVHU SHUIRUPV D OD\RXW SURFHVV )RU WKH SXUSRVH RI WKLV FRXUVH WKLQN RI WKH OD\RXW 
DV WKH SODFHPHQW RI UHFWDQJXODU UHJLRQV RQH IRU HDFK +70/ EORFN HOHPHQW ZLWKLQ WKH ' VSDFH RI WKH EURZVHU 
ZLQGRZ $V PHQWLRQHG EHIRUH WKH UXOHV XVHG E\ WKH EURZVHU IRU GLVSOD\LQJ HDFK YLVXDO HOHPHQW H J WKH EDFNJURXQG 
FRORU RI WKH UHFWDQJXODU UHJLRQ DUH SURYLGHG E\ WKH &#38;66 VW\OH DVVRFLDWHG ZLWK WKDW HOHPHQW ,Q WKLV 
VHFWLRQ ZH GHVFULEH VRPH RI WKH QHZ IHDWXUHV LQ &#38;66 WKDW DUH UHOHYDQW IRU JUDSKLFV SURJUDPPHUV 7KHVH 
QHZ IHDWXUHV VLJQLILFDQWO\ LPSURYH WKH DELOLW\ WR G\QDPLFDOO\ FKDQJH WKH GLVSOD\ VW\OHV RI +70/ HOHPHQWV 
PRVW QRWDEO\ WKH SRVLWLRQ DQG RULHQWDWLRQ RI WKRVH HOHPHQWV &#38;66 DOORZV XV WR HDVLO\ SURGXFH DQLPDWLRQV 
ZLWKRXW QHHGLQJ DQ\ -DYDVFULSW FRGH WR XSGDWH SRVLWLRQV SURJUDPPDWLFDOO\ 6LQFH WKLV WUDQVIHUV WKH DQLPDWLRQ 
IXQFWLRQDOLW\ IURP QRQ QDWLYH -DYDVFULSW FRGH WR QDWLYH DQG FDUHIXOO\ RSWLPL]HG EURZVHU E\WH FRGH LW 
XVXDOO\ SURGXFHV VLJQLILFDQW SHUIRUPDQFH LPSURYHPHQWV  6LPLODU WR WKH +70/ &#38;DQYDV VHFWLRQ ZH LQWURGXFH 
WKH JUDSKLFV FDSDELOLWLHV RI &#38;66 E\ VKRZLQJ KRZ &#38;66 FDQ EH XVHG WR LPSOHPHQW IXQFWLRQDOLW\ FRPPRQO\ 
QHHGHG E\ JUDSKLFV DSSOLFDWLRQV :H VKDOO FRQVLGHU WZR DSSOLFDWLRQV SODQHV LQ VSDFH DQG LPDJH ILOWHUV 
 3ODQHV LQ 6SDFH 7KH WHUP ³SODQHV LQ VSDFH´ UHIHUV WR WKH DELOLW\ WR SODFH ' SODQDU SRO\JRQV DW DQ\ SRVLWLRQ 
DQG RULHQWDWLRQ LQ ' VSDFH 3UHYLRXVO\ ZH FRXOG SODFH DQ\ +70/ EORFN HOHPHQW DW DQ\ SRVLWLRQ ZLWKLQ WKH 
' ZLQGRZ 7\SLFDOO\ WKLV ZDV GRQH E\ FKDQJLQJ WKH ³OHIW´ DQG ³WRS´ VW\OH DWWULEXWHV IRU WKDW HOHPHQW 
&#38;RQVLGHU WKH WDVN RI SODFLQJ D GLY! HOHPHQW DW D KRUL]RQWDO GLVWDQFH  SL[HOV DQG D YHUWLFDO GLVWDQFH 
RI SL[HOV IURP WKH WRS OHIW FRUQHU RI WKH EURZVHU ZLQGRZ :H XVH WKH IROORZLQJ FRGH ... .. ...... .... 
............s .....S ......s .....S ....... s ........S ....s...S ...s ..S ... .... .S.....s.  .. .... 
.w.... .w..... .w.. ..  :H FDQ DGG D ' HIIHFW WR WKH ' HOHPHQW E\ PDNLQJ LW URWDWH E\ GHJUHHV DERXW 
WKH YHUWLFDO < D[LV ,I ZH XVH D EURZVHU XVLQJ WKH :HENLW OD\RXW HQJLQH H J *RRJOH &#38;KURPH $SSOH 6DIDUL 
 ZH XVH WKH IROORZLQJ FRGH ... .. ...... .... ............s .....S ......s .....S ....... s ........S 
....s...S ...s ..S S... ..S... .... s ...... ......bS ... .... .S.....s.  .. .... .w.... .w..... .w.. 
.. 7KH WUDQVIRUPV FDQ EH FRQFDWHQDWHG ... .. ...... .... ............s .....S ......s .....S ....... 
s ........S ....s...S ...s ..S S... ..S... .... s ...... ......b ...... ......bS ... .... .S.....s. 
 .. .... .w.... .w..... .w.. .. :H FDQ VSHFLI\ PRVW FRPPRQ W\SHV RI ' WUDQVIRUPDWLRQV URWDWH WUDQVODWH 
VFDOH VNHZ DQG SHUVSHFWLYH RQ DQ\ EORFN HOHPHQW :H FDQ HYHQ VSHFLI\ GLUHFWO\ WKH [ WUDQVIRUPDWLRQ PDWUL[ 
WR EH DSSOLHG WR WKH HOHPHQW 6HH KWWS GHY Z RUJ FVVZJ FVV WUDQVIRUPV IRU WKH $3, FXUUHQWO\ SURSRVHG 
IRU &#38;66 WUDQVIRUPV $OVR VHH KWWS GHVDQGUR JLWKXE FRP GWUDQVIRUPV IRU DQ H[FHOOHQW H[SODQDWLRQ RI 
WKH &#38;66 WUDQVIRUPV LQ GHSWK 7KH EHQHILW RI DSSO\LQJ ' WUDQVIRUPV WR D EORFN HOHPHQW LV WKDW DIWHU 
WUDQVIRUPLQJ WKH HOHPHQW WKH EURZVHU FRQWLQXHV WR LQWHUDFW ZLWK FRQWHQWV RI WKH HOHPHQW DV EHIRUH 7H[W 
LQ WKH HOHPHQW UHPDLQV VHOHFWDEOH OLQNV VWLOO ZRUN DQG YLGHRV RU LPDJHV DUH GLVSOD\HG ZLWK FRUUHFW WUDQVIRUPDWLRQV 
7KLV PDNHV WKH &#38;66 WUDQVIRUPV YHU\ XVHIXO IRU FUHDWLQJ ' LQWHUDFWLYH XVHU LQWHUIDFHV 7KH OLQNV EHORZ 
JLYH VRPH H[DPSOHV $ VLPSOH LPDJH IOLS RQ PRXVH RYHU KWWS ZZZ ZHENLW RUJ EORJ ILOHV G WUDQVIRUPV LPDJH 
IOLS KWPO 0RUH DGYDQFHG LPDJH HIIHFWV XVLQJ &#38;66 KWWSV GHYHORSHU PR]LOOD RUJ HQ 86 GHPRV GHWDLO 
G LPDJH WUDQVLWLRQV ODXQFK 3ODFLQJ ZHESDJH HOHPHQWV LQ ' VSDFH KWWS ZZZ ZHENLW RUJ EORJ ILOHV G WUDQVIRUPV 
PRUSKLQJ FXEHV KWPO 1RWH <RX PD\ EH ZRQGHULQJ ZK\ ZH QHHGHG WR XVH WKH SUHIL[ ³S... ..S´ IRU VW\OH DWWULEXWHV 
OLNH ... .... 7KH QHZHU &#38;66 VW\OHV DUH QRW \HW ILQDOL]HG E\ WKH : &#38; ,Q RUGHU WR DOORZ EURZVHUV 
WR VXSSRUW WKH QRQ ILQDOL]HG IHDWXUH RU DQ LQFRPSOHWH LPSOHPHQWDWLRQ RI WKH IHDWXUH LQGLYLGXDO EURZVHUV 
VXSSRUW WKH SUHIL[HG IRUPV RI WKH VW\OH DWWULEXWH %URZVHUV ZLWK WKH :HENLW OD\RXW HQJLQH H J *RRJOH &#38;KURPH 
$SSOH 6DIDUL ZLOO UHVSHFW WKH ... .... DWWULEXWH ZLWK WKH ³S... ..S´ SUHIL[ 6LPLODUO\ 0R]LOOD EDVHG )LUHIR[ 
QHHGV WKH ³S ..S´ SUHIL[ ,QWHUQHW ([SORUHU WKH ³S .S´ SUHIL[ DQG 2SHUD WKH ³S.S´ SUHIL[ 7KLV LV D WHPSRUDU\ 
VROXWLRQ XQWLO WKH &#38;66 VSHFLILFDWLRQ LV ILQDOL]HG E\ WKH : &#38; DQG XQLYHUVDOO\ DGRSWHG E\ DOO EURZVHUV 
8QWLO WKHQ \RX QHHG WR VSHFLI\ DOO WKH SUHIL[HV IRU WKH QHZ VW\OH DWWULEXWHV VR \RXU FRGH PD\ UXQ RQ 
DOO EURZVHUV <RX FDQ XVH WRROV OLNH ³3UHIL[ IUHH´ KWWS OHDYHURX JLWKXE FRP SUHIL[IUHH WR SURGXFH WKH 
SUHIL[HG YHUVLRQV RI WKH &#38;66 DWWULEXWHV DXWRPDWLFDOO\ DW VFULSW UXQWLPH $QLPDWLRQV ,Q WKH GHPRV DERYH 
HOHPHQWV SODFHG E\ &#38;66 DUH DQLPDWHG :H FDQ FRQWURO WKH PDQQHU LQ ZKLFK WKH\ PRYH IURP RQH SRVLWLRQ 
WR WKH RWKHU $ VLPSOH PHWKRG WR EULQJ DERXW DQLPDWLRQ LV YLD &#38;66 WUDQVLWLRQV )RU H[DPSOH RXU H[DPSOH 
DERYH IRU URWDWLQJ WKH ..... HOHPHQW FDQ EH DQLPDWHG E\ DSSO\LQJ D WUDQVLWLRQ RQ WKH ... .... VW\OH DWWULEXWH 
7KH ... .... VW\OH DWWULEXWH LWVHOI LV PRGLILHG ZKHQ WKH XVHU FOLFNV RQ WKH HOHPHQW ... .. ...... .... 
............s .....S ......s .....S ....... s ........S ....s...S ...s ..S... .... .S.....s.  S .... 
.w.... .w..... .w.. .. ,Q WKH DERYH H[DPSOH ZH VHW WKH VW\OH RI WKH ³S... ..S... ..... ´ WR EH ³S... 
..S ... .... .. ....S. ´ $V \RX FDQ JXHVV WKLV PHDQV WKDW ZH DUH DGGLQJ D WUDQVLWLRQ WR WKH S... ..S... 
.... DWWULEXWH WKDW LV VHFRQGV ORQJ DQG ZH HDVH LQ IURP WKH ROG YDOXH WR WKH QHZ RQH :H FDQ VSHFLI\ 
WUDQVLWLRQV RQ DOO QRQH VRPH RI WKH VW\OH DWWULEXWHV RI WKH HOHPHQW IRU DQ\ GXUDWLRQ DQG ZLWK GLIIHUHQW 
WLPLQJ IXQFWLRQV HDVH LQ HDVH RXW OLQHDU HWF 7KH FRPSOHWH VSHFLILFDWLRQ IRU WKH WUDQVLWLRQV LV SURYLGHG 
KHUH KWWS ZZZ Z RUJ 75 FVV WUDQVLWLRQV  ,I DQLPDWLRQV XVLQJ WUDQVLWLRQV DUH QRW VXIILFLHQW ZH FDQ DOVR 
FUHDWH H[SOLFLW DQLPDWLRQ RI WKH VW\OH DWWULEXWHV XVLQJ NH\IUDPHV  ... .. ............ ..........w..... 
.S... ..S ..... .. ...... .  .. .   S... ..S... .... s .........r.r.b ...... .....bS  .  ... 
.   S... ..S... .... s .........S.r.S.r.b ...... ......bS  .  ... .   S... ..S... .... s .........S.r.S.r.b 
...... .S.....bS  .    .... .   S... ..S... .... s .........r.r.b ...... .S...bS  .  ..w...... 
.w..... ...... .... ............s ....S ......s ....S ....... s ........S ....s...S ...s ..S... .... 
.S.....s.  S S... ..S. . .... S........ S... .s . .. ...S S... ..S. . .... S.. . .S.. .... s .. ...S.. 
.w.... .w..... .w.. .. ,Q WKH DERYH H[DPSOH ZH ILUVW FUHDWH VRPH NH\IUDPHV DW    DQG  RI WKH DQLPDWLRQ 
GXUDWLRQ $W HDFK NH\IUDPH ZH VSHFLI\ YDOXHV RI WKH VW\OH DWWULEXWHV WKDW QHHG WR FKDQJH 7KH DQLPDWLRQ 
NH\IUDPHV DUH WKHQ VSHFLILHG DV WKH YDOXH RI WKH DQLPDWLRQ QDPH DWWULEXWH DORQJ ZLWK WKH GXUDWLRQ RI 
HDFK F\FOH WKH QXPEHU RI FRPSOHWH F\FOHV LQILQLWH IRU FRQWLQXRXV ORRSLQJ DQLPDWLRQ ZKHWKHU WR VWHS IRUZDUGV 
EDFNZDUGV RU IRZDUG DQG EDFNZDUG DQG WKH WLPLQJ IXQFWLRQ HDVH LQ HDVH RXW OLQHDU HWF 7KH FRPSOHWH VSHFLILFDWLRQV 
IRU &#38;66 DQLPDWLRQV LV DYDLODEOH KHUH KWWS ZZZ Z RUJ 75 FVV DQLPDWLRQV  $ ILQDO QRWH RQ WKH ³SODQHV 
LQ VSDFH´ IXQFWLRQDOLW\ LV WKDW VRRQ EURZVHUV ZLOO EH DEOH WR VXSSRUW FXUYHG SODQHV LQ VSDFH IRU SODFLQJ 
EORFN WDJV &#38;XUYHG SODQHV ZLOO EH SRVVLEOH WKURXJK WKH XVH RI &#38;66 VKDGHUV ZKLFK DUH GHVFULEHG 
LQ WKH QH[W VHFWLRQ 0RUH UHVRXUFHV *HQHUDO SXUSRVH &#38;66 DQLPDWLRQ KWWS WLQ\XUO FRP FVVZDON 7UDGLWLRQDO 
DQLPDWLRQ SULQFLSOHV LPSOHPHQWHG DV &#38;66 KWWS FRGLQJ VPDVKLQJPDJD]LQH FRP   WKH JXLGH WR FVV DQLPDWLRQ 
SULQFLSOHV DQG H[DPSOHV  ,PDJH )LOWHUV 6LPLODU WR WKH +70/ &#38;DQYDV ZH FDQ LPSOHPHQW LPDJH ILOWHUV 
XVLQJ &#38;66 7KH LPDJH ILOWHUV LQ &#38;66 FDOOHG ³ILOWHU HIIHFWV´ DUH HVVHQWLDOO\ WKH VDPH DV WKRVH 
IRU 69* D IL[HG VHW RI LPDJH ILOWHUV DSSOLHG WR HYHU\ SL[HO RI WKH LPDJH 7KH SURSRVHG VSHFLILFDWLRQ IRU 
&#38;66 ILOWHU HIIHFWV LV KWWSV GYFV Z RUJ KJ );7) UDZ ILOH WLS ILOWHUV LQGH[ KWPO /LNH 69* &#38;66 
ILOWHU HIIHFWV VXSSRUW VRPH KDUG FRGHG ILOWHU IXQFWLRQV H J ³EULJKWHQ´ ³VHSLD´ ³JUD\VFDOH´ WKDW DFFHSW 
D IHZ XVHU SDUDPHWHUV 7KH V\QWD[ LV DV VLPSOH DV VSHFLI\LQJ WKH ...... DWWULEXWH OLNH S... ..S......s 
........b ...........bS 1RWH WKDW PXOWLSOH ILOWHU IXQFWLRQV FDQ EH VSHFLILHG DQG WKH RUGHU RI WKH ILOWHU 
IXQFWLRQV LV LPSRUWDQW  7KH IROORZLQJ OLQN VKRZV DOO WKH ILOWHUV LQ DFWLRQ KWWS KWPO GHPRV DSSVSRW 
FRP VWDWLF FVV ILOWHUV LQGH[ KWPO 6LPLODU WR 69* ILOWHUV WKH EHQHILW RI XVLQJ ILOWHU HIIHFWV WKURXJK 
&#38;66 LQVWHDG RI +70/ &#38;DQYDV LV LPSURYHG SHUIRUPDQFH ZH GR QRW QHHG WR LWHUDWH RYHU HYHU\ SL[HO 
LQ WKH LPDJH XVLQJ -DYDVFULSW DQG FDQ H[SORLW WKH SDUDOOHO FRPSXWDWLRQ DELOLW\ RI PRGHUQ *38V )RU WKLV 
UHDVRQ LI \RX ZLVK WR XVH RQH RI WKH ILOWHU HIIHFWV LQ WKH VSHFLILFDWLRQV DQG &#38;66 ILOWHU HIIHFWV 
DUH VXSSRUWHG E\ \RXU WDUJHW EURZVHU ZH UHFRPPHQG \RX XVH &#38;66 ILOWHU HIIHFWV LQVWHDG RI -DYDVFULSW 
FRGHG +70/ &#38;DQYDV LPDJH PDQLSXODWLRQ *HQHUDO 3XUSRVH &#38;66 6KDGHUV 7KH H[FLWLQJ GHYHORSPHQW RI 
&#38;66 ILOWHU HIIHFWV LV WKDW LQ DGGLWLRQ WR WKH IL[HG ILOWHUV WKH &#38;66 ILOWHU HIIHFWV VSHFLILFDWLRQ 
DOORZV XV WR VSHFLI\ D FXVWRP VKDGHU DV D ILOWHU 7KLV LV H[FLWLQJ EHFDXVH LW KDV WKH SRWHQWLDO WR RSHQ 
XS WKH PDVVLYH SDUDOOHO FRPSXWDWLRQ DYDLODEOH RQ PRVW *38V IRU JHQHUDO SXUSRVH FRPSXWLQJ YLD VLPSOH VKDGHU 
SURJUDPV 7KHVH JHQHUDO SXUSRVH SURJUDPV QHHG QRW KDYH DQ\WKLQJ WR GR ZLWK JUDSKLFV DQG WKH &#38;66 VKDGHUV 
FDQ EH XVHG IRU ODUJH VFDOH SDUDOOHO FRPSXWDWLRQ 6LQFH WKLV LV D UDWKHU QHZ VSHFLILFDWLRQ EURZVHU LPSOHPHQWDWLRQV 
WKDW VXSSRUW JHQHUDO SXUSRVH &#38;66 VKDGHUV DUH QRW \HW DYDLODEOH DW WKH WLPH RI WKLV ZULWLQJ 7KLV 
DUWLFOH JLYHV D PRUH LQ GHSWK GHVFULSWLRQ RI &#38;66 VKDGHUV KWWS ZZZ DGREH FRP GHYQHW KWPO DUWLFOHV 
FVV VKDGHUV KWPO $V WKH DERYH OLQN LOOXVWUDWHV ZLWK &#38;66 VKDGHUV ZH FDQ DSSO\ YHUWH[ VKDGHUV WR WKH 
JULG RI SRO\JRQV WKDW LV RYHUODLG RQ WKH +70/ HOHPHQW 7KH FRQWHQWV RI WKH +70/ HOHPHQW DUH UHQGHUHG DV 
D WH[WXUH PDS ILOWHUHG DSSURSULDWHO\ RQWR JULG 7KURXJK WKHVH VKDGHUV ZH FDQ VSHFLI\ D ILOWHU WKDW FKDQJHV 
WKH SRVLWLRQ RI WKH JULG YHUWLFHV WKHUHE\ FUHDWLQJ FXUYHG SODQHV LQ VSDFH DV ZDV PHQWLRQHG EHIRUH  :HE*/ 
 =KHQ\DR 0R .HQQHWK 5XVVHOO *RRJOH ,QF ,QWURGXFWLRQ 6WUHDP 3URFHVVLQJ 9HUWH[ DQG )UDJPHQW 6KDGHUV 
 $ &#38;RQFUHWH ([DPSOH  6HWWLQJ XS :HE*/ /RDGLQJ 6KDGHUV /RDGLQJ 3URJUDPV 6HWWLQJ XS *HRPHWU\ 'UDZLQJ 
WKH 6FHQH  +LJKHU /HYHO /LEUDULHV $FKLHYLQJ +LJK 3HUIRUPDQFH 3LFNLQJ LQ *RRJOH %RG\ 3DUWLFOH 6\VWHPV 
 6SULWH (QJLQHV 3K\VLFDO 6LPXODWLRQ  &#38;RQFOXVLRQ  ,QWURGXFWLRQ :HE*/ EULQJV ' JUDSKLFV WR WKH +70/ 
SODWIRUP :HE*/ LQ VKRUW LV DQ DOWHUQDWLYH UHQGHULQJ FRQWH[W IRU WKH +70/ &#38;DQYDV HOHPHQW ZKLFK SURYLGHV 
WKH 2SHQ*/ (6  $3, WR -DYD6FULSW 2SHQ*/ DQG 2SHQ*/ (6 VXSSO\ D SURYHQ UHQGHULQJ PRGHO EXW RQH ZKLFK 
GLIIHUV IURP RWKHU JUDSKLFV $3,V RQ WKH :HE ,Q 2SHQ*/ 2SHQ*/ (6 DQG :HE*/ WKH WULDQJOH LV WKH EDVLF GUDZLQJ 
SULPLWLYH 'DWD IRU PDQ\ WULDQJOHV LV SUHSDUHG RQFH EXW GUDZQ PDQ\ WLPHV 7KH GDWD XSORDGHG WR WKH JUDSKLFV 
SURFHVVLQJ XQLW *38 LQFOXGHV FRQFHSWV OLNH YHUWH[ SRVLWLRQV FRORUV WH[WXUHV DQG PRUH 6KDGHUV VPDOO 
SURJUDPV WKDW H[HFXWH GLUHFWO\ RQ WKH *38 GHWHUPLQH WKH SRVLWLRQ RI HDFK WULDQJOH DQG WKH FRORU RI HYHU\ 
SL[HO RQ WKH VFUHHQ 7KH :HE*/ 2SHQ*/ (6 DQG 2SHQ*/ $3,V DUH GHYHORSHG E\ WKH .KURQRV *URXS D QRQ SURILW 
LQGXVWU\ FRQVRUWLXP FUHDWLQJ RSHQ VWDQGDUGV IRU WKH DXWKRULQJ DQG DFFHOHUDWLRQ RI SDUDOOHO FRPSXWLQJ 
JUDSKLFV G\QDPLF PHGLD FRPSXWHU YLVLRQ DQG VHQVRU SURFHVVLQJ RQ D ZLGH YDULHW\ RI SODWIRUPV DQG GHYLFHV 
 6WUHDP 3URFHVVLQJ :HE*/ H[SRVHV D VWUHDP SURFHVVLQJ PRGHO (DFK SRLQW LQ ' VSDFH KDV RQH RU PRUH VWUHDPV 
RI GDWD DVVRFLDWHG ZLWK LW IRU H[DPSOH SRVLWLRQ VXUIDFH QRUPDO FRORU RU WH[WXUH FRRUGLQDWH ,Q 2SHQ*/ 
DQG :HE*/ WKHVH VWUHDPV DUH FDOOHG YHUWH[ DWWULEXWHV 7KHVH VWUHDPV RI GDWD IORZ WKURXJK WKH YHUWH[ DQG 
IUDJPHQW VKDGHUV  9HUWH[ DQG )UDJPHQW 6KDGHUV 9HUWH[ DQG IUDJPHQW VKDGHUV DUH VPDOO VWDWHOHVV SURJUDPV 
ZKLFK UXQ RQ WKH *38 ZLWK D KLJK GHJUHH RI SDUDOOHOLVP 7KH YHUWH[ VKDGHU LV DSSOLHG WR HDFK YHUWH[ RI 
HDFK WULDQJOH ,WV SULPDU\ JRDO LV WR RXWSXW WKH ORFDWLRQ ZKHUH WKH YHUWH[ VKRXOG DSSHDU LQ WKH RQ VFUHHQ 
ZLQGRZ 7KH YHUWH[ VKDGHU PD\ DOVR RXWSXW RQH RU PRUH DGGLWLRQDO YDOXHV FDOOHG YDU\LQJ YDULDEOHV WR 
WKH IUDJPHQW VKDGHU )RU HDFK WULDQJOH WKH *38 ILJXUHV RXW ZKLFK SL[HOV RQ WKH VFUHHQ DUH FRYHUHG E\ 
WKH WULDQJOH 7KH *38 WKHQ UXQV WKH IUDJPHQW VKDGHU RQ HDFK RI WKRVH SL[HOV $W HDFK SL[HO WKH *38 DXWRPDWLFDOO\ 
EOHQGV WKH RXWSXWV RI WKH YHUWH[ VKDGHU EDVHG RQ ZKHUH WKH SL[HO OLHV ZLWKLQ WKH WULDQJOH 7KH IUDJPHQW 
VKDGHU WKHQ GHWHUPLQHV WKH FRORU RI WKH SL[HO EDVHG RQ WKRVH LQSXWV 9HUWH[ GDWD LV XSORDGHG LQWR RQH 
RU PRUH EXIIHU REMHFWV ZKLFK UHVLGH RQ WKH *38 7KH YHUWH[ DWWULEXWHV LQ WKH YHUWH[ VKDGHU DUH ERXQG WR 
WKH GDWD LQ WKHVH EXIIHU REMHFWV  $ &#38;RQFUHWH ([DPSOH 1RZ ZH¶OO JR WKURXJK D FRQFUHWH H[DPSOH DGDSWHG 
IURP *LOHV 7KRPDV¶ /HDUQLQJ :HE*/ /HVVRQ 7KH FRGH LV FKHFNHG LQ WR WKH ZHEJOVDPSOHV SURMHFW DQG FDQ 
EH YLHZHG GLUHFWO\ LQ D :HE*/ HQDEOHG EURZVHU 7KH JRDO RI WKLV H[DPSOH LV WR GH P\VWLI\ :HE*/ E\ VKRZLQJ 
DOO RI WKH VWHSV QHFHVVDU\ WR GUDZ D FRORUHG WULDQJOH RQ WKH VFUHHQ 7KH YHUWH[ VKDGHU IRU WKLV H[DPSOH 
LV YHU\ VLPSOH DWWULEXWH YHF SRVLWLRQ$WWU DWWULEXWH YHF FRORU$WWU YDU\LQJ YHF Y&#38;RORU YRLG PDLQ 
YRLG ^ JOB3RVLWLRQ YHF SRVLWLRQ$WWU Y&#38;RORU FRORU$WWU ` ,Q WKLV H[DPSOH WKH YHUWH[ VKDGHU ZLOO 
EH H[HFXWHG D WRWDO RI WKUHH WLPHV EHFDXVH ZH DUH RQO\ GUDZLQJ RQH WULDQJOH FRQWDLQLQJ WKUHH YHUWLFHV 
,Q D W\SLFDO DSSOLFDWLRQ WKRXVDQGV RU WHQV RI WKRXVDQGV RI WULDQJOHV DUH W\SLFDOO\ GUDZQ WRJHWKHU 7KH 
IUDJPHQW VKDGHU IRU WKLV H[DPSOH LV DOVR YHU\ VLPSOH SUHFLVLRQ PHGLXPS IORDW YDU\LQJ YHF Y&#38;RORU 
YRLG PDLQ YRLG ^ JOB)UDJ&#38;RORU Y&#38;RORU ` 7KH YDOXH RI WKH Y&#38;RORU YDU\LQJ YDULDEOH FRPHV IURP 
D ZHLJKWHG FRPELQDWLRQ RI WKH FRORUV VSHFLILHG DW WKH WKUHH LQSXW YHUWLFHV %DVHG RQ WKH ORFDWLRQ RI WKH 
SL[HO ZLWKLQ WKH WULDQJOH WKH *38 DXWRPDWLFDOO\ EOHQGV WKH FRORU WKDW ZDV VSHFLILHG DW HDFK YHUWH[ 7KH 
IUDJPHQW VKDGHU ZLOO EH H[HFXWHG EHWZHHQ D GR]HQ WR WHQV RI WKRXVDQGV RI WLPHV GHSHQGLQJ RQ WKH VL]H 
RI WKH FDQYDV 5HFDOO WKDW WKH IUDJPHQW VKDGHU H[HFXWHV DW HDFK SL[HO RQ WKH VFUHHQ FRYHUHG E\ D JLYHQ 
WULDQJOH 7KH VKDGHU WH[W IRU WKLV H[DPSOH LV HPEHGGHG GLUHFWO\ LQ WKH ZHE SDJH XVLQJ VFULSW HOHPHQWV 
 ,W LV HQWLUHO\ XS WR WKH DSSOLFDWLRQ KRZ WR PDQDJH WKH VRXUFHV IRU LWV VKDGHUV 6FULSW WDJV ZHUH FKRVHQ 
WR KROG WKH VKDGHUV IRU WKLV H[DPSOH IRU VLPSOLFLW\ $ UHDO ZRUOG DSSOLFDWLRQ PLJKW GRZQORDG WKH VKDGHUV 
IURP WKH VHUYHU XVLQJ ;0/+WWS5HTXHVW JHQHUDWH WKH VRXUFH FRGH IRU LWV VKDGHUV XVLQJ -DYD6FULSW FRGH RU 
RQH RI PDQ\ RWKHU RSWLRQV VFULSW LG VKDGHU YV W\SH [ VKDGHU [ YHUWH[ ! DWWULEXWH YHF SRVLWLRQ$WWU DWWULEXWH 
YHF FRORU$WWU VFULSW! VFULSW LG VKDGHU IV W\SH [ VKDGHU [ IUDJPHQW ! SUHFLVLRQ PHGLXPS IORDW YDU\LQJ 
YHF Y&#38;RORU VFULSW!  6HWWLQJ XS :HE*/ 1RZ ZH¶OO ORRN DW KRZ WKH H[DPSOH LQLWLDOL]HV :HE*/ DV LW 
EHJLQV WR UXQ YDU JO QXOO YDU FRQWH[W1DPHV > ZHEJO H[SHULPHQWDO ZHEJO @ IRU YDU LL  LL FRQWH[W1DPHV 
OHQJWK LL ^ WU\ ^ JO FDQYDV JHW&#38;RQWH[W FRQWH[W1DPHV>LL@ LI JO EUHDN ` FDWFK H ^ ` ` LI JO ^ DOHUW 
&#38;RXOG QRW LQLWLDOLVH :HE*/ VRUU\ ` 7KLV LVQ¶W WKH EHVW HUURU GHWHFWLRQ ORJLF LW¶V GHOLEHUDWHO\ NHSW 
VKRUW LQ RUGHU WR NHHS WKH H[DPSOH VLPSOH &#38;RQVXOW RWKHU H[DPSOHV LQ WKH ZHEJOVDPSOHV UHSRVLWRU\ VXFK 
DV WKH :HE*/ $TXDULXP IRU EHWWHU UHIHUHQFHV ZKHQ EXLOGLQJ UHDO DSSOLFDWLRQV <RX VKRXOG OLQN WR KWWS 
 JHW ZHEJO RUJ LI \RXU DSSOLFDWLRQ¶V LQLWLDOL]DWLRQ IDLOV  1RWH WKH QHHG WR FKHFN IRU ERWK WKH ZHEJO 
DQG H[SHULPHQWDO ZHEJO FRQWH[W W\SHV 7KLV LV D WHPSRUDU\ VLWXDWLRQ ZKLOH WKH :HE*/ VSHFLILFDWLRQ LV UHDFKLQJ 
D FHUWDLQ GHJUHH RI FRQIRUPDQFH DFURVV ZHE EURZVHUV DQG RSHUDWLQJ V\VWHPV ,Q WKH QHDU IXWXUH LW LV H[SHFWHG 
WKDW DOO ZHE EURZVHUV VXSSRUWLQJ :HE*/ ZLOO EHJLQ WR DGYHUWLVH WKH ZHEJO FRQWH[W W\SH DQG WKH H[SHULPHQWDO 
ZHEJO FRQWH[W W\SH ZLOO QR ORQJHU KDYH WR EH TXHULHG E\ DSSOLFDWLRQV /RDGLQJ 6KDGHUV 7KH QH[W VWHS LQ 
D :HE*/ DSSOLFDWLRQ LV WR ORDG LWV VKDGHUV $ VSHFLILF VHTXHQFH RI VWHSV LV IROORZHG WR GR WKLV . &#38;UHDWH 
WKH VKDGHU REMHFW ± YHUWH[ RU IUDJPHQW . 6SHFLI\ LWV VRXUFH FRGH . &#38;RPSLOH LW . &#38;KHFN ZKHWKHU 
FRPSLODWLRQ VXFFHHGHG 7KH FRPSOHWH VRXUFH FRGH IRU WKHVH VWHSV IROORZV 6RPH HUURU FKHFNLQJ LV HOLGHG 
IRU EUHYLW\ IXQFWLRQ JHW6KDGHU JO LG ^ YDU VFULSW GRFXPHQW JHW(OHPHQW%\,G LG YDU VKDGHU LI VFULSW W\SH 
 [ VKDGHU [ YHUWH[ ^ VKDGHU JO FUHDWH6KDGHU JO 9(57(;B6+$'(5 ` HOVH LI VFULSW W\SH [ VKDGHU [ IUDJPHQW 
 ^ VKDGHU JO FUHDWH6KDGHU JO )5$*0(17B6+$'(5 ` JO VKDGHU6RXUFH VKDGHU VFULSW WH[W JO FRPSLOH6KDGHU 
VKDGHU  LI JO JHW6KDGHU3DUDPHWHU VKDGHU JO &#38;203,/(B67$786 ^  DOHUW JO JHW6KDGHU,QIR/RJ VKDGHU 
  UHWXUQ QXOO `  UHWXUQ VKDGHU ` 1RWH WKDW WKH :HE*/ $3, FDOOV VKDGHU6RXUFH FRPSLOH6KDGHU HWF PDWFK 
YHU\ FORVHO\ WKH ZULWWHQ GHVFULSWLRQ RI ZKDW QHHGV WR RFFXU GXULQJ ORDGLQJ RI WKH VKDGHU /RDGLQJ 3URJUDPV 
7KH QH[W VWHS LV WR FRQVWUXFW D SURJUDP REMHFW $ SURJUDP REMHFW FRPELQHV WKH YHUWH[ DQG IUDJPHQW VKDGHUV 
DQG FRPSOHWHO\ VSHFLILHV KRZ D SLHFH RI JHRPHWU\ LV UHQGHUHG $JDLQ D VSHFLILF VHTXHQFH RI VWHSV LV IROORZHG 
WR ORDG WKH SURJUDP . /RDG HDFK VKDGHU VHSDUDWHO\ . $WWDFK HDFK WR WKH SURJUDP . /LQN WKH SURJUDP . &#38;KHFN 
ZKHWKHU OLQNLQJ VXFFHHGHG . 3UHSDUH YHUWH[ DWWULEXWHV IRU ODWHU DVVLJQPHQW 7KH FRPSOHWH VRXUFH FRGH 
IRU WKHVH VWHSV IROORZV YDU SURJUDP IXQFWLRQ LQLW6KDGHUV ^ SURJUDP JO FUHDWH3URJUDP  YDU YHUWH[6KDGHU 
 JHW6KDGHU JO VKDGHU YV  JO DWWDFK6KDGHU SURJUDP YHUWH[6KDGHU  YDU IUDJPHQW6KDGHU JHW6KDGHU JO VKDGHU 
IV  JO DWWDFK6KDGHU SURJUDP IUDJPHQW6KDGHU  JO OLQN3URJUDP SURJUDP  LI JO JHW3URJUDP3DUDPHWHU SURJUDP 
JO /,1.B67$786  DOHUW &#38;RXOG QRW LQLWLDOLVH VKDGHUV  JO XVH3URJUDP SURJUDP  SURJUDP SRVLWLRQ$WWU 
 JO JHW$WWULE/RFDWLRQ SURJUDP SRVLWLRQ$WWU  JO HQDEOH9HUWH[$WWULE$UUD\ SURJUDP SRVLWLRQ$WWU  SURJUDP 
FRORU$WWU  JO JHW$WWULE/RFDWLRQ SURJUDP FRORU$WWU  JO HQDEOH9HUWH[$WWULE$UUD\ SURJUDP FRORU$WWU ` 
 6HWWLQJ XS *HRPHWU\ 1RZ ZH QHHG WR VHW XS WKH JHRPHWU\ WKDW ZLOO EH GUDZQ WR WKH VFUHHQ 7KLV VWHS LV 
LQGHSHQGHQW RI WKH LQLWLDOL]DWLRQ RI WKH VKDGHUV DQG SURJUDP REMHFW LW FRXOG MXVW DV ZHOO EH GRQH EHIRUH 
WKH SURJUDP ZDV ORDGHG ,W LV D WZR VWHS SURFHVV ILUVW D EXIIHU REMHFW LV DOORFDWHG RQ WKH *38 DQG WKHQ 
WKH JHRPHWULF GDWD FRQWDLQLQJ DOO RI WKH YHUWH[ VWUHDPV LV XSORDGHG 7KHUH DUH PDQ\ FKRLFHV WR PDNH ZKHQ 
GHFLGLQJ KRZ WR RUJDQL]H WKH JHRPHWU\ LQFOXGLQJ ZKHWKHU WR XVH LQWHUOHDYHG RU QRQ LQWHUOHDYHG YHUWH[ 
GDWD XVLQJ RQH RU PRUH EXIIHU REMHFWV ZKHUH WR SODFH YHUWH[ GDWD ZLWKLQ WKH EXIIHU REMHFW HWF $V H[SHULHQFH 
ZLWK WKH 2SHQ*/ DQG :HE*/ $3, LV JDLQHG PDQ\ RI WKHVH GHFLVLRQV EHFRPH PRUH HDVLO\ DSSDUHQW ,Q JHQHUDO 
LW LV VWURQJO\ GHVLUDEOH WR XVH DV IHZ EXIIHU REMHFWV DV SRVVLEOH 6ZLWFKLQJ EHWZHHQ WKHP LV H[SHQVLYH 
 7KH FRPSOHWH FRGH IRU WKH VHWXS RI WKLV H[DPSOH¶V JHRPHWU\ IROORZV YDU EXIIHU IXQFWLRQ LQLW*HRPHWU\ 
 ^ EXIIHU JO FUHDWH%XIIHU JO ELQG%XIIHU JO $55$<B%8))(5 EXIIHU ,QWHUOHDYH YHUWH[ SRVLWLRQV DQG FRORUV 
YDU YHUWH['DWD > 9HUWH[ SRVLWLRQ 9HUWH[ &#38;RORU 9HUWH[ SRVLWLRQ 9HUWH[ FRORU 9HUWH[ SRVLWLRQ 
 9HUWH[ FRORU @ JO EXIIHU'DWD JO $55$<B%8))(5 QHZ )ORDW $UUD\ YHUWH['DWD JO 67$7,&#38;B'5$: ` 7KLV 
LV DJDLQ D YHU\ VLPSOH H[DPSOH 0RVW UHDO DSSOLFDWLRQV ZRXOG VWRUH FRPSOH[ PRGHOV RQ WKH VHUYHU DQG WUDQVPLW 
WKHP XVLQJ DQ HIILFLHQW UHSUHVHQWDWLRQ VXFK DV WKDW VXSSOLHG E\ WKH :HE*/ /RDGHU SURMHFW 'UDZLQJ WKH 
6FHQH $W WKLV SRLQW DOO RI WKH SLHFHV DUH LQ SODFH LQ RUGHU WR GUDZ WKH VFHQH 7KH VWHSV WR GR VR DUH 
YHU\ VLPSOH . &#38;OHDU WKH YLHZLQJ DUHD . 6HW XS YHUWH[ DWWULEXWH VWUHDPV . ,VVXH WKH GUDZ FDOO 7KH 
FRPSOHWH FRGH IROORZV IXQFWLRQ GUDZ6FHQH ^ JO YLHZSRUW  FDQYDV ZLGWK FDQYDV KHLJKW  JO FOHDU JO 
&#38;2/25B%8))(5B%,7 _ JO '(37+B%8))(5B%,7  JO ELQG%XIIHU JO $55$<B%8))(5 EXIIHU   7KHUH DUH IORDWLQJ 
SRLQW YDOXHV SHU YHUWH[ YDU VWULGH  )ORDW $UUD\ %<7(6B3(5B(/(0(17   6HW XS SRVLWLRQ VWUHDP JO YHUWH[$WWULE3RLQWHU 
SURJUDP SRVLWLRQ$WWU   JO )/2$7 IDOVH VWULGH   6HW XS FRORU VWUHDP JO YHUWH[$WWULE3RLQWHU SURJUDP 
FRORU$WWU   JO )/2$7 IDOVH VWULGH   )ORDW $UUD\ %<7(6B3(5B(/(0(17  JO GUDZ$UUD\V JO 75,$1*/(6 
  ` 7KDW¶V LW 7KH WULDQJOH QRZ DSSHDUV RQ WKH VFUHHQ  +LJKHU /HYHO /LEUDULHV 1RZ WKDW ZH¶YH GUDJJHG 
\RX WKURXJK D FRPSOHWH H[DPSOH PDQ\ OLEUDULHV DOUHDG\ H[LVW WR PDNH LW HDVLHU WR XVH :HE*/ 2QH OLVW QRW 
FRPSUHKHQVLYH LV PDLQWDLQHG RQ WKH :HE*/ ZLNL DW KWWS ZZZ NKURQRV RUJ ZHEJO ZLNL 8VHUB&#38;RQWULEXWLRQV 
)UDPHZRUNV $ IHZ VXJJHVWLRQV IRU IUDPHZRUNV WR ORRN DW . 7KUHH MV XVHG LQ WKH 5RPH GHPR PU GRRE¶V GHPRV 
DQG PDQ\ RWKHUV . &#38;XELF95 XVHG LQ 0R]LOOD¶V :HE*/ GHPRV VXFK DV 1R &#38;RPSO\ . 7'/ XVHG LQ WKH :HE*/ 
$TXDULXP DQG PRVW RI WKH RWKHU ZHEJOVDPSOHV GHPRV . &#38;RSSHU/LFKW VDPH GHYHORSHU DV ,UUOLFKW . 3KLOR*/ 
IRFXV RQ GDWD YLVXDOL]DWLRQ . */*( XVHG IRU HDUO\ SURWRW\SHV RI *RRJOH %RG\ . 6FHQH-6 XQLTXH DQG LQWHUHVWLQJ 
GHFODUDWLYH V\QWD[ . 6SLGHU*/ ORWV RI LQWHUHVWLQJ YLVXDO HIIHFWV  $FKLHYLQJ +LJK 3HUIRUPDQFH 7KHUH 
DUH D IHZ ³ELJ UXOHV´ DVVRFLDWHG ZLWK ZULWLQJ 2SHQ*/ SURJUDPV )LUVW DQG IRUHPRVW LV WR UHGXFH WKH QXPEHU 
RI GUDZ FDOOV SHU IUDPH 2SHQ*/¶V HIILFLHQF\ FRPSDUHG WR PDQ\ RWKHU JUDSKLFV $3,V FRPHV IURP LWV DELOLW\ 
WR VHQG ODUJH DPRXQWV RI JHRPHWU\ WR WKH *38 ZLWK YHU\ OLWWOH RYHUKHDG 6HQGLQJ GRZQ VPDOO EDWFKHV RU 
HYHQ ZRUVH RQH RU WZR WULDQJOHV SHU GUDZ FDOO GRHV QRW JLYH WKH *38 WKH RSSRUWXQLW\ WR RSWLPL]H WKH 
KDQGOLQJ RI PDQ\ WULDQJOHV DW RQFH ,Q RUGHU WR GUDZ PDQ\ WULDQJOHV DW RQFH LW LV W\SLFDOO\ QHFHVVDU\ 
WR VRUW WKH REMHFWV LQ WKH VFHQH E\ WKHLU UHQGHULQJ VWDWH IRU H[DPSOH REMHFWV XVLQJ WKH VDPH WH[WXUH 
VKRXOG EH GUDZQ ZLWK FRQVHFXWLYH GUDZ FDOOV UDWKHU WKDQ GUDZLQJ RQH REMHFW ZLWK WH[WXUH DQRWKHU REMHFW 
ZLWK WH[WXUH DQG D WKLUG REMHFW ZLWK WH[WXUH  2EMHFWV VKRXOG EH VRUWHG DQG GUDZQ DFFRUGLQJ WR WKH IROORZLQJ 
FULWHULD LQ GHFUHDVLQJ RUGHU RI LPSRUWDQFH . 7DUJHW IUDPHEXIIHU RU FRQWH[W VWDWH z %OHQGLQJ FOLSSLQJ 
GHSWK WHVW HWF . 3URJUDP EXIIHU RU WH[WXUH z 6ZLWFKLQJ WKHVH RIWHQ UHTXLUHV D SLSHOLQH IOXVK . 8QLIRUPV 
DQG VDPSOHUV z 6ZLWFKLQJ WKHVH LV UHODWLYHO\ FKHDS PRGXOR -DYD6FULSW RYHUKHDG 7KDQNV WR %HQ 9DQLN DW 
*RRJOH IRU WKLV LQIRUPDWLRQ  :KHUHYHU SRVVLEOH VRUW WKH REMHFWV LQ WKH VFHQH DKHDG RI WLPH DQG PDLQWDLQ 
WKH REMHFWV DV D VRUWHG OLVW IRU UHQGHULQJ SXUSRVHV :DONLQJ WKH REMHFW KLHUDUFK\ DQG SHUIRUPLQJ D VRUW 
RI REMHFWV SHU IUDPH FDQ FDQFHO WKH JDLQV IURP EDWFKLQJ GUDZ FDOOV *HQHUDWH \RXU FRQWHQW ' PRGHOV HWF 
 VR WKDW LW FDQ EH HDVLO\ EDWFKHG PHUJH EXIIHUV WH[WXUHV HWF ,Q :HE*/ DOO RI WKH 2SHQ*/ ³ELJ UXOHV´ 
DSSO\ DORQJ ZLWK DQRWKHU RQH RIIORDG DV PXFK -DYD6FULSW WR WKH *38 DV SRVVLEOH ZLWKLQ UHDVRQ 2IWHQ WKH 
*38 FDQ EH XVHG WR UHSKUDVH D FRPSXWDWLRQ WKDW ZRXOG RWKHUZLVH QHHG WR EH GRQH LQ -DYD6FULSW DQG \RX 
FDQ DFKLHYH QRW RQO\ EHWWHU SDUDOOHOLVP EXW RIWHQ EHWWHU SHUIRUPDQFH E\ GRLQJ VR 7KH IROORZLQJ H[DPSOHV 
VKRZ KRZ WKLV UXOH LV DSSOLHG LQ WKH FRQWH[W RI D IHZ UHDO ZRUOG H[DPSOHV 3LFNLQJ LQ *RRJOH %RG\ 7KDQNV 
WR WKH *RRJOH %RG\ WHDP IRU WKLV LQIRUPDWLRQ  *RRJOH %RG\ LV D EURZVHU IRU WKH KXPDQ DQDWRP\ 2ULJLQDOO\ 
GHYHORSHG DW *RRJOH /DEV LW LV QRZ DYDLODEOH DV =\JRWH %RG\ IURP WKH FRPSDQ\ WKDW GHYHORSHG WKH ' PRGHOV 
7KH PRGHOV LQ WKLV DSSOLFDWLRQ DUH KLJKO\ GHWDLOHG RYHU D PLOOLRQ WULDQJOHV \HW VHOHFWLRQ LV YHU\ IDVW 
2QH FDQ FOLFN DQ\ ERG\ SDUW WR KLJKOLJKW LW DQG VHH LWV QDPH +RZ FDQ SLFNLQJ EH LPSOHPHQWHG" 2QH FRXOG 
FRQVLGHU GRLQJ UD\ FDVWLQJ LQ -DYD6FULSW :KHQ WKH PRXVH LV FOLFNHG VHW XS D UD\ VWDUWLQJ DW WKH H\H SRLQW 
JRLQJ WKURXJK WKH QHDU SODQH RI WKH ³FDPHUD´ DQG LQWHUVHFW WKDW UD\ ZLWK DOO RI WKH WULDQJOHV LQ WKH 
VFHQH 2QH FRXOG DWWHPSW WR GR TXLFN GLVFDUGV LI WKH UD\ GRHVQ¶W LQWHUVHFW DQ REMHFW¶V ERXQGLQJ ER[ WR 
DYRLG UD\ WULDQJOH WHVWV IRU REMHFWV WKDW DUH REYLRXVO\ PLVVHG E\ WKH UD\ 5HJDUGOHVV WKLV LV VWLOO D 
ORW RI PDWK WR GR LQ -DYD6FULSW ,QVWHDG *RRJOH %RG\ XVHV WKH *38 WR LPSOHPHQW SLFNLQJ :KHQ WKH PRGHO 
LV ORDGHG HDFK RUJDQ LV DVVLJQHG D GLIIHUHQW FRORU :KHQ WKH PRXVH LV FOLFNHG WKH PRGHO LV UHQGHUHG RIIVFUHHQ 
ZLWK D GLIIHUHQW VKDGHU WKDQ XVXDO 7KLV VKDGHU GUDZV HDFK VKDSH ZLWK LWV SUHDVVLJQHG FRORU ZLWKRXW DQ\ 
WH[WXULQJ OLJKWLQJ RU WUDQVSDUHQF\ $ WKUHVKROG LV XVHG WR GHWHUPLQH ZKHWKHU WR GUDZ WUDQVOXFHQW OD\HUV 
GXULQJ WKLV SURFHVV OD\HUV ZKLFK DUH ³WRR WUDQVSDUHQW´ WR SLFN DUH VLPSO\ QRW GUDZQ $IWHU WKLV UHQGHU 
SDVV LV FRPSOHWHG WKH FRORU XQGHU WKH PRXVH SL[HO LV UHDG EDFN WR WKH &#38;38 XVLQJ UHDG3L[HOV 7KH VDPH 
WHFKQLTXH ZRUNV DW GLIIHUHQW OHYHOV RI JUDQXODULW\ IRU H[DPSOH HDFK WULDQJOH UDWKHU WKDQ HDFK REMHFW 
FRXOG EH DVVLJQHG D GLIIHUHQW FRORU WR DFKLHYH ILQHU GHWDLO ZKHQ SLFNLQJ 7KH IROORZLQJ WKUHH VFUHHQVKRWV 
VKRZ WKH SLFNLQJ SURFHVV ZKLOH GLIIHUHQW OD\HUV RI WKH PRGHO DUH YLVLEOH 1RWH WKDW WKLV WHFKQLTXH XVHV 
WKH *38 IRU ZKDW LW LV EHVW DW UHQGHULQJ ,W HVVHQWLDOO\ FRQYHUWV WKH SUREOHP RI SLFNLQJ LQWR RQH RI 
UHQGHULQJ 'HVSLWH WKH QHHG WR SHUIRUP D UHDGEDFN IURP WKH *38 WR WKH &#38;38 DW WKH HQG RI WKH DOJRULWKP 
WKH SHUIRUPDQFH JDLQV IURP XVLQJ WKH *38 DUH ZRUWK LW 3DUWLFOH 6\VWHPV 3DUWLFOH V\VWHPV DUH D WHFKQLTXH 
FRPPRQO\ XVHG WR GUDZ JUDSKLFDO HIIHFWV OLNH H[SORVLRQV VPRNH FORXGV DQG GXVW 7KH PRVW REYLRXV ZD\ 
WR LPSOHPHQW D SDUWLFOH V\VWHP LV WR FRPSXWH WKH SRVLWLRQV RI WKH SDUWLFOHV RQ WKH &#38;38 XSORDG WKH 
YHUWLFHV WR WKH *38 DQG GUDZ WKHP LQ D VLQJOH GUDZ FDOO 7KLV WHFKQLTXH FDQ ZRUN IRU VPDOO SDUWLFOH V\VWHPV 
EXW GRHV QRW VFDOH ZHOO EHFDXVH PDQ\ YHUWLFHV DUH XSORDGHG WR WKH *38 HDFK IUDPH $GGLWLRQDOO\ PDWKHPDWLFDO 
RSHUDWLRQV DUH QRW \HW DV IDVW LQ -DYD6FULSW DV WKH\ DUH LQ &#38; RU &#38; $ GLIIHUHQW WHFKQLTXH LV 
GHVLUHG *UHJJ 7DYDUHV KDV GHYHORSHG D SDUWLFOH V\VWHP GHPRQVWUDWLRQ LQ WKH :HE*/ GHPR UHSRVLWRU\ ZKLFK 
DQLPDWHV URXJKO\  SDUWLFOHV DW IUDPHV SHU VHFRQG $OO RI WKH DQLPDWLRQ PDWK LV SHUIRUPHG RQ WKH *38 
(DFK SDUWLFOH¶V PRWLRQ LV GHILQHG E\ DQ HTXDWLRQ LQLWLDO SRVLWLRQ YHORFLW\ DFFHOHUDWLRQ VSLQ DQG OLIHWLPH 
:KHQ WKH SDUWLFOH LV FUHDWHG WKHVH SDUDPHWHUV DUH VHW XS RQFH DQG QHYHU PRGLILHG DIWHUZDUG (DFK IUDPH 
RQO\ RQH SDUDPHWHU QHHGV WR EH VHQW IURP WKH &#38;38 WR WKH *38 WKH FXUUHQW WLPH 7KH YHUWH[ VKDGHU HYDOXDWHV 
WKH HTXDWLRQ RI PRWLRQ DQG PRYHV WKH SDUWLFOH LQWR LWV GHVLUHG ORFDWLRQ IRU WKH FXUUHQW IUDPH 7KLV WHFKQLTXH 
HQVXUHV WKDW WKH DEVROXWH PLQLPXP DPRXQW RI -DYD6FULSW ZRUN LV GRQH SHU IUDPH  1LKLORJLF¶V :RUOGV RI 
:HE*/ GHPRQVWUDWLRQ VKRZV D VLPLODU SDUWLFOH V\VWHP WHFKQLTXH ,Q WKLV GHPRQVWUDWLRQ SDUWLFOHV DVVHPEOH 
WR IRUP YDULRXV VKDSHV IDOOLQJ WR WKH IORRU EHWZHHQ VFHQHV DQG DQLPDWLQJ VPRRWKO\ EHWZHHQ WKHP 7KH DQLPDWLRQ 
LV GRQH VLPLODUO\ WR *UHJJ 7DYDUHV¶ SDUWLFOH V\VWHP DERYH )RU HDFK VFHQH UDQGRP SRVLWLRQV IRU WKH SDUWLFOHV 
DUH FKRVHQ DW VHWXS WLPH 7KH WLPH SDUDPHWHU LQWHUSRODWHV EHWZHHQ WZR YHUWH[ DWWULEXWH VWUHDPV DW DQ\ 
JLYHQ WLPH RQH VWUHDP FRQWDLQV WKH SDUWLFOH SRVLWLRQV RQ WKH IORRU DQG WKH RWKHU WKH SDUWLFOH SRVLWLRQV 
IRU WKH FXUUHQW VKDSH 2QFH WKH FXUUHQW VKDSH KDV EHHQ DVVHPEOHG WKH QH[W LQWHUSRODWLRQ WDUJHW EHFRPHV 
WKH SDUWLFOHV RQ WKH IORRU DJDLQ -DYD6FULSW GRHV DOPRVW QR FRPSXWDWLRQ 6SULWH (QJLQHV ,Q URXJKO\ HDUO\ 
  )DFHERRN UHOHDVHG D VSULWH HQJLQH EHQFKPDUN FDOOHG -6*DPH%HQFK FRPSDULQJ YDULRXV WHFKQLTXHV IRU UHQGHULQJ 
DQLPDWHG VSULWHV ZLWKLQ D ZHE EURZVHU LQFOXGLQJ PRYLQJ DURXQG '20 HOHPHQWV DQG GUDZLQJ ZLWK ERWK ' &#38;DQYDV 
DQG :HE*/ 6SULWHV DUH JHQHUDOO\ VLPLODU WR SDUWLFOH V\VWHPV EXW DUH WHUPLQRORJ\ PRUH FRPPRQO\ XVHG ZKHQ 
DXWKRULQJ FHUWDLQ NLQGV RI ' JDPHV -6*DPH%HQFK GRHVQ¶W DSSHDU WR EH XQGHU DFWLYH GHYHORSPHQW DQ\ PRUH 
EXW VRPH OHVVRQV FDQ EH OHDUQHG DERXW LWV VWUXFWXUH DQG SHUIRUPDQFH FKDUDFWHULVWLFV $W WKH WLPH -6*DPH%HQFK 
ZDV UHOHDVHG LWV :HE*/ EDFNHQG SHUIRUPHG RQH GUDZ FDOO SHU VSULWH ,W VHHPHG WKDW GUDZLQJ WKH HQWLUH VSULWH 
ILHOG ZLWK RQH GUDZ FDOO ZRXOG EH DQ REYLRXV SHUIRUPDQFH ZLQ VR D SURWRW\SH VSULWH HQJLQH OLEUDU\ ZDV 
GHYHORSHG WR WHVW WKLV K\SRWKHVLV 7KH ILUVW TXHVWLRQ ZDV ZKHWKHU LW ZDV HYHQ SRVVLEOH WR GUDZ DOO RI 
WKH VSULWHV DW RQFH WKH\ XVH DOSKD EOHQGLQJ VR WKH RUGHU WKH\ DUH GUDZQ LV LPSRUWDQW ,W¶V D OLWWOH NQRZQ 
IDFW WKDW 2SHQ*/ DFWXDOO\ JXDUDQWHHV WKH RUGHU WKH WULDQJOHV DUH GUDZQ LQ ZKHQ JO'UDZ$UUD\V DQG JO'UDZ(OHPHQWV 
DUH FDOOHG $SSDUHQWO\ *38V FRQWDLQ TXLWH D ELW RI VLOLFRQ WKH 5HQGHU 2XWSXW XQLW RU 523 WR SURYLGH 
WKLV JXDUDQWHH WKDQNV WR 1DW 'XFD DW *RRJOH IRU WKLV LQIRUPDWLRQ 7KHUHIRUH LW ZDV WHFKQLFDOO\ SRVVLEOH 
WR GUDZ WKH HQWLUH VSULWH ILHOG DW RQFH VLQFH WKH RUGHU WKH VSULWHV ZHUH GUDZQ WR WKH VFUHHQ ZRXOG EH 
FRQVLVWHQW IURP IUDPH WR IUDPH 7KH QH[W TXHVWLRQ ZDV KRZ WR PHUJH WRJHWKHU WKH VSULWHV¶ PXOWLSOH LPDJHV 
2QH REYLRXV ZD\ ZRXOG EH WR SXW DOO RI WKH VSULWHV¶ LPDJHV LQWR D VLQJOH WH[WXUH VR WKDW WKH GUDZ FDOO 
ZRXOG UHIHUHQFH RQO\ RQH WH[WXUH 7KLV VHHPHG OLNH DQ LQIOH[LEOH VROXWLRQ VLQFH LW ZRXOG EH ERXQGHG E\ 
WKH PD[LPXP VL]H RI DQ LQGLYLGXDO WH[WXUH ,QVWHDG WKH GHVLUHG VROXWLRQ ZDV WR SDVV GRZQ PXOWLSOH WH[WXUHV 
WR WKH IUDJPHQW VKDGHU DQG KDYH WKH VKDGHU FKRRVH ZKLFK RQH WR VDPSOH IRU WKH VSULWH¶V LPDJH &#38;RQFHSWXDOO\ 
ZH ZRXOG OLNH WR VHQG GRZQ D XQLIRUP DUUD\ RI VDPSOHUV H J XQLIRUP VDPSOHU ' WH[WXUHV> @ DQG FRPSXWH 
DQ LQGH[ LQWR WKLV DUUD\ 8QIRUWXQDWHO\ WKH :HE*/ VKDGLQJ ODQJXDJH ZKLFK LV HVVHQWLDOO\ WKH VDPH DV WKH 
2SHQ*/ (6 VKDGLQJ ODQJXDJH GRHVQ W DOORZ WKLV NLQG RI LQGH[LQJ H[SUHVVLRQ LQ D IUDJPHQW VKDGHU 7KH RQO\ 
NLQG RI LQGH[LQJ H[SUHVVLRQ DOORZHG LV RQH LQYROYLQJ FRQVWDQWV DQG ORRS LQGLFHV 7KH ILUVW DWWHPSW DW 
WKH WH[WXUH VHOHFWLRQ IUDJPHQW VKDGHU ORRNHG OLNH WKLV ,W SDVVHG GRZQ D YHFWRU RI FRHIILFLHQWV IRU HDFK 
YHUWH[ ZKLFK VHOHFWHG RQH RI WKH IRXU LQFRPLQJ WH[WXUHV JOB)UDJ&#38;RORU  WH[WXUH ' XBWH[WXUH YBWH[&#38;RRUG 
 YBWH[WXUH:HLJKWV [  WH[WXUH ' XBWH[WXUH YBWH[&#38;RRUG YBWH[WXUH:HLJKWV \  WH[WXUH ' XBWH[WXUH 
 YBWH[&#38;RRUG YBWH[WXUH:HLJKWV ]  WH[WXUH ' XBWH[WXUH YBWH[&#38;RRUG YBWH[WXUH:HLJKWV Z 7KLV ZRUNHG 
EXW XQIRUWXQDWHO\ WKH UHVXOWLQJ GHPR ZDV VORZHU WKDQ WKH H[LVWLQJ :HE*/ EDFNHQG RI -6*DPH%HQFK DERXW 
 RI WKH SHUIRUPDQFH ([SHULPHQWV ZHUH GRQH WR UHPRYH WKH H[SORVLRQ VSULWH ZKLFK LV WKH ODUJHVW RI DOO 
WKH VSULWHV  [  ILOOLQJ D  [  WH[WXUH DQG VHOHFWLQJ WKH VSULWHV IURP WKH UHPDLQLQJ WKUHH VKHHWV 
7KLV \LHOGHG D VLJQLILFDQW VSHHGXS ZKLFK VWURQJO\ LQGLFDWHG WKDW WKH WH[WXUH EDQGZLGWK RQ WKH FDUG ZDV 
EHLQJ H[KDXVWHG 1DW 'XFD VXJJHVWHG WR XVH D VHULHV RI LI WHVWV LQ WKH IUDJPHQW VKDGHU WR VDPSOH WKH 
GHVLUHG WH[WXUH 3UHYLRXV H[SHULHQFH KDG EHHQ WR DYRLG LI WHVWV LQ VKDGHUV DW DOO FRVWV LQ HDUOLHU ZRUN 
 HYHU\ WLPH DQ LI WHVW LQ D VKDGHU KDG EHHQ UHSODFHG ZLWK D QRQ EUDQFKLQJ RSHUDWLRQ OLNH D FODPS RU VWHS 
SHUIRUPDQFH KDG LPSURYHG 1DW LQGLFDWHG WKDW RQ PRGHUQ FDUGV LI WKH EUDQFK ZLOO JR WKH VDPH ZD\ RYHU ODUJH 
UHJLRQV ZKLFK LW ZLOO LQ WKLV FDVH LWV FRQVWDQW DFURVV WKH HQWLUH VXUIDFH RI WKH VSULWH LW ZLOO ZRUN 
ZHOO 7KH IUDJPHQW VKDGHU ZDV UHZULWWHQ DV IROORZV YHF FRORU LI YBWH[WXUH:HLJKWV [ !   FRORU WH[WXUH 
' XBWH[WXUH YBWH[&#38;RRUG  HOVH LI YBWH[WXUH:HLJKWV \ !   FRORU WH[WXUH ' XBWH[WXUH YBWH[&#38;RRUG 
 HOVH LI YBWH[WXUH:HLJKWV ] !   FRORU WH[WXUH ' XBWH[WXUH YBWH[&#38;RRUG  HOVH YBWH[WXUH:HLJKWV 
Z !   FRORU WH[WXUH ' XBWH[WXUH YBWH[&#38;RRUG  JOB)UDJ&#38;RORU FRORU 7KLV WHFKQLTXH ZRUNHG ZHOO 
RQ WKH GHYHORSPHQW PDFKLQH LW UHQGHUHG  RU PRUH VSULWHV DW WKH VDPH IUDPH UDWH WKDQ WKH :HE*/ EDFNHQG 
RI -6*DPH%HQFK DW WKH WLPH -6*DPH%HQFK ZDV VXEVHTXHQWO\ XSGDWHG WR XVH VLPLODU EDWFKLQJ WHFKQLTXHV 7KH 
VRXUFH FRGH IRU DQG PRUH GHWDLOV RQ WKLV VSULWH HQJLQH SURWRW\SH DUH DYDLODEOH LQ WKH :HE*/ VDPSOHV SURMHFW 
LQ WKH VSULWHV VXEGLUHFWRU\ VHH WKH GRFXPHQWDWLRQ DQG OLYH GHPR 3K\VLFDO 6LPXODWLRQ :HE*/ VXSSRUWV IORDWLQJ 
SRLQW WH[WXUHV DV DQ H[WHQVLRQ PHDQLQJ WKDW HYHU\ WH[HO FDQ VWRUH RQH RU PRUH IORDWLQJ SRLQW YDOXHV 7KH 
IDFW WKDW WKH *38 FDQ RSHUDWH RQ VR PXFK IORDWLQJ SRLQW GDWD DW RQFH PHDQV WKDW LW LV SRVVLEOH WR SHUIRUP 
DGYDQFHG WHFKQLTXHV LQ :HE*/ VXFK DV SK\VLFDO VLPXODWLRQ $Q\ LWHUDWLYH FRPSXWDWLRQ ZKHUH HDFK VWHS UHOLHV 
RQO\ RQ QHDUE\ QHLJKERUV LV D JRRG FDQGLGDWH IRU PRYLQJ WR WKH *38 (YJHQ\ 'HPLGRY KDV GHYHORSHG VHYHUDO 
GHPRQVWUDWLRQV VKRZLQJ KRZ WR VLPXODWH ZDYHV LQWHUIHUHQFH SDWWHUQV ' IOXLG G\QDPLFV DQG RWKHU WHFKQLTXHV 
LQ :HE*/ (YDQ :DOODFH KDV GHYHORSHG GHPRQVWUDWLRQV XWLOL]LQJ IORDWLQJ SRLQW WH[WXUHV WR VLPXODWH LQWHUDFWLYH 
ZDWHU LQ D SRRO DQG HYHQ GR UD\ WUDFLQJ LQ :HE*/  &#38;RQFOXVLRQ :HE*/ LV DQ HYROYLQJ VSHFLILFDWLRQ 
DQG HFRV\VWHP :H ORRN IRUZDUG WR \RXU SDUWLFLSDWLRQ LQ WKH FRPPXQLW\ . :HE*/ ODQGLQJ SDJH DW WKH .KURQRV 
*URXS . :HE*/ ZLNL . :HE*/ VSHFLILFDWLRQ HGLWRU¶V GUDIW . :HE*/ GHYHORSHUV¶ PDLOLQJ OLVW IRU GLVFXVVLQJ 
WKH XVH RI :HE*/ . :HE*/ SXEOLF PDLOLQJ OLVW IRU GLVFXVVLQJ WKH VSHFLILFDWLRQ  ! ! Graphics!Programming!on!the!Web! 
WebCL!Course!Notes! Siggraph!2012! Mikaël Bourges-Sévenier1 Motorola Mobility, Inc. ! ! ! ! ! ! Abstract! 
This document introduces WebCL [1], a new standard under development by the Khronos Group, for high­performance 
computing in web browsers. Since WebCL wraps OpenCL, the course starts by reviewing important OpenCL 
[2] concepts. Next, we detail how to program with WebCL in the browser and on devices such as GPUs. Finally, 
we discuss WebCL WebGL [3] interoperability and provide complete examples of moving from WebGL shaders 
to WebCL. Last, we provide tips and tricks to ease such translation and to optimize WebCL code performance. 
1 mikeseven@acm.org  Table!of!Content! 1! What!is!WebCL?........................................................................................................................ 
3! 2! Glossary!and!conventions ......................................................................................................... 
3! 3! Thinking!in!parallel.................................................................................................................... 
4! 4! OpenCL!memory!model............................................................................................................. 
4! 5! Programming!with!WebCL......................................................................................................... 
7! 5.1! Host/Browser!side........................................................................................................................................................................7! 
5.1.1! Platform,layer...........................................................................................................................................................................................................8! 
5.1.2! Runtime,layer............................................................................................................................................................................................................9! 
5.2! Device!side.....................................................................................................................................................................................15! 
 6! Interoperability!with!WebGL................................................................................................... 
16! 6.1! Fun!with!2!triangles...................................................................................................................................................................17! 
6.1.1! General,CL:GL,interoperability,algorithm................................................................................................................................................18! 
6.1.2! Using,shared,textures.........................................................................................................................................................................................18! 
6.1.3! Using,shared,buffers ...........................................................................................................................................................................................19! 
6.1.4! Example....................................................................................................................................................................................................................20! 
6.2! Other!applications......................................................................................................................................................................22! 
 7! Tips!and!tricks......................................................................................................................... 
22! 7.1! From!GLSL!to!OpenCL!C...........................................................................................................................................................23! 
7.2! Barriers...........................................................................................................................................................................................23! 
7.3! Local!workFgroup!size..............................................................................................................................................................23! 
7.4! Learn!parallel!patterns!...........................................................................................................................................................23! 
 8! WebCL!implementations......................................................................................................... 
23! 9! Perspectives............................................................................................................................ 
24! Appendix!A! CLLGL!code .............................................................................................................. 
25! A.1! Graphics!object............................................................................................................................................................................25! 
A.2! Compute!object............................................................................................................................................................................27! 
A.3! Mandelbulb!kernel!(direct!conversion)............................................................................................................................31! 
A.4! Mandelbulb!kernel!(optimized)...........................................................................................................................................34! 
 Appendix!B! OpenCL!and!CUDA!terminology............................................................................... 
37! Bibliography.................................................................................................................................. 
39! Specifications.............................................................................................................................................................................................39! 
Programming!guides..............................................................................................................................................................................39! 
Books.............................................................................................................................................................................................................39! 
WebCL!prototypes...................................................................................................................................................................................39! 
Articles!and!Presentations...................................................................................................................................................................39! 
 1 What!is!WebCL?! In short, WebCL is to OpenCL what WebGL is to OpenGL. WebCL is a JavaScript API 
over OpenCL API; Khronos Group is defining all these international standards. Historically, OpenGL was 
defined as a standard for hardware accelerated graphics, hence Graphics Language. OpenGL was first a 
fixed pipeline a programmer could change various states to produce images. Then, OpenGL pipeline became 
programmable using shaders, pieces of C like code that can be inserted at some points of the OpenGL rendering 
pipeline. As the need for more complex applications arise, programmers realized that shaders could be 
used for more general programming problems, taking advantage of the massively parallel nature of GPUs; 
this became known as GPGPU. But shaders can only provide limited features for such applications. Few 
years ago, Apple proposed OpenCL to the Khronos Group, a more general framework for computing, hence 
the term Compute Language. Not only OpenCL allows usage of GPUs but also any devices that has a driver 
in the machine: CPUs, DSPs, accelerators, and so on. It is important to note that OpenCL doesn t provide 
any rendering capability, unlike OpenGL; it only processes data, lots of data. The source of such data 
could be OpenGL buffers such as vertex buffers, pixel buffers, render buffers, and so on. To understand 
WebCL, it is necessary to understand the OpenCL programming model. 2 Glossary!and!conventions! Work-item 
The basic unit of work of an OpenCL device Work-group Work-items execute together as a work-group Kernel 
The code of a work-item, a C99 function Program A collection of kernels and other functions, same as 
a dynamic library Context The environment within which work-items executes. This includes devices, their 
memories, their command queues, and so on In this course, we will use the following conventions: Code 
is a yellow box o All lines are numbered o WebCL/OpenCL keywords and methods are in bold red o Comments 
are in light green o Language keywords are in bold dark purple o Strings are in blue  1 __kernel 2 
void multiply(__global const float *a, // a, b, c values are in global memory The method console.log() 
is simplified to log().  All WebCL calls throw exceptions (unlike WebGL that return error codes). For 
simplicity, we may omit try/catch in this document, but you should not!  OpenCL qualifiers start with 
__ (two _ ). For example, one could use __kernel or kernel interchangeably. In this document, we always 
use __kernel.  We use interchangeably CL for OpenCL and WebCL, GL for OpenGL ES 2.x and WebGL.  OpenCL 
files end with extension .cl . On web pages, we use <script type = x-webcl >, although both are not defined 
by any standard.   3 Thinking!in!parallel! Programming a massively parallel device is challenging 
and, for many developers, may require learning new programming skills. By massively parallel, we mean 
that many hardware-processing units run at once or, said differently, many hardware threads are running 
concurrently. While CPUs tend to have 2, 4, or 8 cores, GPUs can have thousands of cores. Even on mobile 
devices, GPUs with hundred of cores are coming. For web developers used to sequential event-based programs, 
with JavaScript language not providing threading support, it is a radical shift. The following example 
shows the main idea: A traditional loop over a (large) set of data can be replaced by a data-parallel 
kernel  Each work-item runs a copy of the kernel function.  With n work-items, the computation is executed 
in 1 pass vs. n passes with a traditional loop The OpenCL concepts are introduced in the next section. 
 3 // in JavaScript 10 11 return c; 12 }  1 // in OpenCL 8 int id = get_global_id(0); // work-item globalID 
9 if(id >= n) return; // make sure work-item don t read/write past array size 10 c[id] = a[id] * b[id]; 
11 }  Code 1 Representing a JavaScript method into a WebCL kernel. 4 OpenCL!memory!model! Before we 
enter into OpenCL programming details, it is important to understand its platform model: A Host contains 
one or more Compute Devices. A Host has its own memory.  Each Compute Device (e.g. CPU, GPU, DSP, FPGA...) 
is composed of one or more compute units (e.g. cores). Each Compute Device has its own memory.  Each 
Compute Unit is divided in one or more Processing Elements (e.g. hardware threads). Each processing element 
has its own memory.  In general, we will refer to Host for the device onto which the WebCL program is 
executed (i.e. within the browser). We refer to Device for a compute device onto which an OpenCL Kernel 
is executed. Hence, a CPU can be both a Host and a Compute Device. Figure 1 - OpenCL platform model 
OpenCL defines 4 types of memory spaces within a Compute Device: Global memory corresponds to the device 
RAM. This is where input data are stored. Available to all work groups/items. Similar to system memory 
over a slow bus, rather slow memory. Not cached.  Constant memory cached global memory  Local memory 
 high-speed memory shared among work-items of a compute unit (i.e. for a work­group). Similar to L1 cache. 
Reasonably fast memory.  Private memory registers of a work-item. Very fast memory.  However, private 
memory is small and local memory is often no more than 64 KB. As a result, programmer must choose carefully 
which variables leave in a memory space for the best performance / memory access performance tradeoff. 
Another type of memory is Texture Memory, which is similar to Global Memory but is cached, optimized 
for 2D spatial locality, and designed for streaming reads with constant latency. In other words, if your 
device has image support and your data can fit in texture memory, it may be better than using buffers 
in global memory.  Figure 2 OpenCL memory model Finally, at an even lower level, work-items are scheduled 
as a group called warp (NVidia) or wavefront (AMD); this is the smallest unit of parallelism on a device. 
Individual work-items in a warp/wavefront start together at the same program address, but they have their 
own address counter and register state and are therefore free to branch and execute independently [8]. 
Threads on a CPU are generally heavyweight entities and context switches (when the operating system swap 
two threads on and off execution channels) are therefore expensive. By comparison, threads on a GPU (i.e. 
work-items) are extremely lightweight entities. Since registers are allocated to active threads, no swapping 
of registers and state occurs between GPU threads. Once threads complete, its resources are de-allocated. 
Each work-item has a global ID into an N-Dimensional index space, where N can be 1, 2 or 3. An N-dimensional 
range (or NDRange) is defined by an array of N values specifying the extent of the index space in each 
dimension starting at an offset F (0 by default). Within a work-group, a work-item also has a local ID. 
Work-group (Wx, Wy) Figure 3 Global and Local IDs for a 2D problem. Using a 2D example, as depicted 
in Figure 3, with a global NDRange of size [Gwidth, Gheight] and local NDRange of size [Lwidth, Lheight], 
1. Indexes always go from 0 to range-1 in each dimension 2. localID of work-item at index (lx, ly) is 
lx + ly * Lwidth 3. globalID of work-item at index (gx, gy) is gx + gy * Gwidth  To favor memory coalescing 
(i.e. the device accesses memory in a batch rather than individual accesses that would require serialized 
accesses to memory), it is useful to keep: 1. The Gwidth of the problem as a multiple of the maximum 
work-group size, eventually adding extra columns with appropriate padding. The maximum work-group size 
is given by cl.KERNEL_WORK_GROUP_SIZE 2. The Lwidth of a work-group as a multiple of the warp/wavefront 
size. This value is given by cl.KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE Both limits can be queried 
on a WebCLKernel object once it is created. They are extremely important for maximum throughput. Host 
and devices communicate via buffers defined in an OpenCL context. Commands are sent to devices via command-queues. 
Commands are used for memory transfers from host and devices, between memory objects in a device, and 
to execute programs.  5 Programming!with!WebCL! Programming with WebCL is composed of 2 parts: The 
host side (e.g. in the web browser) that sets up and controls the execution of the program  The device 
side (e.g. on a GPU) that runs computations i.e. kernels.  5.1 Host/Browser!side! All WebCL methods 
may throw exceptions (rather than error codes as in WebGL), so you should wrap your WebCL methods with 
try/catch, even though for simplicity we will omit them in this document. Code 2 Always wrap WebCL 
method calls with try/catch! Unlike WebGL, WebCL is a global object so that it can be used in a Web page 
or within a Web Worker. Consequently, we first need to create a WebCL object: 1 var cl = new WebCL(); 
Code 3 Creating the WebCL object. The remainder of this section will detail how to use all WebCL objects 
in Figure 4. * * *** Figure 4 WebCL objects. The typical workflow is described in Figure 5 and consists 
in 3 phases: Initialize the platform layer  Load and compile programs/kernels  Interact with devices 
through the runtime layer  Figure 5 OpenCL startup sequence 5.1.1 Platform!layer! The OpenCL platform 
layer implements platform-specific features. They allow applications to query OpenCL devices, device 
configuration information, and to create OpenCL contexts using one or more devices. 1 // let s get all 
platforms on this machine  26 } Code 4 Query WebCL platforms and devices features. In general, to 
ensure your algorithm is portable across various devices (even on the same machine!), it is necessary 
to know details about features on each device. For example, if you require image support, ensure the 
device you choose support them and up to what size, and how many images can be supported at once. If 
your kernel requires atomics, make sure device s extensions return cl_khr_int64_base_atomics . On embedded 
devices, knowing that cl_khr_fp16 is supported (i.e. 16-bit floats or half-floats) can lead to twice 
more performance. When optimizing algorithms, knowing the maximum workgroup size, the number of work-items 
per dimension, the number of parameters to a kernel function, the maximum size of a memory object, and 
other features, are crucial elements to adapt your applications at runtime. On the other end, if you 
just want to use the best device on the machine and let the browser find it for you, you could just do: 
1 var ctx = cl.createContext( { 22 if (device) 23 platform = device.getInfo(cl.DEVICE_PLATFORM); Code 
5 Let the browser figures the best platform/device for a context. Note: in practice, the algorithm in 
Code 5 is often simplified with 1 var devices = ctx.getInfo(cl.CONTEXT_DEVICES); 2 var device = devices[0]; 
3 var platform = device.getInfo(cl.DEVICE_PLATFORM); but this assumes the machine has only 1 GPU device! 
Now that we have created a WebCLContext object, we need to set it up for our program and run it! 5.1.2 
Runtime!layer! The runtime layer manages OpenCL objects such as command-queues, memory objects, program 
objects, kernel objects in a program and calls that allow you to enqueue commands to a command-queue 
such as executing a kernel, reading, or writing a memory object. WebCL defines the following objects: 
 Command Queues  Memory objects (Buffer and Images)  Sampler objects describe how to sample an image 
being read by a kernel  Program objects that contain a set of kernel functions identified with __kernel 
qualifier in the program source  Kernel objects encapsulate the specific __kernel functions declared 
in a program source and its argument values to be used when executing the __kernel function  Event objects 
used to track the execution status of a command as well as to profile a command  Command synchronization 
objects such as Markers and Barriers  5.1.2.1 Loading,and,building,programs, WebCL, like WebGL 1.0, 
assumes program to be provided in source code form i.e. a large string. Currently, any WebCL device is 
required to have an internal compiler. The source code is first loaded to the device, then compiled. 
As with any compiler, CL defines standard compilation options including the standard D (predefined name 
and value) and I (include directory). Code 6 shows how to properly catch compilation errors using WebCLProgram.getBuildInfo(). 
1 // Create the compute program from the source strings 8 throw 'Error building program: ' + err 9 + 
program.getBuildInfo(device, cl.PROGRAM_BUILD_LOG)); 10 } Code 6 Load and build a CL program. Note: 
WebCL currently only supports source code as a set of strings. At this point, our program is compiled, 
and contains one or more kernel functions. These kernel functions are the entry points of our program, 
similar to entry points of a shared library. To refer to each kernel function, we create a WebCLKernel 
object: 1 // Create the compute kernels from within the program2 kernel = program.createKernel( kernel_function_name 
); Code 7 Create a kernel object for each kernel function in the program. In the next section, we will 
discover how to pass arguments to the kernel functions. 5.1.2.2 Passing,arguments,to,kernels, A kernel 
function may have one or more arguments, like any function. Since JavaScript only offers the type Number 
for numerical values, we need to pass the type of such value to the kernel object for each argument. 
For other type of values, we must use WebCL objects: WebCLBuffer and WebCLImage that wrap a Typed Array 
[1]  WebCLSampler for sampling an image  5.1.2.3 Creating,memory,objects, A WebCLBuffer object stores 
a one-dimensional collection of elements. Elements of a buffer can be scalar type (e.g. int, float), 
vector data type, or user-defined structure. 1 // create a 1D buffer 2 var buffer = ctx.createBuffer(flags, 
sizeInBytes, optional srcBuffer); Flag Description cl.MEM_READ_WRITE Default. Memory object is read 
and written by kernel cl.MEM_WRITE_ONLY Memory object only written by kernel cl.MEM_READ_ONLY Memory 
object only read by kernel cl.MEM_USE_HOST_PTR Implementation uses storage memory in srcBuffer. srcBuffer 
must be specified. cl.MEM_ALLOC_HOST_PTR Implementation requests OpenCL to allocate host memory. cl.MEM_COPY_HOST_PTR 
Implementation request OpenCL to allocate host memory and copy  Reading from a WRITE_ONLY memory object, 
or Writing to a READ_ONLY memory object, is undefined. These flags are mutually exclusive. srcBuffer 
must be a Typed Array already allocated by the application and sizeInBytes = srcBuffer.byteLength. MEM_USE_HOST_PTR 
is mutually exclusive with MEM_ALLOC_HOST_PTR and MEM_COPY_HOST_PTR. However, MEM_COPY_HOST_PTR can be 
specified with MEM_ALLOC_HOST_PTR. On AMD and NVidia GPUs and on some operating systems, using MEM_ALLOC_HOST_PTR 
may result in pinned host memory to be used, which may result in improved performance [8][9]. A sub-buffer 
can be created from an existing WebCLBuffer object as a new WebCLBuffer object. 1 // create a sub-buffer 
2 var subbuffer = buffer.createSubBuffer(flags, offset, size); Note: only reading from a buffer object 
and its sub-buffer objects or reading from multiple overlapping sub-buffer objects is defined. All other 
concurrent reading or writing is undefined. A WebCLImage is used to store a 1D, 2D, or 3D dimensional 
texture, render-buffer, or image. The elements of an image object are selected from a predefined list 
of image formats. However, currently, WebCL only supports 2D images. 1 // create a 32-bit RGBA WebCLImage 
object 8 }; 9 10 // Image on device 11 var image = ctx.createImage(cl.MEM_READ_ONLY | cl.MEM_USE_HOST_PTR, 
format, imageBuffer);  'order' refers to the memory layout in which pixel data channels are stored in 
the image. 'data_type' is the type of the channel data type. 'size' refers to the image size. 'rowPitch 
refers to the scan-line pitch in bytes. If imageBuffer is null, it must be 0. Otherwise, it must be at 
least image_width * sizeInBytesOfChannelElement, which is the default if rowPitch is not specified. imageBuffer 
is a Typed Array that contain the image data already allocated by the application. imageBuffer.byteLength 
>= rowPitch * image_height. The size of each element in bytes must be a power of 2. A WebCLSampler describes 
how to sample an image when the image is read in a kernel function. It is similar to WebGL samplers. 
1 // create a sampler object 2 var sampler = ctx.createSampler(normalizedCoords, addressingMode, filterMode); 
 normalizedCoords is cl.TRUE or true indicates image coordinates specified are normalized. addressingMode 
indicated how out-of-range image coordinates are handled when reading an image. This can be set to CL_ADDRESS_MIRRORED_REPEAT, 
CL_ADDRESS_REPEAT, CL_ADDRESS_CLAMP_TO_EDGE, CL_ADDRESS_CLAMP and CL_ADDRESS_NONE. filterMode specifies 
the type of filter to apply when reading an image. This can be cl.FILTER_NEAREST or cl.FILTER_LINEAR. 
 5.1.2.4 Passing,arguments,to,a,kernel, Passing arguments to a kernel function is complicated by JavaScript 
un-typed nature: JavaScript provides a Number object and there is no way to know if this is a 32-bit 
integer, a 16-bit short, a 32-bit float, and so on. In fact, JavaScript numbers are typically 64-bit 
double. As a result, developers must provide the type of arguments used in a kernel function. The WebCLKernel.setArg() 
method has two definitions: one for scalar and vector types and one for memory objects (buffers and images) 
and sampler objects. Table 1 provides the relationships between OpenCL C types and values used in kernel 
methods arguments and setArg() arguments. Values referring to local memory use the special type cl.type.LOCAL_MEMORY_SIZE 
because local variables can t be initialized by host or device but host can tell the device how many 
bytes to allocate for a kernel argument. As a rule of thumb, scalar values are passed by value directly 
in setArg(). Buffers/Images/Vectors values are passed by commands to transfer their host memory to the 
device memory. 1 // Sets value of kernel argument idx with value of scalar/vector type 2 kernel.setArg(idx, 
value, type); 3 4 // Sets value of kernel argument idx with value as memory object or sampler 5 kernel.setArg(idx, 
a_webCLObject); Code 8 WebCLKernel.setArg() definition For example, 1 // Sets value of argument 0 to 
the integer value 5  Kernel argument type setArg() value setArg() cl.type Remarks char, uchar scalar 
CHAR, UCHAR 1 byte short, ushort scalar SHORT, USHORT 2 bytes int, uint scalar INT, UINT 4 bytes long, 
ulong scalar LONG, ULONG 4 bytes float scalar FLOAT 4 bytes half, double scalar HALF, DOUBLE No on all 
implementations 2 bytes (half), 8 bytes (double) <char double>N WebCLBuffer VECN N = 2, 3, 4, 8, 16 May 
be null if global or constant value char, , double * WebCLBuffer May be null if global or constant value 
image2d_t WebCLImage sampler_t WebCLSampler __local LOCAL_MEMORY_SIZE Size initialized in kernel Table 
1 Relationships between C types used in kernels and setArg() s cl.type.* If the argument of a kernel 
function is declared with the __constant qualifier, the size in bytes of the memory object cannot exceed 
cl.DEVICE_MAX_CONSTANT_BUFFER_SIZE. Note 1: OpenCL allows passing structures as byte arrays to kernels 
but WebCL currently doesn t for portability. The main reason is that endianness between host and devices 
may be different and this would require developers to format their data for each device s endianness 
even on the same machine. Note 2: all WebCL API calls are thread-safe, except kernel.setArg(). However, 
kernel.setArg() is safe as long as concurrent calls operate on different WebCLKernel objects. Behavior 
is undefined if multiple threads call on the same WebCLKernel object at the same time. 5.1.2.5 Controlling,device,execution,with,command,queues, 
Operations on WebCL objects such as memory, program and kernel objects are performed using command queues. 
A command queue contains a set of operations or commands. Applications may use multiple independent command 
queues without synchronization as long as commands don t apply on shared objects between command queues. 
Otherwise, synchronization is required. Commands are queued in order but execution may be in order (default) 
or out of order. This means that if a command-queue contains command A and command B, an in-order command-queue 
object guarantees that command B is executed when command A finishes. If an application configures a 
command-queue to be out-of-order, there is no guarantee that commands finish in the order they were queued. 
For out-of-order queues, a wait for events or a barrier command can be enqueued in the command-queue 
to guarantee previous commands finish before the next batch of commands is executed. Out-of-order queues 
are an advanced topic we won t cover in this course. Interested readers should refer to Derek Gerstmann 
Siggraph Asia 2009 on Advanced OpenCL Event Model Usage [20]. Moreover, device support for out-of-order 
queues is optional in OpenCL and many current drivers don t support it. It is useful to test for out-of-order 
support and, if an exception is thrown, then create an in-order queue. Note: a command queue is attached 
to a specific device. And multiple command queues can be used per device. One application is to overlap 
kernel execution with data transfers between host and device [8]. Figure 6 shows the timing benefit if 
a problem could be separated in half: The first half of the data is transferred from host to, taking 
half the time of the full data set. Then kernel is executed, possibly in half time needed with the full 
data set. And finally result is transferred back to device in half the time of the full result set. 
 Just after the first half is transferred, the second half is transferred from host to device, and the 
same process is repeated.  Figure 6 Using multiple command-queues for overlapped data transfer. 5.1.2.6 
CommandDqueue,execution, Once a set of commands have been queued, WebCL offers two ways to execute the 
command-queue: 1 // execute a task2 queue.enqueueTask(kernel); 3 4 // execute a NDRange queue.enqueueNDRange(kernel, 
offsets, globals, locals); With enqueueTask(), the kernel is executed using a single work-item. This 
is a very restricted form of enqueueNDRange(). enqueueNDRange() has first parameters: kernel the kernel 
to execute offsets offsets to apply to globals. If null, then offsets=[0, 0, 0]  globals the problem 
size per dimension  locals the number of work-items per work-group per dimension. If null, the device 
will choose the appropriate number of work-items Recall Figure 3 where globals and locals relationships 
are depicted. If we want to execute a kernel over an image of size (width, height), then globals may 
be [width, height] and locals may be [16, 16]. Since enqueueNDRange() will fail if locals size is more 
than cl.KERNEL_WORK_GROUP_SIZE, in practice, it may be useful to do 1 locals[0] = kernel.getWorkGroupInfo(device, 
cl.KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE); 8 return (x % y == 0) ? (x / y) : (x / y + 1); 9} Code 
10 A way to optimally setup locals and globals NDRanges. 5.1.2.7 Command,Synchronization, Nearly all 
commands available in WebCLCommandQueue class have two final parameters: event_list an array of WebCLEvents 
 event an event returned by the device to monitor the execution status of a command  By default, event_list 
and event are null for a command, meaning that the command is executed as blocking the host thread until 
it is queued in the device s command queue. If a programmer doesn t want to block the host thread while 
a command is being executed, the device can return an event and the host code can register a callback 
to be notified once the command complete. 1 // Enqueue kernel  // event.status = cl.COMPLETE or error 
if negative 40 // event.status = cl.COMPLETE or error if negative 41 // event.data contains a WebCLMemoryObject 
with values from device 42 // data contains Read complete 43 } Code 11 Using WebCLEvent callbacks. In 
Code 11, for the commands we wish to get notified on their cl.COMPLETE status, we first create a WebCLEvent 
object, pass it to the command, then register a JavaScript callback function. Note 1: the last argument 
of WebCLEvent.setCallback() can be anything. And this argument is passed untouched as the last argument 
of the callback function. Note 2: in the case of enqueue Read/Write WebCLBuffers or WebCLImages, as in 
line 22, clBuffer ownership is transferred from host to device. Thus, when read_complete() callback is 
called, clBuffer ownership is transferred back from device to host. This means that once the ownership 
of clBuffer is transferred (line 22), the host cannot access or use this buffer any more. Once the callback 
is called, line 40, the host can use the buffer again. 5.1.2.8 Profiling,commands, To enable timing 
of commands, one creates a command-queue with option cl.QUEUE_PROFILING_ENABLE. Then, WebCLEvents can 
be used to time a command. Code 12 shows how to profile an enqueueReadBuffer() command. 1 // Create a 
command queue for profiling Code 12 How to profile a command. Note: timestamps are given in nanoseconds 
(10-9 seconds). Likewise, to profile the duration of a kernel: 1 // Enqueue kernel  Code 13 Profiling 
a kernel.   5.2 Device!side! Kernels are written in a derivative of C99 with the following caveats: 
 A file may have multiple __kernel functions (similar to a library with multiple entry points)  No recursion 
since there is no call stack on devices  All functions are inlined to the kernel functions  No dynamic 
memory (e.g. malloc(), free() )  No function pointer  No standard libc libraries (e.g. memcpy(), strcmp() 
)  No standard data structures (except vector operations)  Helper functions o Barriers o Work-item 
functions o Atomic operations o Vector operations o Math operations and fast native (hardware accelerated) 
math operations o IEEE754 floating-point o 16-bit floats and doubles (optional)   Built-in data types 
 o 8, 16, 32, 64-bit values o Image 2D (and 3D but not in WebCL 1.0), Sampler, Event o 2, 3, 4, 8, 
16-component vectors   New keywords o Function qualifiers: __kernel o Address space qualifiers: __global, 
__local, __constant, __private (default), o Access qualifiers: __read_only, __write_only, __read_write, 
  Preprocessor directives (#define, #pragma)  Appendices A.3 and A.4 provide examples of kernels. 
 6 Interoperability!with!WebGL! Recall that WebCL is for computing, not for rendering. However, if your 
data already resides in the GPU and you need to render it, wouldn t it be faster to tell OpenGL to use 
it rather than reading it from the GPU memory to CPU memory and send it again to OpenGL on your GPU? 
This is where WebGL comes in. Since WebCL is using data from WebGL, the WebGL context must be created 
first. Then, a shared WebCL context can be created. This GL share group object manages shared GL and 
CL resources such as Textures objects contain texture data in image form,  Vertex buffers objects 
(VBOs) contains vertex data such as coordinates, colors, and normal vectors,  Renderbuffer objects 
 contain images used with GL framebuffer objects.  Initialization Rendering loop (per frame) Figure 
7 Typical algorithm for WebCL WebGL applications 6.1 Fun!with!2!triangles! Applications such as image 
processing and ray tracing produce an output image whose pixels are drawn onto the screen. For such applications, 
it suffices to map the output image onto 2 unlit screen-aligned triangles rendered by GL. A compute kernel 
provides more flexible ways to optimize generic computations than a fragment shader. More importantly, 
texture memory is cached and thus provides a faster way to access data than regular (global) memory. 
However, in devices without image memory support, one should use WebCLBuffers and update GL textures 
with Pixel Buffer Objects. In this section, we use Iñigo Quilez excellent ShaderToy s Mandelbulb fragment 
shader [24] converted as a CL kernel, depicted in Figure 8. The whole WebGL scene consists in 2 textured 
triangles filling a canvas. WebCL generates the texture at each frame. Therefore, for a canvas of dimension 
[width, height], WebCL will generate width * height pixels. We will detail each step and the full program 
is given in Appendix A. In [24], you can find more cool shaders that you can easily convert by the following 
the guidelines for this sample. Figure 8 Two triangles filling the canvas to draw a WebCL generated 
image. 6.1.1 General!CLLGL!interoperability!algorithm! Since CL uses GL buffers for compute, WebGL context 
must first be initialized and then WebCL context is created sharing that WebGL context. Once both contexts 
are initialized, it is possible to create shared objects by creating first the WebGL object, then the 
corresponding WebCL object from the WebGL object. The general algorithm is as follows: function Init_GL() 
{ 54 } Code 14 General algorithm for WebCL-WebGL interoperability. The remainder of this section will 
focus on how to create shared CLGL objects and how to use them. 6.1.2 Using!shared!textures! Initialize 
a WebCLImage object from a WebGLImage object as follows: 1 // Create OpenGL texture object 2 Texture 
= gl.createTexture(); 3 gl.bindTexture(gl.TEXTURE_2D, Texture); 4 gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, 
gl.NEAREST); 5 gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);  gl.texImage2D(gl.TEXTURE_2D, 
0, gl.RGBA, TextureWidth, TextureHeight, 0, gl.RGBA, Code 15 Initialize a WebCLImage object from a 
WebGLImage object. Set the WebCLImage as an argument of your kernel: 1 kernel.setArg(0, clTexture);2 
kernel.setArg(1, TextureWidth, cl.type.UINT);3 kernel.setArg(2, TextureHeight, cl.type.UINT); Finally, 
here is how to use this WebCLImage inside your kernel code: 1 __kernel 8 9 write_imagef(pix, (int2)(x,y), 
color); 10 }  Code 16 Using a WebCLImage data inside a kernel. Note: it should be possible to use write_imagei() 
or write_imageui() with int4 colors. However, at the time of writing (May 2012), this doesn t seem to 
work with latest AMD and NVidia drivers. The code presented in this section is the only way I found to 
work with textures between CL and GL. 6.1.3 Using!shared!buffers! A WebCLBuffer is created from a WebGLBuffer 
as follows. On line 6, it is important to specify the correct sizeInBytes of the buffer. 1 // create 
a WebGLBuffer 15 } Code 17 Using a WebCLImage data inside a kernel. Since a GL ARRAY_BUFFER can be 
used for vertices, normals, colors, texture coordinates, texture data, and more, WebCL can be used to 
schedule processing of these buffers. If the device doesn t support texture interoperability between 
CL and GL, a buffer can be used to update a WebGLImage sub-texture as follows with the assumption that 
WebCLBuffer contains RGBA values for each pixel. 1 // Create OpenGL texture object 2 Texture = gl.createTexture(); 
3 gl.bindTexture(gl.TEXTURE_2D, Texture); 4 gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST); 
5 gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);  gl.texImage2D(gl.TEXTURE_2D, 
0, gl.RGBA, TextureWidth, TextureHeight, 0, gl.RGBA, Code 18 Updating a texture from a WebGLBuffer 
 6.1.4 Example! The following example consists of 2 module objects, whose code is given in Appendix A: 
Graphics encapsulates WebGL calls Compute encapsulates WebCL calls The code is rather large for just 
setting up WebCL and WebGL but, fear not, this is just boilerplate you can reuse! The main method works 
as follows: Create a Canvas object  Instantiate Graphics and Compute objects  Launch the main rendering 
method  If the window is resized, we call Graphics to configure the shared GL texture. Then, we call 
Compute to configure the CL texture from this GL texture.  At each frame, we reset the kernel argument 
with the current timestamp in seconds. Then, the kernel is executed.  Finally, Graphics renders the 
frame   1 var COMPUTE_KERNEL_ID = "mandelbulb.cl"; // <script> id  compute.clean(); Code 19 Main 
method for CL-GL program. The kernel for such applications has the form:  const int yl = get_local_id(1); 
 Code 20 Kernel for texture-based rendering. Note 1: we don t pass the size of the shared texture since 
the dimension of our problem is the full size of the texture itself. In other words, when executing the 
kernel with enqueueNDRange(), the globals argument is [ width, height ], and that s what we retrieve 
in lines 9 and 10 in Code 20. Note 2: for this example, we only pass the timestamp of the current frame 
to the kernel. For user interactivity, one should also pass mouse coordinates, window size, and other 
user/application attributes. In Appendix, we provide the fragment shader code of the Mandelbulb shader 
by Iñigo Quilez [24], as well as the direct transformation to OpenCL and an optimized OpenCL version. 
We chose this example because the ray-marching algorithm (also known as sphere tracing [25]) used to 
render the mandelbulb fractal requires lots of operations per pixel; a good candidate for CL optimizations. 
Note that this is not necessarily the fastest way to render such mathematical objects. On our machine, 
this leads to 6 fps for WebGL version [24], 8 fps for non-optimized OpenCL version (Appendix A.2), and 
12 fps for the optimized OpenCL version (Appendix A.4).  6.2 Other!applications! In general, CL applications 
perform many matrix operations, whether the result is to be rendered directly onto the screen (e.g. in 
a texture) or not. For example, the famous N-body simulation calculates at each frame the position of 
astronomical objects, which are then rendered by GL [23]. An array of structures that contains position 
and other attributes is shared between host and device; the device performing all the calculations of 
the interactions between objects. CL can also share vertex buffers and render buffers with GL. This allows 
developers to do all kind of complex geometry and special effects that can be inserted in GL s rendering 
pipeline.  7 Tips!and!tricks! NVidia and AMD excellent programming guides [8][9] provide lots of tips 
to optimize OpenCL programs. In our experience, we recommend following this strategy: Use Host code 
for serial code, use Device code for parallel code  Write your code in serialized form (i.e. test it 
on a host CPU) and identify the areas that are good candidates for data-parallel optimizations o As 
a rule of thumb: identify where iterations are repeated on data, these are good candidates for data-parallel 
optimizations  In your kernel, initialize first local memory with data from global memory that will 
be used often in your algorithm  Group memory transfers together, this favors memory coalescing  Identify 
where synchronization between work-items of the same work-group is necessary  At the end of your kernel, 
write results from local memory back to global memory  Rewrite your algorithm to minimize control flow 
divergence (i.e. if, switch, for, do, while). If threads in the same warp/wavefront take different execution 
paths, these execution paths will be serialized, thereby reducing throughput until the execution paths 
converge again.  7.1 From!GLSL!to!OpenCL!C! In converting GLSL shader to OpenCL C, we recommend following 
these guidelines: GLSL s vecN type are changed to OpenCL s floatN type o Initializations in OpenCL 
are: (floatN)(val1,...,valN) instead of vecN(val1, ,valN) in GLSL  by default all floating point values 
are double in CL, make sure to add f at the end. out arguments of methods must be pointers  if numerical 
precision is not too important, compile with cl-fast-relaxed-math, -cl-mad-enable, and use native_* functions 
(i.e. native_sin() instead of sin()).  Use rsqrt() instead of 1.0f/sqrt()  7.2 Barriers! Barriers 
are an important mechanism to wait for all work-items to be synchronized at points in the code. However, 
it is very important NOT to use barriers in if/else constructs. The reason is that some work items may 
not sync at the barrier, while others may block at the barrier; resulting in a deadlock of the GPU (i.e. 
you would have to reset your machine!). The pattern to use a barrier is: Load values into local memory 
 barrier(CL_LOCAL_MEM_FENCE); // wait for load to finish  Use local memory in your algorithm  barrier(CL_LOCAL_MEM_FENCE); 
// wait for all work-items  7.3 Local!workLgroup!size! When running a kernel, the method enqueueNDRangeKernel(), 
takes the parameters: global_work_size the global number of work-items in N dimensions i.e. the size 
of the problem. local_work_size the number of work-items that make up a work-group. Synchronization 
between work-items (with barriers) can only be within a work-group. If local_work_size[0] * local_work_size[1] 
* * local_work_size[N-1] > kernel.getWorkGroupInfo(device, cl.KERNEL_WORK_GROUP_SIZE), the program won 
t execute! cl.KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE can be used to make block-size multiple of that 
size. AMD calls that size wavefront size and NVidia calls it warp size. Note: this value is often 32 
for NVidia GPUs, 64 for AMD GPUs. Since kernels can t allocate memory dynamically, one trick could be 
to compile a small program to get such kernel dependent values, add them on top of your real program 
code as constants (or #define) before compiling it. 7.4 Learn!parallel!patterns!! Parallel programming 
is not new. In fact, it might be as old as modern computers. Since the 60s lots of work has been done 
on supercomputers and many patterns have been found but they are still an active area of research. Learning 
how to use these patterns can simplify your code and more importantly lead to faster performance for 
your programs 0[13][21]. Algorithms such as map, reduce, scan, scatter/gather, stencils, pack [21], Berkely 
Parallel Computing Laboratory s pattern language for parallel computing [32], and Murray Cole s algorithmic 
skeletons 0 are examples of such parallel algorithms and methodologies you need to know.  8 WebCL!implementations! 
At the time of writing, the following prototypes are available: Nokia WebCL prototype [16] as a Mozilla 
FireFox extension  Mozilla FireFox implementation [18]  Samsung WebKit prototype [17]  Motorola Mobility 
node-webcl module [15], a Node.JS based implementation.  Motorola Mobility node-webcl implementation 
is based on Node.JS, which uses Google V8 JavaScript engine, as in Google Chrome browser. This implementation 
is up to date with the latest WebCL specification and allows quick prototyping of WebCL features before 
they become available in browsers. Coupled with Node.JS features, it also enables server-side applications 
using WebCL. All examples in this course have been developed and tested first with node-webcl. 9 Perspectives! 
This course provided the foundations for developers to experiment with the exciting world of high-performance 
computing on the web. OpenCL is a rather young technology and it is not uncommon to find bugs in current 
implementations. However, WebCL implementations would abstract these technical issues for safer, more 
robust, more secure, and more portable applications, as the specification mature with feedback from users, 
hardware manufacturers and browser vendors. Meanwhile, prototype WebCL implementations are already available 
and we hope this course gave you all the excitement to start hacking your GPUs today for cool applications 
tomorrow Appendix!A CL>GL!code! This appendix provides source code for applications described in section 
6.1.4. The first two sections provide the Graphics and Compute module objects. The third section is a 
direct translation from GLSL to OpenCL kernel language using techniques described in section 7.1. The 
last section is an example optimized version using local memory and work-groups. A.1 Graphics!object! 
1 /*  61 " gl_FragColor = texture2D(uSampler, vTexCoords.st);", 62 "}" ].join("\n"), 63 }; 64 65 var 
shader; 66 var str = shaders[id]; 67 68 if (id.match(/-fs/)) { 69 shader = gl.createShader(gl.FRAGMENT_SHADER); 
70 } else if (id.match(/-vs/)) { 71 shader = gl.createShader(gl.VERTEX_SHADER); 72 } else { 73 throw 
'Shader '+id+' not found'; 74 } 75 76 gl.shaderSource(shader, str); 77 gl.compileShader(shader); 78 
79 if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) { 80 throw gl.getShaderInfoLog(shader); 81 
} 82 83 return shader; 84 } 85 86 /* 87 * Initialize vertex and fragment shaders, link program and scene 
objects 88 */ 89 function init_shaders() { 90 log(' Init shaders'); 91 var fragmentShader = compile_shader(gl, 
"shader-fs"); 92 var vertexShader = compile_shader(gl, "shader-vs"); 93 94 shaderProgram = gl.createProgram(); 
95 gl.attachShader(shaderProgram, vertexShader); 96 gl.attachShader(shaderProgram, fragmentShader); 97 
gl.linkProgram(shaderProgram); 98 99 if (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) 100 
throw "Could not link shaders"; 101 102 gl.useProgram(shaderProgram); 103 104 shaderProgram.vertexPositionAttribute 
= gl.getAttribLocation(shaderProgram, "aCoords"); 105 gl.enableVertexAttribArray(shaderProgram.vertexPositionAttribute); 
106 107 shaderProgram.textureCoordAttribute = gl.getAttribLocation(shaderProgram, "aTexCoords"); 108 
gl.enableVertexAttribArray(shaderProgram.textureCoordAttribute); 109 110 shaderProgram.samplerUniform 
= gl.getUniformLocation(shaderProgram, "uSampler"); 111 } 112 113 /* 114 * Render the scene at a timestamp. 
115 * 116 * @param timestamp in ms as given by new Date().getTime() 117 */ 118 function display(timestamp) 
{ 119 // we just draw a screen-aligned texture 120 gl.viewport(0, 0, gl.viewportWidth, gl.viewportHeight); 
121 122 gl.enable(gl.TEXTURE_2D); 123 gl.bindTexture(gl.TEXTURE_2D, TextureId); 124 125 // draw screen 
aligned quad 126 gl.bindBuffer(gl.ARRAY_BUFFER, VertexPosBuffer); 127 gl.vertexAttribPointer(shaderProgram.vertexPositionAttribute, 
128 VertexPosBuffer.itemSize, gl.FLOAT, false, 0, 0); 129 130 gl.bindBuffer(gl.ARRAY_BUFFER, TexCoordsBuffer); 
131 gl.vertexAttribPointer(shaderProgram.textureCoordAttribute, 132 TexCoordsBuffer.itemSize, gl.FLOAT, 
false, 0, 0); 133 134 gl.activeTexture(gl.TEXTURE0); 135 gl.uniform1i(shaderProgram.samplerUniform, 
0);   A.2 Compute!object! The compute object reads a kernel from a string. The string may come from 
a <script type= x-webcl > or from a file. var /* cl_context */ clContext; 8 var /* cl_command_queue 
*/ clQueue; 9 var /* cl_program */ clProgram; var /* cl_device_id */ clDevice; 11 var /* cl_device_type 
*/ clDeviceType = cl.DEVICE_TYPE_GPU;12 var /* cl_image */ clTexture; 13 var /* cl_kernel */ clKernel; 
14 var max_workgroup_size, max_workitem_sizes, warp_size;15 var TextureWidth, TextureHeight;16 var COMPUTE_KERNEL_ID; 
17 var COMPUTE_KERNEL_NAME; 18 var nodejs = (typeof window === 'undefined');19 /*21 * Initialize WebCL 
context sharing WebGL context22 * 23 * @param gl WebGLContext24 * @param kernel_id the <script> id of 
the kernel source code25 * @param kernel_name name of the __kernel method 26 */27 function init(gl, 
kernel_id, kernel_name) {28 log('init CL');29 if(gl === 'undefined' || kernel_id === 'undefined' || kernel_name 
=== 'undefined')31 throw 'Expecting init(gl, kernel_id, kernel_name)'; 32 33 COMPUTE_KERNEL_ID = kernel_id; 
34 COMPUTE_KERNEL_NAME = kernel_name; 35 36 // Pick platform37 var platformList = cl.getPlatforms();38 
var platform = platformList[0];39 // create the OpenCL context 41 clContext = cl.createContext({ 42 
 deviceType: clDeviceType, 43 shareGroup: gl, 44 platform: platform }); 45 46 var device_ids = clContext.getInfo(cl.CONTEXT_DEVICES); 
47 if (!device_ids) { 48 throw "Error: Failed to retrieve compute devices for context!"; 49 } 51 var 
device_found = false; 52 for(var i=0,l=device_ids.length;i<l;++i ) { 53 device_type = device_ids[i].getInfo(cl.DEVICE_TYPE); 
54 if (device_type == clDeviceType) { 55 clDevice = device_ids[i]; 56 device_found = true; 57 break; 
58 } 59 } 61 if (!device_found) 62 throw "Error: Failed to locate compute device!"; 63 64 // Create 
a command queue 65 try { 66 clQueue = clContext.createCommandQueue(clDevice, 0); 67 } 68 catch(ex) { 
69 throw "Error: Failed to create a command queue! "+ex; } 71 72 // Report the device vendor and device 
name 73 var vendor_name = clDevice.getInfo(cl.DEVICE_VENDOR); 74 var device_name = clDevice.getInfo(cl.DEVICE_NAME); 
75 log("Connecting to " + vendor_name + " " + device_name); 76 77 if (!clDevice.getInfo(cl.DEVICE_IMAGE_SUPPORT)) 
78 throw "Application requires images: Images not supported on this device."; 79 init_cl_buffers();81 
init_cl_kernels();82 } 83 84 /* 85 * Initialize WebCL kernels 86 */ 87 function init_cl_kernels() { 
88 log(' setup CL kernel'); 89 90 clProgram = null; 91 92 if(!nodejs) { 93 var sourceScript = document.getElementById(COMPUTE_KERNEL_ID); 
94 if (!sourceScript) 95 throw "Can't find CL source <script>"; 96 97 var str = ""; 98 var k = sourceScript.firstChild; 
99 while (k) { 100 if (k.nodeType == 3) { 101 str += k.textContent; 102 } 103 k = k.nextSibling; 104 
} 105 if (sourceScript.type == "x-webcl") 106 source = str; 107 else 108 throw "<script> type should 
be x-webcl"; 109 } 110 else { 111 log("Loading kernel source from file '" + COMPUTE_KERNEL_ID + "'..."); 
112 source = fs.readFileSync(__dirname + '/' + COMPUTE_KERNEL_ID, 'ascii'); 113 if (!source) 114 throw 
"Error: Failed to load kernel source!"; 115 } 116 117 118 // Create the compute program from the source 
buffer 119 try { 120 clProgram = clContext.createProgram(source); 121 } 122 catch(ex) { 123 throw "Error: 
Failed to create compute program! "+ex; 124 } 125 126 // Build the program executable 127 try { 128 clProgram.build(clDevice, 
'-cl-unsafe-math-optimizations -cl-single-precision-constant -cl­ fast-relaxed-math -cl-mad-enable');129 
} catch (err) {130 throw "Error: Failed to build program executable!\n" 131   + clProgram.getBuildInfo(clDevice, 
cl.PROGRAM_BUILD_LOG); 132 }133 134 // Create the compute kernels from within the program135 try {136 
clKernel = clProgram.createKernel(COMPUTE_KERNEL_NAME);137 }138 catch(ex) {139 throw "Error: Failed to 
create compute row kernel! "+ex; 140 }141 142 // Get the device intrinsics for executing the kernel on 
the device143 max_workgroup_size = clKernel.getWorkGroupInfo(clDevice, cl.KERNEL_WORK_GROUP_SIZE);144 
max_workitem_sizes=clDevice.getInfo(cl.DEVICE_MAX_WORK_ITEM_SIZES);145 warp_size=clKernel.getWorkGroupInfo(clDevice, 
cl.KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE); 146 log(' max workgroup size: '+max_workgroup_size);147 
log(' max workitem sizes: '+max_workitem_sizes);148 log(' warp size: '+warp_size);149 }150 151 /*152 
 * (Re-)set kernel arguments 153 * 154 * @param time timestamp in ms (as given by new Date().getTime()155 
* @param image_width width of the image156 * @param image_height height of the image 157 */ 158 function 
resetKernelArgs(time, image_width, image_height) { 159 TextureWidth = image_width; 160 TextureHeight 
= image_height; 161 162 // set the kernel args 163 try { 164 // Set the Argument values for the row kernel 
165 clKernel.setArg(0, clTexture); 166 clKernel.setArg(1, time, cl.type.FLOAT); 167 } catch (err) { 168 
throw "Failed to set row kernel args! " + err; 169 } 170 } 171 172 /* 173 * Initialize WebCL buffers 
174 */ 175 function init_cl_buffers() { 176 //log(' create CL buffers'); 177 } 178 179 /* 180 * Configure 
shared data with WebGL i.e. our texture 181 * 182 * @param gl WebGLContext 183 * @param glTexture WebGLTexture 
to share with WebCL 184 */ 185 function configure_shared_data(gl, glTexture) { 186 // Create OpenCL representation 
of OpenGL Texture 187 clTexture = null; 188 try { 189  clTexture = clContext.createFromGLTexture2D(cl.MEM_WRITE_ONLY, 
190 gl.TEXTURE_2D, 0, glTexture); 191 } catch (ex) { 192 throw "Error: Failed to create CL Texture object. 
" + ex; 193 } 194 195 return clTexture; 196 } 197 198 /* 199 * Execute kernel possibly at each frame 
before rendering results with WebGL 200 * 201 * @param gl WebGLContext 202 */ 203 function execute_kernel(gl) 
{ 204 // Sync GL and acquire buffer from GL 205 gl.flush(); 206 clQueue.enqueueAcquireGLObjects(clTexture); 
207 208 // Set global and local work sizes for kernel 209 var local = []; 210 local[0] = warp_size; 211 
local[1] = max_workgroup_size / local[0]; 212 var global = [ clu.DivUp(TextureWidth, local[0]) * local[0], 
213 clu.DivUp(TextureHeight, local[1]) * local[1] ]; 214 215 // default values 216 //var local = null; 
217 //var global = [ TextureWidth, TextureHeight ]; 218 219 try { 220 clQueue.enqueueNDRangeKernel(clKernel, 
null, global, local); 221 } catch (err) { 222 throw "Failed to enqueue kernel! " + err; 223 } 224 225 
// Release GL texture 226 clQueue.enqueueReleaseGLObjects(clTexture); 227 clQueue.flush(); 228 } 229 
230 return { 231 'init':init,  232 'configure_shared_data': configure_shared_data,  A.3 Mandelbulb!kernel!(direct!conversion)! 
The Mandelbulb 3D fractal, raymarched and colored with orbit traps and fake ambient occlusion by Iñigo 
Quilez [24] with authorization, is converted directly to an OpenCL kernel. 1 // forward declarations 
 58 }59 }60 61 *resColor = trap; 62 *resPot = 0; 63 return true; 64 } 65 66 inline bool ifractal( float3 
ro, float3 rd, float *rest, float maxt, 67 float3 *resnor, float4 *rescol, float fov ) { 68 float4 sph 
= (float4)( 0.0, 0.0, 0.0, 1.25 ); 69 float2 dis; 70 71 // bounding sphere 72 if( !isphere(sph,ro,rd,&#38;dis) 
) 73 return false; 74 75 // early skip 76 if( dis.y<0.001f ) return false; 77 78 // clip to near! 79 
if( dis.x<0.001f ) dis.x = 0.001f; 80 81 if( dis.y>maxt) dis.y = maxt; 82 83 float dt; 84 float3 gra; 
85 float4 color; 86 float4 col2; 87 float pot1; 88 float pot2, pot3, pot4; 89 90 float fovfactor = 1.0f/sqrt(1+fov*fov); 
91 92 // raymarch! 93 for( float t=dis.x; t<dis.y; ) { 94 float3 p = ro + rd*t; 95 96 float Surface = 
clamp( 0.001f*t*fovfactor, 0.000001f, 0.005f ); 97 98 float eps = Surface*0.1f; 99 100 if( iterate(p,&#38;pot1,&#38;color) 
) { 101 *rest = t; 102 *resnor=normalize(gra); 103 *rescol = color; 104 return true; 105 } 106 107 iterate(p+(float3)(eps,0.0,0.0),&#38;pot2,&#38;col2); 
108 iterate(p+(float3)(0.0,eps,0.0),&#38;pot3,&#38;col2); 109 iterate(p+(float3)(0.0,0.0,eps),&#38;pot4,&#38;col2); 
110 111 gra = (float3)( pot2-pot1, pot3-pot1, pot4-pot1 ); 112 dt = 0.5f*pot1*eps/length(gra); 113 114 
if( dt<Surface ) { 115 *rescol = color; 116 *resnor = normalize( gra ); 117 *rest = t; 118 return true; 
119 } 120 121 t+=dt; 122 } 123 124 return false; 125 } 126 127 __kernel 128 void compute(__write_only 
image2d_t pix, float time) { 129 int x=get_global_id(0), y=get_global_id(1); 130 const int width = get_global_size(0); 
131 const int height = get_global_size(1); 132 float2 resolution=(float2)(width,height); 133 float2 gl_FragCoord=(float2)(x,y); 
134 135 float2 p = (float2)(-1.f + 2.f * gl_FragCoord.xy / resolution.xy);  136 float2 s = p*(float2)(1.33,1.0); 
137 138 float3 light1 = (float3)( 0.577f, 0.577f, 0.577f ); 139 float3 light2 = (float3)( -0.707f, 0, 
0.707f ); 140 141 float fov = 1; 142 float r = 1.4f+0.2f*cospi(2.f*time/20.f); 143 float3 campos = (float3)( 
r*sinpi(2.f*time/20.f), 144       0.3f-0.4f*sinpi(2.f*time/20.f), 145 r*cospi(2.f*time/20.f) ); 
146 float3 camtar = (float3)(0,0.1,0); 147 148 //camera matrix 149 float3 cw = normalize(camtar-campos); 
150 float3 cp = (float3)(0,1,0); 151 float3 cu = normalize(cross(cw,cp)); 152 float3 cv = normalize(cross(cu,cw)); 
153 154 // ray dir 155 float3 rd; 156 float3 nor, rgb; 157 float4 col; 158 float t; 159 160 rd = normalize( 
s.x*cu + s.y*cv + 1.5f*cw ); 161 162 bool res=ifractal(campos,rd,&#38;t,1e20f,&#38;nor,&#38;col,fov); 
163 164 if( !res ) { 165 rgb = 1.3f*(float3)(1,.98,0.9)*(0.7f+0.3f*rd.y); 166 } 167 else { 168 float3 
xyz = campos + t*rd; 169 170 // sun light 171 float dif1 = clamp( 0.2f + 0.8f*dot( light1, nor ), 0.f, 
1.f ); 172 dif1=dif1*dif1; 173 174 // back light 175 float dif2 = clamp( 0.3f + 0.7f*dot( light2, nor 
), 0.f, 1.f ); 176 177 // ambient occlusion 178 float ao = clamp(1.25f*col.w-.4f,0.f,1.f); 179 ao=0.5f*ao*(ao+1); 
180 181 // shadow 182 if( dif1>0.001f ) { 183 float lt1; 184 float3 ln; 185 float4 lc; 186 if( ifractal(xyz,light1,&#38;lt1,1e20,&#38;ln,&#38;lc,fov) 
) 187  dif1 = 0.1f; 188 } 189 190 // material color 191 rgb = (float3)(1); 192 rgb = mix( rgb, (float3)(0.8,0.6,0.2), 
(float3)(sqrt(col.x)*1.25f) ); 193 rgb = mix( rgb, (float3)(0.8,0.3,0.3), (float3)(sqrt(col.y)*1.25f) 
); 194 rgb = mix( rgb, (float3)(0.7,0.4,0.3), (float3)(sqrt(col.z)*1.25f) ); 195 196 // lighting 197 
rgb *= (0.5f+0.5f*nor.y)* 198 (float3)(.14,.15,.16)*0.8f + 199 dif1*( float3)(1.0,.85,.4) + 200 0.5f*dif2*( 
float3)(.08,.10,.14); 201 rgb *= (float3)( pow(ao,0.8f), pow(ao,1.00f), pow(ao,1.1f) ); 202 203 // gamma 
204 rgb = 1.5f*(rgb*0.15f + 0.85f*sqrt(rgb)); 205 } 206 207 float2 uv = 0.5f*(p+1.f); 208 rgb *= 0.7f 
+ 0.3f*16.0f*uv.x*uv.y*(1.0f-uv.x)*(1.0f-uv.y); 209 rgb = clamp( rgb, (float3)(0), (float3)(1) ); 210 
 211 write_imagef(pix,(int2)(x,y),(float4)(rgb,1.0f));212 } A.4 Mandelbulb!kernel!(optimized)! The 
main idea is to move to local memory all parameters necessary for computation. #define WARPSIZE 256 
const float k2 = rsqrt( k3*k3*k3*k3*k3*k3*k3 ); 67 const float k1 = x4 + y4 + z4 - 6*y2*z2 - 6*x2*y2 
+ 2*z2*x2; 68 const float k4 = x2 - y2 + z2; 69 70 zz.x = q.x + 64*x*y*z*(x2-z2)*k4*(x4-6.0*x2*z2+z4)*k1*k2; 
71 zz.y = q.y + -16*y2*k3*k4*k4 + k1*k1; 72 zz.z = q.z + -8*y*k4*(x4*x4 - 28*x4*x2*z2 + 70*x4*z4 - 
28*x2*z2*z4 + z4*z4)*k1*k2; 73 74 m = dot(zz,zz); 75 76 trap = min( trap, (float3)(zz.xyz*zz.xyz,m) ); 
77 78 if( m > Bailout ) { 79 *resColor = trap; 80  *resPot = 0.5f*native_log(m)/native_powr(8.0f,i); 
81 return false; 82 } 83 } 84 85 *resColor = trap; 86 *resPot = 0; 87 return true; 88 } 89 90 inline 
bool ifractal( __local Ray *ray) { 91 __local Sphere *sph=&#38;ray->sph; 92 sph->origin = (float3)( 0); 
93 sph->r = 1.25f; 94 95 // bounding sphere 96 if( !isphere(ray) ) 97 return false; 98 99 // early skip 
100 if( sph->dis.y<EPS ) return false; 101 102 // clip to near! 103 if( sph->dis.x<EPS ) sph->dis.x = 
EPS; 104 105 if( sph->dis.y>MAXT) sph->dis.y = MAXT; 106 107 float dt; 108 float3 gra; 109 float4 color, 
col2; 110 float pot1, pot2, pot3, pot4; 111 112 // raymarch! 113 float t=sph->dis.x, Surface, eps; 114 
float3 p = ray->origin + ray->dir * t; 115 116 while(t < sph->dis.y) { 117 if( iterate(p,&#38;pot1,&#38;color) 
) { 118  ray->t = t; 119  ray->nor = fast_normalize(gra); 120  ray->col = color; 121 return true; 
122 } 123 124 Surface = clamp( EPS*t*ray->fovfactor, 0.000001f, 0.005f ); 125 eps = Surface*0.1f; 126 
127 iterate(p+(float3)(eps,0.0,0.0),&#38;pot2,&#38;col2); 128 iterate(p+(float3)(0.0,eps,0.0),&#38;pot3,&#38;col2); 
129 iterate(p+(float3)(0.0,0.0,eps),&#38;pot4,&#38;col2); 130 131 gra = (float3)( pot2-pot1, pot3-pot1, 
pot4-pot1 ); 132 dt = 0.5f*pot1*eps/fast_length(gra); 133 134 if( dt<Surface ) { 135  ray->col = color; 
136  ray->nor = fast_normalize( gra ); 137  ray->t = t; 138 return true; 139 } 140 141 t += dt;  142 
 p += ray->dir * dt; 143 } 144 145 return false; 146 } 147 148 __kernel 149 void compute(__write_only 
image2d_t pix, const float time) { 150 const int x = get_global_id(0); 151 const int y = get_global_id(1); 
152 const int xl = get_local_id(0); 153 const int yl = get_local_id(1); 154 const int tid = xl+yl*get_local_size(0); 
155 const int width = get_global_size(0)-1; 156 const int height = get_global_size(1)-1; 157 158 const 
float2 resolution = (float2)(width,height); 159 const float2 gl_FragCoord = (float2)(x,y); 160 161 const 
float2 p = (float2)(-1.f + 2.f * gl_FragCoord / resolution); 162 const float2 s = p*(float2)(1.33,1.0); 
163 164 const float fov = 0.5f, fovfactor = rsqrt(1+fov*fov); 165 166 const float ct=native_cos(2*M_PI_F*time/20.f), 
st=native_sin(2*M_PI_F*time/20.f); 167 const float r = 1.4f+0.2f*ct; 168 const float3 campos = (float3)( 
r*st, 0.3f-0.4f*st, r*ct ); 169 const float3 camtar = (float3)(0,0.1,0); 170 171 //camera matrix 172 
const float3 cw = fast_normalize(camtar-campos); 173 const float3 cp = (float3)(0,1,0); 174 const float3 
cu = fast_normalize(cross(cw,cp)); 175 const float3 cv = fast_normalize(cross(cu,cw)); 176 177 // ray 
178 __local Ray rays[WARPSIZE+1],*ray=rays+tid; 179 ray->origin=campos; 180 ray->dir = fast_normalize( 
s.x*cu + s.y*cv + 1.5f*cw ); 181 ray->fovfactor = fovfactor; 182 183 barrier(CLK_LOCAL_MEM_FENCE); 184 
185 const bool res=ifractal(ray); 186 187 if( !res ) { 188 // background color 189 ray->rgb = 1.3f*(float3)(1,0.98,0.9)*(0.7f+0.3f*ray->dir.y); 
190 } 191 else { 192 // intersection point 193 const float3 xyz = ray->origin + ray->t * ray->dir; 194 
195 // sun light 196 float dif1 = clamp( 0.2f + 0.8f*dot( light1, ray->nor ), 0.f, 1.f ); 197 dif1=dif1*dif1; 
198 199 // back light 200 const float dif2 = clamp( 0.3f + 0.7f*dot( light2, ray->nor ), 0.f, 1.f ); 
201 202 // ambient occlusion 203 const float aot = clamp(1.25f*ray->col.w-.4f, 0.f, 1.f); 204 const float 
ao=0.5f*aot*(aot+1); 205 206 // shadow: cast a lightray from intersection point 207 if( dif1 > EPS ) 
{ 208 __local Ray *lray=rays+256; 209  lray->origin=xyz; 210  lray->dir=light1; 211  lray->fovfactor 
= fovfactor; 212 if( ifractal(lray) ) 213 dif1 = 0.1f; 214 } 215 216 // material color  217 ray->rgb 
= (float3)(1);  Appendix!B OpenCL!and!CUDA!terminology! NVidia provides CUDA, an older API than OpenCL 
very used on their devices. CUDA and WebCL/OpenCL share similar concepts but a different terminology 
that we give below, borrowed from AMD article [30] and adapted to WebCL. Terminology WebCL/OpenCL CUDA 
Compute Unit (CU) Streaming Multiprocessor (SM) Processing Element (PE) Streaming Processor (SP) Work-item 
Thread Work-group Thread block Global memory Global memory Constant memory Constant memory Local memory 
Shared memory Private memory Local memory Writing kernels: qualifiers WebCL/OpenCL CUDA __kernel function 
__global__ function (no annotation necessary) __device__ function __constant variable __constant__ variable 
__global variable __device__ variable __local variable __shared__ variable  Writing kernels: indexing 
WebCL/OpenCL CUDA get_num_groups() gridDim get_local_size() blockDim get_group_id() blockIdx get_local_id() 
threadIdx get_global_id() No direct equivalent. Combine blockDim,  blockIdx, and threadIdx to get a 
global index. get_global_size() No direct equivalent. Combine gridDim and blockDim to get the global 
size Writing kernels: synchronization WebCL/OpenCL CUDA barrier() __syncthreads() No equivalent __threadfence() 
mem_fence(CLK_GLOBAL_MEM_FENCE | CLK_LOCAL_MEM-FENCE0 __threadfence_block() read_mem_fence() No equivalent 
write_mem_fence() No equivalent Important API objects WebCL/OpenCL CUDA WebCLDevice CUdevice WebCLContext 
CUcontext WebCLProgram CUmodule WebCLKernel CUfunction WebCLMemoryObject CUdeviceptr WebCLCommandQueue 
No equivalent Important API calls WebCL/OpenCL CUDA No initialization required cuInit() WebCLContext.getInfo() 
cuDeviceGet() WebCLContext.create() cuCtxCreate() WebCLContext.createCommandQueue() No equivalent WebCLProgram.build() 
No equivalent. CUDA programs are built off­line WebCLContext.createKernel() cuModuleGetFunction() WebCLCommandQueue.enqueueWriteBuffer() 
cuMemcpyHtoD() WebCLCommandQueue,enqueueReadBuffer() cuMemcpyDtoH() Using locals of WebCLCommandQueue.enqueueNDRange() 
cuFuncSetBlockShape() WebCLKernel.setArg() cuParamSet() Using WebCLKernel.setArg() cuParamSetSize() WebCLCommandQueue.enqueueNDRangeKernel() 
cuLaunchGrid() Implicit through garbage collection cuMemFree()    Bibliography! Specifications! [1] 
Aarnio, T. and Bourges-Sevenier, M. WebCL Working Draft. Khronos WebCL Working Group. https://cvs.khronos.org/svn/repos/registry/trunk/public/webcl/spec/latest/index.html. 
[2] Munshi, A. OpenCL Specification 1.1. Khronos OpenCL Working Group. http://www.khronos.org/registry/cl/specs/opencl-1.1.pdf. 
[3] Marrin, C. WebGL Specification. Khronos WebGL Working Group. http://www.khronos.org/registry/webgl/specs/latest/. 
[4] Munshi, A. and Leech, J. OpenGL ES 2.0.25. Khronos Group. http://www.khronos.org/registry/gles/specs/2.0/es_full_spec_2.0.25.pdf. 
[5] Simpson, R.J. The OpenGL ES Shading Language. Khronos Group. http://www.khronos.org/registry/gles/specs/2.0/GLSL_ES_Specification_1.0.17.pdf. 
[6] Herman, D. and Russell, K., eds. Typed Array Specification. Khronos.org. http://www.khronos.org/registry/typedarray/specs/latest/. 
[7] OpenCL 1.1 Reference Pages. OpenCL 1.1 Reference Pages. Khronos.org. http://www.khronos.org/registry/cl/sdk/1.1/docs/man/xhtml/. 
 Programming!guides! [8] NVidia OpenCL Programming Guide for the CUDA Architecture. 2012. NVidia OpenCL 
Programming Guide for the CUDA Architecture. [9] AMD Accelerated Parallel Processing OpenCL. 2011. AMD 
Accelerated Parallel Processing OpenCL. Books! [10] Gaster, B., Howes, L., Kaeli, D.R., Mistry, P., 
and Schaa, D. 2011. Heterogeneous Computing with OpenCL. Morgan Kaufmann. [11] Scarpino, M. 2011. OpenCL 
in Action: How to Accelerate Graphics and Computations. Manning Publications. [12] Munshi, A., Gaster, 
B., Mattson, T.G., Fung, J., and Ginsburg, D. 2011. OpenCL Programming Guide. Addison-Wesley Professional. 
[13] Kirk, D. and Hwu, W.-M. 2010. Programming Massively Parallel Processors. Morgan Kaufmann. [14] Hillis, 
W.D. and Steele, G. 1986. Data parallel algorithms. WebCL!prototypes! [15] Motorola Mobility. Node-webcl, 
an implementation of Khronos WebCL specification using Node.JS. https://github.com/Motorola-Mobility/node-webcl 
[16] Nokia Research. WebCL. http://webcl.nokiaresearch.com/ [17] Samsung Research. WebCL prototype for 
WebKit. http://code.google.com/p/webcl/ [18] Mozilla. FireFox WebCL branch. http://hg.mozilla.org/projects/webcl/ 
 Articles!and!Presentations! [19] Cole, M.I. 1989. Algorithmic skeletons: structured management of parallel 
computation [20] Gerstmann, D. 2009. Advanced OpenCL. Siggraph 2009. [21] McCool, M.D. 2010. Structured 
parallel programming with deterministic patterns. Proceedings of the 2nd USENIX conference on Hot topics 
in parallelism, 5 5. [22] Bordoloi, U.D. 2010. Optimization Techniques: Image Convolution. 1 25. http://developer.amd.com/zones/openclzone/events/assets/optimizations-imageconvolution1.pdf. 
 [23] BDT Nbody Tutorial. BDT Nbody Tutorial. Brown Deer Technology. http://www.browndeertechnology.com/docs/BDT_OpenCL_Tutorial_NBody-rev3.html. 
[24] Iñigo Quilez. ShaderToy with Mandelbulb shader. http://www.iquilezles.org/apps/shadertoy/?p=mandelbulb 
[25] Donnelly, W. GPU Gems -Chapter 8. Per-Pixel Displacement Mapping with Distance Functions. developer.nvidia.com. 
http://http.developer.nvidia.com/GPUGems2/gpugems2_chapter08.html. [26] Mattson, T.G., Buck, I., Houston, 
M., and Gaster, B. 2009. OpenCL -A standard platform for programming heterogeneous parallel computers. 
SC'09. http://www.crc.nd.edu/~rich/SC09/docs/tut149/OpenCL-tut-sc09.pdf. [27] Feng, W.-C., Lin, H., Scogland, 
T., and Zhang, J. 2012. OpenCL and the 13 dwarfs: a work in progress. [28] Lefohn, A., Kniss, J., and 
Owens, J.D. Chapter 33. Implementing Efficient Parallel Data Structures on GPUs. In: GPU Gems 2. Addison-Wesley. 
[29] Krüger, J. and Westermann, R. Chapter 44. A GPU Framework for Solving Systems of Linear Equations. 
In: GPU Gems 2. Addison-Wesley. [30] Porting CUDA Applications to OpenCL. Porting CUDA Applications to 
OpenCL. developer.amd.com. http://developer.amd.com/zones/OpenCLZone/programming/pages/portingcudatoopencl.aspx. 
[31] Hensley, J., Gerstmann, D., and Harada, T. OpenCL by Example. SIGGRAPH Asia 2010. [32] A Pattern 
Language for Parallel Programming. A Pattern Language for Parallel Programming. parlab.eecs.berkeley.edu. 
http://parlab.eecs.berkeley.edu/wiki/patterns/patterns. OpenCL and the OpenCL logo are trademarks of 
Apple Inc. used by permission by Khronos.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343492</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>54</pages>
		<display_no>9</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Cinematic color]]></title>
		<subtitle><![CDATA[from your monitor to the big screen]]></subtitle>
		<page_from>1</page_from>
		<page_to>54</page_to>
		<doi_number>10.1145/2343483.2343492</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343492</url>
		<abstract>
			<par><![CDATA[<p>Color affects many areas of the computer-graphics pipeline, From texture painting to lighting, rendering, compositing, image display, and the theater, handling color is a tricky problem. Tired of getting your images right on the monitor only to have them fall apart later on? This course presents the best practices used in modern visual-effects and animation-color pipelines, and how to adapt apply these concepts for home use.</p> <p>The course begins with an introduction to color processing and its relationship to image fidelity, color reproducibility, and physical realism. Topics include: common misconceptions about linearity, gamma, and working with high-dynamic-range (HDR) color spaces. Pipeline examples from recent films by Sony Pictures Imageworks explain which color transforms were used and why. The course concludes with a brief discussion of recent developments in color standardization at the Academy of Motion Picture Arts & Sciences and how attendees can experiment with all of these concepts for free using open-source software.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738911</person_id>
				<author_profile_id><![CDATA[81100292186]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Selan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ansel Adams -- The Camera, Book 1. Little, Brown, and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ansel Adams -- The Negative, Book 2. Little, Brown, and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ansel Adams -- The Print, Book 3. Little, Brown, and Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>286104</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Jim Blinn -- Jim Blinn's Corner: Dirty Pixels. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mark Fairchild -- Color Appearance Models. Addison-Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>550888</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ed Giorgianni -- Digital Color Management: Encoding Solutions. Prentice Hall.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1206782</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Glenn Kennel -- Color and Mastering for Digital Cinema. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[R.W.G. Hunt -- Measuring Colour. Fountain Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>233439</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Charles Poynton -- A Technical Introduction to Digital Video. Wiley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1386684</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Erik Reinhard -- Color Imaging, Fundamentals and Applications. AK Peters, Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Gunter Wyszecki &amp; W. S. Stiles-- Color Science: Concepts and Methods. Wiley &amp; Sons, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Giorgianni, Ed. Color Management for Digital Cinema: A Proposed Architecture and Methodology for Creating, Encoding, Storing and Displaying Color Images in Digital Cinema Systems. Submitted to the Science and Technology Council, Academy of Motion Picture Arts and Sciences. 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[
&#60;u&#62;Bruce Lindbloom&#60;/u&#62; - Online resource for color conversion math]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[
&#60;u&#62;Charles Poynton&#60;/u&#62; - Information for video standards and gamma, available for download.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[
&#60;u&#62;DCI Specification&#60;/u&#62; - Standards for theatrical digital distribution and colorimetry.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[
&#60;u&#62;DCraw&#60;/u&#62; - Free, high quality, camera raw convertor]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[
&#60;u&#62;OpenColorIO&#60;/u&#62; - Open Source Color Management Framework for Visual Effects and Animation]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[
&#60;u&#62;Visualizing the XYZ Color Space (YouTube)&#60;/u&#62; - Visual exploration of CIE XYZ]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cinematic Color From Your Monitor to the Big Screen SIGGRAPH 2012 Course Notes Course Organizer Jeremy 
Selan Sony Pictures Imageworks Updated: May 21, 2012 Latest Version Available Online Course Description 
 This class presents an introduction to the color pipelines behind modern feature-film visual-effects 
and animation. Color impacts many areas of the computer graphics pipeline. From texture painting to lighting, 
rendering to compositing, and from image display to the theater, handling color is a tricky problem. 
Tired of getting your images right on the monitor, only to have them fall apart later on? The goal of 
this course is to familiarize attendees with the best practices used in modern visual effects and animation 
color pipelines, and how to adapt these concepts for home use. The course begins with an introduction 
to color processing, and its relationship to image fidelity, color reproducibility, and physical realism. 
We will discuss common misconceptions about linearity, gamma, and working with high-dynamic range (HDR) 
color spaces. Pipeline examples from recent films by Sony Pictures Imageworks will be included, with 
a discussion of what color transforms were utilized, and why these approaches were taken. Finally, we 
will present a brief introduction to the Academy s recent efforts on color standardization in computer 
graphics (ACES), and how the audience can experiment with all of these concepts for free using open-source 
software (OpenColorIO). Level of difficulty: Intermediate Intended Audience This course is intended for 
computer graphic artists and software developers interested in visual-effects and animation. Prerequisites 
Familiarity with computer graphics fundamentals is preferred. On the web http://opencolorio.org About 
the presenters Jeremy Selan is an Imaging Supervisor at Sony Pictures Imageworks, specializing in color, 
lighting, and compositing. His work has been used on dozens of motion pictures, including The Amazing 
Spider-Man, Alice in Wonderland, and The Smurfs. He is one of the lead developers behind Katana - Imagework 
s lighting tool - and is also the founder of OpenColorIO. His work on color processing has been previously 
featured in GPU Gems 2 and Siggraph 2005's Electronic Theater. Presentation schedule 2.00 2.30 Color 
Science 2.30 3.00 Motion-Picture Color Management 3.00 3.30 Implementation, Example, Q&#38;A  Table 
of Contents Introduction Color Science Color Encoding, Color Space, and Image States Display-Referred 
Imagery Scene-Referred Imagery Color Correction, Color Space, and Log Motion-Picture Color Management 
Digital Intermediate, Mastering, and Delivery Lighting, Rendering, Shading Compositing Texture and Matte 
Painting Critical Inspection of Imagery ACES 43  OpenColorIO Appendix Lookup Tables ASC-CDL File Formats 
DCI P3 and X Y Z Daylight Curve vs. Blackbody Curve Acknowledgements References &#38; Further reading 
1. Introduction Practitioners of visual effects and animation encounter color management challenges 
which are not covered in either traditional color-management textbooks or online resources. This leaves 
digital artists and computer graphics developers to fend for themselves; best practices are unfortunately 
often passed along by word of mouth, user forums, or scripts copied between facilities. This course attempts 
to draw attention to the color pipeline challenges in modern visual effects and animation production, 
and presents techniques currently in use at major production facilities. We also touch upon open-source 
color management solutions available for use at home (OpenColorIO) and an industry attempt to standardize 
a color framework based upon floating-point interchange (ACES). This fully computer generated image 
touches upon many modern techniques in color management, including a scene-linear approach to rendering, 
shading, and illumination, in addition to on-set lighting reconstruction and texture management1. Visual 
effects by Sony Pictures Imageworks. Images from The Amazing Spider-Man , &#38;#169; 2012 Columbia Pictures 
Industries, Inc. All rights reserved. 1 This image, though dark, has good detail in the shadows. If these 
shadow areas appear flat black on your display, please confirm the display calibration and gamma. What 
color management challenges are faced in visual effects and animation production? Various Requirements: 
It is difficult to lump all of visual effects and animation into a single bucket, as each discipline 
has potentially differing color pipeline goals and constraints. For example, in visual effects production 
one of the golden rules is that image regions absent visual effects should not be modified in any way. 
This places a constraint on the color pipeline - that color conversions applied to the photography must 
be perfectly invertible. Animation has its own unique set of requirements, such as the elegant handling 
of saturated portions of the color gamut. Thus, color pipelines for motion pictures must keep track of 
the big picture priorities, and are often tailored for specific productions. Various Color Philosophies: 
There are many schools of thought on how to best manage color in digital motion-picture creation. (We 
assert there is far more variation in motion-picture color management than in desktop publishing, for 
example). Some facilities render in high-dynamic range (HDR) color spaces. Other facilities prefer to 
render in low-dynamic range (LDR). Some facilities rely on the output display characteristics (i.e., 
gamma) to as a primary tool in crafting the final image appearance. Others do not. It is challenging 
to provide standardized workflows and toolsets when current practice has so much variation. Furthermore, 
the cost/benefit of adapting new color management techniques is often stacked against change. When something 
goes wrong in a motion-picture color pipeline, it can have potentially large financial consequences if 
work needs to be re-done. Furthermore, while color processing decisions are often made early on during 
the lifetime of a production, the consequences (both positive and negative) may not be evident until 
many months down the line. This decoupling of cause and effect makes experimentation and innovation difficult, 
and all too often leads people to assert We ve always done it this way, it s not worth trying something 
new . The flip-slide is that computer graphics techniques used in motion-picture production are rapidly 
changing, outgrowing many classic color management techniques. For example, the recent trend toward physically-based 
rendering, shading, and lighting are only utilized to their fullest extents when working with dynamic 
ranges plausible in the real world (HDR). We thus assert that going forward, it will become increasingly 
beneficial for computer graphics applications, and visual-effects and animation facilities, to consider 
the approaches outlined in this course. Multiple Inputs &#38; Outputs: In live-action visual effects, 
imagery is often acquired using a multitude of input capture devices (digital motion picture cameras, 
still cameras, etc) and it is often desired to seamlessly merge sources. On the output side, the final 
delivery often appears under different viewing environments: digital theatrical presentation, film theatrical 
presentation, as well as home theater. Each of these outputs has different color considerations. Furthermore, 
artists often work on desktop displays with office viewing conditions, and yet must have high-fidelity 
preview of the final outputs. Complex Software Ecosystem: Another challenge is that the majority of visual 
effects and animation productions use many software tools: image viewers, texture / matte painting, composting 
apps, lighting tools, media generation, etc). Although it is imperative that artists work in a color 
managed pipeline across multiple applications, color support is quite varied between software vendors. 
Ideally, all software tools that interchange images, perform color conversions, or display images should 
be color managed in a consistent manner. The issue of interchange takes on an even more complex angle 
when you consider that multiple facilities often share image assets on a single film. Color management 
practices that encourage high-fidelity interchange are sorely needed. Robust Imagery: Visual effects 
and animation are not the end of the line in terms of image processing. Digital Intermediate (DI) is 
a powerful tool for crafting the final appearance of a motion-picture (even for animated features), and 
may substantially impact the appearance of the final film. It is therefore a necessity to create computer 
graphics which are robust to such workflows, and maintain fidelity even under drastic color corrections. 
If digital intermediate is not considered during production, it is very likely that late stage color 
corrections will reveal latent problems in the computer-generated imagery. The eventual application of 
compression for delivery is also a consideration. Future-Proof: Future improvements to display technology 
(such as wider dynamic range) are on the near horizon. For large productions, it is very prudent to take 
all the steps possible to future-proof the computer graphics material, such that you are only a remaster 
away from taking advantage of the new technology. 2. Color Science While a detailed overview of colorimetry 
is beyond the scope of these course notes, there are many textbooks which introduce color science in 
detail: Color Science [Wyszecki and Stiles, 1982] is the canonical bible for color scientists.  Measuring 
Color [Hunt, 1998] is a compact overview of color measurement and color perception.  Color Imaging: 
Fundamentals and Application [Reinhard, et al. 2008] presents a ground-up view of color fundamentals, 
and also touches upon applications such as camera and display technology.  A Brief Introduction to Color 
Science Color science blends physical measurement along with characterizations of the human visual system. 
The fundamentals of colorimetry (the measurement and characterization of color) provide an important 
conceptual framework on which color management is built. Without color science, it would not be possible 
to characterize displays, characterize cameras, or have an understanding of the imaging fundamentals 
that permeate the rest of computer graphics. While it is possible to immediately jump into color pipeline 
implementations, having a rudimentary understanding of concepts such as spectral measurement, XYZ, and 
color appearance provide a richer understanding of why particular approaches are successful. Furthermore, 
being familiar with the vocabulary of color science is critical for discussing color concepts with precision. 
A study of color science begins with the spectrum. One measures light energy as a function of wavelength. 
The human visual system is most sensitive to wavelengths from 380-730 nm. Light towards the middle of 
this range (yellow-green) is perceived as being most luminous. At the extremes, light emitted above 730 
nm (infrared) or below 380 nm (ultraviolet) appears indistinguishable from black, no matter how intense. 
 350 400 450 500 550 600 650 700 750 800 Wavelength (nm) The electromagnetic spectrum from approximately 
380-730 nm is visible to human2 observers. 2 Other animals can perceive light outside of this range. 
Bees, for example, can see into the ultraviolet. The human visual system, under normal conditions, is 
trichromatic3. Thus, color can be fully specified as a function of three variables. Through a series 
of perceptual experiments, the color community has derived three curves, the CIE 1931 color matching 
functions, which allow for the conversion of spectral energy into a measure of color. Two spectra which 
integrate to matching XYZ values will appear identical to observers, under identical viewing conditions. 
Such spectra are known as metamers. The specific shape of these curves is constrained4; they are based 
upon the results of color matching experiments. Spectral Sensitivity Z Y X  350 400 450 500 550 600 
650 700 750 800 Wavelength (nm) The CIE 1931 Color Matching Functions convert spectral energy distributions 
into a measure of color, XYZ. XYZ predicts if two spectral distributions appear identical to a human 
observer. When you integrate a spectral power distribution with the CIE 1931 curves, the output is referred 
to as CIE XYZ, with the individual components being labelled X, Y, and Z (the capitalization is important). 
The Y component has special meaning in colorimetry, and is known as the photopic luminance function. 
Luminance is an overall scalar measure of light energy, proportionally weighted to the response of human 
color vision. The units for XYZ are candelas per meter squared (cd/m2), and are sometimes called nits 
in the video community. The motion-picture sometimes uses an alternative unit of luminance, foot-lamberts 
, where 1 fL equals 3.426 cd/m2. An convenient trick to remember this conversion is that 14.0 fL almost 
exactly to 48.0 cd/m2, which coincidentally also happens to be the recommended target luminance for projected 
theatrical white. 3 For the purposes of this document we assume color normal vision (non color-blind) 
and photopic light levels (cone vision). 4 These the curves serve as the basis functions for characterizing 
a human s color vision response to light; thus all linear combinations of these color matching functions 
are valid measures of color. This particular basis was chosen by locking down Y to photopic luminance, 
and then picking an X an Z representation to place the visible locus tightly within the +X, +Z octant. 
Note that XYZ does NOT model color appearance. XYZ is not appropriate for predicting a spectral energy 
distribution's apparent hue, determine how colorful a sample is, or determine how to make two color spectra 
appear equivalent under different viewing conditions5. XYZ, in the absence of additional processing, 
is only sufficient for predicting if two spectral power distributions can be distinguished. Spectral 
Sensitivity X  350 400 450 500 550 600 650 700 750 800 Wavelength (nm) multiplied by... X component 
spectral sensitivity 1.0  equals... Weighted function CIE XYZ is calculated by multiplying the energy 
in the input spectrum (top) by the appropriate color matching function (middle), and then summing the 
area under the curve (bottom). As the color matching functions are based upon the sensitivities human 
color vision, the spectral energy during integration is zero-valued outside the visible spectrum. 5 
Color appearance models, far more complex than simple integration curves, model eccentricities of the 
human visual system and be used to creating matching color perceptions under differing conditions. One 
of the most popular color appearance model is iCAM. See [Fairchild, 98] for details. Spectroradiometers 
measure spectral power distributions, from which CIE XYZ is computed. By measuring the spectral energy, 
such devices accurately measure colorimetry even on colors with widely different spectral characteristic. 
Spectroradiometers can also be pointed directly at real scenes, to act as high fidelity light-meters. 
Unlike normal cameras, spectroradiometers typically only measure the color at a single pixel , which 
has a comparatively large visual angle (multiple degrees for the color sample is common). Internally, 
spectroradiometers record the energy per-wavelength of light (often 2, 5, or 10 nm increments), and the 
integrate the spectral measurements with the color matching functions to display the XYZ or Yxy tristimulus 
value. The exposure times of spectroradiometers are such that color can still be accurately measured 
over a very wide range of luminance levels, in addition to light output with high frequency temporal 
flicker (such as DLP digital projector). Spectroradiometers are thus incredibly useful in high-fidelity 
device characterization and calibration. Spectroradiometers (left) accurately measure the visible spectrum 
and can also output an integrated CIE XYZ. Alternative approached to measuring color (such as the monitor 
puck on the right) are far more cost-effective but do not record the full color spectra. Such devices 
therefore are only color accurate when turned to a specific class of display technology. Images courtesy 
Photo Research, Inc., and EIZO. It is often convenient to separate color representations into luminance 
and chroma components, such that colors can be compared and measured independent of intensity. The most 
common technique for doing so is to to normalize Cap X, Y, Z by the sum (X+Y+Z) and then to represent 
color as (x, y, Y). Note the capitalization. XY x = y = (X + Y + Z) (X + Y + Z) The chromaticity coordinates 
(x,y) define color independent of luminance. It is very common to plot these values, particularly when 
referring to display device gamuts. Little x, little y (x,y) is referred to as the chromaticity coordinates, 
and is used to plot color independent of luminance. When one converts all possible spectra into x,y,Y 
space and plots x,y they fall into a horse-shoe shaped region on the chromaticity chart. The edge of 
the horseshoe is called the visible locus, and corresponds to the most saturated color spectra that can 
be created. In this chart, luminance (Y) is plotted coming out of the page, orthogonal to x,y. [brucelindbloom] 
is a wonderful online resource for additional color conversion equations. 0.9 0.8 0.7 0.6 0.5 y 0.4 0.3 
0.2 0.1 0.0 0.0 0.1 x All possible light spectra, when plotted as xy chromaticity coordinates, fill a 
horseshoe shaped region. The region inside the horseshoe represents all possible colors; the region outside 
the horseshoe does not correspond to physically­possible color perceptions. (Such non-physically plausible 
coordinates are often useful for mathematical encoding purposes, but they are not realizable in ANY display 
system.) Additive display systems such as television create colors by blending 3 colors of light. Red, 
green, and blue are often used as these allow for much of the visible gamut to be reproduced. The gamut 
of colors that can be reproduced is the triangle enclosed by the primaries. Note that because the outer 
boundary of horseshoe is curved, there is no possible choice of three colors which encloses the full 
visible gamut. (Remember, you cannot build real displays with primaries outside of the horseshoe). This 
is why some recent television manufactures have begun to experiment with adding a fourth color primary. 
Although it is not immediately apparent, the chromaticity chart has very poor perceptual uniformity. 
The distances between colors in chromaticity space do not directly relate to their apparent perceptual 
differences. Two colors nearby in xy may be perceived as appearing very dissimilar, while colors colors 
far apart may be perceived as being indistinguishable. See MacAdam ellipses in traditional color textbooks 
for precise graphics representations of this non-uniformity. Roughly speaking, saturated green colors 
in xy space are over-accentuated relative to their perceptual similarity. The perceptual nonuniform of 
XYZ (and xy) is not surprising given that XYZ does not model color appearance. Finally, as color is inherently 
a three dimensional quantity, any discussions which make use of two dimensional charts tends to be misleading. 
For a graphical exploration of XYZ using 3-D, see Visualizing the XYZ Color Space [Selan 2005]. 2.1. 
Color Encoding, Color Space, and Image States Thus far we have discussed the measurement of color, but 
have not tied these measurements back to seemingly familiar computer graphics concepts such as RGB. So 
what is RGB? RGB is a color encoding where red, green, and blue primaries are additively mixed to reproduce 
a range (gamut) of colors. The specific color appearance of pure red, green, and blue is tied to the 
chosen display device; often identified using chromaticity coordinates. The code values sent to a display 
device often correspond non-linearly to the emitted light output, as measured in XYZ. This non-linearity 
was originally a consequence of display technology, but today serves a continued purpose in increasing 
the coding efficiency of the transmitted images. All RGB colors have units. Sometimes an RGB pixel s 
units are explicit, such as measuring the emitted light from a display using a spectroradiometer and 
being able to reference pixel values in XYZ cd/m2. However, sometimes the units are only indirectly related 
to the real-world, such as providing a mathematical conversion to measurable quantities. For example, 
having code values represent either the logarithm or exponent of RGB is common. This definition of how 
measurable color quantities relate to image RGB code values is referred to as the color encoding, or 
more commonly in the motion-picture computer graphics community, color space6. In the case of display 
technology, common color encodings (relations of code value to measurable XYZ performance) include sRGB 
and DCI-P3. But considering image display only provides part of the color encoding story. In addition 
to relating RGB values to display measurements, one can also relate RGB values to the performance characteristics 
of an input device (i.e., a camera). Input colorimetry can be measured in real-world units as well. It 
is not difficult to measure an input spectra with the spectrophotometer in XYZ, and then compare this 
to the RGB values output from the camera. This process, called camera characterization, will be discussed 
further in section 2.3. It is a meaning abstraction to categorize color spaces by the direction of this 
relationship to real-world quantities, which we refer to as image state. Color spaces which are defined 
in relation to display characteristic are called display-referred, while color spaces which are defined 
in relation to input devices (scenes) are scene-referred. While there are other flavors of images states 
(intermediate-referred, focal-plane referred) display-referred and scene-referred colorimetry are most 
common used in motion­picture color management, and will be the focus of the next sections. For further 
information on image state and color encodings, the Ed Giorgianni publications provide significantly 
greater detail. [Giorgianni 98] [Giorgianni 05] 6 The color science community looks down upon the use 
of color space to denote the RGG pixel encoding; color space is preferred to refer to the broader class 
of color encoding, examples of which are RGB, CMY, HSV, L*a*b*, etc.). However, the mis-use of the color 
space is so ubiquitous that that we will adhere to industry convention. 2.2. Display-Referred Imagery 
Display-referred imagery is defined colorimetrically with regards to the appearance of the image as presented 
on a display. The display may be either an idealized display standard, or a physical display that exists 
in the real-world. When RGB is used casually without qualification of colorimetry (such as in web standards), 
it is most likely assuming display-referred imagery. The primary advantage of working with display-referred 
imagery is that if the user s display matches the reference display definition, one can accurately display 
the raw pixel values on the screen without any additional color conversions. I.e., if a user creates 
images by directly manipulating an image raster, they are working in display referred spaces. This simplicity 
in color management makes display-referred color processing engines a popular choice in desktop publishing 
applications. Linearized Display-Referred Imagery As mentioned previously, the RGB code values that are 
sent to the display are not directly proportional to the light being output from the display. However, 
there are many cases in computer graphics where working with pixels in a color space proportional to 
light output is preferable. For example, in anti­aliased filtering one important requirement is that 
pixel energy should be preserved. What this means is that the total light energy of the image emitted 
from the display - both before and after filtering ­should be identical. If this were not true, then 
resizing an image would change the apparent brightness, which is not ideal. Such artifacts become even 
more apparent when applied to moving imagery (motion pictures) where slow transitions between light and 
dark regions can crawl when energy preservation is ignored. To linearize display-referred imagery, one 
must come up with a model of the display technology which predicts how much light, as a function of code 
value, is being emitted. When modeled by a single number, this is referred to as Gamma. Gamma value of 
2.2 are a very common approximation to the values seen in real-world display technologies. RGB" RGB 
 linear device The exponent value used to relate RGB code values to linear light is called gamma. Gamma 
is generally defined as the inverse of the exponent, but it is useful to be explicit as there the language 
people use to describe gamma is usually ambiguous. One easy way to remember is that middle gray display-referred 
values, when linearized, become smaller. So a 128 RGB code value (out of 255), when linearized with a 
2.2 gamma, is approximately ~ 0.218. One of the additional benefits of using a gamma function is that 
it offers a more perceptually uniform encoding space, which better utilizes the limited number of bits 
available in the display link. Thus, even on devices which were based upon inherently linear technology 
(such as the DLP digital projectors), it remains useful to artificially emulate a gamma value. See Charles 
Poynton s Gamma FAQ [Poynton 12] for a thorough discussion of gamma. sRGB Due to differences in inherent 
display technologies, there is substantial variation in the appearance of RGB when the same code values 
are sent to multiple displays, making the unambiguous distribution of RGB imagery difficult. As a solution, 
a standard idealized display has been defined, called sRGB, which real displays attempt to reproduce. 
The intent of sRGB ( Standard RGB ) is to define the color characteristics of a standardized average 
RGB monitor, such that imagery on one monitor matches the appearance of a different monitor. When a monitor 
is properly calibrated to sRGB, the output is reproducible and well defined. Older display technology 
such as cathode-ray tube (CRT) technology naturally approach the sRGB specification (which is how it 
was defined), However, even modern technologies such as LCD and OLED, which have very different inherent 
image responses, typically provide an option to emulate the sRGB response to maintain compatibility with 
existing imagery. Linearizing RGBdevice (sRGB) RGB to XYZ conversion (D65 reference white point) RGBlinear 
 0.4124564 0.3575761 0.1804375 R X " 0.2126729 0.7151522 0.0721750 Y G Z B 0.0193339 0.1191920 0.9503041 
 These steps allow one to predict emitted light in CIE XYZ, as emitted from a calibrated sRGB. First, 
device RGB is converted to linearized RGB. Next, the linear RGB is converted to XYZ using the conversion 
matrix. Note that even though the sRGB transfer function uses a 2.4 exponent, due to the inclusion of 
the scaling and offer factor this transfer function approximates a 2.2 gamma over the range of [0,1]. 
Because display-referred imagery is referenced to the light emitted from a display, it s possible to 
convert RGB values to output scene CIE XYZs. For example, it is sensical the specify a display s white 
point and black point. The white point would be the real-world XYZ for the maximum RGB value (on a 8-bit 
display, 255, 255, 255). The black point is the XYZ for the minimum RGB value (0,0,0). As the dynamic 
rage of display-referred image is well defined. (with a min code value and max code value), it is common 
to use integer codings to represent display RGB. 8 bits is common, and lets you represent the range of 
[0, 255]. Note that on high quality displays, under ideal conditions, 8 bits is not sufficient to prevent 
the appearance of banding (this artifact is particularly noticeable on grayscale imagery with smooth 
gradient). For this reason professional displays (such as medical displays, or those used in professional 
color applications) often display images with greater than 8 bits of precision (10 or 12 bits are not 
uncommon). 0.9 0.8 0.7 0.6 0.5 y 0.4 0.3 0.2 0.1 0.0 x y White point 0.3127 0.3290 R 0.64 0.33 G 0.30 
0.60 B 0.15 0.06  G R ec. 709 gamut D65 whi te poi nt R  B 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 x 
sRGB relies on the Rec709 primaries and white point, and thus can re-create any of the color in the above 
triangle (gamut). Display referred imagery is also the realm of ICC profiles and traditional appearance 
modeling techniques. If you have two different displays, with different color reproductions, you can 
use ICC to convert between them while preserving image appearance. You can also use ICC for display calibration, 
where libraries will compute the color transform necessary to have your display emulate an ideal calibration 
. Another display-referred image specification is DCI-P3. This color space is common in digital cinema 
production, and is well suited to theatrical presentation. Where as the sRGB specification uses a color 
encoding suited for the desktop environment, the DCI-P3 specification uses a color encoding suited for 
theatrical luminance levels. Another example of display-referred imagery in motion-picture production 
X Y Z (called x-prime, y-prime, z-prime ) This is the color space for the final imagery actually sent 
to the theater in the digital cinema package (DCP), and is a gamma encoded version of XYZ output colorimetry. 
See appendix 4.4 for further details on DCI-P3 and X Y Z. Limitations of Display-Referred Imagery Display 
referred imagery has dynamic ranges which are inherently tied to displays. Thus, even though the real 
world can have enormously luminous intensities, when working in a display-referred space values above 
the display-white are essentially meaningless. This mismatch between the dynamic ranges of the real world 
and the dynamic range of display-technology makes working in display-referred color spaces (even linear 
ones) ill suited for physically-based rendering, shading, or compositing. Yet even though working with 
display referred imagery has limitations, it forms a critical component of pipelines. Even in full motion-picture 
color pipelines which work in higher dynamic range color spaces, there is always remains a portion of 
the pipe where display referred, and even linearized display referred imagery, is critical. 2.3. Scene-Referred 
Imagery The second major image state in motion pictures imaging pipelines is that of scene-referred imagery, 
where the code values are proportional to measurements of real-world scenes. The typical way to create 
scene-referred image is either through the characterization of a camera system, or though synthetic means. 
As there is no absolute maximum white point, pixel values can be arbitrarily large, within the constraints 
of the capture device. Scene-referred image pipelines are inherently linear, as pixel values are proportional 
to photons in the real-world by definition. As the real world has a very high-dynamic range, it s often 
useful to talk about light in terms of stops , or doublings of light. You can compute the number of stop 
and exposure is by taking the logarithm, base-2. luminance relative exposure " log 2 100 cd/m2 Relative 
exposure in stops, is the log, base-2, relative to some reference exposure level. In this example, the 
choice of 100 cd/m2 has been selected arbitrarily. Any normalization factor would suffice. Stops MultiplicationFactor 
0 1 +0.5 1.414 +1 2 +2 4 +3 8 +8 256 Stops MultiplicationFactor 0 1 -0.5 0.707 -1 0.5 -2 0.25 -3 0.125 
-8 0.0039 Talking about scene-referred exposure values is most often done in units of stops, as the 
range between values is so large. For example, it s difficult to get an intuition for what it means to 
change the emitted light by a factor of 0.0039. However, it s very sensible when that same ratio is expressed 
as -8 stops . In a real scene, if you measure luminance values with a tool such as a spectroradiometer, 
one can observe a very wide range of values in a single scene. Pointed directly at emissive light sources, 
very large values such as 10,000 cd/m2 are possible (and if the sun is directly visible, specular reflections 
may be another +6 stops over that)! Even in very brightly lit scenes, dark values are created either 
through material properties, through scene-occlusions, or a combination of the two. Dark materials reflect 
a small fraction of incoming light, often in the 3-5% range. As a single number, this overall reflectivity 
is called albedo . Considering illumination and scene-geometry, other objects can cast shadows and otherwise 
occlude illumination being transported around the scene. Thus in real scenarios, It s often possible 
with complex occlusions and a wide variety of material properties to have dark values 1,000-10,000 times 
darker then the brightest emissive light sources. Luminance (cd / m )2 Relative exposure Object 1,600,000,000 
23.9 Sun 23,000,000 17.8 Incandescent lamp (.lament) 10,000 6.6 White paper in the sun 8,500 6.4 HDR 
monitor 5,000 5.6 Blue sky 100 0 White paper in typical of.ce lighting (500 lux) 50 to 500 -1.0 to 2.3 
Preferred values for indoor lighting 80 -0.3 Of.ce desktop sRGB display 48 -1.1 Digital Cinema Projector 
1 -6.6 White paper in candle light (5 lux) 0.01 -13.3 Night vision (rods in retina) All values are in 
the case of direct observation. Luminance values we encounter on a daily basis span an enormous dynamic 
range (greater than a million to one). It s important to observe that in the real-world, there is not 
any particular maximum luminance where light can t get any brighter. This differs form display-referred 
imagery, where it s easy to define the maximum light that a display system can possibly emit. On the 
dark side (black point), where one could always remove a bit more light, getting closer and closer to 
zero photons. In real scenes its very hard to create a situation where there is NO light - more typical 
is that you just have very small positive value. When we bring HDR imagery into the computer its useful 
to normalize the scene exposure. Even though an outdoor HDR scene may be at an absolute level 1000 times 
more luminous than the equivalent indoor scene, it is very useful to have them at equivalent overall 
intensities, yet still preserving the relative intra-frame luminance levels. As the absolute maximum 
luminance is quite variable (even frame to frame), scene-referred imagery tends to be normalized with 
respect to an average gray level. Convention in the industry is to pin middle gray at 0.18 (representing 
the reflectivity of an 18% gray card). Observe that even when we normalize the exposure of scene-linear 
imagery, it is expected that many portions of the scene will continue to have luminance values >> 1.0. 
Furthermore, nothing magical happens with bright code values > 1.0. There is generally a continuum of 
luminance values in the scene, and it generally is not feasible to assert that above a particular value 
denotes specularity. Integer representation are not appropriate for storing scene-referred imagery due 
to the distribution of luminance-levels seen in real-world values (even when gray normalized). If you 
look at the actual distribution of luminance levels in the scene, one sees that greater precision is 
required around dark values, and that less precision is required in highlights. Consider doing a test 
to find the smallest just noticeable difference that is suitable for recording shadow detail in linear 
light, without introducing visible banding. Thus if one uses integers to get adequate precision in the 
shadows, when this same light JND is used on very bright pixels, there will be far too many of them and 
bits will be wasted. Conversely, if one tailors a code value step size to provide reasonable luminance 
resolution on the highlights, then the shadows will not have sufficient detail. Floating-point representations 
gracefully address the precision issues associated with encoding scene­linear pixels. Float representations 
are built from two components: the individual storage of an exponent, and a linear scaling (the mantissa). 
This hybrid log / linear coding scheme allows for an almost ideal representation of scene-referred imagery, 
providing both adequate precision in the shadows along with the highlights. In modern visual effects 
an color pipelines, OpenEXR is most commonly used to store floating-point imagery and helped to popularize 
the 16-bit half float format. which now has native support in a wide variety of systems including GPUs. 
See Appendix 4.3 for more information on OpenEXR. It s important to note that while EXR popularized high 
dynamic range file representations in the motion-picture industry, Greg Ward made major contributions 
to HDR storage many years earlier with the RGBE HDR image format. Characterizing Cameras Creating scene 
referred imagery is usually tackled by setting up a camera under known test conditions, and then determining 
how to relate the output camera RGB code values to linear light in the original scene. When going this 
route, its usually best to start with a camera RGB image as close to camera raw as possible, as these 
encodings preserve the greatest fidelity. Even better than characterizing a camera yourself is when the 
manufacturer does this for you by providing a set of curves or lookup tables to unbake the input transformation. 
In the situation where you do need to characterize a new camera (or validate that the linearization being 
used is appropriate) the general approach is to is to setup a scene with a stable light-source, and then 
to do an exposure sweep with the camera in known increments. By changing the exposure in known stop increments 
(a stop is a doubling of light in the scene) you can relate exposure values in scene-linear to code values 
in the output imagery. Camera characterizations are often approached as channel independent mappings, 
using 1-D transforms. However, sometimes the camera s response is different per-channel. The two common 
approaches to handling this are to either perform a weighted average of the channels and then use that 
as the basis for converting to scene-linear, or to do a different 1D conversion for each channel. Common 
practice is that for systems where the channels have approximately equal response curves (most digital 
cameras fall into this category) to use a single mapping for all three channels. Only when capture systems 
have very different responses (such as film negatives) are separate curves per channel appropriate. Channel 
independent mappings to scene-linear are simple, but not always sufficient. Consider two cameras from 
different manufacturers imaging the same scene. Even when we do the proper 1D conversion to scene-linear, 
there are still likely to be differences in the residual color appearance due to to differing color filter 
technologies being utilized. These differences are often accounted for by imaging a series of color patches, 
and then coming up with a 3x3 matrix transform that minimizes the differences between devices. One common 
reference used for this purpose is the macbeth chart, which has standardized patches with known reflectances. 
The chart reflects incident illumination, so its appearance is a function of the wide-spectrum performance 
of the lights. I.e., the Macbeth chart appears slightly different under tungsten vs daylight illumination. 
 The Macbeth color checker is commonly used to validate camera characterizations; the patches have known 
reflectance values. Image courtesy of X-Rite. While a full discussion of input characterization is out 
of the scope of these notes, camera linearizations have a bit of variation when when used at very darkest 
portions of the camera capture range. One major challenge is to determine if the lowest code values out 
of the camera represent no light , in which case the average black level could be a true 0.0 in scene 
linear, or if instead the lowest code values from the camera correspond to a small but positive quantity 
of light. This issue becomes more complex in the context of preserving sensor noise and grain in the 
very darkest regions. If you consider imaging true black in a camera system, which when mapped to linear 
must averages 0.0, it implies that some of the code values of noise will be small, yet positive, and 
other parts of the noise will be small, and negative. While not physically plausible, preserving these 
negative values is critical to maintaining an accurate black level, on average. There are similar considerations 
even when black is mapped to small, positive quantities. Displaying Scene-Referred Imagery The primary 
complexity of working with scene-referred imagery is solving the issue of reproduction on displays. While 
HDR scene-referred imagery is natural for processing, displays can only reproduce relatively low dynamic 
ranges (LDR) portion of the range. While it seems like a reasonable approach to directly map scene-referred 
linear directly to the display (at least for overlapping portions of the contrast range), this has terrible 
results in practice. The inability to directly map scene-linear to display-linear is actually quite a 
large conceptual hurdle to overcome, and it s not immediately obvious to most people this is so. See 
[Giorgianni 05], Color Management for Digital Cinema, for further justifications. So how can you pleasingly 
reproduce high-dynamic range pixels on a low dynamic range reproduction? This process in the color community, 
is called tone rendering (not to be confused with what graphics people call rendering), and is an active 
area of research. Many pleasing tonal renditions of high-dynamic range data use similarly shaped transforms. 
On first glance this may be surprising, but when one sits down and actually designs a tone rendering 
transform there are convergent processes at work corresponding to what looks good . The general solution 
to this tone mapping problem is to intelligently compress the HDR into an smaller tonal range to fit 
the display's possible output space. Unfortunately, simply compressing it linearly results in a low-contrast 
appearance, so the compromise is to compress it more in the high and low ends. To begin with, most tone 
rendering map a traditional scene gray exposure (0.18 by convention) to an appropriately middle value 
on the output display. Under theatrical viewing conditions, mapping middle gray to ~10% of the maximum 
output luminance (in linearized display-referred space) yields pleasing results. If one directly maps 
the remaining scene-linear image to the display (and clips), the resulting image will appear low contrast. 
Thus, a reconstruction slope which is greater in contrast than 1 to 1 bumps the contrast around the middle 
gray and creates a pleasing appearance. But, with this increase in contrast the tops an bottoms are still 
clipped, so a rolloff is applied on both the high and low ends to allow for highlight and shadow detail 
to rolled off The final curve resembles an S shape, with the center of the S centered on the midtones. 
The final transfer curve from scene-linear to output display is shockingly consistent between technologies, 
roughly matching both the end to end transform of both digital and film imaging pipelines. 1.0 Output 
code 0.5 values (sRGB) 0.0 Scene-linear source values An S shaped curve is traditionally used to tone 
render scene referred HDR colorimetry into an image suitable for output on a low dynamic range display. 
The input axis is log, base 2, of scene-linear data, The output axis directly corresponds to code-values 
on a calibrated sRGB display. If we characterize the film color process, from negative to print, we see 
that it almost exactly produces this s-shaped transfer curve, which is not surprising given the beautiful 
tonal reproductions film offers. In the traditional film imaging process, the negative stock captures 
wide dynamic range (well beyond the range of even modern digital camera), and the print stock imparts 
a very pleasing S tone mapping for reproduction on limited dynamic range devices. Broadly-speaking, film 
negatives encode an HDR scene-referred image, and the print embodies a display referred tone-mapping. 
For those interested in further details on the high-dynamic range imaging processes implicit in the film 
development process, see the Ansel Adams Photography Series [Adams 84]. It s worth noting that current 
tone mapping research often utilize spatially varying color correction operators. While that research 
is very promising and may impact motion picture production at some point in the future, most motion-picture 
color management approaches currently assume that each pixel is treated independently. However, much 
of the current look of spatially varying tone-mapping operators are currently achieved in alternate ways. 
For example, tone-mapping operators often accentuate high-frequency detail in shadow areas which would 
have otherwise been masked. In the cinematic world, this is addressed by directly crafting on-set lighting 
by the cinematographer, or as an explicit artistic correction during the digital intermediate (DI) process. 
 To summarize, do not directly map high-dynamic range scene-referred data to the display. A tone­rendering 
is required, and there is great historical precedence for using an S-Shaped curve. HDR scene-acquisition 
accurate records a wide range of scene-luminance value, and pleasing tonal renditions preserve much of 
this range during reproduction even on low-dynamic range devices. Please refer to Chapter 3.2 for a comparison 
of the visual differences between our recommended s­shaped tone mapping operator, versus a naive scene-linear 
to display-linear mapping. Consequences of Scene-Referred Imagery Working with the dynamic ranges typical 
of scene-referred imagery positively impacts almost every area of the computer graphics pipeline, particularly 
when the goal is physical realism. In rendering and shading, scene-referred imagery naturally allows 
for the use of physically-based shading models and global illumination. In compositing, physically-plausible 
dynamic ranges allow for realistic synthetic camera effects, particularly as related to filtering operations, 
defocus, motion-blur, and antialiasing. See Chapter 3 for further details of how a scene-linear workflow 
impacts specific portions of the color pipeline. Unfortunately, scene-referred imagery also presents 
some interesting challenges as well. Filtering, antialiasing, and reconstruction kernels which make use 
of negative lobed filters (sinc, lanczos3, simon, etc.) are much more susceptible to ringing when applied 
to HDR imagery. While high-dynamic range isn t the root cause of the ringing, it certainly exacerbates 
the issue. Another pitfall of working with HDR data is that storage requirements are increased. In a 
low dynamic range workflow, 8-bit textures are sensible, while in common HDR workflows 16-bits of HDR 
is usually a minimum. There are further issues with classic common compositing tricks that rely on assumptions 
which do not hold in floating point. Chapter 3 will also discuss potential solutions to many of these 
issues. A Plea for Precise Terminology The computer graphics community - as a whole - is far too casual 
about using the word linear to reference both scene-referred and display-referred linear imagery. We 
highly encourage our attendees to set a positive example, and to always distinguish between these two 
image states even in casual conversation. To clarify... Are you referencing the linear light, as emitted 
by the display? Did you use the g-word , gamma ? Do odd things happen to your renders when values go 
above 1.0f? If so, please use the term display-linear. Alternatively, are you referencing high-dynamic 
range or physically-plausible lighting models? Is your middle gray at 0.18? Are you talking about light 
in terms of stops ? Does 1.0 have no particular consequence in your renders? If so, please use the term 
scene-linear, and by all means make sure you are using a view transform that goes beyond a simple gamma 
model. Friends don t let friends view scene-linear imagery without an s-shaped view transform. 2.4. Color 
Correction, Color Space, and Log It is sometimes necessary to encode high-dynamic range, scene-referred 
color spaces with integer representations. For example, digital motion picture cameras often record live 
to 10-bit integer tape media (such as HDCAM SR). As noted earlier, if a camera manufacturer were to store 
a linear integer encoding of scene-referred imagery, the process would introduce a substantial amount 
of quantization. It turns out that using a logarithmic log integer encoding allows for most of the benefits 
of floating point representations, without actually requiring float-point storage media. In logarithmic 
encodings, successive code values in the log space map to multiplicative values in linear space. Put 
more intuitively, this is analogous to converting each pixel to a representation of the number of stops 
above or below middle gray, and then storing an integer representation of this quantity. As log images 
represent a very large dynamic range in their coding space, most midtone pixels reside in the middle 
portion of the coding space. Thus, if you directly display a log image on an sRGB monitor it appears 
low contrast. This image appears low contrast because it is using an integer logarithmic encoding to 
represent a very­high dynamic range of scene intensities. This image is colloquially known as Marci , 
and is a common reference image in theatrical exhibition. This image originated from a scan of a film 
negative, courtesy Kodak Corporation. It can be downloaded from their website in DPX and CIN formats. 
Not all log spaces are equal. Most camera manufacturers customize a log encoding equation to optimize 
the usage of code values based on the camera s dynamic range, noise characteristics, and clipping behavior. 
Examples of different log spaces are Sony S-Log, REDLog (from the RED Cameras), Arri LogC, and the classic 
Cineon (used in film negative scanning). Care must be taken during image handling to determine which 
log space is associated with the imagery, and to use the proper linearization. 3. Motion-Picture Color 
Management A Color Pipeline is the set of all color transformations used during a motion-picture production. 
In a simplified computer graphics pipeline (below), one must consider the processes such as texture painting, 
matte painting, on-set lighting capture, traditional motion-picture camera inputs, lighting, rendering, 
compositing, and digital intermediate (DI). In a properly color-managed pipeline, the color encodings 
are well defined for all images, at all stages of processing. Client A well-defined computer-graphics 
color pipeline rigorously manages the color transformations for each stage of the process, in addition 
to the transforms required for accurate image preview. We define the color pipeline by carefully tracking 
the color encodings (color space) for all image inputs, image outputs, and intermediate representations. 
We also rigorously define the transforms used to convert between color spaces. The Visual Effects Color 
Pipeline Traditional visual effects color pipelines are based around the core principle of doing no harm 
to the plate. Plate photography is brought in whatever color space is delivered. (camera log, commonly), 
and then a converted to scene-linear using an invertible transform. Often, a pure 1d camera to linear 
transform is used, though sometimes the linearization is augmented with a matrix transformation. For 
visualization, a 3D-LUT is used which will emulates the output processing, which may either be a print 
film emulation or a similar s-shaped transformation. But during the visual effects process, the display 
transform is never baked into the imagery. 3.1. Digital Intermediate, Mastering, and Delivery Digital 
Intermediate (DI) is the process where the entire motion-picture is loaded into a dedicated hardware 
device, for the purpose of color-correcting in an environment that exactly mirrors the final exhibition 
(i.e., in a theater). Viewed in this final environment, DI is where per-shot color corrections are added, 
and the visual look of the film is finalized. DI is sometime also referred to as color timing , or often 
simply, timing . The final step of baking in view transforms (specific to an output device) and final 
color corrections is known as mastering . Getting the viewing transforms locked down in the DI process 
is the most important part of a cinematic color pipelines, as it pins down every other part of the process. 
The remainder of the pipeline essentially relies on visualizations that are created by working backwards 
from this step. Digital intermediate is done in a viewing environment that exactly mirrors the final 
exhibition. If you are mastering for digital cinema, the DI must be done in an equivalent theatrical 
setting with matched digital projection. If you are mastering for home theater, you should be mastering 
in an equivalent idealized home environment. (There are well-defined mastering specifications for both 
situations). If you are mastering for film release, you typically use a theatrical environment with a 
digital projector, with a film emulation 3DLUT made in conjunction with the print house to confirm a 
visual match. Digital intermediate (and mastering) typically have very tight tolerances on calibration, 
as the decisions made in this process are the last time image color appearance is tweaked. Examples of 
popular commercial DI systems include Lustre, DaVinci, Baselight, NuCoda, etc. Companies that historically 
have historically done DI include Deluxe and Technicolor. Color Correction and Working Space There are 
two main approaches to handing color in digital intermediate. The first video-centric approach is where 
display-referred imagery is loaded into the DI; no viewing transforms is necessary. The display referred 
imagery is directly manipulated (sort of like a motion-picture version of Photoshop). In the second, 
filmic approach, scene-referred imagery is loaded into the machine and a viewing transform (3DLUT) is 
required to create the final the color appearance. The color correction manipulates the underlying scene-referred 
representation, but all color judgements are made previewing through the display transform. The advantage 
of the video approach is one of simplicity. When color correcting pre-rendered imagery, a relative modest 
amount of color correction can be applied without introducing artifacts in the imagery. The downside 
to this approach is that much of the detail in the original imagery is lost when pre-baking in the view 
transform (this baking in some cases may have even happened inside the camera). For example, if a shot 
was originally overexposed, a sensible color correction is to darken the image. However, it is likely 
that in the overexposed image that large portions of the image was clipped to a constant maximum value, 
and no possible correction can bring back this lost detail. In the alternative mode, working with a scene-referred 
imagery, a high-fidelity representation of the full dynamic range from the original camera is loaded. 
Sometimes this is the equivalent of camera raw, but most commonly it is a camera-specific integer log 
encoding. In the future, DI systems that load floating-point scene-referred imagery will become increasingly 
common, but for now log DPX files are the norm. For viewing, an S-Shaped tone curve is used for preview, 
and is crafted to precisely emulate the appearance of the target output. For example, on a show that 
is going to be printed to film, a 3D LUT which emulates the specific print stock + development process 
is appropriate. If this film will also be distributed digitally, this specific print film emulation will 
eventually be baked into the imagery prior to distribution to the theaters. Of course, if the production 
will only be distributed digitally there is far more latitude in selecting a viewing transform. In both 
cases, the appearance of the imagery is modified by color correcting the image in the native log (or 
scene-linear) color space, and making visual decisions based on the appearance post viewing transform. 
Going back to our over-exposed example, remember that more of the dynamic range from the original camera 
capture is preserved when using a scene-referred approach. Thus, when we change the exposure on our log 
data we may expose new details which had not previously been visible (no clipping has occurred). In terms 
of classic film production processes, as more data was captured on the negative, we have a wide latitude 
of possible print exposures over which a high print fidelity is maintained. This allows color correction 
in log DI to be very high fidelity; most modest corrections do not drive the image to flat black or flat 
white images. DPX is commonly used as a log delivery format to Digital Intermediate, and both 10-bit 
and 16-bit versions are common-place. For grainy material, 10-bits is often sufficient, but 16-bit is 
becoming increasingly common for computer generated imagery with low noise. In the case of passing true 
scene-linear renders directly to DI, 16-bit floating point (EXR) is preferable. 16-bit integer linear 
representations are not ever recommended. The original log film plate (top-left) is loaded into the 
color corrector. When viewed with a film emulation table (top-right), the appearance is predictive of 
the final output with a default exposure. If we were to lower the exposure using an additive offset in 
log-space (lower-left), new details in the flame highlights are revealed. When a similar correction is 
applied to a color-corrector working in a display­referred space (lower-right), this detail is lost. 
Imagery from Spider-Man, &#38;#169; 2002 Columbia Pictures. All rights reserved. In scene-linear, a gain 
operation is typically used to change the color balance and scene exposure. In log space, this roughly 
corresponds to offsets. If a mathematically exact log is used, they are in fact identical, though most 
manufacturers tweak the log encodings as previously mentioned. Log offset color corrections are most 
common in motion-picture industry color correction, and are often referred to as the primary grade or 
one-lights . Theatrical fades to black have a very particular appearance to them, which is a direct consequence 
of the fade applying in log space, as viewed through the traditional s-shaped film emulation. Artistically, 
the DI process can be segmented into a per-shot correction that neutralizes shot to shot color variation, 
and then a secondary process that craft the overall look of the film. It is common for the DI house, 
if an initial grade happens early enough, to communicate to the VFX houses the looks being used. This 
often is send as a CDL or a 3DLUT, per shot, and does not have the viewing transform baked into it. It 
is also important to communicate the color space that the color corrections should be applied in. Most 
often, the color correction color space is the same as the visual effects delivery format. The advantage 
of working with this data downstream is that for computer generated imagery, you have have a better preview 
of the eventual appearance of the theatrical release. Trim Passes and Mastering It is common in DI to 
create masters for a multitude of output devices. For example, on major motion pictures one would expect 
to have eventual outputs on digital projectional, film releases, home theater releases in both HD and 
standard def. The general approach is to identify one output process as the gold standard , and to spend 
the majority of the artistic time correcting the images to look perfect on that device (in our experience, 
the theatrical digital projection is usually most appropriate to do first). The director, producers, 
etc will all be present at this process. Once the main color grade is complete, additional outputs are 
handled as trim passes upon the main output. Trim passes are most often implemented as additional color 
correction layers added atop primary corrections, and only utilize minor corrections such as contrast, 
brightness, and saturation. The display transform is device-specific; for trim passes a different view 
transform tailored to the particular output device is a necessity. Viewing environment greatly impacts 
the appearance of colors, most often contrast and colorfulness. Theatrical view environments typically 
have dark surround, combined with lower screen luminance (48 cd/m2 is common in theatrical projection). 
In the home theater, a dim surround is common and a screen luminance of 80 cd/m2 is typical. In a desktop 
office setting, a bright surround is assumed. Thus, if you take the same image, display it using the 
exact same color reproduction in each environment the appearance will be very different. Color appearance 
models can be used to attempt to correct for these settings, but trim passes with a human in the loop 
typically yield higher fidelity results. If mastering for 3D (stereo), an additional trim pass is required. 
Due to the extra optics necessary to create the 3D image (additional filters in the projector, in addition 
to glasses) the 3d image is usually far lower luminance than 2D projection. As lower luminance images 
tend to appear less colorful, the trim pass for the 3D master typically bump both saturation and contrast. 
3.2. Lighting, Rendering, Shading The stages of rendering, lighting, and shading most closely approximate 
physical realism when performed in a high-dynamic range, scene-linear color space. Ideally, no color 
space conversions should be required during the execution of the render, as all input assets such as 
textures, plate reprojections, and skydomes, can be linearized beforehand. Image viewing of scene-linear 
data is typically handled by converting to the color space being delivered to digital intermediate (commonly 
a log color space), and then applying the view transform suitable for the specified display. For convenience, 
these transforms are typically baked into a single 3D-LUT, though care must be taken to assure the LUT 
has suitable fidelity over an appropriate HDR domain. As mentioned earlier, you must use a viewing transform 
when viewing HDR scene-linear data. This image is a direct visualization of a high dynamic range, scene-linear 
render. Values greater than 1.0 (specular highlights) are clipping in this raw visualization (though 
preserved in the data). Using a naive gamma visualization (directly mapping scene-linear to display-linear), 
results in an image with low apparent contrast and poor highlight rendition (observe clipping on the 
table). Using an s-shaped tone curve to visualize the scene-linear render results in a pleasing appearance 
of contrast, with well balanced highlight and shadow details. Renders by Sony Pictures Imageworks, available 
for download at opencolorio.org. Images from Cloudy With a Chance Of Meatballs , &#38;#169; 2009 Sony 
Pictures Animation Inc. All rights reserved. Why is scene-linear preferred for lighting? First, the render 
itself really benefits. Physically plausible light transport renderer mechanisms (aka global illumination) 
yield natural results when given scenes with high dynamic ranges. Physically-based specular models, combined 
with area lights, produce physically plausible results with high-dynamic range data. Rendering in scene-linear 
also allows lights and material components to be re-balanced post-render, with results that track identically 
to if the original render had been tweaked. The output of renderers are typically floating-point imagery, 
and are often stored in the OpenEXR format. See appendix 4.1 for additional details on OpenEXR. One issue 
to watch out for with high dynamic range renders is scene noise. When scene textures (such as skydomes) 
contain what would be considered emissive specular areas that substantially contribute to the scene illumination 
(RGB >> 1.0), care must be taken in terms of sampling or noise is likely. Modern rendering optimizations 
such as multi-importance sampling (MIS) are useful to mitigate such issues. Even still, it is common 
to paint out very compact and/or bright light sources (such as the sun) from skydomes, and then to add 
them back into the scene as native renderer lights to allow for lower­noise sampling. Light shaders also 
benefit from working with scene-referred linear, specifically in the area of light falloff. In the past, 
the default mode of working with lights in computer-graphics was to not use falloff. However, when combined 
with physically-based shading models, using an r2 light falloff behaves naturally. If one tries to shoehorn 
in realistic lighting falloff models into lower dynamic range spaces (such as display-referred linear), 
it s very difficult to avoid clipping. On the downside, one consequence of working with natural light 
falloff is that it s often required to have very high light intensity values. It is therefore common 
to express light intensity in user interfaces in terms of stops , as it s much friendlier to the artist 
to present an interface value of + 20 stops , compared to a RGB value of 1.048e6 . Antialiasing operations 
also benefit from scene-linear, though one must be more careful with renderer reconstruction filters. 
Negatively lobed filters such as Catmull-Rom or Lanczos3 have an increased tendency to exhibit ringing 
due to the extra dynamic range. This is a particular problem on elements lit with very bright rim lights 
, as this creates bright specular highlights directly in contact with edges. There are two common approaches 
to working around such filtering issues. First, very bright specular samples can be rolled-off such that 
these samples do not contribute such large amounts of energy. However, this throws out much of the visually 
significant appearance which adds so much to the realism. Another approach is that the extra energy can 
be spread amongst neighboring pixels such that the specular hits show an effect analogous to camera flare. 
Both of these effects can be implemented either during compositing or internal to the renderer; the advantage 
of the latter is the processing can work on the sub-pixel level for higher quality anti-aliasing. HDR 
Environment Captures HDR captures are increasingly being used to capture onset lighting. The classic 
capture technique to was photograph a chrome ball, but now productions tend to directly capture the scene 
either with a multi-exposure fisheye camera setup or with dedicated hardware. One recent extension to 
this methodology is to capture the scene at high-dynamic ranges from multiple heights and/or locations, 
which through triangulation allows for energy estimations of the scene-lighting. Care must be taken in 
calibrating the HDR lighting acquisition to account for colorimetry, linearity, and white balance, or 
the resulting lighting data may not integrate into the computer-generated environments. Adding a diffuse 
sphere during capture is useful in validating the lighting reconstruction later on. 3.3. Compositing 
Compositing is the process where the live action plates are merged with the computer generated imagery. 
Image processing in both scene-linear and logarithmic encoding spaces are both useful, though scene-linear 
is increasingly the default. As in lighting, the image display must using viewing transforms that emulate 
the eventual look in DI. Examples of commercially available compositing applications include Nuke, Fusion, 
After Effects, and Flame. In feature film visual effects, plates (live action photography) are typically 
represented on disk as a log encoding of the linear camera data, often as DPX files. Frames when brought 
into the compositing package are typically converted to scene-linear on input, and then converted back 
to the original color space on output. End to end, the compositing process represents a no-op , and typically 
has perfect invertibility. The benefits of compositing in scene-linear are numerous, and similar to the 
benefits found in rendering, shading, and lighting. All operations which blend energy with spatially 
neighboring pixels (motion blur, defocus, image distortion, resizing, etc) have more physically plausible 
(aka realistic) results by default. Antialiasing works better, light mixing preserves the appearance 
of the original renders and most importantly, even simple compositing operations such as over produce 
more realistic results, particularly on semi-transparent elements such as hair and volumetrics. In this 
original scene, we use a defocus operation visualize the energy inherent in different linearizations. 
Pay particular attention to the string of lights on the top of the bridge span. Applying a 50 pixel 
defocus in scene-linear causes a beautiful bokeh effect on each of the bridge span lights, mimicking 
the visual look of having performed this defocus during image capture. This is because the pixel values 
are proportional to light in the original scene, and thus specular pixels have sufficient energy to remain 
visible when blended with their neighbors. Other energy effects such as motion-blur achieve similar improvements 
in realism from working in a scene-linear space. Applying a 50 pixel defocus in display-linear tends 
to de-emphasize the specular highlights. This is because even though the operation is being applied in 
linear , post-display transform, the highlights have already been tone-rendered for a lower dynamic range 
display and thus do not have physically­plausible amounts of energy. Scene-Linear Compositing Challenges 
There are some challenges with working with high-dynamic range in compositing. First, filtering operators 
that use sharp, negative lobed kernels such as lanczos, keys, sinc, are very susceptible to ringing (negative 
values around highlights) . While interpolatory filters such as box, gaussian, bilinear, bicubic cannot 
cause this artifact, they do introduce softening. For cases where sharp filtering is required (lens distortions, 
resizing, and other spatial warping techniques) a variety of approaches are useful to mitigate such HDR 
artifacts. The simplest approach to mitigating overshoot/undershoot artifacts is to rolloff the highlights, 
process the image, and then unroll the highlights back. While this does not not preserve highlight energy 
(it has the net effect of reducing specular intensity), the results are visually pleasing and is suitable 
for processing images with alpha. Another approach to HDR filtering is to apply a simple camera flare 
model, where very bright highlights share their energy with neighboring pixels. Finally, for images without 
alpha, converting to log, filtering, and converting back will greatly reduce the visual impact of overshoot 
and undershoot. Though log-space processing results in a gross distortion of energy, for conversions 
which are approximately 1:1 (such as lens distortion effects for plates) this works well. Applying negative-lobed 
filter kernels to high-dynamic range, scene-linear images (in this case a lanczos3 resize) may cause 
visuall-significant ringing artifacts. Observe the black silhouettes around the sharp specular highlights. 
 Ringing artifacts may be avoided when using sharp filters, on scene-referred imagery, by processing 
in alternate color representations (such as log), using energy-rolloff approaches, or by flaring highlight 
regions. This image demonstrates the visual appearance of the roll-off technique. Another challenge in 
working with scene-linear data in compositing is that tricks often relied upon in integer compositing 
may not be effective when applied to floating-point imagery. For example, the screen operator (which 
remains effective when applied to matte passes), is sometimes used on RGB data to emulate a partial add 
. As the screen operator is ill-defined above 1.0, when applied to HDR data unexpected results will be 
common. Some compositing packages swap out screen for max when either input is above 1.0, but this is 
primarily to prevent artifacts and is not artistically helpful. Alternative partial add maths such as 
hypotenuse are useful, but do not exactly replicate the original intent of screen. Another related issue 
to beware of when compositing is that when combining HDR image, alphas must not go outside the range 
of [0,1]. While it is entirely reasonable for the RGB channels to have any values (even negative in certain 
cases!) the compositing operators, such as over, produce totally invalid results on non [0,1] alphas. 
One subtlety of working with floating point imagery is that artists must become familiar with some of 
the corner cases in floating-point representations: NaNs and Infs. For example, if one divides by very 
small values (such as during unpremultiplication) it s possible to drive a color value up high enough 
to generate infinity. NaNs are less frequent, but may be introduced during shading (inside the renderer), 
or during divide by zero operations. Both nans and inf can cause issues in compositing if not detected 
and cleaned up early, as most image processing algorithms are not robust to their presence and generally 
fail in unexpected ways. Working with Log Imagery Despite HDR having benefits in many compositing operators, 
there are just some cases even in modern compositing workflows where log-like spaces are still useful. 
For example, manipulating contrast of HDR images is non-trivial. Contrast is typically describes as a 
color correction which makes the shadows darker, and the highlights brighter, centered around a pivot. 
In unconstrained floating-point spaces, if you use a linear + offset contrast operator (mx+b), it is 
difficult to choose offsets that do not drive pixels negative. And the alternative, a gamma function 
(exponentiation), has the potential to drive highlights to unreasonably large values. This is one of 
the cases where a conversion to a log-like space is very helpful. In log space, both the linear and gamma 
contrast operators have reasonable performance (this is one of the reasons that DI color work is often 
preferred to work in log-like spaces). Other operations which are useful to perform in log are grain 
matching, pulling keys, and many spatial distortion tricks. But be careful when using log color spaces 
in the graph, as not all log transforms preserve the full range of scene-linear code values. Many log 
conversions clip values above certain highlight colors, below certain shadow colors, and typically all 
negative values. Finally, compositing must also generate plates for DI that hold up under a variety of 
color corrections. See the section on Critical inspection of imagery for further details. Plate Timing 
When working with plate photography it is often useful to come up with simple color corrections for each 
plate to neutralize the lighting across a sequence. During the shoot, there will inevitably be color 
drift across the shots (such as the sun moving behind a cloud, the lights moved to different locations, 
etc). Using neutralized plates in lighting and compositing leads to nice gains in efficiency as light 
rigs and sequence standards track better. In most workflows the plate neutralization is done at the top 
of the compositing graph, the rendered neutral elements are composited in, and then before the file is 
written out the neutralization is undone. This approach is often called reverse out , and its advantage 
is that the output is consistent with the input, independent of which color correction was used. The 
color corrections used for plate neutralization must be reversible, and as such are typically handled 
as either simple offsets in camera log space, or as simple gains in linear. More complex operations such 
as contrast, saturation, etc are usually avoided as these type of corrections break the linearity of 
the plate and will likely make compositing more difficult (presuming the linearization was properly done 
to begin with). It is important to draw a distinction between timing number necessary for plate neutralizations, 
and the potentially more sophisticated looks that are crafted in the DI. At DI, the full suite of color 
correction operators can be used (contrast, keys, primaries, secondaries, etc), as there is no need to 
ever invert this process. However, the color neutralization numbers needed for lighting and compositing 
must be invertible, and as such are typically limited to a simple color balance change. Beware that if 
you bake in the neutralized color correction, to make sure area very near the top and bottom of the colorspace 
are preserved. I.e., if your compositing packages uses a log-convert on input, and then you apply timing 
number, you must confirm that downstream log-converts on the neutral graded plate do not clamp either 
the high or low end. Premultiplication Premultiplication gets a unwarranted bad wrap, as the name implies 
that one starts with an RGB color, adds the concept of transparency, and then premultiplies by it. This 
isn t actually true. Premultiplied RGB is the natural representation for color and transparency and is 
output of renderers by default. A nice mental model for premultiplied RGBA is that the RGB channels represent 
how much light is emitted from within the region of the pixel, and A represents how much a pixel blocks 
light behind it. As alpha represents the fraction of occlusion, to maintain physical correctness alpha 
values must be between 0 and 1. RGB channels have no such constraints, and can range from -infinity to 
infinity. Furthermore, there is absolutely no issue with RGBs greater than alpha (a common misconception). 
Even when alpha is zero, a positive RGBs is completely reasonable, and corresponds to light being emitted 
from a pixel without occluding anything behind it. Indeed, this situation is common in FX renders for 
emissive elements such as fire. So how should we interpret unpremultiplied RGBA? Unpremultiplied RGBA 
is most easily understood as, If this pixel were fully opaque, how much light would have been emitted? 
 This representation is certainly useful in special compositing contexts (such as when pulling keys as 
a function of luminance), but as converting to unpremultiplied RGBA is a lossy approximation it s prudent 
to only do the conversion when absolutely necessary. For additional information on premultiplied vs. 
unpremultiplied RGBA representations, see [Blinn 98]. 3.4. Texture and Matte Painting Texture painting 
and matte painting presents a color pipeline challenge. Ideally, one could directly paint in a scene-linear 
space and use appropriate visualizations to get realistic previews of the eventual render appearance. 
However, the majority of texture painting applications work most intuitively in display­referred color 
spaces, where the painted image is directly displayed on the screen. Furthermore, texture reference is 
usually acquired as display-referred imagery (such as the JPG/TIFF output from a digital camera); scene-linear 
texture reference is only available in rare situations. This color map represents the color modulation 
of the diffuse component of a surface shader, and is painted in a display-referred space. When converted 
to linear (prior to mipmapping) the resulting color values should range between [0,1] and are representative 
of physically-plausible surface albedos. Thus, a common solution is to utilize an inverse tone rendering 
, to back convert display-referred imagery to a hypothetical scene-referred space. Conceptually, the 
texture artist is painting the tone­rendered appearance of the texture, not the texture itself. There 
are a few difficulties with this conversion from display-referred to scene-referred space. First, the 
tone rendering may not be invertible, particularly when filmic 3D-LUTs are used for image preview. Second, 
traditional s-shaped tone renderings have very horizontal portions of the curve, which when inverted 
result in very steeply sloped transfer functions. This steep contrast has the effect of amplifying very 
small changes in input code values. I.e, a steep inverse could results in the situation where a single 
code value change in a painted texture could correspond to a delta of a stop or more of scene-linear 
light. This sensitivity is very difficult for artists to work with. A common solution to both of these 
issues is to not use a perfect inverse display transform, but instead to create an approximate 1-D inverse 
that is well behaved in terms of color performance and dynamic range. Of course, as a consequence the 
texture process is not truly WYSIWYG. Thus, a residual difference visualization 3D-LUT is crafted for 
purposes of color preview in the texture painting tool. This residual 3D preview is never baked into 
the imagery, merely used as a visualization. It is often useful to consider matte painting and texture 
painting as two different color transforms. In matte painting, it is very common for the artist to want 
to paint values which will eventually utilize the full scene-linear dynamic range, even when working 
in a display-referred space. Thus, an inversion which allows for the creation of bright specular values 
is preferable. In such cases, it is often convenient to provide a visual preview both at a normal exposure, 
as well as a few stops darker, to let the artist have a good feel for the dynamic range in their plate. 
In texture painting, the artist often paints color maps which when linearized will control material color 
reflectivity, as an input to physically-based shading. In these situations, linear data from [0,1] is 
required and necessitates a color transform that gracefully handles these limits. In both matte painting 
and texture painting, one must also distinguish painted color maps from data maps (bump maps, control 
maps, etc), which should not be processed colorimetrically. Matte painters typically prefer to paint 
in a display-referred color space for convenience. But as renders are natively in scene-linear, an inverse 
display transform is used to synthesize plausible scene-linear colors prior to rendering. When texture 
acquisition is done in a controlled environment, it s possible to create scene-linear texture conversions 
leveraging camera raw workflows. Command-line utilities, such as dcraw, allow color pipelines which capture 
a reasonable linear representation surface color. The use of polarizing filters and softbox lighting 
during acquisition further allows for increased reference texture fidelity. Another axis of variation 
in texturing is when to perform the conversion to scene-linear. The approach we advocate is to linearize 
prior to mipmap texture generation. The primary advantage of this approach is that scene-linear energy 
is preserved through all the mipmap levels, allowing for the highest fidelity sampling and fewest color 
space related artifacts. Further, disallowing color space conversions at shading time prevents shaders 
from doing evil (non-linear) color math. The disadvantage is that the storage requirements for linearized 
data are potentially increased. I.e., even if a texture is painted at 8-bits of precision in a display-referred 
space, after linearization to scene-linear increased bit-depths are required. When dealing with 16-bit 
painted textures, this is less of a concern. 3.5. Critical Inspection of Imagery One unfortunate circumstance 
in visual effects and animation production is that the typical artist desktop display is not as high 
fidelity as the eventual theatrical viewing environment. I.e., the majority of the time the people in 
the theater will see more detail than the artist. Furthermore, while artists typically craft the scene 
at a particular exposure, during DI is possible that the imagery will be stylistic taken in a direction 
the artists did not anticipate. It is therefore critical to inspect the imagery under a variety of worst-case 
color corrections, to make sure the computer generate imagery is as robust as possible. At a minimum, 
all artists working in scene-linear (namely the lighter and compositor) need to view their imagery stopped 
up and down in scene-linear on a regular basis. Only then can the artists get a true sense of the dynamic 
range in their imagery, and to match computer-generated elements in a similar manner. Then in compositing, 
it s very handy to convert the imagery to the colorspace (and quantization / clamping) that will be delivered 
to DI, to test how this imagery holds up to drastic grades. We recommend playing with substantial log 
space offsets, as well as contrast and saturation boost, to make sure all portions of the imagery track 
in a consistent manner. This is particularly important to match across shots and sequences, for it s 
all too easy to make individual shots that are self consistent, yet fall apart when viewed side to side. 
When stopping down, it s important to make sure that highlight intensity, color, and sharpness are consistent. 
And most critical is to stop up the imagery to make sure the shadows have a consistent color range, that 
there are no flat-black portions of the image, and that grain and black noise track between the live 
action and the computer generated imagery. Histograms are a very useful tool when analyzing image fidelity, 
and often help forensically track down what processing has been applied to an image. For example, log 
frames from certain image sources often are hard-clipped in the shadow region. This can be detected in 
the histogram, as well as sharpening filters that applied overly aggressive undershoot and overshoot. 
It s also possible to detect from the histogram if any image quantization occurred (this appears as comb-like 
peaks and valleys). We have also found that its a valuable tool to look at histograms in HSV-like color 
spaces, and to quality-check composites in these alternate spaces. There are other image artifacts, such 
as aliasing, that are best inspected looking at moving imagery. For example, the use of low-tech interpolatory 
filters, such as nearest neighbor interpolation , is sometime unobjectionable on still imagery, yet highly 
objectionable on slowly animating transforms (particularly near horizontal lines). Always be on the lookout 
for moire patterns and other high-frequency artifacts, particularly when spatial transformations have 
been applied to digital motion-picture imagery with high frequency details (such as cloth patterns). 
If all digital imagery were ideally sampled, this would not be an issue. But many camera manufacturers 
generate images which are overly sharp (aka aliased) and certain motion transformations accentuate the 
artifacts). For stereo deliveries, it s critical to quality check both eyes individually, as well as 
together in a proper stereo viewing environment. There are many stereo artifacts which are only visible 
on real 3D displays, and catching these early on is the easiest way to handle artifacts. 3.6. ACES The 
Academy of Motion Picture Arts &#38; Science has proposed a unification of scene-linear floating-point 
workflows, which adheres to many of the approaches taught in this class. If successful, the ACES project 
will allow for the unambiguous interchange of floating point imagery. At its core is the Academy Color 
Encoding Space, called ACES for short. This color space is a high-dynamic range, scene-linear space, 
with middle gray pegged to 0.18, and a very wide color gamut. When stored on disk, aces files are be 
stored in a constrained version of the OpenEXR format, with the .aces file extension. Digital Digital 
Camera Projector The ACES workflow defines scene-referred working space, and a reference viewing transform 
in an attempt to standardize float-linear interchange in the motion-picture industry. The Academy also 
defines the viewing transform necessary for viewing ACES files. The view transform is conceptually segmented 
into two portions. First, the Reference Rendering Transform (RRT) applies a local contrast boost and 
renders scene-linear data to display linear. Then, a second portion of the view transform called the 
Output Device Transform (ODT) is used to provide gamut mapping and further tone mapping to the target 
output device. The RRT portion is constant across all displays, but the ODT varies from output to output. 
ODTs are provided for common display specifications such as sRGB, Rec709, DCI-P3, X Y Z , etc. On the 
input side, there are also a series of published input device transforms (IDTs) which convert input colorimetry 
to scene-linear (ACES). Film scans are a special case, where the academy defines a new density encoding 
standard, the Academy Density Encoding (ADX), which is typically stored in a DPX file. Both 10 and 16 
bit flavors of ADX are provided, which encode for different negative density ranges. The academy input 
transform for film negatives is not stock specific, but relies on a generic film input linearization. 
In the ACES workflow, the intent is for artistic changes to be made by manipulating the aces data, as 
viewed through the RRT + ODT. All of the transforms (both input and output) for ACES are defined using 
the Color Transformation Language (CTL), which was contributed by Industrial Light &#38; Magic. CTL is 
an interpreted language similar in spirit to shader languages commonly used in renderers, but with a 
focus on color transforms. CTL provides a rich color correction API, including scattered data interpolation. 
CTL is run independently per pixel, and is thus suitable for baking into 1D/3D LUTs -allowing for real-time 
performance. ACES is also fully supported using OpenColorIO, as detailed in the next section. 3.7. OpenColorIO 
OpenColorIO (OCIO) is an open source color pipeline created by Sony Pictures Imageworks (and the author), 
which implements most of the concepts concepts in this course. It has two major goals: consistent color 
transforms, and consistent image display, across multi-application / multi-OS color pipelines. Two applications, 
configured with OpenColorIO, provide matched image display. The image on the left is Nuke, which has 
loaded a log film plate in the DPX image format. On the right is the display in Katana, which has loaded 
a scene-linear OpenEXR version of this image. Both applications utilize OpenColorIO for color management. 
The design goal behind OCIO is to decouple the color pipeline API from the specific color transforms, 
which allows supervisors setting up color processes to tightly manage (and experiment) with color transforms 
from a single location. Setup your color once, and all application obey. Unlike other color management 
solutions (such as ICC) OCIO is natively designed to handle both scene-referred (float HDR) and display-referred 
imagery. All color transforms are loaded at runtime, from a color configuration (.ocio file), optionally 
crafted by the user. OCIO does not make any assumptions about your imagery; all color transformations 
are opt in . Sony Pictures Imageworks has also open-sourced some of the real color configurations for 
productions, such as those used in Cloudy with a Chance of Meatballs and Spider-Man. This lets users 
at home (or other studios) experiment with validated color pipelines. OpenColorIO also ships with a configuration 
compatible with the Acedemy s ACES effort, allowing for graceful experimentation with this next-gen color 
pipeline in existing applications. OCIO color configuration files define all of the conversions that 
may be used. For example, if you are using a particular camera color space, one would define the conversion 
from the camera s color encoding to scene-linear. You can also specify the display transforms (for multiple 
displays) in a similar manner. OCIO transforms can rely on a variety of built-in building-blocks, including 
all common math operations and the majority of common lookup table formats. OCIO also has full support 
for both CPU and GPU pathways, in addition to full support for CDLs and per-shot looks. OpenColorIO is 
in use at many of the major visual effects and animation studios, and is also supported out of the box 
in commercial software. See opencolorio.org for up-to-date information on supported software, and to 
download the source code for use at home. 4. Appendix This page intentionally left blank. 4.1. Lookup 
Tables Lookup tables (LUTs) are an technique for optimizing the evaluation of functions that are expensive 
to compute and inexpensive to cache. By precomputing the evaluation of a function over a domain of common 
inputs, expensive runtime operations can be replaced with inexpensive table lookups. If the table lookups 
can be performed faster than computing the results from scratch, then the use of a lookup table will 
yield significant performance gains. For data requests that fall between the table's samples, an interpolation 
algorithm can generate reasonable approximations by averaging nearby samples. LUTs are also useful when 
wanting to separate the calculation of a transform from its application. For example, in color pipelines 
it is often useful to bake a series of color transforms into a single lookup table, which is then suitable 
for distribution and re-use, even in situations where the original data sets are not appropriate for 
distribution. 1-D LUTs A lookup table is characterized by its dimensionality, that is, the number of 
indices necessary to index an output value. The simplest LUTs are indexed by a single variable and thus 
referred to as one­dimensional (or 1D) LUTs. One-dimensional LUTs have long been utilized in image processing, 
most commonly in the form of the monitor gamma table. Typically leveraging three LUTs one for each color 
channel these tables enable the computationally efficient modification of pixel intensity just before 
an image reaches the monitor. Consider an analytical color operator, f(x), applied to an 8-bit grayscale 
image. The naive implementation would be to step through the image and for each pixel to evaluate the 
function. However, one may observe that no matter how complex the function, it can evaluate to only one 
of 255 output values (corresponding to each unique input). Thus, an alternate implementation would be 
to tabulate the function's result for each possible input value, then to transform each pixel at runtime 
by looking up the stored solution. Assuming that integer table lookups are efficient (they are), and 
that the rasterized image has more than 255 total pixels (it likely does), using a LUT will lead to a 
significant speedup. All color operators that can be parameterized on a single input variable can be 
accelerated using 1D LUTs, including the brightness, gamma, and contrast operators. By assigning a 1D 
LUT to each color channel individually, we can implement more sophisticated operations, such as color 
balancing. For those familiar with the Photoshop image-processing software, all "Curves" and "Levels" 
operations can be accelerated with 1D LUTs. Unfortunately, many useful color operators cannot be parameterized 
on a single variable, and are thus impossible to implement using a single-dimensional LUT. For example, 
consider the "luminance operator" that converts colored pixels into their grayscale equivalent. Because 
each output value is derived as a weighted average of three input channels, one would be hard-pressed 
to express such an operator using a 1D LUT. All other operators that rely on such channel "cross talk" 
are equally inexpressible. 3-D LUTs Three-dimensional lookup tables offer the obvious solution to the 
inherent limitation of single­dimensional LUTs, allowing tabular data indexed on three independent parameters. 
Whereas a 1D LUT requires only 4 elements to sample 4 locations per axis, the corresponding 3D LUT requires 
43 = 64 elements. Beware of this added dimensionality; 3D LUTs grow very quickly as a function of their 
linear sampling rate. As a direct implication of smaller LUT sizes, high-quality interpolation takes 
on a greater significance for 3D LUTs. Complex color operators can be expressed using 3D LUTs, as completely 
arbitrary input-output mappings are allowed. For this reason, 3D LUTs have long been embraced by the 
colorimetry community and are one of the preferred tools in gamut mapping (Kang 1997). In fact, 3D LUTs 
are used within ICC profiles to model the complex device behaviors necessary for accurate color image 
reproduction (ICC 2004). The majority of color operators are expressible using 3D LUTs. Simple operators 
(such as gamma, brightness, and contrast) are trivial to encode. More complex transforms, such as hue 
and saturation modifications, are also possible. Most important, the color operations typical of professional 
color­grading systems are expressible (such as the independent warping of user-specified sections of 
the color gamut). Unfortunately, in real-world scenarios, not all color transforms are definable as direct 
input-output mappings. In the general case, 3D LUTs can express only those transforms that obey the following 
characteristics: A pixel's computation must be independent of the spatial image position. Color operators 
that are influenced by neighboring values, such as Bayesian-matting (Chuang et al. 2001) or garbage masks 
(Brinkman 1999), are not expressible in lookup-table form. The color transform must be reasonably continuous, 
as sparsely sampled data sets are ill suited to represent discontinuous transformations. If smoothly 
interpolating over the sampled transform grid yields unacceptable results, lookup tables are not the 
appropriate acceleration technique. The input color space must lie within a well-defined domain. An "analytically" 
defined brightness operator can generate valid results over the entire domain of real numbers. However, 
that same operator baked into a lookup table will be valid over only a limited domain (for example, perhaps 
only in the range [0,1]). 3D LUTs can be extended for use on HDR color spaces by wrapping the 3d LUT 
lookup in a matched set of 1D shaper LUTs. Say we have set our ceiling at a pixel value of 100.0. Dividing 
this into equally sampled regions for a 32x32x32 LUT yields a cell size of about 3. Assuming a reasonable 
exposure transform, almost all perceptually significant results are going to be compressed into the few 
lowest cells, wasting the majority of our data set. We thus want to place our samples in the most visually 
significant locations, which typically occur closer to the dark end of the gamut. We achieve this effect 
by wrapping our 3D lookup-table transform with a matched pair of 1D "shaper" LUTs. The ideal shaper LUT 
maps the input HDR color space to a normalized, perceptually uniform color space. The 3D LUT is then 
applied normally (though during its computation, this 1D transform must be accounted for). Finally, the 
image is mapped through the inverse of the original 1D LUT, "unwrapping" the pixel data back into their 
original dynamic range. This section was adapted from GPU Gems 2, Chapter 24 (available online). See 
original for further implementation details. 4.2. ASC-CDL The world of color correction, particularly 
as it is handled onset, has a huge amount of variation. Even though it is common to apply a primary grade 
(consisting of a scale operation, some offsets, and maybe a gamma and saturation adjustment), every manufacturer 
tends to apply these corrections in a different order which means that the settings are not portable. 
The American Society of Cinematographers (ASC) thus has created a color correction specification to try 
and bring a bit of order to the chaos. Similar to the EDL (edit decision list) in use by editorial systems, 
the ASC came up with the CDL (Color Decision List) format. This specification also defines the math for 
what is expected on a primary correction. The CDL defines a fixed transform color correction: 1.Scaling 
(3 channels independent) 2.Offset (3 channels independent) 3.Power (exponent) (3 channels independent) 
4.Saturation (with a fixed 709 saturation ratio) Having a fixed order is not always ideal. For example, 
in a CDL you cannot desaturate the image to grayscale, and then tint the image using scales + offsets. 
(The opposite is allowed, of course). But having an unambiguous way to interchange simple grade data 
is a huge improvement in interoperability. The ASC has also defined an XML format for the grade data, 
called the ColorCorrectionCollection. The Scaling, Offset and Power are sent as a set of 9 numbers (the 
SOP) and the saturation is sent as a single number (SAT). <ColorCorrectionCollection xmlns="urn:ASC:CDL:v1.01"> 
<ColorCorrection id="example_correction_01"> <SOPNode> <Slope> 1.1 1.1 1.1 </Slope> <Offset> -0.07 -0.07 
-0.07 </Offset> <Power> 1.0 1.0 1.0 </Power> </SOPNode> <SatNode> <Saturation> 1.0 </Saturation> </SatNode> 
</ColorCorrection> </ColorCorrectionCollection> Example .ccc (Color Correction Collection) xml file demonstrate 
the SOP and Sat elements. So what does the ASC CDL not define? Color space. If one applied an additive 
offset to logarithmic encoded data, the result is very different than if the same offset is applied to 
scene-linear imagery. The CDL also does not specify if any viewing LUTs were used. This ambiguity is 
both CDLs biggest strength and its biggest weakness. It is a weakness, because if you get a CDL in isolation 
the color correction is still not well defined. However, this is also CDLs biggest strength as it enables 
CDLs to be highly versatile, serving as building block across many color pipelines. For example, the 
author is familiar with CDLs being used to send plate neutralization (log offsets) between facilities, 
and also used to send display-referred color corrections from onset to the VFX vendors. 4.3. File Formats 
 OpenEXR OpenEXR is an open-source image format creating by ILM 7 in 1999, which has near universal adoption 
in the VFX and animation industries . EXR is primarily intended for storing floating point, scene referred 
imagery. Although EXR was not the first image format suitable for storing HDR float data, is it the most 
popular in feature film production due to its efficient losses compression codecs, support for 16-bit 
(half) float pixel type, in addition to other professional features such as data window / display window 
tracking, multiple layers, rich metadata encoding, and the lack of legacy format baggage. In our experience, 
the half data format is sufficient for color images. (We consider color renders to be RGB(a) channels 
that one would view using a tone-rendered view transform, such as beauty renders, specular color, diffuse 
color, per-light AOVs, etc). For data images which represent numerical quantities (such as depth, normals, 
control maps, etc) the full precision of 32-bit float is usually required. The maximum value for float16 
is 65504.0f. The minimum value which can be represented, without a decrease in numerical precision, is 
6.1035e-05f. It is important to remember when dealing with float16 data, that the bits are allocated 
very differently from an integer encodings. Whereas integer encodings have uniform allocation of bits 
over the entire coding space, the half-float format (like all floating-point formats) has increased precision 
in the low end, and decreased precision in the high end. Relatedly, if one has an uint16 image, converts 
to an f16 representation using the code values from [0,1], and then converts back, 16-bits of precision 
will not be maintained. Specifically, the round trip will preserve 7168 unique code values (of the original 
65535), which corresponds to approximately 12.8 bits of round-trip precision. OpenEXR supports a variety 
of compression options. In our experience, piz (wavelet) offers the highest lossless compression ratio 
on grainy material, but is relatively expensive computationally. The zip options offer a reasonable compression 
ratio on computer-generated elements, and is less computationally intense. The b44 codec is lossy, and 
intended for real-time playback. OpenEXR also can store mipmapped representations, and is thus suitable 
for use in the renderer as input textures. DPX DPX is a SMPTE standardized image format that is commonly 
used in the motion-picture industry. While the DPX format supports a variety of pixel types - even float32 
- DPX is most commonly associated with storing uncompressed, 10-bit unsigned integer RGB imagery (though 
16-bit DPX is catching up in popularity). Unlike EXR - which is synonymous with scene-linear colorimetry 
- DPX does not have a canonical color space. It is common for DPX to store any number of integer camera 
log encodings, in addition to broadcast-ready Rec709. Generally, if you ve got a DPX file the only way 
to know for sure what you have is through communication with the person who sent it (or detailed forensic 
analysis). DPX has limited metadata support, including a few settings related to colorimetry. But beware 
these flags, even under the best of intentions their fidelity is rarely preserved for long. 7 EXR stands 
for EXtended Range . Internally, the format was original known as IMage Format ; a name that lives on 
in the codebase as the C++ namespace. 4.4. DCI P3 and X Y Z The Digital Cinema Initiative (DCI) specification 
defines the standards for digital cinema mastering and theatrical exhibition, including colorimetry. 
A new color encoding, X Y Z , is specified for image encoding, which is an output-referred, gamma 2.6 
encoded version of CIE XYZ with a reference white point at 48cd/m2. As the X Y Z coding space spans an 
area larger than the visual gamut, a minimum display gamut, P3, is defined. The P3 primaries are very 
saturated relative to television standards; sRGB red is a relatively desaturated orange color, while 
P3 red is blood red ; almost on the spectral locus. The intent of the X Y Z coding space is that the 
full color appearance of the theatrical imagery (such as any film emulation 3D-LUTs) is already baked 
into the X Y Z image. So an image in this color space is completely unambiguous; there should be essentially 
no image variation between properly calibrated theatrical digital projectors. The DCI specification chose 
a gamma 2.6 encoding after a series of perceptual experiments using golden-eye observers , to maximize 
the bit-depth fidelity under theatrical viewing conditions. DCI specifies 12 bits per channel, which 
is intended to prevent banding artifacts under even the most trying conditions. DCI also specifies a 
series of color patches which are useful in calibration. (Please refer to the specification for additional 
details.) X Y Z files are encoded using JPEG-2000 compression, which is a lossy, wavelet compression 
codec. No inter-frame compression is used, which allows for both forwards and backwards scrubbing of 
the format. The DCI specification also specifies two resolutions known as 2K and 4K . The 2K raster is 
2048x1080 and the 4K raster is 4096x2160. DCI compliant films must fill at least one of the the axes. 
Material with an aspect ration of 1:85 is delivered for a 2K release at 1998x1080, and 2:35 material 
is delivered at 2048x858. Most X Y Z encoders accept 16-bit tiffs, so it s convention to use the full 
16-bit code range of 0-65535, and then to let the compressor quantize to 12­bits. See [Kennel 07] for 
additional details on mastering for digital cinema. The full DCI specification is also available online. 
DCI-P3 Color Encoding (Gamma 2.6) Red: 0.680, 0.320  Green: 0.265, 0.690  Blue: 0.150, 0.060  White: 
0.3140, 0.3510, 48cd/m2  X Y Z Example Code Values (12-bit representation): P3 Red-1: 2901, 2171, 100 
P3 Green-1: 2417, 3493, 1222 P3 Blue-1: 2014, 1416, 3816 P3 White: 3794, 3960, 3890 4.5. Daylight Curve 
vs. Blackbody Curve Colors near whites are often referenced with respect to their color temperature. 
This single number denotes the relative red vs. blueness of the color. Low color temperatures appear 
redder (warmer), and high temperatures appear bluer (cooler). The projection of a color onto the blackbody 
curve is sometimes known as the CCT (correlated color temperature). As CCT is only a single measure of 
color, a very large number of colors with different appearances can all have matching CCTs. (Roughly 
speaking, these matching CCT colors would vary in the direction of how much green they contained). The 
blackbody curve (and the corresponding measure of color temperature) is based on material physics. Specifically, 
all materials emit light referred to as blackbody radiation as a function of temperature. Hotter objects 
emit more light, and as the temperature increases so does the frequency. Near room temperature, objects 
radiate mostly infrared light, which we feel as heat. When an object is heated to high enough temperatures, 
the emitted radiation increases enough in frequency to spill over into the visible spectrum starting 
with the red wavelengths; the object appears to glow red hot . As an object is heater even further, the 
frequency of the emitted object will increase until reds and blues are in balance (aka, white hot ), 
and then further increases will appear blue when hottest. Plotting these colors forms the blackbody curve. 
The daylight curve is a series of standard CIE illuminants, which have the intent to simulate an average 
color of daylight, with different CCTs. Colors on the daylight curve are often preferred in display calibrations 
due to their their neutral white appearance. For example, rec709 specifies the use of a D65 illuminant. 
Other defined daylight illuminants include D50 and D55. While the daylight illuminants roughly have a 
CCT matching their names, it s important to note that the daylight curve is parallel, but different, 
from the blackbody curve. So you must distinguish between 6500K CCT, vs D65. 0.9 0.8 0.7 0.6 0.5 y 0.4 
0.3 0.2 0.1 0.0   D aylight curve D 65 B lackb ody 6,500 °K c urve  0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 
0.8 x The daylight curve and the blackbody emission curve, while roughly parallel, are distinct. As visualized 
above, D65 is a distinct color from an idealized 6500K blackbody emitter. 5. Acknowledgements Course 
notes by Jeremy Selan. Illustrations by Kazunori Tanaka. A very special thanks goes to: Sony Pictures 
Imageworks and Rob Bredow in particular, for the opportunity to contribute publicly to the color community. 
Addition thanks to Erik Strauss, Bob Peitzman, the Katana team, and all the artists at Imageworks.  
Contributors to OpenColorIO, including Malcolm Humphreys, Ben Dickson, Mark Fickett, and Joseph Slomka. 
 The Foundry  The Academy s Scientific and Technical Council including Alex Forsythe, Ray Feeney, and 
Andy Maltz.  6. References &#38; Further reading Books Ansel Adams The Camera, Book 1. Little, Brown, 
and Company. Ansel Adams The Negative, Book 2. Little, Brown, and Company. Ansel Adams The Print, Book 
3. Little, Brown, and Company. Jim Blinn Jim Blinn s Corner: Dirty Pixels. Morgan Kaufmann. Mark Fairchild 
 Color Appearance Models. Addison-Wesley. Ed Giorgianni Digital Color Management: Encoding Solutions. 
Prentice Hall. Glenn Kennel Color and Mastering for Digital Cinema. Focal Press. R.W.G. Hunt Measuring 
Colour. Fountain Press. Charles Poynton A Technical Introduction to Digital Video. Wiley. Erik Reinhard 
 Color Imaging, Fundamentals and Applications. AK Peters, Ltd. Gunter Wyszecki &#38; W.S. Stiles Color 
Science: Concepts and Methods. Wiley &#38; Sons, Inc. Papers Giorgianni, Ed. Color Management for Digital 
Cinema: A Proposed Architecture and Methodology for Creating, Encoding, Storing and Displaying Color 
Images in Digital Cinema Systems. Submitted to the Science and Technology Council, Academy of Motion 
Picture Arts and Sciences. 2005. Online Bruce Lindbloom - Online resource for color conversion math Charles 
Poynton - Information for video standards and gamma, available for download. DCI Specification - Standards 
for theatrical digital distribution and colorimetry. DCraw - Free, high quality, camera raw convertor 
OpenColorIO - Open Source Color Management Framework for Visual Effects and Animation Visualizing the 
XYZ Color Space (YouTube) - Visual exploration of CIE XYZ 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343493</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>7</pages>
		<display_no>10</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Practical physically-based shading in film and game production]]></title>
		<page_from>1</page_from>
		<page_to>7</page_to>
		<doi_number>10.1145/2343483.2343493</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343493</url>
		<abstract>
			<par><![CDATA[<p>Physically based shading is increasingly important in both film and game production. By adhering to physically based, energy-conserving shading models, one can easily create high-quality, realistic materials that maintain that quality under a variety of lighting environments. Traditional "ad-hoc" models require extensive tweaking to achieve the same result, so it is no surprise that physically based models have increased in popularity in film and game production, particularly because they are often no more difficult to implement or evaluate.</p> <p>This course was last presented at SIGGRAPH 2010. With extensive updates, the SIGGRAPH 2012 course presents two years of advances in physically based shading. It includes new research in the area and more production examples from film and game. The course begins with a brief introduction to the physics and mathematics of shading before instructors share examples of how physically based shading models are being used in production. Real-world examples are a particular focus of this year's course, which is designed to give attendees a practical grounding in the subject.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738912</person_id>
				<author_profile_id><![CDATA[81504687746]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McAuley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ubisoft Montreal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738913</person_id>
				<author_profile_id><![CDATA[81504687284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hill]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ubisoft Montreal]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738914</person_id>
				<author_profile_id><![CDATA[81332504435]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naty]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoffman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision Studio Central]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738915</person_id>
				<author_profile_id><![CDATA[81504688355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yoshiharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gotanda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[tri-Ace]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738916</person_id>
				<author_profile_id><![CDATA[81504685458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smits]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Pixar Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738917</person_id>
				<author_profile_id><![CDATA[81504683688]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Brent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Walt Disney Animation Studios]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738918</person_id>
				<author_profile_id><![CDATA[81466641907]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Adam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martinez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Sony Pictures Imageworks]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical Physically-Based Shading in Film and Game Production SIGGRAPH 2012 Course  Course Organizers 
Stephen Hill Stephen McAuley Ubisoft Montreal Presenters Brent Burley Walt Disney Animation Studios 
Yoshiharu Gotanda tri-Ace Naty Hoffman Activision Studio Central Adam Martinez Sony Pictures Imageworks 
Brian Smits Pixar Animation Studios Course Description Physically-based shading is becoming of increasing 
importance to both .lm and game production. By adhering to physically-based, energy-conserving shading 
models, one can easily create high quality, realistic materials that maintain that quality under a variety 
of lighting environments. Traditional ad-hoc models have required extensive tweaking to achieve the same 
result, thus it is no surprise that physically-based models have increased in popularity in .lm and game 
production, particularly as they are often no more di.cult to implement or evaluate. After the success 
of the Physically-Based Shading Models in Film and Game Production course at SIGGRAPH 2010, this new 
course presents two years of advances in the subject. New research in the area will be covered, as well 
as more production examples from .lm and game. The course begins with a brief introduction into the physics 
and mathematics of shading, before speakers share examples of how physically-based shading models have 
been used in production. New research is introduced; its practical usage in production explained; then 
the advantages and disadvan­tages are discussed. Real-world examples are a particular focus of this year 
s course, giving attendees a practical grounding in the subject. Level of Difficulty: Intermediate Intended 
Audience Practitioners from the videogame, CG animation, and VFX .elds, as well as researchers interested 
in shading models. Prerequisites An understanding of shading models and their use in .lm or game production. 
Course Website All course materials can be found at http://selfshadow.com/publications/s2012-shading-course 
Contact Address questions or comments to s2012course@selfshadow.com  About the Presenters Brent Burley 
is a Principal Software Engineer at Walt Disney Animation Studios working on pro­duction rendering software. 
Recently he led the development of a new physically-based BRDF model now being used in all current productions. 
Prior to joining Disney in 1996, he worked at Philips Media developing a cross-platform game engine, 
and also worked on aircraft training simulators at Hughes Training Inc. Yoshiharu Gotanda is the CEO 
and CTO of tri-Ace, Inc, which is a game development studio in Japan. Stephen Hill is a 3D Technical 
Lead at Ubisoft Montreal, where his current focus is on physically­based methodologies. He previously 
held this role on Splinter Cell Conviction, where he steered de­velopment of the renderer for the entire 
development period. During that time, he developed systems for dynamic ambient occlusion and visibility. 
Naty Hoffman is a Technical Director at Activision Studio Central, where he assists Activision s worldwide 
studios with graphics research and development. Prior to joining Activision in 2008, Naty worked for 
two years on God of War III at SCEA Santa Monica Studio. Naty has also worked at Naughty Dog (where he 
had an instrumental role in the development of the ICE libraries for .rst­party PS3 developers), at Westwood 
Studios (where he was graphics lead on Earth and Beyond) and at Intel as a microprocessor architect, 
assisting in the de.nition of the SSE and SSE2 instruction set extensions. Adam Martinez is a Shader 
Writer for Sony Pictures Imageworks and a member of the Shading Department, which oversees all aspects 
of shader writing and production rendering at Imageworks. He is a pipeline developer, look development 
artist, and technical support liaison for productions at the studio and he is one of the primary architects 
of Imageworks rendering strategy behind 2012 and Alice In Wonderland. Stephen McAuley joined Ubisoft 
in 2011 after spending 5 years at Bizarre Creations, where he worked on games such as Blood Stone, Blur 
and Project Gotham Racing, focusing on rendering archi­tecture, physically-based shading and deferred 
lighting. Brian Smits has worked at Pixar Animation Studios since 2000. He developed the re.ection model 
used for WALL-E and Up. He currently works in the RenderMan group. Before Pixar, he was a research professor 
at the University of Utah working on rendering. He received his PhD in computer science from Cornell 
University. Presentation Schedule 09:00 09:05 Introduction: The Importance of Physically-Based Shading 
(Hill) 09:05 09:30 Background: Physically-Based Shading (Ho.man) 09:30 10:00 Calibrating Lighting and 
Materials in Far Cry 3 (McAuley) 10:00 10:30 Beyond a Simple Physically-Based Blinn-Phong Model in Real-Time 
(Gotanda) 10:30 10:45 Break 10:45 11:15 Physical Production Shaders with OSL (Martinez) 11:15 11:45 Physically-Based 
Shading at Disney (Burley) 11:45 12:15 Re.ection Model Design for WALL-E and Up (Smits)  Abstracts 
Background: Physically-Based Shading Naty Ho.man We will go over the fundamentals behind physically-based 
shading models, starting with a qualitative description of the underlying physics, followed by a quantitative 
description of the relevant mathemat­ical models, and .nally discussing how these mathematical models 
can be implemented for shading. Calibrating Lighting and Materials in Far Cry 3 Stephen McAuley Far 
Cry 3 is a .rst-person shooter due to be released in 2012 on Xbox 360, PlayStation 3 and PC. With indoor 
and outdoor areas, and a real-time day-night cycle, the game engine must support a huge variety of environments. 
Materials must be made to react correctly under all possible lighting conditions and all possible camera 
angles. By using physically-based shading, we were able to ensure that every environment would react 
as expected under any lighting setup, leading to not only a better quality end result, but an improved 
work.ow for artists who were no longer trying to overcome lighting problems. For materials, we observed 
that surface albedo is a physically-based property and developed a colour correction tool that could 
calculate albedo from photographs, using a Macbeth ColorChecker as a reference object. Textures were 
then built using this colour-corrected photographic reference. Not only did this help with balancing 
materials under all lighting conditions, but it enabled us to have the vivid colours required of a tropical 
island. To calibrate lighting, we generated second-order spherical harmonics from the sky dome to use 
for the ambient light, then estimated the sun and sky ratio using photographic data and our calibration 
tool. However, a speci.c sky post-process was needed to achieve the .nal look, simulating the e.ects 
of a polarising .lter. Finally, we implemented a physically-based shading model in order to accurately 
use the data we provided. We used a micro-facet BRDF, but found a fast approximation to the geometric 
term that could be combined for free with the normalisation factor of the NDF. To correctly .lter specular 
highlights, we used Toksvig maps, which could optionally be averaged to a single value to avoid compression 
issues. Beyond a Simple Physically-Based Blinn-Phong Model in Real-Time Yoshiharu Gotanda Physically-based 
shading is becoming standard for video games, though it is typically still based around simple models 
such as Blinn-Phong-Lambert. Our modi.ed Blinn-Phong model not only has physically­based characteristics 
close to Cook-Torrance, but is also computationally reasonable for lower-end GPUs, such as those in existing 
game consoles. However, this is still not enough to accurately represent complicated materials that exist 
in the real world. In this presentation, sophisticated real-time shading models are discussed that are 
capable of greater material .delity and range than a simple physically-based Blinn-Phong model, yet remain 
feasible even for current-generation consoles. 1  Physical Production Shaders with OSL Adam Martinez 
Open Shading Language (OSL), being targeted at advanced ray-tracers, is built on a foundation of physically-based 
shading principles. Concepts such as energy conservation, reciprocity, and physical plausibility are 
built in to the system. This is not to say that these principles cannot be intentionally violated when 
necessary, but the BSDF closure is a core component, and .rst class citizen in the library. The use of 
BSDF closures means that shaders can t help but be energy conserving and reciprocal. And because of the 
way that the aggregate closure is sampled by the renderer, you can t make things implausible by composing 
the BSDFs, either. From a shader writing perspective, OSL strongly encourages view independence. One 
can use the eye vector, absolutely, but you have to bend over backwards to do it. Ultimately, this means 
that the same shaders ought to have the same meaning for traditional backward ray (or path) tracing, 
photon mapping, or bidirectional ray tracing. Certainly, no renderer will support all of these methods, 
but the rendering method is no longer partially encoded in the shader. This talk will focus on the nuts 
and bolts of building practical, physically-based, production-worthy shaders in OSL. Starting with what 
a BSDF closure in the OSL library looks like, I will take a shader from concept to completion. I will 
cover the use of OSL s physically based shading features, and how to reconcile those features with typical 
non-physical needs of feature .lm production. We will also bid farewell to the light loop, and talk about 
why we don t want to do this in shaders for advanced ray tracers. Finally we will discuss the implications 
of writing shaders that compute radiance closures, and not .nal colors. Physically-Based Shading at 
Disney Brent Burley After attending the 2010 version of this course, several of us at Disney Animation 
Studios were inspired to investigate general physically-based material models. We had recently had success 
integrating physically-based hair shading for Tangled and we were now interested in looking at a broader 
range of materials. To start our investigation we developed a new BRDF viewer to compare existing physical 
models with measured BRDF data. We discovered an intuitive way to view measured data and we found interesting 
features in the measured materials that aren t well-represented by any known model. During this course 
we will share observations of di.erent types of measured materials along with insights we ve gleaned 
about which models .t the measured data and where they fall short. We will then present a new model we 
developed which is now being used on all current productions. We will also talk about our production 
experience of adopting this new model and how we were able to add the right level of artistic control 
while preserving simplicitly and robustness. Re.ection Model Design for WALL-E and Up Brian Smits I 
will present the re.ection model used for WALL-E and Up. Before that, Pixar had used a slowly evolving 
model that was .rst developed for A Bug s Life. We wanted a new one that could quickly generate a broad 
range of looks, was e.cient when used in shaders composed of many layers of materi­als, would .t accurately 
into our hardware relighting tool, and minimized the amount of time lighters had to spend adjusting surface 
shader properties. 2 We chose to make energy conservation the default behavior. With that constraint 
and the goal of making it easy to drive the re.ection model with patterns, we came up with a small set 
of parameters that controlled most of the model. Using the same ideas, we developed an approach to layering 
that gave a lot of .exibility and kept the cost of the re.ection model independent of the number of layers. 
This meant that the data needed for the relighting tool was .xed and thus the graphics hardware could 
give the same results from the model as RenderMan did. The results required minimal surface shader adjustments 
from the lighting department. 3  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343494</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>265</pages>
		<display_no>11</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Computational plenoptic imaging]]></title>
		<page_from>1</page_from>
		<page_to>265</page_to>
		<doi_number>10.1145/2343483.2343494</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343494</url>
		<abstract>
			<par><![CDATA[<p>A new generation of computational cameras is emerging, spawned by the introduction of the Lytro light-field camera to the consumer market and recent accomplishments in the speed at which light can be captured. By exploiting the co-design of camera optics and computational processing, these cameras capture unprecedented details of the plenoptic function: a ray-based model for light that includes the color spectrum as well as spatial, temporal, and directional variation. Although digital light sensors have greatly evolved in the last years, the visual information captured by conventional cameras has remained almost unchanged since the invention of the daguerreotype. All standard CCD and CMOS sensors integrate over the dimensions of the plenoptic function as they convert photons into electrons. In the process, all visual information is irreversibly lost, except for a two-dimensional, spatially varying subset: the common photograph.</p> <p>This course reviews the plenoptic function and discusses approaches for optically encoding high-dimensional visual information that is then recovered computationally in post-processing. It begins with an overview of the plenoptic dimensions and shows how much of this visual information is irreversibly lost in conventional image acquisition. Then it discusses the state of the art in joint optical modulation and computation reconstruction for acquisition of high-dynamic-range imagery and spectral information. It unveils the secrets behind imaging techniques that have recently been featured in the news and outlines other aspects of light that are of interest for various applications before concluding with question, answers, and a short discussion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738919</person_id>
				<author_profile_id><![CDATA[81335499586]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738920</person_id>
				<author_profile_id><![CDATA[81331495034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ihrke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universit&#228;t des Saarlandes and Max-Planck-Institut f&#252;r Informatik]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738921</person_id>
				<author_profile_id><![CDATA[81319495323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738922</person_id>
				<author_profile_id><![CDATA[81100644737]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heidrich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738923</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738924</person_id>
				<author_profile_id><![CDATA[81100563035]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kurt]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akeley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lytro, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778766</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adams, A., Talvala, E.-V., Park, S. H., Jacobs, D. E., Ajdin, B., Gelfand, N., Dolson, J., Vaquero, D., Baek, J., Tico, M., Lensch, H. P. A., Matusik, W., Pulli, K., Horowitz, M., and Levoy, M. 2010. The Frankencamera: an Experimental Platform for Computational Photography. <i>ACM Trans. Graph. (SIGGRAPH) 29</i>, 29:1--29:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Adelson, E. H., and Bergen, J. R. 1991. The Plenoptic Function and the Elements of Early Vision. In <i>Computational Models of Visual Processing</i>, MIT Press, 3--20.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>132014</ref_obj_id>
				<ref_obj_pid>132013</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Adelson, E., and Wang, J. 1992. Single Lens Stereo with a Plenoptic Camera. <i>IEEE Trans. PAMI 14</i>, 2, 99--106.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>969966</ref_obj_id>
				<ref_obj_pid>969961</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Aggarwal, M., and Ahuja, N. 2004. Split Aperture Imaging for High Dynamic Range. <i>Int. J. Comp. Vis. 58</i>, 1, 7--17.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Agrawal, A., and Raskar, R. 2007. Resolving Objects at Higher Resolution from a Single Motion-Blurred Image. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Agrawal, A., and Raskar, R. 2009. Optimal Single Image Capture for Motion Deblurring. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Agrawal, A., and Xu, Y. 2009. Coded Exposure Deblurring: Optimized Codes for PSF Estimation and Invertibility. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531401</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Agrawal, A., Xu, Y., and Raskar, R. 2009. Invertible Motion Blur in Video. <i>ACM Trans. Graph. (Siggraph) 28</i>, 3, 95.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Agrawal, A., Gupta, M., Veeraraghavan, A., and Narasimhan, S. 2010. Optimal Coded Sampling for Temporal Super-Resolution. In <i>Proc. IEEE CVPR</i>, 374--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Agrawal, A., Veeraraghavan, A., and Raskar, R. 2010. Reinterpretable Imager: Towards Variable Post-Capture Space, Angle and Time Resolution in Photography. In <i>Proc. Eurographics</i>, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Allen, T., 2010. Time Lapse Tutorial. http://timothyallen.blogs.bbcearth.com/2009/02/24/time-lapse-photography/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320676</ref_obj_id>
				<ref_obj_pid>2319043</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Alleyson, D., S&#252;sstrunk, S., and H&#233;rault, J. 2005. Linear Demosaicing inspired by the Human Visual System. <i>IEEE Trans. Im. Proc. 14</i>, 4, 439--449.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Ashdown, I. 1993. Near-field photometry: A new approach. <i>Journal of the Illuminating Engineering Society 22</i>, 1, 163--180.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Ashok, A., and Neifeld, M. A. 2007. Pseudorandom Phase Masks for Superresolution Imaging from Subpixel Shifting. <i>Applied Optics 46</i>, 12, 2256--2268.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Ashok, A., and Neifeld, M. A. 2010. Compressive Light Field Imaging. In <i>Proc. SPIE 7690</i>, 76900Q.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409085</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Atcheson, B., Ihrke, I., Heidrich, W., Tevs, A., Bradley, D., Magnor, M., and Seidel, H. 2008. Time-resolved 3D Capture of Non-Stationary Gas Flows. <i>ACM Trans. Graph. (SIGGRAPH Asia) 27</i>, 5, 132.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1097663</ref_obj_id>
				<ref_obj_pid>1097114</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Atkinson, G., and Hancock, E. 2005. Multi-view surface reconstruction using polarization. In <i>Proc. ICCV</i>, vol. 1, 309--316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1390923</ref_obj_id>
				<ref_obj_pid>1390857</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Atkinson, G. A., and Hancock, E. R. 2008. Two-dimensional BRDF Estimation from Polarisation. <i>Comput. Vis. Image Underst. 111</i>, 2, 126--141.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1819383</ref_obj_id>
				<ref_obj_pid>1819298</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Babacan, S. D., Ansorge, R., Luessi, M., Molina, R., and Katsaggelos, A. K. 2009. Compressive Sensing of Light Fields. In <i>Proc. ICIP</i>, 2313--2316.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Baek, J. 2010. Transfer Efficiency and Depth Invariance in Computational Cameras. In <i>Proc. ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628815</ref_obj_id>
				<ref_obj_pid>628330</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Baker, S., and Kanade, T. 2002. Limits on Super-Resolution and How to Break Them. <i>IEEE Trans. PAMI 24</i>, 1167--1183.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Baker, S., Robinson, J. S., Haworth, C. A., Teng, H., Smith, R. A., Chirila, C. C., Lein, M., Tisch, J. W. G., and Marangos, J. P. 2006. Probing Proton Dynamics in Molecules on an Attosecond Time Scale. <i>Science 312</i>, 5772, 424--427.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Barone-Nugent, E. D., Barty, A., and Nugent, K. A. 2002. Quantitative Phase-Amplitude Microscopy I: Optical Microscopy. <i>Journal of Microscopy 206</i>, 3, 194--203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649034</ref_obj_id>
				<ref_obj_pid>645310</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Bascle, B., Blake, A., and Zisserman, A. 1996. Motion Deblurring and Super-resolution from an Image Sequence. In <i>Proc. ECCV</i>, 573--582.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Bayer, B. E., 1976. Color imaging array. US Patent 3,971,065.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Ben-Eliezer, E., Marom, E., Konforti, N., and Zalevsky, Z. 2005. Experimental Realization of an Imaging System with an Extended Depth of Field. <i>Appl. Opt. 44</i>, 11, 2792--2798.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1965927</ref_obj_id>
				<ref_obj_pid>1965841</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Ben-Ezra, M., and Nayar, S. 2003. Motion Deblurring using Hybrid Imaging. In <i>Proc. IEEE CVPR</i>, 657--664.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>987621</ref_obj_id>
				<ref_obj_pid>987526</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Ben-Ezra, M., and Nayar, S. 2004. Motion-based Motion Deblurring. <i>IEEE Trans. PAMI 26</i>, 6, 689--698.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1058379</ref_obj_id>
				<ref_obj_pid>1058227</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Ben-Ezra, M., Zomet, A., and Nayar, S. 2005. Video Superresolution using Controlled Subpixel Detector Shifts. <i>IEEE Trans. PAMI 27</i>, 6, 977--987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Ben-Ezra, M. 2010. High Resolution Large Format Tile-Scan Camera. In <i>Proc. IEEE ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2295955</ref_obj_id>
				<ref_obj_pid>2295758</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Ben-Ezra, M. 2011. A Digital Gigapixel Large-Format Tile-Scan Camera. <i>IEEE Computer Graphics and Applications 31</i>, 49--61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Bishop, T., Zanetti, S., and Favaro, P. 2009. Light-Field Superresolution. In <i>Proc. ICCP</i>, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Bodkin, A., Sheinis, A., Norton, A., Daly, J., Beaven, S., and Weinheimer, J. 2009. Snapshot Hyperspectral Imaging - the Hyperspectral Array Camera. In <i>Proc. SPIE 7334</i>, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Bolles, R. C., Baker, H. H., and Marimont, D. H. 1987. Epipolar-plane image analysis: An approach to determining structure from motion. <i>IJCV 1</i>, 1, 7--55.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>825197</ref_obj_id>
				<ref_obj_pid>824469</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Borman, S., and Stevenson, R. 1998. Super-resolution from image sequences - A review. In <i>Proc. Symposium on Circuits and Systems</i>, 374--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., Atcheson, B., Ihrke, I., and Heidrich, W. 2009. Synchronization and Rolling Shutter Compensation for Consumer Video Camera Arrays. In <i>Proc. ProCams</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Brady, D. J., and Hagen, N. 2009. Multiscale Lens Design. <i>Optics Express 17</i>, 13, 10659--10674.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Braun, M. 1992. <i>Picturing Time: The Work of Etienne-Jules Marey (1830-1904)</i>. The University of Chicago Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Bub, G., Tecza, M., Helmes, M., Lee, P., and Kohl, P. 2010. Temporal Pixel Multiplexing for Simultaneous High-Speed, High-Resolution Imaging. <i>Nature Methods 7</i>, 209--211.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Campillo, A., and Shapiro, S. 1983. Picosecond Streak Camera Fluorometry-A Review. <i>Journal of Quantum Electronics 19</i>, 4, 585--603.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2272020</ref_obj_id>
				<ref_obj_pid>2263435</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Cand&#232;s, E., Romberg, J., and Tao, T. 2006. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. <i>IEEE Trans. Information Theory 52</i>, 2, 489--509.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Cao, X., Tong, X., Dai, Q., and Lin, S. 2011. High Resolution Multispectral Video Capture with a Hybrid Camera System. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882309</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Carranza, J., Theobalt, C., Magnor, M. A., and Seidel, H.-P. 2003. Free-viewpoint video of human actors. <i>ACM Transactions on Graphics (TOG) 22</i>, 3, 569--577.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>166153</ref_obj_id>
				<ref_obj_pid>166117</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Chen, S. E., and Williams, L. 1993. View interpolation for image synthesis. In <i>Proc. ACM SIGGRAPH</i>, 279--288.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>292835</ref_obj_id>
				<ref_obj_pid>292724</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Chen, H., and Wolff, L. B. 1998. Polarization Phase-Based Method For Material Classification In Computer Vision. <i>IJCV 28</i>, 1, 73--83.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Chi, W., and George, N. 2001. Electronic Imaging using a Logarithmic Asphere. <i>Optics Letters 26</i>, 12, 875--877.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Chi, W., Chu, K., and George, N. 2006. Polarization Coded Aperture. <i>Optics Express 14</i>, 15, 6634--6642.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Collett, E. 2005. <i>Field Guide to Polarization</i>. SPIE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Cossairt, O., and Nayar, S. K. 2010. Spectral Focal Sweep: Extended Depth of Field from Chromatic Aberrations. In <i>Proc. ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778768</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Cossairt, O., Zhou, C., and Nayar, S. K. 2010. Diffusion Coded Photography for Extended Depth of Field. <i>ACM Trans. Graph. (Siggraph) 29</i>, 3, 31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Cossairt, O., Miau, D., and Nayar, S. K. 2011. Gigapixel Computational Imaging. In <i>Proc. ICCP</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[cri inc, 2009. VariSpec Liquid Crystal Tunable Filters. www.channelsystems.ca/Attachments/VariSpec/VariSpec-Brochure.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1191818</ref_obj_id>
				<ref_obj_pid>1191552</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Cula, O. G., Dana, K. J., Pai, D. K., and Wang, D. 2007. Polarization Multiplexing and Demultiplexing for Appearance-Based Modeling. <i>IEEE Trans. Pattern Anal. Mach. Intell. 29</i>, 2, 362--367.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., and Malik, J. 1997. Recovering High Dynamic Range Radiance Maps from Photographs. In <i>Proc. ACM Siggraph</i>, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344855</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Debevec, P., Hawkins, T., Tchou, C., Duiker, H.-P., Sarokin, W., and Sagar, M. 2000. Acquiring the Reflectance Field of a Human Face. In <i>Proc. ACM SIGGRAPH</i>, 145--156.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618886</ref_obj_id>
				<ref_obj_pid>616075</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 2002. Image-Based Lighting. <i>IEEE Computer Graphics and Applications</i>, 26--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Descour, M., and Dereniak, E. 1995. Computed-tomography Imaging Spectrometer: Experimental Calibration and Reconstruction Results. <i>Applied Optics 34</i>, 22, 4817--4826.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Descour, M. E., C. E. Volin, Dereniak, E., and K. J. Thome. 1997. Demonstration of a High-Speed Nonscanning Imaging Spectrometer. <i>Optics Letters 22</i>, 16, 1271--1273.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Dowski, E., and Cathey, T. 1995. Extended Depth of Field through Wave-Front Coding. <i>Applied Optics 34</i>, 11, 1859--1866.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Du, H., Tong, X., Cao, X., and Lin, S. 2009. A Prism-Based System for Multispectral Video Acquisition. In <i>Proc. IEEE ICCV</i>, 175--182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Eastman Kodak Company. PhotoCD PCD0992. http://r0k.us/graphics/kodak.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Fairchild, M. D. 2005. <i>Color Appearance Models</i>. John Wiley and Sons.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141956</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Fergus, R., Singh, B., Hertzmann, A., Roweis, S. T., and Freeman, W. T. 2006. Removing Camera Shake from a Single Photograph. <i>ACM Trans. Graph. 25</i>, 787--794.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Fergus, R., Torralba, A., and Freeman, W. T. 2006. Random Lens Imaging. Tech. Rep. MIT-CSAIL-TR-2006-058, National Bureau of Standards.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[Fletcher-Holmes, D. W., and Harvey, A. R. 2005. Real-Time Imaging with a Hyperspectral Fovea. <i>J. Opt. A: Pure Appl. Opt. 7</i>, S298--S302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[Foveon, 2010. X3 Technology. www.foveon.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Gabor, D. 1948. A new microscopic principle. <i>Nature</i>, 777--778.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Gaigalas, A. K., Wang, L., He, H.-J., and DeRose, P. 2009. Procedures for Wavelength Calibration and Spectral Response Correction of CCD Array Spectrometers. <i>Journal of Research of the National Institute of Standards and Technology 114</i>, 4, 215--228.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Gao, L., Kester, R. T., and Tkaczyk, T. S. 2009. Compact Image Slicing Spectrometer (ISS) for hyperspectral fluorescence microscopy. <i>Optics Express 17</i>, 15, 12293--12308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Garcia-Guerrero, E. E., Mendez, E. R., and Leskova, H. M. 2007. Design and Fabrication of Random Phase Diffusers for Extending the Depth of Focus. <i>Optics Express 15</i>, 3, 910--923.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Gat, N. 2000. Imaging Spectroscopy Using Tunable Filters: A Review. In <i>Proc. SPIE 4056</i>, 50--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Gehm, M. E., John, R., Brady, D. J., Willett, R. M., and Schulz, T. J. 2007. Single-Shot Compressive Spectral Imaging with a Dual-Disperser Architecture. <i>Optics Express 15</i>, 21, 14013--14027.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383927</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Georgiev, T., Zheng, C., Nayar, S., Curless, B., Salesin, D., and Intwala, C. 2006. Spatio-angular resolution trade-offs in integral photography. In <i>Proc. EGSR</i>, 263--272.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1478192</ref_obj_id>
				<ref_obj_pid>1478172</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Georgiev, T., Intwala, C., Babacan, S., and Lumsdaine, A. 2008. Unified Frequency Domain Analysis of Lightfield Cameras. In <i>Proc. ECCV</i>, 224--237.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Gershun, A. 1936. The light field. <i>Journal of Mathematics and Physics XVIII</i>, 51--151. Translated by P. Moon and G. Timoshenko.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866163</ref_obj_id>
				<ref_obj_pid>1882262</ref_obj_pid>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Chen, T., Peers, P., Wilson, C. A., and Debevec, P. 2010. Circularly polarized spherical illumination reflectometry. <i>ACM Trans. Graph. (Siggraph Asia) 27</i>, 5, 134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Goda, K., Tsia, K. K., and Jalali, B. 2009. Serial Time-Encoded Amplified Imaging for Real-Time Observation of Fast Dynamic Phenomena. <i>Nature</i>, 458, 1145--1149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Gorman, A., Fletcher-Holmes, D. W., and Harvey, A. R. 2010. Generalization of the Lyot Filter and its Application to Snapshot Spectral Imaging. <i>Optics Express 18</i>, 6, 5602--5608.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237200</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Gortler, S., Grzeszczuk, R., Szelinski, R., and Cohen, M. 1996. The Lumigraph. In <i>Proc. ACM Siggraph</i>, 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Gottesman, S. R., and Fenimore, E. E. 1989. New family of binary arrays for coded aperture imaging. <i>Applied Optics 28</i>, 20, 4344--4352.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276462</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[Green, P., Sun, W., Matusik, W., and Durand, F. 2007. Multi-Aperture Photography. In <i>Proc. ACM Siggraph</i>, 68.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Grossberg, M. D., and Nayar, S. K. 2003. High Dynamic Range from Multiple Images: Which Exposures to Combine. In <i>Proc. ICCV Workshop CPMCV</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1805966</ref_obj_id>
				<ref_obj_pid>1805964</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Grosse, M., Wetzstein, G., Grundh&#246;fer, A., and Bimber, O. 2010. Coded Aperture Projection. <i>ACM Trans. Graph. 29</i>, 22:1--22:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Gu, J., Hitomi, Y., Mitsunaga, T., and Nayar, S. K. 2010. Coded Rolling Shutter Photography: Flexible Space-Time Sampling. In <i>Proc. IEEE ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Gunturk, B., Glotzbach, J., Altunbasak, Y., Schafer, R., and Mersereau, R. 2005. Demosaicking: Color Filter Array Interpolation in Single-Chip Digital Cameras. <i>IEEE Signal Processing 22</i>, 1, 44--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1886073</ref_obj_id>
				<ref_obj_pid>1886063</ref_obj_pid>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Gupta, M., Agrawal, A., Veeraraghavan, A., and Narasimhan, S. G. 2010. Flexible Voxels for Motion-Aware Videography. In <i>Proc. ECCV</i>, 100--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Halle, M. W. 1994. Holographic stereograms as discrete imaging systems. In <i>SPIE Practical Holography</i>, 73--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Hamamatsu, 2010. Streak Systems. http://sales.hamamatsu.com/en/products/system-division/ultra-fast/streak-systems.php.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Hanrahan, P., and Ng, R. 2006. Digital Correction of Lens Aberrations in Light Field Photography. In <i>International Optical Design Conference</i>, 1--3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Harvey, A. R., Beale, J., Greenaway, A. H., Hanlon, T. J., and Williams, J. 2000. Technology Options for Imaging Spectrometry Imaging Spectrometry. In <i>Proc. SPIE 4132</i>, 13--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Harvey, A. R., Fletcher-Holmes, D. W., and Gorman, A. 2005. Spectral Imaging in a Snapshot. In <i>Proc. SPIE 5694</i>, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2094496</ref_obj_id>
				<ref_obj_pid>2094437</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Hasinoff, S. W., and Kutulakos, K. N. 2006. Confocal Stereo. In <i>Proc. ECCV</i>, 620--634.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1478243</ref_obj_id>
				<ref_obj_pid>1478237</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Hasinoff, S. W., and Kutulakos, K. N. 2008. Light-Efficient Photography. In <i>Proc. of ECCV</i>, 45--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1487512</ref_obj_id>
				<ref_obj_pid>1487450</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Hasinoff, S. W., and Kutulakos, K. N. 2009. Confocal Stereo. <i>Int. J. Comp. Vis. 81</i>, 1, 82--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Hasinoff, S. W., Kutulakos, K. N., Durand, F., and Freeman, W. T. 2009. Time-Constrained Photography. In <i>Proc. of ICCV</i>, 333--340.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Hasinoff, S. W., Durand, F., and Freeman, W. T. 2010. Noise-Optimal Capture for High Dynamic Range Photography. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[H&#228;usler, G. 1972. A Method to Increase the Depth of Focus by Two Step Image Processing. <i>Optics Communications 6</i>, 1, 38--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Hecht, E. 2002. <i>Optics, fourth edition</i>. Addison Wesley.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2321053</ref_obj_id>
				<ref_obj_pid>2319059</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[Hirakawa, K., and Parks, T. W. 2006. Joint Demosaicing and Denoising. <i>IEEE Trans. Im. Proc. 15</i>, 8, 2146--2157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Hirakawa, K., and Wolfe, P. 2007. Spatio-Spectral Color Filter Array Design for Enhanced Image Fidelity. In <i>Proc. ICIP</i>, II -- 81--II -- 84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2321660</ref_obj_id>
				<ref_obj_pid>2319088</ref_obj_pid>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Hirakawa, K., and Wolfe, P. 2008. Spatio-Spectral Color Filter Array Design for Optimal Image Recovery. <i>IEEE Trans. Im. Proc. 17</i>, 10, 1876--1890.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618505</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Hirsch, M., Lanman, D., Holtzman, H., and Raskar, R. 2009. BiDi Screen: a Thin, Depth-Sensing LCD for 3D Interaction using Light Fields. In <i>ACM Trans. Graph. (SIGGRAPH Asia)</i>, 1--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Hiura, S., Mohan, A., and Raskar, R. 2009. Krill-eye: Superposition Compound Eye for Wide-Angle Imaging via GRIN Lenses. In <i>Proc. OMNIVIS</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[Horstmeyer, R., Euliss, G., Athale, R., and Levoy, M. 2009. Flexible Multimodal Camera Using a Light Field Architecture. In <i>Proc. ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Hunicz, J., and Piernikarski, D. 2001. Investigation of Combustion in a Gasoline Engine using Spectrophotometric Methods. In <i>Proc. SPIE 4516</i>, 307--314.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Hunt, R. W. G. 1991. <i>Measuring Color, 3rd ed</i>. Fountain Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Ihrke, I., Stich, T., Gottschlich, H., Magnor, M., and Seidel, H. 2008. Fast incident light field acquisition and rendering. In <i>Proc. of WSCG</i>, 177--184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Ihrke, I., Wetzstein, G., and Heidrich, W. 2010. A Theory of Plenoptic Multiplexing. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Ihrke, I., Kutulakos, K. N., Lensch, H. P. A., Magnor, M., and Heidrich, W. 2010. Transparent and Specular Object Reconstruction. <i>Computer Graphics Forum 29</i>, 8, 2400--2426.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Inderhees, J., 1973. Optical field curvature corrector. US patent 3,720,454.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Itatani, J., Qur, F., Yudin, G. L., Ivanov, M. Y., Krausz, F., and Corkum, P. B. 2002. Attosecond Streak Camera. <i>Physical Review Letters 88</i>, 17, 173903.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Ives, H., 1903. Parallax Stereogram and Process of Making Same. US patent 725,567.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>113</ref_seq_no>
				<ref_text><![CDATA[Ives, H. 1928. Camera for Making Parallax Panoramagrams. <i>J. Opt. Soc. Amer. 17</i>, 435--439.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882270</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>114</ref_seq_no>
				<ref_text><![CDATA[Kang, S. B., Uyttendaele, M., Winder, S., and Szeliski, R. 2003. High Dynamic Range Video. In <i>Proc. ACM Siggraph</i>, 319--325.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>115</ref_seq_no>
				<ref_text><![CDATA[Kanolt, C. W., 1918. Parallax Panoramagrams. US patent 1,260,682.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>116</ref_seq_no>
				<ref_text><![CDATA[Kapany, N. S., and Hopkins, R. E. 1957. Fiber Optics. Part III. Field Flatteners. <i>JOSA 47</i>, 7, 594--595.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>117</ref_seq_no>
				<ref_text><![CDATA[Kindzelskii, A. L., Yang, Z. Y., Nabel, G. J., Todd, R. F., and Petty, H. R. 2000. Ebola Virus Secretory Glycoprotein (sGP) Diminishes Fc Gamma RIIIB-to-CR3 Proximity on Neutrophils. <i>J. Immunol. 164</i>, 953--958.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>118</ref_seq_no>
				<ref_text><![CDATA[Kirmani, A., Hutchison, T., Davis, J., and Raskar, R. 2009. Looking Around the corner using Transient Imaging. In <i>Proc. ICCV</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>119</ref_seq_no>
				<ref_text><![CDATA[Kolb, A., Barth, E., Koch, R., and Larsen, R. 2010. Time-of-Flight Cameras in Computer Graphics. <i>Computer Graphics Forum 29</i>, 1, 141--159.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276494</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>120</ref_seq_no>
				<ref_text><![CDATA[Kopf, J., Uyttendaele, M., Deussen, O., and Cohen, M. F. 2007. Capturing and Viewing Gigapixel Images. <i>ACM Trans. on Graph. (SIGGRAPH) 26</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>121</ref_seq_no>
				<ref_text><![CDATA[Kutulakos, K. N., and Hasinoff, S. W. 2009. Focal Stack Photography: High-Performance Photography with a Conventional Camera. In <i>Proc. IAPR Conference on Machine Vision Applications</i>, 332--337.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>872496</ref_obj_id>
				<ref_obj_pid>872022</ref_obj_pid>
				<ref_seq_no>122</ref_seq_no>
				<ref_text><![CDATA[Landolt, O., Mitros, A., and Koch, C. 2001. Visual Sensor with Resolution Enhancement by Mechanical Vibrations. In <i>Proc. Advanced Research in VLSI</i>, 249--264.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>123</ref_seq_no>
				<ref_text><![CDATA[Lange, R., and Seitz, P. 2001. Solid-State Time-of-Flight Range Camera. <i>Journal of Quantum Electronics 37</i>, 3, 390--397.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1249384</ref_obj_id>
				<ref_obj_pid>1249243</ref_obj_pid>
				<ref_seq_no>124</ref_seq_no>
				<ref_text><![CDATA[Lanman, D., Wachs, M., Taubin, G., and Cukierman, F. 2006. Spherical Catadioptric Arrays: Construction, Multi-View Geometry, and Calibration. In <i>Proc. 3DPVT</i>, 81--88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409084</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>125</ref_seq_no>
				<ref_text><![CDATA[Lanman, D., Raskar, R., Agrawal, A., and Taubin, G. 2008. Shield Fields: Modeling and Capturing 3D Occluders. <i>ACM Trans. Graph. (Siggraph Asia) 27</i>, 5, 131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866164</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>126</ref_seq_no>
				<ref_text><![CDATA[Lanman, D., Hirsch, M., Kim, Y., and Raskar, R. 2010. Content-Adaptive Parallax Barriers: Optimizing Dual-Layer 3D Displays using Low-Rank Light Field Factorization. <i>ACM Trans. Graph. (Siggraph Asia) 28</i>, 5, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2338028</ref_obj_id>
				<ref_seq_no>127</ref_seq_no>
				<ref_text><![CDATA[Lanman, D. 2010. <i>Mask-based Light Field Capture and Display</i>. PhD thesis, Brown University, School of Engineering.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1752973</ref_obj_id>
				<ref_obj_pid>1752966</ref_obj_pid>
				<ref_seq_no>128</ref_seq_no>
				<ref_text><![CDATA[Lau, D. L., and Yang, R. 2005. Real-Time Multispectral Color Video Synthesis using an Array of Commodity Cameras. <i>Real-Time Imaging 11</i>, 2, 109--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>129</ref_seq_no>
				<ref_text><![CDATA[Lawlor, J., Fletcher-Holmes, D. W., Harvey, A. R., and McNaught, A. I. 2002. In Vivo Hyperspectral Imaging of Human Retina and Optic Disc. <i>Invest. Ophthalmol. Vis. Sci. 43</i>, 4350.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>130</ref_seq_no>
				<ref_text><![CDATA[Levin, A., and Durand, F. 2010. Linear View Synthesis Using a Dimensionality Gap Light Field Prior. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276464</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>131</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Fergus, R., Durand, F., and Freeman, W. 2007. Image and Depth from a Conventional Camera with a Coded Aperture. <i>ACM Trans. Graph. (Siggraph) 26</i>, 3, 70.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>132</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Fergus, R., Durand, F., and Freeman, W. T., 2007. Deconvolution using Natural Image Priors. groups.csail.mit.edu/graphics/CodedAperture/SparseDeconv-LevinEtA107.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360670</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>133</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Sand, P., Cho, T. S., Durand, F., and Freeman, W. T. 2008. Motion-invariant photography. <i>ACM Trans. Graph. (Siggraph) 27</i>, 3, 71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531403</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>134</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Hasinoff, S. W., Green, P., Durand, F., and Freeman, W. T. 2009. 4D Frequency Analysis of Computational Cameras for Depth of Field Extension. <i>ACM Trans. Graph. (Siggraph) 28</i>, 3, 97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237199</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>135</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., and Hanrahan, P. 1996. Light Field Rendering. In <i>Proc. ACM Siggraph</i>, 31--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015806</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>136</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., Chen, B., Vaish, V., Horowitz, M., McDowall, I., and Bolas, M. 2004. Synthetic Aperture Confocal Imaging. <i>ACM Trans. Graph. (SIGGRAPH) 23</i>, 3, 825--834.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141976</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>137</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., Ng, R., Adams, A., Footer, M., and Horowitz, M. 2006. Light Field Microscopy. <i>ACM Trans. Graph. (Siggraph) 25</i>, 3, 924--934.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>138</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., 2010. Computational Photography and the Stanford Frankencamera. Technical Talk. www.graphics.stanford.edu/talks/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>139</ref_seq_no>
				<ref_text><![CDATA[Li, F., Yu, J., and Chai, J. 2008. A Hybrid Camera for Motion Deblurring and Depth Map Super-Resolution. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>140</ref_seq_no>
				<ref_text><![CDATA[Li, X., Gunturk, B., and Zhang, L. 2008. Image Demosaicing: a Systematic Survey. In <i>SPIE Conf. on Visual Comm. and Image Proc</i>., 68221J--68221J--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360654</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>141</ref_seq_no>
				<ref_text><![CDATA[Liang, C.-K., Lin, T.-H., Wong, B.-Y., Liu, C., and Chen, H. H. 2008. Programmable Aperture Photography: Multiplexed Light Field Acquisition. <i>ACM Trans. Graph. (Siggraph) 27</i>, 3, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>142</ref_seq_no>
				<ref_text><![CDATA[Lippmann, G. 1908. La Photographie Int&#233;grale. <i>Academie des Sciences 146</i>, 446--451.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>143</ref_seq_no>
				<ref_text><![CDATA[Liu, C., and Sun, D. 2011. A Bayesian Approach to Adaptive Video Super Resolution. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>144</ref_seq_no>
				<ref_text><![CDATA[Lu, Y. M., and Vetterli, M. 2009. Optimal Color Filter Array Design: Quantitative Conditions and an Efficient Search Procedure. In <i>Proc. SPIE 7250</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>145</ref_seq_no>
				<ref_text><![CDATA[Lucy, L. B. 1974. An iterative technique for the rectification of observed distributions. <i>The Astronomical Journal 79</i>, 745--754.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>146</ref_seq_no>
				<ref_text><![CDATA[Lumsdaine, A., and Georgiev, T. 2009. The Focused Plenoptic Camera. In <i>Proc. ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>147</ref_seq_no>
				<ref_text><![CDATA[Lyot, B. 1944. Le Filtre Monochromatique Polarisant et ses Applications en Physique Solaire. <i>Annales d'Astrophysique 7</i>, 31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383873</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>148</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Hawkins, T., Peers, P., Chabert, C.-F., Weiss, M., and Debevec, P. 2007. Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination. In <i>Proc. EGSR</i>, 183--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>149</ref_seq_no>
				<ref_text><![CDATA[Mann, S., and Picard, R. W. 1995. Being 'Undigital' with Digital Cameras: Extending Dynamic Range by Combining Differently Exposed Pictures. In <i>Proc. IS&T</i>, 442--448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>150</ref_seq_no>
				<ref_text><![CDATA[Mansfield, C. L., 2005. Seeing into the past.' www.nasa.gov/vision/earth/technologies/scrolls.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>151</ref_seq_no>
				<ref_text><![CDATA[Mantiuk, R., Daly, S., Myszkowski, K., and Seidel, H.-P. 2005. Predicting Visible Differences in High Dynamic Range Images - Model and its Calibration. In <i>Electronic Imaging</i>, B. E. Rogowitz, T. N. Pappas, and S. J. Daly, Eds., vol. 5666, 204--214.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964935</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>152</ref_seq_no>
				<ref_text><![CDATA[Mantiuk, R., Kim, K. J., Rempel, A., and Heidrich, W. 2011. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. <i>ACM Trans. Graph. (Siggraph) 30</i>, 3, 1--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>153</ref_seq_no>
				<ref_text><![CDATA[Marshall, J., and Oberwinkler, J. 1999. Ultraviolet Vision: the Colourful World of the Mantis Shrimp. <i>Nature 401</i>, 6756, 873--874.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>154</ref_seq_no>
				<ref_text><![CDATA[Mathews, S. A. 2008. Design and Fabrication of a low-cost, Multispectral Imaging System. <i>Applied Optics 47</i>, 28, 71--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>155</ref_seq_no>
				<ref_text><![CDATA[M&#228;thger, L. M., Shashar, N., and Hanlon, R. T. 2009. Do Cephalopods Communicate using Polarized Light Reflections from their Skin? <i>Journal of Experimental Biology 212</i>, 2133--2140.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015805</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>156</ref_seq_no>
				<ref_text><![CDATA[Matusik, W., and Pfister, H. 2004. 3d tv: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes. <i>ACM Transactions on Graphics 23</i>, 814--824.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344951</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>157</ref_seq_no>
				<ref_text><![CDATA[Matusik, W., Buehler, C., Raskar, R., Gortler, S. J., and McMillan, L. 2000. Image-based visual hulls. In <i>ACM SIGGRAPH</i>, 369--374.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>158</ref_seq_no>
				<ref_text><![CDATA[Maxwell, J. C. 1860. On the Theory of Compound Colours, and the Relations of the Colours of the Spectrum. <i>Phil. Trans. R. Soc. Lond. 150</i>, 57--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073231</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>159</ref_seq_no>
				<ref_text><![CDATA[McGuire, M., Matusik, W., Pfister, H., Hughes, J. F., and Durand, F. 2005. Defocus Video Matting. <i>ACM Trans. Graph. (SIGGRAPH) 24</i>, 3, 567--576.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1251619</ref_obj_id>
				<ref_obj_pid>1251554</ref_obj_pid>
				<ref_seq_no>160</ref_seq_no>
				<ref_text><![CDATA[McGuire, M., Matusik, W., Pfister, H., Chen, B., Hughes, J. F., and Nayar, S. K. 2007. Optical Splitting Trees for High-Precision Monocular Imaging. <i>IEEE Comput. Graph. &amp; Appl. 27</i>, 2, 32--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>161</ref_seq_no>
				<ref_text><![CDATA[Mitsunaga, T., and Nayar, S. K. 1999. Radiometric Self Calibration. In <i>Proc. IEEE CVPR</i>, 374--380.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946716</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>162</ref_seq_no>
				<ref_text><![CDATA[Miyazaki, D., Tan, R. T., Hara, K., and Ikeuchi, K. 2003. Polarization-based inverse rendering from a single view. In <i>Proc. ICCV</i>, 982--998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>951924</ref_obj_id>
				<ref_obj_pid>951849</ref_obj_pid>
				<ref_seq_no>163</ref_seq_no>
				<ref_text><![CDATA[Miyazaki, D., Kagesawa, M., and Ikeuchi, K. 2004. Transparent Surface Modelling from a Pair of Polarization Images. <i>IEEE Trans. PAMI 26</i>, 1, 73--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>164</ref_seq_no>
				<ref_text><![CDATA[Mohan, A., Huang, X., Raskar, R., and Tumblin, J. 2008. Sensing Increased Image Resolution Using Aperture Masks. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>165</ref_seq_no>
				<ref_text><![CDATA[Mohan, A., Raskar, R., and Tumblin, J. 2008. Agile Spectrum Imaging: Programmable Wavelength Modulation for Cameras and Projectors. <i>Computer Graphics Forum (Eurographics) 27</i>, 2, 709--717.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>166</ref_seq_no>
				<ref_text><![CDATA[Moon, P., and Spencer, D. E. 1981. <i>The Photic Field</i>. MIT Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649044</ref_obj_id>
				<ref_obj_pid>645310</ref_obj_pid>
				<ref_seq_no>167</ref_seq_no>
				<ref_text><![CDATA[M&#252;ller, V. 1996. Elimination of Specular Surface-Reflectance Using Polarized and Unpolarized Light. In <i>Proc. IEEE ECCV</i>, 625--635.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>168</ref_seq_no>
				<ref_text><![CDATA[Murphy, D. B. 2001. <i>Fundamentals of Light Microscopy and Electronic Imaging</i>. Wiley-Liss.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>169</ref_seq_no>
				<ref_text><![CDATA[Muybridge, E. 1957. <i>Animals in Motion</i>. first ed. Dover Publications, Chapman and Hall 1899.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1478244</ref_obj_id>
				<ref_obj_pid>1478237</ref_obj_pid>
				<ref_seq_no>170</ref_seq_no>
				<ref_text><![CDATA[Nagahara, H., Kuthirummal, S., Zhou, C., and Nayar, S. 2008. Flexible Depth of Field Photography. In <i>Proc. ECCV</i>, 60--73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>171</ref_seq_no>
				<ref_text><![CDATA[Namer, E., and Schechner, Y. Y. 2005. Advanced Visibility Improvement Based on Polarization Filtered Images. In <i>Proc. SPIE 5888</i>, 36--45.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1048889</ref_obj_id>
				<ref_obj_pid>1048721</ref_obj_pid>
				<ref_seq_no>172</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S., and Nayar, S. 2005. Enhancing Resolution along Multiple Imaging Dimensions using Assorted Pixels. <i>IEEE Trans. PAMI 27</i>, 4, 518--530.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1502491</ref_obj_id>
				<ref_obj_pid>1478237</ref_obj_pid>
				<ref_seq_no>173</ref_seq_no>
				<ref_text><![CDATA[Narasimhan, S. G., Koppal, S. J., and Yamazaki, S. 2008. Temporal Dithering of Illumination for Fast Active Vision. In <i>Proc. ECCV</i>, 830--844.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>946766</ref_obj_id>
				<ref_obj_pid>946247</ref_obj_pid>
				<ref_seq_no>174</ref_seq_no>
				<ref_text><![CDATA[Nayar, S., and Branzoi, V. 2003. Adaptive Dynamic Range Imaging: Optical Control of Pixel Exposures over Space and Time. In <i>Proc. IEEE ICCV</i>, vol. 2, 1168--1175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>175</ref_seq_no>
				<ref_text><![CDATA[Nayar, S., and Mitsunaga, T. 2000. High Dynamic Range Imaging: Spatially Varying Pixel Exposures. In <i>Proc. IEEE CVPR</i>, vol. 1, 472--479.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628640</ref_obj_id>
				<ref_obj_pid>628315</ref_obj_pid>
				<ref_seq_no>176</ref_seq_no>
				<ref_text><![CDATA[Nayar, S. K., and Nakagawa, Y. 1994. Shape from Focus. <i>IEEE Trans. PAMI 16</i>, 8, 824--831.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>177</ref_seq_no>
				<ref_text><![CDATA[Nayar, S., Fang, X.-S., and Boult, T. 1993. Removal of Specularities using Color and Polarization. In <i>Proc. IEEE CVPR</i>, 583--590.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>178</ref_seq_no>
				<ref_text><![CDATA[Nayar, S., Branzoi, V., and Boult, T. 2004. Programmable Imaging using a Digital Micromirror Array. In <i>Proc. IEEE CVPR</i>, vol. I, 436--443.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1138216</ref_obj_id>
				<ref_obj_pid>1138215</ref_obj_pid>
				<ref_seq_no>179</ref_seq_no>
				<ref_text><![CDATA[Nayar, S. K., Branzoi, V., and Boult, T. E. 2006. Programmable Imaging: Towards a Flexible Camera. <i>IJCV 70</i>, 1, 7--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>180</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Br&#233;dif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light field photography with a hand-held plenoptic camera. Tech. Rep. Computer Science CSTR 2005-02, Stanford University.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073256</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>181</ref_seq_no>
				<ref_text><![CDATA[Ng, R. 2005. Fourier Slice Photography. <i>ACM Trans. Graph. (Siggraph) 24</i>, 3, 735--744.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383866</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>182</ref_seq_no>
				<ref_text><![CDATA[Nomura, Y., Zhang, L., and Nayar, S. 2007. Scene Collages and Flexible Camera Arrays. In <i>Proc. EGSR</i>, 1--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>183</ref_seq_no>
				<ref_text><![CDATA[Ogata, S., Ishida, J., and Sasano, T. 1994. Optical Sensor Array in an Artificial Compound Eye. <i>Optical Engineering 33</i>, 11, 3649--3655.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>184</ref_seq_no>
				<ref_text><![CDATA[Ojeda-Castaneda, J., Landgrave, J. E. A., and Escamilla, H. M. 2005. Annular Phase-Only Mask for High Focal Depth. <i>Optics Letters 30</i>, 13, 1647--1649.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>185</ref_seq_no>
				<ref_text><![CDATA[Okamoto, T., and Yamaguchi, I. 1991. Simultaneous Acquisition of Spectral Image Information. <i>Optics Letters 16</i>, 16, 1277--1279.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>186</ref_seq_no>
				<ref_text><![CDATA[Okano, F., Arai, J., Hoshino, H., and Yuyama, I. 1999. Three-Dimensional Video System Based on Integral Photography. <i>Optical Engineering 38</i>, 6, 1072--1077.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>187</ref_seq_no>
				<ref_text><![CDATA[Optec, 2011. Separation prism technical data, Jan. www.alt-vision.com/color_prisms_tech_data.htm.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>188</ref_seq_no>
				<ref_text><![CDATA[Panavision, 2010. Genesis. www.panavision.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>189</ref_seq_no>
				<ref_text><![CDATA[Pandharkar, R., Kirmani, A., and Raskar, R. 2010. Lens Aberration Correction Using Locally Optimal Mask Based Low Cost Light Field Cameras. In <i>Proc. OSA Imaging Systems</i>, 1--3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2191940</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>190</ref_seq_no>
				<ref_text><![CDATA[Pandharkar, R., Velten, A., Bardagjy, A., Raskar, R., Bawendi, M., Kirmani, A., and Lawson, E. 2011. Estimating Motion and Size of Moving Non-Line-of-Sight Objects in Cluttered Environments. In <i>Proc. ICCC CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>191</ref_seq_no>
				<ref_text><![CDATA[Park, J.-I., Lee, M.-H., Grossberg, M. D., and Nayar, S. K. 2007. Multispectral Imaging Using Multiplexed Illumination. In <i>Proc. IEEE ICCV</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>192</ref_seq_no>
				<ref_text><![CDATA[Parmar, M., and Reeves, S. J. 2006. Selection of Optimal Spectral Sensitivity Functions for Color Filter Arrays. In <i>Proc. of ICIP</i>, 1005--1008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1922184</ref_obj_id>
				<ref_obj_pid>1922174</ref_obj_pid>
				<ref_seq_no>193</ref_seq_no>
				<ref_text><![CDATA[Parmar, M., and Reeves, S. J. 2010. Selection of Optimal Spectral Sensitivity Functions for Color Filter Arrays. <i>IEEE Trans. Im. Proc. 19</i>, 12 (Dec), 3190--3203.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>194</ref_seq_no>
				<ref_text><![CDATA[Photron, 2010. FASTCAM SA5. www.photron.com/datasheet/FASTCAM_SA5.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>195</ref_seq_no>
				<ref_text><![CDATA[Pieper, R. J., and Korpel, A. 1983. Image Processing for Extended Depth of Field. <i>Applied Optics 22</i>, 10, 1449--1453.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>196</ref_seq_no>
				<ref_text><![CDATA[Pixim, 2010. Digital Pixel System. www.pixim.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>197</ref_seq_no>
				<ref_text><![CDATA[Project, E. D. C., 2009. Harold 'Doc' Edgerton. www.edgerton-digital-collections.org/techniques/high-speed-photography.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>198</ref_seq_no>
				<ref_text><![CDATA[Prokudin-Gorskii, S. M., 1912. The Prokudin-Gorskii Photographic Records Recreated. www.loc.gov/exhibits/empire/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>199</ref_seq_no>
				<ref_text><![CDATA[Ramanath, R., Snyder, W., Bilbro, G., and Sander, W. 2002. Demosaicking Methods for Bayer Color Arrays. <i>Journal of Electronic Imaging 11</i>, 3, 306--315.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1593098</ref_obj_id>
				<ref_seq_no>200</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., and Tumblin, J. 2009. <i>Computational Photography: Mastering New Techniques for Lenses, Lighting, and Sensors</i>. A. K. Peters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141957</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>201</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., Agrawal, A., and Tumblin, J. 2006. Coded Exposure Photography: Motion Deblurring using Fluttered Shutter. <i>ACM Trans. Graph. (Siggraph) 25</i>, 3, 795--804.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360655</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>202</ref_seq_no>
				<ref_text><![CDATA[Raskar, R., Agrawal, A., Wilson, C. A., and Veeraraghavan, A. 2008. Glare Aware Photography: 4D Ray Sampling for Reducing Glare Effects of Camera Lenses. <i>ACM Trans. Graph. (Siggraph) 27</i>, 3, 56.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>203</ref_seq_no>
				<ref_text><![CDATA[Ray, S. F. 2002. <i>High Speed Photography and Photonics</i>. SPIE Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2191893</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>204</ref_seq_no>
				<ref_text><![CDATA[Reddy, D., Veeraraghavan, A., and Chellappa, R. 2011. P2C2: Programmable Pixel Compressive Camera for High Speed Imaging. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>205</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Khan, E. A., Aky&#252;z, A. O., and Johnson, G. M. 2008. <i>Color Imaging</i>. A K Peters Ltd.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>206</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Ward, G., Debevec, P., Pattanaik, S., Heidrich, W., and Myszkowski, K. 2010. <i>High Dynamic Range Imaging: Acquisition, Display and Image-Based Lighting</i>. Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>207</ref_seq_no>
				<ref_text><![CDATA[Research, V., 2010. Phantom Flex. www.visionresearch.com/Products/High-Speed-Cameras/Phantom-Flex/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>208</ref_seq_no>
				<ref_text><![CDATA[Ri, S., Fujigaki, M., Matui, T., and Morimoto, Y. 2006. Accurate pixel-to-pixel correspondence adjustment in a digital micromirror device camera by using the phase-shifting moir&#233; method. <i>Applied optics 45</i>, 27, 6940--6946.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>209</ref_seq_no>
				<ref_text><![CDATA[Richardson, H. W. 1972. Bayesian-Based Iterative Method of Image Restoration. <i>JOSA 62</i>, 1, 55--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>210</ref_seq_no>
				<ref_text><![CDATA[Robertson, M. A., Borman, S., and Stevenson, R. L. 1999. Estimation-Theoretic Approach to Dynamic Range Enhancement Using Multiple Exposures. <i>Journal of Electronic Imaging 12</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>211</ref_seq_no>
				<ref_text><![CDATA[Rorslett, B., 2008. Uv flower photographs. www.naturfotograf.com/index2.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2192149</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>212</ref_seq_no>
				<ref_text><![CDATA[Rouf, M., Mantiuk, R., Heidrich, W., Trentacoste, M., and Lau, C. 2011. Glare Encoding of High Dynamic Range Images. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>213</ref_seq_no>
				<ref_text><![CDATA[Sadjadi, F. 2007. Extraction of Surface Normal and Index of Refraction using a Pair of Passive Infrared Polarimetric Sensors. In <i>Proc. IEEE CVPR</i>, 1--5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964960</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>214</ref_seq_no>
				<ref_text><![CDATA[Sajadi, B., Majumder, A., Hiwada, K., Maki, A., and Raskar, R. 2011. Switchable Primaries Using Shiftable Layers of Color Filter Arrays. <i>ACM Trans. Graph. (Siggraph) 30</i>, 3, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>215</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y. Y., and Karpel, N. 2004. Clear Underwater Vision. In <i>Proc. IEEE CVPR</i>, 536--543.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>216</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y. Y., and Karpel, N. 2005. Recovery of Underwater Visibility and Structure by Polarization Analysis. <i>IEEE Journal of Oceanic Engineering 30</i>, 3, 570--587.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>217</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Nayar, S. 2001. Generalized Mosaicing. In <i>Proc. IEEE ICCV</i>, vol. 1, 17--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628831</ref_obj_id>
				<ref_obj_pid>628331</ref_obj_pid>
				<ref_seq_no>218</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Nayar, S. 2002. Generalized Mosaicing: Wide Field of View Multispectral Imaging. <i>IEEE Trans. PAMI 24</i>, 10, 1334--1348.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>219</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Nayar, S. K. 2003. Polarization Mosaicking: High dynamic Range and Polarization Imaging in a Wide Field of View. In <i>Proc. SPIE 5158</i>, 93--102.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>644673</ref_obj_id>
				<ref_obj_pid>644655</ref_obj_pid>
				<ref_seq_no>220</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Nayar, S. 2003. Generalized Mosaicing: High Dynamic Range in a Wide Field of View. <i>IJCV 53</i>, 3, 245--267.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1896328</ref_obj_id>
				<ref_obj_pid>1896300</ref_obj_pid>
				<ref_seq_no>221</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Nayar, S. 2004. Uncontrolled Modulation Imaging. In <i>Proc. IEEE CVPR</i>, 197--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1048897</ref_obj_id>
				<ref_obj_pid>1048721</ref_obj_pid>
				<ref_seq_no>222</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Nayar, S. 2005. Generalized Mosaicing: Polarization Panorama. <i>IEEE Trans. PAMI 27</i>, 4, 631--636.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>851573</ref_obj_id>
				<ref_obj_pid>850924</ref_obj_pid>
				<ref_seq_no>223</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., Shamir, J., and Kiryati, N. 1999. Polarization-Based Decorrelation of Transparent Layers: The Inclination Angle of an Invisible Surface. In <i>Proc. ICCV</i>, 814--819.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>224</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., Narasimhan, S. G., and Nayar, S. K. 2001. Instant Dehazing of Images using Polarization. In <i>Proc. IEEE CVPR</i>, 325--332.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>225</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., Narasimhan, S. G., and Nayar, S. K. 2003. Polarization-Based Vision through Haze. <i>Applied Optics 42</i>, 3, 511--525.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272140</ref_obj_id>
				<ref_obj_pid>1271931</ref_obj_pid>
				<ref_seq_no>226</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., Nayar, S., and Belhumeur, P. 2007. Multiplexing for Optimal Lighting. <i>IEEE Trans. PAMI 29</i>, 8, 1339--1354.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>227</ref_seq_no>
				<ref_text><![CDATA[Sch&#246;nfelder, T., 2003. Polarization Division Multiplexing in Optical Data Transmission Systems. US Patent 6,580,535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>228</ref_seq_no>
				<ref_text><![CDATA[Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., and Vorozcovs, A. 2004. High Dynamic Range Display Systems. <i>ACM Trans. on Graph. (SIGGRAPH 2004) 23</i>, 3, 760--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>229</ref_seq_no>
				<ref_text><![CDATA[Semiconductor, C., 2010. LUPA Image Sensors. www.cypress.com/?id=206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>230</ref_seq_no>
				<ref_text><![CDATA[Settles, G. 2001. <i>Schlieren &amp; Shadowgraph Techniques</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2191831</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>231</ref_seq_no>
				<ref_text><![CDATA[Shahar, O., Faktor, A., and Irani, M. 2011. Space-Time Super-Resolution from a Single Video. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649159</ref_obj_id>
				<ref_obj_pid>645315</ref_obj_pid>
				<ref_seq_no>232</ref_seq_no>
				<ref_text><![CDATA[Shechtman, E., Caspi, Y., and Irani, M. 2002. Increasing Space-Time Resolution in Video. In <i>Proc. ECCV</i>, 753--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1048890</ref_obj_id>
				<ref_obj_pid>1048721</ref_obj_pid>
				<ref_seq_no>233</ref_seq_no>
				<ref_text><![CDATA[Shechtman, E., Caspi, Y., and Irani, M. 2005. Space-Time Super-Resolution. <i>IEEE Trans. PAMI 27</i>, 4, 531--545.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>234</ref_seq_no>
				<ref_text><![CDATA[Shimadzu, 2010. HyperVision HPV-2. www.shimadzu.com/products/test/hsvc/oh80jt0000001d6t.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>235</ref_seq_no>
				<ref_text><![CDATA[Siddiqui, A. S., and Zhou, J. 1991. Two-Channel Optical Fiber Transmission Using Polarization Division Multiplexing. <i>Journal of Optical Communications 12</i>, 2, 47--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>236</ref_seq_no>
				<ref_text><![CDATA[Smith, B. M., Zhang, L., Jin, H., and Agarwala, A. 2009. Light Field Video Stabilization. In <i>Proc. of ICCV</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>237</ref_seq_no>
				<ref_text><![CDATA[SpheronVR, 2010. SpheroCam HDR. www.spheron.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1390927</ref_obj_id>
				<ref_obj_pid>1390857</ref_obj_pid>
				<ref_seq_no>238</ref_seq_no>
				<ref_text><![CDATA[Starck, J., and Hilton, A. 2008. Model-based human shape reconstruction from multiple views. <i>Computer Vision and Image Understanding (CVIU) 111</i>, 2, 179--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>239</ref_seq_no>
				<ref_text><![CDATA[Taguchi, Y., Agrawal, A., Ramalingam, S., and Veeraraghavan, A. 2010. Axial Light Fields for Curved Mirrors: Reflect Your Perspective, Widen Your View. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866194</ref_obj_id>
				<ref_obj_pid>1882261</ref_obj_pid>
				<ref_seq_no>240</ref_seq_no>
				<ref_text><![CDATA[Taguchi, Y., Agrawal, A., Veeraraghavan, A., Ramalingam, S., and Raskar, R. 2010. Axial-Cones: Modeling Spherical Catadioptric Cameras for Wide-Angle Light Field Rendering. <i>ACM Trans. Graph. 29</i>, 172:1--172:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1804160</ref_obj_id>
				<ref_obj_pid>1803943</ref_obj_pid>
				<ref_seq_no>241</ref_seq_no>
				<ref_text><![CDATA[Tai, Y., Hao, D., Brown, M. S., and Lin, S. 2010. Correction of Spatially Varying Image and Video Motion Blur using a Hybrid Camera. <i>IEEE Trans. PAMI 32</i>, 6, 1012--1028.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276424</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>242</ref_seq_no>
				<ref_text><![CDATA[Talvala, E.-V., Adams, A., Horowitz, M., and Levoy, M. 2007. Veiling Glare in High Dynamic Range Imaging. <i>ACM Trans. Graph. (Siggraph) 26</i>, 3, 37.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>243</ref_seq_no>
				<ref_text><![CDATA[Tanida, J., Kumagai, T., Yamada, K., Miyatake, S., Ishida, K., Morimoto, T., Kondou, N., Miyazaki, D., and Ichioka, Y. 2001. Thin Observation Module by Bound Optics (TOMBO): Concept and Experimental Verification. <i>Applied Optics 40</i>, 11, 1806--1813.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>244</ref_seq_no>
				<ref_text><![CDATA[Tanida, J., Shogenji, R., Kitamura, Y., Yamada, K., Miyamoto, M., and Miyatake, S. 2003. Color Imaging with an Integrated Compound Imaging System. <i>Optics Express 11</i>, 18, 2109--2117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>245</ref_seq_no>
				<ref_text><![CDATA[Telleen, J., Sullivan, A., Yee, J., Wang, O., Gunawardane, P., Collins, I., and Davis, J. 2007. Synthetic Shutter Speed Imaging. <i>Computer Graphics Forum (Eurographics) 26</i>, 3, 591--598.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>246</ref_seq_no>
				<ref_text><![CDATA[Toyooka, S., and Hayasaka, N. 1997. Two-Dimensional Spectral Analysis using Broad-Band Filters. <i>Optical Communications 137</i> (Apr), 22--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1068902</ref_obj_id>
				<ref_obj_pid>1068507</ref_obj_pid>
				<ref_seq_no>247</ref_seq_no>
				<ref_text><![CDATA[Tumblin, J., Agrawal, A., and Raskar, R. 2005. Why I want a Gradient Camera. In <i>Proc. IEEE CVPR</i>, 103--110.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>248</ref_seq_no>
				<ref_text><![CDATA[Tyson, R. K. 1991. <i>Principles of Adaptive Optics</i>. Academic Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>249</ref_seq_no>
				<ref_text><![CDATA[Ueda, K., Koike, T., Takahashi, K., and Naemura, T. 2008. Adaptive Integral Photography Imaging with Variable-Focus Lens Array. In <i>Proc SPIE: Stereoscopic Displays and Applications XIX</i>, 68031A--9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401643</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>250</ref_seq_no>
				<ref_text><![CDATA[Ueda, K., Lee, D., Koike, T., Takahashi, K., and Naemura, T., 2008. Multi-Focal Compound Eye: Liquid Lens Array for Computational Photography. ACM SIGGRAPH New Tech Demo.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>977382</ref_obj_id>
				<ref_obj_pid>977249</ref_obj_pid>
				<ref_seq_no>251</ref_seq_no>
				<ref_text><![CDATA[Umeyama, S., and Godin, G. 2004. Separation of Diffuse and Specular Components of Surface Reflection by Use of Polarization and Statistical Analysis of Images. <i>IEEE Trans. PAMI 26</i>, 5, 639--647.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882425</ref_obj_id>
				<ref_obj_pid>882404</ref_obj_pid>
				<ref_seq_no>252</ref_seq_no>
				<ref_text><![CDATA[Unger, J., Wenger, A., Hawkins, T., Gardner, A., and Debevec, P. 2003. Capturing and Rendering with Incident Light Fields. In <i>Proc. EGSR</i>, 141--149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>253</ref_seq_no>
				<ref_text><![CDATA[Vagni, F. 2007. Survey of Hyperspectral and Multispectral Imaging Technologies. Tech. Rep. TR-SET-065-P3, NATO Research and Technology.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153597</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>254</ref_seq_no>
				<ref_text><![CDATA[Vaish, V., Szeliski, R., Zitnick, C. L., Kang, S. B., and Levoy, M. 2006. Reconstructing Occluded Surfaces using Synthetic Apertures: Stereo, Focus and Robust Measures. In <i>Proc. IEEE CVPR</i>, 23--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>255</ref_seq_no>
				<ref_text><![CDATA[Valley, G., 2010. Viper FilmStream Camera. www.grassvalley.com.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>256</ref_seq_no>
				<ref_text><![CDATA[van Putten, E., Akbulut, D., Bertolotti, J., Vos, W., Lagendijk, A., and Mosk, A. 2011. Scattering Lens Resolves Sub-100 nm Structures with Visible Light. <i>Physical Review Letters 106</i>, 19, 1--4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276463</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>257</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agrawal, A., Mohan, A., and Tumblin, J. 2007. Dappled Photography: Mask Enhanced Cameras for Heterodyned Light Fields and Coded Aperture Refocussing. <i>ACM Trans. Graph. (Siggraph) 26</i>, 3, 69.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>258</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Raskar, R., Agrawal, A., Chellappa, R., Mohan, A., and Tumblin, J. 2008. Non-Refractive Modulators for Encoding and Capturing Scene Appearance and Depth. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1957615</ref_obj_id>
				<ref_obj_pid>1957383</ref_obj_pid>
				<ref_seq_no>259</ref_seq_no>
				<ref_text><![CDATA[Veeraraghavan, A., Reddy, D., and Raskar, R. 2011. Coded Strobing Photography: Compressive Sensing of High Speed Periodic Videos. <i>IEEE Trans. PAMI</i>, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618520</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>260</ref_seq_no>
				<ref_text><![CDATA[Vlasic, D., Peers, P., Baran, I., Debevec, P., Popovi&#263;, J., Rusinkiewicz, S., and Matusik, W. 2009. Dynamic Shape Capture using Multi-View Photometric Stereo. In <i>ACM Trans. Graph. (SIGGRAPH Asia)</i>, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>261</ref_seq_no>
				<ref_text><![CDATA[Wagadarikar, A., Pitsianis, N., Sun, X., and Brady, D. 2008. Spectral Image Estimation for Coded Aperture Snapshot Spectral Imagers. In <i>Proc. SPIE 7076</i>, 707602.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>262</ref_seq_no>
				<ref_text><![CDATA[Wagadarikar, A., Pitsianis, N., Sun, X., and Brady, D. 2009. Video Rate Spectral Imaging using a Coded Aperture Snapshot Spectral Imager. <i>Optics Express 17</i>, 8, 6368--6388.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>263</ref_seq_no>
				<ref_text><![CDATA[Wandinger, U. 2005. <i>Lidar: Range-Resolved Optical Remote Sensing of the Atmosphere</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>264</ref_seq_no>
				<ref_text><![CDATA[Wang, S., and Heidrich, W. 2004. The Design of an Inexpensive Very High Resolution Scan Camera System. <i>Computer Graphics Forum (Eurographics) 23</i>, 10, 441--450.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>265</ref_seq_no>
				<ref_text><![CDATA[Wehner, R. 1976. Polarized-Light Navigation by Insects. <i>Scientific American 235</i>, 106115.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>266</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Ihrke, I., and Heidrich, W. 2010. Sensor Saturation in Fourier Multiplexed Imaging. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>267</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Ihrke, I., Lanman, D., and Heidrich, W. 2011. Computational Plenoptic Imaging. <i>Computer Graphics Forum 30</i>, 23972426.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964990</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>268</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Lanman, D., Heidrich, W., and Raskar, R. 2011. Layered 3D: Tomographic Image Synthesis for Attenuation-based Light Field and High Dynamic Range Displays. <i>ACM Trans. Graph. (Siggraph)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>269</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Ihrke, I., Gukov, A., and Heidrich, W. 2011. Towards a Database of High-dimensional Plenoptic Images. In <i>Proc. ICCP (Poster)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>270</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Raskar, R., and Heidrich, W. 2011. Hand-Held Schlieren Photography with Light Field Probes. In <i>Proc. ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>271</ref_seq_no>
				<ref_text><![CDATA[Wetzstein, G., Lanman, D., Hirsch, M., and Gutierrez, D. 2012. Computational Displays. In <i>ACM SIGGRAPH 2012 Courses</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>272</ref_seq_no>
				<ref_text><![CDATA[Wilburn, B., Smulski, M., Lee, K., and Horowitz, M. A. 2002. The Light Field Video Camera. In <i>SPIE Electronic Imaging</i>, 29--36.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1896343</ref_obj_id>
				<ref_obj_pid>1896300</ref_obj_pid>
				<ref_seq_no>273</ref_seq_no>
				<ref_text><![CDATA[Wilburn, B., Joshi, N., Vaish, V., Levoy, M., and Horowitz, M. 2004. High Speed Video Using a Dense Array of Cameras. In <i>Proc. IEEE CVPR</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073259</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>274</ref_seq_no>
				<ref_text><![CDATA[Wilburn, B., Joshi, N., Vaish, V., Talvala, E.-V., Antunez, E., Barth, A., Adams, A., Horowitz, M., and Levoy, M. 2005. High Performance Imaging using Large Camera Arrays. <i>ACM Trans. Graph. (Siggraph) 24</i>, 3, 765--776.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>117756</ref_obj_id>
				<ref_obj_pid>117754</ref_obj_pid>
				<ref_seq_no>275</ref_seq_no>
				<ref_text><![CDATA[Wolff, L. B., and Boult, T. E. 1991. Constraining Object Features using a Polarization Reflectance Model. <i>IEEE Trans. PAMI 13</i>, 7, 635--657.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>276</ref_seq_no>
				<ref_text><![CDATA[Wyszecki, G., and Stiles, W. 1982. <i>Color Science</i>. John Wiley and Sons, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>277</ref_seq_no>
				<ref_text><![CDATA[Yang, J., Lee, C., Isaksen, A., and McMillan, L., 2000. A Low-Cost Portable Light Field Capture Device. ACM SIGGRAPH Technical Sketch.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581907</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>278</ref_seq_no>
				<ref_text><![CDATA[Yang, J. C., Everett, M., Buehler, C., and McMillan, L. 2002. A Real-Time Distributed Light Field Camera. In <i>Proc. EGSR</i>, 77--86.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>279</ref_seq_no>
				<ref_text><![CDATA[Yao, S., 2008. Optical Communications Based on Optical Polarization Multiplexing and Demultiplexing. US Patent 7,343,100.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1870349</ref_obj_id>
				<ref_obj_pid>1870348</ref_obj_pid>
				<ref_seq_no>280</ref_seq_no>
				<ref_text><![CDATA[Yasuma, F., Mitsunaga, T., Iso, D., and Nayar, S. K. 2010. Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum. <i>IEEE Trans. Im. Proc. 99</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>281</ref_seq_no>
				<ref_text><![CDATA[Zhang, C., and Chen, T. 2005. Light Field Capturing with Lensless Cameras. In <i>Proc. ICIP</i>, III -- 792--5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>282</ref_seq_no>
				<ref_text><![CDATA[Zhou, C., and Nayar, S. 2009. What are Good Apertures for Defocus Deblurring? In <i>Proc. ICCP</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>283</ref_seq_no>
				<ref_text><![CDATA[Zhou, C., Lin, S., and Nayar, S. K. 2009. Coded Aperture Pairs for Depth from Defocus. In <i>Proc. ICCV</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383905</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>284</ref_seq_no>
				<ref_text><![CDATA[Zwicker, M., Matusik, W., Durand, F., and Pfister, H. 2006. Antialiasing for automultiscopic 3D displays. In <i>Eurographics Symposium on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Plenoptic Imaging  SIGGRAPH 2012 Course Monday, 6 August 2012, 9:00-10:30 am Los Angeles 
Convention Center, Room 408B Gordon Wetzstein MIT Media Lab gordonw@media.mit.edu Douglas Lanman MIT 
Media Lab dlanman@media.mit.edu Kurt Akeley Lytro, Inc. kakeley@lytro.com Ivo Ihrke Saarland University 
ihrke@mpi-inf.mpg.de Wolfgang Heidrich University of British Columbia heidrich@cs.ubc.ca Ramesh Raskar 
MIT Media Lab raskar@media.mit.edu  Abstract and Motivation Spawned by the introduction of the Lytro 
light .eld camera to the con­sumer market and recent accomplishments in the speed at which light can 
be captured [Raskar et al. 2011] , a new generation of computational cam­eras is emerging. By exploiting 
the co-design of camera optics and com­putational processing, these cameras capture unprecedented details 
of the plenoptic function a ray-based model for light that includes the color spectrum as well as spatial, 
temporal, and directional variation. Although digital light sensors have greatly evolved in the last 
years, the visual infor­mation captured by conventional cameras has remained almost unchanged since the 
invention of the daguerreotype. All standard CCD and CMOS sensors integrate over the dimensions of the 
plenoptic function as they convert photons into electrons; in the process all visual information is irre­versibly 
lost, except for a two-dimensional, spatially-varying subset the common photograph. In this course, 
we review the plenoptic function and discuss approaches that aim at optically encoding high-dimensional 
visual information that is then recovered computationally in post-processing. This course is intended 
to review the state of the art in joint optical light modulation and computational reconstruction of 
visual information transcending that captured by traditional photography. In addition to the plenoptic 
dimensions, we also consider high dynamic range image acqui­sition as common sensors have a limited dynamic 
range. In contrast to prior courses on general computational photography [Raskar and Tumblin 2006,2007] 
and a recent survey on the topic [Wetzstein et al. 2011], this course gives a broad, well-structured, 
and intuitive overview of all aspects of plenoptic image acquisition and focuses on two recent developments: 
light .eld acquisition and ultra-fast cameras. We unveil the secrets behind capturing light at a trillion 
frames per second and the Lytro camera. Our course serves as a resource for interested parties by providing 
a categoriza­tion of recent research and help in the identi.cation of unexplored areas in the .eld. We 
will discuss all aspects of plenoptic image acquisition in detail. Speci.cally, we begin by giving an 
overview of the plenoptic dimensions and show how much of this visual information is irreversibly lost 
in con­ventional image acquisition. We proceed by discussing the state of the art in joint optical modulation 
and computation reconstruction for the acqui­sition of high dynamic range imagery as well as spectral 
information. Two parts, focusing on light .eld acquisition and ultra-fast optics respectively, will unveil 
the secrets behind imaging techniques that have recently been 1 featured in the news. We outline other 
aspects of light that are of inter­est for various applications and wrap the course up with a short summary, 
while leaving enough time for questions and a short discussion. Prerequisites This introductory-level 
course has no prerequisites. 2  Speaker Biographies Gordon Wetzstein MIT Media Lab gordonw@media.mit.edu 
http://web.media.mit.edu/ gordonw Gordon Wetzstein is a Postdoctoral Researcher at the MIT Media Lab. 
His research interests include light .eld and high dynamic range dis­plays, projector-camera systems, 
computational optics, computational photography, computer vision, computer graphics, and augmented reality. 
Gordon received a Diplom in Media System Science with Honors from the Bauhaus-University Weimar in 2006 
and a Ph.D. in Computer Science at the University of British Columbia in 2011. His doctoral dissertation 
focuses on computational light modulation for image acquisition and dis­play. He is co-chairing the .rst 
workshop on Computational Cameras and Displays at CVPR 2012, is serving in the general submissions committee 
at SIGGRAPH 2012, has served on the program committees of IEEE ProCams 2007 and IEEE ISMAR 2010, won 
a Laval Virtual Award in 2005 for his work on projector-camera systems, and a best paper award for Hand-Held 
Schlieren Photography with Light Field Probes at the International Conference on Computational Photography 
in 2011, introducing light .eld probes as computational displays for computer vision and .uid mechanics 
applications. Ivo Ihrke Saarland University / MPI Saarbr ¨ ucken ihrke@mpi-inf.mpg.de http://giana.mmci.uni-saarland.de/ 
 Ivo Ihrke is head of the research group Generalized Image Acquisition and Analysis within the Cluster 
of Excellence Multimodal Computing and Interaction at Saarland University and Associate Senior Researcher 
at the MPI Informatik. Prior to joining Saarland University he was a post­doctoral research fellow at 
the University of British Columbia, Vancouver, Canada, supported by the Alexander von Humboldt-Foundation. 
He received a MS degree in Scienti.c Computing from the Royal Institute of Technology (KTH), Stockholm, 
Sweden (2002) and a PhD in Computer Science from Saarland University (2007). His main research interest 
3 are the modeling of forward and inverse light transport processes and computational algorithms for 
solving these large scale problems. Douglas Lanman MIT Media Lab dlanman@media.mit.edu http://web.media.mit.edu/ 
dlanman Douglas Lanman is a Postdoctoral Associate at the MIT Media Lab. His research is focused on 
computational imaging and display systems, including light .eld capture, automultiscopic (glasses-free) 
3D displays, and active illumination for 3D reconstruction. He received a B.S. in Applied Physics with 
Honors from Caltech in 2002 and M.S. and Ph.D. degrees in Electrical Engineering from Brown University 
in 2006 and 2010, respectively. Prior to joining MIT and Brown, he was an Assistant Research Staff Member 
at MIT Lincoln Laboratory from 2002 to 2005. Douglas has worked as an intern at Intel, Los Alamos National 
Laboratory, INRIA Rh one-Alpes, Mitsubishi Electric Research Laboratories (MERL), and the MIT Media Lab. 
He presented the Build Your Own 3D Scanner course at SIGGRAPH 2009 and SIGGRAPH Asia 2009 and the Build 
Your Own 3D Display course at SIGGRAPH 2010, SIGGRAPH 2011, and SIGGRAPH Asia 2010. Douglas is co-chairing 
the .rst workshop on Computational Cameras and Displays at CVPR 2012, is serving on the technical paper 
committee for SIGGRAPH Asia 2012, and is currently writing a book on 3D Photography to be published by 
CRC Press in Fall 2012. Wolfgang Heidrich University of British Columbia heidrich@cs.ubc.ca http://www.cs.ubc.ca/ 
heidrich Professor Wolfgang Heidrich holds the Dolby Research Chair in Computer Science at the University 
of British Columbia. He received a PhD in Computer Science from the University of Erlangen in 1999, and 
then worked as a Research Associate in the Computer Graphics Group of the Max-Planck-Institute for Computer 
Science in Saarbrucken, Germany, before joining UBC in 2000. Heidrich s research interests lie at the 
inter­section of computer graphics, computer vision, imaging, and optics. In particular, he has worked 
on High Dynamic Range imaging and display, 4 image-based modeling, measuring, and rendering, geometry 
acquisition, GPU-based rendering, and global illumination. Heidrich has written over 100 refereed publications 
on these subjects and has served on numerous program committees. He was the program co-chair for Graphics 
Hardware 2002, Graphics Interface 2004, the Eurographics Symposium on Rendering 2006, and ProCams 2011. 
 Kurt Akeley Lytro, Inc. kakeley@lytro.com http://www.lytro.com/team/kurt_akeley A pioneer in the .eld 
of computer graphics and a founding member of Silicon Graphics (later known as SGI), Kurt has helped 
develop innova­tive products like SGIs RealityEngine and the OpenGL graphics system. Hes contributed 
as well to 3D display technology, NVIDIA GPUs, and the Microsoft Research lab. He is an indefatigable 
traveler and prefers to be on the move when closer to home as well, often scheduling walking meetings 
while exploring the tranquil neighborhoods near Lytros Moun­tain View of.ce. Kurt earned his Ph.D. in 
computer science from Stanford University and his B.E.E. from the University of Delaware. In 2005, he 
was awarded membership in the National Academy of Engineering. SIGGRAPH honored him with the Computer 
Graphics Achievement Award in 1995, and hes also been inducted as a Fellow of the Association for Computing 
Machinery. Ramesh Raskar MIT Media Lab raskar@media.mit.edu http://web.media.mit.edu/ raskar Ramesh 
Raskar joined the Media Lab from Mitsubishi Electric Re­search Laboratories in 2008 as head of the Labs 
Camera Culture research group. His research interests span the .elds of computational light transport, 
computational photography, inverse problems in imaging and human-computer interaction. Recent projects 
and inventions include transient imaging to look around a corner, a next generation CAT-Scan machine, 
imperceptible markers for motion capture (Prakash), long distance barcodes (Bokode), touch+hover 3D interaction 
displays (BiDi 5 screen), low-cost eye care devices (Netra,Catra), new theoretical models to augment 
light .elds (ALF) to represent wave phenomena and algebraic rank constraints for 3D displays (HR3D). 
He is a recipient of TR100 award from Technology Review, 2004, Global Indus Technovator Award, top 20 
Indian technology innovators world­wide, 2003, Alfred P. Sloan Research Fellowship award, 2009 and Darpa 
Young Faculty award, 2010. Other awards include Marr Prize honorable mention 2009, LAUNCH Health Innovation 
Award, presented by NASA, USAID, US State Dept and NIKE, 2010, Vodafone Wireless Innovation Award (.rst 
place), 2011. He holds over 40 US patents and has received four Mitsubishi Electric Invention Awards. 
He is currently co-authoring a book on Computational Photography. 6  Course Outline 10 minutes: Introduction 
and Overview Gordon Wetzstein This part will introduce the speakers, present a motivation of the course, 
and outline the individual parts. We will also introduce the plenoptic function in this part and give 
an overview over its dimen­sions along with a variety of different applications for their acquisi­tion. 
 10 minutes: High Dynamic Range Imaging Wolfgang Heidrich This part will review approaches to high dynamic 
range (HDR) im­age acquisition. The focus of this part of the course will be on tech­niques that exploit 
joint optical light modulation and computational reconstruction for HDR imaging. 10 minutes: Spectral 
Imaging Ivo Ihrke This part of the course will give a brief overview of multi-spectral and hyper-spectral 
imaging and then proceed to discuss computational photography techniques for spectral imaging in more 
detail. 10 minutes: Light Field Acquisition Douglas Lanman This part of the course reviews the general 
concepts behind light .elds and discusses a variety of light .eld acquisition approaches, ranging from 
single-shot cameras (e.g., Lytro) to more exotic multi­camera and high-dimensional multiplexing approaches. 
This section also reviews emerging applications for light .elds, including 3D dis­plays, human-computer 
interaction, and medical imaging. 20 minutes: Inside the Lytro Light Field Camera Kurt Akeley With the 
recent launch of the .rst consumer light .eld camera by Lytro, computational cameras have started to 
enter the mass mar­ket. In this part of the course, we reveal the secrets behind the Lytro camera, which 
was listed by TIME Magazine as one of the 50 best inventions of 2011. 7  20 minutes: Light Capture with 
a Trillion Frames per Second Ramesh Raskar This part of the course will cover the latest developments 
in ultra-fast image acquisition. We will show how light propagation in free space can be captured at 
one trillion frames per second. We will show how ultra-fast image acquisition in combination with advanced 
compu­tational reconstructions allows unprecedented applications such as looking around corners and measuring 
BRDFs in the wild. 5 minutes: Further Light Properties Gordon Wetzstein In this part of the course, we 
outline the acquisition of light properties that are not directly included in the plenoptic function 
but related, such as polarization and Schlieren imaging. We will show how joint optical light modulation 
and computational processing can also be useful for these applications. 5 minutes: Summary and Q &#38; 
A All This part will summarize how computational plenoptic cameras are becoming the future of imaging 
by exploiting the co-design of dis­play optics and computational processing. We will outline future di­rections 
of this emerging .eld and allow for suf.cient time to answer questions and stimulate discussions. 8 
Computational Plenoptic Imaging SIGGRAPH 2012 Course Notes Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 
Wolfgang Heidrich3 Kurt Akeley4 Ramesh Raskar1 1MIT Media Lab 2Saarland University 3University of British 
Columbia 4Lytro, Inc. The plenoptic function is a ray-based model for light that includes the color 
spectrum as well as spatial, temporal, and directional variation. Although digital light sensors have 
greatly evolved in the last years, one fundamental limitation remains: all standard CCD and CMOS sensors 
integrate over the dimensions of the plenoptic function as they convert photons into electrons; in the 
process, all visual information is irreversibly lost, except for a two­dimensional, spatially-varying 
subset the common photograph. In this part of the course notes, we provide a review of approaches that 
optically encode the dimensions of the plenoptic function transcending those captured by traditional 
photography and reconstruct the recorded information computationally. The literature survey provided 
in this section is an updated version of a survey paper recently published by some of the authors [Wetzstein 
et al. 2011a]; a course on the dual topic of computational displays is being taught at SIGGRAPH 2012 
as well [Wetzstein et al. 2012].  1 Introduction Evolution has resulted in the natural development 
of a variety of highly specialized visual systems among animals. The mantis shrimp retina, for instance, 
contains 16 different types of photoreceptors [Marshall and Oberwinkler 1999]. The extraordinary anatomy 
of their eyes not only allows the mantis shrimp to see 12 different color channels, ranging from ultra-violet 
to infra-red, and distinguish between shades of linear and circular polarization, but it also allows 
the shrimp to perceive depth using trinocular vision with each eye. Other creatures of the sea, such 
as cephalopods [M¨ athger et al. 2009], are also known to use their ability to perceive polarization 
for communication and unveiling transparency of their prey. Although the compound eyes found in .ying 
insects have a lower spatial resolution compared to mammalian single-lens eyes, their temporal resolving 
power is far superior to the human visual system. Traditionally, cameras have been designed to capture 
what a single human eye can perceive: a two-dimensional trichromatic image. Inspired by the natural diversity 
of perceptual systems and fueled by advances of digital camera technology, computational processing, 
and optical fabrication, image processing has begun to transcend limitations of .lm-based analog photography. 
Applications for the computerized acquisition of images with a high spatial, temporal, spectral, and 
directional resolution are manifold; medical imaging, remote sensing, shape reconstruction, surveillance, 
and automated fabrication are only a few examples. The plenoptic function [Adelson and Bergen 1991] provides 
a ray-based model of light encompassing most proper­ ties that are of interest for image acquisition. 
As illustrated in Figure 1, these include the color spectrum as well as spatial, temporal, and directional 
light variation. In addition to these more traditional plenoptic dimensions [Adelson and Bergen 1991], 
we also consider dynamic range a desirable property, as common sensors have a limited dynamic range. 
1.1 Computational Photography and Plenoptic Imaging What makes plenoptic imaging different than general 
computational photography? Plenoptic imaging considers a subset of computational photography approaches; 
speci.cally, those that aim at acquiring the dimensions of the plenoptic function with combined optical 
light modulation and computational reconstruction. Computational pho­ 1  Figure 1: Taxonomy and overview 
of plenoptic image acquisition approaches. tography has grown tremendously in the last years with dozens 
of published papers per year in a variety of graphics, vision, and optics venues. The dramatic rise in 
publications in this interdisciplinary .eld, spanning optics, sensor technology, image processing, and 
illumination, has made it dif.cult to encompass all research in a single survey. We provide a structured 
review of the subset of research that has recently been shown to be closely-related in terms of optical 
encoding and especially in terms of reconstruction algorithms [Ihrke et al. 2010a]. Additionally, our 
report serves as a resource for interested parties by providing a categorization of recent research and 
is intended to aid in the identi.cation of unexplored areas in the .eld. 1.2 Overview and De.nition 
of Scope In this report, we review the state of the art in joint optical light modulation and computational 
reconstruction approaches for acquiring the dimensions of the plenoptic function. Speci.cally, we discuss 
the acquisition of high dynamic range imagery (Section 2), the color spectrum (Section 3), light .elds 
and directional variation (Section 4), spatial super-resolution and focal surfaces (Section 5), as well 
as high-speed events (Section 6). We also outline the acquisition of light properties that are not directly 
included in the plenoptic function, but related, such as polarization, phase imaging, and time-of-.ight 
(Section 7) and point the reader to more comprehensive literature on these topics. Conclusions and possible 
future avenues of research are discussed in Section 8. Due to the fact that modern, digital acquisition 
approaches are often closely related to their analog predecessors, we outline these whenever applicable. 
For each of the plenoptic dimensions we also discuss practical applications of the acquired data. As 
there is an abundance of work in this .eld, we focus on imaging techniques that are designed for standard 
planar sensors. We will only highlight examples of modi.ed sensor hardware for direct capture of plenoptic 
image information. We do not cover pure image processing techniques, such as tone-reproduction, dynamic 
range compression and tone-mapping [Reinhard et al. 2010], or the reconstruction of geometry [Ihrke et 
al. 2010b], BSDFs and re.ectance .elds.  2 High Dynamic Range Imaging High dynamic range (HDR) image 
acquisition has been a very active area of research for more than a decade. With the introduction of 
the HDR display prototype [Seetzen et al. 2004] and its successor models becoming consumer products today, 
the demand for high-contrast photographic material is ever increasing. Other applications for high dynamic 
range imagery include digital photography, physically-based rendering and lighting [Debevec 2002], image 
2 editing, digital cinema, perceptual difference metrics based on absolute luminance [Mantiuk et al. 
2005; Mantiuk et al. 2011], virtual reality, and computer games. For a comprehensive overview of HDR 
imaging, including appli­ cations, radiometry, perception, data formats, tone reproduction, and display, 
the reader is referred to the textbook by Reinhard et al. [Reinhard et al. 2010]. In this section, we 
provide a detailed and up-to-date list of approaches for the acquisition of high dynamic range imagery. 
2.1 Single-Shot Acquisition According to DxOMark (www.dxomark.com), the latest high-end digital SLR cameras 
are equipped with CMOS sensors that have a measured dynamic range of up to 13.5 f-stops, which translates 
to a contrast of 11,000:1. This is comparable to that of color negative .lms [Reinhard et al. 2010]. 
In the future, we can expect digital sensors to perform equally well as negative .lm in terms of dynamic 
range, but this is not the case for most sensors today. Specialized sensors that allow high dynamic range 
content to be captured, have been commercially available for a few years. These include professional 
movie cameras, such as Grass Valley s Viper [Valley 2010] or Panavision s Genesis [Panavision 2010]. 
The SpheroCam HDR [SpheronVR 2010] is able to capture full spherical 360-degree images with 26 f-stops 
and 50 megapixels in a single scan. A technology that allows per-pixel exposure control on the sensor, 
thereby enabling adaptive high dynamic range capture, was introduced by Pixim [Pixim 2010]. This level 
of control is achieved by including an analog-to-digital converter for each pixel on the sensor. Capturing 
image gradients rather than actual pixel intensities was shown to increase the dynamic range of recorded 
content [Tumblin et al. 2005]. In order to reconstruct intensity values, a computationally expensive 
Poisson solver needs to be applied to the measured data. While a Gradient Camera is an interesting theoretical 
concept, to the knowledge of the authors this camera has never actually been built. The maximum intensity 
that can be resolved with standard ND .lter arrays is limited by the lowest transmission of the employed 
ND .lters. Large, completely saturated regions in the sensor image are usually .lled with data interpolated 
from neighboring unsaturated regions [Nayar and Mitsunaga 2000]. An analysis of sensor saturation in 
multiplexed imaging along with a Fourier-based reconstruction technique that boosts the dynamic range 
of captured images beyond the previous limits was recently proposed [Wetzstein et al. 2010]. Figure 2 
shows an example image that is captured with an ND .lter array on the left and a Fourier-based reconstruction 
of multiplexed data on the right. Figure 2: Sensor image captured with an array of ND .lters [Nayar 
and Mitsunaga 2000] (left). Exposure brackets and magni.cations for Fourier-based HDR reconstruction 
from multiplexed sensor images [Wetzstein et al. 2010] (right). An alternative to mounting a .xed set 
of ND .lters in front of the sensor is an aligned spatial light modulator, such as a digital micromirror 
device (DMD). This concept was explored as Programmable Imaging [Nayar et al. 2004; Nayar 3  et al. 
2006] and allows for adaptive control over the exposure of each pixel. Unfortunately, it is rather dif.cult 
to align a DMD with a sensor on a pixel-precise basis, partly due to the required additional relay optics; 
for procedures to precisely calibrate such a system please consult [Ri et al. 2006]. Although a transmissive 
spatial light modulator can, alternatively, be mounted near the aperture plane of the camera, as proposed 
by Nayar and Branzoi [Nayar and Branzoi 2003], this Adaptive Dynamic Range Imaging approach only allows 
lower spatial frequencies in the image to be modulated. The most practical approach to adaptive exposures 
is a per-pixel control of the readout in software, as implemented by the Pixim camera [Pixim 2010]. This 
has also been simulated for the speci.c case of CMOS sensors with rolling shutters [Gu et al. 2010], 
but only on a per-scanline basis. The next version of the Frankencamera [Adams et al. 2010] is planned 
to provide non-destructive sensor readout for small image regions of interest [Levoy 2010], which would 
be close to the desired per-pixel exposure control. Rouf et al. [Rouf et al. 2011] proposed to encode 
both saturated highlights and low-dynamic range content in a single sensor image using cross-screen .lters. 
Computerized tomographic reconstruction techniques were employed to estimate the saturated regions from 
glare created by the optical .lters. 2.2 Multi-Sensor and Multi-Exposure Techniques The most straightforward 
way of acquiring high dynamic range images is to sequentially capture multiple pho­tographs with different 
exposure times and merge them into a single, high-contrast image [Mann and Picard 1995; Debevec and Malik 
1997; Mitsunaga and Nayar 1999; Robertson et al. 1999]. Some of these approaches simul­ taneously compute 
the non-linear camera response function from the image sequence [Debevec and Malik 1997; Mitsunaga and 
Nayar 1999; Robertson et al. 1999]. Extensions to these techniques also allow HDR video [Kang et al. 
2003]. Here, successive frames in the video are captured with varying exposure times and aligned using 
opti­ cal .ow algorithms. Today, all of these methods are well established and discussed in the textbook 
by Reinhard et al. [Reinhard et al. 2010]. In addition to capturing multiple exposures, a static .lter 
with varying transmissivity, termed Generalized Mosaic­ing [Schechner and Nayar 2003b], can be mounted 
in front of the camera but also requires multiple photographs to be captured. Alternatively, the optical 
path of an imaging device can be divided using prisms [Aggarwal and Ahuja 2004] (Split Aperture Imaging) 
or beam-splitters [McGuire et al. 2007] (Optical Splitting Trees), so that multiple sensors capture the 
same scene with different exposure times. While these approaches allow dynamic content to be recorded, 
the additional optical elements and sensor hardware make them more expensive and increase the form factor 
of the device. 2.3 Analysis and Tradeoffs Given a camera with known response function and dynamic range, 
Grossberg and Nayar [Grossberg and Nayar 2003] analyze the best possible set of actual exposure values 
for a low dynamic range (LDR) image sequence used to compute an HDR photograph. By also considering variable 
ISO settings, Hasinoff et al. [Hasinoff et al. 2010] provide the optimal choice of parameters for HDR 
acquisition with minimal noise.  3 Spectral Imaging Imaging of the electromagnetic spectrum comes in 
a number of .avors. For photographs or movies, the goal is typically to capture the colors perceived 
by the human visual system. Since the human visual system is based on three types of color sensing cells 
(the cones), three color bands are suf.cient to form a natural color impression. This discovery is usually 
credited to Maxwell [Maxwell 1860]. In this report we are mainly concerned with methods for capturing 
the physical properties of light in contrast to their perceptual counterparts that are dealt with in 
the areas of Applied Perception and Color Sciences. For readers interested in issues of color perception, 
we refer to standard literature: Wyszeski and Stiles [Wyszecki and Stiles 4  1982] provide raw data 
for many perceptual experiments. Fairchild s book [Fairchild 2005] is a higher-level treatise focusing 
on models for perceptual effects as, for instance, adaptation issues. Hunt s books [Hunt 1991; ?] deal 
with measurement and reproduction of color for human observers (e.g., in digital imaging, .lm, print, 
and television). Reinhard et al. [Reinhard et al. 2008] discuss color imaging from a computer graphics 
perspective. In this section we discuss spectral imaging from a radiometric, i.e. physical, perspective. 
To simplify the discussion we .rst introduce some terminology as used in this sub.eld of plenoptic imaging. 
3.1 Glossary of Terms Spectral Radiance is the physical quantity emitted by light sources or re.ected 
by objects. The symbol is L. and its unit is [W/m2 · sr · nm]. Spectral radiance is constant along a 
ray. It is the quantity returned by the plenoptic function. Spectral Filters selectively attenuate parts 
of the electromagnetic spectrum. There are two principles of operation, absorptive spectral .lters remove 
parts of the spectrum by converting photons into kinetic energy of the atoms con­stituting the material. 
Interference-based .lters, also referred to as dichroic .lters, consist of a transparent substrate which 
is coated with thin layers that selectively re.ect light, reinforcing and attenuating different wavelengths 
in different ways. The number and thicknesses of the layers determine the spectral re.ection pro.le. 
Absorptive .l­ters have a better angular constancy, but heating may be an issue for narrow-band .lters. 
Interference-based .lters have the advantage that the spectral .lter curve can be designed within certain 
limits by choosing the parameters of the coatings. However, the angular variation of these .lters is 
signi.cant. In general, .lters are available both for transmission and re.ection modes of operation. 
Narrow-Band Filters have a small support in the wavelength domain. Broad-Band Filters have a large support 
in the wavelength domain. They are also known as panchromatic .lters. The Spectral Response Curve of 
a sensor is a function that describes its quantum ef.ciency with respect to photons of different wavelengths. 
A higher value means a better response of the sensor to photons of a particular wavelength, i.e. more 
electrons are freed due to the photo-electric effect. Color is the perceptual interpretation of a given 
electro-magnetic spectrum. The Gamut of an imaging or color reproduction system is the range of correctly 
reproducible colors. Multi-Spectral Images typically consist of a low number of spectral bands. They 
often include a near infrared (NIR) band. The bands typically do not form a full spectrum, there can 
be missing regions [Vagni 2007]. Hyper-Spectral Images contain thirty to several hundred spectral bands 
which are approximations to the full spec­trum [Vagni 2007]. The different spectral bands do not necessarily 
have the same spatial resolution. In this report, we will use the term multi-spectral to refer both to 
multi-spectral and hyper-spectral image acquisition methods. A Multi-Spectral Data Cube is a stack of 
images taken at different wavelength bands. 3.2 Color Imaging In a limited sense, the most common application 
of multi-spectral imaging is the acquisition of color images for human observers. In principle, three 
spectral bands mimicking the human tri-stimulus system, are suf.cient to capture color images. This principle 
was .rst demonstrated by Maxwell performing color photography by time­sequential acquisition of three 
images using different band-pass .lters, see Figure 3. Display was achieved by super­ imposed spectrally 
.ltered black-and-white projections using the same .lters as used for capture. This acquisition principle 
was in use for quite some time until practical .lm-based color photography was invented. One of the 5 
 Figure 3: Tartan Ribbon, considered to be the world s .rst color photograph, taken by Thomas Sutton 
for James Clerk Maxwell in 1861 by successively placing three color .lters in front of the camera s main 
lens and taking three monochromatic photographs (Wikimedia Commons). earliest collections of color photographs 
was assembled by the Russian photographer Sergej Mikhailovich Prokudin-Gorskij [Prokudin-Gorskii 1912]. 
Time-sequential imaging through different .lters is still one of the main modes of capturing multi-spectral 
images (see Sec. 3.3). In the digital age, color .lms have been replaced by electronic CMOS or CCD sensors. 
The two technologies to capture an instantaneous color image are optical splitting trees employing dichroic 
beam-splitter prisms [Optec 2011], as used in three-CCD cameras, and spatial multiplexing [Narasimhan 
and Nayar 2005; Ihrke et al. 2010a], trading spatial resolution for color information. The spatially 
varying spectral .lters in multiplexing applications are also known as color .lter arrays (CFAs). A different 
principle, based on volumetric, or layered measurements is employed by the Foveon sensor [Foveon 2010], 
which captures tri-chromatic images at full spatial resolution. The most popular spatial multiplexing 
pattern is the well known Bayer pattern [Bayer 1976]. It is used in most single­ sensor digital color 
cameras. The associated problem of reconstructing a full-resolution color image is generally referred 
to as demosaicing. An overview of demosaicing techniques is given in [Ramanath et al. 2002; Gunturk et 
al. 2005; Li et al. 2008b]. Li et al. [Li et al. 2008b] present a classi.cation scheme of demosaicing 
techniques depending on the prior model being used (explicitly or implicitly) and an evaluation of different 
classes of algorithms. An interesting result is that the common constant-hue assumption seems to be less 
valid for modern imagery with a wider gamut and higher dynamic range than the classical Kodak photo CD 
test set [Eastman Kodak Company ], which was scanned from .lm and has predominantly been used for evaluating 
demosaicing algorithms. Mostly, demosaicing is evaluated through simulation. However, in a realistic 
setting, including camera noise, Hirakawa and Parks [Hirakawa and Parks 2006] have shown that demosaicing 
on noisy images performs poorly and that subsequent denoising is affected by demosaicing artifacts. They 
propose a joint denoising and demosaicing framework that can be used with different demosaicing and denoising 
algorithms. In recent years, a large number of alternative CFAs have been explored by camera manufacturers, 
some of which are already being used in consumer products. Examples and many references to alternative 
CFA designs can be found in Hirakawa and Wolfe s work [Hirakawa and Wolfe 2008]. Traditionally, imaging 
through CFAs and reconstruction of the signal have been seen as sub-sampling and up-sampling operations, 
respectively. Recent research in the analysis of these multiplexing patterns has, however, produced a 
new view of multiplexing as a projection onto a linear subspace of basis functions (the spectral responses 
of the .lters in this case), i.e. of multiplexing as a coding 6 operation [Lu and Vetterli 2009; Ihrke 
et al. 2010a]. Correspondingly, in this view, the reconstruction process is seen as a recovery of the 
subspace, or a decoding of the signal. This view originated in Fourier analysis of color .lter arrays 
[Alleyson et al. 2005], stimulated by the desire to apply digital signal processing methodology to the 
color multiplexing problem. Being a linear framework, it allows for the optimization of the subspace 
onto which color is projected [Hirakawa and Wolfe 2007; Hirakawa and Wolfe 2008; Lu and Vetterli 2009]. 
Practical realizations are alternative CFA designs that suffer from less aliasing than their ad-hoc, 
heuristically-designed counterparts. While in [Hirakawa and Wolfe 2007; Hirakawa and Wolfe 2008; Lu and 
Vetterli 2009] a .xed number of primary color response functions are assumed which can be linearly mixed 
to optimize the CFA, [Parmar and Reeves 2006; Parmar and Reeves 2010] optimize the spectral response 
functions themselves in order to improve CFA design. More recently, [Sajadi et al. 2011] proposed to 
use shiftable layers of CFAs; this design allows the color primaries to be switched dynamically and provides 
an optimal SNR in different lighting conditions. Generalizing color .lter arrays, Narasimhan and Nayar 
[Narasimhan and Nayar 2005] proposed the Assorted Pixels framework, where individual pixels can be modulated 
by arbitrary plenoptic .lters, yielding an image mosaic that has to be interpolated to obtain the full-resolution 
multi-channel image. In subsequent work [Yasuma et al. 2010], aliasing within the Assorted Pixels framework 
was minimized. Ihrke et al. [Ihrke et al. 2010a] have shown how this (and other) approaches that are 
tiling the image in a super-pixel fashion can be interpreted as belonging to one group of imaging systems 
that share common analysis and reconstruction approaches. In a different application, Wetzstein et al. 
[Wetzstein et al. 2010] explore CFA designs that lead to dynamic range constraints in Fourier space. 
They show how an optimized CFA pattern in conjunction with optimization algorithms allow trichromatic 
high dynamic range images to be captured. 3.3 Multi-Spectral Imaging As for the other plenoptic dimensions, 
the three basic approaches of Figure 1, single-shot capture, sequential image acquisition, and multiple 
device setups are valid alternatives for multi-spectral imaging and have been investigated intensely. 
3.3.1 Spectrometers Traditionally, spectroscopy has been carried out for single rays entering an instrument 
referred to as spectrometer. It was invented by Joseph von Fraunhofer in 1814 and used to discover the 
missing lines in the solar spectrum bearing his name. Typically the ray is split into its constituent 
wavelengths which are displaced spatially. This is achieved by placing either dispersive or diffractive 
elements into the light path, where the latter come both in transmissive and re.ective variants. If dispersion 
is used to split the ray, typically a prism is employed. The separation of wavelengths is caused by the 
wavelength-dependent refractive index of the prism material. The function mapping wavelength to refractive 
index is typically decreasing with increasing wavelength, but usually in a non-linear fashion. Under 
certain conditions, it can even have an inverted slope (anomalous dispersion) [Hecht 2002]. Diffractive 
elements are usually gratings, where maxima of the diffraction pattern are spatially shifted according 
to the grating equation [Hecht 2002]. After the light path is split by some means, the light is brought 
onto a photo-detector which can, for instance, be a CCD. Here, relative radiance of the individual wavelengths 
is measured. Spectrometers have to be calibrated in two ways: .rst, the mapping of wavelengths to pixels 
has to be determined. This is usually done using light sources with few and very narrow emission lines 
of known wavelength, the pixel positions of other wavelengths are then interpolated [Gaigalas et al. 
2009]. The second step establishes the relative irradiance measured for every wavelength. This is done 
by measuring a surface of known .at re.ectance, for example Spectralon, which is illuminated with a known 
broad-band spectrum. The relative inhomogeneities imposed by the device are then divided out [Gaigalas 
et al. 2009]. Spectrometers that are designed to image more than one ray are referred to as imaging spectrometers. 
7  3.3.2 Scanning Imaging Spectrometers Traditional devices are usually based on some form of scanning. 
Either a full two-dimensional image is acquired with changed band-pass .lters, effectively performing 
a spectral scan, or a pushbroom scan is performed where the two-dimensional CCD images a spatial dimension 
on one axis of the image and the spectral dimension on the other. The full multi-spectral data cube is 
then obtained by scanning the remaining spatial dimension. Spectral Scanning can be performed in a variety 
of ways. Most of them involve either a .lter wheel (e.g., [Wang and Heidrich 2004]) or electronically 
tunable .lters (ETFs). The former method usually employs narrow-band .lters such that spectral bands 
are imaged directly. The disadvantage is a low light throughput. Toyooka and Hayasaka [Toyooka and Hayasaka 
1997] present a system based on broad-band .lters with computational inversion. Whether or not this is 
advantageous depends on the camera noise [Ihrke et al. 2010a]. Electronically tunable .lters are programmable 
devices that can exhibit varying .lter curves depending on control voltages applied to the device. Several 
incarnations exist; the most well known include Liquid Crystal Tunable Filters (LCTFs) [cri inc 2009], 
which are based on a cascade of Lyot-.lter stages [Lyot 1944], acousto-optical tunable .lters, where 
an acoustically excited crystal serves as a variable diffraction grating, and interferometer-based systems, 
where the spectrum is projected into the Fourier basis. In the latter, the spectral scan is performed 
in a multiplexing manner: by varying the position of the mirror in one arm of an interferometer, for 
instance a Michelson-type device, different phase shifts are induced for every wavelength. The resulting 
spectral modulation is in the form of a sinusoid. Thus, effectively, the measurements are performed in 
the Fourier basis, similar to the Dappled Photography technique [Veeraraghavan et al. 2007] for light 
.elds (see Sec. 4). The spectrogram is obtained by taking an inverse Fourier transform. A good overview 
of these technologies is given in [Gat 2000]. A more .exible way of programmable wavelength modulation 
is presented by Mohan et al. [Mohan et al. 2008b]. They modulate the spectrum of a entire image by .rst 
diffracting the light and placing an attenuating mask into the rainbow plane of the imaging system. However, 
the authors do not recover multiplexed spectra but only demonstrate modulated spectrum imaging. Usually, 
the scanning and the imaging process have to be synchronized, i.e. the camera should only take an image 
when the .lter in front of the camera is set to a known value. Schechner and Nayar [Schechner and Nayar 
2004] introduce a technique to computationally synchronize video streams taken with a periodically moving 
spectral .lter. All previously discussed techniques attempt to recover scene spectra passively. An alternative 
technique using active spectral illumination in a time-sequential manner is presented by Park et al. 
[Park et al. 2007]. The scene, which can include ambient lighting, is imaged under different additional 
spectral lighting. The acquired images allow for reasonably accurate per-pixel spectra to be recovered. 
Spatial Scanning has been widely employed in satellite-based remote sensing. Two technologies are commonly 
used: pushbroom and whiskbroom scanning. Whereas pushbroom scanning uses a two-dimensional sensor and 
can thus recover one spectral and one spatial dimension per position of the satellite, whiskbroom systems 
employ a one­dimensional sensor, imaging the spectrum of a single point which is then scanned to obtain 
a full scan-line with a rotating mirror. The main idea is that a static scene can be imaged multiple 
times using different spectral bands and thus a full multi-spectral data cube can be assembled. A good 
overview of space-borne remote sensing, and more generally, multi-spectral imaging techniques is given 
in [Vagni 2007] In computer vision, a similar concept, called Generalized Mosaicing, has been introduced 
by Schechner and Na­yar [Schechner and Nayar 2001]. Here, a spatially varying .lter is mounted in front 
of the main lens, .ltering each column of the acquired image differently. By moving the camera and registering 
the images, a full multi-spectral data cube can be recovered [Schechner and Nayar 2002]. 8  3.3.3 Single-Shot 
Imaging Spectrometers To enable the spectral acquisition of fast-moving objects, it is necessary to have 
single-shot methods available. Indeed, this appears to be the focus of research in recent years. We can 
differentiate between three major modes of operation. The .rst is a trade of spatial for spectral resolution. 
Optical devices are implemented that provide empty space on the sensor which can, with a subsequent dispersion 
step through which a scene ray is split into its wavelength constituents, be .lled with spectral information. 
The second option is multi-device setups which operate mostly like their spectral scanning counterparts, 
replacing sequential imaging by additional hardware. The third class of devices employs computational 
imaging, i.e. computational inversion of an image formation process where spectral information is recorded 
in a super-imposed manner. Spatial Multiplexing of the spectrum, in general, uses a dispersive or diffractive 
element in conjunction with some optics redirecting rays from the scene onto parts of the sensor surrounded 
by void regions. The void regions are then .lled with spectral information. All these techniques take 
advantage of the high resolution of current digital cameras. Examples using custom manufactured redirecting 
mirrors include [Harvey et al. 2005; Gao et al. 2009; Gorman et al. 2010]. These techniques achieve imaging 
of up to 25 spectral bands in real-time and keep the optical axis of the different slices of the multi-spectral 
data cube constant. Bodkin et al. [Bodkin et al. 2009] and Du et al. [Du et al. 2009] propose a similar 
concept by using an array of pinholes that limits the rays that can reach the sensor from the scene. 
The pinholes are arranged such that a prism following in the optical path disperses the spectrum and 
.lls the pixels with spectral information. A different approach is taken by Fletcher-Holmes et al. [Fletcher-Holmes 
and Harvey 2005]. They are interested in only providing a small foveal region in the center of the image 
with multi-spectral information. For this, the center of the image is probed with .ber optic cables which 
are fed into a standard spectrometer. Mathews et al. [Mathews 2008] and Horstmeyer et al. [Horstmeyer 
et al. 2009] describe light .eld cameras with spectrally .ltered sub-images. An issue with this design 
is the problem of motion parallax induced by the different view points when registering the images (see 
Sec. 4). In general, this registration problem is dif.cult and requires knowledge of scene geometry and 
re.ectance which cannot easily be estimated. Multi-Device Setups are similar in spirit to spectral scanning 
spectrometers, replacing the scanning process by additional hardware. A straightforward solution recording 
.ve spectral bands is presented by Lau et al. [Lau and Yang 2005]. They use a standard multi-video array 
where different spectral .lters are mounted on each camera. The motion-parallax problem mentioned previously 
is even worse in this case. McGuire et al. [McGuire et al. 2007] discuss optical splitting trees where 
the individual sensors are aligned such that they share a single optical axis. The design of beam-splitter/.lter 
trees is non-trivial and the authors propose an automatic solution based on optimization. A hybrid camera 
was proposed by Cao et al. [Cao et al. 2011], where the output of a high resolution RGB camera was combined 
with that of a prism-based low spatial-resolution, high spectral-resolution camera. Computational Spectral 
Imaging aims at trading computational complexity for simpli.ed optical designs. Com­puted Tomography 
Image Spectrometry (CTIS) was developed by Okamoto and Yamaguchi [Okamoto and Yam­ aguchi 1991]. They 
observed that by placing a diffraction grating in the optical path, several spectral copies overlay on 
the image sensor. Every pixel is measuring a line integral along the spectral axis. Knowing the imaging 
geometry enables a tomographic reconstruction of the spectra. A drawback to this technique is that not 
all data can be mea­sured and thus an ill-conditioned problem, similar to limited angle tomography, is 
encountered. The technique was extended to single shot imaging by Descour et al. [Descour and Dereniak 
1995; Descour et al. 1997]. A relatively novel technique is referred to as Coded Aperture Snapshot Spectral 
Imaging (CASSI) [Gehm et al. 2007; Wagadarikar et al. 2008; Wagadarikar et al. 2009]. In a series of 
papers the authors show how to construct different devices to exploit the compressive sensing paradigm 
[Cand`es et al. 2006] which promises to enable higher resolution computational reconstructions with less 
samples than predicted by the Shannon-Nyquist sampling theorem. 9 The results presented for both CTIS 
and CASSI have only been demonstrated for relatively low-resolution, low­quality spectral images. Therefore, 
these approaches are not yet suitable for high-quality photographic applications.  3.4 Applications 
There is a huge amount of applications for multi-spectral imaging and we are just beginning to explore 
the possi­bilities in computer graphics and vision. Traditional users of multi-spectral imaging technology 
are in the .elds of astronomy and remote sensing where, for instance, the mapping of vegetation, minerals, 
water surfaces, and haz­ardous waste monitoring are of interest. In addition, multi-spectral imaging 
is used for material discrimination [Du et al. 2009], ophthalmology [Lawlor et al. 2002], the study of 
combustion dynamics [Hunicz and Piernikarski 2001], cellular dynamics [Kindzelskii et al. 2000], surveillance 
[Harvey et al. 2000], for deciphering ancient scrolls [Mans­ .eld 2005], .ower photography [Rorslett 
2008], medicine, agriculture, manufacturing, forensics, and microscopy. It should not be forgotten that 
the military is an interested party [Vagni 2007].  4 Light Field Acquisition While the 5D plenoptic 
function parameterizes all possible images of a general scene, being a slice of constant time and wavelength 
of the full plenoptic function, it remains problematic to capture its directional variation over a wide 
.eld of view. Consider the inside of a ceramic vase; a camera must be inserted inside to capture the 
intensity and color of light rays traveling entirely within the concavity. However, if the viewer is 
restricted to move through an isotropic, transparent medium (e.g., air or water) outside of the convex 
hull of a given object, then the plenoptic function can be measured by translating a digital camera throughout 
the allowed viewer region. Levoy and Hanrahan [Levoy and Hanrahan 1996] and Gortler et al. [Gortler et 
al. 1996] realized that, when the viewer is restricted to move outside the convex hull, the 5D plenoptic 
function possesses one dimension of redundancy: the radiance of a given ray does not change in free space. 
Thus, in a region free of occluders, the 5D plenoptic function can be expressed as a 4D light .eld. The 
concept of a light .eld predates its introduction in computer graphics. The term itself dates to the 
work of Gershun [Gershun 1936], who derived closed-form expressions for illumination patterns projected 
by area light sources. Ashdown [Ashdown 1993] continued this line of research. Moon and Spencer [Moon 
and Spencer 1981] introduced the equivalent concept of a photic .eld and applied it to topics spanning 
lighting design, photography, and solar heating. The concept of a light .eld is similar to epipolar volumes 
in computer vision [Bolles et al. 1987]. As demonstrated by Halle [Halle 1994], both epipolar volumes 
and holographic stereograms can be captured by uniform camera translations. The concept of capturing 
a 4D light .eld, for example by translating a single camera [Levoy and Hanrahan 1996; Gortler et al. 
1996] or by using an array of cameras [Wilburn et al. 2002], is predated by integral photography [Lippmann 
1908], parallax panoramagrams [Ives 1903], and holography [Gabor 1948]. This section catalogues existing 
devices and methods for light .eld capture, as well as applications enabled by such data sets. Note that 
a sensor pixel in a conventional camera averages the radiance of light rays impinging over the full hemisphere 
of incidence angles, producing a 2D projection of the 4D light .eld. In contrast, light .eld cameras 
prevent such averaging by introducing spatio-angular selectivity. Such cameras can be classi.ed into 
those that primarily rely on multiple sensors or a single sensor augmented by temporal, spatial, or frequency-domain 
multiplexing. 4.1 Multiple Sensors As described by Levoy and Hanrahan [Levoy and Hanrahan 1996], a light 
.eld can be measured by capturing a set of photographs taken by an array of cameras distributed on a 
planar surface. Each camera measures the radiance of light rays incident on a single point, de.ned in 
the plane of the cameras, for a set of angles determined by the .eld of view of each camera. Thus, each 
camera records a 2D slice of the 4D light .eld. Concatenating these 10  Figure 4: Light .eld cameras 
can be categorized by how a 4D light .eld is encoded in a set of 2D images. Methods include using multiple 
sensors or a single sensor with temporal, spatial, or frequency-domain multiplexing. (Top, Left) Wilburn 
et al. [Wilburn et al. 2002] describe a camera array. (Top, Middle) Liang et al. [Liang et al. 2008] 
achieve temporal multiplexing with a programmable aperture. (Top, Right) Georgiev et al. [Georgiev et 
al. 2008] capture spatially-multiplexed light .elds using an array of lenses and prisms. (Bottom) Raskar 
et al. [Raskar et al. 2008] capture frequency-multiplexed light .elds by placing a heterodyne mask [Veeraraghavan 
et al. 2007; Veeraraghavan et al. 2008; Lanman et al. 2008] close to the sensor. (Figures reproduced 
from [Wilburn et al. 2002], [Liang et al. 2008], [Georgiev et al. 2008], and [Raskar et al. 2008].) slices 
yields an estimate of the light .eld. Wilburn et al. [Wilburn et al. 2002; Wilburn et al. 2005] achieve 
dynamic light .eld capture using an array of up to 125 digital video cameras (see Figure 4, left). Yang 
et al. [Yang et al. 2002] propose a similar system using 64 cameras. Nomura et al. [Nomura et al. 2007] 
create scene collages using up to 20 cameras attached to a .exible plastic sheet, combining the bene.ts 
of both multiple sensors and temporal multiplexing. Custom hardware allows accurate calibration and synchronization 
of the camera arrays. Such designs have several unique properties. Foremost, as demonstrated by Vaish 
et al. [Vaish et al. 2006], the captured light .eld can be considered as if it were captured using a 
single camera with a main lens aperture extending over the region occupied by the cameras. Such large-format 
cameras can not be practically constructed using refractive optics. Vaish et al. exploit this con.guration 
by applying methods of synthetic aperture imaging to obtain sharp images of objects obscured by thick 
foliage. 4.2 Temporal Multiplexing Camera arrays have several signi.cant limitations; foremost, a sparse 
array of cameras may not provide suf.cient light .eld resolution for certain applications. In addition, 
the cost and engineering complexity of such systems pro­hibit their use for many consumer applications. 
As an alternative, methods using a single image sensor have been developed. For example, Levoy and Hanrahan 
[Levoy and Hanrahan 1996] propose a direct solution; using a me­ chanical gantry, a single camera is 
translated over a spherical or planar surface, constantly reoriented to point towards the object of interest. 
Alternatively, the object can be mechanically rotated on a computer-controlled turntable. Ihrke et al. 
[Ihrke et al. 2008] substitute mechanical translation of a camera with rotation of a planar mirror, effectively 
11 creating a time-multiplexed series of virtual cameras. Thus, by distributing the measurements over 
time, single­sensor light .eld capture is achieved. Taguchi et al. [Taguchi et al. 2010a] show how capturing 
multiple images of rotationally-symmetric mirrors from different camera positions allow wide .eld of 
view light .elds to be cap­tured. Gortler et al. [Gortler et al. 1996] propose a similar solution; the 
camera is manually translated and computer vision algorithms are used to estimate the light .eld from 
such uncontrolled translations. These approaches trace their origins to the method introduced by Chen 
and Williams [Chen and Williams 1993], which is implemented by QuickTime VR. The preceding systems capture 
the light .eld impinging on surfaces enveloping large regions (e.g., a sphere encom­passing the convex 
hull of a sculpture). In contrast, hand-held light .eld photography considers capturing the light .eld 
passing through the main lens aperture of a conventional camera. Adelson and Wang [Adelson and Wang 1992], 
Okano et al. [Okano et al. 1999], and Ng et al. [Ng et al. 2005] extend integral photography to spatially 
multiplex a 4D light .eld onto a 2D image sensor, as discussed in the following subsection. However, 
temporal multiplexing can also achieve this goal. Liang et al. [Liang et al. 2008] propose programmable 
aperture photography to achieve time-multiplexed light .eld capture. While Ives [Ives 1903] uses static 
parallax barriers placed close to the image sensor, Liang et al. use dynamic aperture masks (see Figure 
4, middle). For example, consider capturing a sequence of conventional pho­ tographs. Between each exposure 
a pinhole aperture is translated in raster scan order. Each photograph records a pencil of rays passing 
through a pinhole located at a .xed position in the aperture plane for a range of sensor pixels. Similar 
to multiple sensor acquisition schemes, each image is a 2D slice of the 4D light .eld and the sequence 
can be concatenated to estimate the radiance for an arbitrary light ray passing through the aperture 
plane. To reduce the necessary exposure time, Liang et al. further apply Hadamard aperture patterns, 
originally proposed by Schechner and Nayar [Schechner et al. 2007], that are 50% transparent. The preceding 
methods all consider conventional cameras with refractive lens elements. Zhang and Chen [Zhang and Chen 
2005] propose a lensless light .eld camera. In their design, a bare sensor is mechanically translated 
perpendicular to the scene. The values measured by each sensor pixel are recorded for each translation. 
By the Fourier projection-slice theorem [Ng 2005], the 2D Fourier transform of a given image is equivalent 
to a 2D slice of the 4D Fourier transform of the light .eld; the angle of this slice is dependent on 
the sensor translation. Thus, tomographic reconstruction yields an estimate of the light .eld using a 
bare sensor, mechanical translation, and computational reconstruction methods. 4.3 Spatial and Frequency 
Multiplexing Time-sequential acquisition reduces the cost and complexity of multiple sensor systems, 
however it has one sig­ni.cant limitation: dynamic scenes cannot be readily captured. Thus, either a 
high-speed camera is necessary or alternative means of multiplexing the 4D light .eld into a 2D image 
are required. Ives [Ives 1903] and Lipp­ mann [Lippmann 1908] provide two early examples of spatial multiplexing 
with the introduction of parallax barriers and integral photography, respectively. Such spatial multiplexing 
allows light .eld capture of dynamic scenes, but requires a trade-off between the spatial and angular 
sampling rates. Okano et al. [Okano et al. 1999] and Ng et al. [Ng et al. 2005] describe modern, digital 
implementations of integral photography, however numerous other spatial multiplexing schemes have emerged. 
Instead of af.xing an array of microlenses directly to an image sensor, Georgiev et al. [Georgiev et 
al. 2006] add an external lens attachment with an array of lenses and prisms (see Figure 4, right). Ueda 
et al. [Ueda et al. 2008a; Ueda et al. 2008b] consider similar external lens arrays; however, in these 
works, an array of variable focus lenses, im­ plemented using liquid lenses controlled by electrowetting, 
allow the spatial and angular resolution to be optimized depending on the observed scene. Rather than 
using absorbing masks or refractive lens arrays, Unger et al. [Unger et al. 2003], Levoy et al. [Levoy 
et al. 2004], Lanman et al. [Lanman et al. 2006], and Taguchi et al. [Taguchi et al. 2010b] demonstrate 
that a single 12 photograph of an array of tilted, planar mirrors or mirrored spheres produces a spatially-multiplexed 
estimate of the incident light .eld. Yang et al. [Yang et al. 2000] demonstrate a large-format, lenslet-based 
architecture by combining an array of lenses and a .atbed scanner. Related compound imaging systems, 
producing a spatially­multiplexed light .eld using arrays of lenses and a single sensor, were proposed 
by Ogata et al. [Ogata et al. 1994], Tanida et al. [Tanida et al. 2001; Tanida et al. 2003], and Hiura 
et al. [Hiura et al. 2009]. Spatial multiplexing produces an interlaced array of elemental images within 
the image formed on the sensor. Veer­araghavan et al. [Veeraraghavan et al. 2007] introduce frequency 
multiplexing as an alternative method for achieving single-sensor light .eld capture. The optical heterodyning 
method proposed by Veeraraghavan et al. encodes the 4D Fourier transform of the light .eld into different 
spatio-angular bands of the Fourier transform of the 2D sen­sor image. Similar in concept to spatial 
multiplexing, the sensor spectrum contains a uniform array of 2D spectral slices of the 4D light .eld 
spectrum. Such frequency-domain multiplexing is achieved by placing non-refractive, light-attenuating 
masks slightly in front of a conventional sensor (see Figure 4, bottom). As described by Veeraraghavan 
et al., masks allowing frequency-domain multiplexing (i.e., heterodyne detection) must have a Fourier 
transform consisting of an array of impulses (i.e., a 2D Dirac comb). In [Veeraraghavan et al. 2007], 
a Sum-of-Sinusoids (SoS) pattern, consisting of a weighted harmonic series of equal-phase sinusoids, 
is proposed. As shown in Figure 5, such codes transmit signi.cantly more light than traditional pinhole 
arrays [Ives 1903]; however, as shown by Lanman et al. [Lanman et al. 2008], these patterns are equivalent 
to a truncated Fourier series approximation of a pinhole array for high angular sampling rates. In [Lanman 
et al. 2008], Lanman et al. propose tiled-broadband patterns, corresponding to periodic masks with individual 
tiles exhibiting a broadband Fourier transform. This family includes pinhole arrays, SoS patterns, and 
the tiled-MURA patterns proposed in that work (see Figure 5). Such patterns produce masks with 50% transmission, 
enabling shorter exposures than existing methods. In subsequent work, Veeraraghavan et al. [Veeraraghavan 
et al. 2008] propose adaptive mask patterns, consisting of aharmonic sinusoids, optimized for the spectral 
bandwidth of the observed scene. Georgiev et al. [Georgiev et al. 2008] analyze such heterodyne cameras 
and further propose masks placed external to the camera body. Rather than using a global, frequency-domain 
decoding scheme, Ihrke et al. [Ihrke et al. 2010a] demonstrate how spatial-domain decoding methods can 
be extended to frequency-multiplexed light .elds. 4.4 Capture Applications Given the wide variety of 
light .eld capture devices, a similarly diverse set of applications is enabled by such high­dimensional 
representations of light transport. While Kanolt [Kanolt 1918] considers the related concept of a par­ 
allax panoramagram to achieve 3D display, light .elds have also proven useful for applications spanning 
computer graphics, digital photography, and 3D reconstruction. In the .eld of computer graphics, light 
.elds were introduced to facilitate image-based rendering [Levoy and Han­ rahan 1996; Gortler et al. 
1996]. In contrast to the conventional computer graphics pipeline, novel 2D images are synthesized by 
resampling the 4D light .eld. With suf.cient light .eld resolution, views are synthesized without knowledge 
of the underlying scene geometry. Subsequent to these works, researchers continued to enhance the .­delity 
of image-based rendering. For example, a signi.cant limitation of early methods is that illumination 
cannot be adjusted in synthesized images. This is in stark contrast to the conventional computer graphics 
pipeline, wherein arbitrary light sources can be supported using ray tracing together with a model of 
material re.ectance properties. Debevec et al. [Debevec et al. 2000] address this limitation by capturing 
an 8D re.ectance .eld. In their system, the 4D light .eld re.ected by an object is measured as a function 
of the 4D light .eld incident on the object. Thus, an 8D re.ectance .eld maps variations in the input 
radiance to variations in the output radiance, allowing image-based rendering to support variation of 
both viewpoint and illumination. Light .elds parameterize every possible photograph that can be taken 
outside the convex hull of an object; as a result, they have found widespread application in 3D television, 
also known as free-viewpoint video. Carranza et 13  Figure 5: Lanman et al. [Lanman et al. 2008] introduce 
tiled-broadband patterns for mask-based, frequency­ multiplexed light .eld capture. (Top) Each row, from 
left to right, shows broadband tiles of increasing spatial dimen­sions, including: pinholes [Ives 1928], 
Sum-of-Sinusoids (SoS) [Veeraraghavan et al. 2007], and MURA [Gottesman and Fenimore 1989; Lanman et 
al. 2008]. (Bottom) The SoS tile converges to 18% transmission, whereas the MURA tile remains near 50%. 
Note that frequency multiplexing with either SoS or MURA tiles signi.cantly outperforms conventional 
pinhole arrays in terms of total light transmission and exposure time. (Figures reproduced from [Lan­ 
man 2010].) al. [Carranza et al. 2003] describe a system with an array of cameras surrounding one or 
more actors. Similar systems have been developed by Matusik et al. [Matusik et al. 2000] and Starck et 
al. [Starck and Hilton 2008]. Image-based rendering allows arbitrary adjustment of the viewpoint in real-time. 
Vlasic et al. [Vlasic et al. 2009] further demonstrate 3D reconstruction of human actors from multiple-camera 
sequences captured under varying illumination conditions. Light .elds, given their similarity to conventional 
parallax panoramagrams [Ives 1928], have also found application in the design and analysis of 3D displays. 
Okano et al. [Okano et al. 1999] adapt integral photography to create a 3D television system supporting 
both multi-view capture and display. Similarly, Matusik and P.ster [Matusik and P.ster 2004] achieve 
light .eld capture using an array of 16 cameras and implement light .eld display using an array of 16 
projectors and lenticular screens. Zwicker et al. [Zwicker et al. 2006] develop antialiasing .lters for 
automultiscopic 3D display using a signal processing analysis. Hirsch et al. [Hirsch et al. 2009] develop 
a BiDirectional (BiDi) screen, supporting both conventional 2D image display and real-time 4D light .eld 
capture, facilitating mixed multitouch and gesture-based interaction; the device uses a lensless light 
.eld capture method, consisting of a tiled-MURA pattern [Lanman et al. 2008] displayed on an LCD panel 
and a large-format sensor. Recently, Lanman et al. [Lanman 14  et al. 2010] use an algebraic analysis 
of light .elds to characterize the rank constraints of all dual-layer, attenuation­ based light .eld 
displays; through this analysis they propose a generalization of conventional parallax barriers, using 
content-adaptive, time-multiplexed mask pairs to synthesize high-rank light .elds with increased brightness 
and spatial resolution. Wetzstein et al. [Wetzstein et al. 2011b] demonstrate how a stack of attentuating 
layers, such as spaced transparencies or LCD panels, can be used in combination with computerized tomographic 
reconstruction to display natural light .elds of 3D scenes. Post-processing of captured light .elds can 
resolve long-standing problems in conventional photography. Ng [Ng 2005] describes ef.cient algorithms 
for digital image refocusing, allowing the plane of focus to be adjusted after a photograph has been 
taken. In addition, Talvala et al. [Talvala et al. 2007] and Raskar et al. [Raskar et al. 2008] demonstrate 
that high-frequency masks can be combined with light .eld photography to eliminate artifacts due to glare 
and multiple scattering of light within camera lenses. Similarly, light .eld capture can be extended 
to microscopy and confocal imaging, enabling similar bene.ts in extended depth of .eld and reduced scattering 
[Levoy et al. 2004; Levoy et al. 2006]. Smith et al. [Smith et al. 2009] improve conventional image stabilization 
algorithms using light .elds captured with an array of 25 cameras. As described, most single-sensor acquisition 
schemes trade increased angular resolution for decreased spatial resolution [Georgiev et al. 2006]; Bishop 
et al. [Bishop et al. 2009] and Lumsdaine and Georgiev [Lumsdaine and Georgiev 2009] apply priors regarding 
the statistics of natural images and modi.ed imaging hardware, respectively, to achieve super-resolution 
light .eld capture that, in certain conditions, mitigates this resolution loss. As characterized throughout 
this report, the plenoptic function of a given scene contains a large degree of redun­dancy; both the 
spatial and angular dimensions of light .elds of natural scenes are highly correlated. Recent work is 
exploring the bene.ts of compressive sensing for light .eld acquisition. Fergus et al. [Fergus et al. 
2006b] introduce random lens imaging, wherein a conventional camera lens is replaced with a random arrangement 
of planar mirrored surfaces, allowing super-resolution and 3D imaging applications. Babacan et al. [Babacan 
et al. 2009] propose a compressive sensing scheme for light .eld capture utilizing randomly-coded, non-refractive 
masks placed in the aperture plane. Ashok and Neifeld [Ashok and Neifeld 2010] propose compressive sensing 
schemes, again using non-refractive masks, allowing either spatial or angular compressive light .eld 
imaging. As observed in that work, future capture methods will likely bene.t from joint spatio-angular 
compressive sensing; however, as discussed later in this report, further redundancies exist among all 
the plenoptic dimensions, not just the directional variations characterized by light .elds.  5 Multiplexing 
Space and Focal Surfaces The ability to resolve spatial light variation is an integral part of any imaging 
system. For the purpose of this report we differentiate between spatial variation on a plane perpendicular 
to the optical axis and variation along the optical axis inside a camera behind the main lens. The former 
quantity, transverse light variation, is what all 2D sensors measure. In this section, we discuss approaches 
for very high-resolution imaging (Sec. 5.1), focal surface curvature correction techniques of the light 
.eld inside a camera (Sec. 5.2), and extended depth of .eld photography (Sec. 5.3). 5.1 Super-Resolution 
and Gigapixel Imaging Although exotic camera systems can resolve structures in the order of 100 nm [van 
Putten et al. 2011], the resolution of standard photographs is usually limited by the physical layout 
and size of the photosensitive elements, the optical resolution of employed optical elements, and the 
diffraction limit. Attempts to break these limits, which are sig­ni.cantly larger than 100 nm, are referred 
to as super-resolution imaging. Such techniques have been of particular interest to the vision community 
for many years. In most cases a sequence of slightly shifted low-resolution photos is captured and fused 
into a single high-resolution image. The shifts are usually smaller than the pixel size; an exten­sive 
review of such techniques can be found in [Baker and Kanade 2002; Borman and Stevenson 1998]. Sub-pixel 
precise shifts of low-resolution images can be achieved by mechanical vibrations [LANDOLT et al. 2001; 
Ben-Ezra 15  Figure 6: A wide .eld of view 1.7 gigapixel image captured by Cossairt et al. [Cossairt 
et al. 2011]. et al. 2005], by coding the camera s aperture using phase [Ashok and Neifeld 2007] and 
attenuation [Mohan et al. 2008a] masks, by exploiting object motion in combination with temporally coded 
apertures [Agrawal and Raskar 2007], or by analyzing multiple frames of a video [Liu and Sun 2011]. For 
an increased resolution in space and time, successive frames in videos of a single camera [Shahar et 
al. 2011] or multiple devices [Shechtman et al. 2002; Shechtman et al. 2005] (see Sec. 6.2) can be analyzed 
instead. All super-resolution approaches require an optimiza­ tion problem to be solved for the unknown 
super-resolved image given multiple low-resolution measurements. This is computationally expensive for 
higher resolutions and is usually an ill-posed problem requiring additional image priors [Baker and Kanade 
2002]. Gigapixel imaging is a relatively new .eld that, similar to super-resolution, aims at capturing 
very high-resolution imagery. The main difference is that gigapixel imaging approaches generally do not 
try to beat the limits of sensor resolution, but rather stitch a gigapixel panoramic image together from 
a set of megapixel images. These can be photographed by mounting a camera on a computer-controlled rotation 
stage [Kopf et al. 2007], or a high-resolution small-scale sensor that is automatically moved in the 
image plane of a large-format camera [Ben-Ezra 2011]. Both of these techniques implement the concept 
of capturing a sequence of images with a single device that are composited into a high-quality photograph. 
In this case, the parameter that is varied for each image in the sequence is the camera pose. Alternatively, 
the optics of a camera can be modi.ed, for instance with custom spherical lens elements, to allow a single 
very high-resolution image to be captured instantaneously with multiple sensors [Cossairt et al. 2011]. 
An example scene captured with this technique is shown in Figure 6. 5.2 Optical Field Correction Not 
only is the actual resolution of digital photographs limited by the pixel count and the diffraction limit, 
but also by the applied optical elements. Standard spherical lenses have a focal surface that is, unlike 
most sensors, not actually planar but curved. Signi.cant engineering effort is put into the commercial 
development of complex lens systems, especially in variable-focus camera objectives, that correct for 
the resulting image blur at sensor locations away from the optical axis. Several approaches have been 
proposed to correct for what is usually called .eld curvature, or more simply lens aberrations, and also 
phase aberrations caused by atmospheric turbulences in astronomical imaging [Tyson 1991]. These usually 
integrate secondary optical assemblies into the system, such as .ber optics [Kapany and Hopkins 1957], 
prisms [Inderhees 1973], lenslet arrays [Hanrahan and Ng 2006; Brady 16  and Hagen 2009], coded attenuation 
masks [Pandharkar et al. 2010], or spatial light modulators [Tyson 1991] and oftentimes require computational 
processing of the measured data. 5.3 Extended Depth of Field Photography Depth of .eld (DOF), that is 
a depth-dependent (de)focus of a pictured scene, plays an important role in photography. Controlled focus 
and defocus can be useful for highlighting objects of interest, such as people in a portrait where the 
background is blurred. For most applications, however, all-focused imagery is desirable. Ideally, a photographer 
should be able to refocus or completely remove all defocus as a post processing step in an image editing 
software or directly on the camera. While this is one of the main applications for light .elds, as discussed 
in Section 4, in this section we explore alternative focus modulation approaches that do not directly 
capture the full 4D light .eld. Although the depth-dependent size of the point spread function (PSF) 
or circle-of-confusion is effected by a variety of parameters, the most important ones are the aperture 
size and the depth expansion of the photographed scene. Larger apertures result in shallower depths of 
.eld but allow more light to reach the sensor, thereby decreasing the noise level. While a shallow DOF 
is often undesirable, it is also unavoidable in many situations where a low noise level is more important. 
Removing the DOF blur in a standard photograph is a dif.cult problem because it requires a deconvolution 
of the image with a spatially varying PSF. The PSF shape corresponds to that of the camera aperture, 
which can usually be well approximated with a Gaussian distribution; unfortunately, a deconvolution with 
a Gaussian is an ill-posed inverse problem, because high frequencies are irreversibly lost in the image 
capture. Applying natural image priors can improve reconstructions (see e.g., [Levin et al. 2007b]). 
The spatially-varying PSF size is directly proportional to the depth of the scene, which is in most cases 
unknown. A common approach to alleviate this problem is to mechanically or optically modify the depth-dependent 
PSF of the imaging system so that it becomes depth-invariant resulting in a reconstruction that only 
requires a spatially invariant deconvolution, which is much easier and does not require knowledge of 
the scene depth. One family of techniques that only requires a single shot to capture a scene with a 
depth-invariant PSF is called Focal Sweep. Here, the PSF modulation is achieved by moving the object 
[H¨ausler 1972] or the sensor [Nagahara et al. 2008] during the exposure time, or by exploiting the wavelength-dependency 
of the PSF to multiplex multiple focal planes in the scene onto a single sensor image [Cossairt and Nayar 
2010]. Alternatively, the apertures of the imaging system can be coded with cubic phase plates [Dowski 
and Cathey 1995] or other phase masks [Ojeda-Castaneda et al. 2005; Ben-Eliezer et al. 2005; Chi and 
George 2001], diffusers [Garcia-Guerrero et al. 2007; Cossairt et al. 2010], attenuation patterns [Levin 
et al. 2007a], polarization .lters [Chi et al. 2006], or multi-focal elements [Levin et al. 2009]. All 
of the above listed focal sweep and coded aperture approaches optically modify the PSF of the optical 
system for an extended DOF. The captured images usually need to be post-processed, for instance by applying 
a deconvolution. An analysis of quality criteria of attenuation-based aperture masks for defocus deblurring 
was presented by Zhou and Nayar [Zhou and Nayar 2009]; this analysis was extended to also consider PSF 
invertibility [Baek 2010]. Focal Stacks are series of images of the same scene, where the focal plane 
differs for each photograph in the se­quence. A single, focused image can be composited by selecting 
the best-focused match in the stack for each image region [Pieper and Korpel 1983]. The optimal choice 
of parameters, including focus and aperture, for the images in a focal stack are well established [Hasinoff 
et al. 2009; Hasinoff and Kutulakos 2008]. Capturing a focal stack with a large-scale high-resolution 
camera was implemented by Ben-Ezra [Ben-Ezra 2010]. Kutulakos and Hasinoff [Ku­ tulakos and Hasinoff 
2009] proposed to multiplex a focal stack into a single sensor image in a similar fashion as color .lter 
arrays multiplex different color channels into a RAW camera image. However, to the knowledge of the authors, 
this camera has not yet been built. Green et al. [Green et al. 2007] split the aperture of a camera using 
circular mirrors and multiplex the result into 17  Figure 7: Multiple frames of a .ying bird multiplexed 
into a single photograph (left). These kinds of photographs were shot with a photographic gun (right) 
by Etienne-Jules Marey as early as 1882. different regions of a single photograph. In principle, this 
approach captures multiple frames with varying aperture settings at a reduced spatial resolution in a 
single snapshot. Other applications for .exible focus imaging include 3D shape reconstruction with shape 
from (de)focus (see e.g. [Nayar and Nakagawa 1994; Zhou et al. 2009]) or Confocal Stereo [Hasinoff and 
Kutulakos 2006; Hasinoff and Kutulakos 2009], video matting [McGuire et al. 2005], and extended depth 
of .eld projection [Grosse et al. 2010].  6 Multiplexing Time Capturing motion and other forms of movement 
in photographs has been pursued since the invention of the da­guerreotype. Early pioneers in this .eld 
include Eadweard Muybridge (e.g. [Muybridge 1957]) and Etienne-Jules Marey (e.g. [Braun 1992]). As illustrated 
in Figure 7, much of the early work on picturing time focused on the study of anatomy and locomotion 
of animals and humans; photographic apparatuses were usually custom built at that time (Fig. 7, right). 
In this section, we discuss two classes of techniques for picturing motion: image capture at temporal 
resolutions that are signi.cantly lower (Sec. 6.1) or higher (Sec. 6.2) than the resolving capabilities 
of the human visual system and approaches for joint optical and computational motion deblurring (Sec. 
6.3). 6.1 Time Lapse Photography Photographing scenes at very low temporal sampling rates is usually 
referred to as time lapse photography. Tech­nically, time lapses can simply be acquired by taking multiple 
photographs from the same or a very close camera position at larger time intervals and assembling them 
in a video. In order to avoid temporal aliasing, or in sim­pler terms provide naturally looking motion, 
the exposure times should ideally be as long as the interval between successive shots. Timothy Allen, 
photographer for the BBC, provides a very informative tutorial on time lapse pho­tography on his website 
[Allen 2010]. The BBC has produced a number of astounding time lapse videos, including many scenes in 
their Planet Earth and Life series. 6.2 High-Speed Imaging Analog high-speed .lm cameras have been developed 
throughout the last century. A variety of technologies exist that expose .lm at very high speeds including 
mechanical movement through temporally registered pins and rotating prisms or mirrors. For a detailed 
discussion of the history of high-speed photography, applications, and the state of the art about nine 
years ago, the reader is referred to the book by Ray [Ray 2002]. 18 Single Sensor Approaches Today, 
high-speed digital cameras are commercially available. Examples are the Phantom Flex by Vision Re­search 
[Research 2010], which can capture up to 2,570 frames per second (fps) at HD resolution, and the FAST-CAM 
SA5 by Photron, which captures 7,500 fps at megapixel resolution or up to one million frames per second 
at a reduced resolution (64 × 16 pixels) [Photron 2010]; both cameras employ CMOS sensor technology. 
A modi.ed CCD is used in the HyperVision HPV-2 by Shimadzu [Shimadzu 2010], which operates at one million 
fps for an image resolution of 312 × 260 pixels. The Dynamic Photomechanics Laboratory at the University 
of Rhode Island (mcise.uri.edu/dpml/facilities.html) houses an IMACON 468-MkII digital camera operating 
at 200 million fps, but exact speci.cations of that camera are unknown to the authors. With the introduction 
of Casio s Exilim camera series (exilim.casio.com), which records low resolution videos at up to 1,000 
fps, high-speed cameras have entered the consumer market. An alternative to high-speed sensors is provided 
by Assorted Pixels [Narasimhan and Nayar 2005], where spatial resolution is traded for temporal resolution 
by measuring spatially interleaved, temporally staggered exposures on a sensor. This approach is very 
similar to what standard color .lter arrays do to acquire color information (see Sec. 3.2). While this 
concept was initially only theoretical, it has recently been implemented by aligning a digital micromirror 
device (DMD) with a CCD sensor [Bub et al. 2010]. Alternatively, the sensor readout could be con­ trolled 
on a per-pixel basis, as for instance provided by non-destructive sensor readout (e.g., [Semiconductor 
2010]). Reddy et al. [Reddy et al. 2011] built an LCOS-based camera prototype that modulates the exposure 
of each pixel randomly throughout the exposure time. In combination with a non-linear sparse reconstruction 
algorithm, the 25 fps prototype has been shown to capture imagery with up to 200 frames per second without 
loss of spatial resolution by exploiting sparsity in the spatio-temporal volume. Coded rolling shutters 
[Gu et al. 2010] have the potential to implement this concept on a per-scanline basis. Agrawal et al. 
[Agrawal et al. 2010b] demonstrated how a pinhole in the aperture plane of a camera, which moves throughout 
the exposure time, allows the captured data to be adaptively re-interpreted. For this purpose, temporal 
light variation is directly encoded in the different views of the light .eld that is simultaneously acquired 
with a Sum­of-Sinusoids (SoS) attenuation-mask (see Sec. 4.3) in a single shot. Temporal variation and 
different viewpoints cannot be separated in this approach. Multiple Devices Rather than photographing 
a scene with a single high-speed camera, multiple synchronized devices can be used. One of the most popular 
movie scenes that shows high-speed motion captured by a camera array is the bullet time effect in The 
Matrix. Here, a rig of digital SLR cameras, arranged along a virtual camera path, captures a scene at 
precisely controlled time steps so that a virtual, high-speed camera can be simulated that moves along 
the prede.ned path. The direct capture of high-speed events with camera arrays was scienti.cally discussed 
by Wilburn et al. [Wilburn et al. 2004; Wilburn et al. 2005]. In this approach, the exposure windows 
of the cameras are slightly staggered so that a high-speed video can be composed by merging the data 
of the individual cameras. Shechtman et al. [Shechtman et al. 2002; Shechtman et al. 2005] proposed to 
combine the output of multiple low-resolution video cameras for space-time super-resolution. Coded exposures 
have been shown to optimize temporal super-resolution from multi­camera arrays [Agrawal et al. 2010a] 
by alleviating the ill-posedness of the reconstruction. As required for spatial super-resolution (see 
Sec. 5.1), temporal super-resolution requires computationally expensive post-processing of the measured 
data. 19  High-Speed Illumination High-speed imagery can also be acquired by utilizing high-speed illumination. 
Harold Doc Edgerton [Project 2009] created this .eld by inventing electronic strobes and using them to 
depict very fast motions in a similar fashion as Eadweard Muybridge and Etienne-Jules Marey had done 
with more primitive, mechanical technologies decades before him. Today, high-speed illumination, in an 
attosecond time scale, is more conveniently achieved with lasers rather than stroboscopes [Baker et al. 
2006; Ray 2002]. Stroboscopic illumination can be used to compensate for rolling shutter effects and 
synchronize an array of consumer cameras [Bradley et al. 2009]. Narasimhan et al. [Narasimhan et al. 
2008] exploited the high-speed temporal dither­ ing patterns of DLP-based illumination for a variety 
of vision problems, including photometric stereo and range imaging. Coded strobing, by either illumination 
or controlled sensor readout, in combination with reconstructions developed in the compressive sensing 
community, allows high-speed periodic events to be acquired [Veeraraghavan et al. 2011]. Another high-speed 
imaging approach that is inspired by compressive sensing was proposed by Gupta et al. [Gupta et al. 2010]. 
Here, a 3D spatio-temporal volume is adaptively encoded with a .xed voxel budget. This approach encodes 
fast motions with a high temporal, but lower spatial resolution, while the spatial resolution in static 
parts of the scene is maximized. A co-axial projector-camera pair was used to simulate controllable per-pixel 
exposures. Exotic Ultra High-Speed Imaging Other imaging devices that capture ultra high-speed events 
are streak cameras. Rather than recording standard 2D photographs, these devices capture 2D images that 
encode spatial information in one dimension and temporal light variation in the other. These systems 
are usually combined with pulsed laser illumination and operate at temporal resolutions of about one 
hundred femtoseconds [Hamamatsu 2010], corresponding to a framerate of ten trillion fps. Another exotic 
ultra high-speed imager is the STREAM camera [Goda et al. 2009], which optically converts a 2D image 
into a serial time-domain waveform that is recorded with a single high-speed photodiode at 6.1 million 
fps.  6.3 Motion Deblurring Motion deblurring has been an active area of research over the last few 
decades. It is well known that deblur­ring is an ill-posed problem, which is why many algorithms apply 
regularizers [Richardson 1972; Lucy 1974] or natural image statistics (e.g., [Fergus et al. 2006a; Levin 
et al. 2007b]) to solve the problem robustly. Usually, high­ frequency spatio-temporal image information 
is irreversibly lost in the image formation because a standard shutter along with the sensor integration 
time create a temporal rect-.lter which has many zero-crossings in the Fourier domain. In this section, 
we review coded image acquisition techniques that optically modify the motion PSF so that the reconstruction 
becomes a well-posed problem. Coded Single Capture Approaches One of the earliest approaches of coded 
temporal sampling was introduced by Raskar et al. [Raskar et al. 2006] as the Fluttered Shutter. The 
motion PSF in a single sensor image was modi.ed by mounting a programmable liquid crystal element in 
front of the camera lens and modulating its transmission over the exposure time with optimized binary 
codes. These codes were designed to preserve high temporal frequencies, so that the required image deconvolution 
becomes well-posed. Optimized codes and an algorithm for the problem of combined motion deblurring and 
spatial super-resolution of moving objects with coded exposures were analyzed by Agrawal and Raskar [Agrawal 
and Raskar 2007]. Both approaches use programmable, attenuation-based shutters to apply the codes, thereby 
sacri.cing light transmission, and require a manual recti.cation of object motion in the captured images. 
Optimality criteria for motion PSF invertibility were extended to also allow high-quality PSF estimation 
[Agrawal and Xu 2009]; this automates the motion recti.cation step. 20  Figure 8: By varying the exposure 
time for successive frames in a video (left), multi-image deblurring (right) can be made invertible [Agrawal 
et al. 2009]. Inspired by approaches that create depth-invariant point spread functions (see Sec. 5.3), 
Levin et al.[Levin et al. 2008] showed how one-dimensional parabolic sensor motion during the exposure 
time can achieve a motion-invariant PSF along the line of sensor motion. Compared to attenuation-coded 
temporal exposures, this method does not sacri.ce light transmission but requires prior knowledge of 
object motion and only works along one spatial direction. The optimal tradeoffs for single image deblurring 
from either attenuation-coded exposures or sensor motion, in terms of signal-to-noise ratio of the reconstructions, 
were analyzed by Agrawal and Raskar [Agrawal and Raskar 2009]. Image Sequences or Multiple Cameras Synthetic 
Shutter Speed Imaging [Telleen et al. 2007] combines multiple sharp but noisy images captured with short 
exposure times. The resulting image has a lower noise level; motion blur is reduced by aligning all images 
before they are fused. A Hybrid Camera for motion deblurring, consisting of a rig of two cameras, was 
introduced by Ben-Ezra and Nayar [Ben-Ezra and Nayar 2003; Ben-Ezra and Nayar 2004]. One of the cameras 
captures the scene at a high temporal, but low spatial resolution; the output of this camera is used 
to estimate the motion PSF, which in turn is used to deblur the high-quality image captured by the other 
camera. Improvements of reconstructions for hybrid cameras have recently been presented [Tai et al. 2010]. 
Hybrid camera architectures also provide the opportunity to simultaneously deblur captured images and 
reconstruct a high-resolution depth map of the photographed scene [Li et al. 2008a]. Motion blur in a 
video can be synthetically removed by applying super-resolution techniques to multiple successive frames 
[Bascle et al. 1996]. Agrawal et al. [Agrawal et al. 2009] showed that improved results can be achieved 
by modulating the exposure times for successive frames in video sequences so that a reconstruction from 
multiple images becomes a well-posed problem. An example for this is shown in Figure 8.  7 Acquisition 
of Further Light Properties In this section, we review acquisition approaches for light properties that 
are not considered dimensions of the plenoptic function, but are closely related in terms of capture 
or application. 21 7.1 Polarization Polarization is an inherent property of the wave nature of light 
[Collett 2005], which is why we treat it separately from the plenoptic function. Generally, polarization 
describes the oscillation of a wave traveling through space in the transverse plane, perpendicular to 
the direction of propagation. Linear polarization refers to transverse oscillation along a line, whereas 
spherical or elliptical polarization describe corresponding oscillation trajectories. Although some animals, 
including mantis shrimp [Marshall and Oberwinkler 1999], cephalopods (squid, octopus, cuttle.sh) [M¨athger 
et al. 2009], and insects [Wehner 1976], are reported to have photoreceptors that are sensitive to polarization, 
standard solid state sensors are not. The most straightforward way of capturing this information is by 
taking multiple photographs of a scene with different polarizing .lters mounted in front of the camera 
lens. These .lters are standard practice in photography to reduce specular re.ections, increase the contrast 
of outdoor images, and improve the appearance of vegetation. Alternatively, this kind of information 
can be captured using polarization .lter arrays [Schechner and Nayar 2003a] which, similar to generalized 
mosaics [Schechner and Nayar 2005], require multiple photographs to be captured. Recently, polarized 
illumination [Ghosh et al. 2010] has been shown to have the potential to acquire all Stokes parameters 
necessary to describe polarization. Applications for the acquisition of polarized light include image 
dehazing [Schechner et al. 2001; Schechner et al. 2003; Namer and Schechner 2005], improved underwater 
vision [Schechner and Karpel 2004; Schechner and Karpel 2005], specular highlight removal [Wolff and 
Boult 1991; Nayar et al. 1993; M¨uller 1996; Umeyama and Godin 2004], shape [Miyazaki et al. 2003; Miyazaki 
et al. 2004; Atkinson and Hancock 2005] and BRDF [Atkinson and Hancock 2008] estimation, material classi.cation 
[Chen and Wolff 1998], light source separation [Cula et al. 2007], surface normal acquisition [Ma et 
al. 2007], surface normal and refractive index estimation [Sadjadi 2007; Ghosh et al. 2010], separation 
of transparent layers [Schechner et al. 1999], and optical communication [Siddiqui and Zhou 1991; Sch¨ 
onfelder 2003; Yao 2008]. 7.2 Phase Imaging A variety of techniques has been proposed to visualize and 
quantify phase retardation in transparent microscopic organisms [Murphy 2001]. Many of these phase-contrast 
imaging approaches, such as Zernike phase contrast and differential interference contrast (DIC), require 
coherent illumination and are qualitative rather than quantitative. This implies that changes in phase 
or refractive events are encoded as intensity variations in captured images, but remain indistinguishable 
from the intensity variations caused by absorption in the medium. Quantitative approaches exist [Barone-Nugent 
et al. 2002], but require multiple images, are subject to a paraxial approximation, and are limited to 
orthographic cameras. Schlieren and shadowgraph photography are alternative, non-intrusive imaging methods 
for dynamically changing refractive index .elds. These techniques have been developed in the .uid imaging 
community over the past century, with substantial improvements in the 1940s. An extensive overview of 
different optical setups and the historic evo­lution of Schlieren and Shadowgraph imaging can be found 
in the book by Settles [Settles 2001]. As illustrated in Figure 9, recently proposed applications of 
Schlieren imaging include the tomographic reconstruction of transpar­ ent gas .ows using a camera array 
[Atcheson et al. 2008] and the capture of refractive events with 4D light .eld probes [Wetzstein et al. 
2011d]. 7.3 LIDAR and Time-of-Flight LIDAR (LIght Detection and Ranging) [Wandinger 2005] is a technology 
that measures the time of a laser pulse from transmission to detection of the re.ected signal. It is 
similar to radar, but uses different wavelengths of the electromagnetic spectrum, typically in the infrared 
range. Combining such a pulsed laser with optical scanners and a positioning system such as GPS allows 
very precise depth or range maps to be captured, even from airplanes. Over­laying range data with standard 
photographs provides a powerful tool for aerial surveying, forestry, oceanography, 22  Figure 9: Schlieren 
imaging for tomographic gas reconstruction (left) and capture of refractive events using light .eld probes 
(right). (Figures reproduced from [Atcheson et al. 2008] and [Wetzstein et al. 2011d].) agriculture, 
and geology. Flash LIDAR [Lange and Seitz 2001] or time-of-.ight cameras [Kolb et al. 2010] capture a 
photograph and a range map simultaneously for all pixels. Although spatial resolution of the range data 
is often poor, these cameras usually capture at video rates. Streak cameras operate in the picosecond 
[Campillo and Shapiro 1983] or even attosecond [Itatani et al. 2002] range and usually capture 2D images, 
where one dimension is spatial light variation and the other dimension is time-of­.ight. These cameras 
have recently been used to reveal scene information outside the line of sight of a camera, literally 
behind corners [Kirmani et al. 2009; Pandharkar et al. 2011].  8 Discussion and Conclusions In summary, 
we have presented a review of approaches to plenoptic image acquisition. We have used an intuitive categorization 
based on plenoptic dimensions and hardware setups for the acquisition. Alternative categorizations may 
be convenient for the discussion of the more general .eld of computational photography [Raskar and Tumblin 
2009]. The increasingly growing number of publications in this .eld is one of the main motivations for 
this state of the art report, which focuses speci.cally on joint optical encoding and computational reconstruction 
approaches for the acquisition of the plenoptic function. Based on the literature reviewed in this report, 
we make the following observations: most of the discussed approaches either assume that some plenoptic 
dimensions are constant, such as time in sequential image capture, or otherwise restricted, for instance 
spatially band-limited in single sensor inter­leaved capture; these assumptions result in .xed plenoptic 
resolution tradeoffs,  however, there are strong correlations between the dimensions of the plenoptic 
function; so far these are almost exclusively exploited in color demosaicing,  therefore, natural image 
statistics that can be used as priors in computational image reconstruction and in­corporate all plenoptic 
dimensions with their correlations are desirable; so are sophisticated reconstruction techniques employing 
these.  It has recently been shown that all approaches for interleaved plenoptic sampling on a single 
sensor, including spatial [Narasimhan and Nayar 2005] and Fourier multiplexing [Veeraraghavan et al. 
2007; Lanman et al. 2008] methods, can be cast into a common reconstruction framework [Ihrke et al. 2010a]. 
While the exploitation of 23 correlations between plenoptic dimensions, for example spatial and spectral 
light variation, is common practice for imaging with color .lter arrays and subsequent demosaicing, there 
is signi.cant potential to develop similar techniques for demosaicing other multiplexed plenoptic information, 
for instance light .elds [Levin and Durand 2010]. Priors for the correlations between plenoptic dimensions 
can be very useful for plenoptic super-resolution or gener­ally more sophisticated reconstructions. These 
could, for instance, be derived from plenoptic image databases [Wet­ zstein et al. 2011c]; we show examples 
of such data in Figures 10, 11, and 12. Another promising avenue of future research is adaptive imaging. 
Precise control of the sampled plenoptic infor­mation is the key for .exible and adaptive reconstruction. 
An intuitive next step for sophisticated imaging with respect to temporal light variation and dynamic 
range is pixel-precise, non-destructive sensor readout. In the future, however, it is desirable to being 
able to control the optical modulation of all plenoptic dimensions. While most of the reviewed approaches 
make .xed plenoptic resolution tradeoffs, some already show a glimpse of the potential of adaptive re-interpretation 
of captured data [Agrawal et al. 2010b]. Ideas from the compressive sensing community (see e.g., [Cand` 
es et al. 2006]) have also started to play an important role in adaptive plenoptic imaging [Gupta et 
al. 2010; Veeraraghavan et al. 2011]. In these approaches optical coding is combined with content adaptive 
reconstructions that can dynamically trade higher-dimensional resolution in post-processing to best represent 
the recorded data. Picturing space, color, time, directions, and other light properties has been of great 
interest to science and art alike for centuries. With the emergence of digital light sensing technology 
and computational processing power, many new and exciting ways to acquire some of the visual richness 
surrounding us have been presented. We have, however, only begun to realize how new technologies allow 
us to transcend the way evolution has shaped visual perception for different creatures on this planet. 
 24  25  References ADAMS, A., TALVALA, E.-V., PARK, S. H., JACOBS, D. E., AJDIN, B., GELFAND, N., 
DOLSON, J., VAQUERO, D., BAEK, J., TICO, M., LENSCH, H. P. A., MATUSIK, W., PULLI, K., HOROWITZ, M., 
AND LEVOY, M. 2010. The Frankencamera: an Experimental Platform for Computational Photography. ACM Trans. 
Graph. (SIGGRAPH) 29, 29:1 29:12. ADELSON, E. H., AND BERGEN, J. R. 1991. The Plenoptic Function and 
the Elements of Early Vision. In Computational Models of Visual Processing, MIT Press, 3 20. ADELSON, 
E., AND WANG, J. 1992. Single Lens Stereo with a Plenoptic Camera. IEEE Trans. PAMI 14, 2, 99 106. AGGARWAL, 
M., AND AHUJA, N. 2004. Split Aperture Imaging for High Dynamic Range. Int. J. Comp. Vis. 58, 1, 7 17. 
AGRAWAL, A., AND RASKAR, R. 2007. Resolving Objects at Higher Resolution from a Single Motion-Blurred 
Image. In Proc. IEEE CVPR, 1 8. AGRAWAL, A., AND RASKAR, R. 2009. Optimal Single Image Capture for Motion 
Deblurring. In Proc. IEEE CVPR, 1 8. AGRAWAL, A., AND XU, Y. 2009. Coded Exposure Deblurring: Optimized 
Codes for PSF Estimation and Invert­ibility. In Proc. IEEE CVPR, 1 8. AGRAWAL, A., XU, Y., AND RASKAR, 
R. 2009. Invertible Motion Blur in Video. ACM Trans. Graph. (Siggraph) 28, 3, 95. AGRAWAL, A., GUPTA, 
M., VEERARAGHAVAN, A., AND NARASIMHAN, S. 2010. Optimal Coded Sampling for Temporal Super-Resolution. 
In Proc. IEEE CVPR, 374 380. AGRAWAL, A., VEERARAGHAVAN, A., AND RASKAR, R. 2010. Reinterpretable Imager: 
Towards Variable Post-Capture Space, Angle and Time Resolution in Photography. In Proc. Eurographics, 
1 10. ALLEN, T., 2010. Time Lapse Tutorial. http://timothyallen.blogs.bbcearth.com/2009/02/24/time-lapse-photography/. 
ALLEYSON, D., S ¨ ERAULT, J. 2005. Linear Demosaicing inspired by the Human Visual USSTRUNK, S., AND 
H´ System. IEEE Trans. Im. Proc. 14, 4, 439 449. ASHDOWN, I. 1993. Near-.eld photometry: A new approach. 
Journal of the Illuminating Engineering Society 22, 1, 163 180. ASHOK, A., AND NEIFELD, M. A. 2007. Pseudorandom 
Phase Masks for Superresolution Imaging from Subpixel Shifting. Applied Optics 46, 12, 2256 2268. ASHOK, 
A., AND NEIFELD, M. A. 2010. Compressive Light Field Imaging. In Proc. SPIE 7690, 76900Q. ATCHESON, B., 
IHRKE, I., HEIDRICH, W., TEVS, A., BRADLEY, D., MAGNOR, M., AND SEIDEL, H. 2008. Time-resolved 3D Capture 
of Non-Stationary Gas Flows. ACM Trans. Graph. (SIGGRAPH Asia) 27, 5, 132. ATKINSON, G., AND HANCOCK, 
E. 2005. Multi-view surface reconstruction using polarization. In Proc. ICCV, vol. 1, 309 316. ATKINSON, 
G. A., AND HANCOCK, E. R. 2008. Two-dimensional BRDF Estimation from Polarisation. Comput. Vis. Image 
Underst. 111, 2, 126 141. BABACAN, S. D., ANSORGE, R., LUESSI, M., MOLINA, R., AND KATSAGGELOS, A. K. 
2009. Compressive Sensing of Light Fields. In Proc. ICIP, 2313 2316. 26 BAEK, J. 2010. Transfer Ef.ciency 
and Depth Invariance in Computational Cameras. In Proc. ICCP, 1 8. BAKER, S., AND KANADE, T. 2002. Limits 
on Super-Resolution and How to Break Them. IEEE Trans. PAMI 24, 1167 1183. BAKER, S., ROBINSON, J. S., 
HAWORTH, C. A., TENG, H., SMITH, R. A., CHIRILA, C. C., LEIN, M., TISCH, J. W. G., AND MARANGOS, J. P. 
2006. Probing Proton Dynamics in Molecules on an Attosecond Time Scale. Science 312, 5772, 424 427. BARONE-NUGENT, 
E. D., BARTY, A., AND NUGENT, K. A. 2002. Quantitative Phase-Amplitude Microscopy I: Optical Microscopy. 
Journal of Microscopy 206, 3, 194 203. BASCLE, B., BLAKE, A., AND ZISSERMAN, A. 1996. Motion Deblurring 
and Super-resolution from an Image Sequence. In Proc. ECCV, 573 582. BAYER, B. E., 1976. Color imaging 
array. US Patent 3,971,065. BEN-ELIEZER, E., MAROM, E., KONFORTI, N., AND ZALEVSKY, Z. 2005. Experimental 
Realization of an Imaging System with an Extended Depth of Field. Appl. Opt. 44, 11, 2792 2798. BEN-EZRA, 
M., AND NAYAR, S. 2003. Motion Deblurring using Hybrid Imaging. In Proc. IEEE CVPR, 657 664. BEN-EZRA, 
M., AND NAYAR, S. 2004. Motion-based Motion Deblurring. IEEE Trans. PAMI 26, 6, 689 698. BEN-EZRA, M., 
ZOMET, A., AND NAYAR, S. 2005. Video Superresolution using Controlled Subpixel Detector Shifts. IEEE 
Trans. PAMI 27, 6, 977 987. BEN-EZRA, M. 2010. High Resolution Large Format Tile-Scan Camera. In Proc. 
IEEE ICCP, 1 8. BEN-EZRA, M. 2011. A Digital Gigapixel Large-Format Tile-Scan Camera. IEEE Computer Graphics 
and Appli­cations 31, 49 61. BISHOP, T., ZANETTI, S., AND FAVARO, P. 2009. Light-Field Superresolution. 
In Proc. ICCP, 1 9. BODKIN, A., SHEINIS, A., NORTON, A., DALY, J., BEAVEN, S., AND WEINHEIMER, J. 2009. 
Snapshot Hyper­spectral Imaging -the Hyperspectral Array Camera. In Proc. SPIE 7334, 1 11. BOLLES, R. 
C., BAKER, H. H., AND MARIMONT, D. H. 1987. Epipolar-plane image analysis: An approach to determining 
structure from motion. IJCV 1, 1, 7 55. BORMAN, S., AND STEVENSON, R. 1998. Super-resolution from image 
sequences -A review. In Proc. Symposium on Circuits and Systems, 374 378. BRADLEY, D., ATCHESON, B., 
IHRKE, I., AND HEIDRICH, W. 2009. Synchronization and Rolling Shutter Compensation for Consumer Video 
Camera Arrays. In Proc. ProCams, 1 8. BRADY, D. J., AND HAGEN, N. 2009. Multiscale Lens Design. Optics 
Express 17, 13, 10659 10674. BRAUN, M. 1992. Picturing Time: The Work of Etienne-Jules Marey (1830-1904). 
The University of Chicago Press. BUB, G., TECZA, M., HELMES, M., LEE, P., AND KOHL, P. 2010. Temporal 
Pixel Multiplexing for Simultaneous High-Speed, High-Resolution Imaging. Nature Methods 7, 209 211. CAMPILLO, 
A., AND SHAPIRO, S. 1983. Picosecond Streak Camera Fluorometry-A Review. Journal of Quantum Electronics 
19, 4, 585 603. CAND ` ES, E., ROMBERG, J., AND TAO, T. 2006. Robust uncertainty principles: Exact signal 
reconstruction from highly incomplete frequency information. IEEE Trans. Information Theory 52, 2, 489 
509. 27 CAO, X., TONG, X., DAI, Q., AND LIN, S. 2011. High Resolution Multispectral Video Capture with 
a Hybrid Camera System. In Proc. IEEE CVPR, 1 8. CARRANZA, J., THEOBALT, C., MAGNOR, M. A., AND SEIDEL, 
H.-P. 2003. Free-viewpoint video of human actors. ACM Transactions on Graphics (TOG) 22, 3, 569 577. 
CHEN, S. E., AND WILLIAMS, L. 1993. View interpolation for image synthesis. In Proc. ACM SIGGRAPH, 279 
288. CHEN, H., AND WOLFF, L. B. 1998. Polarization Phase-Based Method For Material Classi.cation In Computer 
Vision. IJCV 28, 1, 73 83. CHI, W., AND GEORGE,N.2001.ElectronicImagingusingaLogarithmicAsphere. Optics 
Letters 26, 12, 875 877. CHI, W., CHU, K., AND GEORGE, N. 2006. Polarization Coded Aperture. Optics Express 
14, 15, 6634 6642. COLLETT, E. 2005. Field Guide to Polarization. SPIE Press. COSSAIRT, O., AND NAYAR, 
S. K. 2010. Spectral Focal Sweep: Extended Depth of Field from Chromatic Aberrations. In Proc. ICCP, 
1 8. COSSAIRT, O., ZHOU, C., AND NAYAR, S. K. 2010. Diffusion Coded Photography for Extended Depth of 
Field. ACM Trans. Graph. (Siggraph) 29, 3, 31. COSSAIRT, O., MIAU, D., AND NAYAR, S. K. 2011. Gigapixel 
Computational Imaging. In Proc. ICCP. CRI INC, 2009. VariSpec Liquid Crystal Tunable Filters. www.channelsystems.ca/Attachments/VariSpec/VariSpec-Brochure.pdf. 
CULA, O. G., DANA, K. J., PAI, D. K., AND WANG, D. 2007. Polarization Multiplexing and Demultiplexing 
for Appearance-Based Modeling. IEEE Trans. Pattern Anal. Mach. Intell. 29, 2, 362 367. DEBEVEC, P. E., 
AND MALIK, J. 1997. Recovering High Dynamic Range Radiance Maps from Photographs. In Proc. ACM Siggraph, 
369 378. DEBEVEC, P., HAWKINS, T., TCHOU, C., DUIKER, H.-P., SAROKIN, W., AND SAGAR, M. 2000. Acquiring 
the Re.ectance Field of a Human Face. In Proc. ACM SIGGRAPH, 145 156. DEBEVEC, P. 2002. Image-Based Lighting. 
IEEE Computer Graphics and Applications, 26 34. DESCOUR, M., AND DERENIAK, E. 1995. Computed-tomography 
Imaging Spectrometer: Experimental Calibra­tion and Reconstruction Results. Applied Optics 34, 22, 4817 
4826. DESCOUR, M. E., C.E.VOLIN, , DERENIAK, E., AND K.J.THOME. 1997. Demonstration of a High-Speed Nonscanning 
Imaging Spectrometer. Optics Letters 22, 16, 1271 1273. DOWSKI, E., AND CATHEY, T. 1995. Extended Depth 
of Field through Wave-Front Coding. Applied Optics 34, 11, 1859 1866. DU, H., TONG, X., CAO, X., AND 
LIN, S. 2009. A Prism-Based System for Multispectral Video Acquisition. In Proc. IEEE ICCV, 175 182. 
EASTMAN KODAK COMPANY. PhotoCD PCD0992. http://r0k.us/graphics/kodak. FAIRCHILD, M. D. 2005. Color Appearance 
Models. John Wiley and Sons. FERGUS, R., SINGH, B., HERTZMANN, A., ROWEIS, S. T., AND FREEMAN, W. T. 
2006. Removing Camera Shake from a Single Photograph. ACM Trans. Graph. 25, 787 794. FERGUS, R., TORRALBA, 
A., AND FREEMAN, W. T. 2006. Random Lens Imaging. Tech. Rep. MIT-CSAIL-TR­2006-058, National Bureau of 
Standards. 28 FLETCHER-HOLMES, D. W., AND HARVEY, A. R. 2005. Real-Time Imaging with a Hyperspectral 
Fovea. J. Opt. A: Pure Appl. Opt. 7, S298 S302. FOVEON, 2010. X3 Technology. www.foveon.com. GABOR, D. 
1948. A new microscopic principle. Nature, 777 778. GAIGALAS, A. K., WANG, L., HE, H.-J., AND DEROSE, 
P. 2009. Procedures for Wavelength Calibration and Spectral Response Correction of CCD Array Spectrometers. 
Journal of Research of the National Institute of Standards and Technology 114, 4, 215 228. GAO, L., KESTER, 
R.T., AND TKACZYK,T.S.2009.CompactImageSlicingSpectrometer(ISS)forhyperspectral .uorescence microscopy. 
Optics Express 17, 15, 12293 12308. GARCIA-GUERRERO, E. E., MENDEZ, E. R., AND LESKOVA, H. M. 2007. Design 
and Fabrication of Random Phase Diffusers for Extending the Depth of Focus. Optics Express 15, 3, 910 
923. GAT, N. 2000. Imaging Spectroscopy Using Tunable Filters: A Review. In Proc. SPIE 4056, 50 64. GEHM, 
M. E., JOHN, R., BRADY, D. J., WILLETT, R. M., AND SCHULZ, T. J. 2007. Single-Shot Compressive Spectral 
Imaging with a Dual-Disperser Architecture. Optics Express 15, 21, 14013 14027. GEORGIEV, T., ZHENG, 
C., NAYAR, S., CURLESS, B., SALESIN, D., AND INTWALA, C. 2006. Spatio-angular resolution trade-offs in 
integral photography. In Proc. EGSR, 263 272. GEORGIEV, T., INTWALA, C., BABACAN, S., AND LUMSDAINE, 
A. 2008. Uni.ed Frequency Domain Analysis of Light.eld Cameras. In Proc. ECCV, 224 237. GERSHUN, A. 1936. 
The light .eld. Journal of Mathematics and Physics XVIII, 51 151. Translated by P. Moon and G. Timoshenko. 
GHOSH, A., CHEN, T., PEERS, P., WILSON, C. A., AND DEBEVEC, P. 2010. Circularly polarized spherical illumination 
re.ectometry. ACM Trans. Graph. (Siggraph Asia) 27, 5, 134. GODA, K., TSIA, K. K., AND JALALI, B. 2009. 
Serial Time-Encoded Ampli.ed Imaging for Real-Time Observa­tion of Fast Dynamic Phenomena. Nature, 458, 
1145 1149. GORMAN, A., FLETCHER-HOLMES, D. W., AND HARVEY, A. R. 2010. Generalization of the Lyot Filter 
and its Application to Snapshot Spectral Imaging. Optics Express 18, 6, 5602 5608. GORTLER, S., GRZESZCZUK, 
R., SZELINSKI, R., AND COHEN, M. 1996. The Lumigraph. In Proc. ACM Siggraph, 43 54. GOTTESMAN, S. R., 
AND FENIMORE, E. E. 1989. New family of binary arrays for coded aperture imaging. Applied Optics 28, 
20, 4344 4352. GREEN, P., SUN, W., MATUSIK, W., AND DURAND, F. 2007. Multi-Aperture Photography. In Proc. 
ACM Siggraph, 68. GROSSBERG, M. D., AND NAYAR, S. K. 2003. High Dynamic Range from Multiple Images: Which 
Exposures to Combine. In Proc. ICCV Workshop CPMCV. GROSSE, M., WETZSTEIN, G., GRUNDH ¨ OFER, A., AND 
BIMBER, O. 2010. Coded Aperture Projection. ACM Trans. Graph. 29, 22:1 22:12. GU, J., HITOMI, Y., MITSUNAGA, 
T., AND NAYAR, S. K. 2010. Coded Rolling Shutter Photography: Flexible Space-Time Sampling. In Proc. 
IEEE ICCP, 1 8. 29 GUNTURK, B., GLOTZBACH, J., ALTUNBASAK, Y., SCHAFER, R., AND MERSEREAU, R. 2005. 
Demosaicking: Color Filter Array Interpolation in Single-Chip Digital Cameras. IEEE Signal Processing 
22, 1, 44 54. GUPTA, M., AGRAWAL, A., VEERARAGHAVAN, A., AND NARASIMHAN, S. G. 2010. Flexible Voxels 
for Motion-Aware Videography. In Proc. ECCV, 100 114. HALLE, M. W. 1994. Holographic stereograms as discrete 
imaging systems. In SPIE Practical Holography, 73 84. HAMAMATSU, 2010. Streak Systems. http://sales.hamamatsu.com/en/products/system-division/ultra-fast/streak-systems.php. 
HANRAHAN, P., AND NG, R. 2006. Digital Correction of Lens Aberrations in Light Field Photography. In 
International Optical Design Conference, 1 3. HARVEY, A. R., BEALE, J., GREENAWAY, A. H., HANLON, T. 
J., AND WILLIAMS, J. 2000. Technology Options for Imaging Spectrometry Imaging Spectrometry. In Proc. 
SPIE 4132, 13 24. HARVEY, A. R., FLETCHER-HOLMES, D. W., AND GORMAN, A. 2005. Spectral Imaging in a Snapshot. 
In Proc. SPIE 5694, 1 10. HASINOFF, S. W., AND KUTULAKOS, K. N. 2006. Confocal Stereo. In Proc. ECCV, 
620 634. HASINOFF, S. W., AND KUTULAKOS, K. N. 2008. Light-Ef.cient Photography. In Proc. of ECCV, 45 
59. HASINOFF, S. W., AND KUTULAKOS, K. N. 2009. Confocal Stereo. Int. J. Comp. Vis. 81, 1, 82 104. HASINOFF, 
S. W., KUTULAKOS, K. N., DURAND, F., AND FREEMAN, W. T. 2009. Time-Constrained Photogra­phy. In Proc. 
of ICCV, 333 340. HASINOFF, S. W., DURAND, F., AND FREEMAN, W. T. 2010. Noise-Optimal Capture for High 
Dynamic Range Photography. In Proc. IEEE CVPR, 1 8. H¨ AUSLER, G. 1972. A Method to Increase the Depth 
of Focus by Two Step Image Processing. Optics Communi­cations 6, 1, 38 42. HECHT, E. 2002. Optics, fourth 
edition. Addison Wesley. HIRAKAWA, K., AND PARKS, T. W. 2006. Joint Demosaicing and Denoising. IEEE Trans. 
Im. Proc. 15, 8, 2146 2157. HIRAKAWA, K., AND WOLFE, P. 2007. Spatio-Spectral Color Filter Array Design 
for Enhanced Image Fidelity. In Proc. ICIP, II 81 II 84. HIRAKAWA, K., AND WOLFE, P. 2008. Spatio-Spectral 
Color Filter Array Design for Optimal Image Recovery. IEEE Trans. Im. Proc. 17, 10, 1876 1890. HIRSCH, 
M., LANMAN, D., HOLTZMAN, H., AND RASKAR, R. 2009. BiDi Screen: a Thin, Depth-Sensing LCD for 3D Interaction 
using Light Fields. In ACM Trans. Graph. (SIGGRAPH Asia), 1 9. HIURA, S., MOHAN, A., AND RASKAR, R. 2009. 
Krill-eye : Superposition Compound Eye for Wide-Angle Imaging via GRIN Lenses. In Proc. OMNIVIS, 1 8. 
HORSTMEYER, R., EULISS, G., ATHALE, R., AND LEVOY, M. 2009. Flexible Multimodal Camera Using a Light 
Field Architecture. In Proc. ICCP, 1 8. HUNICZ, J., AND PIERNIKARSKI, D. 2001. Investigation of Combustion 
in a Gasoline Engine using Spectropho­tometric Methods. In Proc. SPIE 4516, 307 314. HUNT, R. W. G. 1991. 
Measuring Color, 3rd ed. Fountain Press. 30 IHRKE, I., STICH, T., GOTTSCHLICH, H., MAGNOR, M., AND SEIDEL, 
H. 2008. Fast incident light .eld acquisition and rendering. In Proc. of WSCG, 177 184. IHRKE, I., WETZSTEIN, 
G., AND HEIDRICH, W. 2010. A Theory of Plenoptic Multiplexing. In Proc. IEEE CVPR, 1 8. IHRKE, I., KUTULAKOS, 
K. N., LENSCH, H. P. A., MAGNOR, M., AND HEIDRICH, W. 2010. Transparent and Specular Object Reconstruction. 
Computer Graphics Forum 29, 8, 2400 2426. INDERHEES, J., 1973. Optical .eld curvature corrector. US patent 
3,720,454. ITATANI, J., QUR, F., YUDIN, G. L., IVANOV, M. Y., KRAUSZ, F., AND CORKUM, P. B. 2002. Attosecond 
Streak Camera. Physical Review Letters 88, 17, 173903. IVES, H., 1903. Parallax Stereogram and Process 
of Making Same. US patent 725,567. IVES, H. 1928. Camera for Making Parallax Panoramagrams. J. Opt. Soc. 
Amer. 17, 435 439. KANG, S. B., UYTTENDAELE, M., WINDER, S., AND SZELISKI, R. 2003. High Dynamic Range 
Video. In Proc. ACM Siggraph, 319 325. KANOLT, C. W., 1918. Parallax Panoramagrams. US patent 1,260,682. 
KAPANY, N. S., AND HOPKINS, R. E. 1957. Fiber Optics. Part III. Field Flatteners. JOSA 47, 7, 594 595. 
KINDZELSKII, A. L., YANG, Z. Y., NABEL, G. J., TODD, R. F., AND PETTY, H. R. 2000. Ebola Virus Secretory 
Glycoprotein (sGP) Diminishes Fc Gamma RIIIB-to-CR3 Proximity on Neutrophils. J. Immunol. 164, 953 958. 
KIRMANI, A., HUTCHISON, T., DAVIS, J., AND RASKAR, R. 2009. Looking Around the corner using Transient 
Imaging. In Proc. ICCV, 1 8. KOLB, A., BARTH, E., KOCH, R., AND LARSEN, R. 2010. Time-of-Flight Cameras 
in Computer Graphics. Computer Graphics Forum 29, 1, 141 159. KOPF, J., UYTTENDAELE, M., DEUSSEN, O., 
AND COHEN, M. F. 2007. Capturing and Viewing Gigapixel Images. ACM Trans. on Graph. (SIGGRAPH) 26, 3. 
KUTULAKOS, K. N., AND HASINOFF, S. W. 2009. Focal Stack Photography: High-Performance Photography with 
a Conventional Camera. In Proc. IAPR Conference on Machine Vision Applications, 332 337. LANDOLT, O., 
MITROS, A., AND KOCH, C. 2001. Visual Sensor with Resolution Enhancement by Mechanical Vibrations. In 
Proc. Advanced Research in VLSI, 249 264. LANGE, R., AND SEITZ, P. 2001. Solid-State Time-of-Flight Range 
Camera. Journal of Quantum Electronics 37, 3, 390 397. LANMAN, D., WACHS, M., TAUBIN, G., AND CUKIERMAN,F.2006.SphericalCatadioptricArrays:Construction, 
Multi-View Geometry, and Calibration. In Proc. 3DPVT, 81 88. LANMAN, D., RASKAR, R., AGRAWAL, A., AND 
TAUBIN, G. 2008. Shield Fields: Modeling and Capturing 3D Occluders. ACM Trans. Graph. (Siggraph Asia) 
27, 5, 131. LANMAN, D., HIRSCH, M., KIM, Y., AND RASKAR, R. 2010. Content-Adaptive Parallax Barriers: 
Optimizing Dual-Layer 3D Displays using Low-Rank Light Field Factorization. ACM Trans. Graph. (Siggraph 
Asia) 28, 5, 1 10. LANMAN, D. 2010. Mask-based Light Field Capture and Display. PhD thesis, Brown University, 
School of Engineering. 31 LAU, D. L., AND YANG, R. 2005. Real-Time Multispectral Color Video Synthesis 
using an Array of Commodity Cameras. Real-Time Imaging 11, 2, 109 116. LAWLOR, J., FLETCHER-HOLMES, D. 
W., HARVEY, A. R., AND MCNAUGHT, A. I. 2002. In Vivo Hyperspectral Imaging of Human Retina and Optic 
Disc. Invest. Ophthalmol. Vis. Sci. 43, 4350. LEVIN, A., AND DURAND, F. 2010. Linear View Synthesis Using 
a Dimensionality Gap Light Field Prior. In Proc. IEEE CVPR, 1 8. LEVIN, A., FERGUS, R., DURAND, F., AND 
FREEMAN, W. 2007. Image and Depth from a Conventional Camera with a Coded Aperture. ACM Trans. Graph. 
(Siggraph) 26, 3, 70. LEVIN, A., FERGUS, R., DURAND, F., AND FREEMAN, W. T., 2007. Deconvolution using 
Natural Image Priors. groups.csail.mit.edu/graphics/CodedAperture/SparseDeconv-LevinEtAl07.pdf. LEVIN, 
A., SAND, P., CHO, T. S., DURAND, F., AND FREEMAN, W. T. 2008. Motion-invariant photography. ACM Trans. 
Graph. (Siggraph) 27, 3, 71. LEVIN, A., HASINOFF, S. W., GREEN, P., DURAND, F., AND FREEMAN, W. T. 2009. 
4D Frequency Analysis of Computational Cameras for Depth of Field Extension. ACM Trans. Graph. (Siggraph) 
28, 3, 97. LEVOY, M., AND HANRAHAN, P. 1996. Light Field Rendering. In Proc. ACM Siggraph, 31 42. LEVOY, 
M., CHEN, B., VAISH, V., HOROWITZ, M., MCDOWALL, I., AND BOLAS, M. 2004. Synthetic Aperture Confocal 
Imaging. ACM Trans. Graph. (SIGGRAPH) 23, 3, 825 834. LEVOY, M., NG, R., ADAMS, A., FOOTER, M., AND HOROWITZ, 
M. 2006. Light Field Microscopy. ACM Trans. Graph. (Siggraph) 25, 3, 924 934. LEVOY, M., 2010. Computational 
Photography and the Stanford Frankencamera. Technical Talk. www.graphics.stanford.edu/talks/. LI, F., 
YU, J., AND CHAI, J. 2008. A Hybrid Camera for Motion Deblurring and Depth Map Super-Resolution. In Proc. 
IEEE CVPR, 1 8. LI, X., GUNTURK, B., AND ZHANG, L. 2008. Image Demosaicing: a Systematic Survey. In SPIE 
Conf. on Visual Comm. and Image Proc., 68221J 68221J 15. LIANG, C.-K., LIN, T.-H., WONG, B.-Y., LIU, 
C., AND CHEN, H. H. 2008. Programmable Aperture Photogra­phy: Multiplexed Light Field Acquisition. ACM 
Trans. Graph. (Siggraph) 27, 3, 1 10. LIPPMANN, G. 1908. La Photographie Int´ egrale. Academie des Sciences 
146, 446 451. LIU, C., AND SUN, D. 2011. A Bayesian Approach to Adaptive Video Super Resolution. In Proc. 
IEEE CVPR, 1 8. LU, Y. M., AND VETTERLI, M. 2009. Optimal Color Filter Array Design: Quantitative Conditions 
and an Ef.cient Search Procedure. In Proc. SPIE 7250, 1 8. LUCY, L. B. 1974. An iterative technique for 
the recti.cation of observed distributions. The Astronomical Journal 79, 745 754. LUMSDAINE, A., AND 
GEORGIEV, T. 2009. The Focused Plenoptic Camera. In Proc. ICCP, 1 8. LYOT, B. 1944. Le Filtre Monochromatique 
Polarisant et ses Applications en Physique Solaire. Annales d Astrophysique 7, 31. MA, W.-C., HAWKINS, 
T., PEERS, P., CHABERT, C.-F., WEISS, M., AND DEBEVEC, P. 2007. Rapid acquisition of specular and diffuse 
normal maps from polarized spherical gradient illumination. In Proc. EGSR, 183 194. 32 MANN, S., AND 
PICARD, R. W. 1995. Being Undigital with Digital Cameras: Extending Dynamic Range by Combining Differently 
Exposed Pictures. In Proc. IS&#38;T, 442 448. MANSFIELD, C. L., 2005. Seeing into the past. www.nasa.gov/vision/earth/technologies/scrolls.html. 
MANTIUK, R., DALY, S., MYSZKOWSKI, K., AND SEIDEL, H.-P. 2005. Predicting Visible Differences in High 
Dynamic Range Images -Model and its Calibration. In Electronic Imaging, B. E. Rogowitz, T. N. Pappas, 
and S. J. Daly, Eds., vol. 5666, 204 214. MANTIUK, R., KIM, K. J., REMPEL, A., AND HEIDRICH, W. 2011. 
HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. 
ACM Trans. Graph. (Siggraph) 30, 3, 1 12. MARSHALL, J., AND OBERWINKLER, J. 1999. Ultraviolet Vision: 
the Colourful World of the Mantis Shrimp. Nature 401, 6756, 873 874. MATHEWS, S. A. 2008. Design and 
Fabrication of a low-cost, Multispectral Imaging System. Applied Optics 47, 28, 71 76. M¨ ATHGER, L. 
M., SHASHAR, N., AND HANLON, R. T. 2009. Do Cephalopods Communicate using Polarized Light Re.ections 
from their Skin? Journal of Experimental Biology 212, 2133 2140. MATUSIK, W., AND PFISTER, H. 2004. 3d 
tv: a scalable system for real-time acquisition, transmission, and autostereoscopic display of dynamic 
scenes. ACM Transactions on Graphics 23, 814 824. MATUSIK, W., BUEHLER, C., RASKAR, R., GORTLER, S. J., 
AND MCMILLAN, L. 2000. Image-based visual hulls. In ACM SIGGRAPH, 369 374. MAXWELL, J. C. 1860. On the 
Theory of Compound Colours, and the Relations of the Colours of the Spectrum. Phil. Trans. R. Soc. Lond. 
150, 57 84. MCGUIRE, M., MATUSIK, W., PFISTER, H., HUGHES, J. F., AND DURAND, F. 2005. Defocus Video 
Matting. ACM Trans. Graph. (SIGGRAPH) 24, 3, 567 576. MCGUIRE, M., MATUSIK, W., PFISTER, H., CHEN, B., 
HUGHES, J. F., AND NAYAR, S. K. 2007. Optical Splitting Trees for High-Precision Monocular Imaging. IEEE 
Comput. Graph. &#38; Appl. 27, 2, 32 42. MITSUNAGA, T., AND NAYAR, S. K. 1999. Radiometric Self Calibration. 
In Proc. IEEE CVPR, 374 380. MIYAZAKI, D., TAN, R. T., HARA, K., AND IKEUCHI, K. 2003. Polarization-based 
inverse rendering from a single view. In Proc. ICCV, 982 998. MIYAZAKI, D., KAGESAWA, M., AND IKEUCHI, 
K. 2004. Transparent Surface Modelling from a Pair of Polar­ization Images. IEEE Trans. PAMI 26, 1, 73 
82. MOHAN, A., HUANG, X., RASKAR, R., AND TUMBLIN, J. 2008. Sensing Increased Image Resolution Using 
Aperture Masks. In Proc. IEEE CVPR, 1 8. MOHAN, A., RASKAR, R., AND TUMBLIN, J. 2008. Agile Spectrum 
Imaging: Programmable Wavelength Modu­lation for Cameras and Projectors. Computer Graphics Forum (Eurographics) 
27, 2, 709 717. MOON, P., AND SPENCER, D. E. 1981. The Photic Field. MIT Press. M¨ ULLER, V. 1996. Elimination 
of Specular Surface-Re.ectance Using Polarized and Unpolarized Light. In Proc. IEEE ECCV, 625 635. MURPHY, 
D. B. 2001. Fundamentals of Light Microscopy and Electronic Imaging. Wiley-Liss. MUYBRIDGE, E. 1957. 
Animals in Motion. .rst ed. Dover Publications, Chapman and Hall 1899. 33 NAGAHARA, H., KUTHIRUMMAL, 
S., ZHOU, C., AND NAYAR, S. 2008. Flexible Depth of Field Photography. In Proc. ECCV, 60 73. NAMER, E., 
AND SCHECHNER, Y. Y. 2005. Advanced Visibility Improvement Based on Polarization Filtered Images. In 
Proc. SPIE 5888, 36 45. NARASIMHAN, S., AND NAYAR, S. 2005. Enhancing Resolution along Multiple Imaging 
Dimensions using Assorted Pixels. IEEE Trans. PAMI 27, 4, 518 530. NARASIMHAN, S. G., KOPPAL, S. J., 
AND YAMAZAKI, S. 2008. Temporal Dithering of Illumination for Fast Active Vision. In Proc. ECCV, 830 
844. NAYAR, S., AND BRANZOI, V. 2003. Adaptive Dynamic Range Imaging: Optical Control of Pixel Exposures 
over Space and Time. In Proc. IEEE ICCV, vol. 2, 1168 1175. NAYAR, S., AND MITSUNAGA, T. 2000. High Dynamic 
Range Imaging: Spatially Varying Pixel Exposures. In Proc. IEEE CVPR, vol. 1, 472 479. NAYAR, S. K., 
AND NAKAGAWA, Y. 1994. Shape from Focus. IEEE Trans. PAMI 16, 8, 824 831. NAYAR, S., FANG, X.-S., AND 
BOULT, T. 1993. Removal of Specularities using Color and Polarization. In Proc. IEEE CVPR, 583 590. NAYAR, 
S., BRANZOI, V., AND BOULT, T. 2004. Programmable Imaging using a Digital Micromirror Array. In Proc. 
IEEE CVPR, vol. I, 436 443. NAYAR, S. K., BRANZOI, V., AND BOULT, T. E. 2006. Programmable Imaging: Towards 
a Flexible Camera. IJCV 70, 1, 7 22. NG, R., LEVOY, M., BR´ EDIF, M., DUVAL, G., HOROWITZ, M., AND HANRAHAN, 
P. 2005. Light .eld photog­raphy with a hand-held plenoptic camera. Tech. Rep. Computer Science CSTR 
2005-02, Stanford University. NG, R. 2005. Fourier Slice Photography. ACM Trans. Graph. (Siggraph) 24, 
3, 735 744. NOMURA, Y., ZHANG, L., AND NAYAR, S. 2007. Scene Collages and Flexible Camera Arrays. In 
Proc. EGSR, 1 12. OGATA, S., ISHIDA, J., AND SASANO, T. 1994. Optical Sensor Array in an Arti.cial Compound 
Eye. Optical Engineering 33, 11, 3649 3655. OJEDA-CASTANEDA, J., LANDGRAVE, J. E. A., AND ESCAMILLA, 
H. M. 2005. Annular Phase-Only Mask for High Focal Depth. Optics Letters 30, 13, 1647 1649. OKAMOTO, 
T., AND YAMAGUCHI, I. 1991. Simultaneous Acquisition of Spectral Image Information. Optics Letters 16, 
16, 1277 1279. OKANO, F., ARAI, J., HOSHINO, H., AND YUYAMA, I. 1999. Three-Dimensional Video System 
Based on Integral Photography. Optical Engineering 38, 6, 1072 1077. OPTEC, 2011. Separation prism technical 
data, Jan. www.alt-vision.com/color prisms tech data.htm. PANAVISION, 2010. Genesis. www.panavision.com. 
PANDHARKAR, R., KIRMANI, A., AND RASKAR, R. 2010. Lens Aberration Correction Using Locally Optimal Mask 
Based Low Cost Light Field Cameras. In Proc. OSA Imaging Systems, 1 3. PANDHARKAR, R., VELTEN, A., BARDAGJY, 
A., RASKAR, R., BAWENDI, M., KIRMANI, A., AND LAWSON, E. 2011. Estimating Motion and Size of Moving Non-Line-of-Sight 
Objects in Cluttered Environments. In Proc. ICCC CVPR, 1 8. 34 PARK, J.-I., LEE, M.-H., GROSSBERG, M. 
D., AND NAYAR, S. K. 2007. Multispectral Imaging Using Multi­plexed Illumination. In Proc. IEEE ICCV, 
1 8. PARMAR, M., AND REEVES, S. J. 2006. Selection of Optimal Spectral Sensitivity Functions for Color 
Filter Arrays. In Proc. of ICIP, 1005 1008. PARMAR, M., AND REEVES, S. J. 2010. Selection of Optimal 
Spectral Sensitivity Functions for Color Filter Arrays. IEEE Trans. Im. Proc. 19, 12 (Dec), 3190 3203. 
PHOTRON, 2010. FASTCAM SA5. www.photron.com/datasheet/FASTCAM SA5.pdf. PIEPER, R. J., AND KORPEL, A. 
1983. Image Processing for Extended Depth of Field. Applied Optics 22, 10, 1449 1453. PIXIM, 2010. Digital 
Pixel System. www.pixim.com. PROJECT, E. D. C., 2009. Harold Doc Edgerton. www.edgerton-digital-collections.org/techniques/high-speed-photography. 
PROKUDIN-GORSKII, S. M., 1912. The Prokudin-Gorskii Photographic Records Recreated. www.loc.gov/exhibits/empire/. 
RAMANATH, R., SNYDER, W., BILBRO, G., AND SANDER, W. 2002. Demosaicking Methods for Bayer Color Arrays. 
Journal of Electronic Imaging 11, 3, 306 315. RASKAR, R., AND TUMBLIN, J. 2009. Computational Photography: 
Mastering New Techniques for Lenses, Lighting, and Sensors. A. K. Peters. RASKAR, R., AGRAWAL, A., AND 
TUMBLIN, J. 2006. Coded Exposure Photography: Motion Deblurring using Fluttered Shutter. ACM Trans. Graph. 
(Siggraph) 25, 3, 795 804. RASKAR, R., AGRAWAL, A., WILSON, C. A., AND VEERARAGHAVAN, A. 2008. Glare 
Aware Photography: 4D Ray Sampling for Reducing Glare Effects of Camera Lenses. ACM Trans. Graph. (Siggraph) 
27, 3, 56. RAY, S. F. 2002. High Speed Photography and Photonics. SPIE Press. REDDY, D., VEERARAGHAVAN, 
A., AND CHELLAPPA, R. 2011. P2C2: Programmable Pixel Compressive Camera for High Speed Imaging. In Proc. 
IEEE CVPR, 1 8. REINHARD, E., KHAN, E. A., AKY ¨UZ, A. O., AND JOHNSON, G. M. 2008. Color Imaging. A 
K Peters Ltd. REINHARD, E., WARD, G., DEBEVEC, P., PATTANAIK, S., HEIDRICH, W., AND MYSZKOWSKI, K. 2010. 
High Dynamic Range Imaging: Acquisition, Display and Image-Based Lighting. Morgan Kaufmann Publishers. 
RESEARCH, V., 2010. Phantom Flex. www.visionresearch.com/Products/High-Speed-Cameras/Phantom-Flex/. RI, 
S., FUJIGAKI, M., MATUI, T., AND MORIMOTO, Y. 2006. Accurate pixel-to-pixel correspondence adjustment 
in a digital micromirror device camera by using the phase-shifting moir´e method. Applied optics 45, 
27, 6940 6946. RICHARDSON, H. W. 1972. Bayesian-Based Iterative Method of Image Restoration. JOSA 62, 
1, 55 59. ROBERTSON, M. A., BORMAN, S., AND STEVENSON, R. L. 1999. Estimation-Theoretic Approach to Dynamic 
Range Enhancement Using Multiple Exposures. Journal of Electronic Imaging 12, 2003. RORSLETT, B., 2008. 
Uv .ower photographs. www.naturfotograf.com/index2.html. ROUF, M., MANTIUK, R., HEIDRICH, W., TRENTACOSTE, 
M., AND LAU, C. 2011. Glare Encoding of High Dynamic Range Images. In Proc. IEEE CVPR, 1 8. 35 SADJADI, 
F. 2007. Extraction of Surface Normal and Index of Refraction using a Pair of Passive Infrared Polari­metric 
Sensors. In Proc. IEEE CVPR, 1 5. SAJADI, B., MAJUMDER, A., HIWADA, K., MAKI, A., AND RASKAR, R. 2011. 
Switchable Primaries Using Shiftable Layers of Color Filter Arrays. ACM Trans. Graph. (Siggraph) 30, 
3, 1 10. SCHECHNER, Y. Y., AND KARPEL, N. 2004. Clear Underwater Vision. In Proc. IEEE CVPR, 536 543. 
SCHECHNER, Y. Y., AND KARPEL, N. 2005. Recovery of Underwater Visibility and Structure by Polarization 
Analysis. IEEE Journal of Oceanic Engineering 30, 3, 570 587. SCHECHNER, Y., AND NAYAR, S. 2001. Generalized 
Mosaicing. In Proc. IEEE ICCV, vol. 1, 17 24. SCHECHNER, Y., AND NAYAR, S. 2002. Generalized Mosaicing: 
Wide Field of View Multispectral Imaging. IEEE Trans. PAMI 24, 10, 1334 1348. SCHECHNER, Y., AND NAYAR, 
S. K. 2003. Polarization Mosaicking: High dynamic Range and Polarization Imaging in a Wide Field of View. 
In Proc. SPIE 5158, 93 102. SCHECHNER, Y., AND NAYAR, S. 2003. Generalized Mosaicing: High Dynamic Range 
in a Wide Field of View. IJCV 53, 3, 245 267. SCHECHNER, Y., AND NAYAR, S. 2004. Uncontrolled Modulation 
Imaging. In Proc. IEEE CVPR, 197 204. SCHECHNER, Y., AND NAYAR, S. 2005. Generalized Mosaicing: Polarization 
Panorama. IEEE Trans. PAMI 27, 4, 631 636. SCHECHNER, Y., SHAMIR, J., AND KIRYATI, N. 1999. Polarization-Based 
Decorrelation of Transparent Layers: The Inclination Angle of an Invisible Surface. In Proc. ICCV, 814 
819. SCHECHNER, Y., NARASIMHAN, S. G., AND NAYAR, S. K. 2001. Instant Dehazing of Images using Polarization. 
In Proc. IEEE CVPR, 325 332. SCHECHNER, Y., NARASIMHAN, S. G., AND NAYAR, S. K. 2003. Polarization-Based 
Vision through Haze. Applied Optics 42, 3, 511 525. SCHECHNER, Y., NAYAR, S., AND BELHUMEUR, P. 2007. 
Multiplexing for Optimal Lighting. IEEE Trans. PAMI 29, 8, 1339 1354. SCH ¨ ONFELDER, T., 2003. Polarization 
Division Multiplexing in Optical Data Transmission Systems. US Patent 6,580,535. SEETZEN, H., HEIDRICH, 
W., STUERZLINGER, W., WARD, G., WHITEHEAD, L., TRENTACOSTE, M., GHOSH, A., AND VOROZCOVS, A. 2004. High 
Dynamic Range Display Systems. ACM Trans. on Graph. (SIGGRAPH 2004) 23, 3, 760 768. SEMICONDUCTOR, C., 
2010. LUPA Image Sensors. www.cypress.com/?id=206. SETTLES, G. 2001. Schlieren &#38; Shadowgraph Techniques. 
Springer. SHAHAR, O., FAKTOR, A., AND IRANI, M. 2011. Space-Time Super-Resolution from a Single Video. 
In Proc. IEEE CVPR, 1 8. SHECHTMAN, E., CASPI, Y., AND IRANI, M. 2002. Increasing Space-Time Resolution 
in Video. In Proc. ECCV, 753 768. SHECHTMAN, E., CASPI, Y., AND IRANI, M. 2005. Space-Time Super-Resolution. 
IEEE Trans. PAMI 27, 4, 531 545. 36 SHIMADZU, 2010. HyperVision HPV-2. www.shimadzu.com/products/test/hsvc/oh80jt0000001d6t.html. 
SIDDIQUI, A. S., AND ZHOU, J. 1991. Two-Channel Optical Fiber Transmission Using Polarization Division 
Multiplexing. Journal of Optical Communications 12, 2, 47 49. SMITH, B. M., ZHANG, L., JIN, H., AND AGARWALA, 
A. 2009. Light Field Video Stabilization. In Proc. of ICCV, 1 8. SPHERONVR, 2010. SpheroCam HDR. www.spheron.com. 
STARCK, J., AND HILTON, A. 2008. Model-based human shape reconstruction from multiple views. Computer 
Vision and Image Understanding (CVIU) 111, 2, 179 194. TAGUCHI, Y., AGRAWAL, A., RAMALINGAM, S., AND 
VEERARAGHAVAN, A. 2010. Axial Light Fields for Curved Mirrors: Re.ect Your Perspective, Widen Your View. 
In Proc. IEEE CVPR, 1 8. TAGUCHI, Y., AGRAWAL, A., VEERARAGHAVAN, A., RAMALINGAM, S., AND RASKAR, R. 
2010. Axial-Cones: Modeling Spherical Catadioptric Cameras for Wide-Angle Light Field Rendering. ACM 
Trans. Graph. 29, 172:1 172:8. TAI, Y., HAO, D., BROWN, M. S., AND LIN, S. 2010. Correction of Spatially 
Varying Image and Video Motion Blur using a Hybrid Camera. IEEE Trans. PAMI 32, 6, 1012 1028. TALVALA, 
E.-V., ADAMS, A., HOROWITZ, M., AND LEVOY, M. 2007. Veiling Glare in High Dynamic Range Imaging. ACM 
Trans. Graph. (Siggraph) 26, 3, 37. TANIDA, J., KUMAGAI, T., YAMADA, K., MIYATAKE, S., ISHIDA, K., MORIMOTO, 
T., KONDOU, N., MIYAZAKI, D., AND ICHIOKA, Y. 2001. Thin Observation Module by Bound Optics (TOMBO): 
Concept and Experimental Veri.cation. Applied Optics 40, 11, 1806 1813. TANIDA, J., SHOGENJI, R., KITAMURA, 
Y., YAMADA, K., MIYAMOTO, M., AND MIYATAKE, S. 2003. Color Imaging with an Integrated Compound Imaging 
System. Optics Express 11, 18, 2109 2117. TELLEEN, J., SULLIVAN, A., YEE, J., WANG, O., GUNAWARDANE, 
P., COLLINS, I., AND DAVIS, J. 2007. Synthetic Shutter Speed Imaging. Computer Graphics Forum (Eurographics) 
26, 3, 591 598. TOYOOKA, S., AND HAYASAKA, N. 1997. Two-Dimensional Spectral Analysis using Broad-Band 
Filters. Optical Communications 137 (Apr), 22 26. TUMBLIN, J., AGRAWAL, A., AND RASKAR, R. 2005. Why 
I want a Gradient Camera. In Proc. IEEE CVPR, 103 110. TYSON, R. K. 1991. Principles of Adaptive Optics. 
Academic Press. UEDA, K., KOIKE, T., TAKAHASHI, K., AND NAEMURA, T. 2008. Adaptive Integral Photography 
Imaging with Variable-Focus Lens Array. In Proc SPIE: Stereoscopic Displays and Applications XIX, 68031A 
9. UEDA, K., LEE, D., KOIKE, T., TAKAHASHI, K., AND NAEMURA, T., 2008. Multi-Focal Compound Eye: Liquid 
Lens Array for Computational Photography. ACM SIGGRAPH New Tech Demo. UMEYAMA, S., AND GODIN, G. 2004. 
Separation of Diffuse and Specular Components of Surface Re.ection by Use of Polarization and Statistical 
Analysis of Images. IEEE Trans. PAMI 26, 5, 639 647. UNGER, J., WENGER, A., HAWKINS, T., GARDNER, A., 
AND DEBEVEC, P. 2003. Capturing and Rendering with Incident Light Fields. In Proc. EGSR, 141 149. VAGNI, 
F. 2007. Survey of Hyperspectral and Multispectral Imaging Technologies. Tech. Rep. TR-SET-065-P3, NATO 
Research and Technology. 37 VAISH, V., SZELISKI, R., ZITNICK, C. L., KANG, S. B., AND LEVOY, M. 2006. 
Reconstructing Occluded Surfaces using Synthetic Apertures: Stereo, Focus and Robust Measures. In Proc. 
IEEE CVPR, 23 31. VALLEY, G., 2010. Viper FilmStream Camera. www.grassvalley.com. VAN PUTTEN, E., AKBULUT, 
D., BERTOLOTTI, J., VOS, W., LAGENDIJK, A., AND MOSK, A. 2011. Scattering Lens Resolves Sub-100 nm Structures 
with Visible Light. Physical Review Letters 106, 19, 1 4. VEERARAGHAVAN, A., RASKAR, R., AGRAWAL, A., 
MOHAN, A., AND TUMBLIN, J. 2007. Dappled Photog­ raphy: Mask Enhanced Cameras for Heterodyned Light Fields 
and Coded Aperture Refocussing. ACM Trans. Graph. (Siggraph) 26, 3, 69. VEERARAGHAVAN, A., RASKAR, R., 
AGRAWAL, A., CHELLAPPA, R., MOHAN, A., AND TUMBLIN, J. 2008. Non-Refractive Modulators for Encoding and 
Capturing Scene Appearance and Depth. In Proc. IEEE CVPR, 1 8. VEERARAGHAVAN, A., REDDY, D., AND RASKAR, 
R. 2011. Coded Strobing Photography: Compressive Sensing of High Speed Periodic Videos. IEEE Trans. PAMI, 
to appear. VLASIC, D., PEERS, P., BARAN, I., DEBEVEC, P., POPOVI C´, J., RUSINKIEWICZ, S., AND MATUSIK, 
W. 2009. Dynamic Shape Capture using Multi-View Photometric Stereo. In ACM Trans. Graph. (SIGGRAPH Asia), 
1 11. WAGADARIKAR, A., PITSIANIS, N., SUN, X., AND BRADY, D. 2008. Spectral Image Estimation for Coded 
Aperture Snapshot Spectral Imagers. In Proc. SPIE 7076, 707602. WAGADARIKAR, A., PITSIANIS, N., SUN, 
X., AND BRADY, D. 2009. Video Rate Spectral Imaging using a Coded Aperture Snapshot Spectral Imager. 
Optics Express 17, 8, 6368 6388. WANDINGER, U. 2005. Lidar: Range-Resolved Optical Remote Sensing of 
the Atmosphere. Springer. WANG, S., AND HEIDRICH, W. 2004. The Design of an Inexpensive Very High Resolution 
Scan Camera System. Computer Graphics Forum (Eurographics) 23, 10, 441 450. WEHNER, R. 1976. Polarized-Light 
Navigation by Insects. Scienti.c American 235, 106115. WETZSTEIN, G., IHRKE, I., AND HEIDRICH, W. 2010. 
Sensor Saturation in Fourier Multiplexed Imaging. In Proc. IEEE CVPR, 1 8. WETZSTEIN, G., IHRKE, I., 
LANMAN, D., AND HEIDRICH, W. 2011. Computational Plenoptic Imaging. Com­puter Graphics Forum 30, 23972426. 
WETZSTEIN, G., LANMAN, D., HEIDRICH, W., AND RASKAR, R. 2011. Layered 3D: Tomographic Image Synthesis 
for Attenuation-based Light Field and High Dynamic Range Displays. ACM Trans. Graph. (Siggraph). WETZSTEIN, 
G., IHRKE, I., GUKOV, A., AND HEIDRICH, W. 2011. Towards a Database of High-dimensional Plenoptic Images. 
In Proc. ICCP (Poster). WETZSTEIN, G., RASKAR, R., AND HEIDRICH, W. 2011. Hand-Held Schlieren Photography 
with Light Field Probes. In Proc. ICCP, 1 8. WETZSTEIN, G., LANMAN, D., HIRSCH, M., AND GUTIERREZ, D. 
2012. Computational Displays. In ACM SIGGRAPH 2012 Courses. WILBURN, B., SMULSKI, M., LEE, K., AND HOROWITZ, 
M. A. 2002. The Light Field Video Camera. In SPIE Electronic Imaging, 29 36. WILBURN, B., JOSHI, N., 
VAISH, V., LEVOY, M., AND HOROWITZ, M. 2004. High Speed Video Using a Dense Array of Cameras. In Proc. 
IEEE CVPR, 1 8. 38 WILBURN, B., JOSHI, N., VAISH, V., TALVALA, E.-V., ANTUNEZ, E., BARTH, A., ADAMS, 
A., HOROWITZ, M., AND LEVOY, M. 2005. High Performance Imaging using Large Camera Arrays. ACM Trans. 
Graph. (Siggraph) 24, 3, 765 776. WOLFF, L. B., AND BOULT, T. E. 1991. Constraining Object Features using 
a Polarization Re.ectance Model. IEEE Trans. PAMI 13, 7, 635 657. WYSZECKI, G., AND STILES, W. 1982. 
Color Science. John Wiley and Sons, Inc. YANG, J., LEE, C., ISAKSEN, A., AND MCMILLAN, L., 2000. A Low-Cost 
Portable Light Field Capture Device. ACM SIGGRAPH Technical Sketch. YANG, J. C., EVERETT, M., BUEHLER, 
C., AND MCMILLAN, L. 2002. A Real-Time Distributed Light Field Camera. In Proc. EGSR, 77 86. YAO, S., 
2008. Optical Communications Based on Optical Polarization Multiplexing and Demultiplexing. US Patent 
7,343,100. YASUMA, F., MITSUNAGA, T., ISO, D., AND NAYAR, S. K. 2010. Generalized Assorted Pixel Camera: 
Post-Capture Control of Resolution, Dynamic Range and Spectrum. IEEE Trans. Im. Proc. 99. ZHANG, C., 
AND CHEN, T. 2005. Light Field Capturing with Lensless Cameras. In Proc. ICIP, III 792 5. ZHOU, C., 
AND NAYAR, S. 2009. What are Good Apertures for Defocus Deblurring? In Proc. ICCP, 1 8. ZHOU, C., LIN, 
S., AND NAYAR, S. K. 2009. Coded Aperture Pairs for Depth from Defocus. In Proc. ICCV. ZWICKER, M., MATUSIK, 
W., DURAND, F., AND PFISTER,H.2006.Antialiasingforautomultiscopic3Ddisplays. In Eurographics Symposium 
on Rendering. 39  Computational Plenoptic Imaging Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 Wolfgang 
Heidrich3 Kurt Akeley4 Ramesh Raskar1 1MIT Media Lab 2Saarland University 3University of British Columbia 
4Lytro, Inc. II. High Dynamic Range Imaging  SIGGRAPH 2012 Course What is HDR? http://en.wikipedia.org/wiki/High-dynamic-range_imaging 
 Dynamic Range of Standard Sensors  13.5 EVs or f-stops = contrast 11,000:1 = color negative HDR Acquisition 
 Exposure Brackets  [Debevec &#38; Malik 97] HDR Display   47 TFT LCD, LED backlight  aspect ratio 
16:9  resolution 1920 x 1080  contrast >1,000,000:1  brightness 4,000 cd/m2  Images courtesy Dolby 
Image Based Lighting  Textbook  HDR image / video encoding  capture, display, tone reproduction 
 visible difference predictors  image based lighting, etc.  We cover some specialized acquisition approaches 
here. II.I Single-Shot Acquisition HDR Cameras  Grass Valley Viper Spheron  Panavision Genesis 
Per-Pixel Exposure Control www.pixim.com no pixim with pixim no pixim with pixim Gradient Camera 
 Measure log-gradient  Reconstruct intensity with Poisson solver   Intensity Log-Intensity Log-Gradient 
 [Tumblin et al. 05] Assorted Pixels  ND filter arrays  Less flexible and costly than Pixim   Conventional 
Camera SVE Camera SVE Reconstruction [Nayar and Mitsunaga 00,Narasimhan and Nayar 05] Fourier-based 
Analysis and Reconstruction  [Wetzstein et al. 10] Fourier-based Analysis and Reconstruction  [Wetzstein 
et al. 10] Fourier-based Analysis and Reconstruction  [Wetzstein et al. 10] Programmable Imaging 
  [Nayar &#38; Branzoi 06] Adaptive Dynamic Range Imaging  [Nayar and Branzoi 03]  Coded Rolling 
Shutter  [Gu et al. 10] 18 Frankencamera  Version 3.0 is supposed to have non­destructive ROI readout 
 [Adams et al. 10, Levoy 10] Tomographic HDR from Star Filter  II.II Multi-Device Techniques Split 
Aperture Imaging  -Multiple sensors -Mirror pyramid in aperture [Aggrawal &#38; Ahuja 04] Optical 
Splitting Trees  Exposure Sequence &#38; Tonemapped Image  [McGuire et al. 07] II.III Analysis and 
Tradeoffs Which Exposures to Combine?  [Grossberg &#38; Nayar 03]  Noise Optimal HDR Capture Vary 
exposure time and ISO for improved noise in HDR  Next: Spectral Imaging  Computational Plenoptic Imaging 
 Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 Wolfgang Heidrich3 Kurt Akeley4 Ramesh Raskar1 1MIT Media 
Lab 2Saarland University 3University of British Columbia 4Lytro, Inc. III. Spectral Imaging  SIGGRAPH 
2012 Course  8 Multi-Spectral vs. Hyper-Spectral multi-spectral hyper-spectral Color Filters James 
Clerk Maxwell 1831 -1879 1861 first color photograph  Examples -Prokudin-Gorskij Self-portrait 1915 
 Examples -Prokudin-Gorskij Photograph 1910, Emir of Bukhara , Prokudin-Gorskii  Examples -Prokudin 
Gorski -Lew Tolstoy  1887 painting, Ilya Repin 1910 photograph, Sergey Prokudin-Gorskii Color Wheel 
  [Wang and Heidrich 2004]  Liquid Crystal Tunable Filter (LCTF) Computer controllable spectral filter 
  VariSpec LCTF APH 2012 Course | III. Spectral Imaging VariSpec spectral curves  Agile Spectrum 
Imaging  Color Filter Arrays  Color Filter Arrays microscopic image of CCD (courtesy of Kevin Collins) 
 Color Filter Arrays  alternative CFA designs  standard subtractive Bayer primaries e.g. Kodak DCS 
620x RGB/Emerald e.g. Sony DCS F828 subtractive primaries + green some video cameras for increased light 
sensitivity yellow/cyan/ green/white e.g. JVC Color Filter Arrays  Lots of work on optimized spectral 
transmissions &#38; layout of CFAs see STAR  Siggraph 11 paper on dynamically switchable CFAs coming 
up  Assorted Pixels generalizes the concept Single Pixel Spectrometers  Principle of Operation -Dispersion 
 disadvantage: dispersion relation is nonlinear  spatial position of wavelengths on screen must be 
calibrated   Prism-Based Systems Vintage Prism-Based Spectrometer  Spectrometer (Diffraction-Based) 
 Spectrometer calibration (all types) 1. mapping pixel wavelength 2. relative intensity of wavelength 
  Diffraction Grating  Center and first order diffraction  At center, no diffraction  For higher 
orders, diffraction is taking place (wavelength dependent)  Scanning Spectrometers Direct Scanning 
  Spatial Scanning Generalized Mosaics [Schechner &#38; Nayar]  linear filter  each pixel column 
filtered differently  rotational motion &#38; registration to assemble image stack   Snapshot Imaging 
Spectrometers Multiplexing: Prism-Mask Based System  Computed Tomography Imaging Spectrometer  [Hagen 
08] Spectrally Filtered Light Fields  [Horstmeyer 09] Applications Applications automatic white 
balancing Spatially uniform illumination raw from RGB tungsten WB `greyworld WB spectral WB spectra 
 Spatially varying illumination [Cao10] Applications improved tracking RGB spectral tracking lost 
tracking OK real and fake skin detection  analyze / restore paintings   Applications  [Calit] 
Applications Satellite-Based Remote Sensing  vegetation mapping urban land use pollution monitoring 
 [DigitalGlobe 10] Next: Light Field Acquisition  Computational Plenoptic Imaging Gordon Wetzstein1 
Ivo Ihrke2 Douglas Lanman1 Wolfgang Heidrich3 Kurt Akeley4 Ramesh Raskar1 1MIT Media Lab 2Saarland University 
3University of British Columbia 4Lytro, Inc. IV. Light Field Acquisition  SIGGRAPH 2012 Course  IV.I 
(Brief) Introduction to Light Fields The 5D Plenoptic Function  P(q,f,l, t) Q: What is the set of 
all things that one can ever see? A: The Plenoptic Function [Adelson and Bergen 1991] (from plenus, complete 
or full, and optic) The 5D Plenoptic Function  P(q,f,l, t) Q: What is the set of all things that one 
can ever see? A: The Plenoptic Function [Adelson and Bergen 1991] (from plenus, complete or full, and 
optic) The 5D Plenoptic Function  P(q,f,l, t, px, py, pz ) P(q,f,l, t, px, py, pz ) defines the intensity 
of light: as a function of viewpoint  as a function of time  as a function of wavelength  The 5D 
Plenoptic Function  P(q,f,l, t, px, py, pz ) P(q,f,l, t, px, py, pz ) defines the intensity of light: 
 as a function of viewpoint  as a function of time  as a function of wavelength  The 5D Plenoptic 
Function Let s ignore color and time (i.e., these are attributes of rays)  P(q,f, px, py, pz) The plenoptic 
function is 5D: 3D position  2D direction  Require 5D to represent attributes across occlusions The 
4D Light Field Consider a region free of occluding objects  P(q,f, px, py, pz) The plenoptic function 
(light field) is 4D 2D position  2D direction  The space of all lines in a 3D space is 4D [Levoy 
and Hanrahan 1996; Gortler et al. 1996] Position-Angle Parameterization 2D position 2D direction  
Two-Plane (Light Slab) Parameterization 2D position 2D position  Alternative Parameterizations  Left: 
Points on a plane or curved surface and directions leaving each point Center: Pairs of points on the 
surface of a sphere Right: Pairs of points on two planes in general (meaning any) position Image-Based 
Rendering  [Levoy and Hanrahan 1996] [Gortler et al. 1996]  [Carranza et al. 2003]  Conventional vs. 
Plenoptic Camera   Slide by Marc Levoy Light Field Photography = Array of (Virtual) Cameras   Slide 
by Marc Levoy Digital Image Refocusing  Kodak 16-megapixel sensor   125µ square-sided microlenses 
 [Ng 2005] IV.II Multiple Sensors  Static Camera Arrays   Stanford Multi-Camera Array Distributed 
Light Field Camera 125 cameras using custom hardware 64 cameras with distributed rendering [Wilburn et 
al. 2002, Wilburn et al. 2005] [Yang et al. 2002] Flexible Camera Arrays  IV.III Temporal Multiplexing 
 Controlled Camera or Object Motion   Stanford Spherical Gantry Relighting with 4D Incident Light 
Fields [Levoy and Hanrahan 1996] [Masselus et al. 2003] Uncontrolled Camera or Object Motion   [Gortler 
et al. 1996; Buehler et al. 2001] Virtual Cameras using a Steerable Mirror   Fast Incident Light 
Field Acquisition and Rendering [Ihrke et al. 2008] Programmable Aperture Photography   [Liang et 
al. 2008; Schechner and Nayar 2007] IV.IV Spatial and Frequency Multiplexing  a  Parallax Barriers 
( Slits and Pinhole Arrays )   barrier sensor wi Spatially-multiplexed light field capture using 
masks (i.e., barriers): Cause severe attenuation long exposures or lower SNR  Impose fixed trade-off 
between spatial and angular resolution (unless implemented with programmable masks, e.g. LCDs)  [Ives 
1903] Parallax Barriers ( Slits and Pinhole Arrays )   Parallax Barriers ( Slits and Pinhole Arrays 
)  looking to the right   [The (New) Stanford Light Field Archive] Integral Imaging ( Lenticular 
or Fly s Eye )   f   sensor wi Spatially-multiplexed light field capture using lenslets: Impose 
fixed trade-off between spatial and angular resolution [Lippmann 1908] Modern, Digital Implementations 
   Digital Light Field Photography Hand-held plenoptic camera [Ng et al. 2005]  Heterodyne light 
field camera [Veeraraghavan et al. 2007]  External, Fixed Lens Attachments   [Georgiev et al. 2006] 
 External, Adaptive Lens Attachments  [Ueda et al. 2008] Virtual Cameras using Mirror Arrays   [Unger 
et al. 2003] [Lanman et al. 2006] [Taguchi et al. 2010] Light Field Analysis of Barrier Cameras  Pinhole 
Array Sensor Lenses &#38; Apertures Scene f ss  fband-limited s0  light field u fu f u0 2D Fourier 
Lreceived(u,s) Transform L received(fu,fs) Modeling Sensors in the Frequency Domain  Pinhole Array 
 Sensor Lenses &#38; Apertures Scene fs sensor slice (Fourier Projection-Slice Theorem)   fu wasted 
sensor bandwidth L received(fu,fs) Heterodyne Light Field Cameras  Heterodyne Mask  Sensor Lenses 
&#38; Apertures Scene fs fu  L received(fu,fs)  modulation along a slanted line through origin modulation 
function is an impulse train Heterodyne Light Field Cameras   Sensor Lenses &#38; Apertures Scene 
fs fu reorder spectral components  L received(fu,fs) Heterodyne Light Field Cameras   Sensor Lenses 
&#38; Apertures Scene fss   (f ,f) Fourier Transform (u,s) L receivedusLreceived Optimal Masks for 
Optical Heterodyning   Angular Resolution Angular Resolution Benefits and Limitations Sum-of-Sinusoids 
converges to 18% transmission  Tiled-MURA near 50% (but only for prime-valued lengths)  Binary vs. 
continuous-tone process (quantization)  [Lanman et al. 2008] Optimal Masks for Optical Heterodyning 
  [Ihrke et al. 2010] IV.V Applications  Scene Relighting with 8D Reflectance Fields  [Debevec 
et al. 2000] 3D Displays   Parallax Panoramagram 3DTV with Integral Imaging MERL 3DTV [Kanolt 1918] 
[Okano et al. 1999] [Matusik and Pfister 2004] 3D Displays  High-Rank 3D (HR3D) [Lanman et al. 2010] 
 (BiDi)rectional Screens  Ambient light sensors Support unencumbered interaction with thin form factor 
[Hirsch et al. 2009] (BiDi)rectional Screens  Components 20.1 inch Sceptre X20WG-NagaII LCD [1680×1050 
@ 60 fps]  Point Grey Flea2 video cameras [1280×960 @ 7 fps] [Hirsch et al. 2009]  (BiDi)rectional 
Screens  [Hirsch et al. 2009] (BiDi)rectional Screens  [Hirsch et al. 2009] Mitigating Glare  Veiling 
Glare in HDR Imaging Glare-Aware Photography [Talvala et al. 2007] [Raskar et al. 2008] Light Field 
Microscopy  [Levoy et al. 2004; Levoy et al. 2006] Image Stabilization  [Smith et al. 2009] Compressive 
Light Field Capture  Random Lens Imaging Compressive Light Field Imaging [Fergus et al. 2006] [Babacan 
et al. 2009; Ashok and Neifeld 2010] Next: Multiplexing Space &#38; Focal Surfaces   Computational 
Plenoptic Imaging Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 Wolfgang Heidrich3 Kurt Akeley4 Ramesh 
Raskar1 1MIT Media Lab 2Saarland University 3University of British Columbia 4Lytro, Inc.  V. Multiplexing 
Space &#38; Focal Surfaces  SIGGRAPH 2012 Course  V.I Gigapixel Imaging &#38; Superresolution Gigapixel 
Panorama Rotate camera! [Kopf et al. 07]   Auto-exposure for each image Processed HDR image  Gigapixel 
Large-Format Tile-Scan Camera  Translate sensor! [Ben-Ezra 11]  Gigapixel Computational Imaging Beat 
lens scaling laws with computational processing!   [Cossairt et al. 11]  Superresolution Overview 
 One of the Output image input images area  1. 2.   http://en.wikipedia.org/wiki/Super-resolution 
 Superresolution Overview   [Baker &#38; Kanade 02] Solve big linear system for high-res image Superresolution 
with Mechanical Vibrations  multiple, successive images  vibrating mirror vibrating sensor  Jitter 
Camera   Superresolution with Aperture Masks   Space-Time Superresolution  Increase resolution 
in space &#38; time from multiple low-resolution videos 3 of 18 low-temporal input sequences [Shechtman 
et al. 05]  V.II Optical Field Correction  What is Field Curvature? Focal surface of most lenses is 
not planar Fiber Optics   [Kapany &#38; Hopkins 57]  Aligned fiber bundle for reshaping curvature 
  Limited resolution  Aberration Correction with Light Field Camera  Digital curvature correction 
by resorting rays of 4D light field [Ng &#38; Hanrahan 06]  Multiscale Lens Design  Optimized multi-layered, 
multi-scale optics  Joint optical encoding and computational processing  Extended FOV &#38; DOF, super-resolution, 
smaller form factor, lower cost  Locally optimal masks  [Brady &#38; Hagen 09] Coded Attenuation 
Masks   [Prandharkar et al. 10] Adaptive Optics   Deformable mirrors to correct wavefront  Used 
in astronomy and retinal imaging  uncorrected wavefront corrected wavefront deformable mirror  C. 
Max, Center for Adaptive Optics www.lyot.org V.III Extended Depth of Field Imaging Depth of Field 
Defocus blur is depth-dependent  Can be described as convolution, where  Kernel size is related to 
depth  Kernel shape is that of aperture   Image from [Levin et al. 07] Deconvolution is Hard Lo 
  e Approaches to Extend DOF  Make PSF invertible deconvolution becomes well-posed  Make PSF depth-invariant 
 shift­invariant deconvolution  Capture multiple images with different focus settings (no deconvolution) 
  Spatially varying deconvolution  Coded Apertures Attenuation Masks   [Levin et al. 07] Coded Apertures 
 Analysis  Analysis of aperture patterns with respect to camera noise [Zhou &#38; Nayar 09]  Coded 
Apertures Cubic Phase Plate (almost) depth-independent PSF  Coded Apertures Lattice Focal Lens  
4D is different than 2D dimensionality gap Coded Apertures Frequency Analysis   Coded Apertures 
 Analysis  Analysis of computational cameras with respect to depth-invariance and transfer efficiency 
 [Baek 10] Coded Apertures Diffusion Masks depth-invariance and invertibility  [Cossairt et al. 10] 
Focal Stacks   Capture multiple images with different focus settings  Merge into single extended 
DOF image   Gigapixel Focal Stack  [Ben-Ezra 11]   Light Efficient Photography  Efficient selection 
of focus and aperture settings for desired DOF less noise  Focal Stack Multiplexing Single Image 
 Multiplex focal stack like Bayer pattern   Interesting concept no implementation  Focal Sweep 
 Moving Sensor  Move sensor along optical axis over exposure time   Movement makes PSF depth-independent 
   Spectral Focal Sweep Use chromatic aberrations  Multiple focal planes on sensor plane   [Cossairt 
and Nayar 10] Applications Confocal Stereo   Applications Depth from Coded Apertures   [Zhou et 
al. 11] Applications Defocus Video Matting    [McGuire et al. 05] Applications Extended DOF Projection 
  [Grosse et al. 10] Next: High Speed Imaging  Object Motion and Coded Exposure Camera   Multi-Aperture 
Photography   [Green et al. 07] Multiple images, single sensor  Synthesize different apertures in 
post­processing   Computational Plenoptic Imaging Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 Wolfgang 
Heidrich3 Kurt Akeley4 Ramesh Raskar1 1MIT Media Lab 2Saarland University 3University of British Columbia 
4Lytro, Inc. VI. Multiplexing Time  SIGGRAPH 2012 Course  History Eadweard Muybridge 1830-1904 
 http://en.wikipedia.org/wiki/Eadweard_Muybridge History Étienne-Jules Marey 1830-1904  www.wikipedia.org 
 VI.I Time Lapse Photography  BBC Time Lapse Look It! Long exposures to avoid temporal aliasing ND 
3.0 filter, f22, 1 minute exposure  VI.II High-Speed Imaging  High-Speed Cameras   Vision Research 
Phantom Flex (CMOS) Shimadzu HyperVision HPV-2 (CCD) 2570 fps at HD resolution one million fps at 312x260 
pixels  Photron FASTCAM SA5 (CMOS) Casio Exilim Series (consumer cam) 7500 fps at megapixel resolution 
1000 fps at reduced resolution one million fps at 64x64 pixels Assorted Pixels   Temporal Mosaic with 
DMD  DMD aligned with CCD in microscope  [Bub et al. 10] Non-Destructive Sensor Readout &#38; Pixim 
  www.pixim.com  Cypress Semiconductor LUPA 3000 3 megapixels, 485 fps Coded Rolling Shutter   
 Reinterpretable Imager Moving pinhole over time in aperture  Capture with light field camera  [Agrawal 
et al. 10] 12 Bullet Time Effect   from The Matrix Stanford Multi-Camera Array   [Wilburn et al. 
04] 14 Coded Temporal Sampling   [Agrawal et al. 10] 15 High-Speed Illumination Electronic Strobes 
  Harold Doc Edgerton 1903-1990   Temporal Dithering with DLP Illumination   [Narasimhan et al. 
08] 17 Coded Strobing Photography   [Reddy et al. 11] 18 Streak Cameras   C5680 $200K  www.hamamatsu.com 
 VI.I Motion Deblurring  Motion Deblurring Overview Motion blur is velocity-dependent  Can be described 
as convolution, where  Kernel shape is motion trajectory  Trajectory is modulated by exposure function 
   http://en.wikipedia.org/wiki/Motion_blur Deconvolution is Still Hard Again problems:  Camera 
noise  Spatially varying kernel (velocity-dependent)  Unknown motion trajectory  Ill-posed problem, 
kernel of box integration function is not invertible (optical cancellation of image frequencies)   
Make PSF invertible coded exposure  Make PSF velocity-invariant shift­invariant deconvolution  Automatize 
PSF estimation  Approaches to Improve Motion Deblurring Flutter Shutter   Optimal Motion PSFs Optimality 
criteria PSF invertibility &#38; estimation   [Agrawal &#38; Xu 07] Motion Invariant Photography 
 Engineer PSF to be motion invariant  Only for 1D motion    Hybrid Cameras  Combined high-speed 
low-quality &#38; low­speed high-quality camera [Ben-Ezra &#38; Nayar 04]  Deblurred Result Ground 
Truth Motion Blur in Video Coded exposure &#38; super-resolution in successive video frames  [Agrawal 
et al. 09] Next: Further Light Properties  Flexible Voxels  Flexible space-time resolution as post­ 
 [Gupta et al. 10]  Synthetic Shutter Speed Imaging Combine multiple short exposures to reduce noise 
 Align with optical flow   Hybrid Cameras Motion deblurring &#38; super-resolution  Hybrid Cameras 
 Motion deblurring &#38; depth from two low­resolution high-speed camers Deblurred result Recovered 
Depth Analysis  Analysis of optimal coded, single image deblurring  MIP becomes worse when velocities 
exceed expectations  Coded Exposure Motion Invariant Photography [Agrawal &#38; Raskar 09]  Computational 
Plenoptic Imaging Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 Wolfgang Heidrich3 Kurt Akeley4 Ramesh 
Raskar1 1MIT Media Lab 2Saarland University 3University of British Columbia 4Lytro, Inc. VII. Further 
Light Properties  SIGGRAPH 2012 Course VII.I Polarization What is Polarization?  Wave property of 
light  Oscillation perpendicular to propagation  Field Guide to Polarization, Collett 05  Polarization 
State http://en.wikipedia.org/wiki/Polarization_(waves) Polarization Acquisition Assorted Pixels Generalized 
Mosaics Polarization Panoramagrams  [Narasimhan &#38; Nayar 05] [Schechner &#38; Nayar 05] Polarization 
Acquisition -Applications Also: optical communication, underwater vision, BRDF &#38; IOR [Ghosh et al. 
10] acquisition, VII.II Phase Imaging Phase Contrast Imaging   Phase Contrast Microscope DIC Microscope 
Brightfield Phase Contrast DIC Phase Contrast Brightfield  http://www.microscopyu.com/articles/phasecontrast/phasemicroscopy.html 
 Schlieren Imaging  images courtesy Gary Settles  Schlieren Imaging Applications  Schlieren Imaging 
 Optical Setups  Background Oriented Schlieren Imaging  Optical Encoding Computational Processing 
 Undistorted Distorted Image Optical Flow Vectors Color Coded Reference Image Optical Flow Vectors 
[Atcheson et al. 2008] BOS Tomographic Reconstruction  High Frequency Background Transparent, Refractive 
Object Camera Array Optical Setup  Camera Image 2D Optical Flow 3D Gradient Field 3D Refractive Index 
 [Atcheson et al. 2008] Light Field Background Oriented Schlieren  Light Field Background Oriented 
Schlieren  Light Field Background Oriented Schlieren  [Wetzstein et al. 2010] VII.III LIDAR and 
Time-of-Flight  University of Washington NASA See [Kolb et al. 10], EG STAR for more details Next: 
Discussion  Computational Plenoptic Imaging Gordon Wetzstein1 Ivo Ihrke2 Douglas Lanman1 Wolfgang Heidrich3 
Kurt Akeley4 Ramesh Raskar1 1MIT Media Lab 2Saarland University 3University of British Columbia 4Lytro, 
Inc. VIII. Discussion  SIGGRAPH 2012 Course  Summary Survey of plenoptic image acquisition  Classification 
based on plenoptic dimension &#38; hardware setup  Also see computational photography    Observations 
 Most approaches use fixed plenoptic resolution tradeoffs Strong correlations between plenoptic dimensions 
 Need for sophisticated reconstruction techniques (e.g. compressive sensing)  Plenoptic datasets 
 Simulate acquisition &#38; reconstruction  Explore redundancies  Future Directions Exploit Plenoptic 
Redundancy  [Wetzstein et al. 11] Computational Plenoptic Imaging | SIGGRAPH 2012 Course | VIII. Dicsussion 
The End  Future Directions Exploit Plenoptic Redundancy  Explore plenoptic priors mathematical formulations 
for correlations between and within dimensions  Common practice in  Color demosaicking  Extended 
DOF and light field acquisition (dimensionality gap prior) [Levin et al. 09,10]   Extend to time, polarization, 
plenoptic manifolds  Future Directions Unified Plenoptic Reconstruction Unified reconstruction in 
terms of Domain (image space vs. Fourier)  Plenoptic dimension   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343495</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>96</pages>
		<display_no>12</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Data-driven simulation methods in computer graphics]]></title>
		<subtitle><![CDATA[cloth, tissue and faces]]></subtitle>
		<page_from>1</page_from>
		<page_to>96</page_to>
		<doi_number>10.1145/2343483.2343495</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343495</url>
		<abstract>
			<par><![CDATA[<p>In recent years, various methods have been introduced to exploit pre-recorded data to improve the performance and/or realism of dynamic deformations, but their differences and similarities have not been adequately analyzed or discussed. So far, the proposed methods have been explored mainly in the research context. They have not been adopted by the computer graphics industry.</p> <p>This course bridges the gap between research labs and industry to present a unifying theory and understanding of data-driven methods for dynamic deformations that may inspire development of novel solutions. It focuses on application of data-driven methods to three areas of computer animation: dynamic deformation of faces, soft volumetric tissue, and cloth. And it describes how to approach these challenges in a data-driven manner, classifies the various methods, and demonstrates how data-driven methods can work in other settings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738925</person_id>
				<author_profile_id><![CDATA[81100035394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Miguel]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Otaduy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[URJC Madrid]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738926</person_id>
				<author_profile_id><![CDATA[81314493860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bernd]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bickel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Zurich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738927</person_id>
				<author_profile_id><![CDATA[81319488885]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Derek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bradley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Zurich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738928</person_id>
				<author_profile_id><![CDATA[81466648783]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Huamin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1103907</ref_obj_id>
				<ref_obj_pid>1103900</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alexa, M., Gross, M., Pauly, M., Pfister, H., Stamminger, M., and Zwicker, M. 2004. Point-based computer graphics. <i>ACM SIGGRAPH 2004 Course Notes</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409085</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Atcheson, B., Ihrke, I., Heidrich, W., Tevs, A., Bradley, D., Magnor, M., and Seidel, H.-P. 2008. Time-resolved 3d capture of non-stationary gas flows. <i>ACM Transactions on Graphics (Proc. SIGGRAPH Asia) 27</i>, 5, 132.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1960983</ref_obj_id>
				<ref_obj_pid>1960926</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baker, S., Scharstein, D., Lewis, J., Roth, S., Black, M., and Szeliski, R. 2011. A database and evaluation methodology for optical flow. <i>International Journal of Computer Vision 92</i>, 1, 1--31.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Becker, M., and Teschner, M. 2007. Robust and efficient estimation of elasticity parameters using the linear finite element method. In <i>SimVis</i>, 15--28.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778777</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Beeler, T., Bickel, B., Sumner, R., Beardsley, P., and Gross, M. 2010. High-quality single-shot capture of facial geometry. <i>ACM Trans. Graph. (Proc. SIGGRAPH)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964970</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Beeler, T., Hahn, F., Bradley, D., Bickel, B., Beardsley, P., Gotsman, C., Sumner, R. W., and Gross, M. 2011. High-quality passive facial performance capture using anchor frames. <i>ACM Trans. Graph. 30</i>, 75:1--75:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>846282</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Bhat, K. S., Twigg, C. D., Hodgins, J. K., Khosla, P. K., Popovi&#263;, Z., and Seitz, S. M. 2003. Estimating cloth simulation parameters from video. In <i>Proc. ACM SIGGRAPH/Eurographics SCA</i>, 37--51.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1632602</ref_obj_id>
				<ref_obj_pid>1632592</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bickel, B., Lang, M., Botsch, M., Otaduy, M. A., and Gross, M. 2008. Pose-space animation and transfer of facial details. In <i>Proc. of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, 57--66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531395</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Bickel, B., B&#228;cher, M., Otaduy, M. A., Matusik, W., Pfister, H., and Gross, M. 2009. Capture and modeling of non-linear heterogeneous soft tissue. <i>ACM Trans. Graph. 28</i>, 3 (July), 89:1--89:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778800</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Bickel, B., B&#228;cher, M., Otaduy, M. A., Lee, H. R., Pfister, H., Gross, M., and Matusik, W. 2010. Design and fabrication of materials with desired deformation behavior. <i>ACM Transactions on Graphics 29</i>, 4 (July), 63:1--63:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1098453</ref_obj_id>
				<ref_obj_pid>1097876</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Boubekeur, T., Reuter, P., and Schlick, C. 2005. Visualization of point-based surfaces with locally reconstructed subdivision surfaces. In <i>Shape Modeling International</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Boubekeur, T., Heidrich, W., Granier, X., and Schlick, C. 2006. Volume-surface trees. <i>Computer Graphics Forum (Proceedings of EUROGRAPHICS 2006) 25</i>, 3, 399--406.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1827832</ref_obj_id>
				<ref_obj_pid>1827715</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., and Heidrich, W. 2010. Binocular camera calibration using rectification error. <i>IEEE Conference on Computer and Robot Vision (CRV)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., Boubekeur, T., and Heidrich, W. 2008. Accurate multi-view reconstruction using robust binocular stereo and surface meshing. In <i>Proc. CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360698</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., Popa, T., Sheffer, A., Heidrich, W., and Boubekeur, T. 2008. Markerless garment capture. <i>ACM Trans. Graph. (Proc. of SIGGRAPH) 27</i>, 3, 99:1--99:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., Atcheson, B., Ihrke, I., and Heidrich, W. 2009. Synchronization and rolling shutter compensation for consumer video camera arrays. In <i>International Workshop on Projector-Camera Systems (PROCAMS 2009)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778778</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Bradley, D., Heidrich, W., Popa, T., and Sheffer, A. 2010. High resolution passive facial performance capture. <i>ACM Trans. Graph. (Proc. SIGGRAPH) 29</i>, 4, 41:1--41:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>192259</ref_obj_id>
				<ref_obj_pid>192161</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Breen, D., House, D., and Wozny, M. 1994. Predicting the drape of woven cloth using interacting particles. In <i>Proc. of ACM SIGGRAPH</i>, 365--372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360697</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.-P., and Thrun, S. 2008. Performance capture from sparse multi-view video. <i>ACM Trans. Graphics (Proc. SIGGRAPH)</i>, 98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778843</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[de Aguiar, E., Sigal, L., Treuille, A., and Hodgins, J. K. 2010. Stable spaces for real-time clothing. <i>ACM Transactions on Graphics 29</i>, 4 (July), 106:1--106:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>618378</ref_obj_id>
				<ref_obj_pid>616042</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Eberhardt, B., Weber, A., and Strasser, W. 1996. A fast, flexible, particle-system model for cloth draping. <i>IEEE Computer Graphics and Applications 16</i>, 5, 52--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Ekman, P., and Friesen, W. 1978. The facial action coding system: A technique for the measurement of facial movement. In <i>Consulting Psychologists</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778845</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Feng, W.-W., Yu, Y., and Kim, B.-U. 2010. A deformation transformer for real-time cloth animation. <i>ACM Transactions on Graphics 29</i>, 4 (July), 108:1--108:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Furukawa, Y., and Ponce, J. 2008. Dense 3d motion capture from synchronized video streams. In <i>Proc. CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Furukawa, Y., and Ponce, J. 2009. Dense 3d motion capture for human faces. In <i>CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1830134</ref_obj_id>
				<ref_obj_pid>1829895</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Furukawa, Y., and Ponce, J. 2010. Accurate, dense, and robust multi-view stereopsis. <i>IEEE Trans. on Pattern Analysis and Machine Intelligence 32</i>, 8, 1362--1376.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360413</ref_obj_id>
				<ref_obj_pid>360401</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Fusiello, A., Trucco, E., and Verri, A. 2000. A compact algorithm for rectification of stereo pairs. <i>Mach. Vision Appl. 12</i>, 1, 16--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Gopi, M., Krishnan, S., and Silva, C. 2000. Surface reconstruction based on lower dimensional localized delaunay triangulation. In <i>Eurographics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1202384</ref_obj_id>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Gross, M., and Pfister, H., Eds. 2007. <i>Point-Based Graphics</i>. Morgan Kaufmann Publishers.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280822</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Guenter, B., Grimm, C., Wood, D., Malvar, H., and Pighin, F. 1998. Making faces. In <i>SIGGRAPH '98: Proceedings of the 25th annual conference on Computer graphics and interactive techniques</i>, 55--66.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Kajberg, J., and Lindkvist, G. 2004. Characterisation of materials subjected to large strains by inverse modelling based on in-plane displacement fields. <i>International Journal of Solids and Structures 41</i>, 13, 3439--3459.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Kauer, M., Vuskovic, V., Dual, J., Szekely, G., and Bajka, M. 2002. Inverse finite element characterization of soft tissues. <i>Medical Image Analysis 6</i>, 3, 257--287.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964988</ref_obj_id>
				<ref_obj_pid>1964921</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Kavan, L., Gerszewski, D., Bargteil, A. W., and Sloan, P.-P. 2011. Physics-inspired upsampling for cloth simulation in games. <i>Proc. of ACM SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1281965</ref_obj_id>
				<ref_obj_pid>1281957</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Kazhdan, M., Bolitho, M., and Hoppe, H. 2006. Poisson surface reconstruction. In <i>Symposium on Geometry Processing</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836861</ref_obj_id>
				<ref_obj_pid>1836845</ref_obj_pid>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Kunitomo, S., Nakamura, S., and Morishima, S. 2010. Optimization of cloth simulation parameters by considering static and dynamic features. In <i>ACM SIGGRAPH Posters</i>, 15:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Lang, J., Pai, D. K., and Woodham, R. J. 2002. Acquisition of elastic models for interactive simulation. <i>International Journal of Robotics Research 21</i>, 8, 713--733.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Lin, I.-C., and Ouhyoung, M. 2005. Mirror mocap: Automatic and efficient capture of dense 3d facial motion parameters from video. <i>The Visual Computer 21</i>, 6, 355--372.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409074</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Jones, A., Chiang, J.-Y., Hawkins, T., Frederiksen, S., Peers, P., Vukovic, M., Ouhyoung, M., and Debevec, P. 2008. Facial performance synthesis using deformation-driven polynomial displacement maps. <i>ACM Trans. Graphics (Proc. SIGGRAPH Asia) 27</i>, 5, 121.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409074</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Jones, A., Chiang, J.-Y., Hawkins, T., Frederiksen, S., Peers, P., Vukovic, M., Ouhyoung, M., and Debevec, P. 2008. Facial performance synthesis using deformation-driven polynomial displacement maps. <i>ACM Trans. Graph. (Proc. of ACM SIGGRAPH Asia) 27</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2318912</ref_obj_id>
				<ref_obj_pid>2318896</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Miguel, E., Bradley, D., Thomaszewski, B., Bickel, B., Matusik, W., Otaduy, M. A., and Marschner, S. 2012. Data-driven estimation of cloth simulation models. <i>Computer Graphics Forum (Proc. of Eurographics) 31</i>, 2.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[mview. http://vision.middlebury.edu/mview/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Open source computer vision library.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383268</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Pai, D. K., van den Doel, K., James, D. L., Lang, J., Lloyd, J. E., Richmond, J. L., and Yau, S. H. 2001. Scanning physical interaction behavior of 3d objects. In <i>Proceedings of ACM SIGGRAPH 2001</i>, Computer Graphics Proceedings, Annual Conference Series, 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Schnur, D. S., and Zabaras, N. 1992. An inverse method for determining elastic material properties and a material interface. <i>International Journal for Numerical Methods in Engineering 33</i>, 10, 2039--2057.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Scholz, V., Stich, T., Keckeisen, M., Wacker, M., and Magnor, M. 2005. Garment motion capture using color-coded patterns. In <i>Proc. Eurographics</i>, 439--448.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Schoner, J. L., Lang, J., and Seidel, H.-P. 2004. Measurement-based interactive simulation of viscoelastic solids. <i>Computer Graphics Forum (Proc. Eurographics) 23</i>, 3, 547--556.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153518</ref_obj_id>
				<ref_obj_pid>1153170</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Seitz, S. M., Curless, B., Diebel, J., Scharstein, D., and Szeliski, R. 2006. A comparison and evaluation of multi-view stereo reconstruction algorithms. In <i>CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057456</ref_obj_id>
				<ref_obj_pid>1057432</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Sorkine, O., Cohen-Or, D., Lipman, Y., Alexa, M., R&#246;ssl, C., and Seidel, H.-P. 2004. Laplacian surface editing. In <i>Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing</i>, 175--184.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360696</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Vlasic, D., Baran, I., Matusik, W., and Popovi&#263;, J. 2008. Articulated mesh animation from multi-view silhouettes. <i>ACM Trans. Graphics (Proc. SIGGRAPH)</i>, 97.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1559762</ref_obj_id>
				<ref_obj_pid>1559755</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Volino, P., Magnenat-Thalmann, N., and Faure, F. 2009. A simple approach to nonlinear tensile stiffness for accurate cloth simulation. <i>ACM Trans. Graph. 28</i>, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Wand, M., Mitra, N., Pauly, M., Chang, W., and Li, H. 2012. Dynamic geometry processing. In <i>Eurographics Tutorials</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778844</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Wang, H., Hecht, F., Ramamoorthi, R., and O'Brien, J. 2010. Example-based wrinkle synthesis for clothing animation. <i>ACM Transactions on Graphics 29</i>, 4 (July), 107:1--107:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964966</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[Wang, H., Ramamoorthi, R., and O'Brien, J. 2011. Data-driven elastic models for cloth: Modeling and measurement. <i>ACM Trans. Graph. (Proc. SIGGRAPH) 30</i>, 4, 71.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276420</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[White, R., Crane, K., and Forsyth, D. 2007. Capturing and animating occluded cloth. <i>ACM Trans. Graphics (Proc. SIGGRAPH)</i>, 34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1896343</ref_obj_id>
				<ref_obj_pid>1896300</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Wilburn, B., Joshi, N., Vaish, V., Levoy, M., and Horowitz, M. 2004. High-speed videography using a dense camera array. In <i>Proc. of CVPR</i>, vol. 2, 294--301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>97906</ref_obj_id>
				<ref_obj_pid>97880</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1990. Performance-driven facial animation. <i>SIGGRAPH Comput. Graph. 24</i>, 4, 235--242.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Zhang, Z. 1999. Flexible camera calibration by viewing a plane from unknown orientations. In <i>ICCV</i>, 666--673.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Zurdo, J. S., Brito, J. P., and Otaduy, M. A. 2012. Animating wrinkles by example on non-skinned cloth. <i>IEEE Trans. on Visualization and Computer Graphics</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Data-Driven Simulation Methods in Computer Graphics: Cloth, Tissue and Faces http://www.gmrv.es/SIG12Course 
Miguel A. Otaduy URJC Madrid Bernd Bickel Disney Research Zurich Huamin Wang Ohio State University Derek 
Bradley Disney Research Zurich Contents 1 Introduction 3 1.1 CourseStructure ............................................ 
3 1.2 CourseSchedule ............................................ 5 1.3 CourseNotes .............................................. 
5 1.4 Instructors................................................ 5 2 Overview of Data-Driven Simulation 
Methods 7 2.1 Example1:Data-DrivenFacialWrinkles................................ 7 2.2 Example2:Data-DrivenSoftTissue 
.................................. 9 2.3 Classi.cationofMethods ........................................ 
10 3 Capturing Geometry and Forces in Real Deformation Examples 12 3.1 CamerasandLights ........................................... 
12 3.2 GeometryReconstruction........................................ 13 3.3 TemporalTracking ........................................... 
17 3.4 ActuationandForces .......................................... 21 3.5 DeformationExamples ......................................... 
23 4 Modeling Nonlinear Soft Tissue from Captured Mechanical Data 26 5 Data-Driven Modeling of Nonlinear 
Elasticity in Cloth 42 6 Animation of Faces with Data-Driven Wrinkles 63 7 Clothing Animation with Wrinkle 
Synthesis from Examples 79 8 Outlook 92 1 Introduction In recent years, the .eld of computer animation 
has witnessed the invention of multiple simulation methods that exploit pre-recorded data to improve 
the performance and/or realism of dynamic deformations. Various methods have been presented concurrently, 
and they present differences, but also similarities, that have not yet been analyzed or discussed. This 
course focuses on the application of data-driven methods to three areas of computer animation, namely 
dynamic deformation of faces, soft volumetric tissue, and cloth. The course describes the particular 
chal­lenges tackled in a data-driven manner, classi.es the various methods, and also shares insights 
for the application to other settings. The explosion of data-driven animation methods and the success 
of their results make this course extremely timely. Up till now, the proposed methods have remained familiar 
only at the research context, and have not made their way through computer graphics industry. This course 
aims to .t two main purposes. First, present a common theory and understanding of data-driven methods 
for dynamic deformations that may inspire the development of novel solutions, and second, bridge the 
gap with industry, by making data-driven approaches accessible. The course targets an audience consisting 
of both researchers and programmers in computer animation. 1.1 Course Structure Current data-driven methods 
for dynamic deformation exploit pre-recorded data in one of two ways. Some methods build on traditional 
mechanical models to simulate deformations of soft tissue [Pai et al. 2001; Lang et al. 2002; Schoner 
et al. 2004; Schnur and Zabaras 1992; Becker and Teschner 2007; Kauer et al. 2002; Kajberg and Lindkvist 
2004; Bickel et al. 2009; Bickel et al. 2010] or cloth [Breen et al. 1994; Eberhardt et al. 1996; Volino 
et al. 2009; Bhat et al. 2003; Kunitomo et al. 2010; Wang et al. 2011; Miguel et al. 2012], but parameterize 
those models in a versatile fashion by interpolation of parameter values estimated from real deformation 
examples. Other methods, on the other hand, interpolate geometric information on cloth [Wang et al. 2010; 
de Aguiar et al. 2010; Feng et al. 2010; Kavan et al. 2011; Zurdo et al. 2012] or faces [Bickel et al. 
2008; Ma et al. 2008b], and de.ne the information, the interpolation domains, and the interpolation functions, 
from pre-recorded data. To properly describe each method and facilitate the discussion of differences 
and similarities, the course starts with an overview and classi.cation of the main approaches. Figure 
1: Example of data-driven cloth simulation with the method of [Miguel et al. 2012]. From left to right, 
image of the real cloth, reconstructed geometry, and simulation result. Then, the course dwells on the 
description of methods that rely on mechanical data. The course covers two different applications of 
mechanical-data-driven methods. Fig. 1 shows an example of cloth simulation where cloth deforma­tion 
models have been estimated from a combination of force-and-deformation information in multiple deformation 
examples. A similar strategy is followed in the example in Fig. 2, but this time to estimate solid deformation 
mod­els for soft tissue simulation. The material for the course is combined and adapted from recent publications 
in Figure 2: From left to right: force-and-deformation capture of a non-linear heterogeneous pillow; 
deformation synthesized with .tted material parameters and the method of [Bickel et al. 2009]; and interactive 
deformation under different boundary conditions. data-driven cloth [Wang et al. 2011; Miguel et al. 2012] 
and soft-tissue modeling [Bickel et al. 2009; Bickel et al. 2010]. The course also describes animation 
methods that rely on geometric data. To demonstrate the possibilities of such methods, we present techniques 
that target two distant applications: animation of highly detailed human faces and cloth animation. Fig. 
3 shows facial animation examples where expression wrinkles are synthesized in a data-driven manner. 
Fig. 4, on the other hand, shows cloth animation examples where folds and wrinkles are synthesized in 
a data-driven manner. The material for the course is adapted mostly from recent publications in these 
.elds [Bickel et al. 2008; Wang et al. 2010], but we also draw connections with other related methods 
[Ma et al. 2008b; Kavan et al. 2011], and we discuss the general challenges in de.ning interpolation 
functions and domains. Figure 3: Facial animation example with the method of [Bickel et al. 2008]. From 
left to right: large-scale defor­mation example interpolating mocap markers, full result after example-based 
.ne-scale correction, the same result with full shading, and comparison to the real actor s face. One 
essential component of data-driven simulation methods is data capture, and due to this importance we 
dedicate a chapter of the course to this problem. It shares challenges with performance capture, but 
it suffers additional challenges too. Unlike traditional performance capture, which aims at obtaining 
a reconstruction of arbitrary motion, data capture for data-driven modeling must be designed with the 
purpose of obtaining a suf.cient representation of an object s range of deformations. Therefore, one 
must design deformation examples that visit the desired range of deformations and suit optimization processes. 
Moreover, for mechanical-data-driven methods, the capture process must obtain force information in addition 
to deformation. The material for the course combines and adapts content Figure 4: The method of [Wang 
et al. 2010] uses a precomputed dataset to synthesize cloth wrinkles (a) that are layered onto a coarse 
base simulation (inset). The precomputed dataset can be used to synthesize wrinkles for a wide range 
of poses (b and c). from several recent publications [Bradley et al. 2008a; Bradley et al. 2008b; Bradley 
et al. 2010; Bickel et al. 2009; Wang et al. 2010; Wang et al. 2011; Miguel et al. 2012]. 1.2 Course 
Schedule 9:00 am Introduction and overview of methods [Otaduy] 9:20 am Tissue and cloth mechanics [Bickel/Otaduy] 
9:50 am Capturing deformation examples [Bradley] 10:30 am Break 10:45 am Facial animation [Bickel] 11:25 
am Cloth animation [Wang] 12:05 pm Conclusion / Q &#38; A [all] 12:15 pm Close 1.3 Course Notes The 
course notes begin with an overview and classi.cation of methods in Chapter 2. Chapter 3 covers solutions 
for capturing both deformations and forces in an ample set of applications. Then, Chapters 4, 5, 6, and 
7 cover, respec­tively, methods for mechanical-data-driven simulation of soft tissue, mechanical-data-driven 
simulation of cloth, geometric-data-driven simulation of faces, and geometric-data-driven simulation 
of cloth. The course notes will be progressively re.ned, and updated versions will be available on the 
course web page http://www.gmrv.es/SIG12Course. The web page also provides links to publications with 
supplementary material. 1.4 Instructors Miguel A. Otaduy is an associate professor in the Department 
of Computer Science at Universidad Rey Juan Carlos (URJC Madrid). His main research areas are physically 
based computer animation and haptic rendering. He obtained his BS (2000) on electrical engineering from 
Mondrag´on University, and MS (2003) and PhD (2004) on computer science from the University of North 
Carolina at Chapel Hill. From 2005 to 2008 he was a research associate at ETH Zurich, and then he joined 
URJC Madrid. He has published over 50 papers in computer graphics and haptics, and has recently co-chaired 
the program committees for the ACM SIGGRAPH / Eurographics Symposium on Computer Animation (2010) and 
the Spanish Computer Graphics Conference (2010). He also leads the ERC Starting Grant Animetrics, on 
measurement-based modeling of complex mechanical phenomena. Bernd Bickel is a part-time visiting professor 
at TU Berlin and a post-doctoral researcher at Disney Research Zurich. His research interests include 
computer graphics and its applications in animation, biomechanics, material science, and computational 
design for digital fabrication. Recent work includes next generation 3D surface scanner devices, performance 
capture, measuring and modeling the deformation behavior of soft tissue, and animation tools. Bernd received 
a M.Sc. in Computer Science in 2006 and spent nine month at Mitsubishi Electric Research Laboratories 
under the supervision of Prof. Hanspeter P.ster. He wrote his PhD thesis at ETH Zurich in the Computer 
Graphics Lab headed by Prof. Markus Gross and defended in November 2010. Derek Bradley is a postdoctoral 
researcher at Disney Research Zurich. He completed his Bachelor of Computer Science in 2003 and Master 
of Computer Science in 2005, both at Carleton University in Canada. In 2010, Derek obtained a PhD from 
the University of British Columbia in Canada, and then started with Disney Research Zurich in September 
2010. Derek s main research interest is real-world modeling and animation, primarily through computer 
vision techniques. He works on various 3D reconstruction projects including multiview stereo, facial 
performance capture, and data-driven simulation. Huamin Wang is an assistant professor in the department 
of Computer Science and Engineering, at the Ohio State University. Previously, he was a postdoctoral 
researcher in the EECS department, at the University of California, Berkeley. He received his Ph.D. from 
Georgia Institute of Technology in 2009, M.S. from Stanford University in 2004 and B.Eng. from Zhejiang 
University in 2002. His research interests are in computer graphics, computer vision, and image processing 
techniques that are related to graphics and visualization applications. He is particu­larly interested 
in incorporating real-world data into physically based simulation techniques, so animations can be ef.ciently 
and realistically generated.  2 Overview of Data-Driven Simulation Methods In the interaction with our 
surrounding world, mechanical properties play a major role in how we perceive this world. Motion, deformation, 
.ow, fracture or contact, are all mechanical phenomena that allow us to discriminate materials and objects, 
and to interact with them. Humans have long strived to understand such mechanical phenomena, creating 
simulation models with which we can replicate or predict the outcome of mechanical processes and events. 
It is important to acknowledge that the physical models of the major macroscopic mechanical phenomena 
are already quite well understood. These models have typically been developed in other disciplines such 
as physics, mathematics or various engineering .elds, and they have made their way through computer animation 
accompanied by algorithms that are geared to obtaining the desired perceptual stimuli, sometimes incurring 
in a trade-off between physical realism and interactivity. Even though the underlying physical models 
of mechanical phenomena are quite well understood, these phenomena display other inherent sources of 
complexity that largely limit the applicability of computer animation. Complexity is produced, for example, 
by nonlinear or anisotropic behaviors, by heterogeneous properties, or by a high dynamic range. These 
sources of complexity are typically addressed by designing complex nonlinear constitutive models to describe 
the mechanical behavior. However, these models are implemented using computationally expensive simulation 
algorithms, which largely limit their applicability. Moreover, their parameters are dif.cult and tedious 
to tune, particularly if the properties are heterogeneous. All in all, the animation of complex mechanical 
phenomena is limited by the domain of effects captured by the underlying physical models and their parameterization 
accuracy. Data-driven methods offer an alternative to complex constitutive models, as they turn the modeling 
metaphor into the knowledge of a system s response under several example conditions. This chapter describes 
the data-driven modeling metaphor in the context of computer animation, formulates the mathematics of 
data-driven modeling using two example applications, and introduces a classi.cation of the various existing 
methods. 2.1 Example 1: Data-Driven Facial Wrinkles Let us consider a face mesh, consisting of vertices 
with positions x . R3. Vertex positions can be decomposed into a low-resolution position x0 and a .ne-scale 
displacement .x, expressed in a local reference system for each vertex (i.e., with orientation R): x 
= x0 + R .x. (1) This de.nition of vertex positions essentially decomposes large-scale facial deformation 
(i.e., the overall expression of the face) from the small-scale deformation (i.e., expressive wrinkles). 
If no assumptions are made, the position of each facial vertex can be de.ned independently as a function 
of the facial muscle activations and facial bone con.gurations. Let us group the muscle and bone con.gurations 
in a large vector u. Moreover, due to dynamics, vertex positions are a function of time too. We can write 
this dependency as x = f(u,t). However, due to the repetitive nature of facial expressions, face tissue 
becomes weaker at certain locations, and expressive wrinkles appear in a deterministic fashion. Moreover, 
due the viscoelastic nature of facial tissue, the motion of expressive wrinkles appears damped to the 
human eye. Under these conditions, we can draw the conclusion that .ne-scale wrinkle displacements can 
be de.ned as a function of some low-dimensional state u * . We can write this dependency as .x = f(u 
*). At this point, we have ingredients to de.ne a data-driven model. The potentially high dimensional 
function of vertex positions has been decomposed into a low-resolution position (inherently low-dimensional), 
plus a displacement function that can be de.ned in some low-dimensional state. The remaining open questions 
are: What low-dimensional state u * describes best the .ne-scale displacement?  Is the data-driven 
de.nition of each vertex completely independent, or can we .nd relationships across vertices that allow 
the de.nition of some global low-dimensional state?  It turns out that, for a vertex, the local strain 
of the low-resolution face representation serves as a good low­dimensional state. In other words, the 
existence of expressive wrinkles is closely related to the local strain of the surface. There are multiple 
choices of strain metrics, and the only major condition for the selection of a strain metric u * is that 
is should be invariant to rigid body motion. Fig. 5 shows the correlation between wrinkles and strain 
de.ned through edge deformations. Figure 5: Correlation between expressive wrinkles and local low-resolution 
strain (measured through edge deforma­tions). The existence of this correlation enables the de.nition 
of a natural low-dimensional domain for interpolating wrinkle displacement data. Once a low-dimensional 
state is selected, the problem is ready for data collection. In our example, the collected data consists 
of vertex displacement values {.xi} and local strain values {u *} in correspondence. This data enables 
i the approximation of the function f through learning methods. One popular example is scattered-data 
interpolation ¯ based on radial basis functions. Then, the approximate function f can be formally de.ned 
as: ** ** ¯ .x ~ f(u , {.xi}, {ui })=.i f(u , u ), (2) i i where f represents a radial basis function, 
and the weights .i are estimated as those that .t best the input data {.xi}. The approach described so 
far is successful at describing vertex positions in a data-driven manner, but de.nes the position of 
each vertex in a completely independent manner, and may suffer from spatial discontinuities. Ideally, 
we seek a solution that ensures spatial continuity (and smoothness). The solution is to impose conditions 
on the captured data and the output of the learning stage, to ensure that vertex displacements are de.ned 
based on continuous (and smooth) functions. [Bickel et al. 2008] achieve continuity by building their 
learning technique as a weighted pose­space deformation method. Their approach is described in detail 
in Chapter 6. From this example, we can draw several important general conclusions. First, there are 
certain animation settings that can be modeled ef.ciently through interpolation of geometric information 
obtained from representative examples. Second, to .nd a function that can be described through interpolation, 
one often successful approach is to decompose the geometric representation in a multi-scale fashion. 
And third, the de.nition of an effective interpolation domain can be simpli.ed through the projection 
of the data to a low-dimensional domain. In the case of expressive wrinkles, local low-resolution strain 
constitutes a natural low-dimensional domain. 2.2 Example 2: Data-Driven Soft Tissue Let us consider 
a deformable solid discretized with tetrahedra. A vector x concatenates the positions of all nodes in 
the solid, and a vector F concatenates the internal forces (due to elastic deformation) acting on the 
nodes. Under linear elasticity theory, the internal forces are simply proportional to the amount of deformation, 
measured as the deviation from the rest con.guration x0. The linear relationship between deformation 
and forces is called the stiffness matrix K, and can be computed using the Finite Element Method. For 
a homogeneous material, this stiffness matrix depends solely on the structure of the tetrahedral mesh 
and two material parameters: Young modulus (E) and Poisson ratio (.). Then, we can formally de.ne the 
internal forces of the solid as: F = -K(E, .)(x - x0). (3) Unfortunately, real materials are nonlinear, 
and two parameters hardly describe real elastic behavior. The traditional solution to tackle this problem 
is to turn to more complex constitutive models, not just linear. However, in a local neighborhood of 
a given deformation state, a linear model is typically a good descriptor of the material. The complete 
deformation state of the solid can be described by concatenating the strain tensors of all tetrahedra 
in a large vector u. Then, the local linear behavior of the material can be de.ned as a function of the 
deformation state, [E, .]= f(u). It turns out that, by describing separately the material parameters 
of each tetrahedron, the local linear behavior is well described as a function of the local strain of 
the tetrahedron itself, which constitutes a considerably lower­dimensional domain. Then, if we de.ne 
the strain of just one tetrahedron with a vector u *, the local linear behavior of that particular tetrahedron 
can be de.ned as a function [E, .]= f(u *). Fig. 6 shows two example distributions of Young modulus under 
different strains. Figure 6: The top row shows a deformable pillow under two different external forces. 
The bottom row compares the distribution of values of Young modulus that best describe the behavior of 
the pillow under these two forces. Now, we have reached the ingredients for a data-driven method. Following 
Example 1, the collected data should consist of material parameter values {[Ei,.i]} and local strain 
values {u *} in correspondence. Unfortunately, mate­ i rial parameter values are dif.cult to be directly 
measured on real solid objects. Instead, we opt to collect measurable data, in particular external force 
values {Fext,i} and position values {xi} in correspondence. If the data is collected under equilibrium 
conditions, we can relate nodal positions and applied forces through a quasi-static deformation problem, 
x = K(E,.)-1 (Fext + Fother)+ x0. (4) This relationship should hold for all collected pairs of force 
and deformation data, and this fact will help us estimate the function f that relates tetrahedral strain 
to material parameter values. To make the problem speci.c, and similar ¯ to Example 1, we can de.ne an 
approximate function f through radial-basis-function interpolation: ** ** ¯ [E, .] ~ f(u , {[Ei,.i]}, 
{ui })= .i f(u , u ), (5) i i But recall that, unlike Example 1, in this case the parameter values {[Ei,.i]} 
are unknown, and the strains {u *}cannot be directly measured either. Instead, the strains {u *} will 
be sampled to suf.ciently cover the range of i strains in the collected data, and, most importantly, 
the unknowns of the problem, i.e., the weights of the radial basis functions, .i, will be estimated by 
solving an optimization problem. The error function for this optimization problem can be de.ned as the 
Euclidean norm between measured positions {xj} and positions estimated using the data-driven method, 
c = j   -1 ** K.i f(u , ui )(Fext,j + Fother)+ x0 - xj i  2 . (6) [Bickel et al. 2009] build on 
a similar data-driven approach to model nonlinear heterogeneous soft tissue, but they select different 
material parameters that simplify the optimization problem. Chapter 4 describes their approach in detail. 
From this example, we can draw several important general conclusions. First, there are certain animation 
settings where mechanical parameters can be modeled ef.ciently through data-driven interpolation. In 
the case of nonlinear elasticity for soft-tissue deformation, the nonlinear behavior can be modeled through 
interpolation of local linear models. However, unlike the previous example, parameter data may not be 
directly measured from examples, which brings us to the second conclusion. By collecting force and deformation 
data from examples, interpolation weights for the model of mechanical parameters can be estimated through 
numerical optimization. 2.3 Classi.cation of Methods To classify data-driven simulation methods in computer 
animation, we assume that their .nal output consists of the (deformed) geometry of simulated objects. 
Then, this geometry is used in the context of rendering algorithms to generate synthetic images of the 
simulated scene. Drawing from the two examples described above, we can draw a clear classi.cation of 
data-driven simulation methods into two major categories. One category, represented by Example 1, models 
in a data-driven manner the geometry itself. The other category, represented by Example 2, models in 
a data-driven manner some mechanical parameters, and the geometry is obtained as a result of a mechanical 
model. Then, we distinguish between geometric-data-driven methods and mechanical-data-driven methods. 
In both cases, the data collected in examples includes geometric information (i.e., deformation), but 
in mechanical-data-driven methods this data should be augmented with force information. Both categories 
of methods may share techniques for learning, interpolation, or subspace projection. But in mechanical-data-driven 
methods, the optimization procedures for model .tting require objective functions that account for the 
mechanical process that relates model parameters to deformation. Based on our dichotomy of methods, a 
representative (although not exhaustive) list of data-driven simulation meth­ods in computer graphics 
(for cloth, tissue and faces) can be classi.ed as follows: Geometric-data-driven methods for cloth [Wang 
et al. 2010; Feng et al. 2010; Kavan et al. 2011; Zurdo et al. 2012].  Geometric-data-driven methods 
for faces [Bickel et al. 2008; Ma et al. 2008b].  Mechanical-data-driven methods for solid tissue [Pai 
et al. 2001; Lang et al. 2002; Schoner et al. 2004; Schnur and Zabaras 1992; Becker and Teschner 2007; 
Kauer et al. 2002; Kajberg and Lindkvist 2004; Bickel et al. 2009].  Mechanical-data-driven methods 
for cloth [Breen et al. 1994; Eberhardt et al. 1996; Volino et al. 2009; Bhat et al. 2003; Kunitomo et 
al. 2010; Wang et al. 2011; Miguel et al. 2012].   3 Capturing Geometry and Forces in Real Deformation 
Examples In this section we will discuss the process of capturing deformation examples for data-driven 
simulation. Recently, many different techniques have emerged for capturing the 3D deformation of real 
surfaces such as cloth [Scholz et al. 2005; White et al. 2007; Bradley et al. 2008b; Furukawa and Ponce 
2008] and faces [Furukawa and Ponce 2009; Bradley et al. 2010; Beeler et al. 2011]. These methods primarily 
use vision-based approaches to acquire both the time-varying shape and corresponding motion of the surface. 
When capturing deformation examples for simulation we can make use of these general methods, however 
recovering only shape and motion is typically not enough. In the simulation setting, we must also reconstruct 
the forces that act on the surface and measure the complete answer that should be predicted by a simulator. 
This additional challenge often leads to additional capture hardware and specialized reconstruction algorithms. 
Another point to consider is that the choice of deformation examples can be more critical when considering 
that the reconstructions will be used in a simulation setting. Often we wish to explore the full range 
of a material s strain space, possibly exciting different subsets of strain independently. As an example, 
we may wish to separate the weft strain from the warp strain when deforming a piece of cloth, or actuate 
different face muscles independently in order to isolate speci.c facial expressions. Several recent methods 
have successfully combined traditional reconstruction algorithms with novel capture tech­niques for data-driven 
simulation [Bickel et al. 2009; Wang et al. 2011; Miguel et al. 2012]. These methods form the main focus 
of our discussion in this course. Here we will give an overview of the related capture setups and recon­struction 
algorithms, starting with the basics of Cameras and Lights (3.1), algorithms for Geometry Reconstruction 
(3.2), computing deformation through Temporal Tracking (3.3), obtaining the complete picture of Actuation 
and Forces (3.4), and .nally concluding with some hints on which Deformation Examples (3.5) might make 
sense to capture. 3.1 Cameras and Lights When designing a capture setup, some thought should go into 
the choice of cameras to use. The .rst question is whether you need video or still cameras, and this 
is depends on the examples you wish to capture. Wang et al. [2011] showed that different cloth strains 
can be isolated in a static way, in which case still cameras such as digital SLRs are suf.cient. Still 
cameras are often used for capturing isolated facial expressions as well [Beeler et al. 2010]. In many 
cases, however, you will want to capture moving surfaces using video cameras. The choice of video cameras 
depends less on the capture application and more on budget. Two options are scienti.c machine vision 
cameras or off-the-shelf consumer camcorders. In addition to cost, other factors to consider are camera 
synchronization, rolling shutter distortions, and system portability. Fig. 7 outlines the tradeoffs between 
the two. The primary bene.t of machine vision cameras is that they can be perfectly synchronized using 
a hardware trigger. They also provide raw, uncompressed images captured with a global shutter model (i.e. 
every pixel is exposed at the exact same time). If budget is not an issue then machine vision cameras 
are the recommended way to go. On a stricter budget, consumer camcorders are evolving as promising alternatives 
to scienti.c cameras in many computer vision applications [Bradley et al. 2008b; Bradley et al. 2010; 
Atcheson et al. 2008]. They offer high resolution and guaranteed high frame rates at a signi.cantly reduced 
cost. Also, integrated hard drives or other storage media eliminate the need to transfer video sequences 
in real-time to a computer, making multi-camera setups more portable. There are two challenges that currently 
limit the use of such camcorders, especially in multi­camera and camera array applications. First, consumer 
camcorders typically do not have support for hardware synchronization. Second, in contrast to the global 
shutter model of scienti.c cameras, most consumer cameras employ a rolling shutter, in which the individual 
scanlines use a slightly different temporal offset for the exposure interval (see, e.g. Wilburn et al. 
[2004]). An illustration of this camera model is shown in Fig. 8. The resulting frames represent a sheared 
slice of the spatio-temporal video volume that cannot be used directly for many computer vision applications. 
Bradley et al. [2009] have proposed a solution to the synchronization and rolling shutter problem by 
 Figure 7: Trade-offs between machine vision cameras and consumer camcorders. capturing under stroboscopic 
illumination. Strobe lights provide short pulses of illumination, exposing the scene to all cameras at 
the same time. Even in the rolling shutter model, this approach will optically synchronize all scanlines 
across all cameras. Fig. 9 illustrates this idea and shows experimental results of synchronizing consumer 
camcorders. Beeler et al. [2010] also use triggered .ashes to synchronize multiple digital SLRs for face 
reconstruction. The tradeoff of these techniques is that more sophisticated lighting hardware is required, 
and capture must occur in a dimly-lit indoor environment. A .nal point on cameras is calibration. Simple 
white-balancing is often suf.cient for radiometric calibration, but more sophisticated color calibration 
can also be achieved by photographing a color calibration chart. For geometric calibration we must determine 
the intrinsic parameters, which de.ne how the camera forms an image, and a set of extrinsic parameters, 
which de.ne the position and orientation of the camera in the world. In most cases, the common calibration 
technique of Zhang [1999] will suf.ce. This method is widely used and an implementation is readily available 
in the OpenCV library [ope ]. The basic idea is to capture a number of images of a planar calibration 
target with known proportions, and then solve for all the camera parameters such that the reprojection 
error of the target is minimized. Some camera setups, such as a hemispherical camera array, require more 
sophisticated calibration techniques. We refer to examples such as Beeler et al. [2010] and Bradley and 
Heidrich [2010]. 3.2 Geometry Reconstruction Reconstructing a deforming surface is often a two-step 
process. First, the geometry of the surface is acquired, capturing the changing shape of the surface 
over time. Second, the motion of the surface is extracted, recovering the full 3D deformation. This section 
describes methods for recovering shape. There exists a large body of computer vision literature on reconstructing 
shape from images. A good survey can be found in Seitz et al. [2006]. One approach is to keep things 
simple, if your simulation environment allows it. Wang Figure 8: Rolling shutter camera model, with 
time as the horizontal axis and scanlines as the vertical axis. The blue region indicates the exposure. 
.e is the exposure time, .t is the frame duration (one over the frame rate), S is the total number of 
scanlines per frame, and t(j) is the read-out time of the topmost (visible) scanline in frame j. The 
just-in-time exposure and readout of the individual scanlines creates a shear along the time axis. Figure 
9: Stroboscopic Illumination. Top: A .ash of light in a dark room exposes all scanlines simultaneously, 
removing the rolling shutter distortion. The image is split across two consecutive frames, but can be 
combined in a post-process. Bottom: Strobe lighting synchronizes two consumer camcorders observing a 
falling ball. As a side effect, motion blur is also removed. et al. [2011] show that in-plane cloth deformation 
can be reconstructed in image-space from a single view and a few labelled feature points (see Fig. 10). 
More complicated 3D shape recovery typically requires several cameras and multi-view reconstruction algorithms 
(for example, Fig. 11).  A good starting point for multi-view reconstruction algorithms is the Patch-based 
Multi-View Stereo (PMVS) ap­proach of Furukawa and Ponce [2010]. Their method begins by matching features 
across multiple pictures to obtain a sparse set of corresponding patches, which are then repeatedly expanded 
to spread the initial matches to nearby pixels until a dense set of correspondences are found. This method 
performs well on benchmark datasets [Seitz et al. 2006; mview ], and the software is available online 
(http://grail.cs.washington.edu/software/pmvs/). The authors have also extended this approach to be usable 
for dense motion capture from video streams of garments [Furukawa and Ponce 2008] and faces [Furukawa 
and Ponce 2009]. An alternative approach is the method of Bradley et al. [2008a], which aims for both 
accuracy and ef.ciency. When reconstructing many frames of a deforming surface, ef.cient runtimes are 
favorable. This technique has been used for several applications of reconstructing deforming surfaces 
[Bradley et al. 2008b; Bradley et al. 2010; Miguel et al. 2012], and so it is a good choice for creating 
deformation examples for simulation. The method is performed in two steps: binocular stereo on image 
pairs, followed by surface reconstruction. Since software is not available, in the following we provide 
more details for the implementation of this technique. Like most reconstruction algorithms, the input 
is a set of calibrated images, captured from different viewpoints around the object to be reconstructed. 
A segmentation of the object from the background should be provided, so that the visual hull is represented 
as a set of silhouette images. This is easy to achieve if you can capture in front of a green screen 
or dark background. As we mentioned, the method is performed in two steps, binocular stereo and surface 
reconstruction. Each step is broken down into individual stages, as illustrated in Fig. 12. The binocular 
stereo part of the algorithm creates depth maps from pairs of adjacent viewpoints. First, image pairs 
are recti.ed [Fusiello et al. 2000] so that each scanline in one image corresponds to exactly one scanline 
in the other image. The depth of each pixel in one image is then computed by .nding the corresponding 
pixel along the scanline in the other image and then triangulating. Matching individual pixels can lead 
to many errors, so a common approach is to match local neighborhoods instead, known as window-matching. 
Two local neighborhoods of N pixels v0 and v1 can be matched using Normalized Cross Correlation (NCC): 
N2 (v0(j) - v0) · (v1(j) - v1)NCC(v0,v1)= j=1, (7) N2 N2 (v0(j) - v0)2 ·(v1(j) - v1)2 j=1 j=1 where v0 
and v1 represent intensity averages over the neighborhoods. An NCC value of 1 indicates a perfect match, 
and -1 is the worst possible match. Bradley et al. [2008a] use NCC in a robust window-matching procedure 
that com­pensates for perspective distortions by matching under various non-uniform window scales. This 
feature improves quality in high curvature regions (like the buckling of cloth) and for large camera 
base-lines (which allows for setups with fewer cameras). The depth images from the binocular stereo pairs 
are converted to 3D points through triangu­lation and simply merged into a single dense point cloud. 
The second part of the algorithm aims at reconstructing a triangular mesh from the point cloud. It consists 
of three steps: 1. Downsampling: The point cloud is usually much denser than required for reproducing 
the amount of actual detail present in the data. The .rst step is thus to downsample the data using hierarchical 
vertex cluster­ing [Boubekeur et al. 2006]. 2. Cleaning: The simpli.ed point cloud remains noisy. While 
some methods integrate the noise removal in the meshing algorithm [Kazhdan et al. 2006], others feel 
that this important data modi.cation must be controlled explicitly, prior to any decision concerning 
the mesh connectivity. In this reconstruction algorithm, the prob­lem is addressed at the point level 
using point-based .ltering tools (see [Alexa et al. 2004; Gross and P.ster 2007] for an introduction), 
producing a .ltered point set. 3. Meshing: The .nal step is to generate a triangle mesh without introducing 
excessive smoothing. Building on lower dimensional triangulation methods [Boubekeur et al. 2005; Gopi 
et al. 2000], triangle mesh patches are created in 2D and then lifted to 3D as mini-height.elds. This 
approach is fast and runs locally, ensuring scalability and good memory-computational complexity.  Here 
we have brie.y described only two out of the multitude of multi-view stereo algorithms that have been 
pub­lished (currently over 50 are evaluated at http://vision.middlebury.edu/mview/eval/). Choosing the 
right method for your application can be a challenging and time-consuming process. Our hope is to provide 
enough background and resources to .nd the best reconstruction algorithm that suits your needs. 3.3 
Temporal Tracking The geometry reconstruction algorithms from the previous section can be used to compute 
a triangle mesh per-frame of the deforming surface. While it is important to obtain the time-varying 
shape, the full 3D deformation must also include surface tracking, such that the 3D motion of each surface 
point is reconstructed. A convenient way to represent the deformation is a triangle mesh with constant 
connectivity over time and varying vertex positions. In this section we discuss different ways to perform 
temporal tracking and couple the result with the per-frame geometry to obtain reconstructed surface deformations. 
In this course we will focus on optical tracking, where it is assumed we have images of the deforming 
surface. When images are not available, a complementary form of tracking can be used, which relies entirely 
on the deforming geometry. For example, non-rigid shape registration can generate dense surface correspondences 
over time. A good overview of these techniques is given in the recent Eurographics Tutorial by Wand et 
al. [2012]. In the remainder of this section, we focus our discussion on image-based tracking methods. 
Early work in tracking deforming surfaces was to use hand-placed markers which can be identi.ed and tracked 
with ease [Williams 1990; Guenter et al. 1998]. This idea has led to great success in marker-based facial 
performance capture [Lin and Ouhyoung 2005; Bickel et al. 2008; Ma et al. 2008a], which currently drives 
facial animation in the entertainment industry. For cloth, some of the .rst research in capturing garment 
motion from video has also employed marker-based techniques [Scholz et al. 2005; White et al. 2007]. 
These methods use a unique encoding of color marker arrays to locate speci.c points on a garment over 
time. Fig. 13 shows a few examples of marker-based reconstruction and motion capture for cloth. More 
recent research has shown that dense motion capture can be achieved in a markerless setting, if the surface 
can be painted with a high-frequency texture [Furukawa and Ponce 2009; Miguel et al. 2012] or with high 
enough image resolution to use .ne details as surface texture [Bradley et al. 2010; Beeler et al. 2011]. 
Furukawa and Ponce track face motion starting with their previous work on dense 3D motion capture [Furukawa 
and Ponce 2008], which assumes tangentially rigid motion, and then introducing a new tangential regularization 
method capable of dealing with the stretching, shrinking and shearing of deformable surfaces such as 
skin [Furukawa and Ponce 2009]. The methods of Bradley et al. [2010], Beeler et al. [2011] and Miguel 
et al. [2012] all rely on dense 2D optical .ow in order to compute the 3D surface motion. Optical .ow 
is an image-space vector .eld that encodes the motion of the pixels from one frame to the next in a video 
sequence (see Baker et al. [2011] for a survey of techniques). Although each reconstruction approach 
varies slightly in the use of optical .ow for 3D tracking, the general ideas are similar. For the purpose 
of notation, lets call the per-frame geometry reconstructions Gt, where t corresponds to the frame number 
or time. These meshes can be the raw result of the reconstruction algorithms described in the previous 
section. Given Gt and the optical .ow .elds of each input video, we would like to generate a set of compatible 
meshes Mt that have the same connectivity as well as explicit vertex correspondence. That is to say, 
we desire one mesh that deforms over time. Without loss of generality, we can choose M0 to represent 
the global topology, and then the goal is to track M0 forward (and possibly backwards) in time to establish 
the mesh sequence Mt. The basic t-1 tracking approach is illustrated on face meshes in Fig. 14, and it 
proceeds as follows. For each vertex v of Mt-1 i we project the vertex onto each camera c in which it 
is visible (i.e. inside the .eld of view of and not occluded). Let pi,c be this projected pixel. We then 
look up the 2D .ow vector that corresponds to pi,c and add the .ow to get a new pixel location p. Back-projecting 
from ponto Gt gives us a guess for the new vertex location, which we i,ci,c call t vi,c. ¯ The .gure 
illustrates the 3D motion estimation for vertex vi according to one camera, c. The estimates from all 
cameras can be combined in a weighted average, giving more in.uence to the cameras that have a better 
view of the surface point: n ¯i t v = w t i,c · ¯ t vi,c, (8) c=1 t where wi,c is the dot product 
between the surface normal at ¯ t v i,c and the vector from there to c. Since each vertex is updated 
independently, a regularization step avoids possible triangle-.ips and removes any unwanted artifacts 
that may have been present in the initial reconstruction. A common regularization approach for meshes 
stems from Laplacian surface editing [Sorkine et al. 2004]. Following de Aguiar et al. [2008], we solve 
a least-squares Laplacian system using cotangent weights and the current positional constraints ¯ vit 
. Thus, we generate the .nal mesh Mt by minimizing tt arg min { vi - v¯i 2 +a Lvt - Lv0 2}, (9) vt where 
L is the cotangent Laplacian matrix. The parameter a controls the amount of regularization. Repeatedly 
propagating the mesh through time using optical .ow can lead to several unpleasant artifacts (illustrated 
in Fig. 15). First, optical .ow tracks can be lost due to occlusion. Second, it is generally well-known 
that optical­.ow based tracking methods suffer from accumulation of error, known as drift. Lets .rst 
consider drift. Although the error from one frame to the next is usually small and imperceptible, the 
error can accumulate over time, resulting in incorrect motion estimation. Drift typically occurs because 
optical .ow is computed between successive video frames only. If it were possible to accurately compute 
.ow between the .rst video image and every other frame, there would be no accumulation of error. Unfortunately, 
most temporally distant video images in a capture sequence are usually too dissimilar to consider this 
option. Bradley et al. [2010] and Beeler et al. [2011] present two different solutions to this problem. 
 Bradley et al. [2010] compute a 2D parameterization of the surface (or a UV-map) and then build per-frame 
texture images from the input videos. Two example texture images are given in Fig. 16. Every vertex of 
the mesh has unique 2D coordinates in the parameter domain, yielding a one-to-one mapping between 2D 
and 3D mesh triangles. Their main observation is that the texture domain of the mesh remains constant 
over time, which means that the computed per-frame texture images are all very similar. Any temporal 
drift in the 3D geometry appears as a small 2D shift in the texture images, which can easily be detected, 
again by optical .ow. Automatic drift correction is then implemented as follows. After computing the 
geometry Mt and texture T t for a given frame, optical .ow is computed between the textures T 0 and T 
t . This .ow (if any is detected) is then used to update Mt on a per-vertex basis using the direct mapping 
between the geometry and the texture. Any shift in texture space becomes a 3D shift along the mesh surface. 
After updating the vertices to account for drift, Laplacian regularization is applied to avoid possible 
triangle .ips. Beeler et al. [2011] take a different approach to eliminating drift in the reconstructed 
sequence. Leveraging the fact that facial performances often contain repetitive subsequences, their method 
identi.es so-called anchor frames as those which contain similar facial expressions to a manually chosen 
reference expression. Anchor frames are automatically computed over one or even multiple performances. 
This method introduces a robust image-space tracking method that computes pixel matches directly from 
the reference frame to all anchor frames, and thereby to the remaining frames in the sequence via both 
forward and backward sequential matching. This allows the propagation of one reconstructed frame to an 
entire sequence in parallel, in contrast to the previous sequential methods. This anchored reconstruction 
approach limits tracking drift, since every anchor frame brings the tracking error back to (nearly) zero. 
The idea of using anchor frames also helps to overcome additional problems with sequential motion tracking, 
and that is occlusion and motion blur. Sequential tracking methods would fail during an occlusion or 
blurred frame, thus losing track of the surface and would not be able to recover. As a result, the motion 
for the remainder of the performance would not be reconstructed. The anchor frame reconstruction framework 
can recover from such tracking failure, as long as an anchor frame exists later on in the sequence. The 
main drawback of the anchor frame approach is that it is designed speci.cally for deformation sequences 
that return to a similar pose often throughout the sequence. With arbitrary non-cyclic deformations, 
you may not experience the bene.ts of this algorithm. Resulting facial performance reconstructions using 
the methods of Bradley et al. [2010] and Beeler et al. [2011] are shown in Fig. 17 and Fig. 18, respectively. 
Each .gure shows a reference image, the reconstructed geometry, and the reconstructed motion illustrated 
with a grid overlaid on the deforming surface.  While these tracking methods have been described in 
the context of facial performance capture, similar tracking methods can be applied to any smooth deforming 
surface. The deformation examples in the data-driven cloth simu­lation work of Miguel et al. [2012] were 
reconstructed using the same techniques. 3.4 Actuation and Forces So far we have discussed different 
physical capture setups and various methods for 3D reconstruction of shape and motion tracking for deforming 
surfaces. In order to capture examples for simulation, we often have additional requirements. First, 
we should have complete control of the surface in order to actuate very speci.c deformations. Secondly, 
we must reconstruct the forces that act on the surface and measure the complete answer that should be 
predicted by a simulator. These additional challenges often lead to additional capture hardware and specialized 
reconstruction algorithms. In this section we will outline some of the physical setups and algorithmic 
approaches that have been used to acquire this additional information. Bickel et al. [2009] acquire a 
set of example deformations of real objects, such as soft pillows and faces, including force information 
using a simple capture system. Their acquisition system consists of force probes and a marker­based reconstruction 
(see Fig. 19). Deformations are induced by physical interaction with the object, meeting the requirement 
of having complete control of the surface. The second requirement (reconstructing forces), is met by 
building contact probes with arbitrary shapes and circular disks of different diameters attached to the 
tip of a long screwdriver. The position and orientation of the contact probe is estimated using two makers 
on the white shaft of the screwdriver. To measure the magnitude of the contact forces, a force sensing 
resistor is used. Wang et al. [2011] devise a setup for measuring in-plane cloth deformations using weights 
to create speci.c forces (see Fig. 20). A piece of cloth is mounted vertically, with the top and bottom 
edges sandwiched between a pair of wooden slats to constrain the motion in a controlled way. The left 
and right edges of the cloth are attached to clips in the middle of each edge. These locations are treated 
as boundary conditions and their positions can be easily measured using a calibrated camera. Wires are 
attached to the clips and then weights are attached to the other end of the wires over pulleys in order 
to apply tension. The top edge of the cloth sample is attached to the top of the testing board, while 
the other three edges have freedom to move. Different weights are applied on these three edges in order 
to drag the cloth sample into different shapes. The left and right sides are loaded with the same weights 
so that the sample does not lose its balance during the experiment. This is a simple mechanism to control 
the magnitude of the force applied to the cloth, and is attractive since the setup is inexpensive and 
no specialized hardware is required. Miguel et al. [2012] designed a more elaborate acquisition system 
for cloth that explores a substantial range of the materials strain space, and records complete information 
about the forces applied to the cloth and the deformation that it undergoes. Like Wang et al. (and previous 
work), this setup focuses primarily on tensile forces. Tests are performed on 100 mm square cloth samples 
using two kinds of plastic clips: small, rounded clips that grab a localized area, and long clips that 
grip one whole side of the sample. Forces are applied to the clips by .ne wire cords that are pulled 
to de.ned displacements by eight linear actuators, and the tension in the cords is monitored by miniature 
load cells located at the actuator ends (see Fig. 21). Using the reconstruction methods described previously, 
the geometry and motion of the cloth is captured. The location and orientation of the cords attached 
to the clips (which reveal the direction of the applied force) are also reconstructed, by .tting 3D lines 
to reconstructed points along each cord (see Fig. 22). This system, although more complex to construct 
than that of Wang et al., is able to produce The 3D con.guration of the cloth sample, represented as 
a deformed mesh with 10K regularly sampled ver­tices.  The 3D positions and orientations of all clips 
attached to the cloth, including a list of clamped cloth vertices.  The 3D forces applied to all clips. 
The magnitudes are determined by the tension measurements, and the directions are determined by the observed 
directions of the cords.  Note that the actuator positions themselves are not part of the output, since 
they are superseded by the displacements measured at the clips. This prevents stretching of the cord, 
or other factors affecting the distance between the clip and the actuator, from affecting displacement 
accuracy. These are just a few examples of physical setups that have been used to generate controlled 
deformation and measure the forces in addition to the deforming geometry. 3.5 Deformation Examples We 
conclude this section with some hints on what deformation examples might make sense to capture, and point 
to data that is already available online. The types of interesting deformations depends on the object 
you are capturing, and of course on the application you have in mind. For cloth, it has been a popular 
idea to isolate speci.c stretching and bending deformations. Wang et al. [2011] follow the biaxial tensile 
method in the textile literature, which tests the cloth sample by stretching it simultaneously in both 
warp and weft directions. Using the setup described previously (recall Fig. 20), a number of stretching 
tests are performed. For cloth materials with symmetric properties to their warp and weft directions, 
they create three 400mm x 400mm cloth samples with bias angles 0., 45. and 90. respectively. The bias 
angle is de.ned as the rotational angle from the warp-weft coordinate system to the samples local coordinate 
system counterclockwise. Warp and weft directions can be recognized from thread directions in the weaving 
structure for most cloth materials. Each sample is typically tested by seven different weights at the 
bottom, going from 0g to 600g, and .ve weights on both sides, from 0g to 400g. In total, there are 35 
tests for each sample and 105 tests for each cloth material. This test set covers the range of forces 
typically experienced by the cloth in clothing when it is worn. Cloth bending is also measured, by clamping 
a sample in a bent position. A sample is incrementally advanced so that progressively more of it protrudes 
from the clamp, and the sample drapes into different curves under its own weight. These curves are captured 
from a side view and the trajectory of each curve is manually labeled using point features.  Motivated 
by the goal of parameter .tting, Miguel et al. [2012] designed deformation sequences of cloth that pro­duce 
near-isolated strains, and allow estimating stretch, shear and bending properties in a separate and incremental 
manner. Unlike standard textile evaluation practices, they relax the requirement of uniform strains. 
Using the setup described previously (recall Fig: 21), stretching is isolated by performing a uni-axial 
tension experiment, with forces applied to two long bar clips attached to either side of the cloth. The 
cloth is slowly stretched until a maximum force is reached and then slowly released back. The process 
is repeated three times, in both weft and warp directions separately. Shearing is captured using an approximate 
picture-frame experiment, where four long clips .x the cloth boundaries and shear stress is applied as 
the cords pull on opposite corners. Similar to Wang et al., to isolate bending deformation they slowly 
push the .at cloth sample off the edge of a table and measure its shape as it bends under its own weight, 
for both weft and warp directions. However, here Miguel et al. measure 3D bending rather than a 2D curve. 
This gives a total of .ve measurements per cloth sample that are used for parameter .tting (two stretch, 
one shear, and two bending). Additional, more complex, deformations are also captured for validating 
their algorithm. These experiments on cloth all aim to capture the full strain-space of the material. 
If instead you are working with faces, it is not as easy to isolate the different modes of deformation. 
However, a lot of work has focused on de.ning a representative set of facial expressions, from which 
many new expressions can be formed. A standard set of expressions has been de.ned in the Facial Action 
Coding System (FACS) of Ekman [1978]. Finally, since it can be a dif.cult task to capture real deformation 
examples for simulation, here is a short list of datasets that are already available online: Cloth datasets 
of Wang et al. [2011] are available at http://graphics.berkeley.edu/papers/Wang-DDE-2011-08/  Example 
garment capture sequences of Bradley et al. [2008b] are available at http://www.cs.ubc.ca/labs/imager/tr/2008/MarkerlessGarmentCapture/data.html 
  Facial performance capture data of Bradley et al. [2010] is available at http://www.cs.ubc.ca/labs/imager/tr/2010/Bradley 
SIG2010 FaceCapture/  An example facial performance from Beeler et al. [2011] can be requested from 
http://graphics.ethz.ch/publications/papers/paperBee11.php  Datasets of articulated mesh animations 
of people from the work of Vlasic et al. [2008] can be obtained from http://people.csail.mit.edu/drdaniel/mesh 
animation/index.html  Finally, similar datasets from the work of de Aguiar et al. [de Aguiar et al. 
2008] can be requested at http://www.mpi-inf.mpg.de/resources/perfcap/  4 Modeling Nonlinear Soft Tissue 
from Captured Mechanical Data   Data Acquisition 3 Canon 40D cameras External trigger for sync Contact 
probe PhigetInterfaceKit with force sensing resistors Data Acquisition 3 Canon 40D cameras External trigger 
for sync Contact probe PhigetInterfaceKit with force sensing resistors     12 deformation examples 
 3,240 elements Results -Comparison measured ours linear co-rotational  5 Data-Driven Modeling of Nonlinear 
Elasticity in Cloth Mti ti Motivation [Baraff et al.] [www.optitex.com] [www.fstructures.com] Cloth 
simulation is of great interest in computer graphics, the textile industry and mechanical engineeringthe 
textile industry, and mechanical engineering Mti ti Motivation [Particle System --Breen] [StVK Volino 
et al.] [Discrete Shells Grinspun et al.] St t f th t lth dl lState-of-the-art cloth modelsrelyon parameters, 
but parameters tuning is a tedious trial and error task a tedious trial-and-error task  ClClassiic Approaches 
 Ah FORCE-BASED Force vs displacement measurements  Force vs displacement measurements  MEASUREMENTS 
Isolate individual deformation modes  Kawabata 80 Uniform deformation Volino 09 DYNAMIC CAPTURED Reduced 
controlled conditions Bhat 03 VIDEOS Impossible to separate internal and  Kunitomo 10 external parameters 
ternalparters exame Stoll 10 Stoll 10 No force information is available AhApproach 1 [Mi 1[Miguell 
et all. 2012] t 2012] MEASUREMENT MODEL FITTING VALIDATION Capture system Fitting method Insight on 
evaluated Detailed 3D geometry Estimated parameters models Force measurements  M t E i tMeasurement: 
Experiments WARP-STRETCH WEFT-STRETCH SHEAR CORNER-PULL WARP-BEND WEFT-BEND COMPLEX SHEAR WARP BEND WEFT 
BEND COMPLEX SHEAR 5 experiments with near-isolated ti d f dt fitti 2 experiments with complex ti d f 
lid tistrain, used for data fitting strain, used for validation Md l Fitti Cl th Md lModel Fitting: Cloth 
Models LINEARLINEAR NON-LINEAR Energy is defined as the stress-strain product. The left block derives 
the force for each deformation mode for a linear,,pseparable model. On the right, the model is extended 
to account for nonlinear elasticity, by interpolating stiffness from control point values. Md lFitti 
Cl thMd Model Fitting: Cloth Modells   MEMBRANE BENDING Spring Springs Springs  Spring  Volino 
09 Grinspun 03  Baraff &#38; Witkina&#38;Witkin 9898 Bridson 03  Barff Bergou 06 Garg 07   Many 
popular models can be written in this fashion and some of Many popularmodels canbe written inthis fashion, 
andsome of them are picked to define three example membrane-bending models: Spring Soft Constraints St. 
VK Spring Membrane Baraff &#38; Witkin BarDiagonalized St. VK aff&#38;Witkin DiagonalizedSt.VK Spring 
Bending Discrete Shells Bending Discrete Shells Bending MdlFitti Oti i ti L Model Fitting: Optimization 
Loop Inpput frames Simulated frames Position Boundary Obj. Conditions Quasi-static Function solver 
Model Parameters  Optimization Md lFitti It lFitti Model Fitting: IncrementalFitting WARP-BEND WARP 
STRETCH WARP-STRETCH DIAG-BEND SHEAR WEFT-STRETCH WEFT-BEND MdlFitti Ct lP itIti Model Fitting: ControlPointInsertion 
 KKKK Valid strain range V lid ti St t h Soft Constraints St.VK Spring Validation: Stretch Cotton satinCotton 
satin Rayon/spandex knit Cotton denim Wool//cotton blend V lid ti ShValidation: Shear SHEAR hi i h l 
h ff iThistime theplot shows aggregate torque vs. effective shear angle V lid ti Sh SpringSt.VK Soft 
Constraints Validation: Shear Cotton satinCotton satin Rayon/spandex knit Cotton denim Wool//cotton blend 
V lid ti ShValidation: Shear WARP-BENDING h l d d i l d lh filTheplot compares measured and simulated 
cloth profiles V lid ti Bd Soft Constraints St.VK Validation: Bend Cotton satinCotton satin Rayon/spandex 
knit Cotton denim Wool//cotton blend Spring V lid ti C Pll Validation: Corner-Pull CORNER-PULL Validation 
under experiments not used for fitting   . .. . . . eu ev ees . . .. . .. .. . ... ......eu evs EltilMdl:ascoeonnearoeEltiMdlNliMdl 
 Nonlinear orthotropic model .)CC,,,,. .. . ... )e) )) ......es .. . .. . ... . eu)CC,,,,uvvv(ee((..... 
.. . ..) )) (CC,,ssMdNli e es es )) )) eveveueuee.e. ...((((((. . ...es )es )eveveeeueu(((((( uuuvevEltiMdl2DPiti 
t:ascoearameerzaonEltiMdl2DPtiti.. . ...= .......................=es evv =........ .. . . . ss su sv 
seues su sv sss .. . ... . . .. . . . .. . . . Simplified model C,.) C,. uumaxuvmax CC( ,.)) CC(,. uvmaxvvmax 
.s CC,. .e ssmax in which in which cos. siin...max cos.-siin. -sin. cos. mmiinn sin. cos. .. ElElastitic 
Modell: BendingMd Bdi Stiffness K d() Edge orientation (anisotropic)  Bendingggangle ((nonlinear)) 
 DtC RtCditi Data Captture: Rest Conditions fixed Cloth Sample 40cm×40cm (40cm×40cm)) ( DtData CCaptture: 
DfDeformati tion 40cm×40cm DtC A Data Captture: Accuracy Average error: 2 58mm 2.58mm DtC Bdi Data 
Captture: Bending simulation features features Fitti Fitting &#38;&#38;Rlt Results: Optimiization Oti 
ti capture device simulator Simulated Shapes Simulated Frames argmin - Simulated Frames stiffness parameters 
 6 Animation of Faces with Data-Driven Wrinkles     Fine-Scale Deformation Problem: Exponential 
growth of example poses Example posesPSD WPSD Fine-Scale Deformation Weighted Pose-Space Deformation 
 Fine-Scale Deformation Fitting RBFs Large training set T Compact basis P Least-squares fit (solve P.P 
system per vertex) Fine-Scale Deformation   Results Results  7 Clothing Animation with Wrinkle Synthesis 
from Examples   Database Construction 14 1412612614 14126126 #Samples 45 # Samples Shirt S t J i t 
( ) 90 90 Separate Joint (our) 14+126+126+14+45=325 90+90+11+11=20290+90+11+11=202 Combined Joint (bad) 
11 11 l Combined Joint (bad) 14×126×126×14×45=140M 90×90×11×11=1M # Samples PtPants Database Construction 
bd h Create a body mesh sequence. Simulate cloth with body motion,y , using a large damping force. 
Pick some frames as joint samplesPick some frames as joint samples. Assign each sample with the corresponding 
cloth mesh. (48 to 72 hours in total )(48 to 72 hours in total.)     Summaryy Focusedd on common 
cllothhes, hhas wide applilications id i Games  i f ffli hili l  Fast preview for offline physical 
simulatiion CCombi bines phhysiics wiithh thhe ddatabbase Greatly reduces the computational cost  P 
id li i  Provides realistic resullts. Reduces a whole pose space to local pose spaces 8 Outlook This 
course focuses on three applications of data-driven methods in computer graphics: cloth, tissue and face 
anima­tion. Data-driven methods have also been successfully applied to character motion, and we expect 
that data-driven approaches will become popular in other areas of simulation in computer graphics too. 
To maximize their applica­bility, it will be crucial to understand which processes and properties can 
be modeled accurately with data-driven approaches. Effort should be devoted to developing general algorithmic 
and methodological procedures, together with a clear understanding of their limitations. One of the major 
dif.culties in data-driven methods is to .nd suitable descriptions of the simulated processes. Those 
descriptions have an impact on the smoothness and fairness of the output functions, which in turn affect 
the robustness and accuracy of interpolation and optimization methods. Multi-scale process decompositions 
appear particularly interesting in situations where .ne-scale effects dominate the computational cost 
and the modeling complexity. To conclude, the application of data-driven simulation methods in the computer 
graphics industry will depend largely on the access to accurate data. To this end, progress must be made 
along two paths. One is the creation of data libraries, both geometric and mechanical, that can be used 
by a large set of developers. The other is a combined progress of algorithms and capture systems, to 
enable fast and cheap synthesis of data-driven models. References ALEXA, M., GROSS, M., PAULY, M., PFISTER, 
H., STAMMINGER, M., AND ZWICKER, M. 2004. Point-based computer graphics. ACM SIGGRAPH 2004 Course Notes. 
ATCHESON, B., IHRKE, I., HEIDRICH, W., TEVS, A., BRADLEY, D., MAGNOR, M., AND SEIDEL, H.-P. 2008. Time-resolved 
3d capture of non-stationary gas .ows. ACM Transactions on Graphics (Proc. SIGGRAPH Asia) 27, 5, 132. 
BAKER, S., SCHARSTEIN, D., LEWIS, J., ROTH, S., BLACK, M., AND SZELISKI, R. 2011. A database and evaluation 
methodology for optical .ow. International Journal of Computer Vision 92, 1, 1 31. BECKER, M., AND TESCHNER, 
M. 2007. Robust and ef.cient estimation of elasticity parameters using the linear .nite element method. 
In SimVis, 15 28. BEELER, T., BICKEL, B., SUMNER, R., BEARDSLEY, P., AND GROSS, M. 2010. High-quality 
single-shot capture of facial geometry. ACM Trans. Graph. (Proc. SIGGRAPH). BEELER, T., HAHN, F., BRADLEY, 
D., BICKEL, B., BEARDSLEY, P., GOTSMAN, C., SUMNER, R. W., AND GROSS, M. 2011. High-quality passive facial 
performance capture using anchor frames. ACM Trans. Graph. 30, 75:1 75:10. BHAT, K. S., TWIGG, C. D., 
HODGINS, J. K., KHOSLA, P. K., POPOVI C´, Z., AND SEITZ, S. M. 2003. Estimating cloth simulation parameters 
from video. In Proc. ACM SIGGRAPH/Eurographics SCA, 37 51. BICKEL, B., LANG, M., BOTSCH, M., OTADUY, 
M. A., AND GROSS, M. 2008. Pose-space animation and transfer of facial details. In Proc. of the ACM SIGGRAPH 
/ Eurographics Symposium on Computer Animation, 57 66. BICKEL, B., B ¨ ACHER, M., OTADUY, M. A., MATUSIK, 
W., PFISTER, H., AND GROSS, M. 2009. Capture and modeling of non-linear heterogeneous soft tissue. ACM 
Trans. Graph. 28, 3 (July), 89:1 89:9. BICKEL, B., B ¨ ACHER, M., OTADUY, M. A., LEE, H. R., PFISTER, 
H., GROSS, M., AND MATUSIK, W. 2010. Design and fabrication of materials with desired deformation behavior. 
ACM Transactions on Graphics 29,4 (July), 63:1 63:10. BOUBEKEUR, T., REUTER, P., AND SCHLICK, C. 2005. 
Visualization of point-based surfaces with locally recon­structed subdivision surfaces. In Shape Modeling 
International. BOUBEKEUR, T., HEIDRICH, W., GRANIER, X., AND SCHLICK, C. 2006. Volume-surface trees. 
Computer Graphics Forum (Proceedings of EUROGRAPHICS 2006) 25, 3, 399 406. BRADLEY, D., AND HEIDRICH, 
W. 2010. Binocular camera calibration using recti.cation error. IEEE Conference on Computer and Robot 
Vision (CRV). BRADLEY, D., BOUBEKEUR, T., AND HEIDRICH, W. 2008. Accurate multi-view reconstruction using 
robust binocular stereo and surface meshing. In Proc. CVPR. BRADLEY, D., POPA, T., SHEFFER, A., HEIDRICH, 
W., AND BOUBEKEUR, T. 2008. Markerless garment capture. ACM Trans. Graph. (Proc. of SIGGRAPH) 27, 3, 
99:1 99:9. BRADLEY, D., ATCHESON, B., IHRKE, I., AND HEIDRICH, W. 2009. Synchronization and rolling shutter 
compen­sation for consumer video camera arrays. In International Workshop on Projector-Camera Systems 
(PROCAMS 2009). BRADLEY, D., HEIDRICH, W., POPA, T., AND SHEFFER, A. 2010. High resolution passive facial 
performance capture. ACM Trans. Graph. (Proc. SIGGRAPH) 29, 4, 41:1 41:10. BREEN, D., HOUSE, D., AND 
WOZNY, M. 1994. Predicting the drape of woven cloth using interacting particles. In Proc. of ACM SIGGRAPH, 
365 372. DE AGUIAR, E., STOLL, C., THEOBALT, C., AHMED, N., SEIDEL, H.-P., AND THRUN, S. 2008. Performance 
capture from sparse multi-view video. ACM Trans. Graphics (Proc. SIGGRAPH), 98. DE AGUIAR, E., SIGAL, 
L., TREUILLE, A., AND HODGINS, J. K. 2010. Stable spaces for real-time clothing. ACM Transactions on 
Graphics 29, 4 (July), 106:1 106:9. EBERHARDT, B., WEBER, A., AND STRASSER, W. 1996. A fast, .exible, 
particle-system model for cloth draping. IEEE Computer Graphics and Applications 16, 5, 52 59. EKMAN, 
P., AND FRIESEN, W. 1978. The facial action coding system: A technique for the measurement of facial 
movement. In Consulting Psychologists. FENG, W.-W., YU, Y., AND KIM, B.-U. 2010. A deformation transformer 
for real-time cloth animation. ACM Transactions on Graphics 29, 4 (July), 108:1 108:9. FURUKAWA, Y., 
AND PONCE, J. 2008. Dense 3d motion capture from synchronized video streams. In Proc. CVPR. FURUKAWA, 
Y., AND PONCE, J. 2009. Dense 3d motion capture for human faces. In CVPR. FURUKAWA, Y., AND PONCE, J. 
2010. Accurate, dense, and robust multi-view stereopsis. IEEE Trans. on Pattern Analysis and Machine 
Intelligence 32, 8, 1362 1376. FUSIELLO, A., TRUCCO, E., AND VERRI, A. 2000. A compact algorithm for 
recti.cation of stereo pairs. Mach. Vision Appl. 12, 1, 16 22. GOPI, M., KRISHNAN, S., AND SILVA, C. 
2000. Surface reconstruction based on lower dimensional localized delaunay triangulation. In Eurographics. 
GROSS, M., AND PFISTER, H., Eds. 2007. Point-Based Graphics. Morgan Kaufmann Publishers. GUENTER, B., 
GRIMM, C., WOOD, D., MALVAR, H., AND PIGHIN, F. 1998. Making faces. In SIGGRAPH 98: Proceedings of the 
25th annual conference on Computer graphics and interactive techniques, 55 66. KAJBERG, J., AND LINDKVIST, 
G. 2004. Characterisation of materials subjected to large strains by inverse modelling based on in-plane 
displacement .elds. International Journal of Solids and Structures 41, 13, 3439 3459. KAUER, M., VUSKOVIC, 
V., DUAL, J., SZEKELY, G., AND BAJKA, M. 2002. Inverse .nite element characteriza­tion of soft tissues. 
Medical Image Analysis 6, 3, 257 287. KAVAN, L., GERSZEWSKI, D., BARGTEIL, A. W., AND SLOAN, P.-P. 2011. 
Physics-inspired upsampling for cloth simulation in games. Proc. of ACM SIGGRAPH. KAZHDAN, M., BOLITHO, 
M., , AND HOPPE, H. 2006. Poisson surface reconstruction. In Symposium on Geometry Processing. KUNITOMO, 
S., NAKAMURA, S., AND MORISHIMA, S. 2010. Optimization of cloth simulation parameters by considering 
static and dynamic features. In ACM SIGGRAPH Posters, 15:1. LANG, J., PAI, D. K., AND WOODHAM, R. J. 
2002. Acquisition of elastic models for interactive simulation. International Journal of Robotics Research 
21, 8, 713 733. LIN, I.-C., AND OUHYOUNG, M. 2005. Mirror mocap: Automatic and ef.cient capture of dense 
3d facial motion parameters from video. The Visual Computer 21, 6, 355 372. MA, W.-C., JONES, A., CHIANG, 
J.-Y., HAWKINS, T., FREDERIKSEN, S., PEERS, P., VUKOVIC, M., OUHY-OUNG, M., AND DEBEVEC, P. 2008. Facial 
performance synthesis using deformation-driven polynomial dis­placement maps. ACM Trans. Graphics (Proc. 
SIGGRAPH Asia) 27, 5, 121. MA, W.-C., JONES, A., CHIANG, J.-Y., HAWKINS, T., FREDERIKSEN, S., PEERS, 
P., VUKOVIC, M., OUHY-OUNG, M., AND DEBEVEC, P. 2008. Facial performance synthesis using deformation-driven 
polynomial dis­placement maps. ACM Trans. Graph. (Proc. of ACM SIGGRAPH Asia) 27, 5. MIGUEL, E., BRADLEY, 
D., THOMASZEWSKI, B., BICKEL, B., MATUSIK, W., OTADUY, M. A., AND MARSCHNER, S. 2012. Data-driven estimation 
of cloth simulation models. Computer Graphics Forum (Proc. of Eurographics) 31, 2. MVIEW. http://vision.middlebury.edu/mview/. 
Open source computer vision library. PAI, D. K., VAN DEN DOEL, K., JAMES, D. L., LANG, J., LLOYD, J. 
E., RICHMOND, J. L., AND YAU, S. H. 2001. Scanning physical interaction behavior of 3d objects. In Proceedings 
of ACM SIGGRAPH 2001, Computer Graphics Proceedings, Annual Conference Series, 87 96. SCHNUR, D. S., 
AND ZABARAS, N. 1992. An inverse method for determining elastic material properties and a material interface. 
International Journal for Numerical Methods in Engineering 33, 10, 2039 2057. SCHOLZ, V., STICH, T., 
KECKEISEN, M., WACKER, M., AND MAGNOR, M. 2005. Garment motion capture using color-coded patterns. In 
Proc. Eurographics, 439 448. SCHONER, J. L., LANG, J., AND SEIDEL, H.-P. 2004. Measurement-based interactive 
simulation of viscoelastic solids. Computer Graphics Forum (Proc. Eurographics) 23, 3, 547 556. SEITZ, 
S. M., CURLESS, B., DIEBEL, J., SCHARSTEIN, D., AND SZELISKI, R. 2006. A comparison and evaluation of 
multi-view stereo reconstruction algorithms. In CVPR. SORKINE, O., COHEN-OR, D., LIPMAN, Y., ALEXA, M., 
R OSSL¨ , C., AND SEIDEL, H.-P. 2004. Laplacian surface editing. In Proceedings of the 2004 Eurographics/ACM 
SIGGRAPH symposium on Geometry processing, 175 184. VLASIC, D., BARAN, I., MATUSIK, W., AND POPOVI C´, 
J. 2008. Articulated mesh animation from multi-view silhouettes. ACM Trans. Graphics (Proc. SIGGRAPH), 
97. VOLINO, P., MAGNENAT-THALMANN, N., AND FAURE, F. 2009. A simple approach to nonlinear tensile stiffness 
for accurate cloth simulation. ACM Trans. Graph. 28, 4. WAND, M., MITRA, N., PAULY, M., CHANG, W., AND 
LI, H. 2012. Dynamic geometry processing. In Euro­graphics Tutorials. WANG, H., HECHT, F., RAMAMOORTHI, 
R., AND O BRIEN, J. 2010. Example-based wrinkle synthesis for clothing animation. ACM Transactions on 
Graphics 29, 4 (July), 107:1 107:8. WANG, H., RAMAMOORTHI, R., AND O BRIEN, J. 2011. Data-driven elastic 
models for cloth: Modeling and measurement. ACM Trans. Graph. (Proc. SIGGRAPH) 30, 4, 71. WHITE, R., 
CRANE, K., AND FORSYTH, D. 2007. Capturing and animating occluded cloth. ACM Trans. Graphics (Proc. SIGGRAPH), 
34. WILBURN, B., JOSHI, N., VAISH, V., LEVOY, M., AND HOROWITZ, M. 2004. High-speed videography using 
a dense camera array. In Proc. of CVPR, vol. 2, 294 301. WILLIAMS, L. 1990. Performance-driven facial 
animation. SIGGRAPH Comput. Graph. 24, 4, 235 242. ZHANG, Z. 1999. Flexible camera calibration by viewing 
a plane from unknown orientations. In ICCV, 666 673. ZURDO, J. S., BRITO, J. P., AND OTADUY, M. A. 2012. 
Animating wrinkles by example on non-skinned cloth. IEEE Trans. on Visualization and Computer Graphics. 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343496</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>20</pages>
		<display_no>13</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Principles of animation physics]]></title>
		<page_from>1</page_from>
		<page_to>20</page_to>
		<doi_number>10.1145/2343483.2343496</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343496</url>
		<abstract>
			<par><![CDATA[<p>An understanding of physics is essential for creating believable animated motion. Many basic concepts in animation (such as follow-through, drag, and weight shift) are clearer when considered in the context of basic mechanics.</p> <p>This course was developed with support from the National Science Foundation by Alejandro Garcia, who was on leave at the Department of Artistic Development at Dreamworks Animation SKG, where he presented over 30 classes and special lectures on physics as it applies to animation. The course covers essential topics from physical mechanics to basic bio-mechanics that apply specifically to character animation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738929</person_id>
				<author_profile_id><![CDATA[81100379394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Garcia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[San Jose State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Principles of Animation Physics Alejandro L. Garcia Dept. of Physics &#38; Astronomy San Jose State 
University SIGGRAPH 2012 Course Notes Introduction Why should a character animator care about physics? 
If you re going to be a surgeon then you need to know advanced anatomy; for life drawing a basic knowledge 
of muscles and bones is helpful. If you re going to be an engineer then you need to know calculus and 
physics; as a character animator a basic understanding of mechanics and bio-mechanics is helpful. In 
Chuck Amuck, Chuck Jones writes, Comparative anatomy is a vital tool of the complete animator or director. 
The purpose of this course is to make physics another tool in your animator s toolbox. At San Jose State 
I teach Physics of Animation, a one semester course for animation artists. The .rst eight weeks of the 
semester cover mechanics and bio-mechanics; this SIGGRAPH course is a distillation of those 16 lectures 
down to a three hours. The focus of today s course is character animation, though many of the concepts 
are also useful for character e.ects, such as hair and clothing. Physics is also useful for e.ects animators 
and for lighting artists but we won t have time to get to those areas today. All animators are familiar 
with the Principles of Animation as described by Ollie Johnston and Frank Thomas in their book The Illusion 
of Life: Disney Animation. These principles, such as Squash and stretch and Anticipation , give helpful 
guidance to animators. And since they describe motion it s not surprising that many of the principles 
of animation are based on physical laws. For example, the animation principle of follow-through is based 
on the Law of Inertia. We ll follow the successful model introduced by Johnston and Thomas and make a 
list of principles to organize our understanding of physical motion. Speci.cally, our Principles of Animation 
Physics will be: = Figure 1: Examples of the Odd Rule for slowing out (left) and slowing in (right). 
 Figure 2: (Left) An example of Fourth Down at Half Time; the timing is three frames per drawing. (Right) 
Using Fourth Down at Half Time in the graph editor. For acceleration under gravity the distance fallen 
from the apex equals (1 inch) times the 3 square of the number of frames. For example, for 6 frames the 
distance fallen is 1 × (6)2 = 12 3 inches or one foot; for 12 frames the distance fallen is 1 × (12)2 
= 48 inches or four feet. 3 These numerical calculations are rarely useful to character animators but 
I m mentioning them because of the importance of timing in creating scale. Suppose that a character jumps 
o. the roof of a house and lands on the ground in 12 frames. The character will feel small because this 
timing indicates that the house is four feet tall. This applies not just to falling motion but also walking, 
running, jumping, etc. since timing of the arms and legs has a similar scaling. This connection between 
timing and scale is used in live-action movies when scale models are .lmed with high-speed cameras; by 
.lming at twice the normal speed the models look four times bigger when the action is played back at 
normal speed. It s important to remember that in order to stay on model the timing of a character s action 
must be consistent with the character s size. Just a 20% error in the timing will make a 6 foot character 
seem like it s only about 4 feet tall. Law of Inertia The Law of Inertia, also known as Newton s First 
Law of Motion, says that a character will move with constant velocity unless acted on by an unbalanced 
force. First, let s be clear what s * This mnemonic makes more sense if you know American football. This 
is for 24 frames per second, as used in .lm; note that video typically uses 30 fps. Figure 3: (Left) 
Character demonstrating the Law of Inertia by standing on a bus when it comes to a sudden stop. (Right) 
This character is seated when the bus stops; notice how her hair .ies forward due to the Law of Inertia. 
At .rst this all seems purely academic until you realize that follow-through in animated motion is entirely 
due to the Law of Inertia. Let s take a simple example of a character standing on a bus (see Figure 3). 
When the bus suddenly stops the character goes .ying forward. Before the bus hit the brakes he was moving 
and by the Law of Inertia, he ll continue moving until a force acts to stop him (such as when he hits 
the .oor). Another example is seen in the follow-through (forward motion) of a seated passenger s hair 
(see Figure 3 again) when the bus hits the brakes. A corollary of the Law of Inertia is that a character 
at rest will remain at rest until acted on by an unbalanced force. If the bus suddenly accelerates forward 
then our standing character falls on his back (see Figure 4) and the seated passenger s hair seems to 
drag behind. As fellow passengers it seems to us as if there s a force pulling everything backwards but 
that s because we re moving with the bus. A stationary observer standing outside the bus would realize 
that the poor chap that s falling has the bus moving out from under him (see Figure 5). This reminds 
us that you have to be careful to include the e.ect of the camera s motion, especially when the camera 
is accelerating. Finally, the Law of Inertia also explains the drag and outward centrifugal force that 
s of Inertia.  Figure 5: When the bus suddenly accelerates the character appears to fall backwards, 
as seen from inside the bus (left), but an observer outside the bus sees the bus moving out from under 
his feet (right). illustrated in Figure 6. As the characters turn their bodies their hair and clothing 
drag behind due to the Law of Inertia (an object at rest will remain at rest until acted on by an unbalanced 
force). Once the hair and clothing start moving they ll continue moving, causing them to .y outward due 
to inertia (or, if you prefer, due to follow-through). In the same fashion, if the bus makes a sudden 
right turn then the character standing on the bus will continue moving in his original direction, causing 
him to fall towards the left side of the bus. Momentum and Force A moving character has momentum, which 
depends on the character s speed and on its weight. A character that weighs 100 pounds can have as much 
momentum as a 300 pound character if Figure 7: (Left) Momentum changes quickly on impact so the force 
of impact is large. (Right) Momentum changes more slowly so the force is softer, the impact is less jarring. 
 Figure 7 shows two examples of a ball hitting the ground. The momentum change could be about the same 
in the two cases but in the realistic impact that change occurs quickly, implying a large force at impact. 
Using squash and stretch the cartoony version softens the impact by Figure 8: Examples where the unbalanced 
forces are not constant. Although the force of gravity is constant, the forces exerted by the surface 
of the curved ramp (left) and the tension in the pendulum (right) depend on the position of the ball. 
 Finally, if the unbalanced force goes to zero then the velocity becomes constant (see the Law of Inertia). 
An example of this is when a falling character reaches terminal velocity because the force of gravity 
is balanced by the force of air resistance. In brief, the texture of the timing depends on the unbalanced 
forces and how those forces change during the action. The animated motion is believable only if the texture 
is consistent with the forces. Center of Gravity The center of gravity is the average location of an 
object s weight. This is the geometric (visual) center for most objects but for non-uniform objects (e.g., 
hammer with a wooden handle and an For humans this maximum speed is over 100 miles per hour but for small 
creatures, such as squirrels, it s quite slow so they survive falls from any height. Figure 9: (Left) 
Location of a character s center of gravity in various poses. (Right) Shaded area is the character s 
base of support. The center of gravity is important for various reasons. First, a character is in a balanced 
stationary pose only if the center of gravity is positioned over the character s feet. More precisely, 
the center of gravity has to be over the base of support , which is the area around the feet (see Figure 
9 again). Second, when a character changes pose the center of gravity shifts, causing a weight shift 
from one leg to another. Simply shifting the center of gravity by a few inches is enough to cause signi.cantly 
weight shift, as shown in Figure 10. Weight shifts from foot to foot are re.ected in the pose, typically 
raising the hip and lowering the shoulder on the weight-bearing side (an e.ect known as contrapposto 
). When animated well it should be clear whether a character is standing or sitting even if the shot 
only shows the character s upper body. Figure 10: (Left) Even weight on each foot when the center of 
gravity is centered above them. (Center and Right) Weight shift due to shift of center of gravity towards 
screen left. Finally, when we consider the path of action of a moving character the point that we re 
 most interested in tracking is the center of gravity. For example, when .ying through the air it is 
the center of gravity that follows a parabolic arc, independent of any rotation in the body or twisting 
of the limbs (see Figure 11). The motion of the center of gravity is also important in walks, as we ll 
discuss later. Figure 11: Path of action of the center of gravity of a spinning hammer .ying through 
the air. Weight Gain and Loss We think of a character s weight as being constant but when the character 
accelerates its weight e.ectively varies. For example, as illustrated in Figure 12, if the character 
is: Moving upward and gaining speed: Gain weight  Moving upward and losing speed: Lose weight  Moving 
downward and gaining speed: Lose weight  Moving downward and losing speed: Gain weight  In brief, if 
you re going against gravity (rising but speeding up or falling but slowing down) then you gain weight 
but if you re going with gravity (rising &#38; slowing down or falling &#38; speeding up) then you lose 
weight. An extreme case is when a character is in free-fall and becomes weightless. You can feel these 
variations of weight by taking a heavy object in your hand, say a dumbbell, and accelerating it up and 
down.§ As a character moves (walks, runs, jumps, etc.) the changes in weight create many overlap­ping 
actions in the movement of hair, clothing, and .esh. Gaining weight pulls these downward while losing 
weight causes them to almost .oat. Richard Williams calls this e.ect counterac­tion and in The Animator 
s Survival Kit he writes, When the character (accelerates) up the drapery or hair or soft bits go down. 
Poorly animated characters sometimes look .oaty as they walk because their actions lack this variation 
in weight. §If your physics friends question this weight gain and loss just tell them that you re working 
in a non-inertial frame of reference. Action-Reaction The principle of action-reaction, also known as 
Newton s Third Law, tells us that when a character interacts with an object (or with another character) 
both are a.ected. Paul Hewitt poetically describes this as, You cannot touch without being touched. A 
more traditional statement of the principle would be, For every action force there s an instantaneous 
reaction force that s equal in magnitude and opposite in direction. Let s consider each part of this 
statement. First, the action and the reaction are a pair of matched forces. For example, suppose Mr. 
Alpha punches Mr. Beta (see Figure 13). The action is the force of Mr. Alpha s .st hitting Mr. Beta s 
face so the reaction is the force of Mr. Beta s face pushing back on Mr. Alpha s .st.¶ Second, these 
two forces are instantaneous so if Mr. Beta punches back that s not the reaction, that s a new action. 
Furthermore, if Mr. Alpha punches with 100 pounds of force towards the left then there s an equal force 
of 100 pounds on his .st towards the right. Since forces change momentum the action force causes the 
Mr. Beta s head to start moving towards the left while the reaction force changes the momentum of Mr. 
Alpha s .st, possibly bringing it to a stop. To animate this scene successfully it s essential to match 
the action and reaction. A common mistake is to focus on animating Mr. Beta s motion while neglecting 
the reaction that must simultaneously be occurring on Mr. Alpha. ¶It doesn t matter which force is labeled 
action and which one is reaction ; you can always switch the names since they re symmetric. Judging 
the e.ect of action-reaction is complicated by the fact that there s usually several action forces to 
be considered. Take the simple case of a man pushing a rock (see Figure 13). The man exerts a force on 
the rock so the rock exerts a force back on him. If he was on roller skates he d move backwards and the 
rock would move forward. But he s barefoot so there s another set of action-reaction forces: he also 
pushes back on the ground and the ground pushes him forward. In order for him to push the rock forward 
the force exerted by his legs cannot be less than the force exerted by his arms. These actions and reactions 
have to appear to match in order to animate this scene believably. Finally, the action-reaction principle 
is often violated in .lms, both animated and live-action. Heros deliver powerful punches (or shoot big 
guns) with negligible recoil while the villain goes .ying backwards. This may be done intentionally for 
dramatic e.ect (to make the hero look powerful) or for comic e.ect (to defuse the violence) but if it 
s unintentional and unexpected then there s a chance of shattering the audience s suspension of disbelief. 
 Applying the Principles Basic Poses In this second half we ll apply the principles of animation physics 
to a variety of examples, starting with basic poses. The principle of the Center of Gravity tells us 
that in a stationary pose the character s center of gravity is located over its base of support (area 
around the feet; see Figure 9). To check this it s helpful to draw the line of gravity , the vertical 
line passing through the center of gravity, and the center of pressure , which is where the line of gravity 
hits the .oor (see Figure 14). If the center of pressure falls inside the base of support then the force 
of gravity is balanced by the upward force of the .oor. Let s see how this applies in some examples: 
A character has to lean when carrying a heavy The character picking up her sunglasses in Figure 16 has 
to move her hips backwards as she leans forward in order to maintain her center of gravity over her feet. 
In this same .gure the character rising from a chair leans forward to position his center of gravity 
over his feet; he also shifts his feet to broaden his base of support. Figure 16 also shows me reaching 
as far as In exceptional cases the line of gravity is not vertical. In Figure 17, as the train accelerates 
the line of gravity tilts forward so the woman leans forward to stay in balance. In the same .gure the 
distracted bus passenger is out of balance when the bus accelerates; the tilting of the line of gravity 
moves the center of pressure out from under his base of support. Finally, when a moving character turns 
a corner there s a change of momentum due to a centripetal force (see the principle of Momentum and Force). 
The resulting acceleration tilts the line of gravity towards the inside of the curve. For this reason 
a runner leans into a turn, also shown in Figure 17. Figure 17: (Left) Maintaining balance on an accelerating 
train. (Center) Losing balance on an accelerating bus. (Right) Runner leaning into a turn. Jumps We 
ll start with the easiest part of a jump, which is when the character is in the air. For human-sized 
characters the force of air resistance is small when traveling below 30 miles per hour so typically the 
only signi.cant force during a jump is gravity. In that case the path of action is a parabolic arc, as 
shown in Figure 18. The horizontal spacings are uniform and the vertical spacings are given by the Odd 
Rule and Fourth Down at Half Time (see the principle of Timing, Spacing, and Scale). In the rare cases 
where air resistance is signi.cant the path of action is skewed, with the descent being at a steeper 
angle than the ascent. Figure 18: Parabolic arc has uniform horizontal spacings and vertical spacings 
that follow the Odd Rule and Fourth Down at Half Time. The timing of the character while in the air is 
also simple since gravitational acceleration is the same for all objects. For example, everything falls 
one foot in six frames so jumping one foot in height takes six frames from take take-o. to apex and another 
six frames from apex to landing. In Figure 19 this corresponds to a jump height of one foot and a jump 
time of six frames. Timing and scale are connected so a four foot jump takes twice as long. Notice that 
the timing only depends on the (vertical) height of the jump, not on the (horizontal) distance jumped. 
Now let s consider the start of a jump, when the character is rising out of the crouch, pushing with 
its legs in order to get into the air. We ll call the distance over which the character pushes o. the 
push height and the distance from take-o. to apex the jump height (see Figure 19). The ratio of these 
heights is the jump magni.cation , Jump height, from take-o. to apex Jump Magni.cation = Push height, 
from crouch to take-o. For example if the character rises from a one foot crouch and jumps four feet 
into the air then the jump magni.cation is 4. The reason that the jump magni.cation is important is that 
it determines the timing of the push. The larger the jump magni.cation, the quicker the push needs to 
be in order for the Figure 19: Push time, = ni.cation is 4). With great force comes great momentum change 
(and vice versa). The force that a char­acter exerts during the push equals the jump magni.cation times 
the character s weight. The character exerts this action force down on the .oor and so it s the reaction 
force that actually As with jumping, the character exerts action forces on the ground and relies on 
the resulting reaction forces to walk. From the contact pose to the passing position the force on the 
character is in the backward direction, slowing the motion. The necessity of this backward force is demonstrated 
by having the character step on a banana peel. From the passing position to the To keep the center of 
gravity from rising very much during the passing position the pelvis tilts to the side of the moving 
leg; this is called the pelvic list (see Figure 23). The knee on the weight-bearing leg bends slightly 
for the same reason while the knee of the passing leg bends to keep the foot from dragging the ground. 
Besides the up-and-down motion of the center of gravity there is also some side-to-side motion. Walking 
is most e.cient when this motion is minimized since forces have to be exerted by the legs to maintain 
it. Nevertheless there is always some side-to-side motion and it is synchronized with the up-and-down 
motion. For slow walks the center of gravity traces out an o-shaped pattern, as seen when the character 
is walking away from the camera. For fast walks the synchronization changes and the center of gravity 
swings almost like a pendulum, tracing a U-shaped pattern. Figure 24 shows the weight shift from one 
foot to the other while walking. The two curves .To fully explain this rotation we need more physics 
principles, as discussed in the Concluding Remarks section. of gravity. pose. Figure 24: Laboratory measurements 
of the weight on each foot while walking. The dashed line is the stationary weight (180 pounds). The 
data in Figure 24 is for an average walk (about one second per cycle). For slower walks the weight variations 
are smaller; for very slow walks the weight on the planted foot is Second, the principles of animation 
physics are useful at all stages, including story, layout, and character design. In the early stages 
of production the director works with a variety of artists on the visual design (vis-dev). Besides the 
general look of the .lm there are many decisions regarding the universe in which the story takes place. 
For example, animals talk in Kung Fu Panda but they don t in How to Train Your Dragon. Similarly, there 
is also the physical design (phys-dev) of the .lm s universe. Take, for example, the character of Mantis 
in Kung Fu Panda. In order for Mantis to .ght villains who are a hundred times his size the principle 
of action-reaction is suspended. This decision leads to other phys-dev questions: Does action-reaction 
apply to the other heros? What about the villains or ordinary citizens? Are there scenes where Mantis 
will obey action-reaction? One can also think of a physics script , similar to a color script, that indicates 
the variation in the physics in each scene. For comedic action the laws of physics are often bent but 
for dramatic action a heightened realism creates more tension. Many of these phys-dev decisions are made 
instinctively by the director and when the design is successful the .lm s universe feels natural, immersing 
the audience in the story. So use the principles wisely and may the forces be with you. Acknowledgements 
The Physics for Animation Artists project has been supported by the National Science Founda­tion s Transforming 
Undergraduate Education in Science, Technology, Engineering and Math­ematics program. The author also 
thanks his colleagues at San Jose State University and Dreamworks Animation SKG. Biosketch Professor 
Alejandro Garcia has taught Physics of Animation at San Jose State since 2009. In 2011 he spent a one-year 
professional leave as physicist-in-residence at Dreamworks Animation SKG. During his time with Dreamworks 
he worked in the Division of Artistic Development, presenting over 30 classes and special lectures for 
various studio departments and consulted on Madagascar 3: Europe s Most Wanted. This work was highlighted 
on Science Nation; see: http://tinyurl.com/8ynw2tt. Dr. Garcia is also the author of the textbook, Numerical 
Meth­ods for Physics, and has published over 80 professional articles in the .elds of computational physics, 
statistical mechanics, and .uid mechanics. Image credits Illustrations by Katie Corna: Figures 16(left), 
22(right); Charlene Fleming: Figures 1, 2(left), 3(right), 4(right), 6(left), 8, 9(left), 14(right), 
15, 17(left,right), 18; Stephanie Lew: Fig­ures 21(right), 23; Dora Roychoudhury: Figures 6(right), 13, 
25; Rebbaz Royee: Figures 3(left), 4(left), 5, 7, 14(left,center), 17(center); Corey Tom: Figures 9(right), 
10, 12, 16(center), 19, 20, 21(left), 22(left). All photos are by the author. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343497</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>162</pages>
		<display_no>14</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Computational aesthetic evaluation]]></title>
		<subtitle><![CDATA[steps towards machine creativity]]></subtitle>
		<page_from>1</page_from>
		<page_to>162</page_to>
		<doi_number>10.1145/2343483.2343497</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343497</url>
		<abstract>
			<par><![CDATA[<p>Programmer and artists have invented a broad range of generative systems that create art and music. These powerful systems sometimes produce results that surprise their human collaborators, but the surprises are not always welcome or useful. Machine creativity needs a computational self-critical function that can guide generative systems toward valuable creative output.</p> <p>This course provides a fast-moving, state-of-the-art overview of computational aesthetic evaluation. Some notable limited successes aside, computational aesthetic evaluation is far from a solved problem, and a "how to" course is not possible at this time. The intent of this course is to identify all of the significant trail heads, to share what previous explorers have found, and to encourage future journeys by artists and researchers along the paths that seem most promising.</p> <p>The course begins with a brief summary of terminology, then reviews classic formulaic and geometric theories of aesthetics that are possibly amenable to digital exploitation, including Birkhoff's "aesthetic measure", the golden ratio, Zipf's law, fractal dimension, basic gestalt design principles, and the rule of thirds. A section on evolutionary art systems focuses on aesthetic evaluation in fitness functions, including interactive systems, strategies for automated evaluation such as performance goals, error measures, complexity measures, multi-objective and Pareto optimization, and biologically inspired methods that produce emergent aesthetic fitness functions such as coevolution, niche construction, swarm behavior, and curious agents. The course concludes with a review of the future of computational aesthetic evaluation, recent developments in the empirical study and psychological modeling of aesthetics, and the nascent field of neuroaesthetics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738930</person_id>
				<author_profile_id><![CDATA[81500647152]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Galanter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
   papers from the seminar are available online at http://drops.dagstuhl.de/opus/portals/index.php?semnr=09291 
 Any one of these topics could branch off into a discussion that would .ll the session  If beauty is 
in the eye of the beholder, what happens when the beholder is a computer?  What will it take to create 
computers that can said to be truly creative? Why is CAE critical to computational creativity?    
We WON T be going into questions such as: Would such a machine actually experience a sense of redness, 
brightness or other qualia? How would we know? Can machine evaluation be successful without such experience? 
If a machine isn t conscious does that mean human aesthetic judgement and computational aesthetic evaluation 
can never converge? But isn t the brain is itself a machine? But even if it is, is human embodiment a 
requirement for human aesthetic evaluation?   Remember this is an introductory level course. It s a 
broad survey to get folks started. The role of computational aesthetic evaluation has a special place 
in evolutionary art.    George Birkhoff notes he is only addressing formal issues and not connotative 
(i.e. symbolic) meaning. Also the measure is only valid within a group of similar works. Birkhoff begins 
with an explicit psychoneurological hypothesis. He describes complexity (C) as the degree to which unconscious 
psychological and physiological effort must be made in perceiving the object. Order (O) is the degree 
of unconscious tension released as the perception is realized. This release mostly comes from the consonance 
of perceived features such as repetition, similarity, contrast, equality, symmetry, balance, and sequence. 
Birkhoff notes, The well known aesthetic demand for unity in variety is evidently closely connected with 
this formula. Birkhoff,G.D.(1933).Aesthetic measure.Cambridge,Mass.,:Harvard University Press. Ultimately 
his formula relies on subjective judgements and cheats such as his F factor. And empirical studies almost 
immediately called his work into question. n = 95 student subjects responding to paired tests Only order 
matters. Correlation measure Series A = .44 and Series B = . 38 Wilson,D.J.(1939).An experimental investigation 
of Birkhoff's aesthetic measure. The Journal of Abnormal and Social Psychology, 34(3), 390-394. Many 
have noted that Birkoff s is more a measure of orderliness than beauty. But he made two lasting contributions: 
He suggests that complexity and order relationships are key. He suggests an underlying neurological basis 
for aesthetics.   Holger,H.(1997).Why a special issue on the golden section hypothesis?:An introduction. 
Empirical Studies of the Arts, 15.  So relative to the most frequent word, the second most frequent 
word will occur 1/2 as often, the third most frequent word 1/3 as often, and so on... Manaris, B.,Vaughan, 
D.,Wagner, C., Romero, J., &#38; Davis, R. B. (2003). Evolutionary music and the Zipf-Mandelbrot law: 
Developing .tness functions for pleasant music.Applications of Evolutionary Computing,2611, 522-534. 
Manaris, B., Machado, P., McCauley, C., Romero, J., &#38; Krehbiel, D. (2005). Developing .tness functions 
for pleasant music: Zipf's law and interactive evolution systems.Applications of Evolutionary Computing,Proceedings, 
3449, 498-507. Machado,P.,Romero,J.,Santos,A.,Cardoso,A.,&#38; Pazos,A.(2007).On the development of 
evolutionary arti.cial artists. [doi: DOI: 10.1016/j.cag. 2007.08.010]. Computers &#38; Graphics, 31(6), 
818-826. Voss, R. F., &#38; Clarke, J. (1975). 1-F-Noise in Music and Speech. [Article]. Nature, 258(5533), 
317-318. Fractals have fractional dimensions. For example a fractal with a dimension of 1.3 would only 
partially .ll the plane it is in. Peitgen, H.-O., Jürgens, H., &#38; Saupe, D. (1992). Chaos and fractals 
: new frontiers of science.NewYork:Springer-Verlag. Taylor, R. P. (2006). Chaos, Fractals, Nature: a 
new look at Jackson Pollock. Eugene, USA: Fractals Research. The box counting method used to empirically 
measure the fractal dimension of Pollock paintings. If we assume that Pollock s technique improved over 
time we can say that when it comes to this body of work fractal dimension is a possible measure for computational 
aesthetic evaluation. This might also (or just) be a case of the peak shift phenomenon.  This idea resonates 
with various cognitive theories of aesthetics where high degrees of stimulation being successfully abstracted 
is experienced as being pleasurable. Is it possible that computer vision techniques can be adapted to 
analyze along the lines of traditional design rules of thumb? Emerson,R.W.(1979).Nature,addresses,and 
lectures (2ded.).NewYork: AMS Press. Coleridge, S.T., Coleridge, H. N., Coleridge, J.T., &#38; Woodring, 
C. (1990). Table talk. Princeton, N.J.: Princeton University Press Design principles around balance 
often re.ect our learned expectations from the physical world where we seek to avoid instability.  Our 
perceptual cognition seeks to extract simplicity of structure.   Peter Paul Rubens,Tiger Hunt, c. 1616. 
 Peter Paul Rubens,Tiger Hunt, c. 1616. Intuitive use of rule of thirds Note focal points Rule of thirds 
not discussed until the end of the 18th century Discussed most frequently with regard to photography 
Can computer vision do this kind of analysis? Requires high level of abstraction. There have been few 
attempts to apply standard design principles in computational aesthetic evaluation. Datta, R., Joshi, 
D., Li, J., &#38; Wang, J. Z. (2006). Studying aesthetics in photographic images using acomputational 
approach.Computer Vision -Eccv 2006, Pt 3, Proceedings, 3953, 288-301.  Inspired by biological neurology, 
but simpli.ed by many orders of magnitude. Input nodes are exposed to input data. Each connection has 
a weight representing the strength of the connection. Each hidden node sums each input scaled by its 
weight. Each output node does the same applying weights. With each exposure to data the weights are 
adjusted either based on feedback from a training set or reoccurring input patterns (SOM or self­organizing 
map). In order to create nonlinear models the input summation commonly uses a sigmoid transfer function. 
Once the network is trained new and novel input should exhibit learned behavior at its output.  For 
discrimination tasks you might have one output node per possible result. If only it was this easy! A 
signi.cant aspect of ANN use is the preprocessing and presentation of input data. Assigning numerous 
input nodes per pixel is computationally unworkable (at this time.) But what if we can use image processing 
to extract overall measures? This is similar to the earlier example by Datta et al Potentially more robust 
to complex nonlinear relationships than statistical regression methods. This is a generative system 
not really an aesthetic evaluation system. But it is an attempt to capture and model an aesthetic style. 
Todd,P.M.(1989).A Connectionist Approach toAlgorithmic Composition. Computer Music Journal, 13(4), 27-43. 
Brooks, Hopkins, Neumann &#38; Wright. "An experiment in musical composition." IRE Transactions on Electronic 
Computers,Vol. 6, No. 1 (1957). Self Organizing Maps clusters arbitrary data presented to the input 
layer without feedback. Phon-Amnuaisuk, S. (2007). Evolving music generation with SOM-.tness genetic 
programming. Lecture Notes in Computer Science, 4448 LNCS, 557-566. Law, E., &#38; Phon-Amnuaisuk, S. 
(2008).Towards Music Fitness Evaluation with the Hierarchical SOM Applications of Evolutionary Computing 
(pp. 443-452): Springer. This system was only demonstrated for a single person! Gedeon,T. s. (2008). 
Neural network for modeling esthetic selection. Lecture Notes in Computer Science, 4985 LNCS(PART 2), 
666-674.  Fogel, L. J. (1999). Intelligence through simulated evolution : forty years of evolutionary 
programming.NewYork:Wiley. Selection -genotypes with better .tness scores are selected more often than 
others, and genotypes with low .tness scores may be removed from the gene pool. Variations -a single 
genotype can be mutated, or two genotypes may be recombined. Expression -use the genotype to create a 
corresponding phenotype. Evaluation -use a .tness function to measure the phenotype competitiveness. 
Integration -genotypes of suf.cient quality are added to the gene pool. There are many variations: 
Only mutation or only crossover. Mutation and/or crossover rate high or low. Select and/or remove genotypes 
on a statistical basis. Allow weak genotypes to survive. Protect genotypes with a score above a certain 
threshold. Alter the gene variation from high to low over the course of evolution. (This is called Simulated 
annealing ) Most of all the design of the genotype data structure invites creativity and innovation on 
the part of the programmer.  For a good overview of evolutionary art systems see: Bentley,P.and Corne,D.(2002).An 
introduction to creative evolutionary systems, in P. Bentley and D. Corne (eds), Creative Evolutionary 
Systems, Morgan Kaufmann,Academic Press,San Francisco,CA,San Diego,CA,pp.1 75.  For an overview of the 
contemporary challenges in evolutionary art see: McCormack, J. (2005). Open problems in evolutionary 
music and art, APPLICATIONS OF EVOLUTIONARY COMPUTING, PROCEEDINGS 3449: 428 436. Galanter,P.(2010).The 
problem with evolutionary art is...,in C.DiChio,A. Brabazon, G.A. DiCaro, M. Ebner, M. Farooq,A. Fink, 
J. Grahl, G. Green.eld, P. Machado,M.Oneill,E.Tarantino andN.Urquhart (eds),Applications of Evolutionary 
Computation, Pt Ii, Proceedings,Vol. 6025 of Lecture Notes in Computer Science, Springer-Verlag Berlin, 
Berlin, pp. 321 330. (see http://philipgalanter.com for a copy) Todd, S., &#38; Latham,W. (1992). Evolutionary 
art and computers. London ; San Diego:Academic Press.  Todd, S., &#38; Latham,W. (1992). Evolutionary 
art and computers. London ; San Diego:Academic Press. The artist/operator essentially navigates through 
a very large multi­dimensional solution space in search of a satisfying form. Todd, S., &#38; Latham,W. 
(1992). Evolutionary art and computers. London ; San Diego:Academic Press. The phenotype is generated 
by plugging each pixel location X andY into the expression. The expression is treated as a genotype by 
storing it as a parsed data structure that allows simple substitutions for mutations and crossover. Sims,K.(1991).Arti.cial 
Evolution For Computer-Graphics. Siggraph 91 Conference Proceedings, 25, 319-328. Again, how could the 
evaluation of these images be automated? Sims,K.(1991).Arti.cial Evolution For Computer-Graphics. Siggraph 
91 Conference Proceedings, 25, 319-328. Todd, P. M., &#38; Werner, G. M. (1998). Frankensteinian Methods 
for Evolutionary Music Composition. In N. Grif.th &#38; P. M.Todd (Eds.), Musical networks: Parallel 
distributed perception and performance. Cambridge, MA: MIT Press/Bradford Books. One can imagine now 
making this even more passive with video based crowd analysis and extracting viewing times. Draves,S.(2005).The 
electric sheep screen-saver:A case study in aesthetic evolution. Applications of Evolutionary Computing, 
Proceedings, 3449, 458-467. The People s Choice project polled the public about their preferences in 
paintings. Based on the results regarding subject matter, color, and so on they created this painting. 
Komar,V.,Melamid,A.,&#38; Wypijewski,J.(1997).Painting by numbers:Komar andMelamid's scienti.c guide 
to art (1st ed.).NewYork:Farrar Straus Giroux. Corresponding to the public s like for historical .gures 
and exotic animals they included these features. But also the popular blue lake, family, moderate vegetation, 
game animals. Of course this isn t serious science. Komar and Melamid s critique was of the politics 
of public relations and institutions that wield statistics as a weapon. But clearly trending towards 
the mean is not a way to create the unique vision most expect of contemporary artists.  Q0 are contact 
and photosensors, E0 and E1 are effectorevolved genotype shown in .gure 5. The effector outputs outputs, 
and those labeled * and s+? are neural nodesof this control system cause the morphology above to roll 
 This begs the question to some extent...what kind of performance yields that perform product and sum-threshold 
functions. forward in tumbling motions. high aesthetic quality? Sims,K.(1994).EvolvingVirtual Creatures.Siggraph 
'94 Proceedings,28, 15-22. A rather minimal aesthetic standard...an existential or size rule. http://notnot.home.xs4all.nl/breed/BREEDinfo.html 
 The results are manufactured using various rapid prototyping or 3D printer technologies. Saying the 
performance goal is make it beautiful doesn t really help. http://notnot.home.xs4all.nl/breed/BREEDinfo.html 
  Once the error measure was suf.ciently minimized the winning genes were then used to drive a robotic 
painting arm. (pictured is Walter Benjamin author of The Work of Art in the Age of Mechanical Reproduction 
) Aguilar, C., &#38; Lipson, H. (2008). A robotic system for interpreting images into painted artwork. 
Paper presented at the International Conference on Generative Art. McDermott, J., Grif.th, N. J. L., 
&#38; O Neill, M. (2005).Toward User-Directed Evolution of Sound Synthesis Parameters. Applications of 
Evolutionary Computing, Proceedings, 3449, 517-526. Mitchell,T.J.,&#38; Pipe,A.G.(2005).Convergence Synthesisof 
Dynamic Frequency Modulation Tones Using an Evolution Strategy Applications on Evolutionary Computing 
(pp. 533-538). http://rogeralsing.com/2008/12/07/genetic-programming-evolution-of-mona­lisa/ Magnus, 
C. (2006). Evolutionary Musique Concrète. In F. Rothlauf &#38; J. Branke (Eds.),Applicationsof Evolutionary 
Computing,EvoWorkshops 2006 (pp. 688-695). Berlin: Springer. Fornari, J. (2007). Creating soundscapes 
using evolutionary spatial control. Lecture Notes in Computer Science, 4448 LNCS, 517-526. Hazan,A.,Ramirez,R.,Maestre,E.,Perez,A.,&#38; 
Pertusa,A.(2006).Modelling Expressive Performance:A Regression Tree Approach Basedon Strongly Typed Genetic 
Programming Applications of Evolutionary Computing (pp. 676-687). Fitness scores based on aesthetic 
quality rather than simple performance or mimetic goals are much harder to come by. Their belief is that 
fractal compression is similar to the way humans process images, i.e. apparent complexity is easily described 
due to self-similarity. Machado,P.,&#38; Cardoso,A.(2002).All the Truth About NEvAr.[10.1023/A: 1013662402341]. 
Applied Intelligence, 16(2), 101-118. Machado,P.,&#38; Cardoso,A.l.(2003). NEvAr System Overview. Paper 
presented at the International Conference on Generative Art. Aesthetic Quality related to the ratio 
of visual stimulus complexity to percept complexity is a sophistication of degree of order / degree of 
complexity ratio of Birkhoff,especially when you recall Birkhoff s underlying cognitive model.   Neufeld, 
C., Ross, B. J., &#38; Ralph,W. (2008).The Evolution of Artistic Filters. In J. Romero &#38; P. Machado 
(Eds.), The art of arti.cial evolution : a handbook on evolutionary art and music (pp. 335-356). Berlin: 
Springer. Ross, B. J., &#38; Zhu, H. (2004). Procedural texture evolution using multi­objective optimization. 
New Generation Computing, 22(3), 271-293. Greenfeld, G. R. (2003). Evolving aesthetic images using multiobjective 
optimization. Cec: 2003 Congress On Evolutionary Computation,Vols 1-4, Proceedings, 1903-1909.  Dorin,A.(2005).EnrichingAestheticswith 
Arti.cial Life.In A.Adamatzky &#38; M.Komosinski(Eds.),Arti.ciallife models in software (pp.415-431). 
London: Springer-Verlag.  Note that this leads to a balance of expected and surprising results.Todd 
says this is because random notes are less surprising because they don t set high expectations. More 
overall surprise is created via note sequences that lead to a high expectation and then violate it. Todd, 
P. M., &#38; Werner, G. M. (1998). Frankensteinian Methods for Evolutionary Music Composition. In N. 
Grif.th &#38; P. M.Todd (Eds.), Musical networks: Parallel distributed perception and performance. Cambridge, 
MA: MIT Press/Bradford Books. Coevolution here creates aesthetics effective in the virtual environment 
but unlistenable to human ears. separation. Saunders, R., &#38; Gero, J. S. (2004). Curious agents and 
situated design evaluations.Ai Edam-Arti.cial Intelligence for EngineeringDesign Analysis and Manufacturing, 
18(2), 153-161. Reynolds,C.W.(1987).Flocks,herds,and schools:A distributed behavioural model. Computer 
Graphics, 21(4), 25 34. Helbing, D., &#38; Molnár, P. (1997). Self-organization phenomena in pedestrian 
crowds. In Self-Organization of Complex Structures: From Individual to Collective Dynamics Schweitzer, 
F., Ed., pp. 569 577. London: Gordon &#38; Breach. Novelty is measured by inference from the error function 
from a self­organizing map arti.cial neural network. Akin to the Wundt Curve and Effective Complexity 
(see later), curiosity is maximized when the stimulus is a balance of similarity and difference to previous 
experience. (a) the paintings are not sequenced to present incremental novelty, so visitors bunch up 
in the .rst room (entering in the left door), and then leave quickly. (b) the paintings are sequenced 
with novelty that is neither great nor small. The result is even traf.c .ow. (c) after several visits 
the agents walk through the gallery rather quickly and ef.ciently.  Urbano,P.(2006).Consensual paintings.ApplicationsOf 
Evolutionary Computing, Proceedings, 3907, 622-632.   McCormack,J.,&#38; Bown,O.(2009).Life'sWhat You 
Make:Niche Construction and Evolutionary Art. Paper presented at the Proceedings of the EvoWorkshops 
2009 on Applications of Evolutionary Computing. Drawing on the left is without niche construction, with 
niche construction on the right. The aesthetics of the drawing develop over time as it is drawn. This 
goes back to the recognition that there is Type 1 and Type 2 CAE. Galanter, P. (2012 in press). Computational 
Aesthetic Evaluation: Past and Future. In J. McCormack &#38; M. d'Inverno (Eds.), Computers and Creativity. 
Berlin: Springer.  Shannon s information theory describes the information capacity of a channel. The 
more disordered the signal, the less compressible it is, the more information it carries. Bense and Moles 
adapted these ideas in Information Aesthetics. This idea of complexity opposing order is found in Berkhoff, 
Machado, and others Shannon,C.E.(1948).A mathematical theory of communication.The Bell System Technical 
Journal, 27(3), 379--423. Bense,M.(1965).Aesthetica;Einführung in die neue Aesthetik.Baden-Baden,:Agis-Verlag. 
Moles,A.A.(1966).Information theory and esthetic perception.Urbana,:University of Illinois Press.  
Kolmogorov has a similar notion of algorithmic complexity. Again relative incompressibility (this time 
of the code used to implement the algorithm in question) is equated with complexity. This is adapted 
in Schmidhuber s Formal Theory of Creativity. Schmidhuber,J.(2012 in press).A FormalTheory of Creativity 
toModel the Creation of Art. In J. McCormack &#38; M. d'Inverno (Eds.), Computers and Creativity. Berlin: 
Springer. Kolmogorov,A.N.(1965).Three approaches to the quantitative de.nition of information. Problems 
in Information Transmission, 1, 1-7. Complexity is a balance of order and disorder Gell-Mann, M., &#38; 
Lloyd, S. (1996). Information measures, effective complexity, and total information. Complexity, 2(1), 
44-52. We .nd the balance of order and disorder in biological life more complex than either highly ordered 
or disordered systems. Effective complexity gives us a way to order our generative art systems And it 
may be a more effective way to apply notions of complexity in aesthetic evaluation Galanter, P. (2003).What 
is Generative Art? Complexity theory as a context for art theory. Paper presented at the International 
Conference on Generative Art, Milan, Italy. (see http://philipgalanter.com for a copy) Galanter, P. (2012 
in press). Computational Aesthetic Evaluation: Past and Future. In J. McCormack &#38; M. d'Inverno (Eds.), 
Computers and Creativity. Berlin: Springer.  In Green.eld s work on coevolution he notes that we don 
t understand aesthetic judgement in humans, and that makes it dif.cult to derive or justify algorithms 
for CAE. Black box techniques such as statistical methods and arti.cial neural networks have had limited 
success Green.eld, G. R. (2008). Co-evolutionary Methods in Evolutionary Art. In J. Romero &#38; P. Machado 
(Eds.),The Art Of Arti.cial Evolution (pp. 357-380): Springer Berlin Heidelberg. No wonder CAE is so 
dif.cult. Note the hardware nature requires for human aesthetics. Don t confuse 10^15 connections with 
10^15 bits. Glial cells seem to be more than just glue an substrate. Koob,A.(2009).The Root of Thought:What 
Do Glial CellsDo? Mind Matters Retrieved 11/29/09, 2009, from http://www.scienti.camerican.com/ article.cfm?id=the-root-of-thought-what 
 Using operant conditioning pigeons were trained using a set of paintings categorized by adults. Then 
tested with a previously unseen holdout set of paintings. So maybe we only need bird brain computation. 
But note that pigeon neurology is heavily invested in visual processing. Watanabe , S. (2009). Pigeons 
can discriminate good and bad paintings by children.[10.1007/s10071-009-0246-8].Animal Cognition,13(1). 
 Watanabe S (2001)Van Gogh,Chagall andpigeons.Animal Cognition 4:147  Pinker,S.(1994).The language 
instinct (1st ed.).NewYork,NY:W.Morrow and Co. It is suggested that this is behind the practice of men 
bringing women .owers, jewelry, etc. *Impractical* gifts are the most romantic of all...and best evidence 
of material wealth. Dutton, D. (2009).The art instinct : beauty, pleasure, &#38; human evolution (1st 
U.S.ed.).NewYork:Bloomsbury Press. Every culture has art, music, dance, story telling, etc. That suggests, 
but doesn t prove, that there is some instinctual force behind it.  Remember, Melamid is one of the 
artists behind the America s most wanted painting and project. My comment -this isn t hard science...but 
it sure is interesting.  Arnheim,R.(1974).Art and visualperception:apsychology of the creative eye (New, 
expanded and revised ed.). Berkeley: University of California Press.  Various attributes can group objects. 
 Without color the objects tend to group by shape. With color your attention can shift from color to 
color creating groups of objects. Here the objects group by color or not at all. But borders decisively 
rede.ne the groups. Note how alignment, spacing, color, and abstract notion of simple geometric shapes 
establish repetition. Proximity can group objects and overlap can fuse them. We tend to see shapes 
even if they are not entirely enclosed. Our cognitive perception .lls in missing information. Our cognitive 
perception will also follow motion cues and unify otherwise separate objects. Again: Arnheim showed us 
that perception is cognition. Berlyne was particularly interested in collative effects that bring together 
experiences in a comparative manner. He noted explicitly the correspondence between collative effects 
and notions of surprise and novelty in information theory. Berlyne, D. E. (1960). Con.ict, arousal, and 
curiosity. NewYork,: McGraw-Hill. Berlyne, D. E. (1971). Aesthetics and psychobiology. NewYork,:Appleton­ 
Century-Crofts. Berlyne was particularly interested in collative effects that bring together experiences 
in a comparative manner. He noted explicitly the correspondence between collative effects and notions 
of surprise and novelty in information theory. Neurological activation of the reward and aversion systems 
combine to produce a positive or negative hedonic response. Despite his interest in information theory 
and related notions of complexity, his proposed hedonic response to arousal potential is not proportional 
to the amount of information carried. Put another way, Berlyne s notion of complexity is not proportional 
to positive aesthetic response. Effective complexity is roughly proportional to Berlyne s hedonic response 
curve. Might this be a clue that our aesthetic response is tuned to effective complexity? In other words 
tuned to the complexity of the biological world? See the following for my formulation of this: Galanter, 
P. (2012 in press). Computational Aesthetic Evaluation: Past and Future. In J. McCormack &#38; M. d'Inverno 
(Eds.), Computers and Creativity. Berlin: Springer. Galanter, P. (2010). Complexity, Neuroaesthetics, 
and Computational Aesthetic Evaluation. Paper presented at the International Conference on Generative 
Art, Milan, Italy. Martindale,C.,Moore,K.andBorkum,J.(1990).Aesthetic preference: Anomalous .ndings 
for berlyne s psychobiological theory, The American Journal of Psychology 103(1): 53 80. Martindale, 
C. (1991). Cognitive psychology : a neural-network approach, Brooks/ Cole Publishing Company, Paci.c 
Grove, California. Martindale,C.(2007).A neural-network theory of beauty, in C. Martindale, P. Locher 
andV.Petrov (eds), Evolutionary and neurocognitive approaches to aesthetics, creativity, and the arts, 
Bay-wood,Amityville,N.Y.,pp.181 194. According to Martindale s model regarding aesthetic preference... 
 See Martindale s the clockwork muse for a more developed theory of stylistic change in the arts.  
In the past couple of decades activity in the area of empirical studies of human aesthetics has been 
on the increase. Such studies are dif.cult because of the complexity of human perception and cognition, 
the challenges in using human subjects, the limited sample sizes, the need to control all manner of experiential 
and context variables, etc. And then taking these highly individual results and trying to .nd a unifying 
theory or model is even more dif.cult. But the individual stamps collected are intriguing nevertheless. 
 Schimmel, K. and J. Forster, How temporal distance changes novices' attitudes towards unconventional 
arts. Psychology of Aesthetics, Creativity, and the Arts, 2008. 2(1): p. 53-60. Parker, S., et al., Positive 
and negative hedonic contrast with musical stimuli. Psychology of Aesthetics, Creativity, and the Arts, 
2008. 2(3): p. 171-174. Smith, L.F., et al., Effects Of Time And Information On Perception Of Art*. Empirical 
Studies of the Arts, 2006. 24(2): p. 229-242. Collier , G.L.,Why Does Music Express Only Some Emotions? 
A Test Of A Philosophical Theory. Empirical Studies of the Arts, 2002. 20(1): p. 21-31. Cooper, J.M. 
and P.J. Silvia Opposing Art: Rejection As An Action Tendency Of Hostile Aesthetic Emotions. Empirical 
Studies of the Arts, 2009. 27(1): p. 109-126. Koneni,V.J., Does music induce emotion? A theoretical and 
methodological analysis. Psychology of Aesthetics, Creativity, and the Arts, 2008. 2(2): p. 115-129. 
 Jacobsen ,T.,&#38; Höfel,L.(2001).AestheticsElectri.ed:An Analysis Of Descriptive Symmetry And Evaluative 
Aesthetic Judgment Processes Using Event-Related Brain Potentials. Empirical Studies of the Arts, 19(2), 
14. Coney , J. and C. Bruce Hemispheric Processes In The Perception Of Art. Empirical Studies of the 
Arts, 2004. 22(2): p. 181-200. Katz, B.F., Color Contrast And Color Preference. Empirical Studies of 
the Arts, 1999. 17(1): p. 1-24. Feist ,G.J.andT.R.Brady OpennessTo Experience,Non-Conformity,AndThe 
Preference For Abstract Art. Empirical Studies of the Arts, 2004. 22(1): p. 77-89. Giannini,A.M.andP.Bonaiuto,SpecialImage 
Contents,Personality Features, And Aesthetic Preferences. Empirical Studies of the Arts, 2003. 21(2): 
p. 143-154. Kozbelt,A.,Dynamic Evaluation Of Matisse s 1935 Large Reclining Nude. Empirical Studies 
of the Arts, 2006. 24(2): p. 119-137. Locher,P.,et al.,Artists Use Of Compositional Balance For CreatingVisual 
Displays. Empirical Studies of the Arts, 2001. 19(2): p. 213-227. Machotka,P.,Artistic StylesAnd Personalities:A 
Close ViewAndA More Distant View.Empirical Studiesof the Arts,2006.24(1):p.71-80. Firstov,V., et al.,The 
Colorimetric Barycenter Of Paintings. Empirical Studies of the Arts, 2007. 25(2): p. 209-217. Latto,R.and 
K.Russell-Duff,An Oblique Effect In The Selection Of Line Orientation By Twentieth Century Painters. 
Empirical Studies of the Arts, 2002. 20(1): p. 49-60. Polzella, D.J., S.H. Hammar, and C.W. Hinkle,The 
Effect Of Color On Viewers Ratings Of Paintings. Empirical Studies of the Arts, 2005. 23(2): p. 153-163. 
 Simonton,D.K.,Film music:Are award-winning scores and songs heard in successful motion pictures? Psychology 
of Aesthetics, Creativity, and the Arts, 2007. 1(2): p. 53-60. Oelmann, H. and B. Laeng,The emotional 
meaning of harmonic intervals. Cognitive Processing, 2009. 10(2): p. 113-131. Here are some light and 
speculative suggestions as to additional future directions functional magnetic resonance imaging (fMRI) 
positron emission tomography scanning (PET) functional near-infrared imaging (fNIR) But would a heat 
map movie of a CPU allow us to infer much about the algorithm being executed? In the Herring Gull the 
red spot on the beak of the parent acts as a stimulus causing the chicks to peck at it, and that in turn 
stimulates feeding behavior by the adult. Oddly, the herring gull chicks will also peck at any red dots, 
such as those painted on a stick, and a greater number of red dots will stimulate a stronger pecking 
response. This kind of behavior has been posited as a neurological precursor to caricature and other 
artistic techniques which exaggerate visual features. The combined effects of peak shift and habituation 
have been suggested as a neurological engine behind the tendency in art to move to increasingly extreme 
styles over time. See Martindale s the clockwork muse for a more developed theory of stylistic change 
in the arts. Martindale, C. (1990).The clockwork muse : the predictability of artistic change. NewYork, 
N.Y.: BasicBooks. for all manner of higher brain function including perception, language, creativity... 
lower levels aggregate inputs and pass the results up to higher levels of abstraction Neurologists know 
that the neocortex consists of a repeating structure of six layers of cells. Hawkins suggests within 
a given level higher layers constantly make local predictions as to what the next signals passed upward 
will be. Correct predictions strengthen connections within that level. Hawkins, J. and Blakeslee, S. 
(2004). On intelligence, 1st edn,Times Books, NewYork.  Glette,K.,Torresen,J.,&#38; Yasunaga,M.(2007).An 
Online EHW Pattern Recognition System Applied to Face Image Recognition Applications of Evolutionary 
Computing (pp. 271-280): Springer.   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343498</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>10</pages>
		<display_no>15</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Delivering creative feedback]]></title>
		<page_from>1</page_from>
		<page_to>10</page_to>
		<doi_number>10.1145/2343483.2343498</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343498</url>
		<abstract>
			<par><![CDATA[<p>Delivering useful, honest and effective feedback to creatives is one of the biggest daily challenges faced by producers, supervisors, teachers, etc. When critiques "feel" subjective, feedback loops can have negative effects on morale and production, regardless of the validity of the criticism.</p> <p>Art and design students learn and practice critique every day during, but after they embark on their careers those skills are quickly forgotten. For professionals in technical and scientific fields, where solutions and hypotheses can be proven right or incorrect, giving feedback on subjective matters (where all answers are shades of grey) or providing constructive criticism face to face can be very challenging.</p> <p>Whether you are dusting off old skills or learning critique techniques for the first time, this hands-on workshop provides a simple, effective framework for delivering actionable criticism to your team everyday, regardless of environment. The course shows attendees how to establish a structure for providing critique that works in creative, technical, and academic environments, and then focuses on some specific techniques for delivering feedback.</p> <p>Topics include: "client" expectations, defining expectations, problem definition, and constraints in creative briefs that serve as the backbone for the project or assignment; methods for delivering feedback against the brief; and the importance of building and sustaining environments that sustain trust and foster open and direct feedback. Actual critique methods are practiced in small groups during the course.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738931</person_id>
				<author_profile_id><![CDATA[81504684279]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Evan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirsch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Engine Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 EvaHirsch EngineCompany4ev@e-­-co4.comev@acm.org Workshop:DeliveringCreativeFeedback Acknowledgement 
After spending about seven years in games, I took some time off and decided it was time to regain some 
core drawing skills that had been withering since I graduated with my BFA in the mid 1980 s. I had the 
great fortune of enrolling in a one-week intensive drawing workshop with Tony Hull at City Lit in London. 
As this was a traditional drawing class, numerous critiques were held each day, and I realized that despite 
many years of sitting in critiques, I had rarely seen anyone conduct such a positive, useful and truly 
constructive critique as Tony had. Tony had the unique ability to focus conversation around the work 
in the most positive and constructive tones I had ever experienced. Unlike most critiques I had been 
part of, Tony was interested equally between the work, its intent, the critique and the environment of 
the critique itself. I then realized that despite working as a professional designer and then a CG artist, 
I had not sat in a single, proper critique session since transitioning into the entertainment business. 
A few pints later, a great friendship was born, and I was able to subsequently able to bring Tony in 
for some drawing and critique workshops with the crew of a feature I was working on. While Tony is still 
the master teacher of these workshops for artists and designers in production environments, we have continued 
to collaborate on the format and content since the first one he did for my crew in 2004. For the inaugural 
SIGGRAPH Business Symposium (SIGGRAPH 2011), I designed a companion version of this workshop specifically 
for business leaders and entertainment executives. This course for SIGGRAPH 2012 and its 2011 predecessor 
are largely due to what I have learned from Tony and for that I am deeply grateful, for his insights, 
wisdom and generosity. For more information on Tony, his teaching and his fine art work, please contact 
him via his website: www.tonyhull.mac. Introduction So much of the SIGGRAPH conference is firmly rooted 
in unequivocal scientific or mathematical fact, whereas the other half of the work presented is in the 
subjective realm. While the data, or facts, are open to thoughtful debate inside and outside of the conference, 
it can be much easier for people to offer feedback on those less subjective topics. However, when the 
content is subjective and based on emotional interpretation, right or wrong observations are more elusive 
(as they should be). The idea for this course started several years ago during a product review, when 
a VP of production in a very successful game studio told his executive producer and game designer, I 
was playing the build [game] last night with my 7-year old son, Marc. He thinks your gameplay sucks, 
and I agree. One morning, a few years later in dailies (on a feature), an Academy Award-winning VFX producer 
exclaimed to a room, That shot is crap. The lighting is wrong, the angle is bad, the color is tragic 
 before walking out of the room, leaving behind a shell­shocked crew. At a studio level, when subjective 
feedback is vague and hard to act upon, collateral damage likely affects trust and morale. Done well, 
a critique can be the oxygen every production team needs to grow and help artists advance their craft 
and core professional skills, all while ultimately benefitting their clients, directors, users, etc. 
This course is intended to provide a simple framework that can be adapted to any production environment. 
While the terminology used in these notes is artist centric, everything contained here can be easily 
applied to engineering/code reviews. Workshopvs.  Lecture The very first iteration of this course was 
more of a lecture format. After reviewing the initial results, we quickly realized there was too much 
emotional distance between the students and the course s message. When students don t have skin, or in 
this case ego in the game, the lessons lacked impact regardless of the participant s role in the organization. 
After we switched to a hands-on workshop where students drew about 40-50% of the time and practiced critique 
for an almost equal duration, the connection between lessons and the techniques being taught became readily 
apparent to the students. (Due to the logistical constraints of SIGGRAPH, this course will be limited 
to 10-15% hands-on.) To keep the course focused on critique technique as opposed to drawing technique, 
it is essential to level the playing field in the course itself. If your class is filled with people 
who can draw, have them draw with their non-dominant hand (left hand for right-handed artists and vice 
versa). For those with artistic ability and/or artistic professional you can have them draw (preferred) 
or alternatively have them use digital cameras and laptops to create the work samples. (Note: if you 
go this route, make sure there is no network access.) Typically our materials are large Post-it style 
easel pads and inexpensive charcoal with paper towels with all participants completing 5-minute black-and-white, 
continuous line-drawing exercises. Exercise1  Picture 2 Pop Quiz: Which is the better photograph and 
why? How do you answer such a question that is vague and lacks context? When offering your opinion in 
a professional situation, your ability to offer actionable and effective criticism is dramatically increased 
when you have, at a minimum, a brief or goal that you can assess the image against, as well as an understanding 
of the state (condition) of the work being evaluated. Exercise2: Setup: If possible, compose a simple 
still life in the center of the table that your students are sitting around. Anything will do. Typically 
we have a speakerphone, a bunch of coffee cups and everyone s cell phones stacked in a pile. Bonus round: 
Add a desk lamp to create shadows (however, it s entirely unnecessary). ExercisePart1 Each person takes 
5 minutes to draw the still life you just assembled (with their weaker hand, see above) as a continuous 
line drawing (they don t lift the pencil from the page). All students should do the same exercise together. 
ExercisePart2 After everyone has created his or her drawing, the next step is discussion. In groups 
of 10-12 people, put each drawing up on the wall (or if sitting around a table, turn the easels inward 
so everyone can see all the drawings). Start with the biggest/tallest person s drawing, and have the 
group provide feedback as to what they like and dislike about the drawing. Repeat this again with the 
person on his/her left, and continue until everyone has received feedback. Evaluation Normally the feedback 
portion of this exercise starts with some awkwardness but then starts to get smoother. However, the feedback 
generally tends to be scattered, inconsistent and is not actionable. As was the case in Exercise 1, the 
feedback was solicited in vague terms, which more than likely resulted in unfocused feedback. Ambiguous, 
subjective, inconsistent and/or thoughtless feedback delivers no practical benefit. Don tTakeitPersonally 
During the evaluation portion of the exercise, students quickly realize that receiving feedback is not 
something anyone looks forward to. Of course, in the work place, when a colleague or direct report gets 
critical feedback on a piece of work they did, people are quick to remind them to act professional and 
not take things personally. In doing so, we dismiss the fundamental reason the vast majority of people 
became artists in the first place. For most people, it was precisely so they could personally find a 
way to contribute to a creative body. No competent director would ever dare tell an actress not to take 
his or her feedback personally as it s widely understood that acting is a deeply personal act of bearing 
one s soul, or imagination, to an audience of strangers. While they may be contributing to someone else 
s vision, the work they contribute is still a deeply personal interpretation of what they understood 
the need to be. (Note: If you are reading these course notes and did not participate in a workshop, this 
concept will seem rather obvious, but without skin in the game, it s hard to appreciate the significance 
of this point.) Contextis  Key If the goal of giving criticism is to provide understandable, useful 
and actionable feedback, the context by which this criticism is delivered is everything. In the second 
exercise above, the assignment was vague, as was the feedback you gave each other and, ultimately, took 
away. Because the assignment lacked a Creative Brief, the viewer is unable to determine if the drawing 
was successful given the vague context of the brief. Likewise, it becomes difficult for the reviewer 
to offer feedback that can help improve the drawing towards that goal. Without a clear and focused brief, 
the feedback will be arbitrary and less likely to be viewed as constructive by the artist. In thinking 
about delivering feedback, a brief need only establish context for the intent of the work. The person 
asking for the feedback should then describe the state of the work and the feedback being requested. 
Once this is understood, the reviewer can deliver feedback that is relevant, useful and appreciated. 
Each time someone (re)establishes context before providing feedback, they remind themselves to provide 
thoughtful, constructive and focused remarks. Too often, people provide feedback that is either purely 
subjective or that comes with little real thought. Thoughtless, unactionable feedback is distracting 
and an enemy of a great creative environment. Context:  Roles In the previous exercise, and in the absence 
of a clear boss, producer, etc. in the group, we started with the tallest person in the group to lend 
the air of intimidation to the exercise. This subtle dynamic reminds participants how people communicate 
differently when they are aware of the role, seniority, rank, etc. of the people they are addressing. 
The rank or weight of people s position is always apparent and unfortunately, will be a significant yet 
unintended factor in any feedback situation. It is human nature for employees to want to please their 
bosses. In creative environments where peoples livelihoods are tied to subjective opinions and criteria, 
it is natural for people to seek reassurance whenever it is available (i.e. the GM/MD is walking across 
the production floor and an artist shows him what he/she is working on.)  Manager/Producers View of 
a team Artists View of a the same team As is the case in relationships, people often want to be heard 
but don t want their problems solved for them. The greatest perk of seniority is the privilege of silence, 
or specifically in these situations, to not offer a solution. It is often far more helpful for people 
to walk through the problem at hand by asking questions and letting them develop the potential answer 
and own the solution. Context:  Directionvs.Opinion To state the obvious, in large creative productions, 
there are many different heads with different responsibilities. Obviously, a director or game designer 
will own the vision, and their supervisors are responsible for working with the crew to get work delivered 
that reflects their vision. Conversely, the head of a VFX or game studio will have responsibilities to 
his shareholders and/or partners that may appear to be at odds with the direction a director has delivered. 
To the average production artist, this is rarely relevant. i.e.; A junior artist is working on a shot 
that came without many reference materials. One evening he shows the shot to a studio head and asks what 
the studio head thinks. The studio head replied he really doesn t care for the red on the house in the 
background . Wanting to curry favor with the studio head, the junior artist later changes the red to 
white, only to be told to change it back to red in dailies the following day. In this example, the studio 
head inadvertently undermined the supervisor on his payroll. Had the studio head asked what the direction 
was for the shot, he would be reminding the artist of what the requirements are and could frame any feedback 
in that context. If he still wanted to share his opinion and frame it as such, he could do so and still 
redirect the artist back to question his own work against the direction he was given. i.e.: I m not sure 
what direction you were given for this shot, but I don t think the red is paying off. Most importantly, 
that s my opinion and you should confirm the actual direction with your supervisor. The  Creative  Brief 
The central theme of this course is to eliminate as much subjectivity from delivering feedback as possible 
and keep feedback focused on the work being done. In product design, industrial designers typically approach 
their work as solving a problem. This approach of framing the assignment as a well-defined problem to 
be creatively solved is the key to making feedback less personal and less subjective. Most projects in 
computer graphics and software development have (or can easily establish) a set of clear and precise 
technical constraints and specifications. (In many cases there will also be a market requirements document.) 
That specification document should be partnered with an equal document that focuses purely on the subjective 
or emotional requirements of the project. The specification and the brief collectively serve as a concise 
and clear definition of the work to be done or the problem to be solved. This latter document, the creative 
brief, will serve as a means of defining the creative problem to be solved for the duration of the project 
and, more precisely, as the highest level of context for evaluating the work. Writing a good, effective 
brief consistently takes longer and more iterations than expected. A good creative brief is not a feature 
list or specification. (Typically, the smaller the number of authors, the more likely a clear, strong 
brief will emerge that serves for the duration of the project.) While less is more in authors and a strong 
creative brief, it is critical to establish consensus and understanding amongst your team. (The following 
template is one proven format and methodology, but adaption is encouraged.) ProjectNameCreative  Brief 
 0.9,Date The  Statement: singlesentencethatdescribes  thesuper-­-objectiveforyourproduct slong-­-termvision.(Remember,thisisnot 
 about  featuresbut  about  theemotionalconnectionyouwant  tobuildbetweenyouandyour  user.)  ExecutiveSummary 
Twoparagraphs(max)thatdescribe  the  overallcreative  goalsforthe  projectandthe  emotionsyouwantusersandreviewerstoexperienceandultimatelytakeawayfromplayingthegame.Thisisnot 
 about  technicalaccomplishmentsand/orspecificfeatures,norshouldyoudescribe  yourgame  ascombinationoralternativetoother 
 games.It saboutthecoremechanic,coreemotionalarcandfictionthatwillbe  the  commonbasisforthe  product 
sgameplayandvisualandbrand  identity. BocceBallAssassin isanewandexcitinggenrethatcombinestheold-­-worldcharmoftraditionalItalian 
 lawn  bowling  withtheintensefeelingofpowerandsheerbrutalitybyusingblunt-­-forcetraumatoeliminate  the 
 menwhodate  yoursisters-­--­-in  keeping  with  theoxymoronicidea  ofkilling  peoplewhilelawn  bowling.Thegamewillfocusonafewcharacterswhoaredeceptivelycharmingandcomfortable,andverybelievable.Thegamewillrelyovisualstylethatisverygraphicantypicalofcomicbookthatalwaysfeeling 
 moreillustrativethan  photoreal.Etc.,etc.,etc .  PrimaryDesignGoals Threebulletpoints:Theseprimarygoalsarekeywordattributesthatthedesignmustachieve 
 Less  is  more:Asyou  willreferto  thesepointsregularlywhen  providingfeedbackorasameansofframingcritique,themorebriefyouare,theeasier 
 it  willbetobecomeconsistentovertime.  Emotive:Thisisnotabouttechnicallimitationsorfeatures,butrather 
 theemotiveresponsesyouwant  toelicit  inyourusers.  (Optional)  What  It s  Not: Onetothreebulletpoints:Ifithelpsclarifyyourproject,includeamanyasthreesinglebulletpoints 
 thatclarifywhatyourproductshouldnotbe.  Avoidcliché:Don temploywidelyusedoreasilymisinterpretedterms,likeexciting,dramatic,warm,inviting,clean,etc.Thatdoesn 
tmeanyoucan tusethem,buttryto  find  termsthatreallydescribehowyou  wantausertofeelabout  theapp.  
SomeNotes  tKeepiMind: Greatproductsmakeaverysimple,clearandconcisepromisetousersandthenover-­-deliverothatpromise.ThinkofthisCreativeBriefasthecorepromise. 
 Thisisnotspecificationortechnicaldocument;thosearelikelyneededandshouldbepartnerto  thisdocument. 
 Avoid  the F word:featuresorafeatureset.Thisdocumentisto  describetheemotionalconnectionyouwant  yourconsumerstohavewithyourproduct. 
   Framework  forDeliveringtheMessage(s) There are always examples to be had of the creative genius 
who crafted their opus as a solitary effort, but they probably weren t in the design or production of 
a modern film, game or TV project. Large projects require large teams that are invested in collaborating 
towards a single vision. As mentioned here repeatedly, the tone and approach used to deliver criticism 
directly impacts how the feedback will be utilized. With that in mind, here is a framework for effectively 
delivering feedback, regardless of the environment: 1. Be on brief. Request and listen to the brief and 
the state, then contain your feedback as to how the work does or does not meet the requirements. 2. 
Be focused on the work and the task at hand. Limit your comments to the work in front of you and direct 
your comments toward the work, never toward the artist, their intentions, their personality, etc. 3. 
Be present. To state the obvious: Don t split your attention between the critique and reading e-mail, 
sending texts, etc. 4. Be constructive. Above all, remain objective and remember that critique is not 
about a reviewer s likes and dislikes, or the approach the artist should have used. Objective critique 
is about discovering opportunities for the work to meet or surpass the requirements of the brief. 5. 
Be clear. Describe what you see and the impact it has. There is no need to provide solutions or to speculate. 
 6. Be honest. Your colleague, employee, etc. deserves to hear honest opinion, and it is their responsibility 
to filter the feedback and decide what to act upon. Providing hollow praise or withholding honest criticism 
only damages the level of trust on a team.   Plussing  Technique   Walt Disney created the term plussing 
for a method he developed at Imagineering to encourage designers to always take each idea one step beyond 
. This philosophy closely follows the premise many design school faculty hold that no idea is ever perfect 
and can therefore always be further improved. Conversely, in improvisation theater, the words no, but 
 are never to be used during improv work, as they shut down conversation. Performers are instead taught 
to treat each line as a gift and respond with Yes, and to advance the work. Applying these two principles 
in concert creates an approach for critique that encourages discussion and expands ideas with the notion 
that listening to dialogue and adding thoughtful feedback helps advance the work. The language used in 
improv, specifically yes, and highlights the value of carefully considering the language used to describe 
and evaluate work. Start critiques by building on what is succeeding against the brief. Some good words 
for plussing include: likely working  effective paying off  successful positive  achieving strong 
 Some examples: The scale and form of the car are working quite well against the brief  The character 
s silhouette is very effective  I think the smaller props are really successful  In any relationship, 
starting a conversation with You look terrible or That shirt is awful is neither likely to start a good 
conversation nor encourage good will. This does not imply one should avoid delivering critical feedback 
or walk on eggshells around unsuccessful work. Starting with what works will always be the easiest part 
of any critique. Delivering critical feedback is likely less pleasant, but good critiques require clear, 
actionable feedback. (Remember when you were receiving the feedback on your sketches.) By its very nature, 
critique is a very personal discussion. The center of discussion is work developed with significant emotional 
investment by the artist and the feedback being delivered is very subjective. Establishing a conversation 
where work is evaluated against the brief eliminates a potentially toxic dynamic of I or me evaluating 
you. Having established this, work can be evaluated with great precision against the brief, increasing 
the effectiveness of the comments offered. Examples: I think red is the wrong choice for this level 
as opposed to The brief calls for this level to be very dramatic. The red does not quite succeed at that. 
 Instead of a vague statement, I hate that gun design it s terrible, try The silhouette of that gun 
is inconsistent with the character s brief and overall design.  While it is human nature to want to 
help solve problems, the reviewer s obligation in critique is limited to offering observation and feedback. 
There is no need or expectation of offering solutions to a challenge, and it is best if the reviewer 
does not offer any. It is essential that the artist own the work they are doing and they remain invested 
in any potential solution they choose to implement. In many situations, it will be more effective for 
a reviewer to ask questions that lead an artist to a particular point. For example, instead of telling 
an artist, The car should not be blue, I think red would work better, it is likely to be more helpful 
if the reviewer asked the artist, Have you considered a much warmer color for the car? or What other 
colors did you explore for the car, and why did you eliminate them? This approach makes it clear that 
there is a question around a particular aspect without providing the solution. Often it is the case where 
the reviewer knows something is not succeeding but will have difficulty in precisely expressing what 
is not working well. Precise language is the goal, but it is not a requirement. When this happens and 
a questioning approach does not work, borrow a technique from drawing instructors: They tell their students 
to draw what they see, not what they think. Likewise, if the reviewer describes what he sees, not what 
he thinks, it will still be useful. It is rarely useful for a reviewer to speculate on intent or ability. 
If a reviewer is not certain or clear, then it is best to simply explain that. Remember, the role of 
the reviewer is to offer critique, not solutions. Above all, remain considerate in how you deliver feedback, 
and deliver a direct, honest critique without being harsh. Keeping  Things  CoolanEasy Critique is a 
dialogue, often a deeply passion-filled discourse in which the stakes are higher for some people than 
others. Heated, passionate and even contentious debate can be healthy provided the debate is always about 
the work and never about the artist or their intentions. When in doubt, keep discussion centered on what 
is presented and how it relates to the brief. Explaining or talking through ideas can be useful, yet 
having people defending their work is likely a sign of trouble. Blaming (or covering for anyone) is toxic 
and unproductive. Avoid lengthy discussions about the past ( how we got here ) and remain single-minded 
as to what can be improved now and going forward. When in doubt, find ways to bring the discussion back 
the brief. Finally, senior people need to be self-aware that their status comes with implications and 
(often false) assumptions that can be counter-productive to the room. Senior staff must remain acutely 
aware of the implicit weight their comments carry, regardless of intention. Keep feedback related to 
the brief and be: Careful in approach  Constructive in tone  Considered in observation and language 
  FailingFastinCritiqueCulture Building environments that foster innovation and embrace a fail fast 
mentality is certainly in vogue right now. Too often, executives confuse creativity with something that 
just happens organically in environments where things are free to happen. Failing without learning is 
simply wasting time. In reality, most successful creative organizations have developed specific processes 
around their crafts that serve as the underlying foundations for great creative work to happen upon. 
Every organization talks about supporting innovation, open dialogue, etc., yet few actually build an 
atmosphere that continually feeds oxygen to small creative sparks. Developing creative briefs, communication 
skills and feedback loops are the foundation for building an atmosphere that encourages innovation. Converting 
this into a core habit is essential. When supervisors in a studio collectively decide to embrace and 
encourage critique on a regular basis, a culture begins to form. Obvious issues will be the need to find 
a regular time and location for it, and establish what the structure for that studio should be (i.e.; 
how many pieces are reviewed each time, who owns uploading/assembling the materials, how long each person 
has, protocols, etc.). Less-obvious issue will be gaining visible support from supervisors and production 
management. It is essential that all supervisors attend critique (to set the example and demonstrate 
the import of critique), production management does not schedule meetings against critique and, most 
importantly, people are not pulled from critique to finish just this one thing I need. When deploying 
a bottom-up approach, groups will need an ambassador to explain the process and, most importantly, the 
benefits to the executive team (the goal: convert executives into participants). Cultural change is certainly 
easier to implement with executive sponsorship, and nothing beats an executive team that demands critique 
become part of the normal planning and production processes. Aside from the obvious benefit of knowing 
they are scheduled and supported at a studio level, when executives participate, transparency of communication 
is established in the studio and the trust starts to grow. When trust increases in the studio, people 
will find it easier to ask for and provide feedback. Trust improves communication and morale across an 
organization. Suggestion  forStructure Each studio and team must find its own methods and structure 
for conducting critiques. After about two years of experimenting, my art supervisors at Microsoft s now-defunct 
Fasa Studio eventually developed this very effective structure. Duration: 1 hour max.  Content: 3 x 
15-minute-long reviews of specific aspects of the game (particular level, hero characters, vehicles, 
lighting, etc.), presented by the lead for each aspect.  Format: The lead starts with the: o Brief: 
1-2 minute overview of the brief for the work o State: 1-2 minute update as to the condition of the 
work being assessed and what work is NOT being assessed. (i.e. Layout is locked, textures are done, animation 
is temp, and lighting is preliminary. Translation: No comments on animation or lightin,g please.)  
 Review: 10 minute max, presentation of materials and group discussion on the work being reviewed.  
The Wrap: The lead will review the feedback received, followed by these two critical steps: 1. Passes: 
The presenter will repeat the feedback he is NOT going to use and explain why. Arguably the most important 
step in the critique as it publicly affirms that the particular feedback is not being ignored, yet will 
not be acted upon, and the reason(s) for that decision. 2. Takes: The presenter sums up the comments 
they will act on and describe, as necessary, how they plan to approach the revisions.   Agreement: 
The art director or design director agrees or disagrees with the wrap. If in agreement, the next presenter 
will start their session. If there is disagreement, the presenter and the art director will discuss it 
until they have an agreement, and then the presenter will do the wrap one final time to ensure everyone 
is on the same page.  DailiesCritiques,  PlaytestsCritiques Many studios mistakenly believe they hold 
critiques in the form of dailies, rounds, etc. or smoke tests, and play tests. Unquestionably these regular 
exercises and reviews are essential to the business but serve a particular need of the project, as opposed 
to the needs of the people creating the work. Critique is a tool for staff to improve their approach, 
their perception and their own skills. Clearly, that ultimately benefits an organization and its clients; 
our goal is to grow a collective body that advances the quality for all the work that it creates and 
delivers, regardless of client or project. Concluding As budgets get tighter and the costs of failure 
increase, every studio needs to find ways to ensure it delivers the highest quality work in the most 
efficient manner possible. SIGGRAPH is one of many incredible opportunities to leverage research and 
technology to advance computer graphics. Critique is an age-old, fundamental skill that offers studios 
a low-cost, low-risk methodology to leverage its people to raise their craft and the quality of the work 
they collectively deliver. Asset management provides the ability to closely track the lifecycle of each 
digital asset. The cost of tracking each asset and even the ability to monitor the percentage an asset 
is technically complete exists. Despite this, the ability to know if employees and contractors are emotionally 
engaged remains elusive. A transparent feedback cycle ensures junior and mid-level people have a safe, 
structured environment in which they can offer their opinions while providing a unique opportunity to 
learn by example how to better communicate and absorb creative feedback. Likewise, senior staff can continue 
their own development by constructively challenging one another while encouraging debate and divergence 
of opinions. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343499</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>33</pages>
		<display_no>17</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[The hitchhiker's guide to the galaxy of mathematical tools for shape analysis]]></title>
		<page_from>1</page_from>
		<page_to>33</page_to>
		<doi_number>10.1145/2343483.2343499</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343499</url>
		<abstract>
			<par><![CDATA[<p>A practical guide for researchers who are exploring the new frontiers of 3D shape analysis and managing the complex mathematical tools that most methods rely on. Many research solutions come from advances in pure and applied mathematics, as well as from re-reading classical mathematical theories. Managing these math tools is critical to understanding and solving current problems in 3D shape analysis. This course is designed to help mathematicians and scientists communicate in a world where boundaries between disciplines are (fortunately) blurred, so they can quickly find the right mathematical tools for a bright intuitive idea and strike a balance between theoretical rigor and computationally feasible solutions.</p> <p>The course presents basic concepts in differential geometry and proceeds to advanced topics in algebraic topology, always keeping an eye on their computational counterparts. It includes examples of applications to shape correspondence, symmetry detection, and shape retrieval that show how these mathematical concepts can be translated into practical solutions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738932</person_id>
				<author_profile_id><![CDATA[81100196607]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Silvia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Biasotti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNR-IMATI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738933</person_id>
				<author_profile_id><![CDATA[81100540810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bianca]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Falcidieno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNR-IMATI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738934</person_id>
				<author_profile_id><![CDATA[81335491092]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daniela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Giorgi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNR-IMATI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738935</person_id>
				<author_profile_id><![CDATA[81336493063]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michela]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Spagnuolo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CNR-IMATI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Biasotti, S., Falcidieno, B., Frosini, P., Giorgi, D., Landi, C., Marini, S., Patan&#232;, G., and Spagnuolo, M. 2007. 3d shape description and matching based on properties of real functions. In <i>Eurographics 2007 Tutorial Notes</i>, The Eurographics Association, 1025--1074.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1403814</ref_obj_id>
				<ref_obj_pid>1403798</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Biasotti, S., Cerri, A., Frosini, P., Giorgi, D., and Landi, C. 2008. Multidimensional size functions for shape comparison. <i>J. Math. Imaging Vis. 32</i>, 2, 161--179.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1391731</ref_obj_id>
				<ref_obj_pid>1391729</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Biasotti, S., De Floriani, L., Falcidieno, B., Frosini, P., Giorgi, D., Landi, C., Papaleo, L., and Spagnuolo, M. 2008. Describing shapes by geometrical-topological properties of real functions. <i>ACM Computing Surveys 40</i>, 4, 1--87.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1343337</ref_obj_id>
				<ref_obj_pid>1343119</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Biasotti, S., Falcidieno, B., Giorgi, D., and Spagnuolo, M. 2008. Reeb graphs for shape analysis and applications. <i>Theoretical Computer Science 392</i>, 1--3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2039660</ref_obj_id>
				<ref_obj_pid>2039448</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Biasotti, S., Cerri, A., Frosini, P., and Giorgi, D. 2011. A new algorithm for computing the 2-dimensional matching distance between size functions. <i>Pattern Recognition Letters 32</i>, 1735--1746.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1825049</ref_obj_id>
				<ref_obj_pid>1825026</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bronstein, A. M., Bronstein, M. M., Kimmel, R., Mahmoudi, M., and Sapiro, G. 2010. A gromov-hausdorff framework with diffusion geometry for topologically-robust non-rigid shape matching. <i>Intl. Journal of Computer Vision (IJCV) 89</i>, 2--3, 266--286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1998230</ref_obj_id>
				<ref_obj_pid>1998196</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dey, T. K., and Wang, Y. 2011. Reeb graphs: approximation and persistence. In <i>Proceedings of the 27th annual ACM symposium on Computational geometry</i>, ACM, SoCG '11, 226--235.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[do Carmo, M. P. 1976. <i>Differential Geometry of Curves and Surfaces</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Donatini, P., and Frosini, P. 2007. Natural pseudodistances between closed surfaces. <i>Journal of the European Mathematical Society 9</i>, 2, 231--253.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Edelsbrunner, H., and Harer, J. 2008. Persistent homology---a survey. In <i>Surveys on discrete and computational geometry</i>, vol. 453 of <i>Contemp. Math</i>. Amer. Math. Soc., Providence, RI, 257--282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Engelking, R., and Sielucki, K. 1992. <i>Topology: A geometric approach</i>. Heldermann, Berlin.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Fomenko, A. 1995. <i>Visual Geometry and Topology</i>. Springer Verlag.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Frosini, P., and Landi, C. 2001. Size functions and formal series. <i>Applicable Algebra in Engineering, Communication and Computing 12</i>, 327--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Frosini, P., and Mulazzani, M. 1999. Size homotopy groups for computation of natural size distances. <i>Bulletin of the Belgian Mathematical Society 6</i>, 455--464.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Frosini, P. 1990. A distance for similarity classes of submanifolds of a Euclidean space. <i>Bulletin of the Australian Mathematical Society 42</i>, 407--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Frosini, P. 1991. Measuring shapes by size functions. In <i>Intelligent Robots and Computer Vision X: Algorithms and Techniques, Proceedings of SPIE</i>, D. P. Casasent, Ed., vol. 1607, 122--133.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ghrist, R. 2008. Barcodes: The persistent topology of data. <i>Bulletin-American Mathematical Society 45</i>, 1, 61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Griffiths, H. B. 1976. <i>Surfaces</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Gromov, M., Pansu, P., LaFontaine, J., Bates, S., and Semmes, S. 2006. <i>Metric Structures for Riemannian and Non-Riemannian Spaces</i>. Modern Birkh&#228;user Classics. Birkh&#228;user.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Guillemin, V., and Pollack, A. 1974. <i>Differential Topology</i>. Englewood Cliffs, New Jersey.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Hatcher, A. 2001. <i>Algebraic Topology</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383282</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Hilaga, M., Shinagawa, Y., Kohmura, T., and Kunii, T. L. 2001. Topology matching for fully automatic similarity estimation of 3D shapes. In <i>SIGGRAPH '01: Proceedings of the 28</i>&#60;sup&#62;<i>th</i>&#60;/sup&#62; <i>Annual Conference on Computer Graphics and Interactive Techniques</i>, ACM Press, 203--212.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Hirsch, M. W. 1997. <i>Differential Topology</i>. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Jost, J. 2005. <i>Riemannian geometry and geometric analysis; 4th ed</i>. Universitext. Springer, Berlin.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Kim, V., Lipman, Y., Chen, X., and Funkhouser, T. 2010. Mobius transformations for global intrinsic symmetry analysis. <i>Computer Graphics Forum (Symposium on Geometry Processing)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531378</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Lipman, Y., and Funkhouser, T. 2009. Mobius voting for surface correspondence. <i>ACM Transactions on Graphics (SIGGRAPH)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Massey, W. 1967. <i>Algebraic Topology: An Introduction</i>. Brace &amp; World, Inc.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Milnor, J. W. 1963. <i>Morse Theory</i>. Princeton University Press, New Jersey.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Reeb, G. 1946. Sur les points singuliers d'une forme de Pfaff compl&#232;tement int&#233;grable ou d'une fonction num&#233;rique. <i>Comptes Rendus Hebdomadaires des S&#233;ances de l'Acad&#233;mie des Sciences 222</i>, 847--849.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1551491</ref_obj_id>
				<ref_obj_pid>1550980</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Reuter, M., Biasotti, S., Giorgi, D., Patan&#232;, G., and Spagnuolo, M. 2009. Discrete laplace-beltrami operators for shape analysis and segmentation. <i>Computers &amp; Graphics 3</i>, 33, 381--390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617692</ref_obj_id>
				<ref_obj_pid>616019</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Shinagawa, Y., Kunii, T. L., and Kergosien, Y. L. 1991. Surface coding based on Morse theory. <i>IEEE Computer Graphics and Applications 11</i>, 66--78.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1735621</ref_obj_id>
				<ref_obj_pid>1735603</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Sun, J., Ovsjanikov, M., and Guibas, L. 2009. A concise and provably informative multi-scale signature based on heat diffusion. In <i>Proceedings of the Symposium on Geometry Processing</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, SGP '09, 1383--1392.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1395041</ref_obj_id>
				<ref_obj_pid>1395016</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Tangelder, J., and Veltkamp, R. 2008. A survey of content-based 3D shape retrieval methods. <i>Multimedia Tools and Applications 39</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[van Kaick, O., Zhang, H., Hamarneh, G., and Cohen-Or, D. 2011. A survey on shape correspondence. <i>Computer Graphics Forum 30</i>, 6, 1681--1707.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Willard, S. 1970. <i>General topology</i>. Addison-Wesley Publishing Company.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1731190</ref_obj_id>
				<ref_obj_pid>1731106</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Zeng, W., Samaras, D., and Gu, X. 2010. Ricci flow for 3D shape analysis. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 32</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2381114</ref_obj_id>
				<ref_obj_pid>2381112</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[M. Ben-Cen, O. Weber, C. Gotsman. Characterizing shape using conformal factors. In Proceedings EUROGRAPHICS 3DOR, pp. 1--8, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1380384</ref_obj_id>
				<ref_obj_pid>1379924</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[S. Biasotti, D. Giorgi, M. Spagnuolo, B. Falcidieno. Size functions for comparing 3D models, Pattern Recognition 41(9), Elsevier, pp. 2855--2873, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[M. M. Bronstein, A. M. Bronstein, On a relation between shape recognition algorithms based on distributions of distances. Tech. Rep. CIS-2009-14, Dept. of Computer Science, Technion, Israel, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1963100</ref_obj_id>
				<ref_obj_pid>1963053</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[M. M. Bronstein, A. M. Bronstein. "Shape recognition with spectral distances", IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), Vol. 33/5, pp. 1065--1071, May 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1899405</ref_obj_id>
				<ref_obj_pid>1899404</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[A. M. Bronstein, M. M. Bronstein, M. Ovsjanikov, L. J. Guibas. Shape Google: geometric words and expressions for invariant shape retrieval. ACM Trans. Graphics (TOG), Vol. 30/1, pp. 1--20, January 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[M. M. Bronstein, I. Kokkinos. Scale-invariant heat kernel signatures for non-rigid shape recognition. In Proc. CVPR 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[F. Chazal, D. Cohen-Steiner, L. J. Guibas, F. Memoli, S. Y. Oudot. Gromov-Hausdorff stable signatures for shapes using persistence. 28, 5, 1393--1403, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1985089</ref_obj_id>
				<ref_obj_pid>1985024</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[M. F. Demirci, Y. Osmanlioglu, A. Shokoufandeh, S. J. Dickinson: Efficient many-to many feature matching under the <i>L</i>&#60;sub&#62;1&#60;/sub&#62; norm. Computer Vision and Image Understanding 115(7): 976--983, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[T. K. Dey, K. Li, C. Luo, P. Ranjan, I. Safa, and Y. Wang. Persistent heat signature for pose-oblivious matching of incomplete models. Computer Graphics Forum. Vol. 29 (5) (2010), 1545--1554. Special isue of SGP 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner, D. Letscher, A. Zomorodian. Topological persistence and simplification, Discrete Comput. Geom. 28(4), 511--533, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964974</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[V. G. Kim, Y. Lipman, T. Funkhouser. Blended Intrinsic Maps, ACM Transactions on Graphics (Proc. SIGGRAPH), August 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2219983</ref_obj_id>
				<ref_obj_pid>2219116</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[G. Lavou, M. Corsini. A Comparison of Perceptually-Based Metrics for Objective Evaluation of Geometry Processing. IEEE Transactions on Multimedia 12(7), 636--649, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778840</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Y. Lipman, X. Chen, I. Daubechies, T. Funkhouser. Symmetry Factored Embedding and Distance, ACM Transactions on Graphics (SIGGRAPH), August 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1805971</ref_obj_id>
				<ref_obj_pid>1805964</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Y. Lipman, R. Rustamov, T. Funkhouser. Biharmonic Distance, ACM Transactions on Graphics, 29(3), June, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Y. Liu, Y. Fang, K. Ramani. IDSS: deformation invariant signatures for molecular shape comparison. BMC Bioinformatics, 10:15, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2068573</ref_obj_id>
				<ref_obj_pid>2068459</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[Y. Liu, K. Ramani, M. Liu. Computing the Inner Distances of Volumetric Models for Articulated Shape Description with a Visibility Graph. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 23(12), 2538--2544, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1994728</ref_obj_id>
				<ref_obj_pid>1994466</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[D. Macrini, S. J. Dickinson, David J. Fleet, K. Siddiqi. Object categorization using bone graphs. Computer Vision and Image Understanding 115(8): 1187--1206, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1649644</ref_obj_id>
				<ref_obj_pid>1649575</ref_obj_pid>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[M. Reuter, F.-E. Wolter, N. Peinecke. Laplace-Beltrami spectra as "shape-DNA" of surfaces and solids. Computer Aided Design 38, 342--366, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1735607</ref_obj_id>
				<ref_obj_pid>1735603</ref_obj_pid>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[R. Rustamov, Y. Lipman, T. Funkhouser. Interior Distance Using Barycentric Coordinates, Computer Graphics Forum (Symposium on Geometry Processing), July 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1390061</ref_obj_id>
				<ref_obj_pid>1390056</ref_obj_pid>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[K. Siddiqi, J. Zhang, D. Macrini, A. Shokoufandeh, S. Bouix, S. J. Dickinson. Retrieving articulated 3-D models using medial surfaces. Mach. Vis. Appl. 19(4): 261--275, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[J. Sun, X. Chen, T. Funkhouser. Fuzzy Geodesics and Consistent Sparse Correspondences For Deformable Shapes, Computer Graphics Forum (Symposium on Geometry Processing), July 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[J. Tierny, J.-P. Vandeborre, M. Daoudi. Partial 3D shape retrieval by Reeb pattern unfolding. Computer Graphics Forum, Vol 28, 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[V. Guillemin and A. Pollack, <i>Differential Topology</i>, Englewood Cliffs, NJ:Prentice Hall, 1974]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[H. B. Griffiths, <i>Surfaces</i>, Cambridge University Press, 1976]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[R. Engelking and K. Sielucki, <i>Topology: A geometric approach</i>, Sigma series in pure mathematics, Heldermann, Berlin, 1992]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[A. Fomenko, <i>Visual Geometry and Topology</i>, Springer-Verlag, 1995]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[J- Jost, <i>Riemannian geometry and geometric analysis</i>, Universitext, 1979]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[M. P. do Carmo, <i>Differential geometry of curves and surfaces</i>, Englewood Cliffs, NJ:Prentice Hall, 1976]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[M. Hirsch, <i>Differential Topology</i>, Springer Verlag, 1997]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[M. Gromov, <i>Metric structures for Riemannian and Non-Riemannian spaces</i>, Progress in Mathematics 152, 1999]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1825049</ref_obj_id>
				<ref_obj_pid>1825026</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[A. M. Bronstein, M. M. Bronstein, R. Kimmel, M. Mahmoudi, G. Sapiro. <i>A Gromov-Hausdorff Framework with Diffusion Geometry for Topologically-Robust Non-rigid Shape Matching. Int. J. Comput. Vision 89</i>, 2-3, 266--286, 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Kim, V., Lipman, Y., Chen, X., and Funkhouser, T. 2010. Mobius transformations for global intrinsic symmetry analysis. <i>Computer Graphics Forum 29(5)(Symposium on Geometry Processing)</i>, 1689--1700]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531378</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Lipman, Y., and Funkhouser, T. 2009. Mobius voting for surface correspondence. <i>ACM Transactions on Graphics 28(3) (SIGGRAPH)</i>
]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1735621</ref_obj_id>
				<ref_obj_pid>1735603</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[Sun, J., Ovsjanikov, M., and Guibas, L. 2009. A concise and provably informative multi-scale signature based on heat diffusion. In <i>Proc. of the Symp. on Geometry Processing</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, SGP '09,1383--1392]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Van Kaick O., Zhang H., Harmaneh G., Cohen-Or D.. A survey on shape correspondence, Computer Graphics Forum, 30(6):1681--1707, 2011]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[S. Willard, <i>General Topology</i>, Addison-Wesley Publishing Co, 1970]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[R. Engelking and K. Sielucki, <i>Topology: A geometric approach</i>, Sigma series in pure mathematics, Heldermann, Berlin, 1992]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[A. Fomenko, <i>Visual Geometry and Topology</i>, Springer-Verlag, 1995M.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[W. Hirsch, <i>Differential Topology</i>, Springer, 1997]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[H. B. Griffiths, <i>Surfaces</i>, Cambridge University Press, 1976]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[V. Guillemin and A. Pollack, <i>Differential Topology</i>, Englewood Cliffs, NJ:Prentice Hall, 1974]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[W. Massey, <i>Algebraic topology: An Introduction</i>, Brace&World Inc., 1967]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[A. Hatcher, <i>Algebraic Topology</i>, Cambridge University Press, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[J. Milnor, <i>Morse theory</i>, Princeton University Press, New Jersey, 1963]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[C. Kosniowski, <i>A First Course in Algebraic Topology</i>, Cambridge University Press, 1966]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[P. Frosini, M. Mulazzani, <i>Size homotopy groups for computation of natural size distances</i>, Bull. of Belgian Mathematical Society, 6:455--464, 1999]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[P. Donatini, P. Frosini, <i>Natural pseudodistances between closed surfaces</i>, J.l of European Mathematical Society, 9(2):231--253, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[P. Frosini, <i>A distance for similarity classes of submanifolds of a Euclidean space</i>, Bull. Australian Mathematical Society 42, 407--416, 1990]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[P. Frosini, <i>Measuring shapes by size functions</i>, SPIE, Intelligent Robots and Computer Vision X, Vol. 1607, pp. 122--133, 1991]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[P. Frosini and C. Landi, <i>Size functions and formal series</i>, Appl. Algebra Engrg. Comm. Comput., Vol. 12, pp. 327--349, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner, J. Harer. <i>Computational Topology - an Introduction</i>. AMS, I-XII, 1--24 1, 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[H. Edelsbrunner, J. Harer, <i>Persistent homology: a survey</i>. Surveys on discrete and computational geometry, vol. 453 of Contemp. Math AMS, 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[R. Ghrist. Barcodes: the persistent topology of data, Bull. Amer. Math. Soc., 45(1):61--75, 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1403814</ref_obj_id>
				<ref_obj_pid>1403798</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[S Biasotti, A Cerri, P Frosini, D Giorgi, C Landi. <i>Multidimensional size functions for shape comparison</i>, J. of Math. Imaging and Vision 32 (2), 161--179]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2039660</ref_obj_id>
				<ref_obj_pid>2039448</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[S. Biasotti, A. Cerri, P. Frosini, D. Giorgi, <i>A new algorithm for computing the 2-dimensional matching distance between size functions</i>, Patt. Rec. Letters 32 (14), 1735--1746]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[G. Reeb, <i>Sur les point sunguil&#233;rs d'une form del Pfaff compl&#232;tement int&#232;grable ou d'une fonction mun&#232;rique</i>, Comptes Rendu de l'Academie des Sciences, 222:847--849, 1946]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>617692</ref_obj_id>
				<ref_obj_pid>616019</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Y. Shinagawa, T. Kunii, Y. Kergosien, <i>Surface coding based on Morse Theory</i>, IEEE Computer Graphics and Applications, 11(5):66--78, 1991]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383282</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[M. Hilaga, Y. Shinagawa, T. Komura, T. Kunii, <i>Topology matching for fully automatic similarity estimation of 3D shapes</i>, Proc. SIGGRAPH 2001, pp. 203--212, 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1391731</ref_obj_id>
				<ref_obj_pid>1391729</ref_obj_pid>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[S. Biasotti, L. De Floriani, B. Falcidieno, P. Frosini, D. Giorgi, C. Landi, L. Papaleo, M. Spagnuolo, <i>Describing shapes by geometrical-topological properties of real functions</i>, ACM Computing Surveys, 40(4):1--87, 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1343337</ref_obj_id>
				<ref_obj_pid>1343119</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[S. Biasotti, D. Giorgi, M. Spagnuolo, B. Falcidieno, <i>Reeb graphs for shape analysis and applications</i>, Theoretical Computer Science, Elsevier, 392(1-3):5--22, 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1998230</ref_obj_id>
				<ref_obj_pid>1998196</ref_obj_pid>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[T. Dey, Y. Wang. <i>Reeb graphs: approximation and persistence</i>, Proc. Symposium comput. Geom., 226--235, 2011]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Hitchhiker s Guide to the Galaxy of Mathematical Tools for Shape Analysis Silvia Biasotti* Bianca 
Falcidieno Daniela Giorgi Michela Spagnuolo§ CNR-IMATI Abstract This course is meant as a practical 
mathematical guide for researchers and practitioners who are willing to explore the new frontiers of 
3D shape analysis, and thus require to manage the rather complex mathematical tools most methods rely 
on. The target audience includes therefore academia as well as industries or companies active in the 
shape analysis area. The attendees will familiarize with basic concepts in Differential Geometry, and 
proceed to advanced notions of Algebraic Topology, always keeping an eye on computational counterparts. 
The attendees will be shown how these notions can be transferred to practical solutions, through examples 
of applications to shape correspondence, symmetry detection, and shape retrieval. The main reason for 
proposing a comprehensive (yet concise) mathematical guide is that a number of research solutions come 
from advances in pure and applied Mathematics, as well as from the re-reading of classical theories and 
their adaptation to the discrete setting. Being able to manage such complex mathematical tools is key 
to understanding and orienting among the growing number of different proposals. In a world where disciplines 
(fortunately) have blurred boundaries, we also believe this guide will give some advice on how to make 
mathematicians, other scientists and practitioners get along well with each other, that is, how to talk 
to each other and get to understand each other. We hope that, at the end of the course, attendees will 
have an idea on how to .nd the right mathematical tools that match a bright intuitive idea, and how to 
strike a balance between being theoretically rigorous and offering computationally feasible solutions... 
possibly keeping our guide on their desks. The course is structured as a half-day course. We assume the 
participants have basic skills in Geometric Modelling and familiarity with basic concepts in Mathematics. 
1 About the Lecturers Silvia Biasotti: Silvia Biasotti is a researcher at at the Institute of Applied 
Mathematics and Information Technologies (IMATI) of the National Research Council (CNR) of Italy, Research 
Unit of Genoa, where she works in the Shape Modelling group. She got a Laurea degree in Mathematics in 
September 1998 from the University of Genova; in May 2004 she got a a PhD in Mathematics and Applications 
and in April 2008 a PhD in Information and Communication Technologies, both from the University of Genoa. 
She authored more than 80 scienti.c peer-reviewed contributions, is a member of the editorial board of 
ISRN Machine Vision and served in the programme committee of SMI06-SMI11. Her research interests include 
the study of topological-geometrical descriptions of 2D and 3D models and the development of geometric 
reasoning techniques for the extraction of shape features from discrete surface models. In the last years, 
her research interests concerned computational topology techniques for the analysis and structuring of 
geometric information in any dimension, and shape similarity based on similarity between structures. 
She is also the proponent and leader of the CNR activity Topology and Homology for analysing digital 
shapes and teaches the Master course Methods of analysis of discrete surfaces and their applications 
at the Dept. of Mathematics, University of Genoa. Bianca Falcidieno Bianca Falcidieno is a Research Director 
at the Institute of Applied Mathematics and Information Technologies (IMATI) of the National Research 
Council (CNR) of Italy. She is the Responsible for the Genova branch of IMATI (RUOS) and the President 
of the CNR Research Area of Genova. She has been leading and coordinating research at international level 
in advanced and interdisciplinary .elds, including Computational Mathematics, Computer Graphics, Multidimensional 
Media and Knowledge Technologies. She coordinated various international and national projects, including 
the EU Network of Excellence AIM@SHAPE (2004-2008), the EU Coordination Action FOCUS K3D (2008-2010), 
the Italy-Israel project FIRB SHALOM (2006-2009). Bianca Falcidieno is the author of over 200 scienti.c 
refereed papers and books. She was in charge of several international commitments, including editorial 
tasks and the chairing or co-chairing of events such as the IEEE Conference on Shape Modeling International 
(SMI, France 2010) and the Conference on Semantics and digital Media Technology (SAMT, Italy 2007). She 
is the editor in chief of the International Journal of Shape Modelling (World Scienti.c). In her training 
activity, she supervised several researchers, while taking care of the guidance and training of PhD and 
master students, both Italians and foreigners, by teaching courses and supervising theses and doctoral 
activities, both in Italy and abroad, on Applied Mathematics and Information Technologies. For the 80th 
CNR anniversary, she was included in the 12 top-level female researchers in the CNR history. In *e-mail: 
silvia.biasotti@ge.imati.cnr.it e-mail: bianca.falcidieno@ge.imati.cnr.it e-mail: daniela.giorgi@cnr.it, 
currently at CNR-ISTI §e-mail: michela.spagnuolo@ge.imati.cnr.it  2011 Bianca Falcidieno was elected 
to a Fellowship of the EUROGRAPHICS Association in recognition of her scienti.c contribution to the advancement 
of Computer Graphics. Daniela Giorgi: Daniela Giorgi graduated cum laude in Mathematics at the University 
of Bologna in 2002, with a thesis on geometric modelling of curves and surfaces; she then got a PhD in 
Computational Mathematics from the University of Padova in 2006, with a thesis on image and 3D model 
retrieval. She joined the Centre of Excellence ARCES in Bologna and then moved to Genova, where she joined 
the Shape Modeling Group at IMATI-CNR as a researcher. Her distinguishing features are strong mathematical 
expertise (Differential Geometry, Morse theory, Topology) together with in-depth knowledge in ICT and 
computational .elds (Computer Graphics, Image and 3D Processing). Her main research interests concern 
multimedia analysis, description and retrieval. She has been developing . She is the author of over 30 
peer-reviewed international publications in high-level journals, books and conferences, about computational 
geometry and topology tools for shape analysis, description, and retrieval. She participated in several 
national and international research projects, and was in charge of the Watertight Models Track (2007) 
and of the Classi.cation of Watertight Models Track (2008) of the SHREC event (SHape REtrieval Contest). 
She has been teaching BS (Engineering) and Master (Mathematics and its Applications) courses at University; 
she has supervised trainees, undergraduates and master students, thus achieving education and knowledge 
transfer competency. She also was a lecturer at International Schools. Michela Spagnuolo: Michela Spagnuolo 
got a Laurea Degree cum laude in Applied Mathematics from the University of Genova and a PhD in Computer 
Science Engineering from the INSA, Lyon. She is currently a Senior Researcher at CNR-IMATI. She authored 
more than 130 reviewed papers in scienti.c journals and international conferences, edited a book on 3D 
shape analysis, and was a guest-editor of several special issues. She is an associate editor of Computers&#38;Graphics, 
Computer Graphics Forum and The Visual Computer. She is a member of the steering committee of the IEEE 
Shape Modelling International (SMI), and was the programme chair of the EG and ACM workshops on 3D Object 
Retrieval (3DOR) and the International Conference on Semantic and Media Technology (SAMT). Her current 
interests include 3D modelling and visualization, shape analysis techniques, shape similarity and matching, 
and computational topology. She was responsible for several EC and national projects of CNR-IMATI and 
is currently responsible for the research unit on Advanced techniques for the analysis and synthesis 
of multidimensional media and for the research unit on Modeling and analysis techniques, and high performance 
and grid omputing of a CNR Project on Bioinformatics. 2 Course Outline MODULE 1: Moderator: Bianca Falcidieno 
Lecturers: Michela Spagnuolo, Silvia Biasotti. A. Introduction and welcome. (14:00-14:05) Overview of 
the course and motivation.  B. Mathematics and shape analysis challenges. (14.05-14.20) Shape properties 
and invariants;  Similarity between shapes.  C. Mathematical Guide, Part 1. (14.20-15.00)  Topological 
spaces, functions, manifolds, metric spaces;  Isometries, geodesics, curvature, Riemann surfaces, Laplace-Beltrami 
operator;  Gromov-Hausdorff distances.   D. Examples of Applications, Part 1. (15.00-15.30) surface 
correspondence;  symmetry detection;  intrinsic shape description.  Break (15.30-15.40) MODULE 2: 
Moderator: Michela Spagnuolo Lecturers: Daniela Giorgi, Bianca Falcidieno.  E. Mathematical Guide, Part 
2. (15.40-16.20)  Basics on algebraic topology, simplicial Complexes, Homology, surface genus;  Critical 
points, Morse Theory.   F. Examples of Applications, Part 2. (16.20-16.50) Persistent topology;  
Reeb graphs.  G. Conclusions (16.50-17.15) Discussion on recent trends and open issues, supported by 
case studies.  3 Introduction In the last decade we have witnessed great interest and a wealth of promise 
in 3D shape analysis, where the goal is to derive geometric, structural and semantic information about 
3D objects from low-level properties. While the .rst half of the decade can be thought of as the initial 
phase of research, which only laid foundation to such promise, the second half saw a large number of 
new techniques and systems, and got many new people involved. The community has started to reason on 
new challenges, including similarity under deformations other than rigid motions, partial matching, correspondence 
.nding, symmetry detection, view-point selection, semantic annotation and attribute transfer. Lateral 
evolution has also occurred in terms of the associated applied domains, spanning various .elds from Medicine 
to Bioinformatics and Architecture. These new challenges required more elaborate methods: a number of 
interesting solutions came from advances in (pure and applied) Mathe­matics, as well as from the re-reading 
of classical mathematical theories and their adaptation to the discrete setting. Being able to manage 
such complex mathematical tools is key to understanding the most recent research solutions, and orienting 
among the growing number of different proposals. In this scenario, this course is meant as a practical 
guide to familiarize with most of the mathematical concepts and computa­tional tools that are used in 
recent work on the analysis of 3D objects, from basic concepts in Differential Geometry to notions of 
Algebraic Topology. The course includes a summary of the background mathematical notions, a detailed 
presentation of the mathematical methods underlying recent shape analysis works, and examples of applications 
to shape correspondence, symmetry detection, shape comparison and retrieval. 3.1 Overview of the Course 
Material The course is structured as a half-day course. The .st part introduces some of the main challenges 
in shape analysis, underlining the key role that Mathematics plays. Then, the .rst part of the mathematical 
guide is presented, dealing with concepts mainly in Differential Geometry and Topology; examples are 
shown about surface correspondence and symmetry detection, to demonstrate how the surveyed mathematical 
concepts have been exploited in recent research works. In the second part, the mathematical guide is 
completed with advanced concepts in Differential Geometry and Algebraic Topology, whose use is demonstrated 
in shape comparison and retrieval applications. In the concluding part, we will draw some conclusions 
about the use of Mathematics in shape analysis: with the help of case studies, possibly taken from recent 
shape analysis contests (e.g., the SHREC 2012 Track on Stability on Abstract Shapes), we shall reason 
about to what extent it has reached his full potential, and what still has to be done. The course material 
is partly based on previously published papers, talks and lectures by the authors. These include: the 
papers published in ACM Computing Surveys [Biasotti et al. 2008b] and in Theoretical Computer Science 
[Biasotti et al. 2008c] about geometrical-topological tools for shape analysis and description, which 
covered mathematical, computational and applicative aspects, and both received a good appreciation from 
the research community;  the tutorial presented at EUROGRAPHICS 2007 [Biasotti et al. 2007], about shape 
comparison and retrieval methods rooted in Morse Theory;  the MiniSymposium Geometric-topological methods 
for 3D shape classi.cation and matching, held at ICIAM (International Council for Industrial and Applied 
Mathematics) 2007;  lectures given at international schools (AIM@SHAPE International Summer School on 
Computational Methods for Shape Modelling and Analysis -2004; AIM@SHAPE International Summer School on 
Shape Modeling and Reasoning -2007; Utrecht Summer School on Multimedia Retrieval -2007; Seminar on Non-Textual 
Data Searching Systems (http://diuf.unifr.ch/diva/3emeCycle08) -2008) and at national events (DIMA Workshop 
Matematica, Forme, Immagini -2010).  The tutorial will also re.ect the many years experience of organizing 
the EUROGRAPHICS workshop on 3D Object Retrieval (EG 3DOR), and the launching and contributions to the 
SHape REtrieval Contest (SHREC): launched by the AIM@SHAPE project in 2006, SHREC has seen an increasing 
participation of researchers, and evolved into a multi-contest featuring diverse tracks on 3D retrieval, 
correspondence .nding, shape segmentation and related topics (http://www.aimatshape.net/event/SHREC). 
This experience will allow us to demonstrate and benchmark recent results, and not just to describe them 
theoretically. 3.2 Educational Role The notes are mostly aimed at researchers who are willing to explore 
the new frontiers of 3D shape analysis, and thus require to manage the rather complex mathematical tools 
which most methods rely on. We assume that the participants have basic skills in Geometric Modelling, 
and familiarity with basic concepts in Mathematics. The educational target is ambitious, in that it requires 
to strike an happy medium between complex (and vast) mathematical theories, computational aspects, and 
practical issues. Our mission is to offer a comprehensive yet concise mathematical guide, which can help 
a new generation of researchers to truly understand what is behind the most recent solutions in shape 
analysis. Figure 1: Mathematics, shapes, invariants and descriptors. Previous SIGGRAPH courses covered 
topics in Mathematics and Discrete Mathematics (including the 2006 course on Discrete Differential Geometry: 
An applied introduction; Surface Modeling and Parametrization with Manifolds, and Manifolds and modeling 
-2005; Geometric signal processing on large polygonal meshes -2001), but a comprehensive course collecting 
the mathematical background pertaining to different .elds in advanced shape analysis, and spanning from 
basics in Differential Geometry to Algebraic Topology, has not been proposed yet. Moreover, existing 
surveys on shape analysis [Tangelder and Veltkamp 2008; van Kaick et al. 2011] do not cover the Mathematics 
behind the research solutions surveyed. We believe it timely to .ll the gap and visit this complex material, 
with the aim of helping a good understanding of novel, complex research solutions, and their transfer 
into practical applications.  4 Contents In many problems in Computer Graphics, it is convenient to 
model shapes as topological spaces, possibly manifolds; often, shape data are endowed with a notion of 
distance between their points, which turns them, in the language of Differential Geometry, into metric 
spaces. Capturing the information contained in shape data thus typically takes the form of computing 
shape properties, and turning them into invari­ants, or signatures, which provide insights about the 
shape characteristics. Measuring shape properties (distances between points, curvature, etc.) and getting 
invariants is a fundamental problem in Computer Graphics, which has applications to correspondence .nding, 
symmetry detection, and more. A more elaborate question concerns the de.nition of distances between shapes. 
Indeed, one of the cornerstone problems in shape analysis is how to de.ne a notion of shape (dis)similarity; 
that is, we may want to analyze to what extent two spaces represent two instances of some common class, 
up to a certain notion of invariance. Having de.ned a proper notion of distances between shapes, it is 
natural to ask for shape descriptors which are able to signal shape (dis)similarity in accordance with 
this de.nition. This has fundamental applications in shape matching, recognition and retrieval. In what 
follows, we expand on these challenges, point out why (and what) Mathematics is needed to make our way 
through complex shape analysis problems, and list the concepts we will present in our tutorial. 4.1 Computing 
3D shape properties and metric invariants When we think about shape properties, the .rst distinction 
to be made is between extrinsic and intrinsic shape properties. Extrinsic properties are the properties 
related to how the shape is laid out in the Euclidean 3D space. If we model a shape as a metric space, 
its extrinsic properties can be described by using the Euclidean distance between points. Euclidean distances 
form the basis for most of the earliest shape analysis methods in Computer Vision and Computer Graphics. 
At the same time, lately the study of intrinsic properties, that is, properties related to the metric 
structure and invariant to shape deformations, started penetrating into the Vision and Graphics communities. 
The reason is that deformable objects are ubiquitous in our reality, from human organs to living beings. 
If a shape is modeled as a metric space, intrinsic properties can be described using geodesic distances, 
which, on a surface, measure the length of the shortest path along the surface between two points. The 
use of geodesic distances proved effective in a number of studies, and paved the road to a number of 
tools for intrinsic non-rigid shape analysis. Recent developments include the introduction of fuzzy geodesics, 
which relax the notion of shortest path so as to increase robustness; diffusion distances (and related 
notions such as biharmonic distances and the heat kernel), which are related to the physical process 
of heat diffusion on a surface from a source point; inner distances and interior distances to be computed 
on volumes. Concerning surface properties and invariants, a fundamental concept is the Gaussian curvature, 
with the peculiarity that it depends on the metric de.ned on the space (different metrics induce different 
curvatures), whereas the total curvature only depends on the space topology. If we stick to the metric 
space model, we can see how distances between points can originate distances between spaces. Well known 
distances are the Hausdorff distance, which measures how far two subsets of a metric space are from each 
other, and the Wasserstein metric, de.ned between probability distributions on metric spaces. Another 
interesting example is the Gromov-Hausdorff distance, which casts the comparison of two spaces as a problem 
of comparing pairwise distances on the spaces. Equivalently, the computation of the Gromov-Hausdorff 
distance between spaces can be posed as measuring the distortion caused by embedding one metric space 
into another, that is, evaluating how much the metric structure is preserved while mapping a shape into 
the other. By considering different metrics between points, we get different notions of metrics between 
spaces [Gromov et al. 2006; Bronstein et al. 2010]. Mathematics gives the whats and whys. From the mathematical 
point of view, understanding and managing all the concepts listed above require a background in Differential 
Geometry and Topology [do Carmo 1976; Guillemin and Pollack 1974; Hirsch 1997]. We will discover how 
to model a shape as a topological space and a metric space, what (Riemannian) manifolds are useful for, 
the precise de.nitions of widely used terms such as geodesic, isometry, curvature, and how they relate 
to conformal geometry and the highly-cited Laplace-Beltrami operator [Jost 2005; Reuter et al. 2009; 
Zeng et al. 2010]. We will see how these notions are fundamental to analyse shape properties and compute 
shape invariants. Having this background in mind, we will analyze all notions of surface properties and 
metric invariants listed above, from the theoretical and the computational point of view. The how-to 
in applications: surface correspondence, symmetry detection and intrinsic shape description. At this 
point, we will be able to show how the surveyed concepts were applied to solve different problems, namely 
symmetry detection, surface correspondence and intrinsic shape description. Concerning symmetry detection, 
we will refer to [Kim et al. 2010], where geodesics distances and conformal mappings are used to generate 
symmetry invariant point sets and detect surface self-isometries, that is, intrinsic symmetries. Concerning 
surface correspondence, reference works will be [Lipman and Funkhouser 2009], where differential and 
conformal geometry give rise to a voting scheme that identi.es corresponding points which are consistent 
with isometric mappings of large surface regions, and [Sun et al. 2009], where diffusion geometry and 
the Heat Kernel Signature are used to detect repeated structure within the same shape and across a collection 
of shapes. 4.2 The mathematical notion of similarity between shapes, and the role of shape descriptors 
If we push further the idea of measuring the distortion of properties while transforming a shape into 
another, we get the concept behind the Natural pseudo-distance. Let us assume now that a shape is a space 
endowed with a real function, which describes some shape properties. To compare two shapes, we can imagine 
to transform one shape into the other, and check how much the properties of the original shapes have 
been preserved/distorted; this amounts to measure how much the values of the real function representing 
those properties have been altered. The Natural pseudo-distance offers a framework in which we can plug-in 
different properties, in the form of different real functions, so as to measure shape (dis)similarity 
up to different notions of invariance. Having de.ned a proper notion of distances between shapes, the 
problem has been addressed of de.ning shape descriptors which are stable under perturbations of the shape 
de.ned in the distance space. These descriptors include size functions, which have been proven to be 
stable under the natural pseudo-distance, and the family of persistent homology tools. These signatures 
are able to naturally combine the classifying power of topology with the descriptive power of geometry, 
and have a close relation with other popular tools such as Reeb graphs, which have their roots in the 
same theoretical settings. Mathematics gives the whats and whys. At this point, we will need to further 
explore the mysteries of mappings between topological spaces, that is, the notions of homeomorphisms 
and diffeomorphisms between topological spaces [Grif.ths 1976; Fomenko 1995]. Basic notions of Algebraic 
Topology will have to be introduced, starting from the notion of simplicial complexes, and going through 
homology [Willard 1970; Engelking and Sielucki 1992; Massey 1967; Hatcher 2001]. We will see how Morse 
Theory elegantly bridges geometrical properties of shapes with their topology [Milnor 1963; Edelsbrunner 
and Harer 2008]. Having this background in mind, we will show how all these mathematical concepts form 
the basis for the de.nition of distances between shapes (e.g. the Natural pseudo-distance [Frosini and 
Mulazzani 1999; Donatini and Frosini 2007]), and the computation of shape descriptors as those listed 
above (size functions, persistence diagrams, Reeb graphs). The how-to in applications: We will overview 
shape description at the light of the persistent topology framework, with particular attention to persistent 
homology [Edelsbrunner and Harer 2008] and barcodes [Ghrist 2008]. Then, we will introduce size theory 
[Frosini 1990; Frosini 1991; Frosini and Landi 2001] and consider [Biasotti et al. 2008a; Biasotti et 
al. 2011], which use persistent topology and multidimensional size functions for retrieving 3D objects 
in database, according to different similarity criteria and invariance concepts. Finally, we will overview 
the use of Reeb graphs [Reeb 1946] in the shape analysis, description and retrieval arena [Shinagawa 
et al. 1991; Hilaga et al. 2001; Dey and Wang 2011]. 4.3 Conclusions At the end of the course, some 
case studies taken from recent shape analysis contests (e.g., the SHREC 2012 Track on Stability on Abstract 
Shapes) will offer us the possibility of further reasoning on what Mathematics has done and still can 
do for shape analysis. As Mathematicians doing research in a world where disciplines (fortunately) have 
blurred boundaries, we will also try to give some advice on how to make mathematicians and other scientists 
get on well with each other, that is, how to talk to each other and get to understand each other. We 
hope that, at the end of the course, attendees will have an idea on how to .nd the right mathematical 
tools that match a bright intuitive idea, and how to strike a balance between being theoretically rigorous 
and offering computationally feasible solutions... possibly keeping our guide on their desks.  Acknowledgements 
 This work is partially supported by the projects: VISIONAIR: Vision Advanced Infrastructure for Research, 
European project FP7 INFRAS-TRUCTURES, 2011-2015 and MULTISCALEHUMAN: Multi-scale Biological Modalities 
for Physiological Human Articulation, European project FP7 PEOPLE Initial Training Network, 2011-2014. 
In addition, the lectures thank P. Frosini, M. Ferri and the Vision Mathematics group of the Univ. of 
Bologna and C. Landi of the Univ. of Modena and Reggio Emilia for the helpful discussions and hints. 
 References BIASOTTI, S., FALCIDIENO, B., FROSINI, P., GIORGI, D., LANDI, C., MARINI, S., PATAN E`, 
G., AND SPAGNUOLO, M. 2007. 3d shape description and matching based on properties of real functions. 
In Eurographics 2007 Tutorial Notes, The Eurographics Association, 1025 1074. BIASOTTI, S., CERRI, A., 
FROSINI, P., GIORGI, D., AND LANDI, C. 2008. Multidimensional size functions for shape comparison. J. 
Math. Imaging Vis. 32, 2, 161 179. BIASOTTI, S., DE FLORIANI, L., FALCIDIENO, B., FROSINI, P., GIORGI, 
D., LANDI, C., PAPALEO, L., AND SPAGNUOLO, M. 2008. Describing shapes by geometrical-topological properties 
of real functions. ACM Computing Surveys 40, 4, 1 87. BIASOTTI, S., FALCIDIENO, B., GIORGI, D., AND SPAGNUOLO, 
M. 2008. Reeb graphs for shape analysis and applications. Theoretical Computer Science 392, 1-3. BIASOTTI, 
S., CERRI, A., FROSINI, P., AND GIORGI, D. 2011. A new algorithm for computing the 2-dimensional matching 
distance between size functions. Pattern Recognition Letters 32, 1735 1746. BRONSTEIN, A. M., BRONSTEIN, 
M. M., KIMMEL, R., MAHMOUDI, M., AND SAPIRO, G. 2010. A gromov-hausdorff framework with diffusion geometry 
for topologically-robust non-rigid shape matching. Intl. Journal of Computer Vision (IJCV) 89, 2-3, 266 
286. DEY, T. K., AND WANG, Y. 2011. Reeb graphs: approximation and persistence. In Proceedings of the 
27th annual ACM symposium on Computational geometry, ACM, SoCG 11, 226 235. DO CARMO, M. P. 1976. Differential 
Geometry of Curves and Surfaces. Cambridge University Press. DONATINI, P., AND FROSINI, P. 2007. Natural 
pseudodistances between closed surfaces. Journal of the European Mathematical Society 9, 2, 231 253. 
EDELSBRUNNER, H., AND HARER, J. 2008. Persistent homology a survey. In Surveys on discrete and computational 
geometry, vol. 453 of Contemp. Math. Amer. Math. Soc., Providence, RI, 257 282. ENGELKING, R., AND SIELUCKI, 
K. 1992. Topology: A geometric approach. Heldermann, Berlin. FOMENKO, A. 1995. Visual Geometry and Topology. 
Springer Verlag. FROSINI, P., AND LANDI, C. 2001. Size functions and formal series. Applicable Algebra 
in Engineering, Communication and Computing 12, 327 349. FROSINI, P., AND MULAZZANI, M. 1999. Size homotopy 
groups for computation of natural size distances. Bulletin of the Belgian Mathematical Society 6, 455 
464. FROSINI, P. 1990. A distance for similarity classes of submanifolds of a Euclidean space. Bulletin 
of the Australian Mathematical Society 42, 407 416. FROSINI, P. 1991.Measuringshapesbysizefunctions.In 
Intelligent Robots and Computer Vision X: Algorithms and Techniques, Proceedings of SPIE, D. P. Casasent, 
Ed., vol. 1607, 122 133. GHRIST, R. 2008. Barcodes: The persistent topology of data. Bulletin-American 
Mathematical Society 45, 1, 61. GRIFFITHS, H. B. 1976. Surfaces. Cambridge University Press. GROMOV, 
M., PANSU, P., LAFONTAINE, J., BATES, S., AND SEMMES, S. 2006. Metric Structures for Riemannian and Non-Riemannian 
Spaces. Modern Birkh¨auser Classics. Birkh¨auser. GUILLEMIN, V., AND POLLACK, A. 1974. Differential Topology. 
Englewood Cliffs, New Jersey. HATCHER, A. 2001. Algebraic Topology. Cambridge University Press. HILAGA, 
M., SHINAGAWA, Y., KOHMURA, T., AND KUNII, T. L. 2001. Topology matching for fully automatic similarity 
estimation of 3D shapes. In SIGGRAPH 01: Proceedings of the 28th Annual Conference on Computer Graphics 
and Interactive Techniques, ACM Press, 203 212. HIRSCH, M. W. 1997. Differential Topology. Springer. 
JOST, J. 2005. Riemannian geometry and geometric analysis; 4th ed. Universitext. Springer, Berlin. KIM, 
V., LIPMAN, Y., CHEN, X., AND FUNKHOUSER, T. 2010. Mobius transformations for global intrinsic symmetry 
analysis. Computer Graphics Forum (Symposium on Geometry Processing). LIPMAN, Y., AND FUNKHOUSER, T. 
2009. Mobius voting for surface correspondence. ACM Transactions on Graphics (SIGGRAPH). MASSEY, W. 1967. 
Algebraic Topology: An Introduction. Brace &#38; World, Inc. MILNOR, J. W. 1963. Morse Theory. Princeton 
University Press, New Jersey. REEB, G. 1946. Sur les points singuliers d une forme de Pfaff compl`egrable 
ou d une fonction num´ etement int´erique. Comptes Rendus Hebdomadaires des S´eances de l Acad´emie des 
Sciences 222, 847 849. REUTER, M., BIASOTTI, S., GIORGI, D., PATAN E`, G., AND SPAGNUOLO,M. 2009. Discretelaplace-beltramioperatorsforshapeanalysis 
and segmentation. Computers &#38; Graphics 3, 33, 381 390. SHINAGAWA, Y., KUNII, T. L., AND KERGOSIEN, 
Y. L. 1991. Surface coding based on Morse theory. IEEE Computer Graphics and Applications 11, 66 78. 
SUN, J., OVSJANIKOV, M., AND GUIBAS, L. 2009. A concise and provably informative multi-scale signature 
based on heat diffusion. In Proceedings of the Symposium on Geometry Processing, Eurographics Association, 
Aire-la-Ville, Switzerland, Switzerland, SGP 09, 1383 1392. TANGELDER, J., AND VELTKAMP, R. 2008. A survey 
of content-based 3D shape retrieval methods. Multimedia Tools and Applications 39. VAN KAICK, O., ZHANG, 
H., HAMARNEH, G., AND COHEN-OR, D. 2011. A survey on shape correspondence. Computer Graphics Forum 30, 
6, 1681 1707. WILLARD, S. 1970. General topology. Addison-Wesley Publishing Company. ZENG, W., SAMARAS, 
D., AND GU, X. 2010. Ricci .ow for 3D shape analysis. IEEE Transactions on Pattern Analysis and Machine 
Intelligence 32. Further Readings M. Ben-Cen, O. Weber, C. Gotsman. Characterizing shape using conformal 
factors. In Proceedings EUROGRAPHICS 3DOR, pp. 1-8, 2008. S. Biasotti, D. Giorgi, M. Spagnuolo, B. Falcidieno. 
Size functions for comparing 3D models, Pattern Recognition 41(9), Elsevier, pp. 2855-2873, 2008. M. 
M. Bronstein, A. M. Bronstein, On a relation between shape recognition algorithms based on distributions 
of distances. Tech. Rep. CIS-2009-14, Dept. of Computer Science, Technion, Israel, 2009. M. M. Bronstein, 
A. M. Bronstein. Shape recognition with spectral distances , IEEE Trans. Pattern Analysis and Machine 
Intelligence (PAMI), Vol. 33/5, pp. 1065-1071, May 2011. A. M. Bronstein, M. M. Bronstein, M. Ovsjanikov, 
L. J. Guibas. Shape Google: geometric words and expressions for invariant shape retrieval. ACM Trans. 
Graphics (TOG), Vol. 30/1, pp. 1-20, January 2011. M. M. Bronstein, I. Kokkinos. Scale-invariant heat 
kernel signatures for non-rigid shape recognition. In Proc. CVPR 2010. F. Chazal, D. Cohen-Steiner, L. 
J. Guibas, F. Memoli, S. Y. Oudot. Gromov-Hausdorff stable signatures for shapes using persistence. 28, 
5, 1393-1403, 2009. M. F. Demirci, Y. Osmanlioglu, A. Shokoufandeh, S. J. Dickinson: Ef.cient many-to 
many feature matching under the L1 norm. Computer Vision and Image Understanding 115(7): 976-983, 2011. 
T. K. Dey, K. Li, C. Luo, P. Ranjan, I. Safa, and Y. Wang. Persistent heat signature for pose-oblivious 
matching of incomplete models. Computer Graphics Forum. Vol. 29 (5) (2010), 1545 1554. Special isue of 
SGP 2010. H. Edelsbrunner, D. Letscher, A. Zomorodian. Topological persistence and simpli.cation, Discrete 
Comput. Geom. 28(4), 511-533, 2002. V. G. Kim, Y. Lipman, T. Funkhouser. Blended Intrinsic Maps, ACM 
Transactions on Graphics (Proc. SIGGRAPH), August 2011. G. Lavou, M. Corsini. A Comparison of Perceptually-Based 
Metrics for Objective Evaluation of Geometry Processing. IEEE Transactions on Multimedia 12(7), 636-649, 
2010. Y. Lipman, X. Chen, I. Daubechies, T. Funkhouser. Symmetry Factored Embedding and Distance, ACM 
Transactions on Graphics (SIG-GRAPH), August 2010. Y. Lipman, R. Rustamov, T. Funkhouser. Biharmonic 
Distance, ACM Transactions on Graphics, 29(3), June, 2010. Y. Liu , Y. Fang, K. Ramani. IDSS: deformation 
invariant signatures for molecular shape comparison. BMC Bioinformatics, 10:15, 2009. Y. Liu, K. Ramani, 
M. Liu. Computing the Inner Distances of Volumetric Models for Articulated Shape Description with a Visibility 
Graph. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 23(12), 2538-2544, 2011. 
D. Macrini, S. J. Dickinson, David J. Fleet, K. Siddiqi. Object categorization using bone graphs. Computer 
Vision and Image Understanding 115(8): 1187-1206, 2011. M. Reuter, F.-E. Wolter, N. Peinecke. Laplace-Beltrami 
spectra as shape-DNA of surfaces and solids. Computer Aided Design 38, 342-366, 2006. R. Rustamov, Y. 
Lipman, T. Funkhouser. Interior Distance Using Barycentric Coordinates, Computer Graphics Forum (Symposium 
on Geometry Processing), July 2009. K. Siddiqi, J. Zhang, D. Macrini, A. Shokoufandeh, S. Bouix, S. J. 
Dickinson. Retrieving articulated 3-D models using medial surfaces. Mach. Vis. Appl. 19(4): 261-275, 
2008. J. Sun, X. Chen, T. Funkhouser. Fuzzy Geodesics and Consistent Sparse Correspondences For Deformable 
Shapes, Computer Graphics Forum (Symposium on Geometry Processing), July 2010. J. Tierny, J.-P. Vandeborre, 
M. Daoudi. Partial 3D shape retrieval by Reeb pattern unfolding. Computer Graphics Forum, Vol 28, 2009 
 outline motivation mathematics and shape analysis challenges shape properties and invariants similarity 
between shapes mathematical guide (Part 1) topological spaces, functions, manifolds metric spaces, 
isometries, geodesics, curvature Gromov-Hausdorff distance concepts in action (Part 1) symmetry detection 
 surface correspondence shape characterization 05/08/2012 Overview 2 outline mathematical guide (Part 
2) simplicial complexes basics on algebraic topology and homology Morse theory natural pseudo-distance 
 concepts in Action (Part 2) persistent topology Reeb graphs discussions and trends conclusions Overview 
305/08/2012   what does shape mean?  all the geometrical information that remains when location, scale, 
and rotational effects (Euclidean transformations) are filtered out from an object [Kendall 1977] uhmmm 
NOT sure about this Overview 805/08/2012  shape, similarity &#38; the observer things possess a shape 
for the observer, in whose mind the association between the perception and the existing conceptual models 
takes place [Koenderink 1990] similarity is a cognitive process, depending on the observer and the context 
Overview 1005/08/2012   intuition vs mathematics similarity: two geometrical objects are called similar 
if one can be obtained by the other by uniform stretching . Formally, a similarity of a Euclidean space 
.. is a function ..: .. -> .. that multiplies all distances by the same positive scalar r, so that: .. 
.. .. , .. .. = .... .., .. , .x, y . .. Overview 17 X 05/08/2012 intuition vs mathematics an affine 
transformation is a deformation that map straight lines into straight lines it doesn't respect lengths 
or angles it preserves all affine combinations (i.e., linear combinations in which the sum of the coefficients 
is 1) Overview 18 X 05/08/2012 mathematics: shape description and similarity similar shapes with respect 
to what? shape descriptions, to code the aspects of shapes to be taken into account and manage the complexity 
of the problem similarity in what sense ? transformations among the shapes that we consider irrelevant 
to the assessment of the similarity invariants or properties Overview 1905/08/2012   things are not 
that easy the simple examples we have shown are clearly not enough to deal with the complexity at hand 
this is where Mathematics comes into play! what tools? topological spaces Riemannian surfaces and 
metrics distances and measures algebraic topology differential geometry and topology Morse theory 
Overview 2605/08/2012  content mathematical concepts topological spaces homeo-and diffeomorphisms manifolds 
metric spaces geodesic distances Riemannian surfaces curvature Laplace-Beltrami operator Gromov-Hausdorff 
distance concepts in action surface correspondence symmetry detection intrinsic shape description mathematical 
guide -part I 205/08/2012 content mathematical concepts topological spaces homeo-and diffeomorphisms 
manifolds metric spaces geodesic distances Riemannian surfaces curvature Laplace-Beltrami operator Gromov-Hausdorff 
distance concepts in action surface correspondence symmetry detection intrinsic shape description mathematical 
guide -part I 305/08/2012  continuous function let .. Y topological spaces an arbitrary subset of .. 
 . . .. . is continuous if for every open set . . . the inverse image ...... is an open subset of . 
why functions? to characterize shapes to measure shape properties to model what the observer is looking 
at to reason about stability to define relationships (e.g., distances) mathematical guide -part I 505/08/2012 
  manifold manifold without boundary a topological Haussdorff space . is called a k­dimensional topological 
manifold if each point ... admits a neighborhood .... homeomorphic to the open disk .. . .... . . .. 
and . . . ..... manifold with boundary a topological Haussdorff space S is called a k­dimensional topological 
manifold with boundary if each point ... admits a neighborhood .... homeomorphic either to the open disk 
.. . .... . . .. or the open half-space .... . .. .. . .... and . . . ..... k is called the dimension 
of the manifold mathematical guide -part I 805/08/2012  examples 3-manifolds with boundary: a solid 
sphere, a solid torus, a solid knot 2-manifolds: a sphere, a torus 2-manifold with boundary: a sphere 
with 2 holes, single-valued functions (scalar fields) 1 manifold: a circle, a line mathematical guide 
-part I 1005/08/2012  Riemannian surfaces a conformal structure is an atlas of the surface such that 
angles among tangent vectors can be coherently defined on different local coordinate systems a surface 
with a conformal structure is called a Riemann surface a Riemannian surface carries the structure of 
a metric space whose distance function is the geodesic distance (uniformization) any simply connected 
Riemann surface is either conformally equivalent to: the open unit disk the complex plane the Riemann 
sphere mathematical guide -part I 1505/08/2012 Riemannian surfaces a Riemann surface is a complex manifold 
of complex dimension one a 2-manifold (real) can be turned into a Riemannian surface iff it is orientable 
and metrizable as a consequence Mobius strip, Klein bottle, projective plan don t admit a conformal 
structure mathematical guide -part I 1605/08/2012 invariance and isometries a property invariant under 
isometries with respect to a Riemannian metric is called an intrinsic property examples: the first 
fundamental form the Gaussian curvature . the geodesic distance the Laplacian operator mathematical 
guide -part I 1705/08/2012  Gaussian and mean curvature given .. and .. the principal curvatures at 
a point surface Gaussian curvature . . .... mean curvature . . ... . ..... according to the behavior 
of the sign of ., the points of a surface may be classified as elliptic hyperbolic parabolic or planar 
mathematical guide -part I 1905/08/2012  discrete Laplacian operator index set of 1-ring of vertex 
function value at vertex mass associated with vertex edge weights 05/08/2012 mathematical guide -part 
I 22 discrete geometric Laplacian Desbrun et al. (1999) the cotangent weights take into account the 
angles opposite to edges, the masses take into account the area around vertices Meyer et al. (2002) 
 cotangent weights, masses considering the Voronoi area Belkin et al. (2003, 2008) weights constructed 
using heat kernels Reuter et at. (2005, 2006) weak formulation of the eigenvalue problem with .. cubic 
form functions mathematical guide -part I 2305/08/2012 metrics between spaces from distances between 
points to distances between metric spaces the Gromov-Hausdorff distance poses the comparison of two 
spaces as the direct comparison of pairwise distances on the spaces; equivalently, it measures the distortion 
of embedding one metric space into another mathematical guide -part I 24 p q 05/08/2012  properties 
 the Gromov-Hausdorff distance is parametric with respect to the choice of metrics on the spaces . and 
. common choices Euclidean distance (estrinsic geometry) geodesic distance (intrinsic geometry) or, 
alternatively, diffusion distance .. ... .. . . . ......... . . . ... ....... where .... ... is the eigensystem 
of the Laplacian operator and . is time mathematical guide -part I 2605/08/2012 references V. Guillemin 
and A. Pollack, Differential Topology, Englewood Cliffs, NJ:Prentice Hall, 1974 H. B. Griffiths, Surfaces, 
Cambridge University Press, 1976 R. Engelking and K. Sielucki, Topology: A geometric approach, Sigma 
series in pure mathematics, Heldermann, Berlin, 1992 A. Fomenko, Visual Geometry and Topology, Springer-Verlag, 
1995 J-Jost, Riemannian geometry and geometric analysis, Universitext, 1979 M. P. do Carmo, Differential 
geometry of curves and surfaces, Englewood Cliffs, NJ:Prentice Hall, 1976 M. Hirsch, Differential Topology, 
Springer Verlag, 1997 M. Gromov, Metric structures for Riemannian and Non-Riemannian spaces, Progress 
in Mathematics 152, 1999 A. M. Bronstein, M. M. Bronstein, R. Kimmel, M. Mahmoudi, G. Sapiro. A Gromov-Hausdorff 
Framework with Diffusion Geometry for Topologically-Robust Non-rigid Shape Matching. Int. J. Comput. 
Vision 89, 2-3, 266-286, 2010 mathematical guide -part I 2705/08/2012 content mathematical concepts 
topological spaces homeo-and diffeomorphisms manifolds and surfaces metric spaces geodesic distances 
Riemannian surfaces curvature Laplace-Beltrami operator Gromov-Hausdorff distance concepts in action 
surface correspondence symmetry detection intrinsic shape description mathematical guide -part I 2805/08/2012 
 intrinsic correspondence [Lipman and Funkhouser 2009] looking for intrinsic correspondence means finding 
corresponding points such that the mapping between them is close to an isometry idea: mathematical guide 
-part I 30 any genus zero surface can be mapped conformally to the unit sphere isometry 1-1 and onto 
conformal map of a sphere to itself (Mobius map): uniquely defined by three corresponding points 05/08/2012 
 intrinsic correspondence [Lipman and Funkhouser 2009] mathematical guide -part I 31 1) sampling points: 
local maxima of Gauss curvature &#38; (geodesically) farthest point algorithm 2) discrete conformal flattening 
to the extended complex plane 3) compute the Mobius transformation that aligns a triplet in the common 
domain 4) evaluate the intrinsic deformation error and build a fuzzy correspondence matrix 05/08/2012 
 intrinsic correspondence [Lipman and Funkhouser 2009] pay attention to what about higher genus surfaces? 
 drawbacks of the discrete (linear) flattening technique mathematical guide -part I 32 5) produce a discrete 
set of correspondences 05/08/2012  global intrinsic symmetry detection [Kim et al. 2010] looking for 
intrinsic symmetry transformations means finding isometric transformations that map a surface onto itself 
(self-isometries); cf. the correspondence finding problem we have: M orientable, genus zero surface 
 we look for: f: M . M intrinsic symmetry orientation-preserving isometries are related to conformal 
maps; orientation-reversing isometries are related to anti-conformal maps ideas similar to the work 
just seen: here the anti-Mobius group (Mobius maps plus Mobius maps composed with a reflection) comes 
into play mathematical guide -part I 3405/08/2012 global intrinsic symmetry detection [Kim et al. 2010] 
1) sampling points: symmetry invariant sets S1 (coarse) and S2 (dense) mathematical guide -part I 35 
 IDEA: Given a symmetry invariant function F: M . R, the set S of its critical points is a symmetry invariant 
set that satisfies f(S) = S IDEA: The Average Geodesic Distance (AGD) and the Minimal Geodesic Distance 
(MGD) are symmetry invariant functions 05/08/2012 global intrinsic symmetry detection [Kim et al. 2010] 
1) sampling points: symmetry invariant sets S1 (coarse) and S2 (dense) mathematical guide -part I 36 
2) discrete conformal flattening to the extended complex plane 3) compute anti-Mobius transformations 
that align triplets and quadruplets of S1 in the common domain 4) apply th transformations to S2 and 
evaluate the intrinsic deformation error (based on geodesics) 5) use the best transformation to extract 
correspondences within the symmetry invariant set S2 pay attention to symmetry invariant functions have 
to be smooth! what about partial and small symmetries? 05/08/2012  intrinsic shape description [Sun 
et al. 2009] heat equation, governing the distribution of heat from a source on a surface .; initial 
conditions: heat distribution at time . . . : the heat kernel is a fundamental solution of the heat 
equation with point heat source at . (and heat value at . after time .) mathematical guide -part I 3805/08/2012 
 intrinsic shape description [Sun et al. 2009] heat kernel signature as a point description over the 
temporal domain: ... . . .. . .. ... .. . . .. .. . multiscale, informative, intrinsic, localized sensitivity 
to topological noise distance between signatures at scale ... .. . mathematical guide -part I 40 Courtesy 
of Michael Bronstein 05/08/2012  references Kim, V., Lipman, Y., Chen, X., and Funkhouser, T. 2010. 
Mobius transformations for global intrinsic symmetry analysis. Computer Graphics Forum 29(5)(Symposium 
on Geometry Processing), 1689-1700 Lipman, Y., and Funkhouser, T. 2009. Mobius voting for surface correspondence. 
ACM Transactions on Graphics 28(3) (SIGGRAPH) Sun, J., Ovsjanikov, M., and Guibas, L. 2009. A concise 
and provably informative multi-scale signature based on heat diffusion. In Proc. of the Symp. on Geometry 
Processing, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, SGP 09,1383 1392 Van 
Kaick O., Zhang H., Harmaneh G., Cohen-Or D.. A survey on shape correspondence, Computer Graphics Forum, 
30(6):1681-1707, 2011 mathematical guide -part I 4205/08/2012 content mathematical concepts basics on 
algebraic topology simplicial complexes homology surface genus critical points Morse theory concepts 
in action persistent topology Reeb graphs mathematical guide -part II 105/08/2012 content mathematical 
concepts basics on algebraic topology simplicial complexes homology surface genus critical points Morse 
theory concepts in action persistent topology Reeb graphs mathematical guide -part II 205/08/2012  mathematical 
guide -part II 4 basics on algebraic topology algebraic topology associates algebraic invariants to 
each space so that two spaces are homeomorphic if they have the same invariants approach: to decompose 
a topological space into simple pieces that are easier to study (e.g. to decompose a polyhedron into 
faces, edges, vertices or a surface into triangles) 05/08/2012  mathematical guide -part II 6 basics 
on algebraic topology a combinatorial structure is generated by the decomposition of the topological 
space basic elements of the decomposition are cells or simplices that are characterized by combinatorial 
aspect: relations among the cells of the complex geometric aspect: related to their embedding in the 
Euclidean space 05/08/2012 mathematical guide -part II 7 examples cells complexes a geographic map 
(which is made of points, lines and regions) a decomposition of a polyhedron into faces simplicial 
complexes e.g., triangle meshes 05/08/2012 simplex a .-simplex in a Euclidean space .., with . = ., 
is the convex hull of . . . affinely independent points. A subset of these points defines a simplex of 
dimension . . called face. in ..: mathematical guide -part II 8 0-simplex 1-simplex 2-simplex 4-simplex 
05/08/2012  loops on a surface a loop (1-cycle) is a closed curve whose initial and final points coincide 
in a fixed point . known as the basepoint p 05/08/2012 mathematical guide -part II 11 simplicial homology 
 the .-th simplicial homology group of ., ..(.), is the quotient group of cycles modulo boundaries an 
element of ..(.) is an equivalence class, called homology class, of homologous .-cycles, that is, cycles 
whose difference is a boundary the rank of ..(.) is called the .-th Betti number of ., and it is a measurement 
of the number of different holes in . for 3D data the three Betti numbers .., .., and .. count the number 
of connected components, tunnels, and voids, respectively the homology ..(.) is a topological invariant 
of K mathematical guide -part II 1205/08/2012 genus the genus . of a surface . without boundary is: 
 half the first Betti number of . the cardinality of a minimal set of mutually non­isotopic loops which 
can be cut along the surface without disconnecting it any orientable surface without boundary is a connected 
sum of . tori, where . is its genus, ... the genus of a surface with boundary is the genus of the surface 
.. obtained by gluing a disc onto each boundary component the genus of a surface is a topological invariant 
mathematical guide -part II 1305/08/2012  critical points mathematical guide -part II 15 formally: 
 a point P is critical for f if: It is Morse if: If . is a non-degenerate critical point of ., the 
number . of negative eigenvalues of . is called the index of . 05/08/2012  Euler formula ....... . 
........ . ....... . .(.) (differential geometry) .. (. . . . .) . .(.) (differential topology) . . 
. . . . .(.) . . . .. (algebraic topology) mathematical guide -part II 1705/08/2012 Morse theory a 
function . is called Morse if all of its critical points are non-degenerate Morse theory studies the 
relationship between a function s critical points and the topology of its domain it indicates when the 
topological type changes and what kind of changes take place it provides a surface decomposition into 
a limited set of primitive topological cells, defined by the surface critical points and their corresponding 
index mathematical guide -part II 1805/08/2012 does any Morse function exist? on any smooth compact 
manifold there exist Morse functions Morse functions are everywhere dense in the space of all smooth 
functions on the manifold any Morse function has only a finite number of critical points on a compact 
manifold the set S of all simple Morse functions is everywhere dense in the set of all Morse functions 
 examples of Morse functions on a smooth manifold: height function, distance functions, geodesic distance, 
etc. mathematical guide -part II 1905/08/2012 Morse theory &#38; critical points Let .. = #{critical 
points of index .} and .. the .-th Betti number of .; then Weak Morse inequalities . .. . .. . . (..).... 
= . (..).... = . . Strong Morse inequalities . ...., .. . .... . . . .. . .. . .... . . . .. ....... 
. ........ . ....... . .(.) . . . .. mathematical guide -part II 2005/08/2012 Morse theory &#38; critical 
point configuration (Morse Lemma) In a neighbourhood of each non-degenerate critical point ., the function 
. can be expressed as: where . is the index of the critical point mathematical guide -part II 2105/08/2012 
  minimum 3-manifolds maximum .=0 saddle .=3 saddle 05/08/2012 .=1 mathematical guide -part II .=2 
23 Morse theory &#38; critical points let .. ... be a real valued function and let }.. .~ . . be an 
interval non containing critical values of .; then the level sets ...(.) and ...(.) are diffeomorphic 
 denote .. . . . . . . = . and . a critical point such that .(.) . .; then: .. . . such that ... }. . 
.. . . .~ contains no other critical points of ., the set .... has the homotopy type of .... with a .-cell 
attached mathematical guide -part II 2405/08/2012 Morse theory &#38; shape decomposition Theorem (CW 
complex decomposition) let . be a smooth compact manifold embedded in an Euclidean space. Let .. ... 
be a smooth, real valued, Morse function on .. Then . is homeomorphic (i.e. topologically equivalent) 
to a cell complex of dimension . for each critical point of index . mathematical guide -part II 2505/08/2012 
  homology, Morse theory, and shape description medial axis transform (Blum 1967) shock graphs (Kimia,Tannenbaum, 
Zucker 1995) surface networks (Pfaltz 1976) skeletons and centerlines (Sethian 1985, Bloomenthal 1991) 
 apparent contours (Haefliger 1960, Pignoni 1991) size functions (Ferri, Frosini 1990) barcodes (Zomorodian 
et al 2004) Reeb graphs (Reeb 1946) &#38; contour trees (Boyell &#38; Ruston1963) 05/08/2012 mathematical 
guide -part II 28 about distances (again) to assess how far two perceptions are a notion of metric 
between topological spaces equipped with functions is needed natural pseudo-distance: shapes are similar 
if there exists a homeomorphism between the spaces that preserves the properties conveyed by the functions 
mathematical guide -part II 2905/08/2012 natural pseudo-distance let . be a (subset of) the set of 
all homeomorphisms .. . . ., the natural size pseudodistance .((.. .). (.. .)) is . .. . . .. . . . 
. . ....... . . . . . ... . . . . is small iff .. that induces a small change on the function f, that 
is, if there exist an homeomorphism mapping one space into the other while preserving the properties 
conveyed by the real function how to compute it? Stay tuned... mathematical guide -part II 3005/08/2012 
 references S. Willard, General Topology, Addison-Wesley Publishing Co, 1970 R. Engelking and K. Sielucki, 
Topology: A geometric approach, Sigma series in pure mathematics, Heldermann, Berlin, 1992 A. Fomenko, 
Visual Geometry and Topology, Springer-Verlag, 1995M. W. Hirsch, Differential Topology, Springer, 1997 
 H. B. Griffiths, Surfaces, Cambridge University Press, 1976 V. Guillemin and A. Pollack, Differential 
Topology, Englewood Cliffs, NJ:Prentice Hall, 1974 W. Massey, Algebraic topology: An Introduction, Brace&#38;World 
Inc., 1967 A. Hatcher, Algebraic Topology, Cambridge University Press, 2001 J. Milnor, Morse theory, 
Princeton University Press, New Jersey, 1963 C. Kosniowski, A First Course in Algebraic Topology, Cambridge 
University Press, 1966 P. Frosini, M. Mulazzani, Size homotopy groups for computation of natural size 
distances, Bull. of Belgian Mathematical Society, 6:455­464, 1999 P. Donatini, P. Frosini, Natural pseudodistances 
between closed surfaces, J.l of European Mathematical Society, 9(2):231 253, 2007 mathematical guide 
-part II 3105/08/2012 content mathematical concepts basics on algebraic topology simplicial complexes 
loops on a surface homology surface genus critical points Morse theory concepts in action persistent 
topology Reeb graphs mathematical guide -part II 3205/08/2012 Morse Theory and computational topology 
a shape is a pair (.. .), that is, a topological space X endowed with a real function .. . . . describing 
its properties X 05/08/2012 mathematical guide -part II 33  persistent topology topological events 
[e.g. birth, merge of connected components] occur while sweeping . through ., i.e., when analysing X 
as the evolution of its sub­level sets .. . {. . .. .(.) . . |. . . . mathematical guide -part II 35 
basic idea: encode the lifespan of topological events; lifespan is proportional to the importance of 
the features they represent: long events stand for significant features, short events stand for either 
details or noise 05/08/2012 an historical perspective different topological features: size functions: 
[Frosini 1991], 0-th degree homology (connected components) persistent homology: [Edelsbrunner et al. 
2000], higher degree homology (cycles) from 1-dimensional to multidimensional properties: 1-dimensional 
setting: .. . . . [Frosini 1991, Edelsbrunner et al. 2000] multidimensional setting: ... . . .. [Biasotti 
et al. 2008] mathematical guide -part II 3605/08/2012 an historical perspective different topological 
features: size functions: [Frosini 1991], 0-th degree homology (connected components) persistent homology: 
[Edelsbrunner et al. 2000], higher degree homology (cycles) from 1-dimensional to multidimensional properties: 
 1-dimensional setting: .. . . . [Frosini 1991, Edelsbrunner et al. 2000] multidimensional setting: 
... . . .. [Biasotti et al. 2008] mathematical guide -part II 3705/08/2012  persistent homology (barcodes) 
 given the pair (.. .), consider the collection of nested lower level sets of .. .(. . .) . {. . . . 
.(.) . .|. measure the scale at which a topological feature (e.g., a connected component, a tunnel, 
a void) is created, and when it is annihilated along this filtration using homology groups encode this 
information as parametrized Betti numbers mathematical guide -part II 3905/08/2012     05/08/2012 
mathematical guide -part II 52 RG properties it provides a 1D structure of the shape it describes the 
shape of an object under the lens of the function f nodes and arcs depend on the number of critical 
points of f it may be computed in operations 05/08/2012 mathematical guide -part II 53 Reeb graph based 
representations Reeb graph variations contour trees (simply-connected domains) component trees (gray-level 
images) centerline skeletons (geodesic distance from a point) for shape matching Multiresolution Reeb 
graph (MRG), Hilaga et al. 2001 augmented Multiresolution Reeb graph (aMRG), Tung&#38;Schmitt 2005 
Extended Reeb graph (ERG), Biasotti et al. 2000  references G. Reeb, Sur les point sunguilèrs d une 
form del Pfaff complètement intégrable ou d une fonction munèrique, Comptes Rendu de l Academie des Sciences, 
222:847-849, 1946 Y. Shinagawa, T. Kunii, Y. Kergosien, Surface coding based on Morse Theory, IEEE Computer 
Graphics and Applications, 11(5):66-78, 1991 M. Hilaga, Y. Shinagawa, T. Komura, T. Kunii, Topology 
matching for fully automatic similarity estimation of 3D shapes, Proc. SIGGRAPH 2001, pp. 203-212, 2001 
 S. Biasotti, L. De Floriani, B. Falcidieno, P. Frosini, D. Giorgi, C. Landi, L. Papaleo, M. Spagnuolo, 
Describing shapes by geometrical-topological properties of real functions, ACM Computing Surveys, 40(4):1-87, 
2008 S. Biasotti, D. Giorgi, M. Spagnuolo, B. Falcidieno, Reeb graphs for shape analysis and applications, 
Theoretical Computer Science, Elsevier, 392(1-3):5-22, 2008 T. Dey, Y. Wang. Reeb graphs: approximation 
and persistence, Proc. Symposium comput. Geom., 226-235, 2011 mathematical guide -part II 5605/08/2012 
 getting out of a maze complex shapes, complex problems and mathematical concepts what is the only 
maths you can't ever apply? The one you don't know! (Mario Pezzana, by way of Massimo Ferri) we had 
a look at mathematical theories and techniques for shape analysis, description and similarity but if 
we go back to shapes and problems, are we sure that everything works ok? let s start by having a look 
to «real» world discussions and open issues 205/08/2012  complex shapes, complex problems online repositories 
of 3D models Google 3D Warehouse http://sketchup.google.com/3dwarehouse/ 3Dvia http:// www.3dvia.com/search/ 
Turbosquid http://www.turbosquid.com/ search our stock catalog to get the 3D model you want, or use 
our Custom 3D modeling service for made­ to-order 3D models. Join the world's top artists who use TurboSquid 
3D models in advertising, architecture, broadcast, games, training, film, the web, and just for fun 05/08/2012 
discussions and open issues 5  questions&#38;answers do these approaches meet the requirements of the 
mathematical methods presented? when and how is robustness really guaranteed? under what conditions 
do methods really work? theoretical answers and benchmarking discussions and open issues 705/08/2012 
 pay attention to  the right metric space rigid transformations (rotations, translations) Euclidean 
distances isometries Riemannian metric curvature (but unstable to local noise/perturbations) geodesics, 
diffusion geometry, Laplacian operators, etc local invariance to shape parameterizations conformal 
geometry similarities (i.e. scale operations) normalized Euclidean distances affinity (and homeomorphisms) 
 persistent topology Morse theory size theory discussions and open issues 1005/08/2012 pay attention 
to  to a suitable shape description coarse coding (but fast) histograms matrices articulated shapes 
 medial axes Reeb graphs overall global appearance silhouettes if shape loops are relevant persistent 
topology graph-based descriptions discussions and open issues 1105/08/2012  acknowledgements VISIONAIR: 
Vision Advanced Infrastructure for Research, European project FP7 INFRASTRUCTURES , 2011-2015 MULTISCALEHUMAN: 
Multi-scale Biological Modalities for Physiological Human Articulation, European project FP7 PEOPLE Initial 
Training Network, 2011-2014 P. Frosini. M. Ferri and the Vision Mathematics group at the Dept. of Mathematics, 
University of Bologna C. Landi, Dept. of Science and Methods of Engineering, University of Modena and 
Reggio Emilia discussions and open issues 1305/08/2012   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343500</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>53</pages>
		<display_no>18</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[Efficient real-time shadows]]></title>
		<page_from>1</page_from>
		<page_to>53</page_to>
		<doi_number>10.1145/2343483.2343500</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343500</url>
		<abstract>
			<par><![CDATA[<p>This course is a resource for applying efficient, real-time shadow algorithms. It builds on a solid foundation (previous courses at SIGGRAPH Asia 2009 and Eurographics 2010, including comprehensive course notes) and the 2011 book <i>Real-Time Shadows</i> (AK Peters) written by four of the presenters. The book is a compendium of many topics in the realm of shadow computation.</p> <p>The course provides an overview of various techniques but moves beyond the basics to practical solutions and game-relevant techniques summarized by a presenter from the production industry. Topics include: the theory behind shadow computation, when physical accuracy can be replaced with plausible shadows, implementation details, and practical issues such as budget considerations and performance trade-offs. Case studies illustrate the techniques behind major game titles and upcoming engines.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738936</person_id>
				<author_profile_id><![CDATA[81310501633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Elmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisemann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[T&#233;l&#233;com ParisTech/CNRS-LTCI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738937</person_id>
				<author_profile_id><![CDATA[81100618307]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ulf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Assarsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chalmers University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738938</person_id>
				<author_profile_id><![CDATA[81540597656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schwarz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738939</person_id>
				<author_profile_id><![CDATA[81100496272]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Valient]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Guerrilla Games/Sony Computer Entertainment]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738940</person_id>
				<author_profile_id><![CDATA[81100084933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wimmer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Vienna University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2383555</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Timo Aila and Samuli Laine. Alias-free shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 161--166, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>581935</ref_obj_id>
				<ref_obj_pid>581896</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tomas Akenine-M&#246;ller and Ulf Assarsson. Approximate soft shadows on arbitrary surfaces using penumbra wedges. In <i>Proceedings of Eurographics Workshop on Rendering 2002</i>, pages 297--306, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tomas Akenine-M&#246;ller, Eric Haines, and Natty Hoffman. <i>Real-Time Rendering</i>. A K Peters, 3rd edition, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>988877</ref_obj_id>
				<ref_obj_pid>988834</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Graham Aldridge and Eric Woods. Robust, geometry-independent shadow volumes. In <i>GRAPHITE '04: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia</i>, pages 250--253, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383857</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz. Convolution shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2007</i>, pages 51--60, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360633</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Thomas Annen, Zhao Dong, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and Jan Kautz. Real-time, all-frequency shadows in dynamic scenes. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2008)</i>, 27(3):34:1--34:8, 2008a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1375741</ref_obj_id>
				<ref_obj_pid>1375714</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Thomas Annen, Tom Mertens, Hans-Peter Seidel, Eddy Flerackers, and Jan Kautz. Exponential shadow maps. In <i>Proceedings of Graphics Interface 2008</i>, pages 155--161, 2008b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1009599</ref_obj_id>
				<ref_obj_pid>1009379</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Jukka Arvo. Tiled shadow maps. In <i>Proceedings of Computer Graphics International 2004</i>, pages 240--246, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882300</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Ulf Assarsson and Tomas Akenine-M&#246;ller. A geometry-based soft shadow volume algorithm using graphics hardware. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2003)</i>, 22(3):511--520, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lionel Atty, Nicolas Holzschuch, Marc Lapierre, Jean-Marc Hasenfratz, Charles Hansen, and Fran&#231;ois X. Sillion. Soft shadow maps: Efficient sampling of light source visibility. <i>Computer Graphics Forum</i>, 25(4):725--741, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866200</ref_obj_id>
				<ref_obj_pid>1882262</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Ilya Baran, Jiawen Chen, Jonathan Ragan-Kelley, Fr&#233;do Durand, and Jaakko Lehtinen. A hierarchical volumetric shadow algorithm for single scattering. <i>ACM Transactions on Graphics</i>, 29(6 (Proceedings of ACM SIGGRAPH Asia 2010)):178:1--178:10, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401061</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Louis Bavoil, Miguel Sainz, and Rouslan Dimitrov. Image-space horizon-based ambient occlusion. In <i>ACM SIGGRAPH 2008 Talks</i>, pages 22:1--22:1, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[P. Bergeron. A general version of crow's shadow volumes. <i>IEEE Computer Graphics and Applications</i>, 6(9):17--28, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1921487</ref_obj_id>
				<ref_obj_pid>1921479</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Markus Billeter, Erik Sintorn, and Ulf Assarson. Volumetric shadows using polygonal light volumes. In <i>Proceedings of High Performance Graphics 2010</i>, pages 39--45, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[William Bilodeau and Mike Songy. Real time shadows, 1999. Creativity 1999, Creative Labs Inc. Sponsored game developer conferences, Los Angeles, California, and Surrey, England.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Venceslas Biri, Didier Arqu&#232;s, and Sylvain Michelin. Real Time Rendering of Atmospheric Scattering and Volumetric Shadows. <i>Journal of WSCG</i>, 14(1):65--72, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944759</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Ji&#345;&#237; Bittner, Oliver Mattausch, Ari Silvennoinen, and Michael Wimmer. Shadow caster culling for efficient shadow mapping. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011</i>, pages 81--88, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>801255</ref_obj_id>
				<ref_obj_pid>800064</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[James F. Blinn. Light reflection functions for simulation of clouds and dusty surfaces. <i>Computer Graphics</i>, 16(3 (Proceedings of ACM SIGGRAPH 82)):21--29, 1982. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Michael Bunnell. Dynamic ambient occlusion and indirect lighting. In Matt Pharr, editor, <i>GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation</i>, pages 223--233, Reading, MA, USA, 2006. Addison-Wesley Professional. ISBN 0-321-33559-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[John Carmack. Z-fail shadow volumes. Internet Forum, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944752</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Jiawen Chen, Ilya Baran, Fr&#233;do Durand, and Wojciech Jarosz. Real-time volumetric shadows using 1D min-max mipmaps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011</i>, pages 39--46, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383556</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Hamilton Y. Chong and Steven J. Gortler. A lixel for every pixel. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 167--172, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Hamilton Yu-Ik Chong. Real-time perspective optimal shadow maps. Senior thesis, Harvard University, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[James H. Clark. Hierarchical geometric models for visible surface algorithms. <i>Communications of the ACM</i>, 19(10):547--554, 1976.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Franklin C. Crow. Shadow algorithms for computer graphics. <i>Computer Graphics</i>, 11(2 (Proceedings of ACM SIGGRAPH 77)):242--248, 1977.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1357016</ref_obj_id>
				<ref_obj_pid>1342250</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Qinghua Dai, Baoguang Yang, and Jieqing Feng. Reconstructable geometry shadow maps. In <i>ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2008: Posters</i>, page 4:1, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>569060</ref_obj_id>
				<ref_obj_pid>569046</ref_obj_pid>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Yoshinori Dobashi, Tsuyoshi Yamamoto, and Tomoyuki Nishita. Interactive rendering of atmospheric scattering effects using graphics hardware. In <i>Proceedings of Graphics Hardware 2002</i>, pages 99--107, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111440</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[William Donnelly and Andrew Lauritzen. Variance shadow maps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2006</i>, pages 161--165, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1111424</ref_obj_id>
				<ref_obj_pid>1111411</ref_obj_pid>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Xavier D&#233;coret. Fast scene voxelization and applications. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2006</i>, pages 71--78, 2006a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Xavier D&#233;coret. Plausible image based soft shadows using occlusion textures. In <i>Proceedings of SIBGRAPI 2006</i>, pages 155--162, 2006b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Xavier D&#233;coret. Occlusion textures for plausible soft shadows. <i>Computer Graphics Forum</i>, 27(1):13--23, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015778</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann and Fr&#233;do Durand. Flash photography enhancement via intrinsic relighting. <i>ACM Transactions on Graphics</i>, 23(3 (Proceedings of ACM SIGGRAPH 2004)):673--678, 2004. URL http://artis.imag.fr/Publications/2004/ED04.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2049989</ref_obj_id>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Elmar Eisemann, Michael Schwarz, Ulf Assarsson, and Michael Wimmer. <i>Real-Time Shadows</i>. A K Peters/CRC Press, Boca Raton, FL, USA, 2011. ISBN 978-1-56881-438-4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730830</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Eric Enderton, Erik Sintorn, Peter Shirley, and David Luebke. Stochastic transparency. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2010</i>, pages 157--164, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Wolfgang Engel. Cascaded shadow maps. In Wolfgang Engel, editor, <i>ShaderX</i>&#60;sup&#62;<i>5</i>&#60;/sup&#62;<i>: Advanced Rendering Techniques</i>, pages 197--206. Charles River Media, Hingham, MA, USA, 2006. ISBN 978-1-58450-499-3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730823</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Thomas Engelhardt and Carsten Dachsbacher. Epipolar sampling for shadows and crepuscular rays in participating media with single scattering. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2010</i>, pages 119--125, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187153</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Randima Fernando. Percentage-closer soft shadows. In <i>ACM SIGGRAPH 2005 Sketches and Applications</i>, page 35, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Vincent Forest, Lo&#239;c Barthe, and Mathias Paulin. Accurate shadows by depth complexity sampling. <i>Computer Graphics Forum (Proceedings of Eurographics 2008)</i>, 27(2):663--674, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1598039</ref_obj_id>
				<ref_obj_pid>1597990</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Pascal Gautron, Jean-Eudes Marvie, and Guillaume Fran&#231;ois. Volumetric shadow mapping. In <i>ACM SIGGRAPH 2009 Talks</i>, pages 49:1--49:1, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1268545</ref_obj_id>
				<ref_obj_pid>1268517</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Markus Giegl and Michael Wimmer. Fitted virtual shadow maps. In <i>Proceedings of Graphics Interface 2007</i>, pages 159--168, 2007a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230112</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Markus Giegl and Michael Wimmer. Queried virtual shadow maps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2007</i>, pages 65--72, 2007b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383922</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Ga&#235;l Guennebaud, Lo&#239;c Barthe, and Mathias Paulin. Real-time soft shadow mapping by back-projection. In <i>Proceedings of Eurographics Symposium on Rendering 2006</i>, pages 227--234, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Tim Heidmann. Real shadows real time. <i>IRIS Universe</i>, 18:28--31, 1991.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730819</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Spatio-temporal upsampling on the GPU. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2010</i>, pages 91--98, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Jared Hoberock and Yuntao Jia. High-quality ambient occlusion. In Hubert Nguyen, editor, <i>GPU Gems 3</i>, pages 257--274. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978-0-321-51526-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[J.-C. Hourcade and A. Nicolas. Algorithms for antialiased cast shadows. <i>Computers &amp; Graphics</i>, 9(3):259--265, 1985.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Jing Huang, Tamy Boubekeur, Tobias Ritschel, Matthias Hollaender, and Elmar Eisemann. Separable approximation of ambient occlusion. In <i>Eurographics 2011 Short Papers</i>, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1283966</ref_obj_id>
				<ref_obj_pid>1283953</ref_obj_pid>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Takashi Imagire, Henry Johan, Naoki Tamura, and Tomoyuki Nishita. Anti-aliased and real-time rendering of scenes with light scattering effects. <i>The Visual Computer</i>, 23(9):935--944, 2007. ISSN 0178-2789.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>957190</ref_obj_id>
				<ref_obj_pid>957155</ref_obj_pid>
				<ref_seq_no>49</ref_seq_no>
				<ref_text><![CDATA[Robert James. True volumetric shadows. In Jeff Lander, editor, <i>Graphics programming methods</i>, pages 353--366. Charles River Media, Rockland, MA, 2003. ISBN 1-58450-299-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095889</ref_obj_id>
				<ref_obj_pid>1095878</ref_obj_pid>
				<ref_seq_no>50</ref_seq_no>
				<ref_text><![CDATA[Gregory S. Johnson, Juhyun Lee, Christopher A. Burns, and William R. Mark. The irregular z-buffer: Hardware acceleration for irregular data structures. <i>ACM Transactions on Graphics</i>, 24(4):1462--1482, 2005. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507159</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>51</ref_seq_no>
				<ref_text><![CDATA[Gregory S. Johnson, Warren A. Hunt, Allen Hux, William R. Mark, Christopher A. Burns, and Stephen Junkins. Soft irregular shadow mapping: Fast, high-quality, and robust soft shadows. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009</i>, pages 57--66, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15902</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>52</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya. The rendering equation. <i>Computer Graphics</i>, 20(4 (Proceedings of ACM SIGGRAPH 86)):143--150, 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>808594</ref_obj_id>
				<ref_obj_pid>800031</ref_obj_pid>
				<ref_seq_no>53</ref_seq_no>
				<ref_text><![CDATA[James T. Kajiya and Brian P Von Herzen. Ray tracing volume densities. <i>Computer Graphics</i>, 18 (3 (Proceedings of ACM SIGGRAPH 84)):165--174, 1984. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>54</ref_seq_no>
				<ref_text><![CDATA[Byungmoon Kim, Kihwan Kim, and Greg Turk. A shadow volume algorithm for opaque and transparent non-manifold casters. <i>Journal of Graphics Tools</i>, 13(3):1--14, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>55</ref_seq_no>
				<ref_text><![CDATA[Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter Seidel. Bent normals and cones in screen-space. In <i>Vision, Modeling and Visualization Workshop</i>, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>56</ref_seq_no>
				<ref_text><![CDATA[Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter Seidel. Screen-space bent cones: A practical approach. In Wolfgang Engel, editor, <i>GPU Pro</i>&#60;sup&#62;<i>3</i>&#60;/sup&#62;: <i>Advanced Rendering Techniques</i>, pages 191--207. A K Peters/CRC Press, Boca Raton, FL, USA, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276497</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>57</ref_seq_no>
				<ref_text><![CDATA[Johannes Kopf, Michael F. Cohen, Dani Lischinski, and Matt Uyttendaele. Joint bilateral up-sampling. <i>ACM Transactions on Graphics</i>, 26(3 (Proceedings of ACM SIGGRAPH 2007)): 96:1--96:5, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073327</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>58</ref_seq_no>
				<ref_text><![CDATA[Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-M&#246;ller. Soft shadow volumes for ray tracing. <i>ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2005)</i>, 24(3):1156--1165, 2005a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073327</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>59</ref_seq_no>
				<ref_text><![CDATA[Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-M&#246;ller. Soft shadow volumes for ray tracing. <i>ACM Transactions on Graphics</i>, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1156--1165, 2005b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>60</ref_seq_no>
				<ref_text><![CDATA[Hayden Landis. Production-ready global illumination, 2002. In <i>ACM SIGGRAPH 2002 Course Notes</i>, RenderMan in Production.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1375739</ref_obj_id>
				<ref_obj_pid>1375714</ref_obj_pid>
				<ref_seq_no>61</ref_seq_no>
				<ref_text><![CDATA[Andrew Lauritzen and Michael McCool. Layered variance shadow maps. In <i>Proceedings of Graphics Interface 2008</i>, pages 139--146, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1944761</ref_obj_id>
				<ref_obj_pid>1944745</ref_obj_pid>
				<ref_seq_no>62</ref_seq_no>
				<ref_text><![CDATA[Andrew Lauritzen, Marco Salvi, and Aaron Lefohn. Sample distribution shadow maps. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2011</i>, pages 97--102, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187126</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>63</ref_seq_no>
				<ref_text><![CDATA[Aaron Lefohn, Shubhabrata Sengupta, Joe Kniss, Robert Strzodka, and John D. Owens. Dynamic adaptive shadow maps on graphics hardware. In <i>ACM SIGGRAPH 2005 Sketches and Applications</i>, page 13, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1289611</ref_obj_id>
				<ref_obj_pid>1289603</ref_obj_pid>
				<ref_seq_no>64</ref_seq_no>
				<ref_text><![CDATA[Aaron E. Lefohn, Shubhabrata Sengupta, and John D. Owens. Resolution matched shadow maps. <i>ACM Transactions on Graphics</i>, 26(4):20:1--20:17, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383561</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>65</ref_seq_no>
				<ref_text><![CDATA[D. Brandon Lloyd, Jeremy Wendt, Naga K. Govindaraju, and Dinesh Manocha. CC shadow volumes. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 197--205, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383921</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>66</ref_seq_no>
				<ref_text><![CDATA[D. Brandon Lloyd, David Tuft, Sung-eui Yoon, and Dinesh Manocha. Warping and partitioning for low error shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2006</i>, pages 215--226, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383554</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>67</ref_seq_no>
				<ref_text><![CDATA[Tobias Martin and Tiow-Seng Tan. Anti-aliasing and continuity with trapezoidal shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 153--160, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>5515</ref_obj_id>
				<ref_obj_pid>5513</ref_obj_pid>
				<ref_seq_no>68</ref_seq_no>
				<ref_text><![CDATA[Nelson Max. Light diffusion through clouds and haze. <i>Computer Vision, Graphics, and Image Processing</i>, 33(3):280--292, 1986a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>15899</ref_obj_id>
				<ref_obj_pid>15922</ref_obj_pid>
				<ref_seq_no>69</ref_seq_no>
				<ref_text><![CDATA[Nelson L. Max. Atmospheric illumination and shadows. <i>Computer Graphics</i>, 20(4 (Proceedings of ACM SIGGRAPH 86)):117--124, 1986b. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1501592</ref_obj_id>
				<ref_obj_pid>1501585</ref_obj_pid>
				<ref_seq_no>70</ref_seq_no>
				<ref_text><![CDATA[&#192;lex M&#233;ndez-Feliu and Mateu Sbert. From obscurances to ambient occlusion: A survey. <i>The Visual Computer</i>, 25(2):181--196, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>71</ref_seq_no>
				<ref_text><![CDATA[Jason Mitchell. Light shafts: Rendering shadows in participating media, 2004. Presentation, <i>Game Developers Conference 2004</i>. http://developer.amd.com/media/gpu_assets/Mitchell_LightShafts.pdf.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1280098</ref_obj_id>
				<ref_obj_pid>1280094</ref_obj_pid>
				<ref_seq_no>72</ref_seq_no>
				<ref_text><![CDATA[Diego Nehab, Pedro V. Sander, Jason Lawrence, Natalya Tatarchuk, and John R. Isidoro. Accelerating real-time shading with reverse reprojection caching. In <i>Proceedings of Graphics Hardware 2007</i>, pages 25--35, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37437</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>73</ref_seq_no>
				<ref_text><![CDATA[Tomoyuki Nishita, Yasuhiro Miyawaki, and Eihachiro Nakamae. A shading model for atmospheric scattering considering luminous intensity distribution of light sources. <i>Computer Graphics</i>, 21(4 (Proceedings of ACM SIGGRAPH 87)):303--310, 1987. ISSN 0097-8930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1230111</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>74</ref_seq_no>
				<ref_text><![CDATA[Christopher Oat and Pedro V. Sander. Ambient aperture lighting. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2007</i>, pages 61--64, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383861</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>75</ref_seq_no>
				<ref_text><![CDATA[Ryan Overbeck, Ravi Ramamoorthi, and William R. Mark. A real-time beam tracer with application to exact soft shadows. In <i>Proceedings of Eurographics Symposium on Rendering 2007</i>, pages 85--98, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>76</ref_seq_no>
				<ref_text><![CDATA[David Pajak, Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Scalable remote rendering with depth and motion-flow augmented streaming. <i>Computer Graphics Forum</i>, 30(2 (Proceedings of Eurographics 2011)):415--424, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>77</ref_seq_no>
				<ref_text><![CDATA[Minghao Pan, Rui Wang, Weifeng Chen, Kun Zhou, and Hujun Bao. Fast, sub-pixel antialiased shadow maps. <i>Computer Graphics Forum</i>, 28(7 (Proceedings of Pacific Graphics 2009)): 1927--1934, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1555902</ref_obj_id>
				<ref_obj_pid>1555880</ref_obj_pid>
				<ref_seq_no>78</ref_seq_no>
				<ref_text><![CDATA[Vincent Pegoraro, Mathias Schott, and Steven G. Parker. An analytical approach to single scattering for anisotropic media and light distributions. In <i>Proceedings of Graphics Interface 2009</i>, pages 71--77, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383634</ref_obj_id>
				<ref_obj_pid>2383616</ref_obj_pid>
				<ref_seq_no>79</ref_seq_no>
				<ref_text><![CDATA[Vincent Pegoraro, Mathias Schott, and Steven G. Parker. A closed-form solution to single scattering for general phase functions and light distributions. <i>Computer Graphics Forum</i>, 29(4 (Proceedings of Eurographics Symposium on Rendering 2010)):1365--1374, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015777</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>80</ref_seq_no>
				<ref_text><![CDATA[Georg Petschnigg, Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography with flash and no-flash image pairs. <i>ACM Transactions on Graphics</i>, 23(3 (Proceedings of ACM SIGGRAPH 2004)):664--672, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37435</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>81</ref_seq_no>
				<ref_text><![CDATA[William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with depth maps. <i>Computer Graphics</i>, 21(4 (Proceedings of ACM SIGGRAPH 87)):283--291, 1987.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>82</ref_seq_no>
				<ref_text><![CDATA[Tobias Ritschel, Thorsten Grosch, and Hans-Peter Seidel. Approximating dynamic global illumination in image space. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009</i>, pages 75--82, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2159641</ref_obj_id>
				<ref_obj_pid>2159616</ref_obj_pid>
				<ref_seq_no>83</ref_seq_no>
				<ref_text><![CDATA[Paul Rosen. Rectilinear texture warping for fast adaptive shadow mapping. In <i>Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2012</i>, pages 151--158, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>84</ref_seq_no>
				<ref_text><![CDATA[Marco Salvi. Rendering filtered shadows with exponential shadow maps. In Wolfgang Engel, editor, <i>ShaderX</i>&#60;sup&#62;<i>6</i>&#60;/sup&#62;<i>: Advanced Rendering Techniques</i>, pages 257--274. Charles River Media, Hingham, MA, USA, 2008. ISBN 978-1-58450-544-0.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383856</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>85</ref_seq_no>
				<ref_text><![CDATA[Daniel Scherzer, Stefan Jeschke, and Michael Wimmer. Pixel-correct shadow maps with temporal reprojection and shadow test confidence. In <i>Proceedings of Eurographics Symposium on Rendering 2007</i>, pages 45--50, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>86</ref_seq_no>
				<ref_text><![CDATA[Michael Schwarz and Marc Stamminger. Bitmask soft shadows. <i>Computer Graphics Forum (Proceedings of Eurographics 2007)</i>, 26(3):515--524, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882301</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>87</ref_seq_no>
				<ref_text><![CDATA[Pradeep Sen, Mike Cammarano, and Pat Hanrahan. Shadow silhouette maps. <i>ACM Transactions on Graphics</i>, 22(3 (Proceedings of ACM SIGGRAPH 2003)):521--526, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>88</ref_seq_no>
				<ref_text><![CDATA[Li Shen, Ga&#235;l Guennebaud, Baoguang Yang, and Jieqing Feng. Predicted virtual soft shadow maps with high quality filtering. <i>Computer Graphics Forum</i>, 30(2 (Proceedings of Eurographics 2011)):493--502, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>89</ref_seq_no>
				<ref_text><![CDATA[Oles Shishkovtsov. Deferred shading in S.T.A.L.K.E.R. In Matt Pharr, editor, <i>GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation</i>, pages 143--166. Addison-Wesley Professional, Reading, MA, USA, 2006. ISBN 0-321-33559-7.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383497</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>90</ref_seq_no>
				<ref_text><![CDATA[Erik Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample based visibility for soft shadows using alias-free shadow maps. <i>Computer Graphics Forum</i>, 27(4 (Proceedings of Eurographics Symposium on Rendering 2008)):1285--1292, 2008a.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383497</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>91</ref_seq_no>
				<ref_text><![CDATA[Erik Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample based visibility for soft shadows using alias-free shadow maps. <i>Computer Graphics Forum (Proceedings of Eurographics Symposium on Rendering 2008)</i>, 27(4):1285--1292, 2008b.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024187</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>92</ref_seq_no>
				<ref_text><![CDATA[Erik Sintorn, Ola Olsson, and Ulf Assarsson. An efficient alias-free shadow algorithm for opaque and transparent objects using per-triangle shadow volumes. <i>ACM Transactions on Graphics</i>, 30(6 (Proceedings of ACM SIGGRAPH Asia 2011)):153:1--153:10, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409080</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>93</ref_seq_no>
				<ref_text><![CDATA[Pitchaya Sitthi-amorn, Jason Lawrence, Lei Yang, Pedro V. Sander, Diego Nehab, and Jiahe Xi. Automated reprojection-based pixel shader optimization. <i>ACM Transactions on Graphics</i>, 27 (5 (Proceedings of ACM SIGGRAPH Asia 2008)):127:1--127:11, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280927</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>94</ref_seq_no>
				<ref_text><![CDATA[Cyril Soler and Fran&#231;ois X. Sillion. Fast calculation of soft shadow textures using convolution. In <i>Proceedings of ACM SIGGRAPH 98</i>, pages 321--332, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>95</ref_seq_no>
				<ref_text><![CDATA[Tiago Sousa, Nickolay Kasyan, and Nicolas Schulz. Secrets of CryENGINE 3 graphics technology, 2011. In <i>ACM SIGGRAPH 2011 Courses</i>, Advances in Real-Time Rendering in 3D Graphics and Games.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566616</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>96</ref_seq_no>
				<ref_text><![CDATA[Marc Stamminger and George Drettakis. Perspective shadow maps. <i>ACM Transactions on Graphics</i>, 21(3 (Proceedings of ACM SIGGRAPH 2002)):557--562, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>97</ref_seq_no>
				<ref_text><![CDATA[Martin Stich, Carsten W&#228;chter, and Alexander Keller. Efficient and robust shadow volumes using hierarchical occlusion culling and geometry shaders. In Hubert Nguyen, editor, <i>GPU Gems 3</i>, pages 239--256. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978-0-321-51526-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073309</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>98</ref_seq_no>
				<ref_text><![CDATA[Bo Sun, Ravi Ramamoorthi, Srinivasa G. Narasimhan, and Shree K. Nayar. A practical analytic single scattering model for real time rendering. <i>ACM Transactions on Graphics</i>, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1040--1049, 2005. ISSN 0730-0301.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1724975</ref_obj_id>
				<ref_obj_pid>1724963</ref_obj_pid>
				<ref_seq_no>99</ref_seq_no>
				<ref_text><![CDATA[L&#225;szl&#243; Szirmay-Kalos, Tam&#225;s Umenhoffer, Bal&#225;zs T&#243;th, L&#225;szl&#243; Sz&#233;csi, and Mateu Sbert. Volumetric ambient occlusion for real-time rendering and games. <i>IEEE Computer Graphics and Applications</i>, 30:70--79, 2010. ISSN 0272-1716. URL http://dx.doi.org/10.1109/MCG.2010.19.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>793029</ref_obj_id>
				<ref_obj_pid>792758</ref_obj_pid>
				<ref_seq_no>100</ref_seq_no>
				<ref_text><![CDATA[Katsumi Tadamura, Xueying Qin, Guofang Jiao, and Eihachiro Nakamae. Rendering optimal solar shadows using plural sunlight depth buffers. In <i>Proceedings of Computer Graphics International 1999</i>, pages 166--173, 1999.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>101</ref_seq_no>
				<ref_text><![CDATA[Katsumi Tadamura, Xueying Qin, Guofang Jiao, and Eihachiro Nakamae. Rendering optimal solar shadows with plural sunlight depth buffers. <i>The Visual Computer</i>, 17(2):76--90, 2001.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>102</ref_seq_no>
				<ref_text><![CDATA[Bal&#224;zs T&#243;th and Tam&#225;s Umenhoffer. Real-time volumetric lighting in participating media. In <i>Eurographics 2009 Short Papers</i>, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>897869</ref_obj_id>
				<ref_seq_no>103</ref_seq_no>
				<ref_text><![CDATA[Yulan Wang and Steven Molnar. Second-depth shadow mapping. Technical Report TR 94-019, University of North Carolina at Chapel Hill, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>104</ref_seq_no>
				<ref_text><![CDATA[D. Weiskopf and T. Ertl. Shadow Mapping Based on Dual Depth Layers. In <i>Eurographics 2003 Short Papers</i>, pages 53--60, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>105</ref_seq_no>
				<ref_text><![CDATA[Lance Williams. Casting curved shadows on curved surfaces. <i>Computer Graphics</i>, 12(3 (Proceedings of ACM SIGGRAPH 78)):270--274, 1978.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383553</ref_obj_id>
				<ref_obj_pid>2383533</ref_obj_pid>
				<ref_seq_no>106</ref_seq_no>
				<ref_text><![CDATA[Michael Wimmer, Daniel Scherzer, and Werner Purgathofer. Light space perspective shadow maps. In <i>Proceedings of Eurographics Symposium on Rendering 2004</i>, pages 143--152, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>130802</ref_obj_id>
				<ref_obj_pid>130745</ref_obj_pid>
				<ref_seq_no>107</ref_seq_no>
				<ref_text><![CDATA[Andrew Woo. The shadow depth map revisited. In David Kirk, editor, <i>Graphics Gems III</i>, pages 338--342. Academic Press, Boston, MA, 1992. ISBN 0-12-409673-5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1900003</ref_obj_id>
				<ref_obj_pid>1899950</ref_obj_pid>
				<ref_seq_no>108</ref_seq_no>
				<ref_text><![CDATA[Chris Wyman. Interactive voxelized epipolar shadow volumes. In <i>ACM SIGGRAPH ASIA 2010 Sketches</i>, pages 53:1--53:2, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>109</ref_seq_no>
				<ref_text><![CDATA[Chris Wyman and Shaun Ramsey. Interactive volumetric shadows in participating media with single scattering. In <i>Proceedings of the IEEE Symposium on Interactive Ray Tracing 2008</i>, pages 87--92, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>110</ref_seq_no>
				<ref_text><![CDATA[Baoguang Yang, Zhao Dong, Jieqing Feng, Hans-Peter Seidel, and Jan Kautz. Variance soft shadow mapping. <i>Computer Graphics Forum (Proceedings of Pacific Graphics 2010)</i>, 29(7): 2127--2134, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1128975</ref_obj_id>
				<ref_obj_pid>1128923</ref_obj_pid>
				<ref_seq_no>111</ref_seq_no>
				<ref_text><![CDATA[Fan Zhang, Hanqiu Sun, Leilei Xu, and Lee Kit Lun. Parallel-split shadow maps for large-scale virtual environments. In <i>Proceedings of ACM International Conference on Virtual Reality Continuum and Its Applications 2006</i>, pages 311--318, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>112</ref_seq_no>
				<ref_text><![CDATA[Hansong Zhang. Forward shadow mapping. In <i>Proceedings of Eurographics Workshop on Rendering 1998</i>, pages 131--138, 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Ef.cient Real-Time Shadows SIGGRAPH 2012 Course Notes www.realtimeshadows.com Elmar Eisemann Télécom 
ParisTech / CNRS-LTCI Ulf Assarsson Chalmers University Michael Schwarz Michal Valient Guerrilla Games 
/ Sony Computer Entertainment Michael Wimmer Vienna University of Technology  About the Authors Elmar 
Eisemann (Organizer) Associate professor, Télécom ParisTech / CNRS-LTCI, France Before being associate 
professor at Télécom ParisTech, Elmar Eisemann headed a research group in the Cluster of Excellence (Saarland 
University/Max-Planck-Institut Informatik) (2008/2009). He studied at the École Normale Supérieure Paris 
(2001) and obtained Master (2004)/Ph.D. (2008) from Grenoble Universities. His interests include real-time 
rendering, shadow algorithms, global illumination, and GPU acceleration techniques. He was a local organizer 
of EGSR 2010 and is organizing EGSR 2012, HPG 2012. In 2011, he received the Eurographics Young Re­searcher 
Award. Ulf Assarsson Associate professor, Department of Computer Science and Engineering, Chalmers University 
of Technology, Sweden Ulf Assarsson received his M.Sc. in Engineering Physics in 1997 and Ph.D. in Computer 
Graph­ics in 2003, and he is now head of a research group focusing primarily on real-time and non-real­time 
soft shadows as well as ray tracing, GPU-techniques and global illumination. Michael Schwarz Michael 
Schwarz received a Diploma in 2005 and a Ph.D. in 2009 from the University of Erlangen-Nuremberg. His 
graphics-related research interests include real-time computer graph­ics, GPU techniques, global illumination, 
procedural modeling, scalable approaches, and percep­tion-aware graphics. About the Authors Michal Valient 
Lead Tech, Guerrilla Games / Sony Computer Entertainment, Netherlands Michal Valient leads the technology 
team at Guerrilla. He spends his time working on the en­gine technology powering highly acclaimed games 
such as Killzone 2 and Killzone 3 as well as some yet unreleased projects. Prior, he worked as a programmer 
and a lead at Caligari where he developed the shader-based real-time rendering engine for Caligari trueSpace7. 
His interests in­clude many aspects of light transfer, shadows and parallel processing. He believes in 
sharing the knowledge and gave GDC and Siggraph talks and wrote graphics papers published in ShaderX 
books and conference journals. Michael Wimmer Associate professor, Institute of Computer Graphics and 
Algorithms, Vienna University of Technology, Austria Michael Wimmer received an M.Sc. in 1997 and a 
Ph.D. in 2001. His current research interests are real-time rendering, computer games, real-time visualization 
of urban environments, point­based rendering and procedural modeling. He has coauthored many papers in 
these .elds, and was papers co-chair of EGSR 2008 and is papers co-chair of Paci.c Graphics 2012.  Course 
Schedule  Introduction (Eisemann, 10 min)  Why shadows? What are shadows? What are the problems?  
Course overview   Basic algorithms (Assarsson, 15 min)  Shadow volumes Z-pass, Z-fail, ZP+, CUDA shadow 
volumes, . . . Shadow mapping Bias, dual-depth shadow maps, . . . Hard shadows (Wimmer, 45 min) Shadow-map 
aliasing  Shadow map reparameterization  Perspective shadow maps, . . . Shadow map partitioning Cascaded 
shadow maps, sample distribution shadow maps, adaptive shadow maps, . . . Shadow reconstruction Silhouette 
shadow maps, . . . Precise hard shadows Irregular z-bu.er, alias-free shadow maps, sub-pixel antialiased 
shadow maps, . . . Other topics Temporal reprojection, shadow caster culling, . . . Filtering hard shadows 
(Eisemann, 20 min) Introduction Percentage closer .ltering E.cient .ltering Variance shadow maps, convolution 
shadow maps, exponential shadow maps, . . . Issues PCF bias, accelerations, . . . Course Schedule Break 
 Soft shadows (Schwarz &#38; Eisemann, 40 min)  Introduction  Image-based solutions   * Percentage-closer 
soft shadows and variants PCSS, CSSM, VSSM, . . . * Occlusion textures * Soft shadow mapping (backprojection) 
  Geometry-based solutions * Penumbra wedges * Soft shadow volumes   Accurate soft shadows  View-sample 
mapping, depth-complexity sampling, . . . Volumetric shadows (Assarsson, 15 min) Ray-marching 1D min-max 
mipmaps Shadow volumes Polygonal light volumes, voxelized shadow volumes, . . . Practical considerations 
for games (Valient, 30 min) Realistic budgets for performance/memory  Upsampling techniques  Varying 
samples, bilateral upsampling, . . . Combination with other e.ects AO, SSAO, ... Showcases AAA titles 
(including Killzone 2 &#38; 3) and upcoming games Conclusion, outlook, and Q&#38;A (All, 5 min) Contents 
 1 Introduction 3 1.1 QualitativeDe.nitionofaShadow ......................... 3 1.2 QuantitativeDe.nitionofaShadow 
........................ 3 1.3 ShadowTypesandComputation .......................... 5  2 Basic Algorithms 
7 2.1 ShadowMaps .................................... 7 2.2 ShadowVolumes .................................. 
8  3 Hard Shadows 11 3.1 Shadow-MapReparameterization.......................... 11 3.2 GlobalShadow-MapPartitioning.......................... 
11 3.3 AdaptiveShadow-MapPartitioning......................... 12 3.4 ShadowReconstruction ............................... 
13 3.5 PreciseHardShadows ............................... 13 3.6 OtherConsiderations ................................ 
13  4 Filtering Hard Shadows 17 4.1 Introduction: Percentage-Closer Filtering . . . . . . . . . . . 
. . . . . . . . . . 17 4.2 E.cientFilteringApproaches ........................... 17 4.2.1 VarianceShadowMaps........................... 
18 4.2.2 ConvolutionShadowMaps......................... 18 4.2.3 ExponentialShadowMaps ......................... 
19  5 Soft Shadows 21 5.1 Image-BasedSolutions ............................... 21 5.2 Geometry-BasedSolutions 
............................. 23  6 Volumetric Shadows 25 6.1 SingleScattering .................................. 
25 6.2 Approaches ..................................... 26 6.2.1 Ray-MarchingApproaches......................... 
26 viii Contents 6.2.2 Shadow-Volume BasedApproaches.................... 27 6.3 Summary ...................................... 
27  7 Practical Considerations 29 7.1 DeferredShadingandUpsampling ......................... 29 7.2 
AmbientOcclusion ................................. 31  8 Conclusion 33 8.1 HardShadows.................................... 
33 8.2 FilteredHardShadows ............................... 33 8.3 SoftShadows .................................... 
34 8.4 FurtherTopics.................................... 34 8.5 LastWords... ................................... 
34  Preface What is this course about? This course is a resource for e.cient real-time shadow algo­rithms. 
It gives an overview of various techniques and addresses practical and game-relevant solutions. Besides 
conventional topics, such as hard or soft shadows, we also address recent practically-relevant topics, 
e.g., volumetric shadows. We will provide the theoretical background but also discuss details on implementation 
issues in order to facilitate e.cient realizations. These elements are of relevance to experts but also 
to practitioners, as they support the general understanding and provide interesting views. A particu­lar 
focus will be put on budget considerations and the analysis of existing performance trade-o.s; physical 
accuracy can sometimes be replaced by plausible shadows, and we will describe sce­narios in which approximate 
methods are likely to work or fail. In particular, we will present showcases that illustrate the techniques 
behind major game titles and upcoming engines. Who should participate? The course will be useful for 
many di.erent .elds, ranging from the game industry (as an overview and guide) over the movie industry 
(as a resource for previ­sualization techniques) to other branches, such as the visualization community 
(shadows have a strong impact on spatial perception). It will allow its participants to gain insight 
into several methods that .nd application in high-performance simulations and to receive practical advice. 
Where can I get more information? The course builds upon a solid foundation in form of previous courses, 
as well as the recent book Real-time Shadows [Eisemann et al., 2011] (A K Peters/CRC Press) that appeared 
in 2011 and was written by four of the presenters. The book is a compendium of many topics in the realm 
of shadow computation on 400 pages. Furthermore, the course features one contributor from the industry, 
who has a strong background in shadow techniques and was a lead for several AAA titles. Please also visit 
our webpage that o.ers additional information on shadow algorithms: http://www.realtimeshadows.com/ 1 
 Introduction 1.1 Qualitative De.nition of a Shadow When asking what a shadow is, people will give various 
answers and for most, you will .nd counterexamples of shadow phenomena that would not have been captured. 
Even dictionaries have a hard time coming up with proper formulations. A mathematical way to de.ne a 
shadow in the context of computer graphics is to consider a point p in space. If each ray from the light 
source L reaches p without encountering any scene object (so-called blockers or shadow casters) on its 
way, p is considered lit, otherwise in shadow. In other words, we are in shadow, if from our point p 
the light source L is at least partially occluded. The region of space where the light source is fully 
occluded is called umbra, whereas the remaining shadow is referred to as penumbra. The di.erent regions 
are indicated in Fig. 1.1. 1.2 Quantitative De.nition of a Shadow While the previous de.nition allows 
us to de.ne where we .nd shadows, it does not allow us to say anything about the way its appearance is 
in.uenced by being in shadow. To clarify this point, we will place ourselves in a simple context. This 
.rst de.nition will be su.cient for most of this course, but we will later give an outlook on extensions. 
The physical interaction can be described well by a soft shadow equation that builds upon a modi.cation 
of the rendering equation introduced by Kajiya [1986] where we only consider direct light from the source: 
Lo(p,.) = fr(p, ., p . q) G(p, q) Le(q, q . p) V(p, q) dq, (1.1) L where Lo(p,.) is the outgoing radiance 
in direction . from point p, f is the BRDF (bidirectional re.ectance function) encoding the material 
surface properties, G is a geometric term taking the con.guration of source and receiver into account, 
Le(q, q . p) is the emitted energy from the source point q towards p, and .nally V(p, q) encodes the 
visibility and is zero if there is a blocker between the two points q and p, and otherwise one. If we 
further assume that all surfaces are Lambertian (perfectly di.use), the BRDF becomes independent of direction, 
i.e. fr(p,., . ) = .(p)/p where .(p) denotes re.ectance. Consequently, Figure 1.1 Depending on the visibility 
of the source, there are three di.erent regions in space; the umbra region, where the light is invisible, 
and the penumbra region, where it is partially visible constitute the shadow, the rest of the scene is 
lit. the outgoing radiance Lo also no longer depends on the outgoing direction. The equation simpli­.es 
to: .(p) Lo(p) = Le(q, q . p) G(p, q) V(p, q) dq, p L In many situations, another simpli.cation can be 
applied in form of a separation of the integral: .(p) 1 Lo(p) = G(p, q) dq · Le(q, q . p) V(p, q) dq 
. |L| p L L _ _ _ _ ___ Shading Shadow Shading and shadows are hereby decoupled. Most existing shadow 
algorithms aim at evaluating the shadow term of the above equation, or even assume a homogenous light 
source of emission ¯ Lc, leading to a visibility integral that modulates the shading and represents the 
actual shadow component of the equation: ¯ Lc V(p, q) dq. (1.2) L Most real-time applications, aim at 
solving this equation when aiming for realistic shadows. Nevertheless, some methods allow us to compute 
Equation 1.1 at a supplementary cost. 1 Introduction 5  1.3 Shadow Types and Computation In many cases, 
the Equation (1.2) is solved by sampling the source point-wise. This has an interesting consequence, 
for a point light source, the whole integral becomes a single visibility query; shadows are binary. This 
is also re.ected by the absence of a penumbra region. Shadows are thus hard because the transition between 
light and darkness is immediate. For volumetric or area sources, the transition is smooth, hence, the 
name soft shadows. Fig. 1.2 shows the result obtained when approximating a soft shadow by evaluating 
several positions on the light source. Even though Equation (1.2) does not seem to complicated, one should 
notice, that it means testing several rays from each point in the scene towards the source. The outcome 
can no longer be locally decided on the point, but all triangles in the scene are potentially involved. 
This makes an e.cient solution di.cult. In the following of this course, we will see various solutions 
of di.ering degrees of accuracy and performance. As we will see, the best algorithmic choice will depend 
heavily on the con.gu­ration of the scene, the type of light source, the wanted accuracy, the way the 
scene is represented, or even the viewpoint. The realm of possibilities is large and to make a good choice 
for your par­ticular needs, a good comprehension, a performance and quality analysis, and a comprehensive 
overview are needed. Our goal is to provide this information with this course. 2  Basic Algorithms In 
this course section, the most basic algorithms for hard shadows will be described. Most of the more advanced 
techniques are derived from these techniques including the ones targeting soft shadows. There are three 
main classes of such methods: projection shadows, shadow maps, and shadow volumes. Projection shadows 
typically mean that the shadow is projected onto a plane and drawn there as a dark object. This will 
not be further treated here. Shadow maps and shadow volumes correspond to two di.erent ways of thinking 
about shadows: as places not seen by the light source and as volumes of space that are dark. The former 
corresponds to shadow maps, explained next, and the latter to shadow volumes. 2.1 Shadow Maps The shadow 
maps method was introduced in 1978 [Williams, 1978]. The algorithm starts by rendering an image from 
the light source. Here, only the depth is stored for each pixel. This image (called the shadow map) represents 
all locations in space that are in light. Next, the scene is rendered from the camera. For each pixel, 
the fragment shader tests if the sampled point is represented in the light s view, i.e., the shadow map. 
If so, the point is in light. Else, the point is in shadow. Due to the discrete resolution of the shadow 
map, a view sample will rarely be exactly rep- 8 2.2 Shadow Volumes  (a) (b) resented in the shadow 
map. This results in two problems: jagged shadows and the need to introduce a tolerance threshold for 
the comparison. The threshold, or bias, must be .ne tuned for each scene, and no bias is guaranteed to 
exist that avoids artifacts. A too large bias results in light leakage at contact shadows, while a too 
small bias results in incorrect self-shadowing. Increasing the shadow map resolution can be helpful, 
since then a smaller bias can be used. But it does not remove the problem. Solving this is the main target 
of most of the more advanced methods for hard shadows, described further on in the course. Relatively 
simple methods exist, however, that mostly pushes the biasing problem to near the silhouette edges, as 
seen from the light source, of the shadow casting objects [Hourcade and Nicolas, 1985; Wang and Molnar, 
1994; Woo, 1992; Weiskopf and Ertl, 2003]. 2.2 Shadow Volumes In its most basic form, the shadow volume 
algorithm creates a volume of space in shadow from each triangle. Each view sample is then tested for 
inclusion in any such shadow volume by using the stencil bu.er to perform the test [Heidmann, 1991]. 
Although the technique was introduced already in 1977 [Crow, 1977], it was not until 1991 that the algorithm 
was e.ciently mapped onto graphics cards. Then, it was, however, no longer fully robust, and .xing this 
became an area for the next 20 years of research, resulting in for instance the Z-fail, ZP+, and ++ZP 
algorithms [Bilodeau and Songy, 1999; Carmack, 2000; Eisemann et al., 2011]. Another problem of the shadow 
volume algorithm is that their rasterization puts a high demand on the .ll-rate capacity of the graphics 
hardware. This inherently makes the algorithm slower than shadow maps. Thus, major focus has been put 
on lowering the amount of necessary rasterization. Shadow volumes are created per object instead of per 
triangle [Bergeron, 1986; Aldridge and Woods, 2004; Kim et al., 2008], and culling and clamping of the 
volumes are used [Clark, 1976; Lloyd et al., 2004; Stich et al., 2007; Eisemann and Décoret, 2006a]. 
2 Basic Algorithms 9 One of the most recent developments of shadow volumes is the Per-Triangle Shadow 
Volume algorithm [Sintorn et al., 2011]. CUDA is used to rasterize per-triangle shadow volumes onto a 
hierarchical frame bu.er resulting in a low overdraw and that transparent shadow casters trivially can 
be supported (see Figure 2.2). 3  Hard Shadows In the previous part of this course, the basic hard 
shadow algorithms have been explained. In this part, we will discuss several methods to reduce shadow-map 
aliasing artifacts. After analyzing aliasing in more detail and showing the di.erent components of aliasing, 
we will show di.erent strategies to reduce sampling error. 3.1 Shadow-Map Reparameterization When projecting 
the view frustum into the shadow map, it becomes apparent that higher sampling densities are required 
near the viewpoint and lower sampling densities far from the viewpoint. In some cases, it is possible 
to apply a single transformation to the scene before projecting it into the shadow map such that the 
sampling density is globally changed in a useful way (Perspective Shadow Maps (PSM) [Stamminger and Drettakis, 
2002]). It can be shown that a logarithmic transformation along the z-axis of the viewer provides an 
optimal sampling rate for the whole depth range in the view frustum [Wimmer et al., 2004], however, this 
requires logarithmic ras­ terization, which is currently infeasible. Practical warping schemes use perspective 
transformations to redistribute samples towards the near plane [Wimmer et al., 2004; Martin and Tan, 
2004; Chong, 2003; Chong and Gortler, 2004]. Figure 3.1 shows the idea based on light-space perspective 
shadow mapping (LiSPSM), where a perspective transform in the shadow-map plane is used to redistribute 
samples. A recent approach tries to adapt the warping locally according to the scene content while still 
using only a single shadow map, which is possible using a rectilinear warping grid [Rosen, 2012]. 3.2 
Global Shadow-Map Partitioning While warping works very well in some con.gurations, especially if the 
light is overhead, there are other con.gurations where warping degenerates to uniform shadow mapping. 
A better alter­native is to use more than one shadow map. The most prominent approach and one of the 
most practical algorithms is to subdivide the view frustum along the z-axis, and calculate a separate 
equal-sized shadow map for each sub­frustum. This algorithm goes by the names of Cascaded Shadow Maps 
(CSM) [Engel, 2006], Parallel Split Shadow Maps (PSSM) [Zhang et al., 2006], or z-partitioning [Lloyd 
et al., 2006], 12 3.3 Adaptive Shadow-Map Partitioning  warp, objects near the viewer appear bigger 
in the shadow map and therefore receive more samples (right). but was actually already discovered earlier 
[Tadamura et al., 1999, 2001]. Using this approach, the sampling density decreases for each successive 
partition, because the same number of shadow map samples cover a larger and larger area. The bene.t can 
be maximized by analyzing the actual distribution of the depth values in the view frustum [Lauritzen 
et al., 2011] in so-called sample distribution shadow maps. Figure 3.2 shows an example con.guration 
for PSSM. Reparametrization and partitioning can also be combined (see Figure 3.3). 3.3 Adaptive Shadow-Map 
Partitioning The advantage of global partitioning algorithm is that they are very fast. On the other 
hand, they completely ignore surface orientation and can therefore not improve undersampling due to surfaces 
that are viewed almost edge-on by the light source (projection aliasing). There are a number of algorithms 
that try to allocate samples in a more optimal way by ana­lyzing the scene before creating the shadow 
map. This inevitably incurs some overhead due to the analysis step (which often necessitates a costly 
read-back), but leads to much better results in general cases. Prominent examples are Adaptive Shadow 
Maps (ASM) [Lefohn et al., 2005], Resolution Matched Shadow Maps (RSMS) [Lefohn et al., 2007], Queried 
Virtual Shadow Maps (QSM) [Giegl and Wimmer, 2007b], Fitted Virtual Shadow Maps (FVSM) [Giegl and Wimmer, 
2007a], and Tiled Shadow Maps (TiledSM) [Arvo, 2004]. Figure 3.4 shows the e.ect of one such adaptive 
partitioning method, QVSM, in action. 3 Hard Shadows 13  3.4 Shadow Reconstruction All methods discussed 
so far assume that shadow maps are sampled at certain positions, and re­construction accesses these samples. 
However, we can also get a more accurate reconstruction of shadow edges by storing, for example, information 
about silhouettes in the shadow map. Notable techniques are forward shadow mapping [Zhang, 1998] and 
silhouette shadow maps [Sen et al., 2003], and reconstructable geometry shadow maps [Dai et al., 2008]. 
 3.5 Precise Hard Shadows The aliasing artifacts in hard shadow mapping stem from the fact that the shadow 
map query locations do not correspond to the shadow map sample locations. Ideally, one would like to 
create shadow map samples exactly in those positions that will be queried later on. Di.cult as that may 
seem, it is actually possible and has been proposed independently by Aila and Laine [2004], and Johnson 
et al. [2005], implemented in hardware [Sintorn et al., 2008b], and later extended to provide e.cient 
antialiasing [Pan et al., 2009]. 3.6 Other Considerations Finally, we will show how to improve shadow 
quality through temporal coherence [Scherzer et al., 2007], and to speed up shadow mapping for large 
scenes by shadow caster culling [Bittner et al., 2011]. 14 3.6 Other Considerations  3 Hard Shadows 
15  4  Filtering Hard Shadows In this part of the course, we discuss several .ltering methods for shadow 
mapping, which are mainly useful for reducing resampling error. Filtering is also often used to hide 
the fact that the resolution of the shadow map is too low by smoothing or blurring the shadow boundaries, 
and sometimes even to provide a rough approximation to soft shadows. We start with a general technique, 
which quickly becomes infeasible for larger .lter kernel sizes, and then show several e.cient .ltering 
techniques that rely on di.erent levels of precomputation. 4.1 Introduction: Percentage-Closer Filtering 
While the idea of shadow-map .ltering is very close to texture mapping, in practice there is an important 
di.erence. It is in general not possible to apply a .lter function to the shadow map and then shadow 
test the result. In that case, the depth values would be averaged, but the resulting shadows would still 
show the same aliasing artifacts because for each view sample, the shadow test still leads to a binary 
outcome. Instead, one needs to .lter the shadow signal, not the depth signal. This approach is called 
percentage-closer .ltering (PCF) [Reeves et al., 1987]. Formally, it is as simple as changing the order 
of depth testing and .ltering, i.e., for every sample in the .lter kernel, a texture lookup is performed, 
the depth test is carried out, and only then is the .lter applied. This approach is very simple and cheap 
for small .lter kernels. Thus, it is mostly useful when the shadow is magni.ed on screen. However, for 
larger .lter kernels, this test implies a large performance penalty because the complete .lter kernel 
needs to be evaluated for every shadow lookup. While PCF produces blurred shadows, these shadows do not 
correspond to soft shadows caused by an area light source, although for smaller .lter kernels, the impression 
of a small area light source can be reasonably invoked. Figure 4.1 shows PCF for small and large .lter 
kernels. 4.2 Ef.cient Filtering Approaches While for texture mapping, the result of .ltering with larger 
.lters can be precomputed, for example using mip-mapping, this is not easily possible with shadow mapping. 
The main problem is that we need to .lter the outcome of the shadow test, and not the depth signal. Since 
the shadow test function is not linear, we cannot change the order of computations. 18 4.2 Ef.cient Filtering 
Approaches  There are two main ways of getting around this limitation: either we interpret the depth 
sam­ples as a distribution and then model the depth comparison statistically. The other option is to 
approximate the depth comparison function with a linear combination of functions that are linear in the 
depth component. 4.2.1 Variance Shadow Maps Variance shadow maps (VSM), introduced by Donnelly and Lauritzen 
[Donnelly and Lauritzen, 2006], are the .rst example of using statistics to facilitate precomputation 
of shadow-map .l­ tering. In VSM, the depth distribution of samples that need to be evaluated by a .lter 
kernel is modeled using .rst and second moments, and the percentage of samples that are hidden is esti­mated 
using the Chebyshev inequality. The .rst two moments correspond to the average depth and average squared 
depth, which can be easily precomputed in a manner similar to mipmap­ping. VSMs su.er from light leaks, 
which can be avoided using layered variance shadow maps (LVSM) [Lauritzen and McCool, 2008], albeit at 
higher cost. 4.2.2 Convolution Shadow Maps Convolution shadow maps (CSM) [Annen et al., 2007] are the 
.rst approach to allow .lter pre­ computation based on a linear signal-theory framework. The shadow-test 
function, which is a step function, is approximated by its (truncated) Fourier expansion. The coe.cients 
of the Fourier expansion are stored in textures and can be mipmapped. Similar to VSMs, CSMs su.er from 
light leaks. Figure 4.2 shows variance shadow maps and convolution shadow maps in comparison. 4 Filtering 
Hard Shadows 19  visible when scaling the light intensity by a factor of four (right of each pair). 
Newer ap­ proaches are able to reduce these artifacts signi.cantly. 4.2.3 Exponential Shadow Maps Annen 
et al. [Annen et al., 2008b] and Salvi [Salvi, 2008] proposed new basis functions for the CSM approach. 
They suggested replacing the Fourier expansion by a simple exponential. This choice voids much of the 
storage requirements and thus addresses one of the major issues. However, the exponential approximation 
is not valid where the depth is in front of the reference depth, and such cases have to be handled by 
resorting to PCF. An interesting alternative is to combine the idea of using exponentials with the VSM 
approach. Lauritzen and McCool [Lauritzen and McCool, 2008] propose using the variance shadow-map approach 
and applying it to depth maps that were warped by an exponential function, leading to a fast and robust 
solution that mostly avoids light leaks. 5  Soft Shadows While point light sources are very popular 
in computer graphics, especially in the real-time do­main, real light sources have a certain extent that 
often cannot be neglected. In particular, the shadows cast by them are not hard but feature partially 
lit penumbrae (transition regions from completely lit to fully occluded), leading to a soft appearance. 
In this part of the course, we will discuss the challenges faced when computing soft shadows and present 
various practical approaches of varying quality, speed, and accuracy. Unlike with hard shadows, it is 
not enough to just determine whether the light source is visible from a receiver point or not. Instead, 
the visible fraction of the light source has to be computed, and it is primarily this point-region visibility 
problem that renders computing soft shadows hard and expensive. For instance, one faces the occluder 
fusion problem: as occluders may interact in non-trivial ways, it is generally not possible to simply 
process individual occluders, determine their respective occlusion factors, and then combine these to 
get the overall light visibility. Con­sequently, many research e.orts were and are still dedicated to 
fast approximate solutions aiming at producing results that are reasonably close to the correct result 
or at least look plausible. On the other hand, advances in computational power and programmability nowadays 
enable approaches that yield accurate results at interactive rates even for complex scenes. 5.1 Image-Based 
Solutions For the majority of real-time applications, approximate methods are currently most relevant 
thanks to their speed. Most of them employ an image-based scene representation, typically re­sorting 
to a standard shadow map. One large group of approaches builds on the observation that blurring hard 
shadow test re­sults yields a soft-shadow-like appearance. Percentage-closer soft shadows (PCSS) [Fernando, 
2005] adaptively choose the amount of blurring using a single-planar-occluder approximation and then 
applies standard percentage-closer .ltering (see Figure 5.1). As this scheme involves many shadow map 
accesses, several techniques for speeding up the computations were devised. Some advanced methods like 
convolution soft shadows [Annen et al., 2008a] and variance soft shadow mapping [Yang et al., 2010] employ 
alternative shadow map representations and pre­ .ltering, rendering the soft-shadow computation basically 
a constant-time operation. Targeting higher quality, Shen et al. [2011] introduce an advanced .ltering 
method and employ adaptive shadow-map partitioning, guided by a perceptual resolution prediction metric 
that exploits the 22 5.1 Image-Based Solutions  (a) Blocker search (b) Penumbra width estimation (c) 
Filtering typically low-frequency nature of penumbrae. A quite di.erent technique is occlusion textures 
[Eisemann and Décoret, 2006b, 2008], where the scene gets decomposed into slices, representing each slice 
by a planar occluder. By adaptively blurring these occluders image-based representations with a box .lter 
[Soler and Sillion, 1998], which can e.ciently be done via pre.ltering, and combining them, approximate 
soft shadows can be obtained rapidly. 5 Soft Shadows 23  Soft shadow mapping [Atty et al., 2006; Guennebaud 
et al., 2006] is a rather accurate approach for which many variants exist. It employs the shadow map 
to reconstruct an approximation of the occluders, unprojecting the shadow map texels into world space. 
The resulting micro-occluders are then backprojected onto the light source, and by aggregating the occluded 
parts, the light s visibility is determined (see Figure 5.2). Like all previously mentioned methods, 
this aggregation combines scalar occlusion values for individual occluders and hence su.ers from the 
occluder fusion problem. This is alleviated by bitmask soft shadows [Schwarz and Stamminger, 2007], where 
the extended light is represented by many point lights, and the binary visibility of these sample points 
is tracked with an occlusion bitmask. While, in principle, this enables accurate results (if a su.ciently 
high number of well-distributed samples is used), the inherently approx­imate nature of image-based representations 
ultimately precludes them. 5.2 Geometry-Based Solutions By contrast, geometry-based solutions avoid 
aliasing problems of image-based approaches, but this typically comes along with a slower speed. Soft 
shadow volumes [Assarsson and Akenine-Möller, 2003] build on shadow volumes for hard shadows and additionally 
employ penumbra wedges [Akenine-Möller and Assarsson, 2002] to account for penumbra regions. These wedges 
are constructed for all silhouette edges and encompass the resulting penumbrae (see Figure 5.3). For 
each covered pixel, the edge is backprojected onto the light, and ultimately, the light area covered 
by the corresponding occluder is computed using Green s formula. However, the method su.ers from the 
occluder fusion problem, leading to wrong results if occluders overlap. This is addressed by depth complexity 
sampling [Laine et al., 2005a] where the light area is represented by several light sample points, and 
a counter for each sample point is maintained, keeping track of the number of occluders overlapping the 
sample point. Originally developed for o.ine ray­tracing, a GPU variant exists as well [Forest et al., 
2008]. 24 5.2 Geometry-Based Solutions Avoiding the high .ll rate of shadow-volume-based methods, view-sample 
mapping [Sintorn et al., 2008b] inserts the view samples (i.e., the shadow-receiving pixel sample points) 
into an alias-free shadow map and then rasterizes the occluders triangles into this map. For each shadow 
map entry, an occlusion bitmask is maintained, and the light sample points overlapped by a triangle are 
set. Ultimately, the number of occluded points yields the amount of occlusion. This method not only produces 
accurate results but is also reasonably fast for interactive applications. A related method is soft irregular 
shadow mapping [Johnson et al., 2009], which makes some compromises concerning accuracy in favor of visual 
smoothness, abandoning point sampling of the light visibility and resorting to silhouettes instead of 
triangles. 6  Volumetric Shadows In this part of the course, we will talk about shadows in participating 
media. The word par­ticipating media means that the medium through which the light travels interacts 
with the light itself. In reality, the world around us is .lled with participating media, e.g., air, 
fog, clouds and smoke. A participating medium scatters light, which means that photons bounce o. the 
particles in the medium. The light undergoes re.ections and possibly also minor refractions, for instance 
when hitting microscopic water drops. The light rays will bounce around before, at least some, will eventually 
reach the eye. This phenomenon makes the participating medium visible. 6.1 Single Scattering For real-time 
purposes, it is common to only consider single scattering, i.e., only one light bounce is taken into 
account. The light travels from the light source, undergoes one re.ective bounce and reaches the eye 
(so called in-scattering). This light, scattered towards the viewer and making the participating medium 
visible, is also referred to as airlight. Multiple scattering is mostly too expensive to compute in real-time. 
Nevertheless, for optically thin media, e.g., air, for which the transmittance is close to 100%, the 
single scattered rays constitute the dominating part of the visual appearance. It is also easy and common 
practice to account for out scattering along the 26 6.2 Approaches light rays. This corresponds to attenuation 
of the intensity with the traveling distance. There are many ways to mathematically solve the airlight 
computation [Sun et al., 2005; Pegoraro et al., 2009, 2010]. There are semi-analytic solutions with texture 
lookups which are used in real-time applications and purely analytical, which are slow. 6.2 Approaches 
Research on volumetric shadows started already in the early 1980s. Blinn introduced a model that describes 
light re.ection for clouds and dusty surfaces consisting of many small particles [Blinn, 1982]. Soon, 
ray-tracing based approaches were used to compute shadows in and by participat­ ing media [Kajiya and 
Von Herzen, 1984]. Shadow volumes were also tried early on to produce atmospheric shadows [Max, 1986b,a; 
Nishita et al., 1987]. With the introduction of programmable shading and compute shaders (e.g. CUDA), 
there has recently been a strong revival on how to compute the shadows in participating media with the 
single-scattering assumption. Many new solutions have been proposed. These are still divided into ray 
marching techniques [Dobashi et al., 2002; Mitchell, 2004; Imagire et al., 2007; Gautron et al., 2009; 
Tóth and Umenho.er, 2009; Engelhardt and Dachsbacher, 2010; Baran et al., 2010; Chen et al., 2011; Wyman, 
2010] and shadow-volume based techniques [James, 2003; Biri et al., 2006; Wyman and Ramsey, 2008; Billeter 
et al., 2010]. These techniques will be presented brie.y. (a) (b) (c) Figure 6.2 (a) Light is scattered 
in the participating medium here by microscopic water drops in the air. (b) Ray marching along a view 
ray to compute airlight contribution for a pixel (i.e., in-scattered light with attenuation from out 
scattering). (c) Alternatively, shadow-volume based approaches can be used to compute the amount of single-scattered 
light towards the eye. 6.2.1 Ray-Marching Approaches Ray-marching approaches step along each per-pixel 
view ray (see Figure 6.2(b)). For each delta step and position that is in light, the in scattering towards 
the eye is computed. Shadows are 6 Volumetric Shadows 27 checked against a shadow map. The ray marching 
can be done by drawing alpha-blended planes [Dobashi et al., 2002; Imagire et al., 2007; Mitchell, 2004], 
looping in a fragment shader [Gautron et al., 2009; Tóth and Umenho.er, 2009; Engelhardt and Dachsbacher, 
2010], or using OpenCL or CUDA [Baran et al., 2010; Chen et al., 2011]. Wyman notes that shadow volume 
planes can be used to bound the ray marching [Wyman and Ramsey, 2008] . Tóth and Umenho.er [Tóth and 
Umenho.er, 2009] only ray marches a few samples per pixel and borrows results from nearby pixels. Chen 
et al. [Chen et al., 2011] instead reduces the ray marching steps by utilizing that all positions further 
from the light than another shadowed position, along the same light ray, have to remain in shadow. The 
most recent method [Wyman, 2010] is based on voxelizing the partic­ ipating media, where each voxel in 
light stores a bit set to one, and each voxel in shadow stores a bit set to zero. The method then very 
rapidly computes the number of lit voxels along each eye ray by using GPU-based pre.x-sums.  6.2.2 Shadow-Volume 
Based Approaches If shadow volumes from separate shadow casters are guaranteed to not overlap, airlight 
can be computed as a sum of order-independent terms by adding contribution for each front-facing shadow 
volume quad and subtracting for each back-facing quad. Otherwise, some sorting of the shadow quads is 
required. Biri et al. [Biri et al., 2006] sort the shadow volume quads, back­ to-front, from the camera, 
while James [James, 2003] instead orders them using depth peeling. Billeter et al. notice that non-overlapping 
shadow volumes can be guaranteed by constructing them from a shadow map [Billeter et al., 2010].  6.3 
Summary We will here summarize the characteristics of the three most recent methods, of which the two 
.rst are ray-marching based [Chen et al., 2011; Wyman, 2010] and the third is shadow-volume based [Billeter 
et al., 2010]. All three methods are very fast and capable of producing frame rates in the order of a 
hundred fps for typical game scenes. The method by Chen et al. is based on utilizing coherency as much 
as possible to reduce the amount of ray marching that needs to be done. This method is the most versatile, 
since it is capable of handling textured light sources. Wyman s algorithm is probably the fastest, but 
is more approximate than the other two, since it does not properly consider the individual light attenuation 
for each voxel, i.e., the intensity and fading of the in-scattered light at each voxel. The approach 
by Billeter et al. avoids any ray marching and voxelization and instead computes the airlight integration 
using a technique very similar to shadow volumes. This one is the easiest to implement and does not require 
CUDA, which even makes it possible to use with WebGL. 7  Practical Considerations The goal of this part 
of the course is to provide an insight into techniques that are often employed in professional game titles. 
We will investigate what happens behind the scenes and which tech­niques are e.cient enough to be applied 
in practice. During the course, these elements will be re.ned, and we will provide statistics on the 
typical rendering budgets that are associated to the various aspects. In these course notes, we will 
concentrate on some techniques that are used to accelerate com­putations and ameliorate shadow quality. 
In particular, we will focus on upsampling techniques and additional e.ects, such as ambient occlusion, 
that allow us to reach a more convincing illu­mination simulation. 7.1 Deferred Shading and Upsampling 
The idea of deferred shading is to derive an image-based representation of the scene. In other words, 
one renders the scene once, while extracting a so-called G-bu.er, a collection of images where each pixel 
stores information about the underlying surface, such as normals, depth, mate­rial properties, etc. Only 
in a second pass, these images are used to evaluate the actual lighting computation, which has proven 
useful in practice for several games (cf. Stalker [Shishkovtsov, 2006]). The advantage is that hidden 
geometry will never see their incoming light evaluated, and, 30 7.1 Deferred Shading and Upsampling 
 furthermore, the computations become more e.cient. Basically, the calculations become more structured 
and map better to the hardware. One can write several attributes of a G-bu.er in a single render pass 
by using multiple render targets. This makes this process particularly e.cient. Despite the fact that 
a pixel only stores at­tributes for one surface location, simple transparency e.ects are possible by 
relying on dithering strategies, similar to stochastic transparency [Enderton et al., 2010]. The second 
possibility to accelerate computations is to distribute calculations spatially over adjacent pixels. 
The idea is to only partly evaluate the shadow. For instance, when employing percentage-closer .ltering 
one can evaluate a di.erent set of samples for each pixel (to de.ne the set of samples to evaluate, di.erent 
sets are de.ned for di.erent positions in the screen plane). Unfortunately, such a choice often leads 
to noisy results. To combat this artifact, one would like to .lter the image, but a standard process 
would lead to visible artifacts at geometric discontinuities and clearly visible halos. In order to avoid 
this problem, a specialized .ltering can be applied. A good strategy is cross-or joint-bilateral .ltering 
[Eisemann and Durand, 2004; Petschnigg et al., 2004]. The principle is to share irradiance values between 
pixels that are geometrically similar (share the same normal, are nearby in space); hereby, .ltering 
is not performed across edges. Basically, the result of the .ltering process for a pixel location t is 
then ti.K(t) .(t, ti)I(ti) I.ltered(t) = , ti.K(t) .(t, ti) where K(t) is a neighborhood around pixel 
position t and the weights . ensure that very di.erent pixels (compared to t) will not contribute to 
its .ltered value. To test this similarity, the weights are based on geometric resemblance evaluated 
via the G-bu.er. Mathematically, the weights can be de.ned as: .(t, ti) = G(sn, 1 - normal(t) · normal(ti)) 
G(sp, .position(t) - position(ti).), where G is a Gaussian kernel, position(·) and normal(·) are the 
G-bu.er values of the extracted position and surface normals. Hereby, only a fraction of the standard 
computation time is neces­sary. To further increase computational e.ciency, one can evaluate shadows 
also for a subset of the pixels in the full-resolution image. The result is then upsampled and spread 
to groups of pixels. In many cases, a good upsampling can be achieved by a slight adaptation of the joint/cross-bilateral 
.ltering, the so-called joint-bilateral upsampling [Kopf et al., 2007]. Here, each pixel s irradiance 
is de.ned as a weighted sum of irradiance values in a low resolution image. Again, geometric similarity 
between the high and low resolution pixels a.ects the weights. Finally, recent strategies making use 
of spatio-temporal upsampling [Herzog et al., 2010] and reprojection [Nehab et al., 2007; Sitthi-amorn 
et al., 2008; Herzog et al., 2010] start to become valuable assets to reuse shading over time. These 
solutions could even play a role for remote rendering con.gurations [Pajak et al., 2011]. 7 Practical 
Considerations 31   7.2 Ambient Occlusion To increase realism in the scene, it is also important to 
approximate complex e.ects, such as global illumination, where light bounces in the scene several times. 
Ambient occlusion approx­imates this e.ect by assuming a uniform white, hemispherical incident illumination. 
A sur­vey [Méndez-Feliu and Sbert, 2009] and recent work [Bunnell, 2006; Hoberock and Jia, 2007] give 
a good overview of various techniques. The most e.cient strategies approximate the incoming illumination 
based on the depth bu.er [Akenine-Möller et al., 2008]. Basically, the depth bu.er from the point of 
view is used as a proxy of the original geometry. To evaluate how much light is impinging at a certain 
point a small neighborhood of surrounding pixels is evaluated. There are various techniques to derive 
an estimate, such as [Bavoil et al., 2008; Szirmay-Kalos et al., 2010; Oat and Sander, 2007; Huang et 
al., 2011]. Directional lighting e.ects become possible via directional occlusion [Ritschel et al., 2009], 
bent normals [Landis, 2002], or bent cones [Klehm et al., 2011, 2012]. The latter are de.ned in screen 
space and lead to a high performance, and variants are employed in recent AAA ti­tles [Sousa et al., 
2011]. 8  Conclusion Today, we have not yet found an ultimate algorithm to compute shadows. Nonetheless, 
for most situations, there are particular techniques that are most suitable. Furthermore, combining several 
methods can be a very good choice. For example, shadow-map repartition and reparameterization can be 
successfully combined to result in a better algorithm than both isolated strategies. Hence, an overview, 
such as in this course, can be very bene.cial as it allows you to choose among the best options. In the 
following, we will pinpoint a few good choices. 8.1 Hard Shadows Hard shadows do not exist in the real 
world. Nonetheless, distant light sources (e.g., the sun) can often be well approximated with such techniques. 
An e.cient accurate computation is possible by relying on shadow-volume solutions with specialized rasterization 
techniques [Sintorn et al., 2011]. In particular, omnidirectional light sources can be treated very easily. 
For a light frustum and depending on the triangle size, the irregular z-Bu.er can also be an option [Johnson 
et al., 2005; Aila and Laine, 2004; Sintorn et al., 2008a]. Approximate solutions can rely on z-partitioning 
approaches [Engel, 2006; Zhang et al., 2006], where the view frustum is decomposed into several distances. 
Such techniques are widely used and a good compromise between quality and cost. Furthermore, partitions 
can even be steered from the viewpoint [Giegl and Wimmer, 2007a]. On top of partitioning, reparameterization 
is possible, such as light perspective shadow maps [Wimmer et al., 2004]. These come basically for free 
and do not in.ict any performance penality. Hence, they are always a good choice for interactive applications. 
A smarter reconstruction of the shadow boundary, e.g., via silhouette shadow maps [Sen et al., 2003] 
is another interesting option. 8.2 Filtered Hard Shadows Reconstruction of the shadow signal also relates 
to .ltering approaches. Most techniques propose accelerations of percentage-closer .ltering [Reeves et 
al., 1987]. 34 8.3 Soft Shadows For low memory cost, variance shadow maps [Donnelly and Lauritzen, 2006] 
are a good choice. Nonetheless, light leaks may appear. Alternatives consuming more memory and re­sources 
exist [Lauritzen and McCool, 2008]. An interesting tradeo. that is exploitable in games are exponential 
shadow maps [Annen et al., 2008b; Salvi, 2008], whose memory consumption is acceptable, but some post 
treatment of artifacts might be necessary. Especially, the variant described in [Lauritzen and McCool, 
2008] is a good option. Filtered shadows are relatively cheap, but not physically based. Nonetheless, 
a designer can often tweak a scene in order to hide these shortcomings. 8.3 Soft Shadows Soft shadows 
approach physically-reasonable shadow behavior and simulate penumbrae. The most e.cient accurate solutions 
[Sintorn et al., 2008a] are too slow for games, but could be used for previsualization and be good additions 
to other o.ine processes [Laine et al., 2005b; Overbeck et al., 2007] . For games and interactive applications 
approximate solutions are most adequate. PCSS-orient­ed solutions [Fernando, 2005] are already often 
applied in practice. Occlusion textures [Eisemann and Décoret, 2008] can be a practical option for scenes 
of a smaller extent, especially if the light source is relatively large. 8.4 Further Topics In the future, 
other shadow-related topics will become important. Volumetric shadow e.ects result in god rays [Chen 
et al., 2011], but also semi-transparent objects, or even shadows cast from indirect sources, will play 
an increasingly important role. 8.5 Last Words . . . We hope you enjoyed this course, and we invite 
you to visit our webpage: http://www.realtimeshadows.com/. Here, we will make the course slides available. 
Further, we will provide more information on recent topics and future trends. We want to keep track of 
the developments in shadow algorithms and be a guiding light that casts a long-lasting shadow.  Acknowledgments 
 Many people helped with their input and support. Many thanks go to Louis Bavoil, Brandon Lloyd, Zhao 
Dong, Cyril Crassin, Tobias Ritschel, Thomas Annen, Robert Herzog, Erik Sintorn, Oliver Klehm, Emmanuel 
Turquin, Bert Buchholz, Aaron Lefohn, Andrew Lauritzen, Martin Eisemann, Hedlena Bezerra, Alice Peters, 
all of which also helped us with our book Real-time Shadows on which this course is based. We also thank 
our colleagues (Telecom ParisTech: Yves Grenier, Isabelle Bloch, Tamy Boubekeur, . . . ; TU Vienna: Daniel 
Scherzer, Werner Purgathofer, . . . ; Chalmers: Ola Olsson, Markus Billeter, . . . ). For the models 
we thank Marco Dabrovic, Martin Newell, the Stanford 3D Scanning Repository, INRIA, and Aim@shape. This 
work was partially funded by the Intel Visual Computing Institute (IVCI) at Saarland University. Bibliography 
 Timo Aila and Samuli Laine. Alias-free shadow maps. In Proceedings of Eurographics Sympo­sium on Rendering 
2004, pages 161 166, 2004. Tomas Akenine-Möller and Ulf Assarsson. Approximate soft shadows on arbitrary 
surfaces using penumbra wedges. In Proceedings of Eurographics Workshop on Rendering 2002, pages 297 
306, 2002. Tomas Akenine-Möller, Eric Haines, and Natty Ho.man. Real-Time Rendering. A K Peters, 3rd 
edition, 2008. Graham Aldridge and Eric Woods. Robust, geometry-independent shadow volumes. In GRAPHITE 
04: Proceedings of the 2nd international conference on Computer graphics and interactive techniques in 
Australasia and South East Asia, pages 250 253, 2004. Thomas Annen, Tom Mertens, Philippe Bekaert, Hans-Peter 
Seidel, and Jan Kautz. Convolution shadow maps. In Proceedings of Eurographics Symposium on Rendering 
2007, pages 51 60, 2007. Thomas Annen, Zhao Dong, Tom Mertens, Philippe Bekaert, Hans-Peter Seidel, and 
Jan Kautz. Real-time, all-frequency shadows in dynamic scenes. ACM Transactions on Graphics (Pro­ceedings 
of ACM SIGGRAPH 2008), 27(3):34:1 34:8, 2008a. Thomas Annen, Tom Mertens, Hans-Peter Seidel, Eddy Flerackers, 
and Jan Kautz. Exponential shadow maps. In Proceedings of Graphics Interface 2008, pages 155 161, 2008b. 
Jukka Arvo. Tiled shadow maps. In Proceedings of Computer Graphics International 2004, pages 240 246, 
2004. Ulf Assarsson and Tomas Akenine-Möller. A geometry-based soft shadow volume algorithm using graphics 
hardware. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2003), 22(3):511 520, 2003. Lionel 
Atty, Nicolas Holzschuch, Marc Lapierre, Jean-Marc Hasenfratz, Charles Hansen, and François X. Sillion. 
Soft shadow maps: E.cient sampling of light source visibility. Computer Graphics Forum, 25(4):725 741, 
2006. Bibliography Ilya Baran, Jiawen Chen, Jonathan Ragan-Kelley, Frédo Durand, and Jaakko Lehtinen. 
A hier­archical volumetric shadow algorithm for single scattering. ACM Transactions on Graphics, 29(6 
(Proceedings of ACM SIGGRAPH Asia 2010)):178:1 178:10, 2010. Louis Bavoil, Miguel Sainz, and Rouslan 
Dimitrov. Image-space horizon-based ambient occlu­sion. In ACM SIGGRAPH 2008 Talks, pages 22:1 22:1, 
2008. P. Bergeron. A general version of crow s shadow volumes. IEEE Computer Graphics and Applications, 
6(9):17 28, 1986. Markus Billeter, Erik Sintorn, and Ulf Assarson. Volumetric shadows using polygonal 
light volumes. In Proceedings of High Performance Graphics 2010, pages 39 45, 2010. William Bilodeau 
and Mike Songy. Real time shadows, 1999. Creativity 1999, Creative Labs Inc. Sponsored game developer 
conferences, Los Angeles, California, and Surrey, England. Venceslas Biri, Didier Arquès, and Sylvain 
Michelin. Real Time Rendering of Atmospheric Scattering and Volumetric Shadows. Journal of WSCG, 14(1):65 
72, 2006. Ji.rí Bittner, Oliver Mattausch, Ari Silvennoinen, and Michael Wimmer. Shadow caster culling 
for e.cient shadow mapping. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2011, pages 81 88, 2011. James F. Blinn. Light re.ection functions for simulation of clouds and dusty 
surfaces. Computer Graphics, 16(3 (Proceedings of ACM SIGGRAPH 82)):21 29, 1982. ISSN 0097-8930. Michael 
Bunnell. Dynamic ambient occlusion and indirect lighting. In Matt Pharr, editor, GPU Gems 2: Programming 
Techniques for High-Performance Graphics and General-Purpose Computation, pages 223 233, Reading, MA, 
USA, 2006. Addison-Wesley Professional. ISBN 0-321-33559-7. John Carmack. Z-fail shadow volumes. Internet 
Forum, 2000. Jiawen Chen, Ilya Baran, Frédo Durand, and Wojciech Jarosz. Real-time volumetric shadows 
using 1D min-max mipmaps. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2011, pages 39 46, 2011. Hamilton Y. Chong and Steven J. Gortler. A lixel for every pixel. In Proceedings 
of Eurographics Symposium on Rendering 2004, pages 167 172, 2004. Hamilton Yu-Ik Chong. Real-time perspective 
optimal shadow maps. Senior thesis, Harvard University, 2003. James H. Clark. Hierarchical geometric 
models for visible surface algorithms. Communications of the ACM, 19(10):547 554, 1976. Bibliography 
Franklin C. Crow. Shadow algorithms for computer graphics. Computer Graphics, 11(2 (Pro­ceedings of ACM 
SIGGRAPH 77)):242 248, 1977. Qinghua Dai, Baoguang Yang, and Jieqing Feng. Reconstructable geometry shadow 
maps. In ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2008: Posters, page 4:1, 2008. Yoshinori 
Dobashi, Tsuyoshi Yamamoto, and Tomoyuki Nishita. Interactive rendering of at­mospheric scattering e.ects 
using graphics hardware. In Proceedings of Graphics Hardware 2002, pages 99 107, 2002. William Donnelly 
and Andrew Lauritzen. Variance shadow maps. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D 
Graphics and Games 2006, pages 161 165, 2006. Elmar Eisemann and Xavier Décoret. Fast scene voxelization 
and applications. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2006, 
pages 71 78, 2006a. Elmar Eisemann and Xavier Décoret. Plausible image based soft shadows using occlusion 
tex­tures. In Proceedings of SIBGRAPI 2006, pages 155 162, 2006b. Elmar Eisemann and Xavier Décoret. 
Occlusion textures for plausible soft shadows. Computer Graphics Forum, 27(1):13 23, 2008. Elmar Eisemann 
and Frédo Durand. Flash photography enhancement via intrinsic relighting. ACM Transactions on Graphics, 
23(3 (Proceedings of ACM SIGGRAPH 2004)):673 678, 2004. URL http://artis.imag.fr/Publications/2004/ED04. 
Elmar Eisemann, Michael Schwarz, Ulf Assarsson, and Michael Wimmer. Real-Time Shadows. A K Peters/CRC 
Press, Boca Raton, FL, USA, 2011. ISBN 978-1-56881-438-4. Eric Enderton, Erik Sintorn, Peter Shirley, 
and David Luebke. Stochastic transparency. In Pro­ceedings of ACM SIGGRAPH Symposium on Interactive 3D 
Graphics and Games 2010, pages 157 164, 2010. Wolfgang Engel. Cascaded shadow maps. In Wolfgang Engel, 
editor, ShaderX5: Advanced Ren­dering Techniques, pages 197 206. Charles River Media, Hingham, MA, USA, 
2006. ISBN 978-1-58450-499-3. Thomas Engelhardt and Carsten Dachsbacher. Epipolar sampling for shadows 
and crepuscu­lar rays in participating media with single scattering. In Proceedings of ACM SIGGRAPH Symposium 
on Interactive 3D Graphics and Games 2010, pages 119 125, 2010. Randima Fernando. Percentage-closer soft 
shadows. In ACM SIGGRAPH 2005 Sketches and Applications, page 35, 2005. Bibliography Vincent Forest, 
Loïc Barthe, and Mathias Paulin. Accurate shadows by depth complexity sam­pling. Computer Graphics Forum 
(Proceedings of Eurographics 2008), 27(2):663 674, 2008. Pascal Gautron, Jean-Eudes Marvie, and Guillaume 
François. Volumetric shadow mapping. In ACM SIGGRAPH 2009 Talks, pages 49:1 49:1, 2009. Markus Giegl 
and Michael Wimmer. Fitted virtual shadow maps. In Proceedings of Graphics Interface 2007, pages 159 
168, 2007a. Markus Giegl and Michael Wimmer. Queried virtual shadow maps. In Proceedings of ACM SIGGRAPH 
Symposium on Interactive 3D Graphics and Games 2007, pages 65 72, 2007b. Gaël Guennebaud, Loïc Barthe, 
and Mathias Paulin. Real-time soft shadow mapping by back­projection. In Proceedings of Eurographics 
Symposium on Rendering 2006, pages 227 234, 2006. Tim Heidmann. Real shadows real time. IRIS Universe, 
18:28 31, 1991. Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Spatio-temporal 
upsampling on the GPU. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2010, pages 91 98, 2010. Jared Hoberock and Yuntao Jia. High-quality ambient occlusion. In Hubert Nguyen, 
editor, GPU Gems 3, pages 257 274. Addison-Wesley Professional, Reading, MA, USA, 2007. ISBN 978­0-321-51526-1. 
J.-C. Hourcade and A. Nicolas. Algorithms for antialiased cast shadows. Computers &#38; Graphics, 9(3):259 
265, 1985. Jing Huang, Tamy Boubekeur, Tobias Ritschel, Matthias Hollaender, and Elmar Eisemann. Sep­arable 
approximation of ambient occlusion. In Eurographics 2011 Short Papers, 2011. Takashi Imagire, Henry Johan, 
Naoki Tamura, and Tomoyuki Nishita. Anti-aliased and real-time rendering of scenes with light scattering 
e.ects. The Visual Computer, 23(9):935 944, 2007. ISSN 0178-2789. Robert James. True volumetric shadows. 
In Je. Lander, editor, Graphics programming methods, pages 353 366. Charles River Media, Rockland, MA, 
2003. ISBN 1-58450-299-1. Gregory S. Johnson, Juhyun Lee, Christopher A. Burns, and William R. Mark. 
The irregular z-bu.er: Hardware acceleration for irregular data structures. ACM Transactions on Graphics, 
24(4):1462 1482, 2005. ISSN 0730-0301. Gregory S. Johnson, Warren A. Hunt, Allen Hux, William R. Mark, 
Christopher A. Burns, and Stephen Junkins. Soft irregular shadow mapping: Fast, high-quality, and robust 
soft shadows. In Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009, pages 
57 66, 2009. Bibliography James T. Kajiya. The rendering equation. Computer Graphics, 20(4 (Proceedings 
of ACM SIGGRAPH 86)):143 150, 1986. James T. Kajiya and Brian P Von Herzen. Ray tracing volume densities. 
Computer Graphics, 18 (3 (Proceedings of ACM SIGGRAPH 84)):165 174, 1984. ISSN 0097-8930. Byungmoon Kim, 
Kihwan Kim, and Greg Turk. A shadow volume algorithm for opaque and transparent non-manifold casters. 
Journal of Graphics Tools, 13(3):1 14, 2008. Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter 
Seidel. Bent normals and cones in screen-space. In Vision, Modeling and Visualization Workshop, 2011. 
Oliver Klehm, Tobias Ritschel, Elmar Eisemann, and Hans-Peter Seidel. Screen-space bent cones: A practical 
approach. In Wolfgang Engel, editor, GPU Pro3: Advanced Rendering Techniques, pages 191 207. A K Peters/CRC 
Press, Boca Raton, FL, USA, 2012. Johannes Kopf, Michael F. Cohen, Dani Lischinski, and Matt Uyttendaele. 
Joint bilateral up­sampling. ACM Transactions on Graphics, 26(3 (Proceedings of ACM SIGGRAPH 2007)): 
96:1 96:5, 2007. Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-Möller. Soft 
shadow volumes for ray tracing. ACM Transactions on Graphics (Proceedings of ACM SIG-GRAPH 2005), 24(3):1156 
1165, 2005a. Samuli Laine, Timo Aila, Ulf Assarsson, Jaakko Lehtinen, and Tomas Akenine-Möller. Soft 
shadow volumes for ray tracing. ACM Transactions on Graphics, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1156 
1165, 2005b. Hayden Landis. Production-ready global illumination, 2002. In ACM SIGGRAPH 2002 Course Notes, 
RenderMan in Production. Andrew Lauritzen and Michael McCool. Layered variance shadow maps. In Proceedings 
of Graphics Interface 2008, pages 139 146, 2008. Andrew Lauritzen, Marco Salvi, and Aaron Lefohn. Sample 
distribution shadow maps. In Pro­ceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 
2011, pages 97 102, 2011. Aaron Lefohn, Shubhabrata Sengupta, Joe Kniss, Robert Strzodka, and John D. 
Owens. Dy­namic adaptive shadow maps on graphics hardware. In ACM SIGGRAPH 2005 Sketches and Applications, 
page 13, 2005. Aaron E. Lefohn, Shubhabrata Sengupta, and John D. Owens. Resolution matched shadow maps. 
ACM Transactions on Graphics, 26(4):20:1 20:17, 2007. Bibliography D. Brandon Lloyd, Jeremy Wendt, Naga 
K. Govindaraju, and Dinesh Manocha. CC shadow volumes. In Proceedings of Eurographics Symposium on Rendering 
2004, pages 197 205, 2004. D. Brandon Lloyd, David Tuft, Sung-eui Yoon, and Dinesh Manocha. Warping and 
partitioning for low error shadow maps. In Proceedings of Eurographics Symposium on Rendering 2006, pages 
215 226, 2006. Tobias Martin and Tiow-Seng Tan. Anti-aliasing and continuity with trapezoidal shadow 
maps. In Proceedings of Eurographics Symposium on Rendering 2004, pages 153 160, 2004. Nelson Max. Light 
di.usion through clouds and haze. Computer Vision, Graphics, and Image Processing, 33(3):280 292, 1986a. 
Nelson L. Max. Atmospheric illumination and shadows. Computer Graphics, 20(4 (Proceedings of ACM SIGGRAPH 
86)):117 124, 1986b. ISSN 0097-8930. Àlex Méndez-Feliu and Mateu Sbert. From obscurances to ambient occlusion: 
A survey. The Visual Computer, 25(2):181 196, 2009. Jason Mitchell. Light shafts: Rendering shadows in 
participating media, 2004. Presentation, Game Developers Conference 2004. http://developer.amd.com/media/gpu_assets/Mitchell_LightShafts.pdf. 
Diego Nehab, Pedro V. Sander, Jason Lawrence, Natalya Tatarchuk, and John R. Isidoro. Ac­celerating real-time 
shading with reverse reprojection caching. In Proceedings of Graphics Hardware 2007, pages 25 35, 2007. 
Tomoyuki Nishita, Yasuhiro Miyawaki, and Eihachiro Nakamae. A shading model for atmo­spheric scattering 
considering luminous intensity distribution of light sources. Computer Graphics, 21(4 (Proceedings of 
ACM SIGGRAPH 87)):303 310, 1987. ISSN 0097-8930. Christopher Oat and Pedro V. Sander. Ambient aperture 
lighting. In Proceedings of ACM SIG-GRAPH Symposium on Interactive 3D Graphics and Games 2007, pages 
61 64, 2007. Ryan Overbeck, Ravi Ramamoorthi, and William R. Mark. A real-time beam tracer with appli­cation 
to exact soft shadows. In Proceedings of Eurographics Symposium on Rendering 2007, pages 85 98, 2007. 
David Pajak, Robert Herzog, Elmar Eisemann, Karol Myszkowski, and Hans-Peter Seidel. Scal­able remote 
rendering with depth and motion-.ow augmented streaming. Computer Graphics Forum, 30(2 (Proceedings of 
Eurographics 2011)):415 424, 2011. Minghao Pan, Rui Wang, Weifeng Chen, Kun Zhou, and Hujun Bao. Fast, 
sub-pixel antialiased shadow maps. Computer Graphics Forum, 28(7 (Proceedings of Paci.c Graphics 2009)): 
1927 1934, 2009. Bibliography Vincent Pegoraro, Mathias Schott, and Steven G. Parker. An analytical 
approach to single scatter­ing for anisotropic media and light distributions. In Proceedings of Graphics 
Interface 2009, pages 71 77, 2009. Vincent Pegoraro, Mathias Schott, and Steven G. Parker. A closed-form 
solution to single scat­tering for general phase functions and light distributions. Computer Graphics 
Forum, 29(4 (Proceedings of Eurographics Symposium on Rendering 2010)):1365 1374, 2010. Georg Petschnigg, 
Richard Szeliski, Maneesh Agrawala, Michael Cohen, Hugues Hoppe, and Kentaro Toyama. Digital photography 
with .ash and no-.ash image pairs. ACM Transactions on Graphics, 23(3 (Proceedings of ACM SIGGRAPH 2004)):664 
672, 2004. William T. Reeves, David H. Salesin, and Robert L. Cook. Rendering antialiased shadows with 
depth maps. Computer Graphics, 21(4 (Proceedings of ACM SIGGRAPH 87)):283 291, 1987. Tobias Ritschel, 
Thorsten Grosch, and Hans-Peter Seidel. Approximating dynamic global illu­mination in image space. In 
Proceedings of ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games 2009, pages 75 82, 2009. Paul 
Rosen. Rectilinear texture warping for fast adaptive shadow mapping. In Proceedings of ACM SIGGRAPH Symposium 
on Interactive 3D Graphics and Games 2012, pages 151 158, 2012. Marco Salvi. Rendering .ltered shadows 
with exponential shadow maps. In Wolfgang En­gel, editor, ShaderX6: Advanced Rendering Techniques, pages 
257 274. Charles River Media, Hingham, MA, USA, 2008. ISBN 978-1-58450-544-0. Daniel Scherzer, Stefan 
Jeschke, and Michael Wimmer. Pixel-correct shadow maps with tempo­ral reprojection and shadow test con.dence. 
In Proceedings of Eurographics Symposium on Rendering 2007, pages 45 50, 2007. Michael Schwarz and Marc 
Stamminger. Bitmask soft shadows. Computer Graphics Forum (Proceedings of Eurographics 2007), 26(3):515 
524, 2007. Pradeep Sen, Mike Cammarano, and Pat Hanrahan. Shadow silhouette maps. ACM Transactions on 
Graphics, 22(3 (Proceedings of ACM SIGGRAPH 2003)):521 526, 2003. Li Shen, Gaël Guennebaud, Baoguang 
Yang, and Jieqing Feng. Predicted virtual soft shadow maps with high quality .ltering. Computer Graphics 
Forum, 30(2 (Proceedings of Eurograph­ics 2011)):493 502, 2011. Oles Shishkovtsov. Deferred shading in 
S.T.A.L.K.E.R. In Matt Pharr, editor, GPU Gems 2: Programming Techniques for High-Performance Graphics 
and General-Purpose Computa­tion, pages 143 166. Addison-Wesley Professional, Reading, MA, USA, 2006. 
ISBN 0-321­33559-7. Bibliography Erik Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample based visibility 
for soft shadows using alias-free shadow maps. Computer Graphics Forum, 27(4 (Proceedings of Eurographics 
Symposium on Rendering 2008)):1285 1292, 2008a. Erik Sintorn, Elmar Eisemann, and Ulf Assarsson. Sample 
based visibility for soft shadows using alias-free shadow maps. Computer Graphics Forum (Proceedings 
of Eurographics Symposium on Rendering 2008), 27(4):1285 1292, 2008b. Erik Sintorn, Ola Olsson, and Ulf 
Assarsson. An e.cient alias-free shadow algorithm for opaque and transparent objects using per-triangle 
shadow volumes. ACM Transactions on Graphics, 30(6 (Proceedings of ACM SIGGRAPH Asia 2011)):153:1 153:10, 
2011. Pitchaya Sitthi-amorn, Jason Lawrence, Lei Yang, Pedro V. Sander, Diego Nehab, and Jiahe Xi. Automated 
reprojection-based pixel shader optimization. ACM Transactions on Graphics, 27 (5 (Proceedings of ACM 
SIGGRAPH Asia 2008)):127:1 127:11, 2008. Cyril Soler and François X. Sillion. Fast calculation of soft 
shadow textures using convolution. In Proceedings of ACM SIGGRAPH 98, pages 321 332, 1998. Tiago Sousa, 
Nickolay Kasyan, and Nicolas Schulz. Secrets of CryENGINE 3 graphics tech­nology, 2011. In ACM SIGGRAPH 
2011 Courses, Advances in Real-Time Rendering in 3D Graphics and Games. Marc Stamminger and George Drettakis. 
Perspective shadow maps. ACM Transactions on Graphics, 21(3 (Proceedings of ACM SIGGRAPH 2002)):557 562, 
2002. Martin Stich, Carsten Wächter, and Alexander Keller. E.cient and robust shadow volumes using hierarchical 
occlusion culling and geometry shaders. In Hubert Nguyen, editor, GPU Gems 3, pages 239 256. Addison-Wesley 
Professional, Reading, MA, USA, 2007. ISBN 978-0-321­51526-1. Bo Sun, Ravi Ramamoorthi, Srinivasa G. 
Narasimhan, and Shree K. Nayar. A practical ana­lytic single scattering model for real time rendering. 
ACM Transactions on Graphics, 24(3 (Proceedings of ACM SIGGRAPH 2005)):1040 1049, 2005. ISSN 0730-0301. 
László Szirmay-Kalos, Tamás Umenho.er, Balázs Tóth, László Szécsi, and Mateu Sbert. Vol­umetric ambient 
occlusion for real-time rendering and games. IEEE Computer Graphics and Applications, 30:70 79, 2010. 
ISSN 0272-1716. URL http://dx.doi.org/10.1109/MCG. 2010.19. Katsumi Tadamura, Xueying Qin, Guofang Jiao, 
and Eihachiro Nakamae. Rendering optimal solar shadows using plural sunlight depth bu.ers. In Proceedings 
of Computer Graphics International 1999, pages 166 173, 1999. Katsumi Tadamura, Xueying Qin, Guofang 
Jiao, and Eihachiro Nakamae. Rendering optimal solar shadows with plural sunlight depth bu.ers. The Visual 
Computer, 17(2):76 90, 2001. Bibliography Balàzs Tóth and Tamás Umenho.er. Real-time volumetric lighting 
in participating media. In Eurographics 2009 Short Papers, 2009. Yulan Wang and Steven Molnar. Second-depth 
shadow mapping. Technical Report TR 94-019, University of North Carolina at Chapel Hill, 1994. D. Weiskopf 
and T. Ertl. Shadow Mapping Based on Dual Depth Layers. In Eurographics 2003 Short Papers, pages 53 60, 
2003. Lance Williams. Casting curved shadows on curved surfaces. Computer Graphics, 12(3 (Pro­ceedings 
of ACM SIGGRAPH 78)):270 274, 1978. Michael Wimmer, Daniel Scherzer, and Werner Purgathofer. Light space 
perspective shadow maps. In Proceedings of Eurographics Symposium on Rendering 2004, pages 143 152, 2004. 
Andrew Woo. The shadow depth map revisited. In David Kirk, editor, Graphics Gems III, pages 338 342. 
Academic Press, Boston, MA, 1992. ISBN 0-12-409673-5. Chris Wyman. Interactive voxelized epipolar shadow 
volumes. In ACM SIGGRAPH ASIA 2010 Sketches, pages 53:1 53:2, 2010. Chris Wyman and Shaun Ramsey. Interactive 
volumetric shadows in participating media with single scattering. In Proceedings of the IEEE Symposium 
on Interactive Ray Tracing 2008, pages 87 92, 2008. Baoguang Yang, Zhao Dong, Jieqing Feng, Hans-Peter 
Seidel, and Jan Kautz. Variance soft shadow mapping. Computer Graphics Forum (Proceedings of Paci.c Graphics 
2010), 29(7): 2127 2134, 2010. Fan Zhang, Hanqiu Sun, Leilei Xu, and Lee Kit Lun. Parallel-split shadow 
maps for large-scale virtual environments. In Proceedings of ACM International Conference on Virtual 
Reality Continuum and Its Applications 2006, pages 311 318, 2006. Hansong Zhang. Forward shadow mapping. 
In Proceedings of Eurographics Workshop on Ren­dering 1998, pages 131 138, 1998. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343501</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>50</pages>
		<display_no>20</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[FEM simulation of 3D deformable solids]]></title>
		<subtitle><![CDATA[a practitioner's guide to theory, discretization and model reduction]]></subtitle>
		<page_from>1</page_from>
		<page_to>50</page_to>
		<doi_number>10.1145/2343483.2343501</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343501</url>
		<abstract>
			<par><![CDATA[<p>A practical guide to finite-element-method (FEM) simulation of 3D deformable solids reviews essential offline FEM simulation techniques: complex nonlinear materials, invertible treatment of elasticity, and model-reduction techniques for real-time simulation.</p> <p>Simulations of deformable solids are important in many applications in computer graphics, including film special effects, computer games, and virtual surgery. FEM has become a popular method in many applications. Both offline simulation and real-time techniques have matured in computer graphics literature.</p> <p>This course is designed for attendees familiar with numerical simulation in computer graphics who would like to obtain a cohesive picture of the various FEM simulation methods available, their strengths and weaknesses, and their applicability in various simulation scenarios. The course is also a practical implementation guide for the visual-effects developer, offering a very lean yet adequate synopsis of the underlying mathematical theory. The first section introduces FEM deformable-object simulation and its fundamental concepts, such as deformation gradient, strain, stress, and elastic energy, discusses corotational FEM models, isotropic hyperelasticity, and numerical methods such as conjugate gradients and multigrid. The second section presents the state of the art in model reduction techniques for real-time FEM solid simulation. Topics include linear modal analysis, modal warping, subspace simulation, domain decomposition, and which techniques are suitable for which application.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738941</person_id>
				<author_profile_id><![CDATA[81409594691]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eftychios]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sifakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Wisconsin-Madison]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738942</person_id>
				<author_profile_id><![CDATA[81100158368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jernej]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barbic]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1050202</ref_obj_id>
				<ref_obj_pid>1048935</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[V. Akcelik, J. Bielak, G. Biros, I. Epanomeritakis, A. Fernandez, O. Ghattas, E. J. Kim, J. Lopez, D. O'Hallaron, T. Tu, and J. Urbanic. High-resolution forward and inverse earthquake modeling on terascale computers. In <i>Proceedings of ACM/IEEE SC2003</i>, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409118</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[S. S. An, T. Kim, and D. L. James. Optimizing cubature for efficient integration of subspace deformations. <i>ACM Trans. on Graphics</i>, 27(5):165:1--165:10, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D. Baraff and A. P. Witkin. Large Steps in Cloth Simulation. In <i>Proc. of ACM SIGGRAPH 98</i>, pages 43--54, July 1998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1354115</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Barbi&#269;. <i>Real-time Reduced Large-Deformation Models and Distributed Contact for Computer Graphics and Haptics</i>. PhD thesis, Carnegie Mellon University, Aug. 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531359</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Barbi&#269;, M. da Silva, and J. Popovi&#263;. Deformable object animation using reduced optimal control. <i>ACM Trans. on Graphics (SIGGRAPH 2009)</i>, 28(3):53:1--53:9, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073300</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[J. Barbi&#269; and D. L. James. Real-time subspace integration for St. Venant-Kirchhoff deformable models. <i>ACM Trans. on Graphics</i>, 24(3):982--990, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409116</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[J. Barbi&#269; and J. Popovi&#263;. Real-time control of physically based simulations using gentle forces. <i>ACM Trans. on Graphics (SIGGRAPH Asia 2008)</i>, 27(5):163:1--163:10, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964986</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[J. Barbi&#269; and Y. Zhao. Real-time large-deformation substructuring. <i>ACM Trans. on Graphics (SIGGRAPH 2011)</i>, 30(4):91:1--91:7, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531395</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[B. Bickel, M. Baecher, M. Otaduy, W. Matusik, H. Pfister, and M. Gross. Capture and modeling of non-linear heterogeneous soft tissue. <i>ACM Trans. on Graphics (SIGGRAPH 2009)</i>, 28(3):89:1--89:9, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618465</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. N. Chadwick, S. S. An, and D. L. James. Harmonic Shells: A practical nonlinear sound model for near-rigid thin shells. <i>ACM Transactions on Graphics</i>, 28(5):1--10, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1032541</ref_obj_id>
				<ref_obj_pid>1032290</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[M. G. Choi and H.-S. Ko. Modal Warping: Real-Time Simulation of Large Rotational Deformation and Manipulation. <i>IEEE Trans. on Vis. and Comp. Graphics</i>, 11(1):91--101, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[M. G. Choi, S. Y. Woo, and H.-S. Ko. Real-Time Simulation of Thin Shells. <i>Eurographics 2007</i>, pages 349--354, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[L. Daniel. Private correspondence with Prof. Luca Daniel, MIT.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[K. K. Hauser, C. Shen, and J. F. O'Brien. Interactive deformation using modal analysis with constraints. In <i>Proc. of Graphics Interface</i>, pages 247--256, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1221819</ref_obj_id>
				<ref_obj_pid>1221585</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[J. Huang, X. Liu, H. Bao, B. Guo, and H.-Y. Shum. An efficient large deformation method using domain decomposition. <i>Computers &amp; Graphics</i>, 30(6):927--935, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1990991</ref_obj_id>
				<ref_obj_pid>1990770</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[J. Huang, Y. Tong, K. Zhou, H. Bao, and M. Desbrun. Interactive shape interpolation through controllable dynamic deformation. <i>IEEE Trans. on Visualization and Computer Graphics</i>, 17(7):983--992, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141983</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[D. L. James, J. Barbi&#269;, and D. K. Pai. Precomputed acoustic transfer: Output-sensitive, accurate sound generation for geometrically complex vibration sources. <i>ACM Transactions on Graphics (SIGGRAPH 2006)</i>, 25(3), 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566621</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[D. L. James and D. K. Pai. DyRT: Dynamic Response Textures for Real Time Deformation Simulation With Graphics Hardware. <i>ACM Trans. on Graphics</i>, 21(3):582--585, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[D. L. James and D. K. Pai. Real Time Simulation of Multizone Elastokinematic Models. In <i>IEEE Int. Conf. on Robotics and Automation</i>, pages 927--932, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015735</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[D. L. James and D. K. Pai. BD-Tree: Output-Sensitive Collision Detection for Reduced Deformable Models. <i>ACM Trans. on Graphics</i>, 23(3):393--398, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409117</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[D. M. Kaufman, S. Sueda, D. L. James, and D. K. Pai. Staggered Projections for Frictional Contact in Multibody Systems. <i>ACM Transactions on Graphics</i>, 27(5):164:1--164:11, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[A. E. Kerdok, S. M. Cotin, M. P. Ottensmeyer, A. M. Galea, R. D. Howe, and S. L. Dawson. Truth cube: Establishing physical standards for soft tissue simulation. <i>Medical Image Analysis</i>, 7(3):283--291, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618469</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[T. Kim and D. James. Skipping steps in deformable simulation with online model reduction. <i>ACM Trans. on Graphics (SIGGRAPH Asia 2009)</i>, 28(5):123:1--123:9, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2019415</ref_obj_id>
				<ref_obj_pid>2019406</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[T. Kim and D. James. Physics-based character skinning using multi-domain subspace deformations. In <i>Symp. on Computer Animation (SCA)</i>, pages 63--72, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[C. L. Lawson and R. J. Hanson. <i>Solving Least Square Problems</i>. Prentice Hall, Englewood Cliffs, NJ, 1974.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[R. Lehoucq, D. Sorensen, and C. Yang. ARPACK Users' Guide: Solution of large scale eigenvalue problems with implicitly restarted Arnoldi methods. Technical report, Comp. and Applied Mathematics, Rice Univ., 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[J.-R. Li. <i>Model Reduction of Large Linear Systems via Low Rank System Gramians</i>. PhD thesis, Massachusetts Institute of Technology, 2000.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[R.-C. Li and Z. Bai. Structure preserving model reduction using a Krylov subspace projection formulation. <i>Comm. Math. Sci</i>., 3(2):179--199, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[J. L. Lumley. The structure of inhomogeneous turbulence. In A. M. Yaglom and V. I. Tatarski, editors, <i>Atmospheric turbulence and wave propagation</i>, pages 166--178, 1967.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015744</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[A. McNamara, A. Treuille, Z. Popovi&#263;, and J. Stam. Fluid control using the adjoint method. <i>ACM Trans. on Graphics (SIGGRAPH 2004)</i>, 23(3):449--456, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>545290</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[J. F. O'Brien, C. Shen, and C. M. Gatchalian. Synthesizing sounds from rigid-body simulations. In <i>Symp. on Computer Animation (SCA)</i>, pages 175--181, 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>74355</ref_obj_id>
				<ref_obj_pid>74334</ref_obj_pid>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[A. Pentland and J. Williams. Good vibrations: Modal dynamics for graphics and animation. <i>Computer Graphics (Proc. of ACM SIGGRAPH 89)</i>, 23(3):215--222, 1989.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[M. Rewienski. <i>A Trajectory Piecewise-Linear Approach to Model Order Reduction of Nonlinear Dynamical Systems</i>. PhD thesis, Massachusetts Institute of Technology, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015754</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[A. Safonova, J. Hodgins, and N. Pollard. Synthesizing physically realistic human motion in low-dimensional, behavior-specific spaces. <i>ACM Trans. on Graphics (SIGGRAPH 2004)</i>, 23(3):514--521, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[A. A. Shabana. <i>Theory of Vibration, Volume II: Discrete and Continuous Systems</i>. Springer--Verlag, New York, NY, 1990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285868</ref_obj_id>
				<ref_obj_pid>285861</ref_obj_pid>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[R. B. Sidje. Expokit: A Software Package for Computing Matrix Exponentials. <i>ACM Trans. on Mathematical Software</i>, 24(1):130--156, 1998. www.expokit.org.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[R. F. Stengel. <i>Optimal Control and Estimation</i>. Dover Publications, New York, 1994.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015736</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[R. Sumner and J. Popovi&#263;. Deformation transfer for triangle meshes. <i>ACM Trans. on Graphics (SIGGRAPH 2004)</i>, 23(3):399--405, 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141962</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[A. Treuille, A. Lewis, and Z. Popovi&#263;. Model reduction for real-time fluids. <i>ACM Trans. on Graphics</i>, 25(3):826--834, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882337</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[A. Treuille, A. McNamara, Z. Popovi&#263;, and J. Stam. Keyframe control of smoke simulations. <i>ACM Trans. on Graphics (SIGGRAPH 2003)</i>, 22(3):716--723, 2003.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531345</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[M. Wicke, M. Stanton, and A. Treuille. Modular bases for fluid dynamics. <i>ACM Trans. on Graphics</i>, 28(3):39:1--39:8, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1218067</ref_obj_id>
				<ref_obj_pid>1218064</ref_obj_pid>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[C. Wojtan, P. J. Mucha, and G. Turk. Keyframe control of complex particle systems using the adjoint method. In <i>Symp. on Computer Animation (SCA)</i>, pages 15--23, Sept. 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SIGGRAPH 2012 Course FEM Simulation of 3D Deformable Solids: A practitioner s guide to theory, discretization 
and model reduction. Part One: The classical FEM method and discretization methodology Eftychios D. 
Sifakis University of Wisconsin-Madison Version 1.0 [10 July 2012]  For the latest version of this 
document, please see: http://www.femdefo.org/ Contents 1 Preface 3 2 Elasticity in three dimensions 
5 2.1 Deformation map and deformation gradient . . . . . . . . . . . . . . 5 2.2 Strainenergyandhyperelasticity 
.................... 8 2.3 Forceandtraction ............................ 9  2.4 TheFirstPiola-Kirchho.stresstensor 
. . . . . . . . . . . . . . . . . 11  3 Constitutive models of materials 14 3.1 Strainmeasures.............................. 
14 3.2 Linearelasticity.............................. 16 3.3 St.Venant-Kirchho.model ....................... 
18 1 3.4 Corotatedlinearelasticity ........................ 19 3.5 Isotropicmaterialsandinvariants.................... 
20 3.6 Neohookeanelasticity .......................... 23  4 Discretization 25 4.1 Energyandforcediscretization 
..................... 25 4.2 Lineartetrahedralelements ....................... 27 4.3 Forcedi.erentials............................. 
30 4.4 Animplicittimeintegrationscheme . . . . . . . . . . . . . . . . . . 32 Chapter 1  Preface Simulation 
of deformable elastic solids has evolved into a popular tool for visual e.ects, games and interactive 
virtual environments. The Finite Element Method has been very popular in this context, especially in 
applications that can bene.t from its versatility in representing elastic bodies with intricate geometric 
features and diverse material properties. Techniques for solids simulation that have been broadly used 
in graphics draw upon a rich, decades-long literature in Galerkin methods, discrete elliptic PDEs and 
continuum mechanics theory. As many of these techniques originated in theoretical and engineering disciplines 
other than graphics and visual computing, it may be somewhat challenging for a practitioner with modest 
theoretical exposure or familiarity with these .elds to navigate some of the most established mechanical 
engineering or computational physics reference textbooks, especially if their goal is to acquire a high-level 
understanding of the basic tools needed for implementing a simulation system. This document aims to provide 
a concise, yet lightweight synopsis of the relevant theory, with adequate attention to implementation 
details from the perspective of a graphics developer. Most of the material referenced in these notes 
resulted from the author s long and rewarding interactions with graduate students at Stanford, UCLA and 
UW-Madison, as well as the experience of the graduate class Introduction to physics-based modeling and 
simulation o.ered at the University of Wisconsin. This document assumes minimal to no exposure to continuum 
mechanics or .nite element discretizations. However, a particular .avor of calculus background is presumed, 
including: Familiarity with functions of several variables, partial derivaties, volume and surface integrals. 
 Some exposure to numerical techniques for solving linear systems of equa­tions, and the Newton-Raphson 
method for .nding approximate solutions to nonlinear problems.  3  A good understanding of linear algebra, 
including concepts such as vectors, matrices and (higher-order) tensors. Familiarity with determinants, 
eigen­value problems and the Singular Value Decomposition is also assumed.  Although many of the proofs 
and derivations are treated as optional reading, the majority of them make heavy use of di.erentials 
(linearized tensors) and reference complex di.erentiation concepts (such as the derivative of a matrix 
function with respect to a matrix argument).  As a supplement to the present introduction to FEM methods 
for deformable solids simulation, the following textbooks are highly recommended: J. Bonet and R. Wood, 
Nonlinear continuum mechanics for Finite Element Analysis, (2nd ed.), Cambridge University Press O. Gonzalez 
and A. Stuart, A .rst course in Continuum Mechanics, Cam­bridge University Press T. Hughes, The Finite 
Element Method: Linear Static and Dynamic Finite Ele­ment Analysis, Dover Publications T. Belytschko, 
W. Lui and B. Moran, Nonlinear Finite Elements for Con­tinua and Structures, Wiley J. Simmonds, A Brief 
on Tensor Analysis, (2nd ed.), Springer-Verlag G. Golub and C. van Loan, Matrix Computations, (3rd ed.), 
Johns Hopkins University Press J. Demmel, Applied Numerical Linear Algebra, SIAM Text in shaded boxes 
presents theoretical proofs, provides examples or provides further insight on the preceding topics. This 
content can be treated as optional reading, and omitting it should not compromise the understanding of 
subsequent topics. Chapter 2 Elasticity in three dimensions In this chapter we focus on three-dimensional 
elastic bodies deforming in space, and discuss how we can formulate quantitative descriptions for the 
deformed shape of an object and the forces resulting from it. To a certain extent, these formulations 
are analogous to similar concepts from mass-spring systems, or deformable elastic strands. However, since 
a volumetric body is able to alter its shape in more complex ways than, for example, a one-dimensional 
elastic strand, many concepts that may be familiar from simpler mechanical systems will need to be extended 
and become more expressive. For the time being, and until chapter 4, we will not concern ourselves with 
discretization issues. Our discussion will focus on the continuous phenomenon of elastic deformation, 
as if we had in.nite resolution at our disposal. 2.1 Deformation map and deformation gradient Our initial 
objective is to provide a concise mathematical description of the defor­mation that an elastic body has 
sustained. This formulation will lay the foundation for appropriate representations of other physical 
properties such as force and en­ergy. We begin by placing the undeformed elastic object in a coordinate 
system, and denote by O the volumetric domain occupied by the object. This domain will be referred to 
as the reference (or undeformed) con.guration, and we follow the con­vention that capital letters X . 
O are used when referring to individual material points in this undeformed shape. Note that the precise 
position and orientation of the undeformed elastic body within the reference space is not important and 
can be chosen at will, as long as the shape of the object corresponds to a rest con.guration. When the 
object undergoes deformation, every material point X is being dis­placed to a new deformed location as 
seen in .gure 2.1 (top) which is, by convention, denoted by a lowercase variable Xx. The relation between 
each material point and its respective deformed location is captured by the deformation function fX: 
R3 . R3 which maps every material point X to its respective deformed location Xx = fX(X ). 5 In some 
cases we also use the notation: .. X X = Y .. Z An important physical quantity derived directly from 
fX(XX), whose utility will become apparent in the next sections, is the deformation gradient tensor F 
. R3×3. T If we write XX=(X1,X2,X3)T and fX(XX)=f1(XX),f2(XX),f3(XX)for the three components of the vector-valued 
function fX, the deformation gradient is written as: . . .f1/.X1 .f1/.X2 .f1/.X3 .(f1,f2,f3) . . F := 
. .f2/.X1 .f2/.X2 .f2/.X3 . .(X1,X2,X3)= .f3/.X1 .f3/.X2 .f3/.X3 or, in index notation Fij = fi,j. That 
is, F is the Jacobian matrix of the deformation map. Note that, in general, F will be spatially varying 
across O; in the next sections we will use the notation F(XX) if such dependence needs to be made explicit. 
Simple examples of deformation .elds The deformation depicted in the top row of .gure 2.1 is indicative 
of ar­bitrary shape changes that are likely to occur in animation tasks. For such instances of deformation 
it would not be possible to write f(XX) in closed form, and simulation would be employed instead to generate 
a numerical approximation. We can provide, however, some intuitive closed-form expressions for certain 
simple examples of deformation scenarios: Figure 2.1(a) depicts a con.guration change which is merely 
a constant trans­lation, say, by a vector Xt. Here, the deformation map and gradient are: Xx = f(XX)= 
XX+ Xt F = .f(XX)/.XX= I Figure 2.1(b) illustrates a scaling by a constant factor ., speci.cally in our 
case a dilation by . =1.5. depicts a con.guration change which is merely a constant translation, say, 
by a vector Xt. In this case, we have: f(XX)= .XXF = .I In .gure 2.1(c) the reference shape has been 
scaled along the horizontal axis by a factor of 0.7, where the vertical axis is stretched by a factor 
of 2. Thus: xX 0.7X 0.70 = f(XX)= f= F = yY2Y 02 The con.guration of .gure 2.1(d) is the result of a 
45. counter-clockwise rotation around the origin. Therefore, we have: x cos45. - sin 45. X cos45. - 
sin 45. = f(XX)= F = y sin 45. cos45.Ysin 45. cos 45. 2.1. DEFORMATION MAP AND DEFORMATION GRADIENT 
 2.2 Strain energy and hyperelasticity One of the consequences of elastic deformation is the accumulation 
of potential en­ergy in the deformed body, which is referred to as strain energy in the context of deformable 
solids. We use the notation E[f] for the strain energy, which suggests that the energy is fully determined 
by the deformation map of a given con.guration. However intuitive, this statement nevertheless re.ects 
a signi.cant hypothesis that led to this formulation: we have assumed that the potential energy associated 
with a deformed con.guration only depends on the .nal deformed shape, and not on the deformation path 
over time that brought the body into its current con.gura­tion. The independence of the strain energy 
on the prior deformation history is a characteristic property of so-called hyperelastic materials (which 
is the only class of materials we will address in this course). This property of is closely related with 
the fact that elastic forces of hyperelastic materials are conservative: the total work done by the internal 
elastic forces in a deformation path depends solely on the initial and .nal con.gurations, not the path 
itself. Di.erent parts of a deforming body undergo shape changes of di.erent severity. As a consequence, 
the relation between deformation and strain energy is better de­.ned on a local scale. We achieve that 
by introducing an energy density function .[f; XX] which measures the strain energy per unit undeformed 
volume on an in­.nitesimal domain dV around the material point XX. We can then obtain the total energy 
for the deforming body by integrating the energy density function over the entire domain O: E[f]= .[f; 
XX]dXX O Let us focus on a speci.c material location XX*. Since the energy density .[f; XX*] would only 
need to re.ect the deformation behavior in an in.nitesimal neighborhood of XX*, we can reasonably approximate 
the deformation map in this tiny region using a .rst-order Taylor expansion: X) f(XX*)+ .f .XX f( X 
 (XX- XX*)= Xx* + F(XX*)(XX- XX*) x X* = F(XX*) X X + X _ _ x* - F(XX*)XX* = F* X X + Xt F* xt This equation 
suggests that .[f; XX*] should be expressible as a function of F* and Xt, as these values fully parameterize 
the local Taylor approximation of f near XX*. Furthermore, we can expect that the value of the vector 
Xt would be irrelevant in this expression: di.erent values of this parameter would indicate deformations 
that di.er only by a constant translation, thus producing the same deformed shape and the same strain 
energy. Thus, we expect that the energy density function should be 2.3. FORCE AND TRACTION expressible 
as .[f; XX] = .(F(XX)), i.e. a function of the local deformation gradient alone. The previous arguments 
have simply established that the energy density func­tion is expected to be a function of the deformation 
gradient. However, we have not provided speci.c formulas for .(F). This is intentional as we want the 
.exi­bility to accommodate a variety of materials. Ultimately, the precise mathematical expression for 
.(F) will be the de.ning property of the material modeled. What would a formula for .(F) look like? Chapter 
3 will provide concrete examples of material models and their associated energy de.nitions. For the time 
being we list a few examples of (largely academic and oversimpli.ed) hypothetical materials. A naturally 
expected property is that the energy is bounded from below, thus minimum-energy states exist where the 
deforming object can settle to. For example: .(F)= k 2IFI 2 F where k> 0. This is an interesting hypothetical 
scenario. We would describe it as a "zero rest­volume material" in analogy to a "zero rest-length spring": 
The minimum energy is attained when F = 0 throughout O, which means that f(XX) = const, i.e. all material 
points have the natural tendency to collapse down to a single point location. Although such a material 
might be useful for glueing tasks, akin to zero rest­length springs, it is unnatural in the sense that 
the reference con.guration O is not an equilibrium con.guration. In order to preserve such a property, 
we would expect that when F = I, corresponding to an undeformed scenario f(XX)= XX, would be a minimum 
of the energy. This could be achieved by setting: .(F)= k 2IF - II 2 F where k> 0. This model will 
have minimum energy when the object is in its reference con­.guration, or a constant translation away 
from it. Unfortunately, this model would not treat a rotation of the undeformed shape as a rest con.guration, 
and the en­ergy would be nonzero in this case. This lack of rotational invariance serves as motivation 
for the material models in the later sections of chapter 3. 2.3 Force and traction The next physical 
concept to be addressed is the elastic force incurred by a given deformation. First, consider a simple 
case from elementary mechanics: a small body g =9.81m/s2 (ideally dimensionless, i.e. a point mass) moving 
in a conservative force .eld. An easy example would be the gravitational .eld, where a body located at 
Xx =(x, y, z) has potential energy E(Xx)= mGX·Xx = mgz, where GX= (0, 0,g) is the acceleration of gravity 
(the z-axis is assumed to be vertical). We can then obtain the gravitational force as the negative gradient 
of the potential energy with respect to the object position Xx: .E(Xx) fX= - = (0, 0, -mg) .Xx However, 
when attempting to express a similar relation between force and energy for deformable bodies we need 
to be cautious of the fact that, in the absence of a prior discretization, such bodies form a continuous 
distribution of material, rather than a collection of isolated point masses. As a consequence, the appropriate 
quan­titative description for elastic forces resulting from deformation would also be via a distribution. 
Thus, we use fX(XX) to denote force density, or more speci.cally force per unit undeformed volume, in 
an in.nitesimal region around XX. The aggregate force on a .nite region A . O would then be computed 
by integrating fXaggregate(A)= fX(XX)dX. X(2.1) A Unfortunately, this description is not appropriate 
for the force exerted by the body along its boundary. Consider an elastic body which is uniformly compressed 
(e.g. f(XX)= aX X, a< 1) to a lower volume. We would expect the body to react by pushing back against 
the apparatus that is causing the compression, and this restorative force would act along the surface 
of contact S . .O. This time we de.ne the traction Xt(XX) to be the (surface) force density function 
that measures the force per unit undeformed area along an in.nitesimal region of the boundary surface 
XX. Once again, the aggregate force on a .nite boundary region B . .O is computed by integrating: . X 
faggregate(B)= Xt(XX)dS. (2.2) B Why treat (interior) force density and (boundary) traction separately? 
Ultimately, don t both of them just refer to standard elastic forces? In loose terms, the reason is that 
on a force-per-volume basis, the net elastic force is quantitatively stronger on the boundary than on 
the interior; the force density would generally be a bounded function on the interior, but might look 
like a Dirac delta function on the boundary. This makes it possible to have a nonzero aggregate force 
along boundary patches, even though those would have had zero volume in an 2.4. THE FIRST PIOLA-KIRCHHOFF 
STRESS TENSOR integral such as (2.1). Instead of dealing with the peculiarities of integrating Delta 
functions just for the sake of having a single force-per-unit-volume descriptor, it makes better practical 
sense to separate force computation into the interior term of equation (2.1) and the boundary term of 
equation (2.2), where the integrands in either case are regular, .nite-valued functions. The question 
that remains is: how is it physically meaningful for elastic forces to have this apparent greater strength 
at the boundary? The important observation here is that fX(XX) is the total force that a point XXreceives 
from its surrounding material, from all directions. Although the force exceeded along each individual 
direction might be substantial, signi.cant cancellation is to be expected when the force contributions 
of all directions are added up. For example, if we stretch a homogeneous material uniformly, each deformed 
material point will receive strong, yet equal (in magnitude) attractive forces along each direction, 
leading to a zero net force. Boundary points, on the other hand, only receive an elastic response from 
their material side, making it easier to accumulate a larger net force. Finally, it is important to note 
that the distinction between force density and traction largely goes away once a discrete representation 
of the deformable body is adopted. In such case, we use nodal forces (instead of densities) as descriptors 
of the elastic material response, and their treatment is practically identical regardless of whether 
they reside on the boundary or interior of the deforming body. 2.4 The First Piola-Kirchho. stress tensor 
The di.erences between interior force density and boundary traction suggest that neither concept is fundamental 
enough to describe all aspects of the elastic response of deforming bodies. There is, however, a fundamental 
force descriptor that both such quantities can be derived from: the stress tensor. There is a variety 
of stress descriptors that can be used for this purpose; for our discussion, we will focus on the 1st 
Piola-Kirchho. stress tensor P,a 3×3 matrix with the following properties: The internal traction at a 
boundary location XX. .O is given by Xt(XX)= -P · NX(2.3) where NXis the outward pointing unit normal 
to the boundary in the reference (undeformed) con.guration. This can serve as a formal de.nition for 
the stress tensor P: for any interior point XX. O \ .O we can hypothetically slice the material with 
a cut through XXand perpendicular to NX, and compute the traction along such a cut. Then, P would be 
the unique matrix that relates Xt and NXas in equation (2.3) for all possible boundary orientations. 
The internal force density can also be computed from P, as follows: 3 3 .Pi1 .Pi2 .Pi3 fX(XX)= divXxP(XX), 
or component-wise: fi = Pij,j = ++. .X1 .X2 .X3 j=1 We emphasize that the divergence operator and/or 
its component derivatives are taken with respect to the undeformed/reference coordinates XX. For hyperelastic 
materials, P is purely a function of the deformation gradient, and is related to the strain energy via 
the simple formula: P(F)= ..(F)/.F As described, the 1st Piola-Kirchho. stress tensor can be used to 
yield formulas both for force and tension, and is readily computed from the strain energy density de.nition. 
In fact, there are two equally popular (and, in fact, equivalent) ways to describe the material properties 
of a hyperelastic material: (a) an explicit formula for . as a function of F, or (b) an explicit formula 
for P as a function of F. We will provide both types of de.nitions for all materials discussed in in 
this document. Example In section 2.2 we listed a hypothetical hyperelastic material with energy density 
.(F )=(k/2)IF - II2 . We are now in a position to give quantitative descriptions F for the force and 
traction such a model would generate in response to deformation. The Piola stress is computed as follows 
.. d.(F )=(k/2)d [(F-I):(F-I)] = k(F-I):dF = .F :dF thus P = ../.F = k(F-I), or component-wise: Pij = 
k(fi,j - dij) From this, internal forces are computed as fi =jPij,j =jkfi,jj = k.fi . fX= k.fX Given 
such a material and appropriate boundary conditions, a rest con.guration would be found by solving fX= 
X0 (in the absence of external forces) or .fX= X0. Lastly, let us assume a uniform expansion by a function 
of 2. Thus f(XX)=2 XX, F =2I, and P = kI. The traction that would result from this stress on a surface 
perpendicular to NXwould be Xt = -kNX(generating boundary forces that trigger inwards motion to restore 
the original shape and volume). 2.4. THE FIRST PIOLA-KIRCHHOFF STRESS TENSOR 13 We note that in much 
of the relevant literature, a di.erent notational convention is followed where fXand Xt refer to the 
externally applied force density and traction, respectively. The relation between these quantities and 
stress is then expressed by assuming that the body is in an equilibrium con.guration, where such externally 
ap­plied forces and tractions balance out exactly the internal elastic force and traction. The equations 
obtained under this convention would be: X X f + divP =0, and Xt = P · N. In this document, we will retain 
our original de.nition where fXand Xt refer to internal forces, along with their respective relations 
to P from earlier in this section, as these formulas hold true even if the deforming body is not in an 
equilibrium con.guration. In cases where we need to refer to any externally applied force or traction 
we will use symbols fXext and Xtext, instead. We provide a brief justi.cation for the formulas relating 
the Piola stress P to force and traction. The intent of the derivations that follow is not to give a 
rigorous proof, but rather to explain the thought process that gave rise to these de.nitions. Consider 
an arbitrary deformation Xx = fX(XX), and a small perturbation dfX(XX) away from it. As the deforming 
body transitions from con.guration fXto the nearby con.guration fX+dfXthe strain energy will be reduced 
by a certain amount dE equal to the work done by the elastic forces: . dE = - fX(XX) · dfX(XX)dXX- Xt 
(XX) · dfX(XX)dS. (2.4) O .O Note that the work is separately integrated in the interior and boundary 
regions, due to the quantitative di.erence of force and traction. The change in strain energy can also 
be expressed as: .. dE = d.(F)dXX= d [.(F)] dXX= .F : dFdXX=[P : dF] dXX OOOO 33 3 3 . = PijdFijdXX= 
Pij · dfi(XX)dXX O O .Xj i,j=1 i,j=1 Using integration by parts, this is equivalently written as . 3 
3 . dE = - [Pij] · dfi(XX)dXX+ PijNj · dfi(XX)dXXi,j=1O .Xj.O . = - divP · dfX(XX)dXX+(P · NX) · dfX(XX)dXX(2.5) 
O .O From equations (2.4, 2.5) and using the fundamental lemma of variational calculus we have that fX(XX)= 
divP, and Xt (XX)= -PNX. .. N1 X.. N = N2 N3 Chapter 3  Constitutive models of materials In this section 
we survey a number of di.erent simulated materials and describe how their physical properties are encoded 
in their respective governing equations. The mathematical description of the physical traits of a given 
material is referred to as its constitutive model and includes the equations that relate stimuli (e.g. 
deformations) to the material response (e.g. force, stress, energy) they trigger. In the spirit of the 
preceding chapter, two possibilities for what a constitutive equation can be are given by the formula 
for the Piola stress P as a function of the deformation gradient F, or the formula for the energy density 
. as a function of F. For simplicity, we will focus on isotropic materials, whose response to deformation 
is independent of the orientation that such deformation is applied in. 3.1 Strain measures In principle, 
an explicit formula that relates . and F (or P and F) would be perfectly adequate as a constitutive equation: 
think of the formula .(F)= IF-II2 from the F previous chapter as an example of this fact. The challenge, 
however, with designing constitutive models in this fashion is that using the raw elements of the matrix 
F can be a very unintuitive way to argue about the .avor and severity of a given deformation. Perhaps 
a certain material s response is dominated by its a.nity for volume conservation, while a di.erent material 
might prioritize resistance to shear. One would imagine that metrics such as the ratio of volumetric 
expansion or the shear angle would be much more e.ective in expressing the severity of the types of deformation 
that are most relevant to such materials. As a consequence, it is common for the design process for constitutive 
models to de.ne certain intermediate quantities (examples of which are strain measures and invariants, 
discussed in this chapter) which are derived from F, yet capture the speci.c traits of the deformation 
that the energy or stress values depend on more concisely than the deformation gradient itself. 14 3.1. 
STRAIN MEASURES A strain measure is intended to be a quantitative descriptor for the severity of a given 
deformation, i.e. a way to gauge how far this con.guration is from a rest con.guration. For this reason, 
although strain measures are derived from the deformation gradient, they strive to retain as much information 
from it that is relevant to assessing deformation magnitude while disregarding any information contained 
in it that is unrelated to shape change. Consider the Green strain tensor E . R3×3, de.ned as: 1 E = 
FT F - I . (3.1) 2 The Green strain tensor exempli.es many of the properties that we would ask for in 
a strain measure. When the body is in its reference con.guration, i.e. fX(XX)= XX, we have F = I and 
thus E = 0. The Green strain would also be zero if the elastic body is merely rotated and translated 
from its reference position, without changing its shape; in such a case fX(XX)= RXX+Xt (where R is a 
rotation matrix), thus F = R and E = 0 since RT R = I. More generally, even for non-rigid motions, the 
deformation gradient can be decomposed as F = RS into the product of a rotation matrix R, and a symmetric 
factor S via the polar decomposition. As a 3D rotation matrix R encapsulates three degrees of freedom, 
while the symmetric S has 6 independent degrees of freedom. Substituting the polar decomposition into 
equation (3.1) we obtain: 1 E = S2 - I . 2 Thus, the Green strain succeeds in discarding the rotational 
degrees of freedom, which have no bearing on the severity of deformation, and retains the stretch/shear 
information in the 6-DOF symmetric factor S. This is also accomplished without explicitly forming the 
polar decomposition. The price one has to pay for the useful properties the Green strain o.ers, is that 
the expression of equation (3.1) is a nonlinear (quadratic) function of deformation. This increases the 
complexity of constitutive models that are constructed based on it and, as we will see next, will lead 
to discretizations with nodal forces being nonlinear functions of nodal positions. In an e.ort to remedy 
this, we construct a linear approximation of equation (3.1) by forming a Taylor expansion around the 
undeformed con.guration F = I. .E E(F) E(I)+ :(F - I) _ __ _ .F F=I =0 The derivative .E/.F is most 
conveniently de.ned via the di.erential dE: .E 1 .F : dF = dE = dFT F + FT dF 2 Thus .E 11 :(F - I)= 
(F - I)T I + IT (F - I)= F + FT - I .F F=I22 The matrix resulting from this linear approximation of E(F) 
is denoted by E, where: 1 E = F + FT - I 2 and called the small strain tensor, or the in.nitesimal strain 
tensor. This strain tensor will give rise to a computationally lightweight constitutive model called 
linear elasticity, described in the next section, and enable discretizations which have a linear mapping 
between nodal positions and nodal elastic forces. As expected, this convenience comes with a certain 
limitation: the small strain tensor can be considered a reliable measure of deformation for small motions 
only (hence the name) while pronounced artifacts will occur if used in a large deformation scenario. 
 3.2 Linear elasticity The simplest practical constitutive model is linear elasticity, de.ned in terms 
of the strain energy density as: . .(F)= µE:E + 2tr2(E) (3.2) where E is the small strain tensor and 
µ, . are the Lamé coe.cients, which are related to the the material properties of Young s modulus k (a 
measure of stretch resistance) and Poisson s ratio . (a measure of incompressibility) as: k k. µ = . 
= 2(1 + .) (1+ .)(1 - 2.) Sym{A} is The relation between the Piola stress P and F can be derived as follows: 
the symmetric component of 1 matrix A dE = dF + dFT = Sym{dF} 2 Due to the E:dE = E:Sym{dF} = E:dF tr(dE)= 
I:Sym{dF} = I:dF symmetry of E and I d.=2µE:dE + .tr(E)tr(dE) = [2µE + .tr(E)I]: dF =../.F Thus P =2µE 
+ .tr(E)I = ..Since P .F or, after one .nal substitution for E (and a few algebraic reductions): P(F)= 
µ(F + FT - 2I)+ .tr(F - I)I. 3.2. LINEAR ELASTICITY 17 These expressions allow us to make the following 
observations: The stress P is a linear function of the deformation gradient. As we will see in chapter 
4 this would also result on the nodal elastic forces having a linear dependence on nodal positions. As 
a consequence, this constitutive model is characterized by a signi.cantly lower computational cost than 
other, nonlinear materials.  Since the small strain tensor was designed to be accurate exclusively in 
a small deformation scenario, it would only be advisable to use linear elasticity when the magnitude 
of motion is small. For example, a rigid motion fX(XX)= RXX+Xt would generally produce a non-zero strain 
E = 12(R + RT ) - I and ultimately a nonzero stress, even though no shape change has taken place.  The 
Partial Di.erential Equation form of linear elasticity For this simple material model it is relatively 
straightforward to derive the dif­ferential equation that de.nes an equilibrium con.guration. Assume 
an externally applied force distribution fXext(XX). When the object has settled to an equilibrium (rest) 
con.guration, the deformation function will satisfy: 33 (i) divP+fXext = 0 . Pij,j +fext = 0 . [for i 
=1, 2, 3] j=1 3 3 3 . 3 (i).- µ(fi,j + fj,i - 2dij)+ di,j .(fk,k - 1) = fext . .Xj j=1 k=1 3 3 33 (i).- 
µ(fi,jj + fj,ij)+ di,j .fk,kj = fext . j=1 k=1 3 3 3 3 (i) .- [µ(fi,jj + fj,ji)] - .fk,ki = f ext . j=1 
k=1 33 (i) .- [µfi,jj +(µ + .)fj,ji]= fext . j=1 . (i) .-µ.fi - (µ + .)[\· fX]= f ext . .Xi .-µ.fX- (µ 
+ .)\[\· fX]= fXext Which is a linear, second order Partial Di.erential Equation. fj,ij = fj,ji Summation 
variables j and k are in­terchangable 3.3 St. Venant-Kirchho. model With the understanding that the 
small strain tensor is a mere approximation of the rotationally invariant Green strain E, it makes sense 
to attempt an improvement of the linear elasticity model by using E in the place of E in equation (3.2): 
. .(F)= µE:E + 2tr2(E) The resulting constitutive model is called a St. Venant-Kirchho. material, and 
is the .rst truly nonlinear material we will examine. The .rst Piola-Kirchho. stress tensor can be computed 
via a process similar to the one followed for linear elasticity: 1 dE = dFT F + FT dF = Sym{FT dF} 2 
 Due to the E:dE = E:FT dF= {FE}:dF tr(dE)= I:FT dF= F:dF symmetry of E and I d.=2µE:dE + .tr(E)tr(dE)= 
F[2µE + .tr(E)I]: dF =../.F Thus P(F)= F[2µE + .tr(E)I] . (3.3) This is a rotationally invariant model; 
deformations that di.er by a rigid body transformation are guaranteed to have the same strain energy. 
As a consequence a St. Venant-Kirchho. material exhibits plausible material response in many large deformation 
scenarios where linear elasticity would not be applicable. Equation (3.3) indicates that stress is a 
3rd degree polynomial function of the components of F; after discretization, nodal forces will likewise 
be expressed as cubic polynomials of nodal positions. Although the St. Venant-Kirchho. model o.ers signi.cant 
bene.ts over a linear elastic model, its scope is limited to a certain degree due to its poor resistance 
to forceful compression: as a St. Venant-Kirchho. elastic body is compressed, starting from its undeformed 
con.guration, it reacts with a restorative force which initially grows with the degree of compression. 
However, once a critical compression threshold is reached ( 58% of undeformed dimensions, when compression 
occurs along a single axis) the strength of the restorative force reaches a maximum. Further compression 
will be met with decreasing resistance, in fact the restorative force will vanish as the object is compressed 
all the way down to zero volume (an indication of this is that when F = 0 we also have P = 0). Continued 
compression past the point of zero volume (forcing the material to invert) will then create a restorative 
force that pushes the body towards complete inversion (re.ection) along one or more axes. In practical 
computer simulation examples this behavior often manifests itself as a tendency of the material to locally 
tangle and invert itself when subjected to strong compressive forces or kinematic constraints. 3.4. 
COROTATED LINEAR ELASTICITY 3.4 Corotated linear elasticity The use of the quadratic Green strain in 
the St. Venant-Kirchho. guaranteed the rotational invariance of the constitutive model. At the same time, 
the increased complexity inherent in highly nonlinear materials leads to unintended side e.ects, such 
as the non-physical zero stress con.gurations of St. Venant-Kirchho. materials under extreme compression. 
Corotated linear elasticity is a constitutive model that attempts to combine the simplicity of the stress-deformation 
relationship in a linear material with just enough nonlinear characteristics to secure rotational invariance. 
Using the polar decomposition F = RS we construct a new strain measure as Ec= S - I, which is linear 
on the symmetric tensor S obtained by factoring away the rotational component of F. Replacing the small 
strain tensor in equation (3.2) we obtain the energy for corotational elasticity: . .(F)= µEc:Ec + 2tr2(Ec) 
= µIS - II2 +(./2)tr2(S - I) F which can also be equivalently written in any of the following ways: .(F)= 
µIF - RI2 +(./2)tr2(RT F - I) F .(F)= µIS - II2 +(./2)tr2(S - I) (3.4) F where S is the diagonal matrix 
with the singular values of F, from the Singular Value Decomposition F = USVT . We can show that the 
1st Piola-Kirchho. stress tensor for corotated linear elasticity is given by: P(F)= R [2µEc+ .tr(Ec)I]= 
R [2µ(S - I)+ .tr(S - I)I] =2µ(F - R)+ .tr(RT F - I)R (3.5) Proof of the stress formula Taking di.erentials 
of the Singular Value Decomposition F = USVT we have: dF =(dU)SVT + U(dS)VT + USdVT . . UT (dF)V =(UT 
dU)S+dS + S(VT dV)T (3.6) (*)(**) For any orthogonal matrix Q we have QT Q = I . d(QT Q)= 0 . (dQ)T Q 
+ QT dQ = 0 . (QT dQ)T = -QT dQ Thus, the matrices marked with (*) and (**) above are column-and row-scaled 
versions of skew symmetric matrices, and consequently have zero diagonal elements. This implies that 
if we restrict equation (3.6) to its diagonal component only, terms (*) and (**) will vanish to yield 
the .nal expression for the di.erential of S: dS = Diag{UT (dF)V} Using this result, the di.erential 
of equation (3.4) becomes: d.=2µ(S - I):dS + .tr(S - I)tr(dS) =2µ(S - I):(UT dFV)+ .tr V(S - I)VT tr(UT 
dFV) =2µ U(S - I)VT :dF + .tr(S - I)tr (UVT )T dF =2µ(F - R):dF + .tr(S - I)R:dF = P : dF from which 
equation (3.5) follows. The motivation behind corotational elasticity is to mimic what linear elasticity 
would have been, if the undeformed con.guration had been rotated in the same way as encoded in the rotational 
factor R from the polar decomposition. Of course, in typical deformations where the value of R varies 
across the domain, making the transition from linear to corotated elasticity more complex than a change 
of variables due to a (constant) rotation of the undeformed con.guration. From a computational cost perspective, 
the overhead of corotated vs. linear elasticity includes the cost of the polar decomposition, and the 
need to employ nonlinear solvers for certain types of simulation. 3.5 Isotropic materials and invariants 
The constitutive models of St. Venant-Kircho. and Corotated linear elasticity have been constructed to 
be rotationally invariant. We can formally de.ne this property by considering a pair of deformations, 
denoted by their deformation maps fX1(XX) and fX2(XX), that di.er only by a rigid body transform, speci.cally: 
X f2(XX)= RfX1(XX)+ Xt, where R is a 3 × 3 rotation matrix. (3.7) A constitutive model is rotationally 
invariant if and only if it guarantees that the strain energy will satisfy E[f1]= E[f2] for any such 
deformation pair. For hyperelastic materials, an equivalent de.nition can be stated in terms of the strain 
energy density function. By taking gradients, we can see that any two deformations that satisfy equation 
(3.7) will have deformation gradients related as F2 = RF1. The energy density associated with these deformations 
must satisfy .(F1) = .(F2), leading to the following equivalent de.nition of rotational invariance: 3.5. 
ISOTROPIC MATERIALS AND INVARIANTS De.nition: A hyperelastic constitutive model is rotationally invariant 
if and only if the strain energy density satis.es .(RF) = .(F) for any value of the deformation gradient 
F and any 3 × 3 rotation matrix R. A consequence of this de.nition is that the strain energy in rotationally 
invariant models can be expressed solely as a function of the symmetric factor S from the polar decomposition 
of F = RS, since: .(F) = .(RS) = .(S) Although some model may be de.ne . directly as a function of S 
(corotated elasticity was presented this way), we may avoid the need to compute the polar decomposition, 
if we are able to express . as a function of some other intermediate quantity, which is a function of 
S, yet also computable without an explicit polar decomposition. For example, St. Venant-Kirchho. materials 
de.ned the energy 1 density as a function of the Green strain E = 2(S2 - I), which although fully determined 
by S can also be computed without an explicit polar decomposition as E = 12(FT F - I). A similar, yet 
distinct property of certain constitutive models (including St. Venant-Kirchho. and Corotated linear 
elasticity) is that of isotropy. In plain terms, a material is isotropic if its resistance to deformation 
is the same along all possi­ble orientations that such deformation may be applied. Rubber and metal would 
be examples of isotropic materials, as they do not exhibit any particular direc­tion/orientation along 
which are softer or sti.er. Steel-reinforced concrete would be an example of an anisotropic material, 
as its resistance to deformation is notably di.erent along the direction of the steel supports, compared 
to a direction perpen­dicular to them. Human muscles are also quoted as an anisotropic structure, as 
a distinct material response is observed along the direction aligned with muscle .bers. Isotropy is a 
property that needs to be assessed on a local scale, as it is always possible to generate directional 
features in larger structures by arranging material in speci.c ways (think of suspension bridges built 
from otherwise isotropic steel). In terms of a quantitative criterion for isotropy, we can think of an 
in.nitesimal spherical volume of material dV , and consider the strain energy resulting from a prescribed 
deformation. Now, consider the scenario where we .rst transform the sphere dV by rotating it about its 
center and then apply the same deformation. If the material is isotropic, both scenarios would lead to 
the same strain energy. This is concretely expressed using the strain energy function, as follows: De.nition: 
A hyperelastic constitutive model is isotropic if and only if the strain energy density satis.es .(FQ) 
= .(F) for any value of the deformation gradient F and any 3 × 3 rotation matrix Q. A material that is 
both rotationally invariant and isotropic would satisfy .(RFQ) = .(F) for arbitrary rotations R and Q. 
Using the Singular Value Decomposition F = USVT we conclude that rota­tionally invariant, isotropic materials 
satisfy: .(F) = .(USVT ) = .(S). While the strain energy for rotationally invariant materials was a function 
only of 6 out of 9 degrees of freedom in F (those captured in the symmetric S), for materials that are 
also isotropic the energy density is actually only a function of the three singular values of F. Equation 
(3.4) reveals that this is certainly the case for corotated linear elasticity. St. Venant-Kirchho. can 
also be shown to satisfy all criteria for isotropy, after some simple algebraic manipulations. An example 
of a material that is rotationally invariant but not isotropic is described by the energy: .(F)= kw T 
FT FwX 2 Xwhere wXis a given constant vector. This material behaves like a zero-restlength spring along 
the direction wX, while it does not have any resistance to deformation along directions perpendicular 
to wX. Although it is possible to de.ne an isotropic material by a relation between . and S (which encodes 
the only 3 relevant degrees of freedom in F), this is not necessarily the preferred approach, since the 
overhead of an SVD computation would be necessary when evaluating any of these quantities. St. Venant-Kirchho. 
materials avoided the need for an explicit polar decomposition, by using the Green strain E to convey 
(qualitatively) the same information as S, while using a computationally inexpensive formula. For isotropic 
materials, the purpose is served by the three isotropic invariants of the deformation gradient, which 
are equally expressive as the singular values, but can be computed inexpensively. Invariants are denoted 
by I1,I2,I3 (or I1(F), etc., to emphasize the dependence on F) and de.ned as: I1(F) = tr(FT F),I2(F)= 
tr (FT F)2 ,I3(F) = det(FT F) = (det F)2 3.6. NEOHOOKEAN ELASTICITY Their relation to S is revealed 
by replacing F with its SVD in the previous expressions, where (after extensive cancellation) we obtain: 
333 33 3 I1 = tr(S2)= si 2 ,I2 = tr(S4)= si 4 ,I3 = det(S2)= s2 i i=1 i=1 i=1 Also of use are the derivatives 
of the invariants with respect to the F: dI1 = d[tr(FT F)] = 2tr(FT dF) = (2F):dF . .I1 =2F .F dI2 = 
d[tr(FT FFT F)] = 4tr(FT FFT dF) = (4FFT F):dF . .I2 =4FFT F .F .I3 dI3 = d[(det F)2] = 2det F · d[det 
F] = 2(det F)2F-T :dF . .F =2I3F-T When the common practice of de.ning an isotropic constitive model 
via invari­ants is followed, the strain energy density is provided as a function .(I1,I2,I3). In such 
case, we can use the chain rule to compute the stress P as: ..(I1,I2,I3) .. .I1 .. .I2 .. .I3 P = = + 
+ or, after substitution: .F .I1 .F .I2 .F .I3 .F , .. .. P(F)= .. · 2F + · 4FFT F + · 2I3F-T (3.8) .I1 
.I2 .I3 Finally, we note the additional invariant J = det F = v I3 that is often used in replacement 
of I3 while de.ning certain constitutive models. This quantity has an important physical interpretation 
as it represents the fraction of volume change due to deformation: a value of J = 1 implies that volume 
is preserved exactly while, J = 2 would indicate an expansion to twice the undeformed volume and J =0.2 
would be a compression down to 20% of the rest volume.  3.6 Neohookean elasticity An example of an isotropic 
constitutive model de.ned via isotropic invariants is Neohookean elasticity: .(I1,J)= µ 2(I1 - 3) - µ 
log(J)+ . 2 log2(J), or equivalently .(I1,I3)= µ 2(I1 - log(I3) - 3) + . 8 log2(I3) From this de.nition, 
we can easily compute .. µ.. µ. log(I3) = and = - + .I1 2 .I3 2I3 4I3 Thus, using equation (3.8) we obtain: 
. log(I3) P(F)= µF - µF-T + F-T 2 or P(F)= µ(F - µF-T )+ . log(J)F-T . The Neohookean model has the following 
notable characteristics: By construction, the material exhibits a very strong reaction to extreme com­pression. 
Due to the logarithmic term log2(J) in the energy, as J . 0 we have . .8. This constructs a powerful 
energy barrier that strongly resists extreme compression. This is the only constitutive model we have 
seen so far that has this property; models discussed earlier in this chapter will allow the material 
to compress to zero volume, even invert, while only absorbing a .nite amount of energy.  Modeling materials 
as strongly incompressible amounts to using a very large value for the second Lamé coe.cient (.). Doing 
so in the case of Neohookean elasticity would emphasize the log2(J) energy term, and strongly enforce 
J =1 which produces an volume-preserving formulation. Incidentally, setting a high value for . in the 
earlier constitutive models does not quite have the desired e.ect, as their respective terms scaled by 
. do not correspond to true volume change (as J does). For example, a high . value for linear elasticity 
would enforce  X tr(F - I)=0 . div f(XX) - XX=0 i.e. this will ensure that the displacement .eld Xx(XX) 
- XXis divergence free. This condition approximates volume preservation only for small deformations. 
The fact that the strain energy de.nes a (theoretically) impassable barrier at compression magnitudes 
leading to zero volume J = 0 implies that there is no mechanism for handling what happens when, accidentally, 
the simulated model is forced into a (theoretically impossible) inverted con.guration. In such cases, 
energy and stress are unde.ned, since J< 0. We note that such inversions (although theoretically impossible) 
can easily occur in practice, as a result of nonphysical kinematic constraints, instability of time integration 
techniques, or inadequate convergence of numerical solvers. Should such a scenario arise, it is advised 
that the deformation gradient F be temporarily replaced by the nearest physically plausible value F (with 
det F >E). Chapter 4  Discretization The preceding chapters detailed a variety of physical laws that 
may be used to describe the response of elastic materials to deformation. Up to this point, these laws 
were expressed relative to a continuous deformation in space. Naturally, in order to enable numerical 
simulation all such laws have to be discretized; physical quantities such as the deformation map, the 
elastic strain energy, stress tensors and elastic forces all have to be reformulated as functions of 
our discrete state variables. 4.1 Energy and force discretization When modeling a deformable body on 
the computer we only store the values of the deformation map f(XX) on a .nite number of points XX1, XX2, 
. . . , XXN , corresponding to the vertices of a discretization mesh. The respective deformed vertex 
locations Xxi = f(XXi),i =1, 2,...,N are our discrete degrees of freedom, and we can write x =(Xx1, Xx2, 
. . . , XxN ) for the aggregate state of our model. As a .rst step, we need to specify a method for reconstructing 
a continuous deformation map f from the discrete samples Xxi = f(XXi). In essence this is just a choice 
of an interpolation scheme. For example, if a tetrahedral mesh is used to describe the deforming body, 
barycentric interpolation will extend the nodal deformations to the entire interior of the mesh. Trilinear 
interpolation would be a natural choice for lattice discretiza­tions. At any rate, we denote the interpolated 
deformation map by f (XX; x) which emphasizes that this interpolated deformation is dependent on the 
discrete state x. For a hyperelastic material, the strain energy of any given deformation f(Xx) is computed 
by integrating the energy density . over the entire body O: E[f] := .(F)dXX O We can now de.ne a discrete 
energy, expressed as a function of the degrees of freedom x, by simply plugging the interpolated deformation 
f into the de.nition of 25 the strain energy E(x) := Ef (XX; x)= . F (XX; x) dXX(4.1) O where F (XX; 
x) := .f (XX; x)/.XXis the deformation gradient computed from the interpolated map f . It is understandable 
that equation (4.1) may appear quite cryptic at this point, since both the energy .(F) and the interpolated 
f are likely de.ned via complex formulas. In this chapter we will focus on two common spatial discretizations 
(tetrahedral meshes and Cartesian hexahedral lattices) and explain how the energy in equation (4.1) and 
all its derived quantities can be evaluated systematically and e.ciently. Having de.ned the discrete 
energy E(x) we can now compute the elastic forces associated with individual mesh nodes, by taking the 
negative gradient of the elastic energy with respect to the corresponding degree of freedom: X.E(x) XX.E(x) 
fi(x)= - or, collectively f := (fX1,f2,..., fN )= - .Xxi .x Outline of a brief (albeit over-simpli.ed) 
proof ... For simplicity, let us assume that (a) the deforming body is not subject to any internal friction 
forces which would reduce its overall energy, and (b) the mass of the body is distributed exclusively 
to the mesh nodes. The total energy of the body is the sum of strain energy (E) and kinetic energy (K) 
as follows: N 3 1 Etotal = E(x)+ K(v)= E(x)+ 2miIXviI2 i=1 Since no friction forces are in e.ect the 
total energy is conserved over time, thus: N 3 . .E(x) Etotal = 0 .· Xvi + miXai · Xvi =0 .t .Xxi i=1 
Since the last equality holds for any value of the velocities {Xvi}, we must have: .E(x) .E(x) + miXai 
=0 . fXi = miXai = - for all i =1, 2,...,N .Xxi .Xxi In practice, prior to computing each force, we .rst 
separate the energy integral of equation (4.1) into the contributions of individual elements Oe (e.g. 
triangles, 4.2. LINEAR TETRAHEDRAL ELEMENTS hexahedra, etc.) as follows:  33 E(x)= Ee(x)= . F(XX; x) 
dXX ee Oe Subsequently, the force fXi on each node can be computed by adding the contributions of all 
elements in its immediate neighborhood Ni: 3 .Ee(x) fXi(x)= fXie(x), where fXie(x)= - .Xxi e.Ni For simplicity, 
the following sections will focus on computing the nodal forces on an element-by-element basis, with 
the understanding that the aggregate forces are computed by accumulating the contributions from all elements 
in the mesh. 4.2 Linear tetrahedral elements Tetrahedral meshes are among the most popular discrete 
volumetric geometry rep­resentations. At the same time, they o.er one of the most straightforward options 
for constructing a discretization of the elasticity equations. The convenience of tetrahedral discretizations 
is largely due to the simple interpolation method they imply; the reconstructed deformation map f can 
be de.ned to be a piecewise linear function over each tetrahedron. Speci.cally, in every tetrahedron 
Ti we have f (XX)= AiXX+Xbi for all XX.Ti (4.2) where the matrix Ai . R3×3 and the vector Xbi . R3 are 
speci.c to each tetrahe­dron. The interpolation scheme implied by equation (4.2) is no other than simple 
barycentric interpolation on every element. Di.erentiating (4.2) with respect to XXreveals that the deformation 
gradient F = . X is constant on each ele­ f/. X= Ai ment, and as a consequence so will be any discrete 
strain measure and stress tensor; this justi.es why linear tetrahedral elements are also referred to 
as constant strain tetrahedra. For simplicity of notation we write f(XX)= FXX+Xb where we dropped the 
tetrahedron index, and replaced matrix Ai with its equal deformation gradient. Interestingly, it is possible 
to determine F (and Xb, if de­sired) directly from the locations of the tetrahedron vertices, without 
involving any reasoning related to barycentric interpolation. Let us denote with XX1, . . . , XX4 the 
undeformed (reference) locations of the tetrahedron vertices, and let Xx1, . . . , Xx4 symbolize the 
respective deformed vertex locations as illustrated in .gure 4.1. Each vertex must satisfy Xxi = f(XXi), 
or . .. . Xx1 = FXX1 +Xb Xx1 - Xx4 = F XX1 - XX4 . . . . . . . . Xx2 = FXX2 +Xb . Xx2 - Xx4 = F XX2 
- XX4Xx3 = FXX3 +Xb . . Xx3 - Xx4 = F XX3 - XX4 . . . . . . Xx4 = FXX4 +Xb where the last system was 
derived by subtracting the equation Xx4 = FXX4+Xb from the three others, to eliminate the vector Xb. 
It is possible to group the last three (vector) equations as a single matrix equation, by placing each 
one into the respective column of a 3 × 3 matrix: Xx1 - Xx4 Xx2 - Xx4 Xx3 - Xx4 = F XX1 - XX4 F XX2 - 
XX4 F XX3 - XX4 Xx1 - Xx4 Xx2 - Xx4 Xx3 - Xx4 = F XX1 - XX4 XX2 - XX4 XX3 - XX4 Ds = FDm (4.3) where 
 . . x1 - x4 x2 - x4 x3 - x4 .. .. Ds := (4.4) y1 - y4 y2 - y4 y3 - y4 z1 - z4 z2 - z4 z3 - z4 is the 
deformed shape matrix and . . X1 - X4 X2 - X4 X3 - X4 .. Y1 - Y4 Y2 - Y4 Y3 - Y4 Z1 - Z4 Z2 - Z4 Z3 
- Z4 .. Dm := is called the reference shape matrix (or material-space shape matrix). 4.2. LINEAR TETRAHEDRAL 
ELEMENTS 29 We note that Dm is a constant matrix, as it only depends on the vertex coor­dinates in the 
reference (undeformed) con.guration; Furthermore, the undeformed 1 volume of the tetrahedron equals W 
= 6| det Dm|; assuming that the reference shape of the tetrahedron is non-degenerate (i.e. nonzero volume, 
W 0), the = matrix Dm is nonsingular and equation (4.3) can be solved for F as: F = DsD-1 or F(x)= Ds(x)D-1 
(4.5) mm where the last expression emphasizes that the deformed degrees of freedom appear only in the 
expression for Ds; while the constant D-1 is precomputed and stored. m Since F is constant over the linear 
tetrahedron, the strain energy of this element reduces to: Ei = .(F)dXX= .(Fi) dXX= W · .(Fi) or E(x)= 
W · .(F(x)) (4.6) Ti Ti We may subsequently use Equation (4.6) to derive the contribution of element 
Ti to the elastic forces on its four vertices as fXi = -.Ei(x)/.Xxk. In fact, the forces on k all four 
vertices can be collectively computed via the following equations: XXXX H = f1 f2 f3 = -W P(F)D-T and 
f4 = -fX1 - fX2 - X(4.7) m f3 Proof (1) (2) (3) De.ne x,x,xto be the x-, y-and z-coordinates of the vertex 
Xxi. iii (1) (2) (3) Likewise for the components of the nodal force fXi =(fi ,f,fi ) i Lemma For i =1, 
2, 3 (j) D-1 .F/.x= ejeT i im (j) Proof From equation (4.3) we have .Ds/.x= ejeTi . The Lemma follows 
i directly from this equation and F = DsD-1. m We proceed to compute the force component: (j) .E(x) ..(x) 
.F Hji = f= - = -W := -W P(F): ejeT D-1 = m i (j)(j) i .x.F .x ii eieT = -W eT P(F)D-T -W P(F)D-T = -W 
tr P(F)D-mT j ei = m jm ji Thus H = -W P(F)D-mT . The equation fX4 = -fX1 - fX2 - fX3 can be proved in 
a directly similar fashion, but is also a consequence of conservation of momentum; if the sum of all 
four nodal forces (which are internal to the body) did not sum to zero, this would violate conservation 
of linear momentum. P(F) is the Piola stress de.ned in section 2.4 The computation of all elastic forces 
in a tetrahedral mesh is summarized in pseudocode as follows: Algorithm 1 Batch computation of elastic 
forces on a tetrahedral mesh 1: procedure Precomputation(x, Bm[1 ...M],W [1 ...M]) for each Te . .. 
=(i, j, k, l) .M do Xi - Xl Xj - Xl Xk - Xl Yi - Yl Yj - Yl Yk - Yl .M is the number of tetrahedra . 
.. 2: 3: Dm . Zi - Zl Zj - Zl Zk - Zl 4: Bm[e] . D-1 m 5: W [e] . 61 det(Dm) .W is the undeformed volume 
of Te 6: end for 7: end procedure 8: procedure ComputeElasticForces(x, f, M, Bm[],W []) 9: f . 0 . M 
is a tetrahedral mesh for each Te . .. =(i, j, k, l) .M do 10: . xi - xl xj - xl xk - xl yi - yl yj 
- yl yk - yl zi - zl zj - zl zk - zl .. 11: Ds . 12: F . DsBm[e] 13: P . P(F) . From the constitutive 
law 14: H .-W [e]P(Bm[e])T XXXX 15: fi += Xh1, fj += Xh2, fk += Xh3 . H = h1 Xh2 Xh3 16: fXl += (-Xh1 
- Xh2 - Xh3) 17: end for 18: end procedure  4.3 Force di.erentials We have seen how discrete nodal 
forces (f) can be computed for an arbitrary con­stitutive model, given nodal positions (x) as input. 
This is all that is necessary to implement an explicit (e.g. Forward Euler) time integration scheme; 
however im­plicit methods such as Backward Euler will also require a process for computing force di.erentials, 
i.e. linearized nodal force increments around a con.guration x*, relative to a small nodal force displacement 
dx. We denote this by: df = df(x*; dx) := .f · dx .x x=x* Although in this expression we used the sti.ness 
matrix .f/.x to aid in the de.­nition of the force di.erential, in practice it may be preferable to avoid 
constructing 4.3. FORCE DIFFERENTIALS this matrix explicitly, as the construction cost and memory footprint 
associated with it may impact performance. Instead, we aim to compute the force di.erentials df directly, 
using only the information in the current state x*, the displacement dx and a small amount of additional 
meta-data. As was the case with force computation, we evaluate the force di.erential vector df =(dfX1, 
d fX2, . . . , d fXN ) on an element-by-element basis, accumulating the contri­bution of each element 
to the aggregate value of each of its nodes. Consequently, we only focus on the process for computing 
di.erentials of nodal forces for a sin­gle tetrahedron. As before, we can pack the di.erentials of the 
.rst three vertices (dfX1, d fX2 and dfX3) in a single matrix representation: dH = dfX1 dfX2 dfX3 Once 
dH has been evaluated, the force di.erential for the fourth node can be computed as dfX4 = -dfX1 - dfX2 
- dfX3. Taking di.erentials on equation (4.7) we obtain the following expression for dH: dH = -WdP(F; 
dF)D-T m Thus, the computation of nodal force di.erentials has been reduced to a computa­tion of the 
stress di.erential dF. There are two steps in completing this evaluation: (a) we need to construct the 
deformation gradient increment dF (the deformation gradient F itself is computed as detailed in the previous 
section) and (b) we need to provide a usable formula for dP(F; dF). We start with the di.erential of 
the deformation gradient dF, which is easily computed by taking the di.erentials on equation (4.5) to 
obtain: dF =(dDs)D-1 m Matrix dDs itself is simply computed by arranging the nodal displacements in the 
same fashion as nodal positions were for Ds: . . dx1 - dx4 dx2 - dx4 dx3 - dx4 dDs := .. dy1 - dy4 dy2 
- dy4 dy3 - dy4 dz1 - dz4 dz2 - dz4 dz3 - dz4 .. The one remaining task is to provide a concise formula 
for dP(F; dF). By necessity, this will be a process that depends on the constitutive model itself. Here, 
we provide examples of this derivation for the St. Venant-Kirchho., and Neohookean material models: 
Stress di.erentials for St. Venant-Kirchho. materials We start by assessing the di.erential of the Green 
strain tensor: 11 E =(FT F - I) . dE =(dFT F + FT dF) 22 We then proceed to compute the di.erential of 
the stress tensor itself: P(F)= F[2µE + .tr(E)I] . dP(F; dF)= dF[2µE + .tr(E)I]+ F[2µdE + .tr(dE)I] 
Stress di.erentials for Neohookean materials We will use without proof the following two expressions 
for the di.erential of the matrix inverse and matrix determinant: d[F-1]= -F-1dFF-1 , also d[F-T ]= -F-T 
dFT F-T d[det F] = det F · tr(F-1dF) With these results, the di.erential of P is computed as P(F)= µ(F 
- F-T )+ . log(J)F-T . d[det F] . dP(F; dF)= µ(dF + F-T dFT F-T )+ . F-T - . log(J)FT dFT F-T J . dP(F; 
dF)= µdF +[µ - . log(J)] F-T dFT F-T + .tr(F-1dF)F-T The force di.erential computation is summarized 
in pseudocode as Algorithm (2).  4.4 An implicit time integration scheme We are now in a position to 
describe a complete, implicit-time integration scheme for nonlinear elastic bodies. The formulation that 
follows is based on the Backward Euler method, and thus is unconditionally stable for any timestep .t 
(subject to the nonlinear equations involved being solved to satisfactory accuracy). We will .rst introduce 
some notation: fe(x*) : Elastic forces at con.guration x*, as de.ned in previous sections. 4.4. AN IMPLICIT 
TIME INTEGRATION SCHEME Algorithm 2 Batch computation of elastic force di.erential on a tetrahedral mesh. 
Assumes that the precomputation routine from algorithm 1 is also available. 1: procedure ComputeForceDifferentials(x, 
f,dx,df, M, Bm[],W []) 2: f . 0 . M is a tetrahedral mesh for each Te . .. =(i, j, k, l) .M do 3: . 
xi - xl xj - xl xk - xl .. 4: Ds . yi - yl yj - yl yk - yl zi - zl zj - zl zk - zl .. dxi - dxl dxj - 
dxl dxk - dxl dyi - dyl dyj - dyl dyk - dyl dzi - dzl dzj - dzl dzk - dzl .. .. 5: dDs . 6: F . DsBm[e] 
7: dF . (dDs)Bm[e] 8: dP . dP(F; dF) . From the stress derivative formula 9: dH .-W [e](dP)(Bm[e])T 10: 
dfXi += dXh1, dfXj += dXh2, dfXk += dXh3 .dH = dXh1 dXh2 dXh3 11: dfXl += (-dXh1 - dXh2 - dXh3) 12: end 
for 13: end procedure K(x*)= - .fe : This is the elasticity sti.ness matrix evaluated around .xx* the 
con.guration x* . In most cases, the matrix K will never be explicitly constructed; interative solvers 
that involve this matrix will only require the evaluation of matrix-vector products of the form Kw. These 
products can be computed in a matrix-free fashion by calling the force di.erential computation procedure 
detailed in Algorithm (2) with an argument dx . (-w) fd(x* , v*)= -.K(x*)v* : Damping forces at position 
x* and velocity v* according to the Rayleigh damping model. The parameter . does not have a predetermined 
range (it is not con.ned to an interval such as [0, 1]) and can be spatially varying, or constant for 
simplicity.  f(x* , v*)= fe(x*)+ fd(x* , v*) : The aggregate forces, including elastic and damping components. 
 M : The mass matrix. We shall assume M is lumped to diagonal form.  In order to de.ne a backward Euler 
integration scheme, we will need to main­tain both the position (xn) and the velocity (vn) of the deforming 
body at time tn. Alternatively, it would have been possible to maintain just the two previous posi­tions 
xn and xn-1. The Backward Euler scheme computes the positions xn+1 and velocities vn+1 at time tn+1(:= 
tn +.t) as the solution of the (nonlinear) system of equations: xn+1 = xn +.tvn+1 (4.8) vn+1 , vn+1) 
= vn +.tM-1f(xn+1 , vn+1) = vn +.tM-1 fe(xn+1)+ fd(xn+1 (4.9) Since the Backward Euler system is nonlinear 
due to equation (4.9), we shall de.ne an iterative process to compute the unknowns xn+1 and vn+1. We 
will con­, xn+1 , xn+1 , vn+1 , vn+1 struct sequences of approximations xn+1 ,... and vn+1 ,... (0) (1) 
(2) (0) (1) (2) k.8k.8 respectively, such that xn+1 -. xn+1 and vn+1 -. vn+1 respectively. We will (k)(k) 
use the positions and velocities at the previous time step as initial guesses, i.e. xn+1 , vn+1 = xn 
= vn. (0) (0) We introduce the position and velocity correction variables, de.ned as .x(k) := xn+1 and 
.v(k) := vn+1 . (k+1) - xn(k+1) (k+1) - vn+1 (k) In most cases, unless there is risk of ambiguity, we 
will drop the subscript and de­note these corrections simply as .x, .v. At every step of our iterative 
scheme for the nonlinear Backward Euler system, we will linearize equations (4.8) and (4.9) (k) , vn+1 
around the current iterate xn+1 (k) , and the solution of the linearized system will de.ne the next iterate 
xn+1 (k+1), vn+1 (k+1). Lemma 1. .x =.t.v. Proof. Equation (4.8) is in fact linear. Therefore, at every 
iteration it will simply linearize to itself, i.e. xn+1 = xn +.tvn+1 for all k. (k)(k) , Subtracting 
the above equations for iterations k and k + 1, we obtain xn+1 =.t(vn+1 (k) ) (k+1) - xn+1 (k+1) - vn+1 
(k) or .x =.t.v , vn+1 The linearization of equation (4.9) around (xn+1 (k) ) yields: (k) 4.4. AN IMPLICIT 
TIME INTEGRATION SCHEME vn+1 (xn+1 (k) +.v = vn +.tM-1 fe(k) )+ ..fx e xn· +1 .x (k) -.K(xn+1 (k) )(vn+1 
+.v) (k) Note that this equation is not quite an exact linearization, because in the damp­ing term we 
.xed the sti.ness matrix at the value it had around con.guration xn+1 (k) instead of performing a .rst-order 
Taylor expansion. This modi.cation leads to a much simpler (modi.ed) Newton scheme for the Backward Euler 
system, and prac­tically doesn t a.ect the convergence of the Newton scheme. We further manipulate the 
previous equation as follows: M(vn - vn+1 (xn+1 .1 t2 M.x =1 (k) )+ fe(k) ) .t -K(xn+1 (k) )(vn+1 1 
(k) ).x - .K(xn+1 (k) +.t .x) . K(xn+1 1 1+ (k) )+ .t2 M .x = .t 1 M(vn - vn+1 (k) ) - .K(xn+1 (xn+1 
(k) )vn+1=.t (k) )+ fe(k) 1 M(vn - vn+1 (xn+1 (k) , vn+1 =.t (k) )+ fe(k) )+ fd(xn+1 (k) ) 1 M(vn - vn+1 
(k) , vn+1 =.t (k) )+ f(xn+1 (k) ) (4.10) The system described by equation (4.10) is symmetric and positive 
de.nite, and can be solved e.ciently with a Krylov subspace method such as Conjugate Gradients. We also 
note that equation (4.10) only determines the update for the positions at time tn+1. Velocities should 
be updated at each iteration using the relation vn+1 + 1 .x. (k+1) = vn+1 .t (k) As a .nal observation, 
equation (4.10) can be modi.ed to yield a quasistatic simulation, where every con.guration over time 
is the result of a rest con.gura­tion (subject to the imposed kinematic constraints and boundary conditions). 
We achieve this by setting .t .8, e.ectively indicating that at every simulated instance we allow in.nite 
time for the elastic body to settle into an equilibrium con.gruation. The Newton iteration for this quasistatic 
problem simply becomes: K(xn+1 = f(xn+1 (k) ).x (k) ) after which positions are updated as xn+1 (k) +.x. 
(k+1) . xn+1 SIGGRAPH 2012 Course Notes FEM Simulation of 3D DeformableSolids:Apractitioner s guide to 
theory, discretization and model reduction. Part 2: Model Reduction (version: June 11, 2012) Jernej Barbi.c 
 Course notes URL: http://www.femdefo.org 1 Introduction to model reduction Figure 1: Model reduction 
overview: a high-dimensional ordinarydifferential equation is approximated with a projection to a low-dimensional 
space. Model reduction (also called dimensional model reduction, or model order reduction (MOR)) is a 
tech­nique to simplify the simulation of dynamical systems describedbydifferential equations. The idea 
is to project the original, high-dimensional, state-space onto a properly chosen low-dimensional subspace 
to arrive at a (much) smaller system having properties similar to the original system (see Figure 1). 
Complex systems can thusbe approximatedbysimpler systems involvingfewer equations and unknownvariables, 
which canbe solved much more quickly thanthe originalproblem. Suchprojection-based modelreduction appears 
in literature under the names of Principal Orthogonal Directions (POD) Method, or Subspace Integra­tion 
Method, and it has a long historyin the engineering and applied mathematics literature [29]. See [27] 
and [33] for good overviews of model reduction applied to linear and nonlinear problems, respectively. 
Model reduction has been used extensively in the .elds of control theory, electrical circuit simulation, 
computational electromagnetics and microelectromechanical systems [28]. Most model reduction tech­niques 
in these .elds, however, aim at linear systems, and linear time-invariant systems in particular, e.g., 
small perturbations of voltages in some complex nonlinear circuit. Another common characteristic of these 
applicationsisthatboththeinputandoutputarelow-dimensional,i.e.,onemay wanttostudyhow the voltage level 
at some circuit location depends on the input voltage at another location, in a complex nonlinear circuit. 
In computer graphics, however, one is often interested in nonlinear systems (e.g., large deformations 
of objects) that exhibit interesting, veryvisible, dynamics. The output in computer graphics is usually 
high-dimensional, e.g., the deformation of an entire 3D solid object, or .uid velocities sampled on a 
high-resolution grid. For these reasons, many conventional reduction techniques do not immediately apply 
to computer graphics problems. 1.1 Survey of POD-based model reduction in computer graphics The initial 
model reduction applications to deformable object simulation in computer graphics investi­gated linear 
FEM deformable objects [32, 18, 14]. These models are veryfast, but are (due to linear Cauchy 1 strain) 
accurate only for small deformations and produce visible artifacts under large deformations. In ordertoavoid 
such artifacts,itis necessaryto applyreductionto nonlinear elasticity. For real-time geomet­rically nonlinear 
deformable objects (quadratic Green-Lagrange strain), such an approach was presented by Barbi.c and James 
[6, 4], who also gave an automatic approach to select a quality low-dimensional basis, using modal derivatives. 
An and colleagues [2] demonstrated how to ef.ciently support arbitrary nonlinear material models. Model 
reduction has also been used for fast sound simulation [17, 10] and to simulate frictional contact between 
deformable objects [21]. For deformable FEM of.ine simulations, Kim and James [23] applied online model 
reduction to adaptively replace expensive full simulation steps with reduced steps, which made it possible 
to throttle the simulation costs at run-time. Treuille and colleagues applied model reduction to .uid 
simulation in computer graphics [39]. Wicke and colleagues [41] im­provedTreuille s .uid methodto supportreduced 
.uid simulationsonseveral(inter-connected) domains with specialized basis functions on each domain (domain 
decomposition for .uids). Recently, Barbi. c and Zhao[8] demonstrated a domain decomposition method foropen-loop 
solid deformable models,by em­ploying gradients of polar decomposition rotation matrices, whereas Kim 
and James [24] tackled a similar problem using inter-domain spring forces.  2 Linear modal analysis 
 Figure 2: Linear modes for a cantilever beam. Although elastic objects can in principle deform arbitrarily, 
theytend to have a bias in deforming into certain characteristic, low-energy shapes. Most of us will 
remember the high-school physics example of a string stretched between two walls (or, say, a violin string), 
where one studies the natural frequencies .i of the string, together with their associated shapes, typically 
of the form sin(.ix) and cos(.ix). The same intuitioncarriesover to arbitrarythree-dimensionalelastic 
objects deformingby a smallamount around their rest con.guration, whether it be a metal wire, a thin 
shell (e.g., cloth on a character), or a solid 3D tet mesh model of a skyscraper. The low-frequency modes 
are the deformations, which, for a given amount of displaced mass (or volume), subject to speci.c boundary 
conditions such as .xed vertices, increase the elastic strain energyof the objectby the least amount. 
In otherwords, they are the shapes with the least resistance to deformation. How are the modes and frequencies 
computed? One has to .rst form the system mass and stiffness matrices M. R3n×3n and K. R3n×3n, where 
n is the number of mesh vertices. The speci.c approach to compute Mand Kdepends on each particular mechanical 
system. For example, the tet skyscraper maybe modeled using 3D FEM elasticity, whereas for a cloth model 
one maycompute M bylumping the mass at the vertices, and set K to the gradient of the internal cloth 
forces (in the rest con.guration), computed, say, using the Baraff-Witkin cloth model [3]. Once Mand 
Kare known, onehas to prescribe boundaryconditions, i.e., specify how the object is constrained. The 
modes and frequencies greatly depend on this choice. It is possible to set no boundaryconditions, in 
which case one obtains free­.ying modes. In order to compute the modes, one forms matrices M and K where 
the rows and columns corresponding to the .xed degrees of freedom have been removed from M and K. Then, 
one solves the generalized eigenvalue problem Kx= .Mx. (1) Matrices M and K are typically large and sparse. 
One can solve the eigenvalue problem, say, using the Arnoldi iteration implementedby the ARPACK eigensolver 
[26]. This solver is free, and has performed very wellinvarious modelreduction computer graphicsprojectsbyJernej 
Barbi.c and otherresearchersin the .eld. Because Mand Kare symmetric positive-de.nite (in typical applications, 
e.g., deformable object 2 in the rest con.guration), the eigenvalues are real and non-negative. One seeks 
the smallest eigenvalues .i and their associated eigenvectors .i, i = 1,2,. . ., k, where k is the number 
of modes to be retained. For objects with no constrained vertices (free-.ying objects), the .rst six 
eigenvalues are zero and the modes correspond to rigid translations and in.nitesimal rotations; these 
modes are typically discarded. The eigenvalues are squares of the natural frequencies of vibration, .i 
= .i2, and the eigenvectors .i are the modes. It should be noted that one typically inserts zeros into 
.i at locations of .xed degrees of freedom,so thattheresultingvectorisof length3n. In order to check 
that the eigensolver was successful, it is common to visualize the individual modes, by animating them 
as .isin(.it), where t is time. The different modal vectors are typically assembled into a linear modal 
basis matrix U=[.1,. . ., .k] . R3n×k . 2.1 Small deformation simulation using linear modal analysis 
Small deformations u . R3n (n is the number of mesh vertices)follow the equation Mu¨+ Du.+ Ku= f, (2) 
where M,D,K . R3n×3n are the mass, damping and stiffness matrices, respectively, and f . R3n are the 
external forces [35].Equation2isa linear, high-dimensional ordinary differential equation, obtainedby 
applying the Finite Element Method (FEM) to the linearized partial differential equations of elasticity. 
It is most commonly applied to 3D solids, but can also model shells and strands. It is only accurate 
under small deformations;veryvisible artifacts appear under large deformations. Equation2models the object 
at full resolution (no reduction), which means that it incorporates transient effects such as (localized) 
waves traveling across the object. It is often employed, for example, to perform earthquake simulation, 
typically using supercomputers on large meshes involving millions of degrees of freedom [1]. For real­time 
applications, the computational costs of timestepping Equation2 may be prohibitive. Instead, one can 
perform modelreduction,byapproximatingthe deformationvector u as u = Uz, where z . Rk is a vector of 
modal amplitudes (typically k « 3n). If we choose damping to be a linear combination of Mand K, D = aM+ 
ßK, for some scalars a,ß > 0(this is calledRayleigh damping), and pre-multiply Equation2 byUT, Equation2projects 
to kindependentone-dimensional ordinarydifferential equations z¨i +(a + ß.i)z.i + .izi = .iTf, (3) for 
i = 1,. . ., k. Here, we have used the fact that the modes are generalized eigenvectors K.i = .iM.i, 
and are therefore mass-orthonormal, (M.i)T.j = 0 when i = j, and (M.i)T.i = 1 for all i. The one­dimensional 
equations given in (3) can be timestepped independently. This can be done veryef.ciently (see, e.g., 
[18]). The full deformation can be reconstructed by multiplying u = Uz. This multiplication is fast when 
k is small (a few hundred modes). It can also be performed very ef.ciently in graphics hardware [18]. 
It can be shown that as k. 3n, this approximation converges to the solution of Equation 2. This property 
is veryuseful, as it makes it possible to trade computation accuracy for speed. 2.2 Application to sound 
simulation Sound originatesfrom mechanical vibrationof objects.These mechanical vibrations excitethe 
surrounding medium (typically air or water), and the pressure waves then propagate to the listener location. 
The object mechanical vibrations areusually modeled using FEM and the equations of elasticity, whereas 
the pressure propagationis usually modeledbythewave equation. There arevarying degreesof approximation 
that can be applied to each of these two tasks. Because deformation amplitudes in sound applications 
are small, linear elasticity is often employed for their simulation [31, 17]. However, richer, nonlinear 
sound can be produced using nonlinear simulation [10]. Linear simulation is straightforward and follows 
the materialfromSection2.1.Foreachobjectthatistoproducesound,one .rsthastosetthe .xedvertices (if any), 
and extract the modes. Next, one runs any physical simulation (typically rigid body simulation), producing 
contact forces. These forces are then used as external forces f for the modal oscillators in Equation 
3, producing modal excitations zi(t). It remains to be described how zi(t) are used to generate 3 the 
sound signal s(t).A very common approach is to assign some meaningful weights wi to each mode, and compute 
sound as k s(t)= . wizi(t), (4) i=1 where k is the number of modes. The weights wi can be set to a constant, 
wi = 1 [31], or they can be made non-constant to model the fact that different modes radiate with different 
intensities. Alternatively, weights can be made to depend on the listener location x, wi = wi(x,t),bysolving 
the spatial part of the wave equation (Helmholtz equation) [17, 10]. Such spatially-dependent weights 
can model diffraction of sound around the scene geometry.  3 Model reduction of nonlinear deformations 
Up to this point, we have considered model reduction of linear systems (Equation 2). Linear systems have 
an important limitation: theyproduce veryvisible artifacts under large deformations (see Figure 3). These 
artifactscanberemovedbyapplying modelreductiontothe nonlinear equationsofa deformableobject: Mu¨+ Du.+ 
fint(u)= f. (5) As detailed in the .rst part of the course, the nonlinearity in the internal forces fint(u) 
arises due to large­deformation strain(geometric nonlinearity), and due to nonlinearities in the strain-stress 
relationship. How to apply model reduction to Equation 5? We proceed in the same way as with linear systems; 
we assume the availability of a basis U . R3n×r (r is basis size) and approximate u = Uz, where z . Rr 
is the vector of reduced coordinates. After projectionby UT, we obtain z¨+ UTDUz.+ UTfint(Uz)= UTf. (6) 
This equation determines the dynamics of the reduced coordinates z = z(t) . Rr, and therefore also the 
dynamics of u(t)= Uz(t). Equation 5 is similar to Equation 3, except that it is nonlinear and the components 
of z are not decoupled. At this point, two questions emerge: (1) How can we timestep Equation 6? (2) 
How do we choose the basis U? Figure 3: Model reduction applied to a linear and nonlinear system. 3.1 
Timestepping the reduced nonlinear equations of motion Inorderto timestepEquation5, oneneedstoevaluatethe 
reduced internal forces, f int = UTfint(Uz) for arbi­trarycon.gurations z . Rr. Furthermore, as equations 
of elasticity are typically stiff, implicit integration is required, necessitating a further derivative 
of f int, the reduced tangent stiffness matrix df int . Rr×r K(z)= = UTK(Uz)U . (7) dz 4 Here K(u)= dfint/duis 
the (unreduced) tangent stiffness matrix in con.guration u. Note that in general, the term f int cannot 
be algebraically simpli.ed; its evaluation must proceed by .rst forming Uz, then evaluating fint(Uz) 
and .nally forming a projection by pre-multiplying with UT. Evaluation of K (z) is even more complex. 
Once f int(z) and K (z) are known, one can use any implicit integrator to timestep the system (see [4] 
for details). The keyimportant fact is that this integrator will need to solve a dense r × r linear systemas 
opposedtoasparse3n × 3n system as is the case with implicit integration of unreduced systems. Since r 
« 3n, this usually leads to signi.cant computational savings. How toevaluate f int(z) and K (z) in practice? 
If the simulation is geometrically nonlinear, but materially linear, then it can be shown [4] that each 
component of fint(u) is a cubic polynomial in the components of u [4]. Consequently, f int are cubic 
polynomialsin the componentsof z. Note that this is a manifestation of a more general principle: for 
any polynomial function G(u), its projection UTG(Uz) will be a polynomial in z,of the same degree.Treuille 
and colleagues, for example, exploited this fact with quadratic advection forces for reduced .uids [39]. 
For geometrically nonlinear materials, one can precompute the coef.cients of the cubic polynomials. As 
there are r components of the reduced force, each of which is a cubic polynomial in r variables, the 
necessary storage is O(r4). For moderate values of r (r < 30), this storage is manageable (under one 
megabyte; details are in [4]). Because the reduced stiffness matrix K (z) is the gradient of f int with 
respect to z, the reduced stiffness matrix is a quadratic function in z with coef.cients directly related 
to those of the reduced internal forces. For exact evaluation of internal forces and tangent stiffness 
matrices, all polynomial terms must be touched exactly once. Therefore, the cost of evaluation of reduced 
internal forces and tangent stiffness matrices is O(r4), whereas the cost of implicit integration is 
O(r3). For general materials, An and colleagues [2] have designed a fast approximation scheme which can 
decrease thereduced internal force and stiffness matrix computation time to O(r2) and O(r3), respectively. 
For simulations that use implicit integration, the runtime complexity is therefore O(r3).The methodworks 
byobserving that the elastic strain energy E(z) andinternal forces f int(z), for reduced coordinates 
z, are obtainedbyintegration of the energy density .(X,z) and its gradient over the entire mesh: E(z)= 
.(X,z)dV, (8) O . ..(X,z) f int(z)= dV. (9) O .z As opposed to evaluating f int using the exact formula 
f int = UTfint(Uz), An and colleagues approximate the integral in (9) usingnumerical quadrature. In order 
to do so, theydetermine positions Xi . O, and weights wi . R, such that T . ..(X,z) f int(z)= dV wig(Xi,z), 
(10) O .z . i=1 T .g(Xi,z)K (z) . wi , (11) .z i=1 where g(X,z)= ..(X,z)/.z. At runtime, given a value 
z, one then only has to evaluate g(Xi,z) and .g(Xi,z)/.z, for i = 1,. . ., T and sum the terms together. 
The number of quadrature points T is usually set to T = r. Positions and weights are obtained using a 
training process. Given a set of representa­tive training reduced coordinates z(1),..., z(N), the method 
computes positions and weights that best approximate the reduced force f int for these training datapoints. 
To avoid over.tting and to keep the stiffness matrix symmetric positive-de.nite, the weights wi are chosen 
to be non-negative, using nonneg­ative least squares (NNLS) [25]. The positions Xi are determined using 
a greedy approach, designed to minimize the NNLS error residual (details in [2]). 5 3.2 Choice of basis 
The matrix Uis a time-invariant matrix specifying a basis of some r-dimensional(r « 3n)linear subspace 
of R3n. The basis is assumed to be mass-orthonormal, i.e., UTMU = I. If this is not the case, one can 
easily convert U to such a basis using a mass-weighted Gramm-Schmidt process. For each .xed r > 1, there 
is an in.nite number of possible choices for the linear subspace and for its basis. Good subspaces are 
low-dimensional spaces which well-approximate the space of typical nonlinear deformations. The choice 
of subspace depends on geometry, boundary conditions and material properties. Selection of a good subspace 
is a non-trivial problem. We now present two choices: basis from simulation data ( POD basis ), and basisfrom 
modal derivatives. The formerrequirespre-simulation (usinga general deformable solver), whereas the latter 
can create a basis automatically without pre-simulation. 3.2.1 Basis from simulation data ( POD basis 
) In model reduction literature, a very common approach to create a basis for nonlinear systems is to 
obtain some snapshots of the system, u1,u2,. . ., uN, and then use statistical techniques to extract 
a representative low-dimensional space. The snapshots can be obtained by running a full (unreduced) simulation,orusing 
measurementsofareal system.Giventhe snapshots,one obtainsthe subspace U by performing singular value 
decomposition (SVD) on A=[u1,u2,. . ., uN], A= USVT , (12) and retains the columns of Ucorresponding 
to the largest r singular values. In practice, it is advantageous toapplySVDnotwithrespecttothe standardinnerproductin 
R3n, but with respect to a mass-weighted inner-product < Mx,y> (mass-PCA;see [4] for details). It is 
challenging to measure transient volumetric deformation .elds with high accuracy [22, 9]. There­fore, 
large-deformation model reduction applications in computer graphics have so far relied on simula­tion 
data, or on modal derivatives, which we describe next. 3.2.2 Modal derivatives Linear modal analysis 
(Section 2) provides a quality deformation basis for small deformations awayfrom therestpose.Thelinearmodes,however,arenotagoodbasisforlargedeformations, 
becausetheylackthe deformations that automatically activate in a nonlinear system. For example, when 
a cantilever beam de.ects sideways in the direction of the .rst linear mode, it also simultaneously compresses, 
in a very speci.c, non-uniform way. This happens automatically in a nonlinear system. A linear basis, 
however, lacks the proper (non-uniform) compression mode, and therefore the system projected onto the 
linear basis will be stiff(it locks ). In practice, such locking manifests as a rapid loss of energy 
(numerical damping), and as an increase in the natural oscillation frequencies of the system, a phenomenon 
also observed with modelreductionof electricalcircuits [13]. One could attempttoresolve these issuesby 
retaininga larger numberof linear modes. Suchanapproachis,however,notverypracticalwith nonlinear systems, 
because a verylarge number of modes would be needed in practice, whereas the time to solve the reduced 
system for implicit integration scales as O(r3). These problems can be remedied using modal derivatives: 
deformations that naturally co-appear in a nonlinear system when the system is excited in the direction 
of linear modes. By forming a basis that consists of both linear modes and their modal derivatives, we 
arrive at a compact, low-dimensional basis, that can represent large deformations and that can be computed 
purely based on the mesh geometry and material properties; no advance knowledge of run-time forcing or 
pre-simulation is required. We will illustrate modal derivatives for deformable objects that are suf.ciently 
constrained so that they do not possess rigid degrees of freedom, but modal derivatives can also be computed 
for unconstrained sys­tems. Under a static load f, the system will deform into a deformation u, where 
u satis.es the unreduced static equation fint(u)= f. Consider what happens if we statically load the 
system into the direction of 6 Figure 4: Modal derivatives for a cantilever beam. linear modes. In particular, 
suppose we apply a static force load MUlin.p, where M is the mass ma­trix, Ulin =[.1,.2,. . ., .k] is 
the linear modal matrix, . is the diagonal matrix of squared frequencies diag(.12,. . ., .k2), and p 
. Rk is some parameter that controls the strength of each mode in the load. It can be easily veri.ed 
that these are the force loads which, for small deformations, produce deforma­tions within the space 
spanned by the linear modes. Given a p, we can solve the nonlinear equation fint(u)= MUlin.p for u, i.e., 
we can de.ne a unique function u = u(p) (mapping from Rk to R3n, and C8 differentiable), such that fint(u(p)) 
= MUlin.p, (13) for every p. Rk in some suf.ciently small neighborhood of the origin in Rk.Canwe compute 
theTaylor series expansion of u in terms of p? By differentiating Equation 13 with respect to p, one 
obtains . fint .u = MUlin., (14) .u .p which is valid for all p in some small neighborhood of the origin 
of Rk. In particular, for p = 0k, we get K.u = MUlin.. Therefore, .u = Ulin, i.e., the .rst-order response 
of the system are the linear modes, as .p .p expected.To compute the second order derivativesof u,we 
differentiate Equation14 one order furtherby p, which, when we set p= 0k, gives us .2u K = -(H : .j).i. 
(15) .pi.pj Here, His the Hessian stiffness tensor, the .rst derivative of the tangent stiffness matrix, 
evaluated at u = 0 (see [4]). The deformation vectors .2u Fij = (16) .pi.pj are called modal derivatives. 
They are symmetric, Fij = Fji, and can be computed from Equation 15 by solving linear systems with a 
constant matrix K(stiffness matrix of the origin). Because Kis constant and symmetric positive-de.nite, 
it can be pre-factored using Cholesky factorization. One can then rapidly (in parallel if desired) compute 
all the modal derivatives,0 = i = j< k. Note that the modal derivatives are, byde.nition, the second 
derivatives ofu = u(p). The second-orderTaylor series expansion is therefore k kk 1 u(p)= ..ipi + 2 ..Fijpipj+ 
O(p3). (17) i=1 i=1j=1 7 Figure 5: Extreme shapes captured by modal derivatives: Although modal derivative 
are computed about the rest pose, their deformation subspace contains suf.cient nonlinear content to 
describe large deformations. Left:Spoon(k= 6,r = 15) is constrained at far end. Right: Beam(r = 5, twist 
angle=270.) ,F44,F49,F99 is simulatedina subspace spannedby twist linear modesand their derivatives .4,.9. 
The modal derivatives, together with the linear modes, therefore span the natural second-order system 
response for large deformations around the origin. Creating the basis U: Equation 17 suggests that the 
linear space spanned by all vectors .i and Fij is a natural candidate for a basis (after mass-Gramm-Schmidt 
mass-orthonormalization). However, its dimension k+ k(k+ 1)/2maybe prohibitive for real-time systems. 
In practice, we obtain a smaller basis byscaling the modes and derivatives according to the eigenvalues 
of the corresponding linear modes, and applying mass-PCA on the dataset .1.j | j= 1,. . ., k . .21 Fij 
| i= j; i,j= 1,. . ., k . (18).j .i.j The scaling puts greater weight on dominant low-frequency modes 
and their derivatives, which could otherwise be maskedbyhigh-frequency modes and derivatives. Figure 
6: Reduced simulations: Left: Model reduction enables interactive simulations of nonlinear de­formable 
models. Right: reduction also enables fast large-scale multibody dynamics simulations, with nonlinear 
deformable objects undergoing free .ight motion. Collisions among the 512 baskets were re­solved using 
BD-Trees [20]. 8  4 Model reduction and domain decomposition Model reduction as described in the previous 
section is global: it reduces the entire object using a sin­gle, global basis. Unless r is large, it 
is dif.cult to capture local detail using such a basis. Because the computation time grows at least as 
O(r3) (time to solve the dense r × r system [2]), large values of r (several hundreds of modes) are not 
practical. Itis therefore natural to ask if the object can somehow be decomposed into smaller pieces, 
each of which is reduced separately, and the pieces are then connected into a global system. This is 
the idea of domain decomposition, a classical technique in applied mathe­matics and engineering. In engineering 
applications, however, the deformations are typically small. In computer graphics, we have to accommodate 
large deformations (e.g., rotations) in the interfaces joining two domains, which means that standard 
domaindecomposition techniques cannot simply be extended to computer graphics problems. In computer graphics, 
domain decomposition for deformable models has initially been applied to small domain deformations and 
with running times dependent on the number of domain and interface vertices. For example, a linear quasi-static 
application using Green s functions has been presented in [19], whereas Huang and colleagues [15] exploited 
redundancy in stiffness matrix inverses to combine linear FEM with domain decomposition. Recently, domain 
decomposition under large deformations has received signi.­cant attention in computer graphics literature. 
Barbi.c and Zhao [8] demonstrated a domain decomposition methodby employing gradientsof polar decomposition 
rotation matrices, whereas Kim and James [24] tackled a similar problem using inter-domain spring forces. 
 Figure 7: Model reduction with a large number of localized degrees of freedom: Left: nonlinear reduced 
simulation of an oak tree (41 branches(r = 20), 1394 leaves(r = 8), d = 1435 domains, r = 11, 972 total 
DOFs) running at5fps. Right: simulation detail. 5 Model reduction and control Optimal control problems 
occur frequently in computer animation. Often, they are cast as space-time optimization problems involving 
human motion [34], .uids [40, 30] and deformations [42]. With optimal control of deformable objects, 
one seeks a sequence of forces(control vectors) fi . R3n , i = 0,...,T- 1, such that the resulting deformations(state 
vectors)ui . R3n , i = 0,...,T- 1, obtained by timestepping Equation5forwardin time under those forces, 
minimize some scalar objective E(u0,...,uT-1). The scalar objective typically includes terms such as 
magnitude of control vectors, deviation from some reference trajectory, deviation from keyframes at speci.c 
moments of time, and magnitude of deformation velocities and accelerations. The forces are sometimes 
expressed as fi = Bgi, where the matrix B . R3n×m gives the control basis. The problem is said to be 
underactuated when m < 3n and fully actuated for m = 3n. Underactuated problems can model objects that 
can propelthemselves using muscles , and are generally much more dif.cult to solve than fully actuated 
problems. Because optimal control problems compound both spaceand time,theyhaveaveryhigh dimensionality: 
thereare3nT unknowns (the control vectors fi)in the optimal control problem. Such a huge state space 
leads to optimization problems that diverge, converge to local minima, or take a verylong time to converge 
to a plausible solution. 9 Figure 8: Fast authoring of animations with dynamics [5]: This soft-body 
dinosaur sequence consists of .ve walking steps, and includes dynamic deformation effects due to inertia 
and impact forces. Each stepwas generatedbysolvinga space-time optimizationproblem, involving3 user-providedkeyframes, 
andrequiringonly3minutes totaltosolveduetoaproper applicationof modelreductiontothe control problem. 
Unreduced optimization took1hour for each step. The four images show output poses at times corresponding 
to four consecutive keyframes (out of 11 total). For comparison, the keyframe is shown in the top-left 
of each image. Model reduction is very bene.cial to optimal control because it greatly reduces the state 
and control size. The states ui are replaced with the reduced states zi, the control vectors fi are replaced 
with the reduced internal forces f i, and Equation 5 is replaced with its reduced version (Equation 6). 
Although such a reduced optimal control problem only approximates the original problem, its dimensionality 
is only rT « 3nT; therefore, the occurrence of local minima is greatly decreased. Optimal reduced forces 
canbe found faster than unreduced forces, because one can rapidly explore the solution spacebyrunning 
manyreducedforwardsimulationsandbyquicklyevaluatingthereduced objective gradient[5](Figure8). Standardcontrollers 
such as the linear-quadratic regulator [37] are impractical with deformable objects as they involve dense 
3n × 3n gain matrices. With reduction, however, such control becomes feasible as the gain matrices are 
now much smaller(r × r). Barbi.candPopovi´c[7] exploited sucha combinationof LQR control and model reduction 
for real-time tracking of nonlinear deformable object simulations, using minimal ( gentle ) forces. 
6 Free software for model reduction The implementation of [6] (byJernej Barbi.c) is freely available 
on the web (BSD license) at: http://www.jernejbarbic.com/code. It includes 1. a precomputation utility 
to compute linear modes (Equation 1) and modal derivatives (Equation 15), and to construct the simulation 
basis U(mass-PCA applied to the dataset of Equation 18); optionally, the basis can also be computed from 
pre-existing simulation data ( POD basis ), 2. a precomputation utility to compute the cubic polynomial 
coef.cients for reduced internal forces f int and stiffness matrices K (Section 3.1), for isotropic geometrically 
nonlinear material model(St. Venant-Kirchhoff material) 3. an ef.cient C/C++ library to timestep the 
reduced model precomputed in the above steps 1., 2. (Equation 6), and an example run-time driver.  The 
code shares basic classes with Vega, a general-purpose simulator for FEM nonlinear 3D deformable objects 
(including deformable dynamics), also available at the same URL (BSD license). 10 Figure 9: Precomputation 
utility to compute linear modes, derivatives, basis for large deformations, and cubic polynomial coef.cients. 
Freely available (BSD license) at http://www.jernejbarbic.com/code . 7 Deformation warping As outlinedinSection3, 
linear modal analysis(Section2)leadsto visible artifacts underlarge defor­mations,and these artifacts 
canberemovedby applying modelreductiontothe nonlinear equationsof motion. Deformation warping is an alternative, 
purely geometric, approach to remedy the same problem. The idea is to keep Equation 2 as the underlying 
dynamic equation, but to post-process the resulting deformations u using a geometric .lter that removes 
large-deformation artifacts. For example, given a tetrahedral mesh, warping establishes a mapping that 
maps linearized deformations u . R3n (away from the rest con.guration) to good-looking large deformations 
q = W(u) . R3n (also awayfrom the rest con.guration). The user is only shown the corrected deformations 
q. Warping is very robust. For example, twisting deformations, with local rotations as large as several 
complete 360 degrees cycles, can be easily accommodated. The underlying dynamics, however, is still linear, 
and this is visible with large­deformation motion as linear deformations essentially follow a sinusoidal 
curve, sin(.it). In contrast, objects simulated using nonlinear methods usually stiffen under large deformations, 
and therefore spend a smaller percentage of the oscillation cycle time at large deformations. The idea 
that modes could be warped to correct large deformation artifacts was .rst observed by Choi andKo [11]. 
They noticed that,by taking the curlof each modalvector, one can derive per-vertex in.nitesimal rotations 
due to the activation of each mode. These rotations can then be integrated in time, resulting in large 
deformations free of artifacts. Although not surveyed here in detail, their approach is fast, and laid 
the foundation for other warping methods developed later. 7.1 Rotation-strain coordinate warping In these 
notes,we describearecentef.cient.avorofwarping,the rotation-strain coordinate warping [16]. Let Gj . 
R9×3n be the discrete gradient operator of tet j, i.e., deformation gradient of tet junder deformation 
u . R3n equals I+ Gju. Decompose the3 × 3matrixGju into a symmetric and antisymmetric component, Gju+(Gju)T 
Gju- (Gju)T Gju =+ . (19) 22 We can then denote the upper triangle of the symmetric part as j . R6, and 
the skew-vector correspond­ingto theantisymmetricpart as .j . R3.We then assemble [ j,.j] for all tets 
into a vector y(u) . R9#tets; 11 denote y ,j = jand y.,j = .j.Vector yforms the rotation-strain coordinates 
of the mesh. Given a linearized deformation u, we then postulate that we should seek a deformation q 
so that the deformation gradient inside tet jequals expYI+ sym(y ,j). (20) y.,j Here, sym(x) is the3 
× 3 symmetric matrix corresponding to its upper-triangle x . R6, x is the 3 × 3 skew-symmetric cross-product 
matrix corresponding to vector x . R3( = x× v for all v . R3), and exp xv is the matrix exponential function 
[36]. This condition cannotbe satis.ed for all the tets simultaneously. Instead, given u, we .nd a deformation 
qunder which the deformation gradients are as close as possible to those givenbyEquation 20. This can 
be donebysolving the following least-squares problem: #tets argmin . YI+ sym(y ,j)F, Vj||I+ Gjq- expy.,j||2 
(21) q j=1 subject to pinned vertices (22) where || ||F denotes theFrobenius normofa3 × 3matrix, andVjis 
the volume of tet j. The pinned vertices are the vertices where the model is rooted to the ground (boundaryconditions). 
For free-.ying objects, a constraint can be formed that keeps the center of mass unmodi.ed. The objective 
function in Equation 21 is quadratic in q, and can be rewritten as ||VGq- b||22, (23) vv v for V = diag( 
V1, V1,. . ., V#tets) (each entry repeated 9x), and where G . R9#tets×3n is the gradient matrix assembled 
from all Gj. The nine-block of vector b . R9#tets corresponding to tet j, expressed as a row-major3× 
3matrix, equals  bj =Vjexp(Y. y.,j)(I+ sym(y ,j)) - I(24) In a typical tet mesh, there are more tets 
than vertices, therefore, the optimization problem is overcon­strained. The minimization canbe performed 
via Lagrange multipliers,bysolving . .. .. . Ld q (VG)Tb = , (25) dT 0 . 0 where L = GTV2G, and where 
d corresponds to the pinned input vertices (note: an implementation can simply remove the rows-columns 
from L;this is equivalent). MatrixLis called the discrete Laplacian of the mesh. It only depends on the 
input mesh geometry, and not on U or u. The system matrix in Equation 25 is sparseand constant,and canbepre-factored,sowarping 
canbe performedef.cientlyat runtime. 12 7.2 Warping for triangle meshes Triangle meshes are commonly 
employed in computer graphics, say, for simulation of thin shells and cloth. Such physical systems also 
produce the mass matrix Mand stiffness matrix K. Therefore, the small deformation analysis (Equation 
2) and model reduction (as in Equation 3) apply also to such problems. In ordertoapplywarping,however,wemust 
de.nedeformation gradientsforeach triangle. Asobserved by Sumner and Popovi´c [38], the three vertices 
of a triangle before and after deformation do not fully determine the af.ne transformation since theydo 
not establish how the space perpendicular to the triangle deforms. They resolve this issuebyaddinga (.ctitious) 
fourthvertex v4, (v2 - v1) × (v3 - v1) v4 = v1 + , (26)|(v2 - v1) × (v3 - v1)| where v1,v2,v3 are the 
triangle vertices. Vertices v1,v2,v3,v4 de.ne a tetrahedron. Let v1i,v2i,v3idenote the deformed vertex 
positions; then, we can use (26) to compute the deformed .ctitious vertex v4i. The deformation gradient 
Ffor the triangle equals . .. . -1 F= . v2 i- vi v3 i- vi v4 i- vi .. v2 - v1 v3 - v1 v4 - v1 . . (27) 
111 Given the deformation gradient, warping then proceeds in the same way as described for tetrahedral 
meshes in previous sections. A more principled version of triangle mesh warping has been presented by[12]. 
 8 Acknowledgements Thisworkwas sponsoredbythe NationalScience Foundation (CAREER-53-4509-6600). References 
[1] V. Akcelik, J. Bielak, G. Biros, I. Epanomeritakis, A. Fernandez, O. Ghattas, E. J. Kim, J. Lopez, 
D. O Hallaron,T.Tu, and J. Urbanic. High-resolution forward and inverse earthquake modeling on terascale 
computers. In Proceedings of ACM/IEEE SC2003, 2003. [2]S.S.An,T.Kim,andD.L. James. Optimizing cubatureforef.cient 
integrationof subspace deforma­tions. ACMTrans. on Graphics, 27(5):165:1 165:10, 2008. [3] D. Baraff 
and A.P.Witkin. Large Steps in Cloth Simulation. In Proc. of ACM SIGGRAPH 98, pages 43 54, July 1998. 
[4] J. Barbic..Real-time Reduced Large-Deformation Models and Distributed Contact forComputer Graphics 
and Haptics. PhD thesis, Carnegie Mellon University,Aug. 2007. [5] J. Barbi.c,M.da Silva, andJ. Popovi´c. 
Deformable object animation usingreduced optimal control. ACMTrans. on Graphics (SIGGRAPH2009), 28(3):53:1 
53:9, 2009. [6] J. Barbi.c andD.L. James. Real-time subspace integration for St.Venant-Kirchhoffdeformable 
models. ACMTrans. on Graphics, 24(3):982 990, 2005. [7] J. Barbi.c and J. Popovi´c. Real-time control 
of physically based simulations using gentle forces. ACM Trans. on Graphics (SIGGRAPH Asia 2008), 27(5):163:1 
163:10, 2008. [8] J. Barbi.c and Y. Zhao. Real-time large-deformation substructuring. ACM Trans. on Graphics 
(SIG-GRAPH 2011), 30(4):91:1 91:7, 2011. 13 [9] B. Bickel, M. Baecher, M. Otaduy, W. Matusik, H. P.ster, 
and M. Gross. Capture and modeling of non-linear heterogeneous soft tissue. ACMTrans. on Graphics (SIGGRAPH 
2009), 28(3):89:1 89:9, 2009. [10] J. N. Chadwick, S. S. An, and D. L. James. Harmonic Shells: Apractical 
nonlinear sound model for near-rigid thin shells. ACMTransactions on Graphics, 28(5):1 10, 2009. [11] 
M. G. Choi and H.-S. Ko. ModalWarping: Real-Time Simulation of Large Rotational Deformation and Manipulation. 
IEEETrans. onVis. and Comp. Graphics, 11(1):91 101, 2005. [12] M. G. Choi, S.Y.Woo, and H.-S. Ko. Real-Time 
Simulation of Thin Shells. Eurographics 2007, pages 349 354, 2007. [13] L. Daniel. Private correspondence 
with Prof. Luca Daniel, MIT. [14] K. K. Hauser, C. Shen, and J. F. O Brien. Interactive deformation using 
modal analysis with con­straints. In Proc. of Graphics Interface, pages 247 256, 2003. [15] J. Huang, 
X. Liu, H. Bao, B. Guo, and H.-Y. Shum. An ef.cient large deformation method using domain decomposition. 
Computers&#38;Graphics, 30(6):927 935, 2006. [16] J. Huang,Y.Tong,K. Zhou,H. Bao, andM. Desbrun. Interactive 
shape interpolation through control­lable dynamic deformation. IEEETrans. onVisualization andComputer 
Graphics, 17(7):983 992, 2011. [17] D. L. James, J. Barbi.c, and D. K. Pai. Precomputed acoustic transfer: 
Output-sensitive, accurate sound generation for geometrically complex vibration sources. ACMTransactions 
on Graphics (SIGGRAPH 2006), 25(3), 2006. [18] D.L. James andD.K. Pai. DyRT: Dynamic ResponseTextures 
for RealTime Deformation Simulation With Graphics Hardware. ACMTrans. on Graphics, 21(3):582 585, 2002. 
[19] D. L. James and D. K. Pai. RealTime Simulation of Multizone Elastokinematic Models. In IEEE Int. 
Conf. on Robotics and Automation, pages 927 932, 2002. [20] D. L. James and D. K. Pai. BD-Tree: Output-Sensitive 
Collision Detection for Reduced Deformable Models. ACMTrans. on Graphics, 23(3):393 398, 2004. [21] D. 
M. Kaufman, S. Sueda, D. L. James, and D. K.Pai. Staggered Projections for Frictional Contact in Multibody 
Systems. ACMTransactions on Graphics, 27(5):164:1 164:11, 2008. [22] A. E. Kerdok, S. M. Cotin, M. P. 
Ottensmeyer, A. M. Galea, R. D. Howe, and S. L. Dawson. Truth cube: Establishing physical standards for 
soft tissue simulation. Medical Image Analysis, 7(3):283 291, 2003. [23] T. Kim and D. James. Skipping 
steps in deformable simulation with online model reduction. ACM Trans. on Graphics (SIGGRAPH Asia 2009), 
28(5):123:1 123:9, 2009. [24] T. Kim and D. James. Physics-based character skinning using multi-domain 
subspace deformations. In Symp. on Computer Animation (SCA), pages 63 72, 2011. [25] C. L.Lawson and 
R. J. Hanson. Solving Least Square Problems. Prentice Hall, Englewood Cliffs, NJ, 1974. [26] R. Lehoucq, 
D. Sorensen, and C. Yang. ARPACK Users Guide: Solution of large scale eigenvalue problems with implicitly 
restarted Arnoldi methods. Technical report, Comp. and Applied Mathe­matics, Rice Univ., 1997. [27] J.-R. 
Li. Model Reduction of Large Linear Systems via Low Rank System Gramians. PhD thesis, Mas­sachusetts 
Institute ofTechnology, 2000. 14 [28] R.-C. Li and Z. Bai. Structure preserving model reduction using 
a Krylov subspace projection formu­lation. Comm. Math. Sci., 3(2):179 199, 2005. [29] J. L. Lumley. The 
structure of inhomogeneous turbulence. In A.M.Yaglom andV.I.Tatarski, editors, Atmospheric turbulence 
and wave propagation, pages 166 178, 1967. [30] A. McNamara, A.Treuille, Z. Popovi´c, and J. Stam. Fluid 
control using the adjoint method. ACM Trans. on Graphics (SIGGRAPH 2004), 23(3):449 456, 2004. [31] J.F. 
O Brien, C. Shen, and C. M. Gatchalian. Synthesizing sounds from rigid-body simulations. In Symp. on 
Computer Animation (SCA), pages 175 181, 2002. [32] A. Pentland andJ.Williams. Good vibrations: Modal 
dynamics for graphics and animation. Computer Graphics (Proc. of ACM SIGGRAPH 89), 23(3):215 222, 1989. 
[33] M. Rewienski. ATrajectory Piecewise-Linear Approach to Model Order Reduction of Nonlinear Dynamical 
Systems. PhD thesis, Massachusetts Institute ofTechnology, 2003. [34] A. Safonova, J. Hodgins, and N. 
Pollard. Synthesizing physically realistic human motion in low­dimensional, behavior-speci.c spaces. 
ACMTrans. on Graphics (SIGGRAPH2004), 23(3):514 521, 2004. [35] A. A. Shabana. Theory ofVibration,Volume 
II: Discrete and Continuous Systems. Springer Verlag, New York, NY, 1990. [36] R.B. Sidje. Expokit:ASoftware 
Package for ComputingMatrix Exponentials. ACMTrans. on Mathe­matical Software, 24(1):130 156, 1998. www.expokit.org. 
[37] R.F. Stengel. Optimal Control and Estimation. Dover Publications, NewYork, 1994. [38] R. Sumner 
and J. Popovi´c. Deformation transfer for triangle meshes. ACMTrans. on Graphics (SIG-GRAPH 2004), 23(3):399 
405, 2004. [39]A.Treuille,A.Lewis,andZ.Popovi´c. Modelreductionforreal-time .uids. ACMTrans. on Graphics, 
25(3):826 834, 2006. [40] A.Treuille, A. McNamara, Z.Popovi´c, and J. Stam. Keyframe control of smoke 
simulations. ACM Trans. on Graphics (SIGGRAPH 2003), 22(3):716 723, 2003. [41] M.Wicke, M. Stanton, and 
A.Treuille. Modular bases for .uid dynamics. ACMTrans. on Graphics, 28(3):39:1 39:8, 2009. [42] C.Wojtan,P. 
J. Mucha, and G.Turk. Keyframe control of complex particle systems using the adjoint method. In Symp. 
on Computer Animation (SCA), pages 15 23,Sept. 2006. 15  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2343502</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>46</pages>
		<display_no>21</display_no>
		<article_publication_date>08-05-2012</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Advanced (quasi) Monte Carlo methods for image synthesis]]></title>
		<page_from>1</page_from>
		<page_to>46</page_to>
		<doi_number>10.1145/2343483.2343502</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2343502</url>
		<abstract>
			<par><![CDATA[<p>Monte Carlo ray tracing has become ubiquitous in most commercial renderers and in custom shaders used for visual effects and feature animation. But many advanced Monte Carlo algorithms are not widely used and are often misunderstood. In this course, attendees learn about the practical aspects of variance-reduction methods with a focus on all variants of importance sampling. The course also covers quasi-Monte Carlo methods at the industry level, as well as the practical aspects of bidirectional path tracing combined with multiple importance sampling and Metropolis Light Transport. Practical advice is provided throughout the course.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P3738943</person_id>
				<author_profile_id><![CDATA[81436593326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Rendering Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738944</person_id>
				<author_profile_id><![CDATA[81100604694]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Premoze]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Rendering Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P3738945</person_id>
				<author_profile_id><![CDATA[81504681983]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raab]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Advanced Rendering Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ashikhmin, M., Premoze, S., Shirley, P., and Smits, B. A variance analysis of the metropolis light transport algorithm. <i>Computers &amp; Graphics 25</i>, 2 (2001), 287--294.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383474</ref_obj_id>
				<ref_obj_pid>2383465</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clarberg, P., and Akenine-M&#246;ller, T. Exploiting Visibility Correlation in Direct Illumination. <i>Comp. Graph. Forum (Proc. of EGSR 2008) 27</i>, 4 (2008), 1125--1136.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clarberg, P., and Akenine-M&#246;ller, T. Practical Product Importance Sampling for Direct Illumination. <i>Comp. Graph. Forum (Proc. of Eurographics 2008) 27</i>, 2 (2008), 681--690.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073328</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Clarberg, P., Jarosz, W., Akenine-M&#246;ller, T., and Jensen, H. W. Wavelet Importance Sampling: Efficiently Evaluating Products of Complex Functions. <i>ACM Trans. Graph. 24</i>, 3 (2005), 1166--1175.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383909</ref_obj_id>
				<ref_obj_pid>2383894</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cline, D., Egbert, P. K., Talbot, J. F., and Cardon, D. L. Two stage importance sampling for direct lighting. In <i>Rendering Techniques 2006: 17th Eurographics Workshop on Rendering</i> (June 2006), pp. 103--114.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Coleman, W. Mathematical Verification of a certain Monte Carlo Sampling Technique and Applications of the Technique to Radiation Transport Problems. <i>Nuclear Science and Engineering 32</i> (1968), 76--81.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hammersley, J. M., and Handscomb, D. C. <i>Monte Carlo Methods</i>. Wiley, New York, N. Y., 1964.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hastings, W. Monte carlo sampling methods using markov chains and their applications. <i>Biometrika 57</i> (1970), 97--109.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hesterberg, T. Weighted average importance sampling and defensive mixture distributions. <i>Technometrics 37</i>, 2 (May 1995), 185--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>7050</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Kalos, M. H., and Whitlock, P. A. <i>Monte Carlo Methods</i>. John Wiley and Sons, New York, N. Y., 1986.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kelemen, C., Szirmay-Kalos, L., Antal, G., and Csonka, F. A Simple and Robust Mutation Strategy for the Metropolis Light Transport Algorithm. <i>Computer Graphics Forum 21</i>, 3 (Sept. 2002), 531--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kollig, T., and Keller, A. <i>Monte Carlo and Quasi-Monte Carlo Methods</i>. Springer-Verlag, 2000, ch. Efficient Bidirectional Path Tracing by Randomized Quasi-Monte Carlo Integration, pp. 290--305.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., and Teller, E. Equations of State Calculations by Fast Computing Machine. <i>Journal of Chemical Physics 21</i> (1953), 1087--1091.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>237265</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Mitchell, D. P. Consequences of stratified sampling in graphics. In <i>Proc. of ACM SIGGRAPH 1996</i> (1996), Addison-Wesley, pp. 277--280.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Raab, M., Seibert, D., and Keller, A. Unbiased global illumination with participating media. In <i>Monte Carlo and Quasi-Monte Carlo Methods</i> (2006), A. Keller, S. Heinrich, and H. Niederreiter, Eds., Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Spanier, J., and Gelbard, E. M. <i>Monte Carlo Principles and Neutron Transport Problems</i>. Addison-Wesley, New York, N. Y., 1969.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383674</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Talbot, J., Cline, D., and Egbert, P. Importance resampling for global illumination. In <i>Rendering Techniques 2005: 16th Eurographics Workshop on Rendering</i> (June 2005), pp. 139--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>927297</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Veach, E. <i>Robust Monte Carlo Methods for Light Transport Simulation</i>. PhD thesis, Stanford University, 1997.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>218498</ref_obj_id>
				<ref_obj_pid>218380</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. Optimally combining sampling techniques for Monte Carlo rendering. In <i>Proc. of ACM SIGGRAPH 1995</i> (1995), Addison-Wesley, pp. 419--428.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>258775</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Veach, E., and Guibas, L. Metropolis Light Transport. <i>Computer Graphics 31</i> (1997), 65--76.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Woodcock, E., Murphy, T., Hemmings, P., and Longworth, T. Techniques used in the gem code for monte carlo neutronics calculations in reactors and other systems of complex geometry. <i>Proc. Conf. Applications of Computing Methods to Reactor Problems</i> (1965).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advanced (Quasi)-Monte Carlo Methods for Image Synthesis Siggraph 2012 Course Image courtesy Delta 
Tracing with NVIDIA iray. PRESENTERS Leonhard Grünschloß Weta Digital Alexander Keller NVIDIA ARC GmbH 
Simon Premo e Double Negative Matthias Raab NVIDIA ARC GmbH COURSE DESCRIPTION Monte Carlo ray tracing 
has become ubiquitous in most commercial renderers and in custom shaders used for visual e.ects and feature 
animation. However, many advanced Monte Carlo algorithms are not widely used and are often misunderstood. 
Course attendees will learn about the practical aspects of variance reduction methods with a focus on 
all variants of importance sampling. The course also covers quasi-Monte Carlo methods at industry level, 
as well as the practical aspects of bidirectional path tracing combined with multiple importance sampling 
and Metropolis Light Transport. The audience will bene.t from the intuition provided by the practical 
advice throughout the course. Level of difficulty: advanced COURSE LENGTH Half-day course. INTENDED 
AUDIENCE Renderer engineers, shader writers, and students with an interest in physically-based rendering, 
e.cient simulation technology, and practical advice. PREREQUISITES Basic understanding of raytracing 
and light transport. Understanding of probability and linear algebra. Leonhard Grünschloß is working 
on physically based rendering at WetaDigital. He received his Master's degree in Computer Science at 
Ulm University in 2008.Afterwards he worked at mental images for three years, where he was designing 
new rendering architectures and shading languages. His main research interests include e.cient and robust 
sampling techniques for photorealistic image synthesis in the context of large production scenes, with 
a focus on quasi-Monte Carlo methods. Alexander Keller is a member of NVIDIA Research and leads advanced 
rendering research at NVIDIA ARC GmbH, Berlin. Before, he had been the Chief Scientist of mental images 
and had been responsible for research and the conception of future products andstrategies including the 
design of the iray renderer. Prior to industry, he worked as a full professor for computer graphics and 
scienti.c computing at Ulm University, where he co­founded the UZWR (Ulmer Zentrum für Wissenschaftliches 
Rechnen). He holds a PhD in computer science and he authored more than 21 patents, and published more 
than 40 papersmainly in the area of quasi-Monte Carlo methods and photorealistic image synthesis. Simon 
Premo e is currently writing physically-inspired shaders at Double Negative. He graduated from the University 
of Utah where he primarily studied appearance models, volume rendering and global illumination. Previously, 
he was an R&#38;D engineer at Industrial Light and Magic where he worked on a variety of rendering problems 
in production. His current research interests include interactive global illumination and rendering algorithms, 
Monte Carlo methods, modeling natural phenomena and re.ectance models. Matthias Raab is working at NVIDIA's 
Advanced Rendering Center in Berlin, where he is one of the key engineers of NVIDIA iray. Before moving 
to industry in 2007 and working on the mental ray renderer, he had been a researcher at Ulm university, 
where he received his Master's degree in Computer Graphics in 2005. Matthias Raab has a strong background 
in mathematics and scienti.c computing, especially in Monte Carlo methods. His current work is focussing 
on variance reduction methods and light transport simulation algorithms. COURSE SYLLABUS 1.Introduction 
  [Premo e/Keller] a. Monte Carlo and Quasi-Monte Carlo methods [Keller]   b. Applications to Light 
Transport [Premo e] 2. Variance reduction methods [Premo e] a. Overview of Variance Reduction Techniques 
b. Importance Sampling Principles What works and what doesn't work and why Weighted Importance Sampling 
 Multiple Importance Sampling Deterministic Mixture Sampling Adaptive Multiple Importance Sampling c. 
Control Variates Practical applications Approximating visibility d. Other Techniques Separation of the 
main part Correlated Sampling Adaptive Sampling  3. QMC Methods in Photorealistic Image Synthesis [Keller 
/ Grünschloß] a. Consistent vs. Biased vs. Unbiased b. Quasi-Monte Carlo Points Halton sequence and Hammersley 
points (t,s)-sequences and (t,m,s)-nets in base b Rank-1 lattice sequences and rank-1 lattices c. Quasi-Monte 
Carlo Rendering Techniques Hybrid sequences Path tracing Anti-aliasing Motion blur Bidirectional scattering 
distribution functions Connecting path segments by shadow rays Connecting path segments by proximity 
 4. Bidirectional Path Tracing (BDPT) [Premo e] Connecting path tracing and light tracing Conversion 
of densities for Multiple Importance Sampling Vertex merging Implementation details Issues with Bidirectional 
Path Tracing 5. Metropolis Light Transport (MLT) [Raab] Metropolis Sampling Algorithm Application to 
Light Transport Strengths &#38; Weaknesses (Implementing) Mutation Strategies 6. Conclusion and Questions 
 [All] Advanced (Quasi-) Monte Carlo Methods for Image Synthesis Course Notes Leonhard Grünschloß 
Alexander Keller Simon Premo e Matthias Raab May 2012 Abstract Monte Carlo ray tracing has become ubiquitous 
in most commercial renderers and in custom shaders used for visual e.ects and feature animation. However, 
many advanced Monte Carlo algorithms are not widely used and are often misunderstood. Course at­tendees 
learn about variance reduction methods ranging from importance sampling and its derivatives to control 
variates and correlated sampling. Audience also learns about Quasi Monte Carlo methods, deterministic 
pseudo-random number generation and how to correctly incorporate them into a raytracer. Last part of 
the course is de­voted to advanced algorithms such as bidirectional path tracing and Metropolis Light 
Transport with special emphasis on how to implement these algorithms in practice. Contents 1 Introduction 
to Monte Carlo Methods 4 1.1 MonteCarlo Methods .............................. 4 1.1.1 Estimators ................................. 
4 1.1.2 MonteCarlo Integration ........................ 7 1.1.3 VarianceReductionTechniques .................... 
8 1.2 MultipleImportance Sampling ......................... 13 1.3 PracticalNotesonMonteCarloSampling 
. . . . . . . . . . . . . . . . . . 17 1.3.1 ChoosingSamplingDensity ...................... 17 1.3.2 
Filtered Importance Sampling For Area Lights . . . . . . . . . . . 17 1.3.3 WhatAboutVisibility? ......................... 
19 1.3.4 Resampled ImportanceSampling ................... 20 2 Metropolis Light Transport 22 2.1 MetropolisSampling 
............................... 22 2.2 ApplicationtoLightTransport ......................... 24 2.2.1 
E.ciency ................................. 27 2.3 MutationStrategies ................................ 
28 2.4 PrimarySampleSpace Mutations ........................ 28 2.5 PathManipulation ................................. 
34 2.5.1 BidirectionalMutations ......................... 34 2.5.2 Pertubations ................................ 
36 2.6 Two-StageMLT .................................. 37 2.7 ParallelizationofMLT .............................. 
38 2.8 Summary ...................................... 39 Bibliography 40 1 Introduction to Monte Carlo 
Methods 1.1 Monte Carlo Methods The term Monte Carlo refers to all methods that use a statistical sampling 
processes to approximate solutions to quantitative problems. It can be used for a wide variety of probabilistic 
problems ranging from numerical integration to optimization. These methods are used in many application 
domains such as economics, robotics and nuclear engineering. In this section, we describe some basic 
concepts of Monte Carlo integration. After a brief overview of the Monte Carlo methods, we introduce 
the principle of Monte Carlo integration and look at some of the basic statistical properties. Then, 
we describe some basic variance reduction techniques, such as importance sampling, control variates, 
and mixture sampling, that we use in later sections in the context of light transport. This section is 
only a brief summary, but it does provide some insights and intuition about why some methods perform 
better than other and describes the circumstances one should use a particular method. We encourage interested 
readers to learn more about Monte Carlo methods and probability in many excellent books [10, 16, 7] and 
papers that exist on the topic. Readers who are mostly interested in the practical implementation Monte 
Carlo methods for rendering can skip this section. 1.1.1 Estimators A continuous random variable X is 
a quantity that randomly takes on a value x that lies on the real line (-., .). The values of x can be 
quantitatively described by the probability density function (PDF) p. The probability that x will take 
a value on some interval between a and b is then . b Pr{a : X : b} = p(x)dx. (1.1) a The probability 
density function p(x) must satisfy two conditions: 1. It is always positive: p(x) 2 0 2. It is normalized: 
. * p(x)dx =1 -* It is important to understand the di.erence between probability and the probabil­ity 
density function. The probability, or likelihood, of an event takes values strictly between 0 (impossible 
event, it never happens) and 1 (certain event, it always occurs). On the other hand, the probability 
density function describes the relative likelihood of a random variable (or event) having a certain value. 
For instance, if p(x1) = 10 and p(x2) = 100, then the random variable with the PDF p is ten times more 
likely to have a value near x1 than near x2. The relationship between the probability density function 
p and the probability Pr is de.ned in Equation (1.1). Other important concepts to understand are expected 
value and variance of a random variable. The expectation (or expected value) of a random variable Y = 
f(X) is . E[Y ] = f(x)p(x)dx (1.2) . and its variance is V [Y ]= E[(Y - E[Y ])2]. (1.3) Intuitively, 
expected value (or mean value) is just the average value of the random vari­able. Note that expected 
value should not be confused with the most probable value. On the other hand, variance measures how much 
the values of some random variable deviate from its mean or expected value. The higher the variance, 
the more values di.er from the average value. The expected value has a few useful properties: 1. The 
expected value of the sum of two random variables X and Y is the sum of the expected values of those 
variables: E[X + Y ]= E[X]+ E[Y ] Since functions of random variables are also random variables, this 
principle ap­plies to the sum of functions of random variables: E[f(X)+ g(Y )] = E[f(X)] + E[g(Y )] The 
above holds true even if variables X and Y are correlated. 2. For any constant a, the expected value 
and variance for aX are E[aY ]= aE[Y ] V [aY ]= a 2V [Y ] If we want to compute an approximation to some 
unknown quantity Q (i.e.the esti­mand or quantity of interest), a function F of random variables X1,...,XN 
is called an estimator if its mean (expected value) E[F ] is a usable approximation to Q: FN = FN (X1,...,XN 
) (1.4) A particular numerical value of FN is called an estimate. Q can be any function that we might 
be interested in. In rendering, Q can be the amount of light that reaches a point on a surface or the 
amount of light re.ected from the surface. There are many possible estimators. In general, we want Monte 
Carlo estimators to provide good estimates as fast as possible. How do we choose a good estimator?First, 
we need to establish some criteria for what good means by looking at the proper­ties of Monte Carlo estimators: 
Error error = FN - Q Mean Square Error (MSE) of an estimator F is then MSE = E[(FN - Q)2] (1.5) Bias 
Bias j is the expected value of the error: j[FN ] = E[FN - Q] The estimator is unbiased if j[FN ] = 0 
for sample size N: E[FN ] = Q for allN 2 1. (1.6) (1.7) An obvious advantage of the unbiased estimator 
is that we are guaranteed to get the correct value of quantity of interest Q if enough samples are taken. 
Further­more, the expected value of an unbiased estimator will be the correct value after any number 
of samples. The mean square error of the estimator can be also writ­ten as MSE[FN ]= V [FN ]+ j[FN ]2 
. (1.8) For unbiased estimators, the MSE is the same as the variance. For biased estima­tors, the error 
is much more di.cult to estimate. It is also important to know that a biased estimator may not give a 
correct estimate for Q even if an in.nite number of samples are taken. In practice, a biased estimator 
may have some de­sirable properties, such as lower variance, which makes it very appealing for use in 
computer graphics. For example, in rendering, noise is a manifestation of vari­ance. While taking more 
samples reduces the amount of noise, rendering using a biased estimator may have less noise for the same 
number of samples albeit producing di.erent images. Consistency An estimator is consistent if the error 
goes to zero as the number of samples grows: Pr{ lim FN = Q} =1. (1.9) N-* The above equation is essentially 
saying that if we use a consistent estimator, we are one hundred percent certain that the answer is correct 
if we increase the num­ber of samples. Consistency is a stronger condition than requiring the estimator 
to be unbiased. It is still possible that an unbiased estimator is not consistent, in which case its 
variance is in.nite. A biased estimator is consistent if its bias j decreases to 0 as the number of samples 
N increases. 1.1.2 Monte Carlo Integration The basic idea behind Monte Carlo integration is evaluation 
of the integral . I = f(x)dx (1.10) . using random sampling. Here, N random points X1,X2,...,XN are independently 
sampled from some density function p and used to approximate I, N . 1 IN = f(Xi). (1.11) N i=1 Notation 
note: A realization of an estimator F , namely FN , is the same as I N. The subscript N emphasizes that 
I is still a random variable and therefore its properties depend on how many samples were chosen. As 
N increases, the expected error of this estimate decreases. We want to choose N such that we have con.dence 
that the estimate I N is good. The estimator I N is a crude but unbiased estimator for I and its variance 
is N . 1 1 V [I N ]= V [ f(Xi)] = V [f(X)]. (1.12) N N i=1 From this variance estimate V [I N ] we can 
conclude the following: 1. The standard error of the estimator decreases with the square root of the 
sample size N. Recall that the standard error of I N is V [I N ]2, so while the variance of the estimate 
is proportional to 1/N the standard deviation is proportional to 8 1/ N. Therefore, to reduce the error 
in half, we have to quadruple the number of samples. 2. The statistical error is independent of the dimensionality 
of the integral. This simply means that the computation does not increase exponentially when the dimensionality 
of the integral increases. So far, we have not made any assumptions about function f(x) we are trying 
to in­tegrate. On the other hand, in the above discussion we have assumed that our random variable X 
is uniformly distributed over the integration domain .. Loosely speaking, a uniform distribution implies 
that the probability of choosing each sample is equal. Un­fortunately, real problems are rarely this 
simple. For example, function f(x) can be zero in many regions and have very high values in other. If 
uniformly sampling the domain ., we may get very large variance. Also, sometimes it may not be possible 
to sample a space uniformly. In order to alleviate these problems, we can rewrite the estimator from 
Equation (1.11) as . I = f(x)dx .. f(x) = p(x)dx, . p(x) where p(x) is a probability density function 
in .. We can now generate N samples from distribution p(x) (instead of uniformly sampling .) to get . 
1 Nf(Xi) I p = . (1.13) Np(Xi) i=1 The simple Monte Carlo estimator was saw in Equation (1.11) is just 
a special case of the more general estimator in Equation (1.13) with p(x) being a uniform distribution 
in .. This estimator has the same variance properties we have seen above. One major advantage of Monte 
Carlo integration is that it is easy to understand and simple to use. If we can generate random samples 
using some density p(x) and have the f(Xi) ability to compute the sample weights, wi = ,i =1,...,N, then 
we can evaluate p(Xi) the integral. Monte Carlo methods are also .exible, robust and work well in higher 
dimensions where other numerical methods might fail. 1.1.3 Variance Reduction Techniques One of the biggest 
disadvantages of Monte Carlo methods is a relatively slow conver­gence rate. As we have already discussed 
above, the root mean square (RMS) error 8 converges slowly at a rate of O(1/ N), so we need to quadruple 
number of samples N to halve the error. Ideally, we would like to use an estimator which has both small 
variance and is com­putationally e.cient. E.ciency of a Monte Carlo estimator F is 1 .[F ]= (1.14) V 
[F ]T [F ] where V [F ] is the variance and T [F ] is the time needed to evaluate F. Therefore, the more 
e.cient the estimator is the lower the variance in a given (.xed) amount of time. One of the fundamental 
goals in researching Monte Carlo methods is to .nd or design e.cient estimators. These techniques are 
often calledvariance reduction techniques and include importance sampling, control variates, and adaptive 
sampling. We brie.y review some of these techniques. In later sections, we apply these techniques to 
the direct illumination rendering problem. Importance Sampling Recall that a Monte Carlo estimator for 
some function f(x) over domain . is . f(x) I = p(x)dx . p(x) and the estimator is . 1 Nf(Xi) I p = . 
Np(Xi) i=1 The variance of the estimator I p depends on the density p(x) from which random sam­ples are 
drawn. If we choose the density p(x) intelligently, the variance of the estimator is reduced. This is 
called importance sampling. p(x) is called the importance density f(Xi) and wi = is the importance weight. 
p(Xi) The best possible sampling density is p*(x)= cf(x) where c is proportionality con­stant 1 c = . 
. (1.15) . f(x)dx Here, the constant ensures that p* is normalized (i.e., it integrates to 1). The density 
p*(x) yields an estimator with zero variance. In practice, we cannot use this density, because we must 
know the value of the integral we want to compute to evaluate c. However, if we choose an importance 
density p(x) that has a similar shape to f(x), the variance can be reduced. It is also important to choose 
an importance density p such that it is simple and e.cient to evaluate. In practice, p can be designed 
by doing some of the following: 1. Discard or approximate some parts of f(x) such that function g(x)= 
f(x)p(x) can be integrated analytically. 2. Construct a low dimensional discrete approximation of f(x). 
 3. Approximate f(x) by using Taylor expansion.  After g(x) is designed with any of the above methods, 
the density is then set to p(x) e g(x). We show in next sections how to choose and compute densities 
in practice.Note: If the sampling density is not chosen carefully, the variance can be increased and 
can actually be in.nite. Importance sampling is very e.ective when function f(x) has large values on 
small portions of the domain. Another common problem that happens in importance sampling is when the 
sampling density has a similar shape to f(x) except that f(x) has longer (wider) tails. In this case, 
the variance can become in.nite. While importance sampling is a useful and powerful technique it should 
be used with care. Inappropriate importance density can result in poor estimates of the integral. Strati.ed 
Sampling If we partition integration domain . into a set of m disjoint subspaces .1,..., .m (strata), 
we can evaluate the integral as a sum of integrals over the stratum .i. If we generate ni samples in 
each stratum (subspace .i), the estimator becomes .. m1 ni I = f(Xi,k) (1.16) ni i=1 k=1 whose variance 
is mVi . V (I ) = (1.17) ni i=1 where Vi is the variance of f(x) in stratum .i. The expected error of 
this method, strati.ed sampling, is never higher than variance of ordinary unstrati.ed sampling [14]. 
However, strati.ed sampling is often better than importance sampling. The two meth­ods can be combined 
to lower variance even further. Strati.ed sampling works well for low-dimensional integration, but it 
does not scale well for integrals of high dimension­ality. The number of samples must also be chosen 
such that there is at least one sample drawn from each stratum. Control variates If we can rewrite the 
estimator as .. I = g(x)dx +(f(x) - g(x))dx (1.18) .. where function g(x) can be analytically integrated 
and has the following property: V [f(x) - g(x)] : V [f(x)] (1.19) then a new estimator is . N . 1 f(Xi) 
- g(Xi) F = g(x)dx + . (1.20) . Np(Xi) i=1 The variance of this new estimator will be lower than the 
original estimator. How do we decide whether to use importance sampling or control variates? Given function 
g(x) that is an approximation of f(x), then g(x) can be used either as an importance density or a control 
variate. If f(x) - g(x) is approximately uniform (constant), then using g(x) as a control variate is 
more e.cient. If f(x)/g(x) is approx­imately constant, then using importance sampling is more e.cient. 
Note that if g is proportional to p then the two estimators di.er only by a constant, and have there­fore 
the same variance. If g is already used as the importance density, it would not be useful as a control 
variate, because the variance would not be reduced. Another cri­teria for choosing between importance 
sampling and control variates is whether g(x) can be integrated analytically (control variates may be 
preferable) or g can be sampled analytically (importance sampling). Defensive importance sampling We 
already mentioned in Section 1.1.3 that even if a sampling density p(x) has roughly the same shape as 
a target function f(x), but f(x) has longer tails, importance sampling will fail. When we draw a sample 
from the tails of p(x), the importance weight can be many times larger than weights in other parts of 
p(x). This causes high variance and in extreme cases the variance can be in.nite. This de.ciency can 
be address with defensive importance sampling [9] which uses a defensive mixture distribution pa(x) instead 
of only the density p(x): pa(x)= aq(x) + (1 - a)p(x). (1.21) Here, 0 <a< 1 and q(x) is the target distribution. 
If we want to compute the integral . I = f(x)q(x)dx (1.22) . where q(x) is the target density on the 
integration domain. The defensive mixture dis­tribution pa(x) guarantees that the variance is bounded 
by 1/a times the variance of the uniform distribution estimator. It also bounds the importance weight 
to 1/a. However, oftentimes it may not be easy or possible to sample from the target density q(x). If 
q(x) can be decomposed into a product of several (simpler) densities, q(x)= q1(x),...,qn(x) and each 
qi(x) can be easily sampled, then a more general mixture distribution of m densities can be used: m . 
pa = a0p(x)+ ajqj(x). (1.23) j=1 Here, the sum of all weights aj is one and each weight is greater than 
zero. Multiple Importance Sampling Many times we have to integrate a complex function whose target distribution 
has mul­tiple modes (peaks or bright regions) and sampling with a single importance density may not capture 
all regions of the integrand. For example, this is a very common problem in rendering. If our scene contains 
di.use and glossy surfaces illuminated by small and large area lights, we face a di.cult decision about 
what sampling strategy to use. For di.use surfaces, one sampling strategy might be preferable to another. 
However, the opposite might be true for glossy surfaces. Suppose that we have n di.erent densities, p1(x),...,pn(x), 
and generate ni samples for each pi(x). Now, we have many di.erent sampling strategies that work well 
in dif­ferent regions of the integrand, but are not good over the entire domain. The question is how 
to combine multiple strategies that minimize the overall variance without intro­ducing bias. As a naïve 
approach, one could average the sampling strategies. However, this will not produce optimal results [19]. 
Instead, we combine all n sampling strategies giving the estimator .. n1 nif(Xi,j) F = wi(Xi,j) (1.24) 
ni pi(Xi,j) i=1 j=1 where weight wi, w1,...,wn, provide weight for each sample drawn from some sampling 
strategy pi. All weights must be non-zero and the total sum must be 1 to ensure that the estimator remains 
unbiased. An obvious weighting function would be pi(x) wi(x)= ci (1.25) q(x) where q(x)= c1p1(x)+ ··· 
+ ckpk(x) (1.26) and all coe.cients ci are nonzero and sum to 1. In general, the best choice of weights 
turns out to be nipi(x) wi(x)= . . (1.27) k nkpk(x) If we take exactly one sample, ni =1 from each sampling 
density, the weight wi will be set according to current sampling strategy at x compared to the rest of 
the strategies: nipi(x) wi(x)= . . (1.28) k nkpk(x) This weighting strategy is called the balance heuristic 
and is nearly optimal. It is possible to design better strategies for special cases, but universally 
the balance heuristic out performs most other stratigies. What is the di.erence between MISand Defensive 
Importance Sampling?Multiple importance sampling (MIS) is optimal for a given set of sampling strategies. 
However, if we have chosen a bad or inadequate sampling strategy, MIS will not reduce variance. For example, 
if one of the strategies takes too many samples from low-valued regions and not enough from high-valued 
regions, the variance will increase. On the other hand, defensive importance sampling (DIS) can improve 
variance when a sam­pling strategy p is inadequate. When combined with a uniform density, DIS guarantees 
that the integrand will be sampled over entire domain. MIS with balance heuristic can be viewed as a 
special case of DIS (see Equation (1.21), and factor a coming from balance heuristic weights). We will 
show in later sections how to use MIS in practice and how to design sampling strategies for rendering. 
1.2 Multiple Importance Sampling Recall that one of the main objectives in rendering is to approximate 
the illumination integral in Equation (??): . Lo(x,.o)= Li(x,.i)f(.o,.i) cos aid.i . which can be split 
into three components: incoming illumination Li, cosine weighted BRDF B and visibility function V . If 
we drop the spatial and angular, the illumination integral becomes . Lo = LiBV d.i (1.29) . and the traditional 
Monte Carlo estimator is N . 1 LiBV Lo = (1.30) Np(.i) i=1 where p(.i) is the importance sampling density. 
Ideally, the density function would be proportional to the product of LBV . Unfortunately, this is impossible 
for all but some arti.cially contrived scenes. We have to resort to some other density that will hopefully 
generate low variance in estimate. Let us examine a few possible options. When we have a di.use BRDF 
and multiple area lights of di.erent sizes, we have two obvious choices for importance sampling densities. 
We can either sample according to the di.use BRDF or lighting. Figure 1.1 illustrates the two scenarios. 
Using the di.use BRDF, we sample the entire hemisphere, but only small portions of the hemisphere contain 
any lighting. So, many samples are completely wasted since the contribution will be zero. On the other 
hand, if we sample according to the lighting, none of the samples will be wasted because for any direction 
in which light is emitted the BRDF will re.ect some light. For di.use surfaces, it is better to sample 
according to lighting only. When we have glossy surfaces and many area lights, we can also sample according 
to the glossy BRDF or lighting densities. Figure 1.2 shows two sampling scenarios. In contrast to di.use 
surfaces, glossy surfaces re.ect light from a small solid angle. Using light sampling densities, most 
of the samples will be wasted because the surface will not re.ect any light from those directions. Therefore, 
a better choice is to sample according to the glossy surface re.ection, because there is a much larger 
chance that at least some sampled directions within a re.ectance cone will have non-zero lighting contributions. 
When we have very glossy surfaces or di.use only surfaces, the choice of sampling densities is fairly 
obvious. As highly glossy surfaces become duller (more di.use) the choice becomes murkier and not straightforward. 
For slightly glossy surfaces, a combi­nation of two sampling strategies should be used. Figure 1.1: 
Di.use BRDF and area lights. When we have a di.use BRDF and mul­tiple area lights of di.erent sizes, 
two obvious sampling density choices are lighting (left) and BRDF (right). Note that BRDF sampling produces 
many samples that will be wasted, because there is no light emission in those directions. Sampling according 
to only the lighting produces lower variance.  So far, we have only looked at idealized situations where 
we have simple surfaces (composed of simple BRDFs) and no occluders. This is obviously an unrealistic 
situ­ation. As shown in Figure 1.3 for a di.use only surface, once we add occluders to the scene the 
sampling strategy becomes more complicated. Occluders can prevent light reaching the surface. Sampling 
according to lighting will generate proper directions, but lighting from those directions might be blocked. 
For the time being, we ignore vis­ibility in our sampling density but we will return to it later and 
discuss what we could do to incorporate visibility into the sampling density. It is clear that complex 
lighting, surface properties, and occlusions cause the function we want to approximate to be complex 
and discontinuous. This function can have many bright and dark regions and intensities can di.er by orders 
of magnitude. Since the function is very complex and does not have a nice formulation (due to occlusion) 
it is clear that either our sampling density will be complex or that we need more than one sampling density. 
Veach and Guibas [19] have demonstrated that by combining multiple sampling strate­gies, the variance 
can be reduced in situations where a single sampling strategy is bad (see Figure 1.4). Howdo weimplementMultipleImportanceSampling? 
Given the two sampling strategies for lighting and BRDF discussed in Section ??, let p1(x) and p2(x) 
be a BRDF and light sampling density. The random variables X and Y are then X1,i " p1(x) X2,i " p2(x) 
f(X1,i) f(X2,i)Y1,i = Y2,i = . p2(X2,i) p1(X1,i) Now, we just need to combine the samples together: Yi 
= w1Y1,i + w2Y2,i. (1.31) The only remaining question is how to compute weights wi(x). We have already 
men­tioned a few possible options in Section 1.1. One is using the balance heuristic, where the weights 
are: pi(x) wi(x)= (1.32) p1(x)+ p2(x) and the .nal PDF p(x) for the combined sampling densities is: p(x)= 
w1(x)p1(x)+ w2(x)p2(x). (1.33) Now, we have all the ingredients to implement multiple importance sampling. 
One of the remaining questions is how do we choose the number of samples for each sampling strategy. 
There are several possibilities: Select a .xed number of samples for each strategy. For example, if 
N = 100, then N1 = 50 would be used for lighting sampling and N2 = 50 for BRDF sampling. Note that this 
is a relatively safe choice, although it could lead to suboptimal sample generation. If the BRDF is very 
glossy, some of the samples might be wasted, because too many samples are allocated for lighting sampling. 
 Alternatively, the number of samples for each sampling strategy can be adjusted based on a heuristic, 
such as a combination of the solid angle of the light and glossiness of the surface. A reasonable strategy 
might be to have a minimum number of samples that will be taken according to each strategy and then dis­tribute 
the rest based on the heuristic. For instance, if N = 100, we might allo­cate 20 samples to each sampling 
strategy. The remaining 60 samples would be distributed based on the glossiness of the surface and the 
light s solid angle.  Notes. Multiple importance sampling is an unbiased method for reducing the variance 
of Monte Carlo estimators. However, if it is used in conjunction with .ltered impor­tance sampling (e.g., 
.ltered importance sampling is used to .lter environment lighting) the method is biased due to the nature 
of FIS. For visual e.ects applications, this is not troublesome as the noise can be greatly reduced. 
While MIS reduces the variance, there are still con.gurations where the variance will be high. As Kollig 
and Keller [12] pointed out, multiple importance sampling attempts to hide a weakness of using a single 
density function. If, however, only one sampling density exists for some region of our integration domain 
., the multiple importance sampling will revert to a standard importance sampling. Kollig and Keller 
call this an insu.cient set of techniques [12]. We emphasize again, if inappropriate sampling densities 
are chosen, multiple importance sampling will not help to reduce the variance. 1.3 Practical Notes on 
Monte Carlo Sampling 1.3 Practical Notes on Monte Carlo Sampling 1.3.1 Choosing Sampling Density The 
e.ectiveness of importance sampling depends on the choice of the importance sampling density p(x). Figure 
1.5 shows the di.erences between uniform and impor­tance sampling. Figure 1.5: (Left) A function f(x) 
can has many peaks. There might not be a sin­gle importance sampling density p(x) that can capture regions 
where the function f(x) has large values. (Middle) If samples (in red) are chosen uni­formly, the variance 
will be high, because we oversample regions where the function is low (dark regions) and undersamples 
regions where the function is high (bright regions). (Right) If appropriate sampling density is used, 
we take many more samples (in green) in regions where the function has high values and thus reduce the 
variance. Figure 1.6 illustrates why the choice of importance sampling density is crucial for vari­ ance 
reduction. The examples in the .gure demonstrate that inappropriate sampling density can increase variance, 
which can even become in.nite. 1.3.2 Filtered Importance Sampling For Area Lights Previous sections 
have described in great detail how to apply .ltered importance sam­pling for in.nite (hemispherical) 
lights. A small extension could be used for .ltered importance sampling for textured area lights. 1.3 
Practical Notes on Monte Carlo Sampling Recall that each sampled ray has a solid angle: 1 .s = . Np(a, 
.) When the intersection with the sampled ray is found, we can approximate the area on the surface of 
the hit object by looking at the distance to the hit object and the solid angle of the ray: .xi - x.2 
· .s A(xi) = . (1.34) cos ai For the above to hold true, we assume that the cross-sectional area is locally 
.at (Fig­ ure 1.7). Now that we have the estimated cross-section of the intersection A(xi), we can estimate 
the mipmap level l based on this area: () 1 A(xi)1 l = log2 =(log2 A(x1) - log2 Apixel) (1.35) 22 Apixel 
where Apixel is the object space area covered by one pixel. It is used to convert areas from object space 
to texel space. We can use this formula to .lter the texture on area lights when using multiple importance 
sampling. It is important to recognize that this is a crude approximation. When part of the ray footprint 
is outside the textured area, the light contribution will be underestimated and wrong. Still, this approximation 
gives plausible results. 1.3 Practical Notes on Monte Carlo Sampling 1.3.3 What About Visibility? We 
have seen in Section 1.1.3 that the idealMonte Carlo estimator should be N . 1 LiBV Lo = . Np(.) i=1 
So far, we have been focusing on sampling from either the lighting, BRDF or a combina­tion of sampling 
strategies using MIS. However, several recent sampling algorithms ex­plicitly compute and sample an approximation 
of the product between the lighting and BRDF. Two-stage importance sampling [5] and quadtree-based product 
sampling [4, 3] hierarchically approximate a BRDF at a given point on the surface and combine it with 
the incoming light Li. Some of these methods can be fairly costly or may require pre­computed data structures 
for BRDFs. If BRDFs are spatially varying, some of these methods may not be practical since they would 
require too much storage for all the BRDFs in the scene. We can take this further by adding visibility 
into the mix to lower the variance. Sam­pling from a triple product of lighting, BRDF and visibility, 
LBV , is di.cult at best. Exact visibility would take a long time to pre-compute and would be expensive 
to store. We can use an approximate visibility V¯, thus making the estimator LBV¯. The problem ¯ with 
this approach is that approximate visibility V would have to be nonzero every­where true visibility V 
is nonzero. This brings us back to the initial problem, because in order to guarantee this condition 
is satis.ed we would have to compute visibility in all directions. On the other hand, we can use the 
estimator LBV¯as a control variate: N. 1 . LiBV¯- aLiBV¯ Lo =+ aLiB ¯(1.36) V d.i. Np(.) i=1 Remember 
that a control variate requires the function f - g to be approximately constant. Even if visibility is 
crudely approximated, this condition can be satis.ed. Clarberg and Akenine Möller [2] analyze the variance 
for this case and describe an algorithm for using approximate visibility: Create a compressedvisibility 
cache using a compact bitwise representation stored at a sparse set of points in screen space.  Compute 
a rough estimate of LiBV¯using the approximations.  Evaluate the di.erence between this approximation 
and the correct solution us­ing Monte Carlo integration  The reader is directed to [2] for detailed 
discussion and implementation notes. 1.3 Practical Notes on Monte Carlo Sampling 1.3.4 Resampled Importance 
Sampling E.ciency of a Monte Carlo estimator depends on the expense of the sample evaluation versus the 
cost of drawing a sample from better densities. For example, if function is cheap to compute, but .nding 
the importance density is computationally expensive, there is probably an advantage to using a simpler 
sampling density with more samples. In rendering, casting visibility rays can be expensive and oftentimes 
we still want to avoid tracing too many shadow rays. Consider a situation where we have a glossy surface 
and we choose N samples based on the BRDF density. We also know that the surface is very glossy and therefore 
the cone around re.ected lobe is fairly narrow. We might be able to use less than N visibil­ity rays 
to approximate the light transport integral. We can do that by using resampled importance sampling [17]. 
The idea is that from N partial estimates (BRDF times light­ing, LiB) we only choose M values for which 
we will compute the visibility. More formally, resampled importance sampling is a generalization of importance 
sampling that permits unnormalized sampling densities or di.cult to sample densi­ties (in our case, visibility) 
denoted as q. In rendering, the best density q would be q e LiBV , but we can realistically at best only 
sample from another density p that is proportional to LiB. Instead of sampling from q, we generate a 
set of samples from a source distribution p and weight these samples appropriately. Then, we resamplethese 
samples by drawing a single sample from them with probability proportional to its weight. The basic algorithm 
proceeds as follows: Choose a set of sample Xi from a known distribution p q(Xi) Associate a weight wi 
= with each Xi, where q is the desired (possibly un­ p(Xi) known distribution) Generate the .nal samples 
Yi by sampling Xi with a distribution proportional to wi q(Xi) If the weights wi are chosen to be wj 
= then the resulting samples Yi will be p(Xi) approximately distributed to q. The processes of resampling 
is equivalent to .ltering. In rendering applications, we use importance resampling as follows: Generate 
N samples from some proposal distribution p(x). This is as before, where we created samples from either 
lighting density, BRDF density or com­bined MIS density.  Compute the weights (e.g., luminance) of partial 
contributions (LiB, but no visi­bility yet).  Compute the discrete distribution from these weights. 
 1.3 Practical Notes on Monte Carlo Sampling  Chose M samples from the above N samples. These samples 
are chosen based on the importance density that we computed in previous step.  Shoot shadow rays for 
these M samples, add computed visibility to each sample and apply proper weighting (based on the probability 
with which each sample was chosen).  Note that although the desired target density q(x) is unknown a 
priori because of the visibility, we never sample from it. We only need to be able to evaluate it and 
that is straightforward as long as as we can evaluate visibility V . Resamples Importance Sampling (RIS) 
is better than importance sampling when: q is a better importance sampling density than p.  Computing 
proposals is much cheaper than computing actual samples.  RIS takes advantage of di.erences in the variance 
computation expense. More details and examples can be found in [17]. 2 Metropolis Light Transport Metropolis 
light transport (MLT) is often considered to be the most sophisticated of the unbiased light transport 
simulation algorithms, with a reputation to be able to ren­der scenes e.ciently where other methods fail. 
To be fair, MLT also has its .aws and much of the e.ciency is tightly related to whether the correct 
mutation strategies for the problem are implemented or not. MLT certainly is not the answer to each and 
ev­ery light transport problem, but its high robustness often make it the tool of choice to render demanding 
scenes, like architectural shots with pronounced caustics. A common misconception about MLT is that is 
generally hard implement. It can cer­tainly get quite complicated, mainly depending on how involved the 
mutation strategies are and how many are implemented. But then one can also implement a simple MLT variant 
in very few lines of code. MLT de.nitely is quite di.erent from classical Monte Carlo rendering approaches 
and also has somewhat di.erent requirements on what functionality the rendering core needs to provide. 
The amount of special functionality needed, however, is heavily depending on what .avor of MLT is actually 
implemented. In general MLT is more like a family of approaches than a single light transport algo­rithm. 
In the following we give a mostly self-contained introduction to MLT, also addressing some implementations 
issues. The goal is to establish an understanding of how MLT works, when it works best, and what its 
di.erences to other Monte Carlo light transport algorithms are. 2.1 Metropolis Sampling The Metropolis 
sampling algorithm [13, 8] is a powerful method to generate a process of samples that are, in the limit, 
distributed according to any target function f. Since the only restriction posed on f is that we need 
to be able to evaluate it, this provides means to (importance) sample functions that are hard or impossible 
to handle by other means. We will outline some of the theory and the motivation behind this method without 
going into to much detail (for a thorough discussion we refer to [10]). To understand how Metropolis 
sampling works we look at a stochastic system given on a domain . of states and featuring a concept of 
energy .ow between two states. Assume we have such a system and the energy .ow from on state x to another 
state y is governed by a probability density function K(y|x). If we now assume that this system is in 
equilibrium (i.e. the energy concentration per state does no longer change over time) and that the energy 
in each state x is expressed by our target function f(x), we have f(x)K(y|x)= f(y)K(x|y). (2.1) This 
condition, called detailed balance, states that the amount of total energy .ow from x to y has to be 
the same as the amount that is .owing into the opposite direction. If this would not be the case, the 
system would not be in equilibrium. For some applications such a system would be a real world physical 
process and T (y|x) would be given by the physical laws driving that process. In our case the system 
is merely a mathematical construct and we do not directly know what a suitable K(y|x) for the target 
function f(x) would to look like. The key idea now is to replace K(y|x) by an almost arbitrary distribution 
T (y|x) and only accept a movement from x to y with probability a(y|x). This means that we have set K(y|x)= 
a(y|x)T (y|x) and detailed balance is now expressed as f(x)a(y|x)T (y|x)= f(y)a(x|y)T (x|y) which leads 
to the relation a(y|x) f(y)T (x|y) = a(x|y) f(x)T (y|x) for the acceptance probability. We can ful.ll 
this relation by setting {} f(y)T (x|y) a(y|x) := min , 1. (2.2) f(x)T (y|x) The stochastic system de.ned 
by a(y|x)T (y|x) is a Markov chain, where the tran­ ' sition from the current state x to the next state 
x is based on a proposal y (chosen with probability density T (y|x)) and an acceptance probability for 
that proposal, i.e we '' set x = y with probability a(y|x) and x = x with probability 1 - a(y|x). Since 
we have constructed a(y|x) such that we obtain detailed balance for the target function f(x), the chain 
s stationary distribution p(x) is proportional to the target function, i.e. p(x)= f(x) where the normalization 
b = . f(x)dx does not need to be known for the b . method to work. If we now simulate a trajectory of 
this Markov chain we know that once it has reached the stationary distribution its states are distributed 
according to the target function f(x). Simulating the Markov chain thus provides the means to draw samples 
from f: this is the Metropolis sampling algorithm. Let us summarize what we need to imple­ment it. We 
need to be able to evaluate the target function f.  We need to implement a mutation strategy that given 
a state x creates a tentative state y according to a probability density T (y|x). In order to ensure 
the ergodicity of the resulting Markov chain we need to have T (y|x) > 0 whenever f(x) > 0 and f(y) > 
0. This also ensures that equation 2.2 is always well de.ned for all proposals we create.  In order 
to compute the acceptance probability we need to be able to compute both T (y|x) and T (x|y) (ideally 
they are equal and cancel in equation 2.2).  We can only use the states produced by the simulation process 
after the station­ary distribution has been reached, thus we need to discard a certain amount of samples 
in the beginning. This issue, known as start-up bias, is generally a hard one but can be avoided or compensated 
under certain circumstances.  Apart from the points we just mentioned there is a lot of freedom to construct 
the mutations which is one of the key strengths of the algorithm. Often a Gaussian along each coordinate 
axis (and centered in the current state) is used as mutation. This favors small step sizes but still 
ensures that the whole domain is covered. However, there is little restriction on what the mutation strategy 
has to look like. In fact it can be hand tailored to the problem that is faced, e.g. we can construct 
mutations targeted to explore certain lighting e.ects. Also we do not need to restrict ourselves to a 
single mutation strategy, we can randomly choose from a set of possible mutations T1,...,Tn. . nIf we 
choose Ti with probability pi (where pi =1), then we have the probability i=1 . n density of the combined 
strategy as T (y|x)= piTi(y|x). i=0 2.2 Application to Light Transport Photorealistic image synthesis 
can be seperated into two parts, computing the radiance function throughout the scene (and how it arrives 
in the camera model) and then pro­jecting that function onto a discretized two dimensional image, i.e. 
the grid of pixels. The projection is de.ned by a pixel .lter function wj and the value of pixel j is 
given as . vj = f(x)wj(x)dx. (2.3) . Here . is the domain where the light transport paths are de.ned 
on and f gives the measurement contribution of such a path. The pixel .lter function wj is de.ned on 
the image plane (which is a two dimensional subset of the whole integration domain .) and usually has 
a small support. Estimating the radiance function and projecting it to a pixel are often tightly con­nected 
in an implementation since it feels natural to work on a per pixel basis. A typical example is a forward 
path tracer that creates the image pixel per pixel by shooting rays through that pixel s .lter s support. 
However, we can also view this as an integration problem over the whole image plane, where rays are started 
from random positions on the image plane and just those pixels values are in.uenced where wj is non-zero. 
Like this a forward path tracer that works per pixel can interpreted as sampling that works on the whole 
image plane and employs stratis.cation such that each pixel receives the same number of samples. In the 
following we will work on the whole image plane and not compute integrals pixel by pixel. Of course we 
still compute the integral vj per pixel, but we do this by sampling x using some density p(x) on the 
whole domain ., and thus on the whole image plane. (Usually the .rst two dimensions of . will be the 
image plane and the p(x) is a product density where a uniform distribution is used for those .rst two 
dimensions.) For x1,x2,... sampled according to p(x) we now can compute f(xi)/p(xi), and wj(xi) for all 
pixels (for most of which wj(xi) will be zero), yielding a Monte Carlo estimator that computes all pixel 
integrals: []. N . 1 f(xi) vj = f(x)wj(x)dx = E · wj(xi). . N i=1 p(xi) Now it is just one more step 
to MLT. Instead of using the density p we use the Metropolis sampling algorithm with target function 
f to create samples x1,x2,.... If we assume we have already reached the stationary distribution then 
those samples are distributed according to f and we have []. N . 1 vj = f(x)wj(x)dx = Eb · wj(xi). N 
. i=1 The samples from the Markov chain simulation randomly move around the image plane, with a distribution 
proportional to the radiance arriving on the image plane. As such we just need to accumulate a histogram 
per pixel, where we add up b · wj(x) for each Markov chain sample falling into the support of the pixel. 
This is MLT: 1. Estimate normalization constant, e.g. by using a Monte Carlo estimator . N . 1 b = f(x)dx 
= f(xi)/p(xi). (2.4) N . i=1 2. Choose a initial sample x0, set x = x0. 3. For i =1 to M do:  y = mutate 
x (according to T (y|x)) {} f(y)T (x|y) compute a = min , 1 f(x)T (y|x) set x = y with probability a 
 for all pixels j with wj(x) > 0, add value wj(x)·b  M Strictly speaking, step 1 can already be a hard 
problem. However, let us put this into perspective by comparing it to the problem of computing an actual 
image. Think about computing a 1000 × 1000 image with one sample per pixel: this already translates to 
having thrown one million samples at equation 2.4. And even if the normalization con­ stant is o. by 
small amount, this basically only means that the resulting image will just be a bit too dark or a bit 
too bright. Unless we want to render an animation this might not be as important after all. Using the 
Expected Value for Contributions We will accept the proposal y from the current state x with probability 
a(y|x), so we will add a contribution b · wj(y) for state y (i.e. to all pixels j with wj(y) > 0) with 
probability a(y|x) and a contribution b · wj(x) for state x with probability 1 - a(y|x). Instead of randomly 
adding the contribution for one state or the other, we simply add the expected value of contribution 
for both states, which means we add b · wj(y) · a(y|x) for state y and b · wj(x) · (1 - a(y|x)) for state 
x. Colors The outlined MLT algorithm assumes that f(x) is a scalar function and thus computes a scalar 
output image. Since we usually are interested in color output we need to apply some minor modi.cation. 
For a vector-valued f(x) we use an arbitrary intensity function l that transforms those vectors to scalars, 
and then use the scalar­valued fl(x) := l (f(x)) as target function. Instead of recording equal valued 
contributions of magnitude b on the image plane we now record bl · f(x)/l(f(x)) where bl is the normalization 
constant resulting for the scalar-valued target function fl. A typical choice for l is the CIE luminance 
of the computed color. The intention here is to have the distribution follow the response characteristics 
of the human eye. However, sometimes is better to be more conservative and not to undersam­ple colors 
that have lower CIE luminance, e.g. by using the maximum of all color channels as l.  For full spectral 
simulation, the wavelength is usually one more dimension of the problem and can be sampled by the Metropolis 
sampling algorithm along with all the other components. Here the result is an intensity value and the 
sampled wavelength. Accumulation then usually takes place in a tristimulus color space, such as CIE XYZ, 
where the spectral values are converted to. So we could again use the CIE luminance or, more conservatively, 
just the spectral intensity value (without further wavelength dependent weighting) as l.  Start-Up Bias 
The Markov chain created by Metropolis sampling has the desired sta­tionary distribution but it can take 
a while until the simulation process reaches that distribution. The intuitive understanding here is that 
the process is biased towards a starting state which needs to be forgotten . Unfortunately, it is impossible 
to tell to when this is the case, it is depending on how correlated successive samples are and how many 
have been taken. If we simulate a single Markov chain start-up bias can mostly be ignored. As long as 
the initial state has a non-zero contribution (and thus a non-zero probability to be created by the Markov 
chain) it does not really matter where we start since the state could have been chosen anyways (albeit 
at low probability).  If many Markov chains are simulated in parallel all of the chains are biased towards 
the distribution used to sample the initial states. Since we can typically spent less samples per chain 
if we simulate many chains, this will in.uence the result for quite a while.  Start-up bias obviously 
would not be an issue if we could already choose the start­ing state according to the stationary distribution. 
This, of course, is something we probably cannot do otherwise there would be no need to apply Metropolis 
sam­pling in the .rst place. However, we can approximately do it using re-sampling from a larger set 
(see next item).  If the initial state x0 is chosen using a density p(x0) and if we use f(x0)/p(xo) 
as weight for all samples created by the chain (instead of b), the estimator is unbiased [20, 18], i.e. 
the expected value is the correct integral. Of course, this is not directly useful if applied to a single 
Markov chain, since it only leads to the correct image brightness in expectation, i.e. when many chains 
weighted like this are averaged.  Better is to sample a large number n0 of initial states x0,j according 
to p(x0,j) for j =1,...,n0 (those samples can also be used to estimate the normalization constant b). 
Then we re-sample a smaller set of n « n0 chains (or even a single chain) based on the values w(x0,j)= 
f(x0,j)/p(x0,j) and use an equal weight for the re-sampled chains [18]. 2.2.1 E.ciency Metropolis sampling 
follows the targeted function exactly which, from a Monte Carlo simulation point of view, is as good 
as we can get. Of course this property does not come for free: the samples are highly correlated. We 
can di.erentiate between two sources of correlation. Correlation due to rejection: if the probability 
of acceptance for proposals is low, we can get stuck in one state for a very long time.  Correlation 
due to small mutations: the smaller the mutation, the more similar successive states are.  Ideally, 
we would have a mutation strategy were the proposals are fully independent and are accepted with probability 
one. It is pretty clear that this would mean that we could already perfectly importance sample the target 
function by other means. In fact it boils down to exactly this: setting the acceptance probability in 
equation 2.2 to one and having independent mutations, i.e. T (b|a)= T (b), we have f(y)/f(x)= T (y)/T 
(x) which implies that f and T can only di.er by scaling, so T is already perfectly importance sampling 
f. Similarly as a standard path tracer can su.er from high variance problems when a highly contributing 
e.ect is sampled at low probability, the samples from MLT can yield high variance if the mutations are 
bad in .nding signi.cant regions of the integrand and have low acceptance probability. It is clear that 
the key to high e.ciency is good mutations strategies that keep the variance low. Asymptotically, however, 
the behavior of MLT is the same as for uncorrelated standard Monte Carlo path tracing, the variance per 
pixel is decreasing linearly in the number of samples taken [1], so we do not have a gain or loss from 
using correlated samples in general. A certain drawback of the Metropolis sampling approach is that stratis.cation, 
es­pecially across the image plane, cannot be obtained. There is no control to in.uence into what pixel 
a sample should fall to and the resulting variance in the amount of how many samples a pixel has received 
at a certain time leads to more low frequency noise even for simplest scenes. 2.3 Mutation Strategies 
The Metropolis sampling framework leaves a great degree of freedom to what the mu­tations that create 
a proposal state y from a current state x can look like. In practice there are basically two approaches 
for implementing mutations, one is to work on the sample numbers directly, the other to explicitly modify 
light transport paths. 2.4 Primary Sample Space Mutations The straight forward application of Metropolis 
sampling to light transport is to just mutate the random numbers (in the in.nitely dimensional unit cube 
I) that are used to drive an unbiased Monte Carlo light transport estimator [11]. This estimator, let 
us call it primary estimator, can be any kind of Monte Carlo light transport algorithm, e.g. a path tracer, 
a light tracer, or a bidirectional path tracer. Fed by a uniform ran­dom sample x . I it creates a light 
transport path (or a set of light transport paths) with a certain contribution, e.g. for a simple path 
tracer t(x)= f(x)/p(x). Although in expectation the contribution of t(x) recorded on the image plane 
is exactly what we want to compute, the e.ciency of the primary sampler might be low because the probability 
density p(x) just does not match some parts of the integrand well enough. In practice these are the parts 
were the primary sampler does not employ a suitable importance sampling technique, like caustics in the 
case of a simple path tracer. Even with the many techniques at hand with a bidirectional path tracer, 
there might be a high contribution e.ect that is just not handled by any of them. In order to address 
the problems of the primary sampler we can wrap it up in Metropo­lis sampler: we mutate the random sample 
x according to some probability density function on I. Generally two types of distribution are used: 
 A large step mutation proposes a (fully independent) new random sample, drawn from a uniform distribution. 
Here the proposal state does not depend on the current state, i.e. Tlarge(y|x)= Tlarge(y)=1.  A small 
step mutation adds a small o.set (following some convenient distribu­tion) to the current sample. Usually, 
the distribution is constructed such that Tsmall(y|x)= Tsmall(x|y), e.g. by sampling an independent and 
symmetric o.set per component.  We now randomly decide whether to use a large step mutation with probability 
plarge or a small step mutation with probability 1 - plarge. Then we have T (y|x)= plarge + psmall · 
Tsmall(y|x). Note that T is symmetric if Tsmall is symmetric, in which case we do not have to compute 
it since it cancels out in equation 2.2. Implementing Small Steps Small step mutations can be implemented 
by applying a small o.set to the current sample number, where we have to keep in mind that smaller steps 
are generally more likely to be accepted but yield higher correlation. A typical choice is to apply a 
random, preferably small, symmetric o.set per component, i.e. if x =(x1,x2,... ) we add an independent 
o.set per xi. The proposal in [11] is to use (y1,y2,... )=(x1 + smallstep(x1),x2 + smallstep(x2),... 
) where - ln(s2/s1)·ei smallstep(xi)= ±s2e for ßi uniformly distributed in [0, 1] and the sign chosen 
at random. This results in a minimum step size of s1 and maximum step size of s2 with higher probabilities 
for smaller steps. The recommendation from [11] is to use s1 = 1/1024 and s2 = 1/64. Multiple Importance 
Sampling for Large Steps Besides being a valid mutation strategy for the Metropolis sampler, large step 
mutations could also form a standard Monte Carlo estimator. In fact that standard Monte Carlo estimator 
can yield higher e.ciency for some regions of the integrand, essentially those regions which it oversamples. 
Since we have computed them anyways, it can thus be useful to interpret large step mutations as both 
samples of the Metropolis sampler and a standard Monte Carlo sampler [11]. Then we can combine the samples 
from the two techniques in a low variance fashion using multiple importance sampling [19]. In the terms 
of multiple importance sampling we have two techniques, the .rst tech­nique is Metropolis sampling and 
its probability density p1 (de.ned on the in.nite di­mensional unit cube) is proportional to the target 
function t(x)= f(x)/p(x), i.e. we have p1(x)= 1 t(x). The second technique is Monte Carlo sampling formed 
by large b steps, which are uniformly sampled and created with probability plarge, i.e. we have p2(x)= 
plarge · 1. The current state x is a sample from Metropolis sampling, using the balance heuristic we 
get a multiple importance sampling weight of p1/(p1 + p2). Com­bining it with the usage of the expected 
value, the contribution we add for the current state is p1(x) t(x) t(x) (1 - a(y|x)) ·· = (1 - a(y|x)) 
· . p1(x)+ p2(x) p1(x) t(x)/b + plarge Analogously, the proposed state y is a sample from Metropolis 
sampling, where it gets contribution p1(y) t(y) t(y) a(y|x) ·· = a(y|x) · . p1(y)+ p2(y) p1(y) t(y)/b 
+ plarge If the proposed state results from a large step mutation, it is also a sample from Monte Carlo 
technique and we have the additional contribution p2(y) t(y) t(y) · = . p1(y)+ p2(y) p2(y) t(y)/b + plarge 
As desired the weighting now favors the Monte Carlo estimator over the Metropolis estimator for large 
steps if t(y) is small and unlikely to be accepted. The higher plarge is, the more Monte Carlo samples 
we produce and the more they in.uence the weight for the Metropolis samples. Note that it is not necessary 
to have the exact value of b since multiple importance sampling does not require p1 to be the exact probability 
density in order to be unbiased. However, it should be a good approximation, such that the balance heuristic 
works well. Of course we could also use the power or maximum heuristics. Estimation of the Normalization 
Constant We can make further use of the fact that large step mutations compute valid samples for the 
primary estimator: by summing up t(x) we can progressively re.ne the normalization constant b. In fact, 
unless there is a need for early feedback or we want to employ multiple importance sampling for large 
step mutations, we can completely skip the step of pre-estimating the normalization constant and only 
compute it on the .y using large steps. Large Step Probability Essentially, we need to balance large 
step mutations, which reduce correlation and can help to move out of local maxima, against small step 
muta­tions, which explore the space more locally and are usually more likely to be accepted. Furthermore, 
we need to take into account that large steps can have additional value, such as to compute the normalization 
constant and (weighted by multiple importance sampling) as direct contribution for darker images areas. 
As there is no general best way to set plarge, a conservative strategy is to not set it too high and 
not set it too low. A relatively robust choice is to use a value between 0.1 and 0.5. Lazy Evaluation 
of Mutations Although in theory light transport is a in.nitely di­mensional integration problem, in practice 
only a limited number of light bounces are simulated, each of them usually consuming a limited amount 
of pseudo-random num­bers. Even if the primary sampler employs Russian roulette a real world implementation 
should limit the amount of bounces in order to avoid unpredictable runtime or even in.nite loops due 
to numerical imprecision (in .oating point world path tracers tend to get stuck somewhere). So let us 
assume that the maximum amount of sample numbers drawn for each path space sample of the primary estimator 
is limited. We can now store both the current state and the mutation state in a .xed size array of that 
maximum size. A naïve imple­mentation would loop over all samples of the current state, apply a mutation 
to that number, and store it in the mutation state. Then, if the mutation state is accepted, we overwrite 
the current state with the mutation state, otherwise we keep it unchanged. In practice, however, a path 
will likely terminate (much) earlier and not all dimensions are used. For e.ciency reasons it is therefore 
desirable that we only have to compute the mutations that are actually required. For large steps mutations 
this is very simple, here we do not need to take the current state into account and just create a new 
sample number dimension by dimension as needed. Small steps on the other hand need the previous state, 
so in case the previous path used less dimensions we might need to replay the history of previous mutations 
up to the current point in time. Of course we only have to do this starting from the last accepted large 
step, since that one is independent of any previous history. This is the implementation proposed in [11], 
where a time stamp is stored alongside each dimension, to keep track of how many small steps need to 
be performed since the last large step. A drawback is that sometimes this only postpones the e.ort: if 
a path is much longer then the previous one and it has been a while since the last large step was accepted 
the sampling gets very costly. So let us apply a small trick to simplify this a bit more. For dimensions 
that were not used in the previous state we just always apply a large step. This means we formally de.ne 
the probability density for a small step proposal of the i-th component as { 1, component i is used in 
f(x) or f(y) T (i)(yi|xi)= smallstep(xi), otherwise. Now we have T (y|x)= . m T (i)(yi|xi), where m is 
the minimum of the maxima of i=1 dimensions up to which we use the samples in f(x) and f(y). In fact 
we do not really need to care about that if we move from a longer to shorter path, because then we do 
not create the sample in the .rst place. However, by de.ning it like this we conveniently have T (y|x)= 
T (x|y). Now we only need to store the maximum dimension we have actually used along with the state and 
only need to check against that whether to do a small or large step for the current component. An additional 
bene.t is that we only need to back up an accepted state up to that number, potentially copying much 
less data. The resulting sampler implementation is quite compact, listing 2.1 contains the code in C. 
Mapping of Dimensions One key assumption for applying small mutations on the pseudo-random numbers directly 
is that small changes there translate to small changes on the resulting light transport path. For most 
cases this true, since the implementa­tions of importance sampling employed by the primary estimators 
typically are rather smooth functions. However, there are usually also random choices involved, like 
choos­ing between BSDF components, that can cause drastic changes for all subsequent ver­tices. To a 
certain extent this unavoidable, but we can try to minimize the impact by mapping the dimensions from 
the unit cube to the problem carefully. A bidirectional path tracer creates both a light and eye path 
by sampling a particle trajectory through the scene. Each of them is using a certain number of pseudo­random 
samples, which in the case of Russian roulette might not be known ini­tially. Now in order to have small 
changes on the sample result in small changes on the actual path it is important that the same dimensions 
are used for the same path segment. We absolutely should avoid that the length of the light path changes 
the dimensions used for the eye path (and vice versa). A popular way to do this is to use odd dimensions 
for one path type and even ones for other. Alternatively, we can just have two separate arrays of current 
sample points, one is used for the light path and one is used for the eye path. This is also useful for 
a lazy sampler implementation, since we can apply the trick to always do a large step mutation in case 
the current dimension was not used for the previous sample separately per path type.  Sampling the next 
path vertex from the existing one might need a di.erent amount of pseudo-random samples depending on 
the material model. On the extreme side we have perfectly specular re.ection which does not consume any 
samples, while a typical multi-component BSDF model usually consumes three. As a con­sequence, a small 
mutation might suddenly change subsequent vertices radically, just because a di.erent material is encountered, 
e.g. a direct light sample chooses a di.erent light source. Thus it often pays o. to always draw the 
maximum num­ber of samples per bounce even if they are not used (especially since the cost of generating 
a sample is negligible compared to cost of ray tracing and material evaluation).  Sometimes the number 
of samples needed to create the next vertex cannot be bounded. The typical example here is the usage 
of rejection sampling to impor­tance sample the BSDF which, fortunately, is seldom used in practice since 
most BSDFs o.er more convenient means to be sampled. Another example is using Woodcock tracking to importance 
sample the transmittance function of hetero­geneous participating media [21, 6], a key element for solving 
volume light trans­port in an unbiased way.  7 int current_dim; 10  16  22  27 else 30 33  42 
 Listing 2.1: Primary sample space lazy Metropolis Sampler Let us assume we use Woodcock tracking to 
sample the next interaction vertex, which might either be a point in the volume or the intersection point 
with the next surface along the ray. If the volume is thin along the current path, chances are that we 
have used a high number of sample points for Woodcock tracking and still reach the same surface. Here 
we ideally want to use mutations of the numbers we used for that vertex in the current state and nothing 
something that depends on the number of Woodcock tracking steps we did in the volume. However, for the 
Woodcock tracking process we of course also want to use mutations of the numbers that were used for the 
Woodcock tracking process before, since it might very well be that the volume is rather thick and the 
interesting e.ect we want to explore is a volume e.ect. In order to obtain both, we need to reserve the 
dimensions in a two dimensional fashion, e.g. an array of arrays, where each array is associated to one 
bounce and holds a .xed number of points for BSDF or phase function sampling, and an arbitrary number 
of points for Woodcock tracking [15]. In an implementation the arrays of course should be of .nite size, 
but even as such the memory requirements for storing the state are increased drastically. 2.5 Path Manipulation 
The original formulation of Metropolis light transport [20] constructs the mutations directly in path 
space. This has both advantages and drawbacks: while it allows to manipulate existing paths very locally 
and focused on very speci.c e.ects, it is no longer an orthogonal technique that can just be applied 
to an already existing primary light transport estimator. Here the mutations themselves shape the rendering 
core and more care needs to be taken in order to be able to compute T (y|x). We can basically di.erentiate 
between two mutation strategies, so called bidirec­tional mutations that create new paths from old by 
deleting or adding vertices, and pertubations, which change the positions of already existing vertices 
slightly. 2.5.1 Bidirectional Mutations Let us assume we have a light transport path x = x0 ...xk with 
k +1 vertices, where the .rst vertex x0 is on a light source and last vertex xk is on the lens. A bidirectional 
mutation now constructs a new path y by replacing a subpath of x with a new subpath (of possibly di.erent 
length). In order to compute a(y|x) we need to compute f(x), f(y), T (y|x), and T (x|y). The target function 
is the contribution function of the path, i.e. if we have a path x = x0x1 ...xk, then k-1 . f(x)= Le(x0 
~ x1)G(x0 x1)(fs(xi-1 ~ xi ~ xi+1)G(xi xi+1)) W (xk-1 ~ xk) i=1 We quickly recall the terms used in 
above equation, for details please refer to [18], Le characterizes the radiance emitted on the light 
source,  f(x ~ y ~ z) is the BSDF for light coming from point x that is scattered at point y into the 
direction of point z,  |cos Bx||cos By| G(x y)= V (x y) is the geometric term, the product of the 
.y-x.2 cosines at the points x and y over the squared distance times the binary visibility between those 
points, and W is the response of the camera model. In a standard Monte Carlo estimator f(x) is computed 
alongside a probability density p(x) used to create the sample x which yields the contribution of the 
sample, f(x)/p(x). In contrast to that, for MLT we are interested in the ratio de.ning the acceptance 
probability 2.2. Therefore note that we do not have to compute the parts of f that are shared between 
the paths x and y, since the cancel out in that equation. But now for the steps that create a new path 
from the current path: We choose to delete the subpath xs ...xt (not including the vertices xs and xt) 
where -1 : s<t : k +1 is chosen with probability pdelete(s, t). Note that there is the possibility that 
the current path is disposed completely.  We extend the remaining path segments by starting random particle 
walks from  ' ' their end points. The light path x0 ...xs is extended by s vertices, l1 ...ls , at its 
beginning, the eye path xt ...xk is extended by t ' vertices, e1 ...et ' , at its end. The two paths 
are then joined by connecting the endpoints, resulting in the mu­ ' tated path y = x0 ...xsl1 ...ls et 
' ...e1xt ...xk =: x0 ...xsz1 ...zs ' +t ' xt ...xk. For the light path we sample the BDSF at xs and 
trace a ray to determine the next ' vertex xs+1. This process is continued until we have s new vertices. 
In case the light path is empty initially, we need to sample a position on the light sources and start 
the random walk by sampling the EDF there. Analogously we sample t ' new vertices starting from vertex 
xt for the eye path. If the eye path is empty initially, we need to sample a new pixel position and a 
point on the lens to obtain the .rst vertex and direction.  We now can describe f for the parts of the 
path that di.er between x and y. For x this is  t-1 . f ' (x)= (fs(xi-1 ~ xi ~ xi+1)G(xi xi+1)) fs(xt-1 
~ xt ~ xt+1), i=s for y we have f ' (y)=fs(xs-1 ~ xs ~ z1)G(xs z1)fs(xs ~ z1 ~ z2)G(z1 z2) s ' +t ' 
+1 . (f(zi-1 ~ zi ~ zi+1)G(zi zi+1)) i=2 f(zs ' +t '-1 ~ zs ' +t ' ~ xt)G(zs ' +t ' xt)f(zs ' +t ' 
~ xt ~ xt+1). Note that for simplicity we have ignored the special cases at the path ends where the emission 
or the camera response come into play. In order to obtain the probability density, we need to look at 
the probability den­sity involved in implementing the change. All probability densities are expressed 
with respect to the area measure. This is best illustrated by an example: assume we would have a mutation 
strategy that either shortens the path with probability q by the last vertex or extends it there with 
probability 1-q. Now assume we have added one vertex xn+1 to a path x1 ...xn by sampling the BSDF at 
vertex xn: If we use a density on the projected hemisphere ps to do that and xn+1 is the resulting next 
surface intersection point, we have T (x1 ...xnxn+1|x1 ...xn) = (1 - q) · ps(xn-1 ~ xn ~ xn+1) · G(xn 
 xn+1). For the reverse direction we have T (x1 ...xn|x1 ...xnxn+1)= q. Similarly, we can now compute 
T (y|x) for the subpath we have inserted, and T (x|y) for the subpath that was deleted. Note that it 
is important to include all possible ways a given subpath with l vertices was created in T , i.e. all 
combi­ '' nations of s and t ' with s + t ' = l need to be taken into account. More details will be included 
in an updated version of the course notes. Ergodicity Bidirectional mutations ful.ll the ergodicity requirement 
for the Markov chain, i.e. T (y|x) > 0 whenever f(x) > 0 and f(y) > 0. In other words we can move from 
any contributing state to any other contributing state. A consequence is that we (theoretically) always 
have to compute Tbidirectional(y|x) (and Tbidirectional(x|y)) and include that in T (y|x) (and T (x|y)) 
for any other mutation strategy, since it could have been constructed by a bidirectional mutation as 
well. 2.5.2 Pertubations Pertubations are modifying the positions of (some of the) vertices of the current 
path, usually by slightly modifying the direction from one vertex to next, while leaving the path topology 
intact. They are intended to yield high acceptance probability by only do small chances, targeted at 
very speci.c lighting e.ects. A positive side e.ect of per­tubations is that the implementation is often 
simpler and more e.cient as compared to the more general case of bidirectional mutations. More details 
on pertubations will be included in an updated version of the course notes 2.6 Two-Stage MLT A direct 
consequence of importance sampling of the target function, which in expec­tation is the intensity arriving 
on the image plane, is that brighter areas receive more samples that darker areas do. More precisely, 
the ratio of the number of samples ar­riving in a pixel to a given total number of samples is proportional 
to the brightness of that pixel. This is not really desirable, since it leads to more perceivable noise 
in dark image regions whereas bright details might receive more samples than a (most likely tonemapped) 
.nal image would need. The problem is most prominent if very bright light sources are directly visible, 
as an extreme example think about looking at the daylight sky with the sun in the .eld of view. Even 
though the sun only subtends a very small solid angle, it directly contributes most of the intensity 
of daylight. Thus, the majority of samples produced by MLT will be concentrated in the (probably very 
few) pixels showing the sun.  The most exteme cases can be avoided by excluding directly visible light 
sources, or even direct lighting [20], from MLT since they usually can be computed ro­bustly by other 
means. The obvious drawback is that we then have an inhomo­geneous solution, where we need to decide 
how much computation to spent on each subproblem.  Ideally we would want to have approximately the same 
(expected) number of sam­ples per pixel and still importance sample the target function within that pixel, 
 i.e. the intensity arriving within that pixel. Assume that we would know the .nal intensity of a pixel 
vj, then we could change the target function t(x) using that intensity to t(x) t ' (x)= . . wj(x)vj pixels 
j:wj (x)>0 This would lead to equal probabilities for all pixels to be sampled. We would obtain the .nal 
image then by multiplying the result (an equal valued image) with the corresponding pixel values vj. 
Of course we do not know the exact pixel intensity, since that is what we want to compute in the .rst 
place. However, we can try to approximate it and change the target function accordingly. Generally, we 
can apply any masking function m(x) on the image plane and change the target function to t ' (x) := t(x)/m(x). 
In order to obtain the desired output image, we just need to multiply the result with m(x). Like this 
we can mask down directly visible light sources by setting m(x) high there,  use a low resolution image 
as an approximate intensity mask, or more gen­erally,  use any user-given mask.  As always, it is important 
to be quite conservative with the approximate guesses used to drive the sampling. If m(x) is set to high 
for a speci.c area (like in the vicinity of light sources for a low resolution mask), those areas could 
get under­sampled a lot. 2.7 Parallelization of MLT Standard Monte Carlo based light transport algorithms 
are typically straightforward to parallelize, at least if we ignore that we should ensure that the seeds 
for the pseudo­random number generator are su.ciently independent. Usually the only sampling spe­ci.c 
information we need in order to compute a task is a state for the random number generator or, in the 
case of quasi-Monte Carlo, just a single index for the sequence we use. In contrast to this the Markov 
chain nature of MLT, i.e. fact that the next state depends on the current state, makes it somewhat harder 
to parallelize things. The obvious approach is to simulate many chains in parallel. Each thread can then 
simulate its own chain mostly independently of what other threads do, using a pseudo­random number generator 
state which is uniquely associated with that chain. In order to get reproducible results, a pool of chains 
can be used and iterations for all chains in that pool can be scheduled such that each chain gets the 
same amount of iterations. Simulating independent chains is generally a good thing, since it reduces 
the amount of correlation. However, there are some things we have to keep in mind for an imple­mentation. 
Shared Framebu.er Access Di.erent chains, and therefore di.erent threads, need to access the framebu.er 
at random positions. The requirement here is the same as for light tracing (or bidirectional path tracing 
in general): we need to ensure that there are no con.icts. Having a dedicated framebu.er per thread is 
typically not feasible if the number of threads is high, so it is preferable to use atomic instructions 
(if available) or streams of full samples that get transferred to the framebu.er in batches. Start-up 
Bias Issues The more chains we have, the higher the impact of start-up bias is. In addition to that, 
compensating start-up bias by re-sampling becomes more involved: in order to re-sample a larger number 
of chains, the initial set of chains to re-sample them from should to be larger accordingly. If this 
is not the case, there is a higher amount of correlation in the beginning, since the probability of having 
duplicate initial states is higher. State Storage Size In contrast to standard (quasi-)Monte Carlo we 
have the require­ment to store the current state. While a (quasi-)Monte Carlo estimator only needs the 
next sequence index or pseudo-random number generator state, MLT needs all the information to construct 
a mutated path based on the current path. 2.8 Summary Metropolis light transport works particularly well 
in complicated scenes where other unbiased methods have trouble. There are basically two advantages it 
draws its strength from: First there is the powerful framework of Metropolis sampling that guarantees 
the right distribution and second we have the possibility to construct almost arbitrary mutations that 
can locally explore important illumination e.ects that are hard to create by other sampling methods. 
 The e.ciency of MLT of course massively depends on the mutation strategy that is actually implemented 
and whether that strategy works well for a given scene or not.  An obvious question is: when does MLT 
fail? In general you can pretty much outsmart any light transport algorithm by constructing exactly the 
kind of scene where it falls apart. For MLT this is harder to predict though, since we need to construct 
a scene where all mutation strategies yield low acceptance. As such, MLT is generally very robust. Of 
course there are also many cases where Metropolis light transport does not necessarily outperform other 
algorithms, e.g. if the target function is su.ciently simple and traditional means of importance sampling 
work well enough already. For some rendering work.ows this might already be the case for about every 
possible scene, especially if low noise for smooth areas is important right from the start and where 
the missing stratis.cation hurts a lot.  MLT poses somewhat higher constraints on a rendering system 
than other algo­rithms have.  Acknowledgments Many thanks go to my colleagues at nvidia ARC, in particular 
Daniel Seibert and Di­etger van Antwerpen, for many fruitful discussions and challenges not only concerning 
Metropolis light transport. Bibliography [1] Ashikhmin, M., Premoze, S., Shirley, P., and Smits, B. A 
variance analysis of the metropolis light transport algorithm. Computers &#38; Graphics 25, 2 (2001), 
287 294. [2] Clarberg, P., and Akenine-Möller, T. Exploiting Visibility Correlation in Di­rect Illumination. 
Comp. Graph. Forum (Proc. of EGSR 2008) 27, 4 (2008), 1125 1136. [3] Clarberg, P., and Akenine-Möller, 
T. Practical Product Importance Sampling for Direct Illumination. Comp. Graph. Forum (Proc. of Eurographics 
2008) 27, 2 (2008), 681 690. [4] Clarberg, P., Jarosz, W., Akenine-Möller, T., and Jensen, H. W. Wavelet 
Im­portance Sampling: E.ciently Evaluating Products of Complex Functions. ACM Trans. Graph. 24, 3 (2005), 
1166 1175. [5] Cline, D., Egbert, P. K., Talbot, J. F., and Cardon, D. L. Two stage importance sampling 
for direct lighting. In Rendering Techniques 2006: 17th Eurographics Workshop on Rendering (June 2006), 
pp. 103 114. [6] Coleman, W. Mathematical Veri.cation of a certain Monte Carlo Sampling Tech­nique and 
Applications of the Technique to Radiation Transport Problems. Nu­clear Science and Engineering 32 (1968), 
76 81. [7] Hammersley, J. M., and Handscomb, D. C. Monte Carlo Methods. Wiley, New York, N.Y., 1964. 
[8] Hastings, W. Monte carlo sampling methods using markov chains and their ap­plications. Biometrika 
57 (1970), 97 109. [9] Hesterberg, T. Weighted average importance sampling and defensive mixture distributions. 
Technometrics 37, 2 (May 1995), 185 194. [10] Kalos, M. H., and Whitlock, P. A. Monte Carlo Methods. 
John Wiley and Sons, New York, N.Y., 1986. [11] Kelemen, C., Szirmay-Kalos, L., Antal, G., and Csonka, 
F. A Simple and Ro­bust Mutation Strategy for the Metropolis Light Transport Algorithm. Computer Graphics 
Forum 21, 3 (Sept. 2002), 531 540. [12] Kollig, T., and Keller, A. Monte Carlo and Quasi-Monte Carlo 
Methods. Springer-Verlag, 2000, ch. E.cient Bidirectional Path Tracing by Randomized Quasi-Monte Carlo 
Integration, pp. 290 305. [13] Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., and Teller, 
E. Equations of State Calculations by Fast Computing Machine. Journal of Chemical Physics 21 (1953), 
1087 1091. [14] Mitchell, D. P. Consequences of strati.ed sampling in graphics. In Proc. of ACM SIGGRAPH 
1996 (1996), Addison-Wesley, pp. 277 280. [15] Raab, M., Seibert, D., and Keller, A. Unbiased global 
illumination with partic­ipating media. In Monte Carlo and Quasi-Monte Carlo Methods (2006), A. Keller, 
S. Heinrich, and H. Niederreiter, Eds., Springer. [16] Spanier, J., and Gelbard, E. M. Monte Carlo Principles 
and Neutron Transport Problems. Addison-Wesley, New York, N.Y., 1969. [17] Talbot, J., Cline, D., and 
Egbert, P. Importance resampling for global illumina­tion. In Rendering Techniques 2005: 16th Eurographics 
Workshop on Rendering (June 2005), pp. 139 146. [18] Veach, E. Robust Monte Carlo Methods for Light Transport 
Simulation. PhD thesis, Stanford University, 1997. [19] Veach, E., and Guibas, L. Optimally combining 
sampling techniques for Monte Carlo rendering. In Proc. of ACM SIGGRAPH 1995 (1995), Addison-Wesley, 
pp. 419 428. [20] Veach, E., and Guibas, L. Metropolis Light Transport. Computer Graphics 31 (1997), 
65 76. [21] Woodcock, E., Murphy, T., Hemmings, P., and Longworth, T. Techniques used in the gem code 
for monte carlo neutronics calculations in reactors and other systems of complex geometry. Proc. Conf. 
Applications of Computing Methods to Re­actor Problems (1965).  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
</content>
</proceeding>
