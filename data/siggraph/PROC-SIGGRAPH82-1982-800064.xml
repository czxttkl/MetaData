<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07-26-1982</start_date>
		<end_date>07-30-1982</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Boston]]></city>
		<state>Massachusetts</state>
		<country>USA</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>800064</proc_id>
	<acronym>SIGGRAPH '82</acronym>
	<proc_desc>Proceedings of the 9th annual conference</proc_desc>
	<conference_number></conference_number>
	<proc_class>conference</proc_class>
	<proc_title>Computer graphics and interactive techniques</proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn>0-89791-076-1</isbn>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>1982</copyright_year>
	<publication_date>07-26-1982</publication_date>
	<pages>333</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source>ACM Order Number: 428820</other_source>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node>I.3</cat_node>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<ccs2012>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>100</concept_significance>
		</concept>
		<concept>
			<concept_id>0.10010147.10010371</concept_id>
			<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
			<concept_significance>500</concept_significance>
		</concept>
	</ccs2012>
	<general_terms>
		<gt>Theory</gt>
	</general_terms>
	<chair_editor>
		<ch_ed>
			<person_id>PP14173105</person_id>
			<author_profile_id><![CDATA[81100495153]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[R.]]></first_name>
			<middle_name><![CDATA[Daniel]]></middle_name>
			<last_name><![CDATA[Bergeron]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Chairman]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
	<ccc>
		<copyright_holder>
			<copyright_holder_name>ACM</copyright_holder_name>
			<copyright_holder_year>1982</copyright_holder_year>
		</copyright_holder>
	</ccc>
</proceeding_rec>
<content>
	<article_rec>
		<article_id>801252</article_id>
		<sort_key>1</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Clamping]]></title>
		<subtitle><![CDATA[A method of antialiasing textured surfaces by bandwidth limiting in object space]]></subtitle>
		<page_from>1</page_from>
		<page_to>8</page_to>
		<doi_number>10.1145/800064.801252</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801252</url>
		<abstract>
			<par><![CDATA[<p>An object space method is given for interpolating between sampled and locally averaged signals, resulting in an antialiasing filter which provides a continuous transition from a sampled signal to its selectively dampened local averages. This method is applied to the three standard Euclidean dimensions and time, resulting in spatial and frame to frame coherence. The theory allows filtering of a variety of functions, including continuous and discrete representations of planar texture.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Aliasing]]></kw>
			<kw><![CDATA[Clamping]]></kw>
			<kw><![CDATA[Filtering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Measurement</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31025503</person_id>
				<author_profile_id><![CDATA[81100065198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Thomas J. Watson Research Center, Yorktown Heights, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P15779</person_id>
				<author_profile_id><![CDATA[81100334998]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alyn]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Rockwood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evans and Sutherland Computer Corporation, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332947</person_id>
				<author_profile_id><![CDATA[81100100613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Skolmoski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Evans and Sutherland Computer Corporation, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shannon, C., Communication in the presence of noise, Proc. Inst. Radio Eng., Vol 37, No. 1, Jan 1949 (10-21).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Crow, F. C., A Comparison of Antialiasing Techniques, IEEE Computer Graphics and Appl., Vol. 1, No. 1, Jan 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Clamping: A Method of Antialiasing Textured Surfaces by Bandwidth Limiting in Object Space Alan Norton 
IBM Thomas J. Watson Research Center Yorktown Heights, NY 10598 Alyn P. Rockwood Philip T. Skolmoski 
Evans and Sutherland Computer Corporation Salt Lake City, Utah 84108 ABSTRACT An object space method 
is given for interpolating between sampled and locally averaged signals, resulting in an antialiasing 
filter which provides a continuous transition from a sampled signal to its selectively damp- ened local 
averages. This method is applied to the three standard Euclidean dimensions and time, result-ing in spatial 
and frame to frame coherence. The theo- ry allows filtering of a variety of functions, including continuous 
and discrete representations of planar tex-ture. CR Categories and Subject Descriptors: 1.3.7 [Computer 
Graphics]: Three-dimensional Graphics and Realism -color, shadowing, shading, and texture; 1.3.3 [Computer 
Graphics]: Picture/Image Generation digitizing and scanning. General Terms: Theory, measurement. Additional 
key words and phrases: aliasing, filtering, clamping. Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. (~) 1982 ACM0-89791-076-1/82/007/0001 $00.75 Introduction 
The Shannon sampling theorem [1] states that a signal may be accurately reconstructed from samples of 
it, if those samples are taken more often than at twice the highest frequency present in the signal. 
Therefore a signal with very high frequencies may be accurately reproduced by sampling it at a correspondingly 
high rate. However, it is often the case that the reproduced signal is not required to have complete 
accuracy. Of-ten in computer graphics the resolution at which an image is displayed is much lower than 
the resolution at which it can be calculated. When the input signal is sampled at the lower output frequency, 
the result--jagged edges, interference patterns, spurious light and dark spots--is called aliasing. Aliasing 
can he reduced or avoided by increasing the sampling rate to exceed that of the input signal, then doing 
a convolution, or averaging the samples obtained. While this process often works, it can nevertheless 
be very wasteful of computing time, because a very large number of sam-ples may be required just to eliminate 
high frequency components of the signal. The alternate approach which we employ here is to filter the 
input signal according to the rate at which it is to be reproduced. Rather than truncating the higher 
frequency terms, we selectively damp them according to a simple power series approximation of a box filter. 
The result is an anti-aliasing technique which requires only one texture determination per pixel, and 
is suita- ble for real-time implementation. Textured Planes We may define a visual texture on a plane 
by means of an intensity function I(x,y), which assigns illumination values according to point coordinates 
(x,y) on the plane. Ordinarily such an intensity function may be specified as a Fourier series (an FFT). 
When such a textured surface is displayed on a raster-scan device, it is necessary to convert this function 
to an array of pixel intensities. To fix notation, let P(u,v) be the perspective transformation which 
converts screen coordinates (u,v) to coordinates (x,y) on the textured surface. (Some pairs (u,v) may 
not correspond to any ground coordinates; that is, P is not generally defined for all values of u and 
v.) The simplest way to display such a texture is to directly sample the intensity func- tion, taking 
I(P(u,v)) to be proportional to the the intensity of the pixel at screen coordinates (u,v). Aliasing 
results when the distance between adja-cent samples approaches the smallest wave length of the intensity 
function. Aliasing may be avoided by replacing the sampling process with an averaging proc- ess: We may 
take the intensity at screen coordinates (u0,v0) to be a convolution: (1) f f I(P(u-uo,v-vol)h(u,v) du 
dv This results in a screen intensity which is a weighted average of the values of I(P(u,v)) near (uO,vO). 
The "filter" h(u,v) determines the relative weighting of these values. Such an integral can be approximated 
by discrete sampling methods; this is the usual approach. See [2]. However any sampling technique requires 
the sampled function to be band-width limited: Otherwise high frequency terms in I(P(u,v)) may continue 
to cause aliasing difficulties. In order to meet the Shannon sampling criterion, the number of samples 
per pixel (i.e., the number of subpixel samples) must approach infinity as the perspective image P(u,v) 
of the pixel at (u,v) approaches the horizon. Filtering in the frequency domain Another way of evaluating 
the above convolution integral is to perform a Fourier transform, transferring the problem to the frequency 
domain. The convolution transforms to the product of Fourier transforms: A A (2) IoP(s,t). h(s,t) (He~'e 
we denote the Fourier transform of a function f by f. loP, the composition of I and P, defines the tex- 
ture in screen coordinates.) Expressed in this form, we see that the transform of h selectively damps 
the signal IoP according to its constituent frequencies. For high frequencies, (large values of s and 
t), h(s,t) will be small, cancelling the effect of high frequency terms and thereby limiting the bandwidth. 
However, since the transform of IoP must be calculated each time there is a change of perspective, this 
can be a costly calculation. Use of an FFT can introduce more aliasing problems associated with sampling 
the function IoP. Derivations and Approximations We develop a means of approximating the convolu- tion 
above in the frequency domain. The bandwidth- limiting is performed separately for each pixel. This relies 
on three separate simplifications: 1. A simple "box filter" is used, which enables it to be easily transformed 
(in perspective) to object space. 2. The perspective transformation is locally ap- proximated by a linear 
transformation, so that the "box filter" translates to a "parallelogram filter", via the perspective 
change. 3. The actual value of the convolution integral is approximated by the low-order terms in a 
power-series. Band-width limiting is achieved by ignoring those terms for which the associated approximation 
becomes unreli- able.  The derivation of the approximation formulas pro- ceeds most easily with a simple 
complex-valued signal. Let I(x,y) = ei(kx+ly). We consider the problem of determining the average of 
I(x,y) over a parallelogram centered at (x0,Y0), as pic- tured below: Y2 ) ~ V ~ (Xo+Xl ' Yo+Yl ) All 
points in such a parallelogram can be parameterized by (x0,Y0) + S(Xl,Yl) + t(x2,Y2), with -1 < s,t < 
+1 Where the sides of the parallelogram have length and direction given by the vectors 2°(xl,yl) and 
2-(x2,y2). The average value of I(x,y) is then the double integral r,+l r+l (3) 1/4 / / e(ik(x°+sxl +tx2) 
+i/(y°+sYl +ty2))dsdt ~-1 "-1 This is easily integrated to yield e(ikxo+ilYo).g(kXl+lYl)'g(kx2+lY2) 
 where g(x) is the function sin(x)/x (g(0)=l). (Basically, this is just the result that a box filter 
trans- forms to a sin(x)/x filter in the frequency domain.) This expression shows the relationship between 
the sampled value e(ikx0+ilY0) __. I(x0,Y0) and the local average. If we approximate sin(x)/x by the 
first two terms in its power series at 0, we obtain 1 -x2/6. Therefore, if kXl+/y 1 and kx2+/y 2 are 
small, the above expression can be approximated by e(ikx0+i/Y0) ( 1 -(kXl+lYl)2/6 -(kx2+lY2)2/6)) If 
kXl+lY 1 or kx2+lY 2 are large, then this approxima-tion is not valid; however in that case the parallelogram 
of integration extends across several wavelengths of the function I(x,y), and it is appropriate to bandwidth-limit 
the function I(x,y), replacing it by zero. The above formula provides a convenient means of interpolating 
between the sampled value I(xo,Yo) and the local aver-age, by eliminating the power-series approximation 
when it becomes negative. We therefore define i e(ikxo+ilYo)(l_ro((kXl +lYl)2-(kx2+lY2)2)) (4) C = 
... if r'((kXl+lYl)Z+(kXz+ly2) 2) < 1 0 otherwise (with the constant r = 1/6) as the desired approxim 
a- tion to the average value of I(x0,Y0) over the parallelo- gram. The constant r = 1/6 should be modified 
for best results in each given situation. Decreasing r has the effect of increasing the area over which 
the filter acts, increasing the size of the "box" used in the "box filter". Note that this formula amounts 
to multiplying the given function e(ikx0+ilY0) by the function C(Xl,Yl,X2,Y2,k,l ) = max(0,1 -r((kXl+lyl)2+(kx2+lY2)2)). 
 We shall refer to such a function C as a "clamping function." Multiplying an oscillatory term by a clamp-ing 
function has the effect of forcing (or clamping) the oscillatory term to its local average value. We 
may extend this formula to a complex Fourier series in two dimensions by applying it to each term separately. 
The non-zero terms in the approximated series all lie within an ellipse in the frequency domain. By considering 
real and imaginary parts separately, we see that the above clamping function can also be used for functions 
of the form sin(kx+/y) or cos(kx+/y). A trigonometric series in two dimensions involves terms of the 
form sin(kx)sin(/y) sin(kx)cos(ly) .... etc. which can be converted to sums of functions of the forms 
sin(kx-ly), cos(kx+ly), etc., by using well-known trigonometric identities. Filtering of textures in 
perspective Consider now the problem of evaluating the convo-lution (2) when a rectangular filter (in 
screen coordi-nates) is used. A rectangle on the screen will map to a quadrilateral via the perspective 
transformation P. If the perspective transformation were linear (which it is not), the rectangle would 
map to a parallelogram. But we may regard a parallelogram as a first-order approxi- mation to the actuaI 
shape. Specifically, consider the rectangle R in screen coordinates centered at (x0,Y0) of width 28x 
and height 26y. Let PO = P(xO,YO), P1 +=P(xo+Sx,YO), PI_=P(xo-Sx,YO), P2+ = P(xO,YO+6Y), and P2- = P(xO,YO-SY)- 
 We approximate P(R) by the parallelogram on the tex-tured surface parametrically defined by PO+ (el+-el-)s+ 
(P2+-P2-)t -1 <s,t < +l Let V 1 by the vector (in object space) from P1-to PI+ and let V 2 be the vector 
from P2- to P2+-A typical term in a complex Fourier series may be written as e(iK.X) = e(ikx+ily) where 
K = (k,/) and X = (x,y) are 2-vectors. If this term is averaged over the parallelogram we obtain the 
approximation from (4) e(iK.x) C(K,VI,V2) where C(K,VI,V2) = (5) max( 0 , 1 -r'((K.V 1) 2 - (K.V2)2)) 
 is the clamping function in coordinate-free form. V 1 and V 2 represent the changes in object space 
coordi-nates respectively associated with horizontal and verti-cal screen displacement of one pixel. 
Note that in the above formulation there is no ref-erence to the plane which we assumed to be textured 
by the intensity function. The above formula may be used as well for any surface on which the texture 
is defined, even surfaces which are not flat. Thus V 1 and V 2 are regarded as 3-vectors and the frequency 
vector K has three components corresponding to terms of the form e(ikx+ily +imz) (here K = (k,l,m) ) 
 The vectors V I and V 2 are the displacements in 3-space associated with one horizontal (resp., vertical) 
pixel displacement on the viewing screen at the pixel in question. Time aliasing During animation of 
computer-generated textured images, time-aliasing can result from changing the observer's position or 
direction of view. A texture which evolves in time will also produce aliasing. The above clamping function 
can easily be generalized to deal with this problem as well, by regarding time as another coordinate. 
A typical frequency term which changes in time is e(ikx+ily+imt) = e(iK°X) and the corresponding clamping 
function is given by max( 0, (1 -r.((K-V1)2 -(K°V2) 2 - (K-V3)2)) Where V 1 and V 2 are the same as 
above, with third coordinate zero, and V 3 represents the change in P(xo,Y0) between successive frames. 
If the texture does not evolve in time (i.e., m = 0) then the third coordinate of V 3 is irrelevant. 
Otherwise this coordi- nate should be the time between frames, using the units in which t is measured. 
Shortcuts It is also possible to use the clamping function technique for anti-aliasing textures whose 
Fourier se-ries is not known. Consider a texture function of the form I(x,y) = A + F(x,y) where A is 
a constant and F(x,y) is a function whose average value is zero. It may be the case that F has a narrow 
frequency spectrum; i.e., the Fourier transform of F may be near zero except in a small region of the 
frequency domain. For example, suppose that the Fourier transform of F is supported in a neighborhood 
of the point (k,l) = K. Then the clamping function C(x,y) defined above (with this value of K) may be 
used with the function F. The antialiased form of I(x,y) is then A + C(x,y,K) F(x,y). Often it is the 
case that an intensity function I(x,y) has no predominate directions associated with it. We have obtained 
satisfactory results by ignoring the direc- tionality of formula (4), obtaining a clamping function of 
the form (6) C = max (O,l-r(Vl'Vl+V2"V2)) where the constant r must be chosen in accordance with the 
predominate spatial frequencies of the texture. Applications and Illustrations As an example of the above, 
a sine wave is t~sed as the texture function in figures 1 and 2. Specifically, the texture function is 
of the form A + C sin(x) and the texture is viewed in perspective looking down the y-axis. The two illustrations 
show the difference be-tween no clamping and the use of the clamping func-tion in (5) with r = 0.1 . 
Figures 3 and 4 show a checkerboard pattern in perspective. This texture is defined by using the first 
few terms in the Fourier series for such a pattern. Again the difference between the two pictures is 
the presence of the clamping function with parameter r = 0.1. Using the above formulation, a combination 
of texture functions can be defined on the same polygon. Separate frequency vectors K are defined for 
each tex-ture function and the clamping function is then com-puted and applied to each texture function 
independ-ently. Figure 5 shows the use of a series of clipped sine waves used to define the texture of 
a brick wall. Thirteen of these sine waves were combined to define the bricks and mortar in this picture. 
Pseudo random texture functions which vary as a function of x and y on the textured plane have been used 
to texture polygons. The clamping function con-cept (with some modifications) has been applied to these 
textures for effective antialiasing. Several varia- tions of the clamping function (5) have been used. 
Figures 6a and 6b demonstrate two such methods ap-plied to a digitized texture pattern. In 6a, the clamping 
function has a sharp cutoff. In 6b, a quintic (polynomial of degree 5) which closely approximates a Gaussian 
curve is used as clamping function. The primary impetus for this development was the introduction of 
textured surfaces into the Evans and Sutherland Novoview image generators. These systems are used mainly 
in commercial airline pilot training. In this training, texturing supplies important height and motion 
cues as well as realism. The benefits of textur- ing can be rendered ineffective by aliasing effects. 
Clamping not only solves the aliasing problem but does it economically in a real-time environment. Figures 
7 through 10 illustrate the use of antialiased digitized water and sky textures in flight simulation. 
Conclusion The work presented here provides a viable alterna- tive to discrete sampling methods. Band-width 
limiting after image computation requires many discrete sam-ples to produce an anti-aliased image. The 
clamping function technique presented here requires some knowledge about the function to be bandwidth-limited; 
Figure i A sine wave in perspective without clamping. Figure 2 A sine wave in perspective with clamping. 
 Figure 3 A checkerboard pattern without clamping. Figure 4 A checkerboard pattern with clamping.  
 for instance that it be a Fourier series, or that it be a sum of terms whose predominant frequencies 
are known. In order to apply a clamping function to a texture function, the average intensity value of 
the function must be known. These constraints are howev- er not difficult to obtain. When compared with 
dis-crete sampling techniques requiring many sub-pixel computations, this method is a simplification 
and prod- uces more natural-looking results. Acknowledgements We express thanks to Evans and Sutherland 
Com-puter Corp. for the support of this development, in particular to the Novoview R&#38;D Group. The 
color pictures are from a Rediffusion-Evans and Sutherland advanced Novoview development system. The 
black- and-white halftones were produced at IBM using graphics software written by Peter Capek.  References 
 1. Shannon, C., Communication in the presence of noise, Proc. Inst. Radio Eng., Vol 37, No. 1, Jan 1949 
(10-21). 2. Crow, F. C., A Comparison of Antialiasing Techni- ques, IEEE Computer Graphics and Appl., 
Vol. 1, No. 1, Jan 1981.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801253</article_id>
		<sort_key>9</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A more flexible image generation environment]]></title>
		<page_from>9</page_from>
		<page_to>18</page_to>
		<doi_number>10.1145/800064.801253</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801253</url>
		<abstract>
			<par><![CDATA[<p>A supervisory process is used to distribute picture-generation tasks to heterogeneous subprocesses. Significant advantages accrue by tailoring the subprocesses to their tasks. In particular, scan conversion algorithms tailored to different surface types may be used in the same image, a changing mixture of processors is possible, and, by multiprogramming, a single processor may be used more effectively. A two-level shape data structure supports this execution environment, allowing top-level priority decisions which avoid comparisons between surface elements from non-interfering objects during image construction.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Composite structures]]></kw>
			<kw><![CDATA[Display algorithms]]></kw>
			<kw><![CDATA[Graphs]]></kw>
			<kw><![CDATA[Hidden line/surface elimination]]></kw>
			<kw><![CDATA[Multiprocessing/multiprogramming]]></kw>
			<kw><![CDATA[Performance measures]]></kw>
			<kw><![CDATA[Picture description languages]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39070752</person_id>
				<author_profile_id><![CDATA[81100447047]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[F.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Crow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Research Group, Ohio State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807403</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Atherton, Peter R., Weiler, Kevin J., and Greenberg, Donald P., "Polygon Shadow Generation," Computer Graphics Vol. 12(3) pp. 275-281 Proc. Siggraph 78, (August 1978).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Babaoglu, Ozalp, Joy, William, and Porcer, Juan, "Design and Implementation of the Berkeley Virtual Memory Extensions to the UNIX Operating System," Technical Report, Dept of EECS, University of California at Berkeley (1979).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360353</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F. and Newell, Martin E., "Texture and Reflection in Computer Generated Images," Communications of the ACM Vol. 19(10) pp. 542-547 (October 1976).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F., "Computer Display of Curved Surfaces," Tech. Rpt. 1060-126 Jet Propulsion Lab, Pasadena, University of Utah (December 1978). PhD Thesis]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F., Carpenter, Loren C., Lane, Jeffrey M., and Whitted, Turner, "Scan Line Methods for Displaying Parametrically Defined Surfaces," Communications of the ACM Vol. 23(1) pp. 23-34 (January 1980).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>362739</ref_obj_id>
				<ref_obj_pid>362736</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Bouknight, W. Jack, "A Procedure for Generation of Three-D Half-Toned Computer Graphics Reps.," Communications of the ACM Vol. 13(9) pp. 527-536 (September 1970).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Christiansen, Henry N and Stephenson, Michael B., "MOVIE.BYU - A Computer Graphics Software System," Journal of the Technical Councils of ASCE, pp. 3-12 (April 1979).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Clark, James H., "Hierarchical Geometric Models for Visible Surface Algorithms," Communications of the ACM Vol. 19(10) pp. 547-554 (October 1976).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C., "Shaded Computer Graphics in the Entertainment Industry," Computer Vol. 11(3) pp. 11-22 (March 1978).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807441</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Fuchs, Henry, Kedem, Zvi M., and Naylor, Bruce F., "Predetermining Visibility Priority in 3-D Scenes," Computer Graphics Vol. 13(2) pp. 175-181 SIGGRAPH-ACM, (August 1979).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Goldstein, R.A. and Nagel, R., 3-D Visual Simulation, Simulation (1971).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807438</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Kay, Douglas S., "Transparency for Computer Synthesized Images," Computer Graphics Vol. 13(2) pp. 158-164 Proc. Siggraph '79, (August 1979). Cornell University]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Knowlton, Kenneth C. and Cherry, Lorinda, "ATOMS-A Three-D Opaque Molecule System for Color Pictures of Space-Filling or Ball-and-Stick Models," Computers &amp; Chemistry Vol. 1 pp. 161-166 (1977).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807439</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Max, Nelson L., "ATOMLLL: ATOMS with Shading and Highlights," Computer Graphics Vol. 13(2) pp. 165-173 Proc. Siggraph '79, (August 1979).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Myers, Allan, "An Efficient Algorithm for Computer Generated Pictures," Tech. Report, Ohio State University (1975). Computer Graphics Research Group]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Newell, Martin E., Newell, Richard G., and Sancha, Tom L., "A New Approach to the Shaded Picture Problem," Proc. ACM National Conference, pp. 443-450 (August 1972).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Newell, Martin E., "The Utilization of Procedural Models in Digital Image Synthesis," UTEC SCc-76-218, Salt Lake City (1975). Department of Computer Science]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>639789</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Porter, Thomas K., "Spherical Shading," Computer Graphics Vol. 12(3) pp. 282-285 Proc. Siggraph '78, (August 1978).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Rougelot, Rodney S., "The General Electric Computer Color TV Display," in Pertinent Concepts in Computer Graphics, ed. J. Nievergelt, University of Illinois Press, Urbana (1969).]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sutherland, Ivan E., Sproull, Robert F., and Schumaker, Robert A., "A Characterization of Ten Hidden-Surface Algorithms," Computing Surveys Vol. 6(1) pp. 1-55 (March 1977).]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>905548</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Watkins, Gary S., "A Real-Time Visible Surface Algorithm," UTEC-CSc-70-101, Dept. Computer Science, U. Utah, Salt Lake City (June 1970). Ph.D. thesis, U. of Utah]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563896</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Weiler, Kevin J. and Atherton, Peter A., "Hidden-Surface Removal Using Polygon Area Sorting," Computer Graphics Vol. 11(2) pp. 214-222 SIGGRAPH-ACM, (July 1977).]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807375</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Weinberg, Richard, "Computer Graphics in Support of Space Shuttle Simulation," Computer Graphics Vol. 12(3) pp. 82-86 SIGGRAPH-ACM, (August 1978).]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Whitted, J. Turner, "An Improved Illumination Model for Shaded Display," Communications of the ACM Vol. 23(6) pp. 343-349 (June 1980).]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806815</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Whitted, Turner and Weimer, David, "A Software Test-Bed for the Development of 3-D Raster Graphics Systems," Computer Graphics Vol. 15(3) SIGGRAPH-ACM, (August 1981).]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Williams, Lance, "Casting Curved Shadows on Curved Surfaces," Computer Graphics Vol. 12(3) pp. 270-274 SIGGRAPH-ACM, (August 1978).]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Williams, Lance, personal communication 1979.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A More Flexible Image Generation Environment F. C. Crow Computer Graphics Research Group Ohio State 
University ABSTRACT A supervisory process is used to distribute picture-generation tasks to heterogeneous 
sub- processes. Significant advantages accrue by tailoring the sub processes to their tasks. In particular, 
scan conversion algorithms tailored to different surface types may be used in the same image, a changing 
mixture of processors is possible, and, by multiprogramming, a single pro- cessor may be used more effectively. 
A two-level shape data structure supports this execution environment, allowing top-level priority deci- 
sions which avoid comparisons between surface elements from non-interfering objects during image construction. 
 Keywords: Display algorithms~ hidden line/surface elimination, picture description languages, multiprocessing/multiprogramming, 
graphs, composite structures, performance meas- ures Categories: D.2.8, D.4.1, E.], E.2, I .... , 1.3.4,1.3.7 
 INTRODUCTION Most systems for computer generated realistic imagery are either too restrictive (eg. 
forcing all surfaces to be defined as static polygonal meshes) or demand too much of the operator (eg. 
best [4:9]. This work was supported by the National Sci- ence Foundation under grant #MCS 79-20977. 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
(~) 1982 ACM 0-89791-076-1/82/007/0009 $00.75 An alternative approach based on small single- purpose 
display programs and intimate user involvement has been used at some sites [27]. This approach cuts the 
cost of integratin~ new display techniques to a minimum. However, in order to make a complex image a 
complex command sequence is necessary, forcing the user of the display programs to have most of the skills 
of an experienced programmer. Furthermore, the user must be unecessarily intimately involved in the task 
of image generation, a task which proceeds automatically in the monolithic systems. Whitted[25] has 
recently shown a system using a clever compressed raster description which allows him to merge scan converted 
descriptions of dif- ferent surface typeS. This goes a long way toward eliminating the above complaints 
since objects using different surface modeling tech- niques may be combined in an image. I am currently 
exploring a different approach which grows out of ideas expressed by Newell[17] and Clark[8]. A supervisory 
process serves the integrating function of the user in the second scenario by determining object pr~orlty. 
The supervisory process then oversees production of the image by directing a host of independent display 
tasks which build the image object by object in a frame buffer. This involves implementing a system 
which separates the process of image generation into two distinct phases: scene analysis and object rendering. 
During scene analysis interobject interactions are determined solely on the basis of crude estimations 
of the size and position of objects making up the scene. Information from the scene analysis is then 
used to determine bow to generate the image. Object priority is used to decide the order in which objects 
are to be rendered. Object overlap is used to find those objects which may be rendered in parallel. 
Object rendering may be done by a number of independent processes except in those cases where objects 
interfere by interpenetrating or inter- locking. The independent processes may be prior- itized by execution 
sequence on a single- processor system or by interprocess signals on a multiprocessor system. Interfering 
objects may be handled by recasting them into a single object, if their relationship is static, or by 
passing them to a single rendering process if they move with respect to each other from frame to frame. 
 SCENE ANALYSIS To even consider building the kind of software described here a flexible operating system 
which allows a user process to spawn and control daughter processes is imperative. While more and more 
systems of this sort are available, commer- cial operating systems frequently have not anti- cipated 
such use. At the Computer Graphics Research Group we are using the UNIX operating system with VAX extensions 
from Berkeley[2]. While the existing software provides a minimally acceptable environment, planned extensions 
prom- ise to make this system better suited to the task. These extensions will provide better inter- 
process communication and improved access to the process page table for rapid virtual memory access to 
very large files. The organization of the flexible image generation system requires frequent use of very 
large shape description and raster image files and spawns many processes with large execution-time memory 
requirements. The primary task of the supervisory process is to determine which objects overlap in the 
image and then determine the depth order within a group of overlapping objects. A variation on any of 
a number of extant priority algorithms would suf- fice for this purpose[10,]6,|9]. In fact, the first 
images made on this system used a priority algorithm consisting of nothing more than a sort based on 
the depth of the centroid of an object's bounding box. The growth law of the priority algorithm or the 
amount of available memory provide no important restrictions since there should never be more than a 
few hundred objects compared by the super- visory process. I feel no worries in restricting the number 
of objects in a scene for a number of reasons. Describing the motion of hundreds of independently moving 
objects in an animated sequence requires an enormous amount of informa- tion. Such scenes are unlikely 
to be produced except by automatic methods. Furthermore, there are only a very small number of sites 
where extremely complex images can be made in reason- able amounts of time (I will reconsider this issue 
in a few years when very complex images become considerably more practical). Any scene with more than 
a few hundred objects can probably be structured more efficiently. For example, while it may be reasonable 
to describe the bricks in a wall individually for a close view, any view with more than a few hundred 
should use a description in which the wall is the unit object. Obviously there are counter exam- ples 
such as swarms of insects or simulated explosions with flying debris. However, images of more than a 
few hundred objects are likely to be uselessly complex. The current object sort builds a directed graph 
encoding the precedence and dependencies of all objects in the image Nodes in the graph represent objects 
and arcs describe depenSencies. First, a preliminary depth sort orders objects by the centroids of their 
bounding spheres. ~nile not strictly necessary, this cuts down on the amount of computation needed to 
build the graph. The graph is then built by checking each object in turn against everything previously 
loaded in the graph (computation grows with the square of the number of objects). Interfering objects 
are indicated by specially tagged arcs. Because of the prelimary sort, most objects are added to the 
graph as leaf nodes (no outgoing arcs). Complications in the graph-building process occur (I) when an 
object is found to be improp- erly ordered by the preliminary sort and (2) when a large number of interfering 
objects must be grouped. In the first case, an incorrectly sorted object raises the possibility of cycli- 
cally overlapping objects [Figure 7]. Therefore, when a node is entered in the graph with outgoing arcs, 
the subgraph thus defined must be checked for cyclic dependencies. Where a cycle occurs, all objects 
involved must be considered insepar- ab i e In the second case7 a group of inseperable objects may occasionally 
surround an additional object so that it both hides and is hidden by objects included in the group. This 
situation can also be detected by cyclic dependencies in the graph. Therefore, whenever a new link is 
established to anything but a leaf node, the subgraph below the node is checked for cycles. Two tests 
are used to determine the relation- ship of a pair of objects, (I) an overlap test and (2) a priority 
and separability test. The overlap test consists of testing the perspective images of the bounding spheres 
from two objects for overlap. If overlap is found then a succeed- ing test tries to find a line separating 
the per- spective images of the bounding boxes of the objects. If both overlap tests fail then the bounding 
spheres are tested to see if the sum of their radii is greater than the Euclidean distance between them 
in object space. If so, the objects are separable and the closer centrold has higher priority. Otherwise, 
the bounding boxes are tested to see if a separating plane can be found. If such a plane is found, its 
orientation (direc- tion of its normal vector) determines priority. Otherwise the objects are considered 
inseparable. There are numerous opportunities to improve the current graph-building algorithm. However, 
any improvements appear to require significant addi- tional complication in the algorithm. Until our 
images get to the point where the computation devoted to graph-building becomes a significant part of 
the total cost of a picture, there seems to be little reason for pursuing optimizations. It is not likely 
this point will be reached before a great deal of specialized hardware is included in the image building 
part of the sys- tem. The supervisory process reads a description of a scene, giving positions and orientations 
of objects, light source specifications, and view parameters. Succeeding scenes may then be described 
incrementally, providing only the changes from the previous scene. This form of description, while bulkier 
than some form of key-frame description, provides a well-defined description of frame-to-frame changes 
while free- ing the supervisory program from the tasks of interpolation between key frames. Interpolation 
is another aspect ~f animation subject to experi- mentation and is thus better done elsewhere. The major 
motivation for this whole approach to image generation is to break the system out into manageably sized 
pieces communicating over rela- tively clean interfaces. Therefore, the larger environment into which 
all this fits may have many different programs which generate scene descriptions for different purposes. 
However, all such programs provide output in a common for- mat (Appendix A) which can also be typed in 
by a relatively naive user. Similarly, the super- visory process sends standardized information to display 
programs. This allows us to experiment with different display techniques and different animation and 
shape modeling techniques with minimal difficulty and without losing the ability to test any new techniques 
immediately on non- trivial scenes. Therefore, any computation which is not directly involved in interpreting 
the standard input, building the priority graph, or interpreting the graph for display belongs in another 
process. The incremental frame-to-frame description allows the use of whatever coherence exists between 
frames to be put to good use. For exam- ple, %f the view parameters and light sources are unchanged then 
only moving objects change the image. In such cases, only those parts of the picture overlapping the 
changing objects need be updated. The supervisory process may reset the viewspace clipping parameters 
to limit the compu- tation to dynamic areas of the image. The scene description allows some information 
on object interactions to be specified. For example, an object may be attached to another object to build 
a structure. An object may be placed "on" another object (atop the bounding box). Such relationships 
could be used to pre- calculate separating planes, making priority decisions easier. However, the computational 
savings don't yet appear to be worth the trouble. On the other hand, interacting objects which are not 
explicitly declared may represent uninten- tional intersections, which can be flagged. It has also been 
made possible to escape from the supervisory program to do other operations on data. For example, some 
objects are defined by procedures which vary the object's shape over time. To get the shape for a given 
frame, input to the supervisory program may specify execution of another process to update the object 
shape. The abilities of the supervisory process are lim- ited to manipulations of rigid objects. To 
facilitate development of complicated objects assembled from more primitive parts, input may be redirected 
to a nested description file to read in a predefined set of objects. For convenience, the objects in 
a nested file may be grouped and treated collectively when referred to by the group name. Arbitrary levels 
of nesting are allowed, limited only by a predefined stack depth (currently 16). For each frame generated, 
the supervisory pro- cess controls slave processes by traversing the priority graph. A recursive traversal 
algorithm starts up display pro~esses for non-overlapping objects in parallel and waits for completion 
sig- nals from processes for higher priority objects before initiating processes for lower-priority objects 
which overlap. The information passed to the display process includes (i) the display device to be used, 
(2) The object file name, (3) position, color and strength of all light sources which can affect the 
object and ambient light, (4) whether clip- ping is necessary (where the supervisory process can't trivially 
accept or reject), and (5) the transformation matrix which positions the object (see appendix C). The 
name of the program to be used to display the object is, of course, part of the object description. Therefore, 
wierd objects may have custom display programs. Example appli- cations conclude this paper. OBJECT RENDERING 
 Independent slave processes can do the actual work of image generation including all operations on individual 
surface elements (ie. clipping, shading, scan conversion). As long as individual objects do not interact, 
the slave processes can be specialized for efficiency in rendering one particular kind of surface. The 
most obvious example would be the sphere. There have been a few published algorithms for rendering 
images consisting solely of spheres [13,1~,I$]. These algorithms are significantly superior to polygon 
algorithms for the purpose. and have proven useful in several areas. However sphere-specialized algorithms 
have been of lim- ited general-purpose use since other types of surfaces cannot be included in the 
images they produce. In the current implementation, the display algorithms must retrieve the shape 
data anew for each frame. Therefore there is an advantage to display algorithms which make use of a very 
com- pact shape description. The specialized algo- rithms in use take a description of a solid of revolution, 
for example, and expand it to a polygonal representation appropriate to the size at which it is to be 
displayed. More generally, an object defined by bicubic patches may be simi- larly expanded, or ultimately, 
directly ren- dered[5]. In general, an image may be made more effi- ciently by algorithms tailored to 
surface types. The big advantage, however, lies in the greater simplicity of some tailored algorithms. 
This sim-  The image of figure 3 was made in with three different rendering qualities, (i) no anti- 
aliasing and dull surfaces (jaggy tiler), (2) anti-aliased with dull surfaces (dull tiler), and (3) 
anti-aliased with highlights (shiny tiler). Non-linear shading was used only on those polygons containing 
part of a highlight. The cost of figuring which polygons participate in highlights is reflected in the 
difference between calculating vertex shades for the highlighted objects (shiny vtx shade) and the dull-surfaced 
objects (dull vtx. shade). No clipping was necessary. However, the code which does the clipping was 
checked for the case where the supervisory process finds no clipping necessary (null clipping) and the 
case where it does (trivial accept). Times are in CPU-seeonds and are accurate only to about ten percent. 
Measurements were made on a VAX 11/780 with floating point accelerator running Berkeley Unix under a 
light time sharing load. Table ] Sample Execution Times banana egg george polygon count 256 3552 ]9666 
shiny tiler 4.0 26.0 l&#38;0.0 shiny vtx shade 2.0 20.0 125.0 dull tiler 3.0 23.0 120.0 dull vtx. shade 
].0 8.5 65.0 jaggy tiler 1.5 5.5 22.0 null clipping .I .3 1.5 trivial accept .2 .9 6.0 bucket sort .] 
.6 3.3 transformations .3 2.5 17.2 data input .3 2.7 10.5 Removing the need to worry about intersections 
can similarly simplify algorithms for other types of surfaces. The question then becomes what to do when 
an object does intersect itself, or two or more objects cannot be separated by the super- visory process. 
Obviously, we can retrench and use a last-resort algorithm which handles such surfaces at a loss in efficiency 
(An algorithm similar to Newell, Newell and Sancha[16] is currently used). However, if the displayprocess 
feeds back a characterization of the true nature of the intersection it may be possible to make subsequent 
images more efficiently. For example, tests by the supervisory process may show that two nearby objects 
intersect. The last-resort process will find those intersections if they exist or indicate that there 
are no such intersections. In either case, if the objects don't move relative to one another, the informa- 
tion obviates the need to use the last-resort process in succeeding images. The objects may be declared 
non-intersectlng or may be modified by a "clusterizing" algorithm to resolve the intersec- tions. Since 
surfaces of many different formulations may co-exist, there can be great difficulty in trying to resolve 
intersections. Therefore, the last-resort algorithm must be based on a common surface description~ by 
default a polygonal one. Therefore all data types which are to be inter- sected must have an associated 
algorithm for expansion of the surface description to polygons for use with the last-resort process. 
The alter- native is to produce intersection algorithms for each possible combination of surface types, 
which would not be practical for interesting collec- tions of surfaces. SUPPORTING DATA STRUCTURE The 
data structure describing objects consists of two basic levels of description. The top level is a human-readable 
format (described in appendix B) providing information to the super- visory process . Various keywords 
and associated fields identify a bounding box for the object, the name of the program which is normally 
used to display the object, the name of the program which can expand the object to a polygon description 
if necessary, the names of files containing descrip- tions of the actual shape and surface treatment 
of the object, and comments. Most of the data we use is produced by automated techniques involving interactive 
graph- ics. The data generation software automatically generates and formats the information needed for 
the top-level description. Similar programs are used for imported or hand-built data. These pro- grams 
read shape data in various common formats, calculate the bounding box, prompt for other information such 
as file names, and output a pair of properly formatted files. Therefore, a user doesn't really have to 
know more about an object than the file name he/she has assigned to it. The second level of description 
consists of files which actually describe the object to be displayed. Several different files may be 
listed for a given object, one describing the shape to be displayed and perhaps several describing colors 
for individual surface elements, s texture pattern to be mapped over the surface, or other surface characteristics. 
 Each of these files may contain a number of shape definitions, each of which corresponds to a different 
level of detail. As an object is displayed larger, the display program must read deeper in the file to 
obtain a greater level of detail. Since the display process knows from the bounding box and transform 
matrix roughly how many pixels the image of the object will cover. The various detail levels are characterized 
by a maximum appropriate height, in pixels. This whole process is a bit clumsy since levels of detail 
must be designed by hand and there doesn't seem to be a good way to settle on an appropriate plxel height. 
Methods for automating much of the process are under investigation. Other aids to the supervisory process 
have been considered. For example, a somewhat better description than the hounding box or sphere, perhaps 
some form of convex hull enclosing the surface, would give a better reading on potential i-tersections 
if included in the top-level description. Also, a maximal box or hull enclosed by the surface has been 
suggested. I have seen relatively few pictures which would benefit from either refinement, however, so 
not much effort has been expended in this direction. COMPLICATIONS AND FUTURE RESEARCH So far, the 
chief difficulty with this scheme appears to lie in the problems posed by modelling objects on a terrain 
or inside an enclosure. The terrain must be broken up into sectors if it undulates at all. Otherwise 
the gross interac- tion tests at the supervisory level always fail and the display system must default 
to the last resort algorithm. Similarly a group of enclosed objects cannot be separated from the enclosure. 
These are just special cases of the general prob- lem of modelling nearby objects of greatly vary- ing 
size. The separation of such objects is often impossible using bounding boxes or spheres. Further investigation 
into quick algorithms for separating objects based on descriptions somewhat more complicated than bounding 
boxes should shed some light on this problem. Problems arise in trying to implement advanced shading 
techniques involving object interactions. Recent work in ray-tracing algorithms [12,24] has shown the 
effectiveness of refraction, specular reflection, and diffuse reflection. Since the advantages to this 
display scheme lie in dividing the environment into manageable pieces, ray trac- ing algorithms don't 
fit very well. However, Kay [12] has shown that a somewhat simpler refraction model may be used effectively 
when the tran- sparent object is rendered in a frame buffer over the image of what lies behind it. Furthermore, 
Blinn and Newell [3] have published an algorithm for reflections which depends only on an image of the 
environment as seen from the reflecting object. Such an image may be calculated and used under most circumstances. 
Diffuse reflection need not be as accurate. Perhaps it can be done by using proximity information alone. 
 Therefore, many of the effects demonstrated by ray-tracing algorithms could be simulated by means more 
compatible with the system described here. However, it would also be possible to make selective use of 
a ray-tracing algorithm by applying it only to certain highly reflective objects in the scene. As a slave 
process, the ray-tracer would need to be passed the object to be rendered and then the names of all the 
remain- ing objects in the scene. Under some rather extreme circumstances this might be worth doing. 
 We are working on an implementation of shadows similar to that reported by Atherton[l]. A reversed priority 
graph calculated from the point of view of a light source will make it easy to find those objects which 
potentially cast a sha- dow on a given object. New second-level shape files which contain surface elements 
modified by shadow edges tagged with the light source which caused them will then be generated for shadowed 
objects . For the moment, only polygonal shapes are to he considered, in order to avoid an ever- proliferating 
collection of algorithms for inter- secting all combinations of different surface descriptions. Of course 
all other surface types may be expanded to polygons, but we lose some of the advantages of having heterogeneous 
surface types. The most valuable visual cue given by shadows reveals the relationship between nearby 
objects. It is not necessary to calculate the exact pro- jection of the silhouette of one object on the 
other to accomplish this. In real-life situa- tions, where the light source is overhead, the strength 
of the ambient light often reduces sha- dows to a subtle darkening below an object. Thus "pseudo-shadows" 
can be used to suggest proximity without having to calculate shadow volume inter- sections. This approach 
is also under investiga- tion EXAMPLE The image shown in figure 4 was made by the system in its current 
state. Figure 5 shows the priority graph representing the scene of figure 4. Tables P, 2, and 4 show 
the input to the supervisory process, the top-level description of two of the objects and the output 
generated by the supervisory process for input to two of the display programs (the glass uses a program 
for applying texture to polygonal objects, the egg uses one for faceted polygonal objects with indi- 
vidually colored polygons). Table 2 Input to Supervisory Process for Figure 4 call chkrbd_b ik.obj 
by board call chkrbd brn.obj by board2 call pysnkw~b .obj by egg scale egg by .5 .5 .5 call champagngl.obj 
by glass scale glass by .~ .4 .6 call fractal.obj by mntns scale mntns by 70 I0 I000 call fb corinth.obj 
by postlb scale postYb by 2 2 2.5 call fm corlnth.obj by postlm call ft corlnth.obj by postlt call bot 
third.obj by post2 scale pos~2 by 2 2 2.5 call fb_corinth.obj by post2 scale post~ by 2 2 2.5 paint background 
with .8 .-~ I . ]5 place center of interest at 0 0 .~ place eyepoint at -.~ -4 .6 place light at 1000 
-i000 500 place board at -8-4 0 rotate board about 8 8 0 8 8 ] by L5 attach board2 to board at -2 0 
0 paint board2 with .8 .3 .~ place egg on board at I 2 0 rotate egg about 0 0 0 0 0 I by -135 place glass 
on board at -.2 .6 0 rotate glass about -.7 .7 0 -.6 .8 0 by 80 place mntns at 0 8 .2 place post]b at 
-.5 -1.5 0 rotate post]b about 0 0 0 0 0 1 by -45 then about 0 0 0 1 -i 0 by 90 attach post]m to post]b 
at 0 0 0 rotate post]m about 0 0 .46 1 0 .46 by 30 attach post]t to post]b at 0 0 0 rotate post]t about 
0 0 .71 I 0 .71 by 20 place post2 at -I 0 0 rotate post2 about 0 0 0 0 0 1 by ]80 place post3 at -I 1.5 
0 rotate post~ about 0 0 0 0 0 1 by -130 Table 3 Object Description for Glass title Champagne glass 
(from Utah) display ftb_zsort detail champagngl.det type polygon open texture champagngl.txc stripes.txtr 
color 1 1 1 shininess ]00 transmittance 0.9 1 boundingbox -I I -1 1 -0.9 0.9 Table 4 Display Proc. Output 
for Glass and Egg device fb light 1071.99 429.513 -953.086 1 I I ~.9104e+11 ambient_light .15 object 
champagngl.obj no_clipping transform 0.2217 0.2913 0.]610 0 0.]472 -0.2593 0.2665 0 -0.4478 0.1326 
0.3765 0 -0.8644 0.0582 5.]381 I device fb light 1071.99 429.513 -953.086 1 1 I ~.910~e+11 ambient_light 
.]5 object pysnkwrb.obj no_clipping transform -0.3790 0.0243 0.3252 0 -0.3261 -0.0282 -0.2779 0 0 0.4986 
-0.0372 0 0.8476 0.0546 6.0933 1  Don Stredney designed and laboriously assembled "George", the skeleton. 
Rick Balabuck designed the broken columns. The author produced the banana, checkerboard, glass and striped 
texture pattern. Dave Zeltzer provided the fractal moun- tains. Finally, the Ukrainian easter egg (pysanka) 
was made with data assembled by Ron Resch, Robert McDermott, and Jim Blinn many years ago at the University 
of Utah. Figure 6 shows a rocking chair which demon- strates three levels of detail. (1) The top level 
 [figure 6a] shows high detail with spoolwork on all the connecting struts. (2) the middle level [figure 
6b] shows roughly the same structure missing the finer detail such as the spoolwork. (3) the lowest 
level [figure 6c] shows only the larger masses~of the chair with connecting struts removed. Figure 6d 
shows all three descriptions used at appropriate sizes. Rick Balabuck designed the ~ocking chair. APPENDI~ 
A The Scene Description Format ........... User conveniences ........... dump_to <filename> dump image 
description to file show <object> show active object descriptions ? help message (list of commands) 
 ]<shell command> execute shell command @<filename> temporarily take input from file #<comment> comment 
line for explanations ....... Image production commands ....... dlsplay_on <device> r<frame number>] 
generate image, quality determined by device (no background fill) render_on <device> f<frame_number>] 
generate full-quality image or line drawing as appropriate sketch on <device> generate image using only 
bounding boxes ~~~ Object attributes and positioning ~~~ attach <object> to <object> at <point> set 
up object hierarchy call <filename> by <object> read in object description and tag it with object name 
 delete <object> remove object from further use detach <object> detach object (and descendents) from 
hierarchy  group <filename> as <object> tak~ i-put from file and allow collective operations on name 
 paint <object> with <color> assign color to object place <object> [on <object>] at <point> position 
object with optional support relationship replace <object> read in object description anew (for dynamically 
changing shapes) rotate <object> about <axis> by <number> |then about <axis> by <number>] rotation 
about arbitrary axis (up to four may be concatenated by using "then") scale <object> by <point> scale 
object on its ~ axes .............. Metasymbol s .............. <axis> :== <point> <point> <color> :== 
<fraction> <fraction> <fraction> <device> :== <string> unique string for each display device <fraction> 
:== floating pt. number between 0.0 and 1.0 <frame_number> :== integer <number> :== floating pt. number 
<object> :== "all" (where appropriate) I "eyepoint" [ "center of interest" [ "light" [ "background" 
[ "foreground" I user-defined string <point> :== <number> <number> <number>  APPENDIX B The Human-Readable 
Top-Level Object Description (some keywords for polygonal objects) title <string> title for the data 
 # <string> comment line for explanations detail <filename> file for detailed shape data vertex colors 
<filename> colors for vertices of shape poly_colors <filename> colors for polygons of shape texture 
<filename> <filename> texture coordinate file followed by texture image file poly_neighbors <filename> 
neighboring polygon pointers bounding_box <Xmln Xmax Ymin Ymax Zmin Zmax> bounding box for object in 
definition space centrold&#38;radius <x y z radius> centroid and radius of enclosing sphere display 
<filename> executable file which displays from object description file poly_expansion <filename> executable 
file which expands detail file to polygons type <datatype> <characteristics> type of surface element 
and further useful i-fo (for "polygon", "open" and "faceted" are characteristics) color <r g b> overall 
color of object transmittance <float> <float> transmittance and rolloff power shininess <float> exponent 
for highlights APPENDIX C Format Fed to Display Algorithms device <device> <frame #> specify display 
device, quadrant, etc. (frame number for sequences) light <x y z r g b radius> position, color and radius 
covered by light (one entry for each source), radius for square-law attenuation ambient_light <float> 
ambient (fill) light, range 0 -] object <filename> multi-level shape detail file no_clipping no clipping 
needed (default clips) transform <4 rows of ~ floats> transformation to eyespace for object  References 
 ]. Atherton, Peter R., Weiler, Kevin J., and Greenberg, Donald P., "Polygon Shadow Genera- tion," Computer 
Graphics Vol. 12(3) pp. 275-281 Proc. Siggraph 78, (August 1978). 2. Babaoglu, Ozalp, Joy, William, 
and Porcer, Juan, "Design and Implementation of the Berke- ley Virtual MEmory Extensions to the UNIX 
Operating System," Technical Report, Dept of EECS, University of California at Berkeley (1979).  3. 
Blinn, James F. and Newell, Martin E., "Tex- ture and Reflection in Computer Generated Images," Communications 
of the ACM Vol. 19(10) pp. 542-547 {October 1976).  ~. Blinn, James F., "Computer Display of Curved 
Surfaces," Tech. Rpt. ]060-126 Jet Propulsion Lab, Pasadena, University of Utah (December 1978). PhD 
Thesis 5. Blinn, James F., Carpenter, Loren C., Lane, Jeffrey M., and Whitted, Turner, "Scan Line Methods 
for Displaying Parametrically Defined Surfaces," Communications of the ACM Vol. 23(1) pp. 23-34 (January 
]980).  6. Bouknight, W. Jack, "A Procedure for Genera- tion of Three-D Half-Toned Computer Graphics 
Reps.," Communications of the ACM Vol. ]3(9) pp. 527-536 {September 1970).  7. Christiansen, Henry N 
and Stephenson, Michael B., "MOVIE.BYU -A Computer Graphics Software System," Journal of the Technical 
Councils of ASCE, pp. 3-12 (April 1979).  8. Clark, James H., "Hierarchical Geometric Models for Visible 
Surface Algorithms," Commun- ications of the ACM Vol. ]9(10) pp. 547-554  (October 1976).  9. Crow, 
Franklin C., "Shaded Computer Graphics in the Entertainment Industry," ~omputer Vol. ]i(3) pp. ]1-22 
(March ]978).  ]0. Fuchs, Henry, Kedem, Zvi M., and Naylor, Bruce F., "Predetermining Visibility Priority 
in 3-D Scenes," Computer Graphics Vol. ]3{2) pp. ]75-181SIGGRAPH-ACM, (August ]979). ]I. Goldstein, 
R.A. and Nagel, R., 3-D Visual Simulation, Simulation (1971). ]2. Kay, Douglas S., "Transparency for 
Computer Synthesized Images," Computer Graphics Vol. ]3(2) pp. ]58-164 Proc. Siggraph "79, (August ]979). 
Cornell University ]3. Knowlton, Kenneth C. and Cherry, Lorinda, "ATOMS-A Three-D Opaque Molecule System 
for Color Pictures of Space-Filling or Ball-and- Stick Models," Computers &#38; Chemistry Vol. ] pp. 
]61-166 {1977). 1&#38;. Max, Nelson L., "ATOMLLL: ATOMS with Shading and Highlights," Computer Graphics 
Vol. 13(2) pp. 165-173 Proc. Siggraph "79, (August J979). 15. Myers, Allan, "An Efficient Algorithm 
for Computer Generated Pictures," Tech. Report, Ohio State University (1975). Computer Graph- ics Research 
Group  16. Newell, Martin E., Newell, Richard G., and Sancha, Tom L., "A New Approach to the Shaded 
Picture Problem," Proc. ACM National Confer- ence, pp. ~43-450 (August ]972).  J7. Newell, Martin E., 
"The Utilization of Pro- cedural Models in Digital Image Synthesis," UTEC SCc-76-218, Salt Lake City 
(1975). Department of Computer Science 18. Porter, Thomas K., "Spherical Shading," Com- puter Graphics 
Vol. ]2~3) pp. 282-285 Proc. Siggraph "78, (August 1978).  19. Rougelot, Rodney S., "The General Electric 
Computer Color TV Display," in Pertinent Con- cepts in Computer Graphics, ed. J. Nievergelt,University 
of Illinois Press, Urbana (1969).  20. Sutherland, Ivan E., Sproull, Robert F., and Schumaker, Robert 
A., "A Characterization of Ten Hidden-Surface Algorithms," Computing Sur- veys Vol. 6(]) pp. ]-55 (March 
1977).  2]. Watkins, Gary S., "A Real-Time Visible Sur- face Algorithm," UTEC-CSc-70-10], Dept. Com- 
puter Science, U. Utah, Salt Lake City (June 1970). Ph.D. thesis, U. of Utah 22. Weiler, Kevin J. and 
~therton, Peter ~., "Hidden-Surface Removal Using Polygon Area Sorting," Computer Graphics Vol. ]I(2) 
pp. 21~-222 SIGGRAPH-ACM, (July 1977).  23. Weinberg, Richard, "Computer Graphics in Sup- port of Space 
Shuttle Simulation," Computer Graphic s Vol. ]2(3) pp. 82-86 SIC~RAPH-ACM,  (August ]978).  24. Whitted, 
J. Turner, "An Improved Illumination Model for Shaded Display," Communications of the ACM Vol. 23(6) 
pp. 243-349 (June ]980).  25. Whitted, Turner and Weimer, David, "A Software Test-Bed for the Development 
of ~-D Raster Graphics Systems," Computer Graphics Vol. ]5(3)SIGGRAPH-ACM, (August 1981).  26. Williams, 
Lance, "Casting Curved Shadows on Curved Surfaces," Computer Graphics Vol. ]2(3) pp. 270-274 SICGRAPH-ACM, 
(August 1978).  27. Williams, Lance, personal communication ]979.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801254</article_id>
		<sort_key>19</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Anti-aliasing through the use of coordinate transformations]]></title>
		<page_from>19</page_from>
		<doi_number>10.1145/800064.801254</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801254</url>
		<abstract>
			<par><![CDATA[<p>The use of the point-line distance in evaluating the 2-dimensional anti-aliasing convolution is studied. We derive transformations of the point-spread function (PSF) that give the effective convolution in terms of the point-line distance when the class of object space primitives is limited to lines and polygons. Because the quality of filtering is embedded in a table indexed by the point-line distance, this approach allows one to use arbitrarily complex PSF's, only the width and not the shape of the PSF affects the amount of computation. We apply the CORDIC algorithm to point-line distance evaluation, and show its merits. Also, we show the more standard use of the CORDIC algorithm for coordinate rotation, polar-to-rectangular and rectangular-to-polar conversion, and calculating the norm of a vector. Rounded end points can be achieved by using the point-segment distance, computational methods are given, including CORDIC implementation. The CORDIC algorithms for the aforementioned geometric operations are prime candidates for VLSI implementation because of their inherent parallel/pipeline nature.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31097753</person_id>
				<author_profile_id><![CDATA[81100206946]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenneth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turkowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CADLINC, Inc., 480 California Avenue, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Anti-Aliasing Through the Use of Coordinate Transformations Kenneth Turkowski CADLINC, Inc. 480 California 
Avenue Palo Alto, CA 94306 Abstract The use of the point-line distance in evaluating the 2- dimensional 
anti-aliasing convolution is studied. We derive transfor- mations of the point-spread function (PSF) 
that give the effective convolution in terms of the point-line distance when the class of object space 
primitives is limited to lines and polygons. Because the quality of filtering is embedded in a table 
indexed by the point-line distance, this approach allows one to use arbitrarily complex PSF's, only the 
width and not the shape of the PSF affects the amount of com- putation. We apply the CORDIC algorithm 
to point-line distance evaluation, and show its merits. Also, we show the more standard use of the CORDIC 
algorithm for coordinate rotation, polar-to-rectangular and rectangular-to-polar conversion, and calculating 
the norm of a vector. Rounded end points can be achieved by using the point-segment distance, computational 
methods are given, including CORDIC implemen- tation. The CORDIC algorithms for the aforementioned geometric 
opera- tions are prime candidates for VLSI implementation because of their inherent parallel/pipeline 
nature. * This paper has been selected for publication in the July 1982 issue of Transactions on Graphics. 
 19 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801255</article_id>
		<sort_key>21</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Light reflection functions for simulation of clouds and dusty surfaces]]></title>
		<page_from>21</page_from>
		<page_to>29</page_to>
		<doi_number>10.1145/800064.801255</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801255</url>
		<abstract>
			<par><![CDATA[<p>The study of the physical process of light interacting with matter is an important part of computer image synthesis since it forms the basis for calculations of intensities in the picture. The simpler models used in the past are being augmented by more complex models gleaned from the physics literature. This paper is another step in the direction of assimilating such knowledge. It concerns the statistical simulation of light passing through and being reflected by clouds of similar small particles. (It does not, however, address the cloud structure modeling problem). By extension it can be applied to surfaces completely covered by dust and is therefore a physical basis for various theories of diffuse reflection.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P131722</person_id>
				<author_profile_id><![CDATA[81100294395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Blinn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, MS 201-209, Pasadena, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F., Models of light reflection for computer synthesizes pictures, Computer Graphics, Vol 11, No. 2, 192-198.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806819</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cook, R. L., and Torrance, K. E., A reflectance model for computer graphics, Computer Graphics, Vol 15, No. 3 (1981), 307-316.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chandrasekhar, S., Radiative Transfer, Dover, New York, 1960.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Esposito, L. W., and Lumme, K. (1977). The tilt effect for Saturns's rings. Icarus 31, 157-167.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Esposito, L. W. (1979). Extensions to the classical calculation of the effect of mutual shadowing in diffuse reflection. Icarus 39, 69-80.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Esposito, L. W., et al. (1979). International planetary patrol observations of Saturn's rings II. Four color phase curves and their analysis. Astron. J. 84, 1408-1415.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hansen, J. E., and Travis, L. (1974). Light scattering in planetary atmospheres. Space Sci. Rev. 16, 527-610.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Hapke, B. W. (1963). A theoretical photometric function for the lunar surface. J. Geophys. Res. 68, 4571-4586.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Henyey, L. G., and Greenstein, J. L. (1941), Diffuse reflection in the galaxy, Astrophys. J. 93, 70.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Irvine, W. M. (1966). The shadowing effect in diffuse reflection. J. Geophys. Res. 71, 2931-2937.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kawata, Y., and Irvine, W. M., Models of Saturns's rings which satisfy the optical observations. in Woszczyk and Iwaniszewska (eds.) Exploration of the Planetary System, 441-464.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Klaasen, K. P., Duxbury, T. C. and Veverka, J. (1979). Photometry of Phobos and Deimos from viking orbiter images. J. Geophys. Res. 84, 8478-8486.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806820</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Max, N. L., Vectorized Procedural Models for Natural Terrain: waves and islands in the sunset, Computer Graphics, Vol. 15, No. 3 (1981), 317-324.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>810236</ref_obj_id>
				<ref_obj_pid>800179</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Newell, M. E., and Blinn, J. F., The progression of realism in computer generated images, Proceedings of ACM National Conf., 1977, 444-448.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Veverka, J., Goguen, J., Yang, S., and Elliot, J. (1978). Scattering of light from particulate surfaces. I. A laboratory assesment of multiple scattering effects. Icarus 34, 406-414.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Veverka, J., The physical meaning of phase coefficients. NASA SP-267, 79-90.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LIGHT REFLECTION FUNCTIONS FOR SIMULATION OF CLOUDS AND DUSTY SURFACES James F. Blinn Jet Propulsion 
Laborat6ry California Institute of Technology 4800 Oak Grove Drive, MS 201-209 Pasadena, CA 91109 ABSTRACT 
 The study of the physical process of light interacting with matter is an important part of computer 
image synthesis since it forms the basis for calculations of intensities in the picture. The simpler 
models used in the past are being augmented by more complex models gleaned from the physics literature. 
This paper is another step in the direction of assimilating such knowledge. It concerns the statistical 
simulation of light passing through and being reflected by clouds of similar small particles. (It does 
not, however, address the cloud structure modeling problem). By extension it can be applied to surfaces 
completely covered by dust and is therefore a physical basis for various theories of diffuse reflection. 
 CR Categories and Subject Descriptors: 1.3.3 [Computer Graphics] : Picture/Image Generation - display 
algorithas; I. 3.7 [Computer Graphics] : Three-Dimensional Graphics and Realism -Visible I ine/surface 
algorithn General Terms: Algorithms, Theory i.INTRODUCTION Computer image synthesis requires the calculation 
of intensities of light reflecting from an object. Such calculations are based on the physics of light 
interaction with the surface and on the geometry of the light sources. Intensity functions are generally 
expressed in terms of the vector quantities shown in figure i. The research described in this paper 
was carried out by the Jet Propulsion Laboratory, California Institute of Technology, under contract 
with the National Aeronautics and Space ~dministration. Permission to copy without fee all or part of 
this material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. N = Vector perpendicular to the reflecting surface L = Vector 
in the direction of the light source E = Vector in the direction of the observer Figure i - Lighting 
Geometry Care must be taken in choosing the direction of the vector N. Two opposite directions will 
equally well describe the orientation of the surface. It usual to choose the direction which forms a 
positive dot product with E. Computer graphics first used the simplest of models, Lambert' s law, where 
the observed intensity is assigned proportional to the projected areas of the light source on the surface, 
thus: I = a (N.L) for N.L > 0 = 0 for N.L < 0 Later models [1,2] enhanced this by adding a specular 
component which is a function of all three vectors: I = a (N.L) + b Spec(N,L,E) These models were applied 
to solid surfaces and produced good simulations of both matte" (diffuse reflecting) and shiny (specularly 
reflecting) sur faces. One of the current frontiers in computer image synthesis is the simulation of 
fuzzy or cloudy surfaces [14]° Some initial efforts on this problem have been undertaken by Nelson Max 
at Lawrence Livermore Laboratory [13] and Roger Wilson at Ohio State University. The problem has two 
components, the modeling of the cloud density as a function of position in space, and the simulation 
,of how light interacts with this density function. The modeling problem will not be addressed here. 
 (~) 1982 ACM 0-89791-076-1/82/007/0021 $00.75 21 This paper presents some simple simulations relevant 
to the second part of the problem. This work was motivated by the need to synthesize images of another 
type of cloudy object, the rings of the planet Saturn. This problem has the advantage that it is a fairly 
simple geometric situation and attention can be paid to the light reflection portion of the problem. 
 2. SINGLE SCATFERING CLOUD MODEL The exact simulation of light interacting with clouds of particles 
is, in general, a very complex problem. It is extensively studied by the discipline known as Radiative 
Transport Theory. The classic work in the field is by Chandrasekhar [3]. Unfortt~nately this book is 
somewhat inaccessible for all but the mathematically sophisticated. Various of the simpler results have 
been presented elsewhere, however [5,8,10] and it is to these that we turn. 2.1 Geometry of the Model 
 The basic model, shown in figure 2, assumes a cloud of spherical reflecting particles of radius p , 
positioned randomly in a layer of thickness T, and having number density n (That is, there are n particles 
per unit volume). The proportional volume of the cloud occupied by particles is then the number density 
times the volume of one particle: 3 D = n (4/3)'~" p This will have values between 0 and 1 but will 
presumably be small for fairly diffuse clouds. N 0 0 0 0 0 T 0 ~ 0 0 ~0 0 O0 ;-F ~ol"-oJ '0'0000 0 
n po.r{'~cle.~ per ~Ln[t; volume Figure 2 - Geometry of Cloud Layer We wish to find the amount of light 
escaping from the layer in the direction of E after entering from the direction L and reflecting off 
one or more particles. We would also like to find the transparency of the layer, Tr, the amount of light 
showing through the layer from behind, i.e. from the direction -E. In the literature on the subject 
the following notation is used for the angles between the three lighting vectors N, L and E. incident 
angle = i cos(i) = ~/0 = N.L emission angle = e cos(e) = 2~ = N.E phase angle = a cos(a) = L.E The 
angle between L and E is called the phase angle. A phase angle of zero occurs when light is coming from 
directly behind the viewer. When observing the moon, for example, this situation corresponds to a full 
moon. As the moon goes through its various phases the phase angle cycles through 360 degrees. We will 
make a series of simplifying assumptions about the physical system which will make the problem analytically 
tractable. The result will be a brightness function of form: B =w/y ~(a) s where w = albedo of individual 
particles (section 2.3) u = cosine of emission angle = N.E = Phase function of a (section 2.2) S = 
Scattering probability (section 2.4) The brightness calculations to follow will determine a brightness 
per unit area of the surface. An observer viewing the surface at a shallow angle e sees a greater portion 
of the surface projected into one pixel than an observer viewing it perpendicularly. This projected 
area is taken into consideration by the division by N.E. The remaining terms are discussed in sections 
 below.  2.2 Phase Function Each particle is too ~nall to be seen individually and so its observed 
brightness is the integral of the contributions over its observed area. Due to the symmetry of the particles 
this net brightness can be assumed to vary only as a function of the lighting direction from the point 
of view of the observer. This is the angle between E and L, the phase angle a. A function characterizing 
the total brightness of a particle as a function of this angle is called a phase function, ~(a). For 
exanple, if the particles are substantially larger than the wavelength of light, diffraction at the edges 
of the particles is negligible and the phase function will be as in figure 3. ~he exact form of various 
useful functions for ~ will be discussed in section 3.  TopView: EL E E Eye View: &#38;#169; @ @ 0_=0 
0.. = qO ° ~= IBO ° o /~o o.._i, Figure 3 - Definition of Phase Function  2.3 Albedo The next simplifying 
assumption is that the primary effect is from the interaction of a ray of light with a single particle, 
i.e. multiple reflections will be considered negligable. This will be true if the albedo, w, of each 
particle is small. (Albedo is the proportion of light reflected from a particle vs. light impinging on 
it, i.e. the reflectivity of the particle). The net brightness will be proportional to w. 2.4 Scattering 
 The final effect on the brightness will be due to the shadowing and blocking effect of other particles 
on the light ray as it enters and exits the cloud layer. The model asserts that a ray of light will be 
visible if there are no other particles in the way along this path. For example; for a particle at depth 
T' in the layer, illuminated from above, a ray of light will bounce off it and be visible if no other 
particles encroach on the volt,he formed by the two cylinders Vin and Vout having radius p in figure 
4.  ut eem t ...... Vm'6~--~ Vim for light ray to enter at L and escape to E. Figure 4 -Scattering 
Conditions All other particles are assLmed to be of radius p also. For such a particle to be completely 
outside the cylinder its center must be outside this volume. Statistically, then, the attenuation of 
light traversing a cylinder of radius p and volume V is P(0;V) = the probability of 0 particles in volume 
V. Now the expected number of particles in a given volume V is nV. If n is small this can be modelled 
as a Poisson process and P(0;V) = exp(-nV)  An approximation is made here that two particles being 
inside V are independent events. In reality, the impossibility of mutual overlap makes them slightly 
dependant. We can neglect this if D is small. The brightness due to a given layer dT' within the cloud 
is the product of 2 projected viewing area = ~7 p/)] brightness of particle = w ~(a) expected number 
of particles/unit area = n dT' probability particle will be illt~inated = p(0;V) The net brightness 
of the cloud will be the integral of this function over T. This reduces to 2fT There are two cases to 
be considered here. The top lit case, where N.L > 0, is shown in figure 5. \ 2 Figure 5 -Top Lit Note 
that there is an approximation introduced here in that the overlap between cylinders Vin and Vout is 
being neglected. That is, some of the volume is being counted twice. This becomes significant only when 
E~L and .will be addressed later. B=__w ,r --("P ' +~P_C_CJ-; , ~(~-) n~-p ~ ~ ~o /~ " ~I.T J o ~lhe 
bottom lit case, where N.L < 0, is illustrated in figure 6. < v,. °   TT ~(r-T')fi~o note:/.~ ,;-" 
# Vo-~" = IT p~" TV,/~ Figure 6 - Bottom Lit "F _nFFpZ (TLT ' ~____._~_ ~7-') d.-/" D L Note that the 
variables T, n and p occur in the above expressions only in the form of 2 n~" p T This dimensionless 
quantity is called the "optical depth" and is written . Light traveling through a cloud of optical depth 
"~" is attenuated by the factor exp(-T). In terms of this quantity the final brightness function is 
B = w/]J ~(a) n ~ p P(0;V) dT' 0 Top lit: -"r(lll.,o + l/F) B = w ~(a) }/01(p0+p) (i- e Bottom lit: 
 B = w ~(a) p0/(F0+p) (e -e )  Note that in this latter case we may have a singularity if ~0=-~. Here 
we must perform the substitution before the integration and get  _.r/~ B = W ~(a) "r/~ e 2.5 Transparency 
 The transparency of the cloud layer is the amount of light coming from directly behind the cloud which 
is not blocked off by particles. This will be the probability that a light ray does not encounter a particle 
in travelling through the layer. See figure 7. For arguments similar to the above it will be the probability 
that a cylinder of radius p extending through the layer has no particles in it. That is:  -~-/~ Tr 
= e ,z&#38; Figure 7 - Transparency  One may,_ in addition add a term for the forward scattering of 
the background through the cloud e Tr = + w ~(=)  2.6 Net Intensity calculation  The new brightness 
to be displayed at a given pixel for a cloud overlayed on a background color Bkg is Bnew = B + Tr 
* Bkg  2.7 The HaDke-Irvin e Function  An important special case of this function was presented by 
Ha~ke [8] and Irvine [i0]. They proposed to model a dust covered surface or a dense atmosphere as a cloud 
with an infinite optical depth. In this case the top lit brightness function becomes B = w ~(a) (N.L)/(N.L+N.E) 
 Note that for the case where L=E (i.e. where the light is directly behind the observer) this reduces 
to B =1/2w ~(a) i.e.. a constant for any value of N. Recall that L=E corresponds to the situation during 
a full moon. This result, then, says that the entire disk of the full moon should have a uniform intensity, 
independant of the variation in normal vector from center to edges. 3. PHASE FUNCTIONS We now turn 
to the phase function used in the above models. The form of the phase function depends on the physical 
structure of the individual particles. Several functions have been proposed in the literature. The simplest 
are motivated more due to their mathematical simplicity than for physical reasons. 3.1 Constant Function 
 The simplest function used in the earliest of models assumes isotropic scattering [4] and is simply 
a constant. ~(a) = 1 This function corresponds to the situation where the size of the scattering particles 
is substantially less than the wavelength of the I ight.  3.2 Anisotropic The next most complex form 
takes account of the fact that more light should be reflected back toward the light source than forward. 
Its form has been chosen to be simple algebraically and of roughly the correct shape for this effect. 
 ~(a) = F(a) = 1 + x cos(a) where x = adjustable property of material.  3.3 Lambert surfaces The first 
really physically motivated function assumes each sphere to reflect light according to Lambert's Law. 
Integrating the brightness of the visible disk of a particle for a given viewing direction yields [4] 
 ~0(a) = L(a) = (8/3~r) (sin(a) + (17"-a) cos(a))  3.4 Rayleigh Scattering For particles which are 
small compared to the wavelength of the light, diffraction effects predominate. This situation was first 
discussed by Lord Rayleigh [7] yielding the function 2 ~0(a) = 3/4 (i + cos a) Note that with this 
function, as much light is scattered in the forward direction as in the backward direction.  surface 
normal (36, 72, 108, and 144 degrees).   3.5 Henyey-Greenstein Note that the brightness function for 
an incident angle of 144 degrees is the same as the function  The analytic function which see~s most 
popular in at 36 degrees viewed from the other side of the  the literature is due to Henyey and Greenstein 
[9] cloud layer. 2 2 3/2 ~(a) = HG(g,a) = (l-g) / (l+g -2 g cos(a))  This is just the equation of 
an ellipse in polar coordinates, centered at one focus. The parameter g is the eccentricity of the ellipse 
and is a property of the material. It can be used to generate a primarily forward scattering function 
 (g<0) or a primarily backward scattering function (g>0) or an isotropic function (9=0). The physical 
relevance of this function is confirmed by its very good fit, using a value of g=.325, to empirical data 
of dark rough surfaces such as furnace slag [16].  3.6 Empirical Measurements  Some surfaces have been 
empirically measured and the resulting phase function tabulated. One example is measurements of the surfaces 
of the two moons of mars, Deimos and Phobos. The results are listed in [12]  3.7 Sums of Functions 
 Some approximate simulation can be made of clouds of non-equal-sized particles by using a net phase 
function that is a weighted average of several functions, each applicable to a different size of particle. 
This was done originally by Hapke in approximating the phase function for particles on the surface of 
the moon. He added a forward scattering function to a Lambert function to get: (a) = wl L(a) + w2 F(a) 
 The first term accounts for the back scattering of the rough particles and the second term accounts 
for the forward scattering of glass-like spherical particles. This technique was later employed by Esposito 
and Lumme [4] for the rings of Saturn, using two Henyey-Greenstein functions ~(a) = wl HG(gl,a) + w2 
HG(g2,a)  They achieved a fair match to earth based observations with wl = .596 gl = .5 w2 = .404 g2 
= -.5  The first term accounts for the back-scattering of the large particles, the second term accounts 
for the forward scattering of the smaller particles. 4. RESULTS OF SINGLE SCATFERING MODEL  4.1 Variation 
with Incident/Emission Angle  Figure 8 shows a plot of the amount of light reflected from a surface 
as a result of an incident ray at four different angles from the The rectangular shape represents the 
cloud layer and has two embedded coordinate axes to emphasize the location of the incident ray, coming 
from the right. The distance from this point to the surface in a given direction represents the amount 
of light scattered in that direction. This is the value of the brightness function, omitting the division 
by the viewing angle term N.E. In addition, to minimize confusion, the phase function is taken to be 
unity. Any given phase function will be symmetrical about the incident ray and will scale the brightness 
values plotted here. Note that most of the light is reflected in directions perpendicular to the cloud 
layer. This is to be expected since this represents the shortest emission path and thus the least likelihood 
of encountering a blocking particle. Also note that the reflected light diminishes with increasing incidence 
angle since this represents a longer incident path and thus a greater likelihood of encountering a blocking 
particle.  4.2 Variations With Optical Depth Figure 8 also shows the function for four different values 
of "~= i000, 2., 0.5 and 0.I. For light reflected from the top of the cloud the brightness increases 
as 9" increases since there are more and more scattering particles. The brightness reaches a finite limit 
(the Hapke-Irvine function) as ";~approaches infinity. Light scattered through the cloud appears as 
a blob on the opposite side of the rectangle than the incident light ray. Note that for low values of 
"r an appreciable amount of light comes out the bottom of the cloud. As "Y increases the scattered brightness 
reaches a maximtun and then begins to decreased as the shadowing and blocking effect predominates. In 
the limit of~=infinity the cloud layer is opaque and no light gets through.  4.3 Application to the 
rings of Saturn The rings of Saturn consist of a cloud of reflective ice particles in orbit about the 
planet. The optical depth, albedo and size distribution of scattering particles in the rings varies with 
radius from the planet. A phase function which can account for this is w(r) ~(a,r) = wl(r) L(a) + w2(r) 
HG(-.5,a) Note that we have merged the albedo and phase function proportions into just the two coefficients 
wl and w2. Tabulated values for wl(r), w2(r), and T(r) were derived from Voyager 1 photographs taken 
at a few known viewing geometries by substituting observed brightnesses into the above equations and 
solving for wl,w2, and q~. The database was then used to synthesize    Computer Graphics Vin (=Vout). 
Substituting ~=p0 into the old expression Iold = w ~(a) .5(1 - e )  Substituting V=Vin into the original 
integral equation and re-integrating Inew = w ~(a) (i - e )  The overlap for more general values of 
a is a bit more tricky. Solutions to this problem have been given by HaFke [8], Irvine [i0], and Esposito 
[6]. They each conclude that the correction to compensate for this overlap effect depends on both 
the phase angle, a, and on the volt, he density, D. The mathematical form of these various corrections 
 is complex. We can, however, generate a correcton factor with approximately the right shape in a more 
simple manner. At a=0 the factor is Inew/Io!d. As a increases the effect of the overlap becomes more 
and more unimportant and the correction factor drops back to 1.0. The rate of decrease of the correction 
is a function of D; for small values of D the correction drops off quickly, for larger values it drops 
off slowly. Such a function can be constructed in a similar manner to the bt~np functions of [i]. 
 5.3 Multiple Scattering  If the individual particles have an appreciable albedo the effects of second 
and higher order scattering cannot be ignored. Veverka [16] has shown experimentally that this happens 
at values of w above 0.3. Above this value, multiple scattering effects must be accounted for. The basic 
idea of simulations of this effect is to expand the net intensity in what is called a Neumann series. 
 o~ n I (q',N,E, L) = ~--w In ('r,N, E, L) where w = single scattering albedo I (~,N,E, L) = intensity 
at optical depth in direction E  In (~',N, E, L) = intensity of photons scattered exactly n times 
 With the single scattering theory we have effectively calculated only Ii ('<,N, E, L). Various approaches 
have been used to find the total intensity I. Chandrasekhar [3] has reduced the solution to  I o<H(i,a) 
H(e,a)  where the H functions are defined by some quite complex simultaneous integral equations. Another 
approach [6] uses Markov chains to nt~nerically simulate the possible random scattering sequences. Yet 
another approach, called the doubling method, constructs a total picture of reflected intensity by building 
up layers of increasing optical depth.   Volume 16, Number 3 July 1982 The intensity is discretized 
over a finite namber of angular directions. Tne intensity for a layer of depth 2~'is found from two layers 
of depth q'. This is done by processing all sets of interactions for all possible combinations of the 
discretized directions between the two layers. This algorithm is described more fully in [7]. As might 
be inferred from the above discussion, the accurate simulation of multiple scattering requires a substantial 
amount of computation time, certainly more than would be practical on a pixel by pixel basis for the 
purposes of image synthesis. About the only practical approach would be to pre-evaluate such a function 
for various input parameters and generate a large look-up table for use in graphics applications. Certainly 
more work needs to be done here. The importance of multiple scattering is alluded to by Veverka when 
he points out that in the limit the multiple scattering law should provide a physical basis for the, 
so far, purely empirical Lambert's law. In fact, the surface of Jupiter, which is all clouds, follows 
the ideal Lambert law very closely. 6. CONCLUSIONS Computer graphics can benefit greatly from examination 
of the existing literature on light interacting with matter. Early models used for image synthesis, crude 
but effective, are being replaced by more accurate models. Earlier efforts in this regard have done extremely 
well for specular reflection. The models presented here are the beginnings of a more complete simulation 
of diffuse reflection. ~he problem of clouds is still not solved. The extension of this model to the 
simulation of an arbitrarily varying spatial density function of scatterers, with multiple reflections 
and different amotmts of shadowing from any direction is not straightforward. The models provided here 
do, however, represent some initial steps in that direction. REFERENCES [i] Blinn, J. F., Models of 
light reflection for computer synthesizes pictures, Computer Graphics, Vol ii, No. 2, 192-198. [2] Cook, 
R. L., and Torrance, K. E., A reflectance model for computer graphics, Computer Graphics, Vol 15, No. 
3 (1981), 307-316. [3] Chandrasekhar, S., Radiative Transfer, Dover, New York, 1960. [4] Esposito, 
Ltilt effect 157-167. . W., and for Saturns' Lupine, s ri K. ngs. (1977). Icarus The 31, [5] Esposito, 
L. W. (1979). Extensions to the classical calculation of the effect of mutual shadowing in diffuse reflection. 
Icarus 39, 69-80.  [6] Esposito, L. W., et al. (1979). International planetary patrol observations of 
Saturn's rings II. Four color phase curves and their analysis. Astron. J. 84, 1408-1415. [7] Hansen, 
J. E., and Travis, L. (1974). Light scattering in planetary atmospheres. Space Sci. Rev. 16, 527-610. 
 [8] Ha~ke, B. W. (1963). A theoretical photometric function for the lunar surface. J. Geophys. Res. 
68, 4571-4586. [9] Henyey, L. G., and Greenstein, J. L. (1941), Diffuse reflection in the galaxy, Astrophys. 
J. 93, 70. [i0] Irvine, W. M. (1966). THe shadowing effect in diffuse reflection. J. Geophys. Res. 71, 
 2931-2937.  [ii] Kawata, Y., and Irvine, W. M., Models of Saturns's rings which satisfy the optical 
observations, in Woszczyk and Iwanisze~ka (eds.) Exploration of the Planetary System, 441-464.  [12] 
Klaasen, K. P., Duxbury, T. C. and Veverka,  J. (1979). Photometry of Phobos and Deimos from viking 
orbiter images. J. Geophys. Res. 84, 8478-8486.  [13] Max, N. L., Vectorized Procedural Models for 
Natural Terrain: waves and islands in the sunset, Computer Graphics, Vol. 15, No. 3 (1981), 317-324. 
 [14] Newell, M. E., and Blinn, J. F., The progression of realism in computer generated images, Proceedings 
of ACM National Conf., 1977, 444-448. [15] Veverka, J. , Goguen, J., Yang, S., and Elliot, J. (1978). 
Scattering of light from particulate surfaces. I. A laboratory assesaent of multiple scattering effects. 
Icarus 34, 406-414. [16] Veverka, J., The physical meaning of phase coefficients. NASA SP-267, 79-90. 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801256</article_id>
		<sort_key>31</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[The impact of graphics standards an American point of view (Panel Session)]]></title>
		<page_from>31</page_from>
		<page_to>32</page_to>
		<doi_number>10.1145/800064.801256</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801256</url>
		<abstract>
			<par><![CDATA[<p>This panel session will consist of reports from participants in the effort to standardize computer graphics functions in America. There are two principal goals. The first is to provide status reports for the groups represented by the panelists. The second is to explain relationships between different standardization efforts.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329815</person_id>
				<author_profile_id><![CDATA[81100550687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Straayer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tektronix, Inc.]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31101931</person_id>
				<author_profile_id><![CDATA[81450594467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wright]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISSCO GRAPHICS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334016</person_id>
				<author_profile_id><![CDATA[81332522908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Theodore]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Los Alamos National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39083432</person_id>
				<author_profile_id><![CDATA[81100603093]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shuey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[McAuto Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329249</person_id>
				<author_profile_id><![CDATA[81543198656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Bruce]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329234</person_id>
				<author_profile_id><![CDATA[81100079558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Bradford]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Smith]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Bureau of Standards]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL The Impact of Graphics Standards An American Point of View CHAIR: David H. Straayer Tektronix, 
Inc. This panel session will consist of reports from participants in the effort to standardize computer 
graphics functions in America. There are two principal goals. The first is to provide status reports 
for the groups represented by the panelists. The second is to explain relationships between different 
standardization efforts. Panelist: Tom Wright ISSCO GRAPHICS Mr. Wright is the chairman of the ANSI 
X3H35 task group to develop an American National Standard for a Programmer's Minimal Interface to Graphics 
(PMIG). His topic is: The Proposed Programmer's Minimal Interface to Graphics Mr. Wright will be addressing 
the following questions: What is the PMIG? Who is the PMIG for? What functions are in the PMIG? What 
is the state of PMIG today? What form will the PMIG spec have? What about implementations of PMIG? 
 Panelist: Theodore Reed Los Alamos National Laboratory Mr. Reed is the chairman of the ANSI X3H33 task 
group to develop Amer- ican National Standards for Virtual Device Interface (VDI) and Virtual Device 
Metafile (VDM). His topic is: Graphics Metafile and Virtual Device Interface Mr. Reed will be addressing 
the following questions: What is a Metafile? What does the Metafile contain? What is a VDI? What will 
the VDI contain? How does the Metafile relate to the VDI? What is the relationship between ANSI X3H33 
and ISO? What is the status of the standards? 31 PANEL (continued): The Impact of Graphics Standards 
 Panelist: David Shuey McAuto Corporation Mr. Shuey is the chairman of the ANSI X3H31 task group. This 
group is developing an American National Standard for computer graphics func- tionality at a similar 
level to the SIGGRAPH GSPC Core. His topic is: Current Status of 3-D GKS Extensions and a "Rich" Graphics 
System Mr. Shuey will be addressing the following questions: What is GKS? What is the ANSI relationship 
to GKS? What types of extensions are being considered for GKS? What is the role of the GSPC Core in the 
developing ANSI standard? What is the "Rich" Graphics System? What is the status of the "Rich" Graphics 
System? Where is X3H31 concentrating its future efforts? Panelist: Bruce Cohen Intel Corporation Mr. 
Cohen is a member of the ANSI X3H33 task group and is the liaison to the ANSI X3L2 Committee. X3L2 is 
developing an American National Standard for coded character sets for Videotex/Teletext. This Videotex 
standard will be based on the North American Presentation Level Protocol drafted by the Bell System. 
Mr. Cohen's topic is: The Relationship Between Videotex and Graphics Standards Mr. Cohen will be addressing 
the following questions: What is the NAPLPS? Does the NAPLPS attempt to standardize graphics? What is 
the relationship between VDM and NAPLPS? Will VDI/VDM and NAPLPS be compatible? How are these committees 
working together? Panelist: Bradford M. Smith National Bureau of Standards Mr. Smith is head of the 
Manufacturing Systems Group at the National Bureau of Standards. He serves as the program chairman of 
the Initial Graphics Exchange Standard (IGES) specification. His topic is: Status, Use, and Implementations 
of the IGES Specification Mr. Smith will address the following questions: HOW does IGES function as 
a critical interface in CAD/CAM systems? What is the extent of vendor commitment to its implementation? 
What future improvements can be expected? Where is additional information available? What is the relationship 
between IGES and ANSI YI4.26-M? 32 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801257</article_id>
		<sort_key>33</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[The detailed semantics of graphics input devices]]></title>
		<page_from>33</page_from>
		<page_to>38</page_to>
		<doi_number>10.1145/800064.801257</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801257</url>
		<abstract>
			<par><![CDATA[<p>The concept of <italic>virtual input devices,</italic> enunciated by Wallace, has been the accepted basis for producing device-independent interactive graphics systems. It was used by GSPC for the Core System, and it underlies the draft international standard GKS.</p> <p>During the recently concluded technical review of GKS, the input facilities became a bone of contention. The discussions revealed many inadequacies in the virtual input device concept, and were finally resolved using a refined and extended model of input, which is presented here by some of the participants in the discussions. Examples are included, showing how the GKS facilities derive from the model, and the Core's &#8220;STROKE&#8221; device is used to show how the model controls future extensions to GKS. The model is also used to describe the other differences between the input facilities of the Core System and GKS.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Device independence**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14102481</person_id>
				<author_profile_id><![CDATA[81100271629]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[S. H.]]></middle_name>
				<last_name><![CDATA[Rosenthal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[EdCAAD Studies, Dept. of Architecture, Edinburgh University, Scotland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P133036</person_id>
				<author_profile_id><![CDATA[81100509207]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Michener]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intermetrics, Inc., Cambridge, Mass., U.S.A.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P88416</person_id>
				<author_profile_id><![CDATA[81100389252]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[G&#252;nther]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pfaff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institut f&#252;r Informationsverwaltung und Interaktive Systeme, Technische Hochschule Darmstadt, Federal Republic of Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333217</person_id>
				<author_profile_id><![CDATA[81100168385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Rens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kessener]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rekencentrum, Technische Hogeschool Eindhoven, The Netherlands]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31036576</person_id>
				<author_profile_id><![CDATA[81100307545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Malcolm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sabin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CAD Centre, Cambridge, England]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807432</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Anson, "The Semantics of Graphical Input," Computer Graphics13(2), pp. 113-120 (August 1979).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[I. W. Cotton, Network Graphic Attention Handling, Proc. Online '72 Conf., Brunel University, Uxbridge, England (September 1972).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DIN, "Graphical Kernel System (GKS)&#8212;Functional Description," (Version 6.6) (May 1981).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807497</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Encarna&#231;&#227;o, G. Enderle, and others, "The Workstation Concept of GKS and the Resulting Conceptual Differences to the GSPC Core System," Computer Graphics14(3), pp.226-230 (July 1980).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. D. Foley and V. L. Wallace, "The Art of Natural Man-Machine Communication," Proc. IEEE62(4), pp.462-471 (April 1974).]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[R. A. Guedj and others (eds.), IFIP Workshop on Methodology of Interaction, (publishers North-Holland), Seillac, France (May 1979).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[ISO, "Graphical Kernel System (GKS)&#8212;Functional Description," DP 7942 (January 1982).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[W. M. Newman, "A System for Interactive Graphical Programming," AFIPS Conference Proceedings (SJCC)32, pp.47-54 (1968).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[SIGGRAPH-ACM (GSPC), "Status Report of the Graphics Standards Planning Committee," Computer Graphics13(3) (August 1979).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804734</ref_obj_id>
				<ref_obj_pid>957197</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[V. L. Wallace, "The Semantics of Graphics Input Devices," Computer Graphics10(1) (Spring 1976).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Detailed Semantics of Graphics Input Devices David S. H. Rosenthal t EdCAAD Studies, Dept. of Architecture 
Edinburgh University, Scotland James C. Michener Intermetrics, Inc. Cambridge, Mass., U.S.A. Giinther 
Pfaff Institut fiJr Informationsverwaltung und Interaktive Systeme Technische Hochschule Darmstadt, Federal 
Republic of Germany Rens Kessener Rekencentrum Technische Hogeschool Eindhoven, The Netherlands Malcolm 
Sabin CAD Centre, Cambridge, England Abstract The concept of virtual input devices, enunciated by Wallace, 
has been the accepted basis for producing device-independent interactive graphics systems. It was used 
by GSPC for the Core System, and it underlies the draft international standard GKS. During the recently 
concluded technical review of GKS, the input facilities became a bone of contention. The discussions 
revealed many inadequacies in the virtual input device concept, and were finally resolved using a refined 
and extended model of input, which is presented here by some of the participants in the discussions. 
Examples are included, showing how the GKS facilities Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. derive from the model, and the Core's "STROKE" device is used 
to show how the model controls future extensions to GKS. The model is also used to describe the other 
differences between the input facilities of the Core Sys- tem and GKS. CR Categories and Subject Descriptors: 
1.3.4 [Computer Graphics]: Graphics Utilities --graphics packages; 1.3.6 [Computer Graphics]: Methodology 
and Techniques --device independence; interaction techniques General Terms: Standardization t Address 
for 1982 --Stichting Mathematisch Centrum, Am- (~)1982 ACM 0-89791-076-1/82/007/0033 $00.75 sterdam, 
the Netherlands. 33 I. Introduction "There is a theory which states that if ever anyone finds out exactly 
what the Universe is for and why it is here, it will .instantly disappear and be replaced by something 
even more bizarre and inexplicable." "The Restaurant at the End of the Universe" by Douglas Adams On 
a considerable basis of earlier work[5, 2, 8], Wal- lace enunciated the concept of virtual input devices[10] 
as a means whereby interactive graphics applications could be insulated from the peculiarities of the 
input devices of particular terminals, and thereby become portable. The idea was that the applications 
programmer had available a range of virtual input devices, the only visible aspect of which was the type 
of the value they returned. Thus, if a position were required, a LOCATOR device would be selected, if 
a character string were required, a STRING device would be selected, and so on. From the start, it was 
evident that the pure virtual device concept was inadequate. Virtual devices needed other visible aspects, 
controlling details of the operator interface such as echoing. This logical device concept formed the 
basis for the GSPC Core System[9], and it was also used by the initial versions of the draft interna- 
tional standard GKS[3]. During the recently concluded technical review of GKS by a working group of the 
International Standards Organisation, the input facilities became a bone of contention. Criticisms of 
these ver-sions of GKS concentrated on: -- The precise data types to be returned by the different classes. 
-- The different levels of detail at which different kinds of input behaviour were specified. -- The 
lack of uniformity among the different logical device classes as to the details of their behaviour. -- 
The lack of clear distinction between the concepts of: -- Simulating a logical device using particular 
types of hardware. -- Prompting an operator for input. -- Echoing an operator's actions. -- Acknowledging 
an operator's generation of events. -- The difficulty of relating any of these "output" concepts to 
logical input devices. It became clear that these prgblems were all related to deficiences in the underlying 
model of input, and the dis- cussions were finally resolved by a refined and general- ised concept of 
logical input devices, which is described below. 2. The Model An application program obtains input from 
a number of logical input devices, divided into classes according to the type of data obtained. Typical 
classes are LOCATOR, VALUATOR, PICK, CHOICE, and STRING. 2.1. Application Program Use of Logical Input 
Devices At the highest level, a logical input device is either accessible to the application program, 
or it is inaccessi-ble. Initially, all devices are inaccessible. If a device is to be used for input, 
it must first be acquired, that is, a connection must be made between the identifier known to the application 
program and the external name(s) for the physical device(s). Once this connection is made, the device 
becomes accessible and interactions may take place using the device. The device may also be released, 
breaking the connection between the identifier and the physical devices. The device is then inaccessible 
until it is re-acquired. There are three generally accepted ways in which application programs can obtain 
data from logical input devices; they are known as different modes of operation: -- In SAMPLE mode the 
application program invokes a function to obtain the current logical data value from the device. -- In 
EVENT mode the operator's actions create events, records of the logical data value of the device at a 
specific moment in time; the system preserves these records in one or more event queues, the contents 
of which the application pro- gram can process at its convenience. -- In REQUEST mode the application 
program invokes a function permitting the operator to adjust the logical data value of the device and 
then indicate that the value is satisfactory; the function waits until this has been done and then returns 
the value. Initially, when acquired, a device is in REQUEST mode. When a device is in REQUEST mode, but 
is not currently the subject of a REQUEST function invocation, the device is not available to the operator. 
This corresponds to what has been called a disabled state, although the term seems to be dropping from 
use. A logical input device is taking part in an interaction while the device is in SAMPLE or EVENT mode, 
or while the device is the subject of a REQUEST function invoca- tion. The term enabled has been used 
for this device state. Note that the definitions of these modes refer to two concepts, the current logical 
data value, and specific moments in time at which the operator indicates that the current value is important. 
These concepts are described more fully in the next section. 2.2. Measures and Triggers All logical 
input devices, being abstractions, are con- sidered to be "simulated" by an implementation, even if the 
mapping from physical device(s) to a logical device is quite direct. The simulation of a logical input 
device has two major parts, the measure part and the trigger part. The measure part determines how the 
operator controls the logical data value, and the trigger part determines how the operator indicates 
that the current value is important. The measure part of a logical input device is only active when 
the device is taking part in an interaction. At these times, an independent measure process is (con- 
ceptually) in existence that uses the states and changes of state of various physical input devices to 
control a logical data value (the device's measure) appropriate to the logi- cal device's class. It is 
permitted that a single physical input device simultaneously affect the measures of several logical input 
devices; nevertheless, the measure processes are still considered distinct. The trigger part of a logical 
input device responds to changes in the state of physical input devices either by remaining quiescent,* 
or by firing. When the trigger of a logical input device fires, it sends a message to the meas- ure process 
for the device. The effect of the signal depends upon the mode of the device, and is explained below. 
Several logical input devices may have their trigger part in common. Whenever any of the devices sharing 
a particular trigger part are taking part in an interaction, a single trigger process for that trigger 
is (conceptually) in existence. The firing of the trigger signals all the appropriate measure processes. 
It is a requirement that a single operator action cause the firing of no more than one trigger. Thus, 
in contrast to measure processes (which must be unique to a logical device), it is normal for trigger 
processes to be shared between logical devices. When a measure process receives a signal from the trigger 
process of its logical device, the actions it takes depend upon the device's mode as follows: -- In REQUEST 
mode, the measure process returns its data value to the application, and then dies. -- In SAMPLE mode, 
signals from triggers are always ignored. The application may obtain the data value from the measure 
process whenever it pleases. -- In EVENT mode, the measure process attempts to add an event record, containing 
its identification and data value, to the appropriate input queue. Other measure processes using the 
same trigger process may also be attempting to add records to the same queue at the same time. The resulting 
group of simultaneous event records must either all be added to the queue, or none must be added to the 
queue. If the measure processes fail to add their events to the queue, input queue overflow must be reported 
for that queue. Groups of simultaneous event records are marked as such; when an event record is dequeued, 
the application must be able to discover whether more events in the same group remain in the queue. 
2.3. Attributes of Logical Input Devices When accessible to the application program, a logical input 
device has certain characteristics or attributes that distinguish it, in a general fashion, from other 
logical input devices of the same input class. Depending on the * Its invisible internal state may change. 
 particular graphics system, some attributes will be under application control, while others will have 
been fixed by the implementor. Some of the attributes of logical input devices are: -- Current mode of 
operation. -- How the implementation simulates the logical dev- ice using physical devices. -- How the 
operator is informed that a measure pro- cess has come into existence, and thus that its associated physical 
devices are available for mani- pulation. This is called the prompt. -- How the operator is informed 
of the logical device's measure (its logical data value). This is called the echo. -- How the operator 
is informed of a significant firing of the input device's trigger; this is called acknowledgement. A 
significant trigger firing is one satisfying a REQUEST function invocation, or adding events to the queue, 
-- An initial value, of the type appropriate to the class, for use by the device's measure process when 
it comes into existence. -- A switch turning echo on or off. The attributes may also contain extra information 
used, for example, by particular simulation, prompting and echoing techniques. Note, however, that the 
device's measure is not considered an attribute.   2.4. The Life Cycle of an Input Device It is now 
possible to outline the sequence of opera- tions that corresponds to a specific logical input device 
taking part in a interaction: -- A measure process is created for the device, and its value is set to 
the initial value in the device's state. -- If the trigger process for the device is not in existence, 
it is created. -- The operator is prompted for input, using the selected technique. -- If the echo switch 
in the device's state is on, echo- ing is commenced, using the selected technique. -- As the operator 
manipulates the physical input devices, the trigger may fire, causing the appropri- ate one of the set 
of actions outlined above, depending on the device's mode. -- If a trigger firing is significant, it 
is acknowledged. -- Eventually, either because in REQUEST mode the trigger fires, or because the device 
leaves SAMPLE or EVENT mode, the measure process dies. -- When a trigger process has no measure processes, 
it also dies. 3. Applying the Model to GKS As an example of the use of the model, we take the input facilities 
of GKS[7]. A fundamental concept of GKS is the workstation [4], a collection of input and out- put facilities, 
treated as a unit by the application pro- gram, forming a single logical channel of communication to 
the user. An application may drive many worksta- tions, several of which may support one or more logical 
input devices. However, there is a single event queue shared by all workstations. Each logical input 
device is treated as part of a par- ticular workstation, and is acquired and released as its workstation 
is opened and closed. The attributes of each logical device are part of the Workstation State List for 
the corresponding workstation. The application program name for a logical input device (shown below as 
ID) is a pair, thus: <Workstation identifier, Device identifier> The implementor of the workstation selects 
for each logical device a single technique by which it is simulated using the available hardware. The 
implementor may pro- vide different simulations as different logical devices in the same class, but GKS 
does not permit the application program to change individual simulation techniques. 3.1. GKS Modes All 
GKS logical devices can operate in each of the three modes, REQUEST, SAMPLE, and EVENT. By default, devices 
are in REQUEST mode. Given this and omitting some details, the set of input functions becomes at least: 
-- Operations on the device's attributes: INITIALISE <class>(ID, INITIAL_VALUE) SET <class> MODE(ID, 
MODE) -- Input directly from the device: REQUEST <class>(ID, VALUE) SAMPLE <class>(ID, VALUE) -- Input 
from event queue: AWAIT EVENT(ID, CLASS) -- Examination of most recently awaited event record: GET <class>(VALUE) 
-- Detection of simultaneous events: INQUIRE MORE SIMULTANEOUS EVENTS(FLAG) An interaction with a device 
starts whenever REQUEST <class> , or SET <class> MODE with MODE=EVENT or MODE=SAMPLE, is invoked. At 
this point, the measure process is (re-)created and initialised to the value from the workstation state 
list. 3.2. GKS Device Classes GKS provides five device classes, LOCATOR, VALUA- TOR, CHOICE, PICK, 
and STRING. The Core System, fol- lowing Wallace, considers BUTTON more appropriate as a primitive input 
class, than CHOICE. The Core System provides an additional class, STROKE, which is used below as an example 
of how the model controls possible extensions to GKS. 3.2.1. LOCATOR Both GKS and the Core use a two-stage 
process to transform from the world coordinates used by the appli- cation to the individual device coordinates 
used by each display. Coordinates are first transformed to a single space shared by all devices, called 
normalised device coordinates (NDC) by the window/viewport transforma- tion. Then each workstation has 
a private transforma- tion from NDC to its own device coordinates. Wallace originally suggested that 
LOCATOR devices returned a position in device coordinates. The Core System's LOCATOR devices return a 
position in NDC. The application eventually needs a position in world coordinates, since these are used 
for output. The difficulty in providing world coordinates lies in selecting the window/viewport transformation 
whose inverse is to be applied. Consider an application in which a view of a drawing, a part of a symbol 
library, and some menus share the screen. Each was created using a different window/viewport transformation; 
the operator's actions may require the application to change any of these views, and thus to re-establish 
an appropriate transformation for drawing them. The transformation to be used cannot simply be that active 
for output, which must be set according to output requirements, and changed even while devices are in 
EVENT mode. The Core System's NDC locators avoid this problem, leaving it to the application to select 
an appropriate transformation. GKS takes an alternative approach, providing multi- ple window/viewport 
transformations,* referred to by an index. For output, the application selects one using its index. For 
input, the application arranges the transfor- mations in a priority order. When a physical locator returns 
a coordinate, the workstation transforms it back to NDC, and then transforms to world coordinates by: 
-- Scanning the list of transformations in decreasing priority order, until the NDC position lies inside 
the viewport of a transformation. -- Using the inverse of this transformation to provide a world coordinate 
value. -- Returning as the measure both the world coordi- nate value and the index of the selected transfor- 
mation. In this way, the locator itself selects an appropriate transformation. In the example above, 
if the locator's device coordinate position lies within the part of the screen showing the drawing, the 
drawing's window/viewport transformation will be used. If it is in the part showing the symbol library, 
the library's transformation will be used. The application knows which was used, because the index of 
the transformation used is part of the measure. In general, there will be enough transformations to assign 
one to each part of the screen in use, so that they will only need to be changed infrequently. In any 
case, there is a default transforma- * Termed normalisation transformations, since they transform to 
normalised device coordinates. tion that cannot be changed, in effect returning NDC if no other transformation 
can be found. 3.2.2. VALUATOR GKS provides a classical VALUATOR class, whose measures are real values 
in ranges specified on a per-device basis by the application. 3.2.3. CHOICE GKS provides a CHOICE class, 
whose measures are either integers up to a device-specific limit, or an indica- tion of "no choice". 
No choice might, for example, be a state in which no buttons on a button box were depressed. The class 
is intended to provide a "menu" capability, and has potentially complex application-controlled prompting 
techniques, including the display of a menu consisting of strings or the primitives in a seg-ment.  
  3.2.4. PICK GKS provides a PICK class, whose measures are either segment name and pick identifier 
pairs, or an indication of "no pick". No pick might, for example, be a state in which the light pen was 
not pointing at any detectable segment. 3.2.5. STRING The measures of GKS STRING class devices are (pos- 
sibly null) strings of characters. The operator is presented with the initial string, and a cursor at 
an application-specified position within it. Replacement of characters starts at the cursor position, 
and may extend the string up to an application-specified maximum length. 3.3. A Possible STROKE Device 
Class GKS does not provide a STROKE device class. How-ever, using the model it would be easy to design 
one. The design would proceed in three stages. First, omitting some details, the functions required are: 
INITIALISE STROKE(ID, INITIAL VALUE) SET STROKE MODE(ID, MODE) REQUEST STROKE(ID, VALUE) SAMPLE STROKE(ID, 
VALUE) GET STROKE(ID, VALUE) Secondly, the data type appropriate to the class is determined. The measure 
of a STROKE device is a (possi- bly null) string of positions in world coordinates, and a normalisation 
transformation number. The coordinates of the returned polyline are re-transformed by the inverse of 
the window/viewport transformation of highest prior- ity in whose viewport they all lie; the index of 
this transformation is part of the value. Because their measures are both values resulting from a sequence 
of operator actions, the STROKE class behaves analogously to the STRING class, in that it takes an initial 
stroke and a cursor position within it. Replacement of strokes starts at the cursor position and may 
extend the polyline up to an application-specified maximum. Details such as whether the individual positions 
of the stroke are triggered by distance, time, or operator action, and how the operator "rubs-out" erroneous 
positions, are left to the workstation implementor. 3.4. Prompting and Echoing The details omitted from 
the descriptions above con- cern prompting and echoing. For each class, GKS defines several prompt/echo 
techniques. At least one very simple technique must be supported for every dev- ice. When a device is 
initialised, a particular prompt/echo technique is requested, and appropriate parametric information 
is supplied. These attributes include an echo area, which the technique may use to display the prompt 
or echo, and a data record containing device- and implementation-specific information such as an array 
of strings for a CHOICE device using text menus. 3.5. Differences Between GKS and the Core Although they 
are conceptually similar, there are some detail differences between the input facilities of GKS and the 
Core. The model is equally useful for describing the Core, though in this section we only describe the 
differences. Because the Core has no workstation concept, it has explicit functions for acquiring and 
releasing logical input devices, for example INITIALIZE_DEVICE. Note that this does not provide an initial 
value for the device; the Core has individual functions for setting particular attributes of logical 
input classes, and the only classes for which an initial value setting function is provided are LOCATOR 
and VALUATOR. The Core recognises only SAMPLE and EVENT modes. Each device class operates only in one 
mode; LOCATOR and VALUATOR in SAMPLE mode, and PICK, KEYBOARD, BUTTON, and STROKE in EVENT mode. The 
measure and trigger processes are created by an ENABLE__DEVICE invocation, and destroyed by a DISABLE_DEVICE 
invoca- tion. Facilities are provided to associate one or more LOCATOR or VALUATOR devices with a device 
in an "event" class. An association between a device in a "sample" class and a device in an "event" class 
in effect creates a new logical device, operating in event mode, which has the measure of the "sample" 
device* and the trigger of the "event" device. An group of associated devices share the same trigger 
(that of the "event" device) and so generate a group of simultaneous events. Unlike GKS, the Core combines 
all the reports in a group of simultaneous events into a sin- gle complex report; the firing of a single 
trigger can place at most one report in the queue. In GKS, the creation of groups of associated devices 
is the preserve of the workstation implementor. Facilities to provide application control over associations 
would require the addition of two new functions (and the corresponding inquiries) to GKS: * It is thus 
in the class of the "sample" device. ASSOCIATE(ID1, ID2) DISSOCIATE(ID2) The effect would be to disconnect 
the measure of device ID2 from its trigger, and to connect it to the trigger of device IDI until it was 
dissociated. When the trigger of device ID1 fired, the resulting group of simultaneous events would contain 
an event from device ID2. 4. Implications of the Model Because there was otherwise no exit from a REQUEST 
except by supplying a valid value, GKS provides a "break" facility. This permits an operator, when REQUESTed 
for a value, to refuse to supply one. It pro- vides, among other capabilities, an easy way for the operator 
to indicate "end-of-input". Another implication of the model may reflect on the current discussions of 
graphics virtual device interfaces. The model now insists that the essential preliminary to any input 
operation is an output operation providing an initial value of the appropriate type. This strongly encourages 
a symmetric approach to incorporating input into a virtual device definition, insisting that the responses 
from input devices are similar to output com-mands. This symmetry is enhanced by the observation that 
the prompt/echo information now behaves in effect as the attributes of an input primitive, modifying 
its visible appearance in a workstation-dependent fashion. 5. Conclusion Since it was proposed, the concept 
of virtual input devices has been extremely useful, but has also attracted severe criticism[6, 1] from, 
among others, one of us. Although this refined model answers some of these attacks, the fundamental problems 
brought to light by the critics are still present. Nevertheless, it is clear that the time is not yet 
ripe for a standard, whose r61e is to codify existing good practice, to incorporate a more radi- cal 
approach to input. We expect that the generalised and rejuvenated con-cept of logical input devices will 
remain the basis for the device-independence of interactive graphics applications for some considerable 
time. ,However, now that they have a more robust and detailed target, we would wel- come renewed attention 
from the critics. 6. Acknowledgements Our grateful thanks are tendered to all those who took part in 
the discussions on input in the Draft Stan- dards Subgroup of ISO TC97/SC5/WG2, and in the national discussions 
supporting them. Particular thanks are due to Paul ten Hagen of the Stichting Mathematisch Centrum, Amsterdam, 
to Ray Spiers, to Dick Puk, and to Marceli Wein of the National Research Council of Canada, who was still 
willing to talk to us, even after being thrown in at the deep end. David Rosenthal was supported in this 
work by Sci- ence and Engineering Research Council grants N2B 1R 0371 and GR/A80341.   References 
1. E. Anson, "The Semantics of Graphical Input," Com-puter Graphics 13(2), pp. 113-120 (August 1979). 
 2. I. W. Cotton, Network Graphic Attention Handlin&#38; Proc. Online '72 Conf., Brunel University, 
Uxbridge, England (September 1972). 3. DIN, "Graphical Kernel System (GKS) --Functional Description," 
(Version 6.6) (May 1981). 4. J. Encarnaq~o, G. Enderle, and others, "The Worksta- tion Concept of GKS 
and the Resulting Conceptual Differences to the GSPC Core System," Computer Graphics 14(3), pp.226-230 
(July 1980). 5. J. D. Foley and V. L. Wallace, "The Art of Natural Man-Machine Communication," Proc. 
IEEE 62(4), pp.462-471 (April 1974). 6. R. A. Guedj and others (eds.), IFIP Workshop on Methodology 
of Interaction, (publishers North-Holland), Seillac, France (May 1979). 7. ISO, "Graphical Kernel System 
(GKS) --Functional Description," DP 7942 (January 1982). 8. W. M. Newman, "A System for Interactive 
Graphical Programming," AFIPS Conference Proceedings (SJCC) 32, pp.47-54 (1968). 9. SIGGRAPH-ACM (GSPC), 
"Status Report of the Graphics Standards Planning Committee," Computer Graphics 13(3) (August 1979). 
 10. V. L. Wallace, "The Semantics of Graphics Input Devices," Computer Graphics 10(1) (Spring 1976). 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801258</article_id>
		<sort_key>39</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[A Metafile for efficient sequential and random display of graphics]]></title>
		<page_from>39</page_from>
		<page_to>43</page_to>
		<doi_number>10.1145/800064.801258</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801258</url>
		<abstract>
			<par><![CDATA[<p>Graphics metafiles have been in use at the Los Alamos National Laboratory since early 1977. The first metafile format was defined in 1976 and has been updated several times to allow efficient graphics support in the Los Alamos computing environment. History and current applications of the Common Graphics System (CGS) Metafile are given. Design objectives, details of the format, and random access extensions incorporated in the Metafile are described.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Device-independent]]></kw>
			<kw><![CDATA[Metafile]]></kw>
			<kw><![CDATA[Random access]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P334015</person_id>
				<author_profile_id><![CDATA[81332522908]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Theodore]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Reed]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Los Alamos National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807407</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Keller, R. G., Reed, T. N., and Solem, A. V. An implementation of the ACM/SIGGRAPH proposed graphics standard in a multisystem environment. Computer Graphics, 12,3 (August 1978), 308-312.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wright, T. A schizophrenic system plot package. Computer Graphics, 9, 1 (Spring 1975), 252-255.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Groot, D. GPGS 16 bits device independent picture code. University of Nijmegen, Nijmegen, The Netherlands (October 1975).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Status report of the Graphic Standards Planning Committee of ACM/SIGGRAPH. Computer Graphics, 11, 3 (Fall 1977).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Status report of the Graphic Standards Planning Committee of ACM/SIGGRAPH. Computer Graphics, 13, 3 (August 1979), Part IV.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1115921</ref_obj_id>
				<ref_obj_pid>1115918</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Warner, J. R. Device independent intermediate display files. Computer Graphics, 13, 1 (March 1979), 78-109.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[X3H33 SD-3 proposal for an ANSI X3 standards project for the computer graphics virtual device metafile. CBEMA, 311 First Street N.W., Washington, DC (November 1980).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Reed, T. N. The common graphics system. Los Alamos National Laboratory report LALP 81-67 (October 1981).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Reed, T. N. Experiences in the design and support of a graphics device driver interface. Eurographics '81 Proceedings of the International Conference and Exhibition (September 1981), 281-289.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A METAFILE FOR EFFICIENT SEQUENTIAL AND RANDOM DISPLAY OF GRAPHICS Theodore N. Reed Los Alamos National 
Laboratory ABSTRACT Graphics metafiles have been in use at the Los Alamos National Laboratory since 
early 1977. The first metafile format was defined in 1976 and has been updated several times to allow 
efficient graphics support in the Los Alamos computing environment. History and current applications 
of the Common Graphics System (CGS) Metafile are given. Design objectives, details of the format, and 
random access extensions incorporated in the Metafile are described. CR Cate$ories and Subject Descriptors: 
H.3.2 [Computer Graphics]: Graphics Systems - modular software, metafile generation; H.3.4 [Computer 
Graphics]: Graphics Utilities - metafile processing; H.3.6 [Computer Graphics]: Methodology and Techniques 
-random access of metafile frames. Additional Key Words: metafile, random access, device-independent 
 INTRODUCTION A graphics metafile is a device-independent representation of a picture intended for subsequent 
display on a graphics output device. Two concepts are contained in this definition. First, the metafile 
is a device-independent representation of a picture that can be displayed on a wide variety of graphics 
devices. Second, the metafile is intended for subsequent display; thus, it is passive in nature. This 
paper discusses the metafile used at Los Alamos National Laboratory for both sequential and random display 
of graphics. HISTORY In 1976, a graphics metafile was defined for the Los Alamos National Laboratory's 
Common Graphics System (CGS) [I]. It had 14 bits of addressability based on 8-bit bytes. This format 
was defined after analyzing a variety of plots and looking at existing graphics metafiles [2,3]. In 1978, 
the Metafile was modified to include 15 bits of addressability based on a 16-bit word and was restructured 
to include the ACM/SIGGRAPH Graphic Standards Planning Committee CORE capabilities [4]. This revision 
was proposed to an interlaboratory task group consisting of Los Alamos National Laboratory, Sandia National 
Laboratory in Albuquerque, and the Air Force Weapons Laboratory in Albuquerque. After some modification, 
it was adopted and called the Basic Graphics Package (BGP) metafile. The current CGS Metafile is the 
BGP metafile with extensions to allow efficient random access. In 1979, the ACM/SIGGRAPH Graphic Standards 
Planning Committee (GSPC) published its status report, which included a proposal for a graphics metafile 
[5]. This metafile was based on a study of four existing metafiles [6]. One of these was the BGP metafile 
from which much of the GSPC metafile command structure and format was derived. In 1980, the ANSI X3H3 
Computer Graphics Technical Committee formed the X3H33 Virtual Device Interface Task Group to standardize 
a computer graphics metafile [7]. APPLICATION The computing environment at the Los Alamos National 
Laboratory includes an extensive network of supercomputers, on-line storage facilities, remote computers, 
and user terminals. Figure 1 illustrates the graphics software components available in this network. 
The Common Graphics System provides the subroutine library containing device-independent graphics primitives, 
primitive attributes, and device control functions. The higher-level graphics utilities, plotting packages, 
and user application programs are built upon this foundation [8]. Device drivers interface to this device-independent 
software and translate the device-independent graphics data into the form required by the physical graphics 
devices [9]. A special device driver records this device-independent information in the Metafile (a disk 
file) for subsequent processing. Metafile postprocessors (translators) use the appropriate CGS device 
driver to display the Metafile on the user's local interactive or passive graphics device. The more 
expensive film recorders and paper plotters are in a central location and are connected to a computer 
remote to the user. Device drivers on this computer translate the Metafile into the form required by 
the various plotting devices. Users access Metafile postprocessors (transporters) to send the Metafile 
to this computer for processing. ~V' CE- I NDEPENDENT~ REMOTE PASSFVE COS INTERACTIVE LOCAL PASSIVE 
GRAPHICS DEVICES METAFILE(s) GRAPHICS DEVICES GRAPHICS DEVICES I Figure i. Graphics software components 
at the Los Alamos National Laboratory. OBJECTIVES The key design objectives for the CGS Metafile included 
designing a metafile that  maintains the smallest file size possible while supporting necessary addressability 
requirements,  is efficient to process,  is extensible,  can be moved across computers of different 
word lengths without conversion,  can be moved across different operating systems without conversion, 
and  allows a particular frame to be displayed without sequentially processing preceding frames.  
 These design objectives have been met. METAFILE FORMAT The Metafile design provides a compact format 
that can be processed efficiently. This format is simple yet easily extensible to allow later enhancement. 
The format is based on multiples of 16-bit words to facilitate processing on most mini- and microcomputers, 
as well as many of the larger computers. This format also facilitates the processing of ASCII characters, 
which are packed two per word. Coordinate-Positioning Command Format In many graphics applications, 
most graphics data consists of coordinate-positioning information. Each Metafile coordinate is contained 
within one 16-bit word to preserve as much addressability as possible and still keep the total Metafile 
size reasonably small. Each coordinate-positioning command consists of an x, y, and optional z coordinate 
(Figure 2). Analysis of graphics output at Los Alamos indicates that no significant size reduction 
results when either the x or the y coordinate is eliminated if unchanged from the previous position. 
Because of the extra control bits necessary to identify whether a coordinate consists of x or y or 
both instead of a "simple" x, y coordinate pair, either the total size must increase or the addressability 
must decrease. The coordinate-positioning commands maintain 15 bits of addressability. This is sufficient 
for current and planned graphics devices. These coordinates are absolute normalized device coordinates, 
which have been transformed and clipped. After completion of a coordinate- positioning command, the current 
position is at the specified x, y coordinate pair. Either two-or three-dimensional coordinates can be 
output, allowing for support of three-dimensional devices. For three-dimensional devices, a mode is set 
and the coordinate-positioning commands that follow consist of x, y, and z coordinates. The coordinate-positioning 
commands are distinguished from the noncoordinate-positioning commands by the high-order bit of the x 
coordinate (Figure 2). This bit is set to zero for the coordinate-positioning commands and is set to 
one for the noncoordinate-positioning commands (Figure 3). The high-order bit of the y coordinate determines 
whether a move or draw is to take place. A draw can be either a line from the current position to the 
specified coordinates or a marker at the specified coordinates, depending on the mode. The high-order 
bit of a z coordinate is always zero. Noncoordinate-Positionin$ Command Format The noncoordinate-positioning 
command format contains the remainder of the Metafile commands (Figure 3). Although this format provides 
most of the possible Metafile capabilities, it comprises a small portion of the total graphics data 
in the Metafile since most of the data consists of coordinate-positioning commands. Because of this, 
 attention has been given to providing a simple, uniform command format that can be processed efficiently. 
Each command contains a count of the words associated with it. Searching the file for particular commands 
is simplified and unsupported commands are easily skipped because the number of words associated with 
a particular command is immediately available. The 7-bit op-code is divided into a 3-bit class and a 
4-bit subclass. This allows the various op- codes to be defined in an organized fashion with 8 major 
classes, each consisting of up to 16 subclasses. By dividing the op-code in this fashion, jump tables 
can provide efficient processing of the class and subclass operations. Both the coordinate and noncoordinate 
commands are written on disk as a "bit-stream"; that is, no explicit record or file structure exists. 
This allows the Metafile to be moved between computers of different word lengths or different operating 
systems without conversion. All that is required to process the Metafile is "primitive" I/O, allowing 
transfer of a specified amount of data to or from a particular location on disk. To avoid word-boundary 
conflicts, the end of each Metafile is padded with a "no-operation" command so that the file is a multiple 
of 60 16-bit words. The file is thus forced to align on word boundaries for all of the Los Alamos computers 
(64-, 60-, 32-, and 16-bit words). A multiple of IgO 16-bit words would be necessary to include computers 
with word lengths of 36-or 18-bit words. io I X-COORDINATE (15 bits) i i i i i i i i ! ! i i i i i 
i ! I Word 1 i Mi Y-COORDINATE (15 bits) ! IDI ............ Word 2 iOi Z-COORDINATE(15 bits) i i , !!!i!iiiiiii, 
i Word 3 (3D mode only) Figure 2. Coordinate-positioning command format. I! CLASS SUBCLASS iOP WORD 
COUNT (0-255) :: i :: :: :: :: i i ! ! ! ! i i i i i ; ': OP CODE i i Word 1 OP WORD I i i i i i i 
i i i i i ! ! ! ! ! ! ! Word 2 OP WORD N ! i i :; :; i :; i i i i i ! ! ! ! i !i Word n+l Figure 3. 
Noncoordinate-positioning command format. RANDOM ACCESS EXTENSIONS The Metafile format described thus 
far allows for efficient sequential processing of the Metafile. The following additions allow rapid random 
access with minimal impact on the size of the Metafile or the efficiency of the sequential processing. 
 A function was added to the escape command, and two existing commands (end-of-data and new-frame) were 
modified. The index-block escape command contains the 16-bit-word disk addresses of the preceding 28 
frames. Each address or pointer to the preceding frame consists of 32 bits. The index- block escape command 
consists of 60 words (Figure 4). Word 1 indicates the escape command and that 59 words follow. Word 2 
indicates the index-block escape command. Words 3 and 4 contain the 16-bit- word disk address of the 
preceding index block. Words 5 through 60 contain the 16-bit-word disk address of the preceding 28 
frames. An address or pointer of zero indicates the end of this linked list of index-block escape commands. 
 i ESCAPE OP J 59 INDEX BLOCK IDENTIFIER POINTER TO PRECEDING INDEX BLOCK POINTER TO PRECEDING FRAME 
(N) POINTER TO PRECEDING FRAME (N+28) Figure 4. Index block escape command format. The new-frame and 
end-of-data commands each consists of seven words (Figure 5). Word 1 gives the command and word count. 
Words 2-4 are a synchronization pattern that forces the first 64 bits of each command to a bit pattern 
unique to the end-of-data and new-frame commands. Words 5 and 6 are the 16-bit-word disk address pointer 
to the previous index-block escape command. Word 7 is the current frame number for the new-frame command 
or the total number of frames for the end-of-data command. NEW FRAME OPII 6 SYNC PATTERN POINTER TO 
PRECEDING INDEX BLOCK FRAME NUMBER END OF DATA oPJ 6 i SYNC PATTERN POINTER TO LAST INDEX BLOCK TOTAL 
FRAME COUNT Figure 5. New frame and end- of-data command format. File Generation As the file is generated, 
the disk address of each frame is saved. This disk address is a 16- bit-word address independent of 
the word length of the computer that is generating the Metafile. When 28 frames have been generated, 
the index block is written to disk and its disk address is saved (Figure 6). Each new-frame command 
written to disk  contains the disk address of the previous index block. When 28 more frames have been 
generated, the index block with the disk address of the previous index block is written to disk. When 
the Metafile is complete, the last partial index block is written to disk and is followed by an end-of-data 
command containing the disk address of the last index block. This linked list structure is sequentially 
generated and is written to disk using only sequential writes. J~ ESCAPE INDEX BLOCK BLOCK POINTER FRAME 
POINTER FRAME POINTER NEW FRAME SYNC PATTERN BLOCK POINTER FRAME NUMBER   f NEW FRAME SYNC PATTERN 
BLOCK POINTER INDEX BLOCK BLOCK POINTER FRAME POINTER  ii I  FRAME POINTER .~ END OF DATA SYNC PATTERN 
BLOCK POINTER TOTAL FRAMES Figure 6. CGS Metafile random access linked list structure. (Pointers to index 
block are on the left; pointers to frames are on the right.) Random Access of a Frame  When the file 
is sequentially processed, the index-block escape command is ignored. However, when the file is randomly 
processed, a table of frame addresses can be quickly constructed. The end-of-data command is located 
at the end of the file. It is read, and the total frame count is used to give the frame address table 
entry for the last frame. The disk address of the last index block is also obtained from the end-of-data 
command. This index block is read, and the disk address of each frame is stored in the frame address 
table. The address of the previous index block is obtained; it is read, and disk addresses of those frames 
are stored in the frame address table. This process continues until a complete table of frame addresses 
has been constructed. Each frame can now be immediately accessed by disk address. The frame address table 
can be constructed at a fraction of the cost of sequential reading of each frame because only one disk 
access is made for every 28 frames. To further increase efficiency, the index-block escape command 
could be increased in size in multiples of 60 16-bit words to allow additional frame addresses to be 
stored. To simplify random accessing, the end of each frame is padded to a multiple of 60 16-bit words 
so that each new-frame command and index-block escape command start at the beginning of a word (independent 
of computer word length). This ensures that the addresses will be at the beginning of a word on disk 
when the 16-bit-word disk addresses are converted to an actual disk address for a machine of a particular 
word length. To randomly access a frame and obtain a correct picture, it is necessary that the current 
attributes be associated with each frame. All of the current attributes are written to disk after each 
new frame command. These can be ignored when processing the file sequentially, but are necessary to establish 
the environment when randomly processing frames. The current Metafile does not contain segments or color/font 
table definitions. A method to support both sequential and random access of a metafile containing segments 
and color/font table definitions is discussed under Future Extensions. This method of providing random 
access to Metafile frames has been very successful. The most heavily used Metafile translators at Los 
Alamos are those that provide random access to frames in the Metafile. Users are able to preview and 
edit particular frames of a Metafile at their terminal before sending it to a graphics hardcopy device. 
Facilities have also been provided to copy frames from one Metafile to another and to merge selected 
frames from several Metafiles into one. These operations would not be feasible without rapid random access 
to frames of a Metafile. ERROR RECOVERY When a job aborts, the Metafile may not have been properly 
terminated. When this happens, the last index-block escape command and end-of-data command have not been 
written to disk. The frame address table is constructed by searching the file backward from the end, 
looking for a new-frame command. Once it is found, the pointer to the previous index block can be obtained, 
and the frame address table can be constructed. The file can be searched forward from this index block, 
looking for new-frame commands to complete the last few entries in the frame address table. FUTURE EXTENSIONS 
 Segmentation When segmentation is supported in the CGS Metafile, the following scheme could be adopted 
to allow efficient sequential and random processing while maintaining a small file size. A new escape 
function would be defined, called the frame environment block. This would contain all current attributes 
and disk addresses (in 16-bit words) of all segments. This escape command would be ignored when processing 
the file sequentially, but would he  used to provide a correct picture when processing REFERENCES the 
file randomly. Little additional file space would be required in the "typical" case where there is a 
small number of large segments. This same scheme could be used for color definition tables and font definition 
tables if they are added to the Metafile. Random Processing by Key Identifier An extension that would 
facilitate random processing is the addition of a user-specified identifier to the CGS new-frame subroutine 
call. This identifier would be written as part of the CGS Metafile new-frame command. When the frame 
address table is constructed, this identifier would be associated with the frame number and disk address. 
This identifier would then he used as a key to randomly access a particular frame, thus allowing the 
user to specify a logical identifier rather than a frame number. SUMMARY The COS Metafile is efficient 
in file size and processing time for both sequential and random display. Random access efficiency was 
accomplished by incorporating an escape function containing frame pointers in a linked list. Portability 
of the Metafile across different computers and operating systems without conversion was achieved by avoiding 
an explicit record structure. These techniques are extensible and will be used to support future extensions 
to the CGS Metafile. ACKNOWLEDGMENTS I would like to thank Raymond Elliott and his computer graphics 
group for their support of this work. I would also like to thank Richard Kellner for the many valuable 
discussions he and I had concerning efficiency in the generation and random accessing of frames in the 
CGS metafile. I. Keller, R. G., Reed, T. N., and Solem, A. V. An implementation of the ACM/SIGGRAPH 
proposed graphics standard in a multisystem environment. Computer Graphics, 12,3 (August 1978), 308-312. 
 2. Wright, T. A schizophrenic system plot package. Computer Graphics, 9,1 (Spring 1975), 252-255.  
3. Groot, D. GPGS 16 bits device independent picture code. University of Nijmegen, Nijmegen, The Netherlands 
(October 1975).  4. Status report of the Graphic Standards Planning Committee of ACM/SIGGRAPH. Computer 
Graphics~ 11 1 3 (Fall 1977).  5. Status report of the Graphic Standards Planning Committee of ACM/SIGGRAPH. 
Computer Graphics~ 13, 3 (August 1979), Part IV.  6. Warner, J. R. Device independent intermediate display 
files. Computer Graphics, 13, 1 (March 1979), 78-109.  7. X3H33 SD-3 proposal for an ANSI X3 standards 
project for the computer graphics virtual device metafile. CBEMA, 311 First Street N.W., Washington, 
DC (November 1980).  8. Reed, T. N. The common graphics system. Los Alamos National Laboratory report 
LALP 81-67 (October 1981).  9. Reed, T. N. Experiences in the design and support of a graphics device 
driver interface. Enrographics '81 Proceedings of the International Conference and Exhibition (September 
1981), 281-289.  43  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801259</article_id>
		<sort_key>45</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Graphics standardization]]></title>
		<page_from>45</page_from>
		<page_to>46</page_to>
		<doi_number>10.1145/800064.801259</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801259</url>
		<abstract>
			<par><![CDATA[<p>The big challenge for a graphics standardization project is to realise a truly device independent graphics package which nevertheless allows for unrestricted use of facilities offered by a particular device.</p> <p>In 1980, it was reported at a SIGGRAPH session that the Graphics Working Group of ISO (ISO/TC97/SC5/WG2) had decided to define a 2D core-like standard based on the DIN GKS proposal. At the same session, a new concept was introduced called a workstation, which was to be a basis for achieving device independence in GKS.</p> <p>In the two years of reviewing which has since taken place, the workstation concept has been further developed and as a result has become much more important than it was, say, in the 1979 version of GKS. The workstation supports a two stage abstraction mechanism for input and output primitives, especially with respect to their attributes (including transformations). In this way, every aspect which can be dealt with in a similar manner for each device (e.g. purely geometric information) is moved up to the higher abstraction level and called workstation independent. Aspects with a limited device independence, which apply only to certain classes of devices (e.g. colour) are in the lower level of abstraction and are called workstation dependent.</p> <p>Following this two level abstraction, a two level mapping is defined in GKS, leading to two classes of functions: those addressing all active workstations at the same time (w.s. independent) and those addressing a particular work station (identified by a parameter). Workstations are now allowed to differ in the value ranges for the attributes they support. The two-level mapping provides setting of an abstract primitive attribute which is next either applied on the same abstract level, or subsequently mapped on the most appropriate value available for each individual workstation. The second mapping can also be controlled by workstation functions. In this way, two workstations can, for example, provide a different presentation of the same abstract picture simply by selecting different representations for various attributes (e.g. quick and dirty versus high quality). Moreover, they can also provide a &#8220;best likeness&#8221; picture by selecting an appropriate combination of attributes with the same visual effect (for example, each workstation may provide different means for generating the same text-font).</p> <p>It will be illustrated why this uniform scheme for a two level attribute mechanism was to be developed beyong current practice in order to get a satisfactory solution for device independence. A further basic facility derived from the workstation concept is the ability to configure a workstation out of abstract devices (i.e. 0 or 1 output plus 0 or more input devices). Most input devices depend for their user feedback, on the output devices available. All inputs that need the same output device are now integrated into one work station. Again, the two level structure is very helpful. All configuration aspects of providing prompts and echoes are local to the workstation. They can be controlled via abstract functions which are mapped on physical realisation at the workstation. Echoes and prompts as well as other aspects for input are treated like workstation dependent attributes. Here also, the ISO reviewing process has brought about new concepts and methods which will be presented and illustrated in this session.</p> <p>The European members of ISO especially, are very satisfied with these achievements and are developing further on the basis of GKS.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.2.9</cat_node>
				<descriptor>Software process models (e.g., CMM, ISO, PSP)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10011007.10011074.10011081</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management->Software development process management</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332825</person_id>
				<author_profile_id><![CDATA[81100046290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[J. W. ten]]></middle_name>
				<last_name><![CDATA[Hagen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Mathematical Centre, Amsterdam]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 16, Number 3 July 1982 Graphics Standardization Activities of ISO/TC97/SC5/WG2 
and Their Impact in Europe Paul J. W. ten Hagen Mathematical Centre Amsterdam Abstract The big challenge 
for a graphics standardization project is to realise a truly device independent graphics package which 
nevertheless allows for unrestricted use of facilities offered by a particular dev- ice. In 1980, it 
was reported at a SIGGRAPH session that the Graphics Working Group of ISO (ISO/TC97/SC5/WG2) had decided 
to define a 2D core-like standard based on the DIN GKS proposal. At the same session, a new concept was 
introduced called a workstation, which was to be a basis for achieving device independence in GKS. In 
the two years of reviewing which has since taken place, the workstation concept has been further developed 
and as a result has become much more important than it was, say, in the 1979 version of GKS. The workstation 
supports a two stage abstraction mechanism for input and output primitives, especially with respect to 
their attri- butes (including transformations). In this way, every aspect which can be dealt with in 
a similar manner for each device (e.g. purely geometric information) is moved up to the higher abstraction 
level and called workstation independent. Aspects with a limited device independence, which apply only 
to certain classes of devices (e.g. colour) are in the lower level of abstraction and are called worksta- 
tion dependent. Following this two level abstraction, a two level mapping is defined in GKS, leading 
to two classes of functions: those addressing all active workstations at the same time (w.s. independent) 
and those addressing a particular work station (identified by a parameter). Workstations are now allowed 
to differ in the value ranges for the attributes they support. The two-level mapping provides setting 
of an abstract primitive attribute which is next either applied on the same abstract level, or subsequently 
mapped on the most appropriate value available for each individual workstation. The second mapping can 
also be controlled by workstation functions. In this way, two worksta- tions can, for example, provide 
a different presentation of the same abstract picture simply by selecting different representations for 
various attributes (e.g. quick and dirty versus high quality). More- over, they can also provide a "best 
likeness" picture by selecting an appropriate combination of attributes with the same visual effect (for 
example, each workstation may provide different means for generating the same text-font). It will be 
illustrated why this uniform scheme for a two level attribute mechanism was to be developed beyong current 
practice in order to get a satisfactory solution for device independence. A further basic facility derived 
from the workstation concept is the ability to configure a workstation out of abstract devices (i.e. 
0 or 1 output plus 0 or more input devices). Most input devices depend for * This presentation was selected 
by the Eurographics Association. 45 their user feedback, on the output devices available. All inputs 
that need the same output device are now integrated into one work station. Again, the two level structure 
is very helpful. All configuration aspects of providing prompts and echoes are local to the workstation. 
They can be controlled via abstract functions which are mapped on phy- sical realisation at the workstation. 
Echoes and prompts as well as other aspects for input are treated like workstation dependent attri- 
butes. Here also, the ISO reviewing process has brought about new concepts and methods which will be 
presented and illustrated in this session. The European members of ISO especially, are very satisfied 
with these achievements and are developing further on the basis of GKS. 46 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801260</article_id>
		<sort_key>47</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Graphics standards for three-dimensional modelling]]></title>
		<page_from>47</page_from>
		<doi_number>10.1145/800064.801260</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801260</url>
		<abstract>
			<par><![CDATA[<p>Much effort has been spent in defining a standard call interface for a library of routines with display primitives embedded in a three-dimensional space. This paper argues that this is now an inappropriate target. There is too much diversity in the three-dimensional things we want to draw and in the kinds of picture we want to draw of them for any single standard to be both broadly acceptable and specifically useful.</p> <p>If the decision is made to go for a set of specialized standards for wire frame and for rendered graphics for data presentation, for sculptured surfaces and for object models, it becomes apparent that far more can be gained from a higher level standard, at the data presentation or object modelling level.</p> <p>In the object modelling community, at least, there is indeed work toward such a standard based on the IGES proposals. This has very few concepts in common with the graphics standard effort, and it is not clear at present what contribution the graphics community has to make in this area.</p> <p>The suggestion made here is that there may well be a number of paragraphs or sections common to more than one of the graphics-related and higher level standards. One such is the coordinate transformation (the dreaded 4 &#215; 4 matrix). Its definition and its use both for modelling and viewing concepts are suggested which resolve some of the black art associated with transformation in the past. These could become the conceptual basis for a short simple standard in this area.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Standards</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011017</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Domain specific languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010919.10010177</concept_id>
				<concept_desc>CCS->Computing methodologies->Distributed computing methodologies->Distributed programming languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Standardization</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31094985</person_id>
				<author_profile_id><![CDATA[81100307545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Malcolm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sabin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FEG Limited, Cambridge, ENGLAND]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphics Standards for Three-Dimensional Modelling Malcolm Sabin FEG Limited Cambridge, ENGLAND Abstract 
 Much effort has been spent in defining a standard call interface for a library of routines with display 
primitives embedded in a three-dimensional space. This paper argues that this is now an inap- propriate 
target. There is too much diversity in the three- dimensional things we want to draw and in the kinds 
of picture we want to draw of them for any single standard to be both broadly acceptable and specifically 
useful. If the decision is made to go for a set of specialized standards for wire frame and for rendered 
graphics for data presentation, for sculptured surfaces and for object models, it becomes apparent that 
far more can be gained from a higher level standard, at the data presentation or object modelling level. 
 In the object modelling community, at least, there is indeed work toward such a standard based on the 
IGES proposals. This has very few concepts in common with the graphics standard effort, and it is not 
clear at present what contribution the graphics community has to make in this area. The suggestion made 
here is that there may well be a number of paragraphs or sections common to more than one of the graphics-related 
and higher level standards. One such is the coordinate transformation (the dreaded 4 x 4 matrix). Its 
definition and its use both for model- ling and viewing concepts are suggested which resolve some of 
the black art associated with transformation in the past. These could become the conceptual basis for 
a short simple standard in this area. * This presentation was selected by the Eurographics Association. 
 47 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801261</article_id>
		<sort_key>49</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Professional Workstations (Panel Session)]]></title>
		<page_from>49</page_from>
		<page_to>50</page_to>
		<doi_number>10.1145/800064.801261</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801261</url>
		<abstract>
			<par><![CDATA[<p>Andries van Dam</p> <p>The panel will examine the evolution of the professional workstation from a time shared terminal to powerful graphics-based personal computer connected to a resource-sharing local network. The panelists will speculate on the future evolution of both the hardware/software architecture and end user environment.</p> <p>Workstation development at Stanford</p> <p>James H. Clark</p> <p>Workstation development at Stanford is closely linked with graphics, distributed systems and networking. Two distinct systems have evolved over the last 3 years: the SUN system and the IRIS system. The SUN is a low-cost system based upon an efficient MC68000 processor/memory design and a relatively low-performance, high-resolution bit-map display. The IRIS (Integrated Raster Imaging System) is based upon the same MC68000 design, but the graphics part of the system is a modest- to low-cost, high-performance, high-resolution, color or black and white system that uses the Geometry Engine and several other custom IC parts. Both SUN and IRIS interface to the Stanford Ethernet network, and both are being used for distributed systems research, VLSI design stations, and graphics research. In addition, IRIS will probably be used for mechanical CAD research by the mechanical engineering department and in situations where high-performance graphics is important.</p> <p>Improvement Goals for Workstation Facilities</p> <p>Robert M. Dunn</p> <p>Three major areas of improvement are needed: interaction techniques that are simpler, provide faster reaction, are useful for higher-level inputs, and have user-style-oriented alternatives. The second area is for image rendering based on local capabilities, in varying degrees of image quality as a function of desired &#8220;grade of service&#8221;. The third area is to provide support to incremental model construction and approximate design evaluation. There must be evaluation techniques that can work on partial models and give one approximate results. Implications of these criteria for workstation architecture will be discussed.</p> <p>The application of network workstations to large-scale engineering</p> <p>Harvey Kriloff</p> <p>The development of computerized analysis procedures during the last twenty years has been largely oriented toward providing increased analytic function. This has meant that considerations of user access to specific capabilities or ease of use of these mammoth programs are only now becoming user concerns. The emerging new technologies for display, input and their interaction, when applied to the professional workstation, will be playing an increasingly important role in satisfying these concerns. A professional workstation can be used both to improve the efficiency by which data is collected for an existing analysis program and to assist the user in the preparation of data for a formal presentation or report. This performance improvement can be accomplished through the development of user-adapted &#8220;macro procedures&#8221; for data entry, the execution of processes to check program input data for accuracy and consistency, and workstation assistance in the training of both new and existing users. At the output stage of analysis, the workstation can be used to select and reformat information prepared by the analysis program, explore the interrelationship of results from several different analyses, and derive the data for a succeeding analysis in an iterative design mode.</p> <p>At Boeing Computer Services, we have been exploring the benefits realized by the development of such a workstation when applied to the field of structural engineering. An interaction-rich workstation with a human-engineered executive that controls and integrates the user interface, connected to our national network of IBM, CDC and CRAY computers with a broad variety of engineering software, supplies the engineer with a full range of tools for analysis, data preparation, report writing and data relationship exploration that are only beginning to be appreciated. In the next few years it is expected that this workstation (distributed) methodology for doing engineering and other quantitative professional functions will radically change the way these analytical processes are performed.</p> <p>Personal Workstations in a Local Network</p> <p>David Nelson</p> <p>For many applications, personal workstations provide a superior form of computing compared to timesharing. The requirements of workstations, however, go well beyond the facilities provided by the local computer; they must include the equivalent advantages of timesharing such as user-to-user communications, shared programs and data files, shared peripherals, etc. The preferred way to implement these functions is to interconnect all workstations by means of a high-speed local network, controlled by a distributed operating system which provides transparent access to all network resources through a network-wide virtual memory system. Additional workstation functions such as large virtual address space and a concurrent multiple-window display system significantly increase user productivity. A high resolution bit-map display system with hardware support for dynamics is an essential component. For the future we look towards both significant cost reductions and improvement in performance, as well as significantly higher level software to better implement the user-computer interface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>C.5.3</cat_node>
				<descriptor>Workstations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P18590</person_id>
				<author_profile_id><![CDATA[81452592989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andries]]></first_name>
				<middle_name><![CDATA[van]]></middle_name>
				<last_name><![CDATA[Dam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Brown University]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39070428</person_id>
				<author_profile_id><![CDATA[81332493735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31076777</person_id>
				<author_profile_id><![CDATA[81100095587]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Dunn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Summagraphics Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330739</person_id>
				<author_profile_id><![CDATA[81332510007]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Harvey]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kriloff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Computer Services]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14144264</person_id>
				<author_profile_id><![CDATA[81406597454]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nelson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Apollo Computer, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL: Professional Workstations CHAIR: Andries van Dam Brown University The panel will examine the 
evolution of the professional worksta- tion from a time shared terminal to powerful graphics-based personal 
computer connected to a resource-sharing local network. The panelists will speculate on the future evolution 
of both the hardware/software architecture and end user environment. PANELISTS: ABSTRACT: Workstation 
development at Stanford James H. Clark Stanford University Workstation development at Stanford is closely 
linked with graph- ics, distributed systems and networking. Two distinct systems have evolved over the 
last 3 years: the SUN system and the IRIS system. The SUN is a low-cost system based upon an efficient 
MC68000 processor/memory design and a relatively low-performance, high- resolution bit-map display. The 
IRIS (Integrated Raster Imaging Sys- tem) is based upon the same MC68000 design, but the graphics part 
of the system is a modest-to low-cost, high-performance, high- resolution, color or black and white system 
that uses the Geometry Engine and several other custom IC parts. Both SUN and IRIS interface to the Stanford 
Ethernet network, and both are being used for distri- buted systems research, VLSI design stations, and 
graphics research. In addition, IRIS will probably be used for mechanical CAD research by the mechanical 
engineering department and in situations where high- performance graphics is important. ABSTRACT: Improvement 
Goals for Workstation Facilities Robert M. Dunn Summagraphics Corporation Three major areas of improvement 
are needed: interaction tech- niques that are simpler, provide faster reaction, are useful for higher-level 
inputs, and have user-style-oriented alternatives. The second area is for image rendering based on local 
capabilities, in varying degrees of image quality as a function of desired "grade of service". The third 
area is to provide support to incremental model construction and approximate design evaluation. There 
must be evalua- tion techniques that can work on partial models and give one approxi- mate results. Implications 
of these criteria for workstation archi- tecture will be discussed. 49 PANEL (continued): Professional 
Workstations ABSTRACT: The application of network workstations to large-scale engineering Harvey Kriloff 
Boeing Computer Services The development of computerized analysis procedures during the last twenty 
years has been largely oriented toward providing increased analytic function. This has meant that considerations 
of user access to specific capabilities or ease of use of these mammoth programs are only now becoming 
user concerns. The emerging new technologies for display, input and their interaction, when applied to 
the professional workstation, will be playing an increasingly important role in satis- fying these concerns. 
A professional workstation can be used both to improve the efficiency by which data is collected for 
an existing analysis program and to assist the user in the preparation of data for a formal presentation 
or report. This performance improvement can be accomplished through the development of user-adapted "macro 
pro- cedures" for data entry, the execution of processes to check program input data for accuracy and 
consistency, and workstation assistance in the training of both new and existing users. At the output 
stage of analysis, the workstation can be used to select and reformat informa- tion prepared by the analysis 
program, explore the interrelationship of results from several different analyses, and derive the data 
for a succeeding analysis in an iterative design mode. At Boeing Computer Services, we have been exploring 
the benefits realized by the development of such a workstation when applied to the field of structural 
engineering. An interaction-rich workstation with a human-engineered executive that controls and integrates 
the user interface, connected to our national network of IBM, CDC and CRAY com- puters with a broad variety 
of engineering software, supplies the engineer with a full range of tools for analysis, data preparation, 
report writing and data relationship exploration that are only begin- ning to be appreciated. In the 
next few years it is expected that this workstation (distributed) methodology for doing engineering and 
other quantitative professional functions will radically change the way these analytical processes are 
performed. ABSTRACT: Personal Workstations in a Local Network David Nelson Apollo Computer, Inc. For 
many applications, personal workstations provide a superior form of computing compared to timesharing. 
The requirements of work- stations, however, go well beyond the facilities provided by the local computer; 
they must include the equivalent advantages of timesharing such as user-to-user communications, shared 
programs and data files, shared peripherals, etc. The preferred way to implement these func- tions is 
to interconnect all workstations by means of a high-speed local network, controlled by a distributed 
operating system which pro- vides transparent access to all network resources through a network- wide 
virtual memory system. Additional workstation functions such as large virtual address space and a concurrent 
multiple-window display system significantly increase user productivity. A high resolution bit-map display 
system with hardware support for dynamics is an essen- tial component. For the future we look towards 
both significant cost reductions and improvement in performance, as well as significantly higher level 
software to better implement the user-computer interface. 50 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801262</article_id>
		<sort_key>51</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[An inversion algorithm for geometric models]]></title>
		<page_from>51</page_from>
		<page_to>59</page_to>
		<doi_number>10.1145/800064.801262</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801262</url>
		<abstract>
			<par><![CDATA[<p>Instead of storing boundary models of solids directly into a data base, it would be advantageous to map them first into a simpler form. This approach calls for a procedure called in this paper the <underline>inversion algorithm</underline> of a geometric model. We present and analyze an inversion algorithm which constructs a sequence of Euler Operators capable of creating a given boundary representation. The algorithm is completely based on the use of Euler Operators enabling us to keep the algorithm simple and to hide implementation and data structure details.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Engineering data bases]]></kw>
			<kw><![CDATA[Euler operators]]></kw>
			<kw><![CDATA[Geometric modelling]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P192797</person_id>
				<author_profile_id><![CDATA[81100402407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Martti]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[M&#228;ntyl&#228;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Helsinki University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baer, A., C.M.Eastman and M.Henrion: "Geometric Modeling: A Survey", Computer Aided Design, Vol. 11, No. 5 (1979), pp. 253-272]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907204</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baumgart, B.G.: "Geometric modeling for computer vision", Ph.D. Thesis, Rep. No. CS-463, Computer Science Dept., Stanford Univ. (1974)]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Baumgart, B.G.: "A polyhedron representation for computer vision", AFIPS Conf. Proc. Vol. 44, National Computer Conf. (1975), pp. 589-596]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Braid, I.C., R.C.Hillyard and I.A.Stroud: "Stepwise Construction of Polyhedra in Geometric Modeling", in: Brodlie, K.W.: "Mathematical Methods in Computer Graphics and Design", Academic Press, New York (1980), pp. 123-141]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Eastman, C.M. and K.Weiler: "Geometric Modeling Using the Euler Operators", Res. Report No. 78, Institute of Physical Planning, Carnegie-Mellon Univ. (1979)]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[M&#228;ntyl&#228;, M.: "Methodological Background of the Geometric Workbench", Report No. HTKK-TKO-B30, Laboratory of Inf. Proc. Science, Helsinki University of Technology (1981)]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1300340</ref_obj_id>
				<ref_obj_pid>1299962</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[M&#228;ntyl&#228;, M. and R.Sulonen: "GWB - A Solid Modeller With Euler Operators", to appear in IEEE Computer Graphics &amp; Applications]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[M&#228;ntyl&#228;, M. and T.Takala: "The Geometric Workbench (GWB) - an Experimental Geometric Modeling System", in: Encarnacao, J. (ed.): "EUROGRAPHICS '81", North-Holland, Amsterdam (1981), pp. 205-215]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356833</ref_obj_id>
				<ref_obj_pid>356827</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Requicha, A.A.G.: "Representations of Rigid Solids - Theory, Methods, and Systems", ACM Computing Surveys, Vol. 12 No. 4 (1980), pp. 437-464]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Requicha, A.A.G. and H.B.Voelcker: "Constructive Solid Geometry", Production Automation Project, Tech. Memo. No. 25, Univ. of Rochester (1977)]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AN INVERSION ALGORITHM FOR GEOMETRIC MODELS Martti M~intyl~i Helsinki University of Technology ABSTRACT 
Instead of storing boundary models of solids directly into a data base, it would be advantageous to map 
them first into a simpler form. This approach calls for a procedure called in this paper the inversion 
al- gorithm of a geometric model. We present and analyze an inversion algorithm which constructs a se-quence 
of Euler Operators capable of creating a given boundary representation. The algorithm is com-pletely 
based on the use of Euler Operators enabling us to keep the algorithm simple and to hide imple-mentation 
and data structure details. CR Categories and Subject Descriptors: 1.3.5 [Com-puter Graphics]: Computational 
Geometry and Object Modeling -sofid representations; 3.6 [Computer-Aided Engineering] -computer-aided 
design (CAD) General Term: Algorithms Additional Key Words and Phrases: Geometric Model-ling, Engineering 
Data Bases, Euler Operators 1. Introduction A computer-aided design (CAD) system can be characterized 
as a collection of design tools, which are based on a computational model of the physical artifact designed. 
In application areas of CAD deal-ing with three-dimensional physical solids, geometric data form the 
core of such a model. These data are refered to as geometric models of solids. A geometric model should 
form a sufficient source of data for all parts of a CAD system such as engineer-ing analysis, drafting 
and computer-aided manufactur-ing (CAM). To allow this, we demand a geometric model to be unambiguous 
]9], i.e. the data structures Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169; 1982 ACM0-89791-076-1/82/007/0051 $00.75 used in the model must designate 
unique physical ob- jects. There are two main approaches to unambiguous geometric models of solids, namely 
(I) the Construc- tive Solid Geometry (CSG) approach /I0/ and (2) the Boundary Representation (BR) approach. 
In a CSG model, solids are described by Boolean combinations of basic solids, while in a BR model solids 
are described by a collection of faces. Faces, in turn, are represented in terms of their bounding edges 
and vertices. To perform certain tasks, also geometric modellers based on the CSG approach transform 
the original definition into a kind of a BR model. In any geometric modelling system the computational 
model has several levels of representation. Initially the user describes it by a definition languag% 
which is mapped into computationalIy suitable internal representations. The definition language text 
is, of course, a model in its own right. A CAD data base is intended to store a computation-al model 
during the design cycle of the object. Of course, also data base representations must be unam-biguous. 
It seems appropriate to consider internal representations and data base representations of geometric 
models as independent of each other. This view leads to the following main alternatives for data base 
representations of geometric models: (1) Store the original definition of the object into the data base. 
Such a definition can be stored into a sequential form by a log of definition opera-tions used. (2) 
Store also the internal representation derived. We consider this approach being equal to storing boundary 
representations into a data base. (3) Transform the internal representation into a sim-ple form, preferably 
into a linear sequence (i.e. string of characters), which then is stored into the data base. Of course, 
it must be possible to re-evaluate the internal data structure based on the derived representation. 
 Let us discuss the pros and cons of these approaches. The first one leads to concise data base formats, 
which serve also as a documentation of the object. To perform analyses or create drawings of the object, 
the definition must be re-evaluated, however. This may be computationally expensive. Boundary models 
of solids are often rather complex network data structures /I/. This means that the second approach can 
lead to difficulties in mapping a complex data structure into a data base format; for instance, physical 
pointers must be mapped into data base references, ultimately secondary storage ad-dresses. Furthermore, 
this approach leads to large data bases, which are difficult to maintain. The re-evaluation of internal 
representations from the data base format is also a nontrivial operation. The third approach is clearly 
advantageous, provided that re-evaluation of internal representations can take place efficiently. This 
motivates the study of the inversion problem: (Inversion Problem) Given a boundary representation, construct 
a linear string of characters such that the string forms an unambiguous mode] of the object de-fined 
by the original representation. To be useful at all, an inversion algorithm must be accompanied by a 
regeneration algorithm~ which is defined by the following: (Regeneration) Given a string of characters 
constructed by an inversion algorithm, construct a boundary representation which unambiguously models 
the same object as the string. Note that we do not require that exactly the original representation is 
constructed. It is sufficient to gen-erate any consistent representation of the object. From the user's 
point of view, this approach is not harmful, because in an unambiguous model all proper-ties of distinct 
representations of an object are equal. 2. An Informal Introduction to the Algorithm Any boundary representation 
is a data structure con-sisting of a finite number of faces, edges and ver-tices. For instance, a cube 
is represented by six faces, twelwe edges and eight vertices such as shown by Figure 2-1. Our algorithm 
will be based on modi-fying the model into simpler forms in a stepwise fashion and recording the incremental 
changes. Figure 2-1 A Cube separated by the edge are joined and we have a col-lection of five faces, 
eleven edges and eight vertices, i.e. a simpler model. Of course, the geometry of the combined face is 
no more planar. Obviously, by re-peating the same operation still thrice we may pro-duce the object depicted 
in Figure 2-2. Figure 2-2 A transformed cube Let us introduce another removal operation, which deletes 
a vertex and an edge such that the vertex is adjacent to no edges except the one to be removed. By this 
operation we can remove the four edge-vertex pairs marked in Figure 2-2. The result of this operation 
is shown in Figure 2-3. i~ ~ I Figure 2-3 A further transformed cube Obviously, by applying once the 
first removal opera-tion and thrice the second, we are left with just one vertex and no edges at all. 
The basis of our algorithm is the fact that a set of modification operations such as those described 
infor-mally above can be identified. These operations are called the Euler Operators /2, 3/. Euler Operators 
are invertible, i.e. each "removal" operator has a "creation" counterpart. This property means that if 
we are able to find a sequence of Euler Operators, which modify a given boundary representation into 
a null object, we can reconstruct it by applying the corresponding inverse operators in the reverse order. 
Thus the inverse sequence can ac-tually be considered an unambiguous model of the ob-ject. Since the 
sequence can be represented by a concise linear data structure, we may conclude that an algorithm capable 
of generating a sequence of Euler Operators corresponding to a given boundary representation is actually 
a solution to the inversion problem defined in chapter 1. Consider the effect of removing the edge marked 
in the figure. In the resulting object~ the two faces 3. Modelling Space and Model Representations Following 
Requicha ]9/, any geometric modelling framework should be based on rigorously defined se-mantics of the 
modelling operations and data struc-tures. This semantics is expressed by the notion of a modelling space. 
The modelling space considered in this paper is the family of "three-dimensional rectilinear polyhedra". 
From now on the term "solid" is used to denote these objects. Solids may have "holes" and multiconnected 
faces. Hollow or disconnected objects are not includ-ed, however. A rigorous definition of solids can 
be found in /6/. In the modelling space chosen, faces can include internal cavities. Thus the boundary 
of a face can consist of several disconnected components or "loops" in our terminology. One of the loops 
represents the outer boundary of the face; the others (if any) represent the cavities and will be called 
"rings". Boundary representations store explicitly certain topo-logical relations between faces, edges 
and vertices /I/. If the model is unambiguous) all other meaning-ful relations can be derived from these. 
In essence) a boundary data structure must be capable of answer-ing any meaningful geometric ("is the 
point <x,y,z> in the interior of the solid?") or topological ("list the neighbour faces of face F") query 
in order to be feasible. Because of the above property, we shall quite freely use topological queries 
in our algorithm. First, we assume the ability of scanning through the edges of a given solid. Given 
an edge E, we assume the availa-bility of its neighbouring faces (denoted E.left face and E.right_face), 
loops (E.left_loop and E.rightjoop) and vertices (E.vertexl and E.vertex2). Furthermore, we assume that 
the edges attached to each vertex are available. 4. Euler Operators Revisited In order to make this paper 
more self-contained we include an introduction to Euler Operators. Euler Operators have derived their 
name from the well-known Euler's Law: in any simple polyhedron) the numbers of faces (f), edges (e) and 
vertices (v) must satisfy the equation (1) v -e + f = 2. In simple polyhedra for which (1) is valid faces 
must be simply-connected and no holes are possible. For-mula (l) may be generalized to solids defined 
in the preceding chapter by introducing three other parame-ters, namely the total number of rings (r) 
in the faces of the object, the number of holes through the object (h) and the number of connected surfaces, 
"shells" (s) in the object. In our modelling space s normally equals unity. The general formula is writ-ten 
(2) v -e + f = 2*s -2*h + r or by setting s = 1) (2") v -e + f = 2 -2*h + r. As noted by Braid, Hillyard 
and Stroud 151, five operators are sufficient to describe all objects satis-fying (2). While these five 
may be chosen in several ways, considerations of modularity and mutual in-dependence have lead to only 
small variations in the collections appearing in the literature /2, 4) 5/. The set of basic operators 
chosen for this work is listed below with an informal definition of the semantics of each operator. mvsf(~ 
Z) Make Vertex) Shell, Face Makes an "initial solid" consisting of one face (f) and one vertex (v) only. 
mev(vl, v2, e) Make Edge, Vertex Adds a new edge (e) connecting an existing ver-tex (vl) and a new vertex 
(v2). Following Baer, Eastman and Henrion [1/, we shall call such an edge a strut. mef(vl, v2, fl, ~ 
_e) Make Edge, Face Splits an existing face (fl) with a new edge (e) between two existing vertices (vl 
and v2) and creates a new face (f2). Note that the vertices may be equal. kemr(e) Kill Edge, Make Ring 
Splits a boundary of an existing face into two components by removing an edge (e). Kemr is applicable 
only if the edge is adjacent only to the face or is a strut and the face remains connected. kfmrh(fl, 
f2) Kill Face, Make Ring, Hole Joins two faces so that the boundary of one (f2) becomes a component (ring) 
in the boundary of the other (fl). For simplicity, we assume that face f2 has no rings. We assume that 
identifiers for faces, edges and ver-tices are assigned by the operators creating them. Above these output 
parameters were underscored. For explanatory purposes the particular parameteriza-tion above is somewhat 
simplified. To represent the effects of the Euler Operators graphically) we may think of them operating 
on a planar graph. In this graph analogy, nodes) arcs and areas of the graph have a natural correspondence 
with the vertices, edges and faces of the model. The mvsf, for instance) creates an "initial" graph with 
one node only. The meanings of Euler Opera- tors can be defined mathematically rigorously by em-ploying 
so called incidence matrices /6/. As an example of Euler Operators, we depict the de-finition of the 
cube with a hole in Figure 4-I by the graph analogy. For brevity, parameter lists are ex-cluded. ! x 
\ /'~ I i I \ i I i | t i ! %" I %. / \ ~ \" mvsf mev mev mef II "ll II/~" II I/ %1 , ,l, ,,cJ--ci,, 
', k / mev mef mev mev mev mef mev mev mef mev mev mef mef ~k~inr mev m~ef kfmrh mev mef mev mef mev 
mef Figure 4-i Euler Operators Note that Euler Operators are not restricted to the case of rectilinear 
objects. For instance, a topologi-cally minimal representation of a torus can be con-structed by Euler 
Operators as depicted in Figure 4- 2. In the resulting object, face fl has two loops sharing edge e. 
Thus v = e = f = r = h = ! and for-mula (2~ is satisfied, as expected. mvsf(fl, vl) mef(vl, vl, fl, f2, 
e) kfmrh(fl, f2) @ Figure 4-2 Construction of a Torus by Euler Operators The operators introduced so 
far are "constructive" in that they are useful for creating boundary models of solids from scratch. For 
the purposes of this paper, their inverse "removal" operators are also interesting. Their names and informal 
meanings are as follows: kvsfO Kill Vertex, Shell, Face Removes a solid such as that created by mvsf. 
kev(e, v) Kill Edge, Vertex Removes an edge (e) and a vertex (v). Kev is applicable only if v has no 
edges except e. kef(e) Kill Edge, Face 3oins two faces by removing the edge (e) separating them. mekr(vl, 
v2, e) Make Edge, Kill Ring 3oins two loops by adding an edge (e) between two vertices (vl, v2), one 
from each of the loops. mfkrh(fl, f2, e) Make Face, Kill Ring, Hole Transforms a ring of face fl into 
a new face (f2). The ring is specified by an edge (e) be-longing to it. The graph models of these operators 
are exactly the inverses of those of the ordinary Euler Operators represented in Figure 4-1. The reader 
may note that the three removal operations of chapter 2 actually equal kef, key and kvsf. Formula (2) 
can be considered the equation of a 5-dimensional hyperplane in the 6-dimensional (grid) space with axes 
<% e, f, h, r, s> /4/. From this point of view, Euler Operators can be thought of as transition vectors 
in the 5-dimensional subspace of the grid defined by (2). For instance, mev operator adds one edge and 
one vertex, thus corresponding to the transition vector <I, I, O, O, O, 0>. The transitions of the ordinary 
Euler Operators are combined into a 6x6 transition matrix M given in Figure 4-3. The last line of M is 
simply formula (2). v e f h r S mev II I 0 0 0 mef ki 01 1 1 0 0 mvsf 0 kfmrh 0 -1 l 1 kemr -I 0 0 1 
 -1 1 2 -1 Figure 4-3 Operator Displacement Matrix  54 5. Derivation of The Algorithm Our algorithm 
is based on the idea of transforming the solid to a null object by Euler Operators. While this is rather 
easy for simple models, holes pose a problem. Obviously, to remove holes the mfkrh operator should be 
used, but how? To derive an algorithm capable of handling the case of holes, let us define the skeleton 
of a boundary model by the following: A SKELETON is a boundary model, where for each edge it is true 
that its faces equal, but its loops do not. The concept of a skeleton is useful in that each edge (if 
any) in a skeleton necessarily bounds a torus-like hole (see Figure 4-2). This means that holes can easily 
be identified and removed in a skeleton. In essence, the definition means that the surface of a skeleton 
can be torn along the skeleton edges while restoring its connectedness -a property characteriz-ing toroidal 
surfaces. Any boundary model can be transformed into its skeleton by inducing a topological classification 
on the edges of the solid and removing each edge violat-ing the definition of a skeleton. In this process, 
if both the faces and loops of an edge equal, it can be removed with either a key or a kemr. If the faces 
differ, it can be removed with a kef. The construc-tion of a skeleton is implemented by the algorithm 
represented in Figure 5-1. makeskeleton(S: solid) begin for all edges E of S do /~ classify E ~/ if E.right_face 
= E.left face then if E.right loop = E.left_loop then if E.vertexl is adjacent only to E then key(E, 
E.vertexl)  else if E.vertex2 is adjacent only to E then key(E, E.vertex2) else kemr(E) else ignore 
E else kef(E) end; Figure 5-1 The Skeleton Construction Algorithm After one make skeleton() step, skeleton 
edges can be removed by transforming holes one by one into new temporary faces by mfkrh. Edges occurring 
in their boundaries are removed by a new make_skeleton0 step. The algorithm for hole removal is represented 
in Figure 5-2. After these steps there remains one vertex for each ring in the original solid or generated 
during the pre-vious steps. Each but one of these vertices may be removed by a mekr followed by a key. 
The last remaining vertex may be removed by a kvsf. The last step is represented in Figure 5-3. The sequence 
of Euler Operators constructed by the algorithm is simply the backward inverse of Euler remove_holes(S: 
skeleton) begin while edges left in S do begin E := any edge of S; F := E.left_face; mfkrh(F, F2, E); 
make_skeleton(S) end end; Figure 5-2 Hole Removal Algorithm remove_rings_and the .object(S: skeleton) 
begin V := any vertex of S; for all other vertices V" in S do begin mekr(V, V, g); key(E, V~  end; kvsf0 
 end; Figure 5-3 Ring Removal Algorithm Operators applied during the algorithm consisting of these steps. 
For simplicity, construction of the se= quence is not explicitly written in the algorithms. Noticing 
that each hole can as well be removed im-mediately when it has been reduced into a skeletal form, all 
three steps can be accommodated in a sin-gle algorithm represented in Figure 5-4. invert(S: solid) begin 
for each edge E of S do if E.right_face = E.left face then if E.right_loop = E.left_loop then if E.vertexl 
is adjacent only to E then key(E, E.vertexl) else if E.vertex2 is adjacent only to E then key(E, E.vertex2) 
else kemr(E) else begin mfkrh(E.left face, F2, E); kef(E) end else kef(E) end; V := any vertex of 
S; for all other vertices V" of S do begin rnekr(V, V, E); key(E, V~  end; kvsf0 end; Figure 5-4 
The Inversion Algorithm The algorithm generates a representation of the to- v8 pology of the object. 
Of course, also the geometry must be stored. In the case of our simple modelling space, it is sufficient 
to store vertex coordinates as a part of each mev and mvsf operator. Face and edge geometries can be 
stored in a similar fashion in the context of the operators creating new faces or edges. In the more 
general case, the geometry of a curve might be defined implicitly as the intersection of two surfaces. 
In this case our algorithm should be con-sidered to invert only the representation of the topol-ogy. 
Note that any implicit geometry representation is invertible, since the definition graph must be acy-clic 
and thus has a partial order. fl: vl v2 v3 v4   v~'~ v'~ f2: vl v5 v6 v2 f3:v2 v6 v7 v3 IS: vt$ vg 
v5 vl f6: vg v7 v6 v5 v~l~l~1~ vl. kef(el2) ~ kemr(e7) " "'~i~] kef(el0) kemr(e2) kef(e9) kemr(el) 
kef(eg) kef(e$) 6. Analysis To be useful, an inversion algorithm should lead to a data representation, 
which is concise as compared to the original data structure and can be transformed more efficiently than 
the original definition language into internal representation. Starting with the second point above, 
we assume that each Euler Operator runs in constant expected time. This can be achieved by using an internal 
symbol table mechanism enabling names for faces, edges and vertices to be mapped into model entities 
in constant time and by storing explicitly certain topological re-lations in the internal model. This 
means that the length of the Euler Operator se-quence is the most important factor. The length of the 
sequence of operators generated is determined by the ordering of edges used during the make skeleton() 
step. In the worst case, Lv/2J rings may be generat-ed, as exemplified in Figure 6-1. Let v, e, f, r 
and h denote the numbers of vertices, edges, faces, rings and holes. The worst case length of the sequence 
may be counted as follows: - each face except the last one takes one kef and the last one one kvsf - 
each vertex except the last is removed by a kev; the last is removed by the kvsf already counted - each 
original ring not removed by kfkrh takes one mekr  - in the worst case, Lv/2J rings may be generated 
by the removal of all edges; they take at most Lvl2j kemr's and mekr's - each hole takes one mfkrh, which 
adds one face and removes one ring. Thus we count for faces h+f-1 kef's i kvsf for vertices v-i kev's 
for rings Lv/2j kemr's r-h + Lv/2J mekr's for holes h mfkrh's. kev(ell,vg) .~r--_ mekr(vl, v2, e) .'P-~ 
kev(e6, v6) ~'[ ) "'~ kev(e, v2) ~ i -~ kev(eS, vS) I ~r~ mekr(vl, v3, e) ' I i"~I " [ t I ! ' t [ l 
 kev(e3, vt$) , ' ' kev(e, v3) --t I .-'~"" i-' mekr(vl, v7, e) ~. , 2--..... ~. =.i~kev(e, v7) .... 
- kvsfO Figure 6-1 Worst Case Example  By the operators listed above, a total of h+f-i + v-i + Lvl2J 
-(r-h+Lvl2J) = v + f -2 + 2*h -r edges are removed. Since this equals e by (23, all edges are removed 
and these operators are sufficient to purge the object. Thus a total of h+f + v-I + r-h+2*Lv/2J+ h = 
 (3) f+2*v+r+h-I (v even) f+2*v+r+h-2 (v odd) operators are generated by the derived algorithm in the 
worst case. 7. The Enhanced Algorithm The simple algorithm derived and analyzed in the preceding chapters 
has the unpleasant property that it may generate new rings during its first phase only to remove them 
during the second one. This results in a longer sequence of operators than would intuitively seem necessary. 
What we would prefer to do is to avoid applying kemr operators in the first phase of the algorithm. This 
can be accomplished by replacing the simple one-pass scan of edges by a more complex algorithm. For simplicity, 
let us use only the list of edges of a solid as the basis of the scan algorithm. While the simple algorithm 
just takes the first edge of the list and classifies it, we now scan the list to locate strut edges. 
As long as struts are found, they are re-moved with a key. Note that the removal of a strut can transform 
some other edge into a strut. If there still remain edges after this process, the al-gorithm tries to 
pick up an edge for which kef is ap- plicable. If no such edge exists, a kemr is applied. Then the first 
process is started again in order to re-move edges possibly transformed to struts. This two-phase scanning 
process is depicted by Figure 7-1 (a-d). The object of Figure 7-1(a) has only two struts el and e2 which 
are removed by key's in Fig-ure 7-1(b). After the second kev, edge e3 is transformed into a strut and 
can also be removed, which gives us the object of Figure 7-t(c). No struts remain and a kef is performed 
onto, say, edge e6 giving the object of Figure 7-1(d). Edges e5 and e7 become struts and the process 
can continue. e4 v e} (a) C b) Co) Cd) Figure 7-1 Edge Scanning Process  It can be seen that removal 
of holes can also be in-cluded into the strut removal phase of the algorithm. Draft for the strut scanning 
algorithm is given in Figure 7-2. It has been written in the form of a boolean function returning true 
as long as a strut or a hole is removed. /* remove one strut or hole-edge */ scanl0: boolean begin for 
all edges E of S do begin if E.left face = E.right_face then if E.~ft_loop = E.right_loop then if E.vertexl 
is adjacent only to E then begin key(E, E.vertexI); return(true) end else if E.vertex2 is adjacent only 
to E then begin key(E, E.vertex2); return(true) end else ignore E else begin /* remove a hole */ mfkrh(E.left_face, 
F, E); kef(E, F); return(true) end else ignore E end; return(false) end; Figure 7-2 The Refined Scanning 
Algorithm  or a kemr. After all edges have been removed, the remaining steps are the same as in the 
simple algo-rithm. The revised inversion algorithm is given in Figure 7-3. invert'(S: solid) begin /* 
step 1" remove edges */ while edges left in S do begin /* remove struts or skeleton holes */ while scant0 
do; /* perform a kef or a kemr */ if edges left in S then scan20 end; /* steps 2 &#38; 3 as in Figure 
5-# */ end; /* perform a kef or a kemr */ scan20 begin /* try to apply a kef */ for all edges E of S 
do if E.left_face != E.right_face then begin kef(E); return end /* apply a kemr */ E := any edge of S; 
kemr(E) end; Figure 7-3 The Revised Inversion Algorithm How good is the revised algorithm? Consider 
again the operator displacement matrix M of Figure #-3. Inverting M we get another 6x6 matrix Minv given 
in Figure 7-g. I I~3 -55 2-2 -22 3-3 1-I 1 - X -3 7 2 -2 3 1 12 2 4 8 -6 2 5 -2 2 9 -I -2 8 # -6 -2 
Figure 7-# The Inverse Displacement Matrix Considering Euler Operators a set of spanning vectors in 
the 6-space <v, e, f, h, r, s>, Minv represents ac-tually a coordinate transformation from the counts 
of the elements of a boundary model (i.e. v, e, f, h, r, s) into numbers of operators needed to generate 
it. For instance, since a tetrahedron has four vertices, six edges, four faces and one surface, we can 
calcu-late the product <% 6, 4, O, O, I> x Minv = <3, 3, I, O, O, O> to infer that to create the tetrahedron, 
at least three mev's, three reef's and one mvsf are needed I#I. Calculating the product The main algorithm 
simply applies the scanning algo-<v, e, f, h, r, s> x Minv rithm until a false is returned and then applies 
a kef and taking into account formula (21 and that s = 1, the following operator counts can be derived: 
(~,a) v-1 mev's (t~b) f+h-1 mef's (t~c) 1 mvsf (4d) h kfmrh's (t~e) r-h kemr's or a total of v+f+r+h-I 
operators. Since in a nonempty model v, f > 0, quantities (t~a) -(4d) are positive numbers. (4e), however, 
may be negative. This indicates that there are models which cannot be constructed without the mekr operator. 
An example is shown in Figure 7-5. Figure 7-5 A Model Needing the Mekr Operator The calculation above 
says nothing of whether a se-quence with exactly these amounts of operators ex-ists, since inverse operators 
contribute to the counts as -1. With regard to mev's~ mef's, mvsf's and kfmrh's, however, our algorithm 
shows that any model can be constructed without the corresponding inverse operators. By the analysis 
of chapter 6 it can be seen, that the enhanced algorithm generates exactly the desired amounts of these 
operators. With regard to the kemr/mekr operators, no similar result exists. Further analysis shows that 
mekr operators can be generated only when holes are present, which gives us at most h mekr~ and by (t~e) 
r kemr's. Thus the refined algorithm generates at most v-1 + f+h-1 + 1 + h + h + r = (5) v+f+r+3*h-1 
operators, which is a better result than (3). The enhanced algorithm is the best that can be ex-pected 
in the sense that if a model can be construct-ed without mekr~ they will not appear in the inver-sion 
generated. In this case the algorithm generates exactly v+f+r+h-I operators. Mekr's will be included 
in the inversion only when strictly needed. Interestingly, restricting the modelling space and the set 
of Euler Operators to simply-connected faces, the lower bound deriveable from the corresponding inverse 
transition matrix would be achieved by the simple al-gorithm. Result (5) is achieved by spending more 
time in the scanning process: while the simple version runs in linear time in terms of the number of 
edges (e), the enhanced version takes a O(e*e) time. A more ela-borate scanning algorithm based on "winged-edge" 
queries can avoid this, however. 8. Discussion Result (5) should be compared against the more con-ventional 
data base formats for boundary models. To give some practical flavour, let us discuss various representations 
of a cube. An inversion for a cube is represented by Figure 8-1. The parameterization used is slightly 
different from that of chapter 4. Calculating 2 bytes for each pointer (object name) and 4 bytes for 
each real number, the inversion would take a total of 182 bytes in the secondary storage. mvsf(vl, fl, 
0.0, 0.0, 0.0); mev(fl, vl, v2, I0.0, 0.0, 0.0); mev(fl, v2, v3, I0.0, i0.0, 0.0); mev(fl, v3, v% 0.0, 
I0.0, 0.0); mef(vl, v4, fl, f2); mev(f2, vl, v5, 0.0, 0.0, 10.0); mev(f2, v2, v6, 10.0, 0.0, 10.0); 
mef(v5, v6, f2, f3); mev(f2, v3, v7, 10.0, 10.0, 10.0); mef(v6, v7, f2, f4); mev(f2, vt~, v8, 0.0, 10.0, 
10.0); mef(v7, vS, f2, f5); reef(v8, v5, f2, f6); Figure 8-1 Inversion of a Cube  In contrast, one 
of the most compact ways to store a boundary model directly is to store vertex coordinates in a list. 
Faces are represented by listing the ver-tices occurring in their boundaries. This vertex-based representation 
would occupy 168 bytes. If ob-ject data on edges is desired, face boudaries should be represented as 
lists of edges. Edges, in turn, have pointers to two vertices. This edge-based representation would take 
192 bytes. This overly simplified comparison shows that Euler Operator se-quences are roughly on the 
same level of verbosity than the more conventional representations. Euler Operators are currently used 
in a number of geometric modelling systems as the basic definition operations. So is the case in the 
Geometric Work-bench (GWB) system under development at the Hel-sinki University of Technology /7, g/. 
In GWB, all definition operations are eventually REFERENCES evaluated into operator sequences. These 
sequences can be evaluated very quickly; for instance, re-evaluation of the inverted results of a set 
operation takes a fraction of the time spent in performing the (i) set operation. The inversion algorithm 
makes GWB a "closed" system in the sense that all object manipula-tion algorithms can be viewed as functions 
mapping operator sequences to operator sequences. (2) Certain operations can be performed directly based 
on the sequences; for instance) to generate a line figure of an object, we need to evaluate only the 
(3) wire-frame corresponding to its inversion, not the whole boundary representation. Linear transforma-tions 
can be applied to objects just by scanning through the sequence and transforming all vertex (4) coordinates. 
There are also other potential uses of the inversion algorithm. It seems to be useful for the important 
problem of transmitting data from one boundary modeller to another. By adding an inversion algo-(5) rithm 
and a regeneration algorithm into both modell-ers, models can be exchanged by inverting the inter-nal 
representation of one modeller into an operator sequence and evaluating it into to internal represen-tation 
of the other one. In the case of N modellers) (6) N such interfaces will be needed as compared to the 
fully connected case of N*(N-I)/2 interfaces. (7) 9. Conclusions We have represented an inversion algorithm 
for boun-(8) dary models of solid objects. The Euler Operator se-quence constructed by the algorithm 
is an unambigu-ous representation of the object modelled by the ori-ginal boundary model; an equivalent 
boundary model can be easily generated based on it. Being a linear data structure, the sequence can easily 
and efficient-(9) ly be used as a secondary storage format for boun-dary representations. The operator 
sequence generat- ed by the algorithm is of minimal length. The usefulness of the algorithm is not restricted 
to (10) the problem of the secondary storage format. It fa-cilitates the separation of the logical data 
represen-tation from the actual representation chosen in geometric modellers based on the BR approach. 
We suggest that operator sequences might be considered models of solids in their own right in geometric 
modelling systems. This approach should enhance their modularity and flexibility. Baer) A., C.M.Eastman 
and M.Henrion: "Geometric Modeling: A Survey") Computer Aid-ed Design) Vol. Ii No. 5 (1979), pp. 253-272 
Baumgart, B.G.: "Geometric modeling for com-puter vision", Ph.D. Thesis, Rep. No. CS-463, Computer Science 
Dept.) Stanford Univ. (197t0 Baumgart, B.G.- "A polyhedron representation for computer vision", AFIPS 
Conf. Proc. Vol. t*t 4 National Computer Conf. (1975), pp. 589-596 Braid, I.C.) R.C.Hillyard and l.A.Stroud: 
"Stepwise Construction of Polyhedra in Geometric Model-ing") in-Brodlie, K.W.: "Mathematical Methods 
in Computer Graphics and Design", Academic Press, New York (1980), pp. 123-141 Eastman, C.M. and K.Weiler--"Geometric 
Model-ing Using the Euler Operators") Res. Report No. 78, Institute of Physical Planning, Carnegie-Mellon 
Univ. (1979) M~intylti, M.: "Methodological Background of the Geometric Workbench", Report No. HTKK-TKO-B30, 
Laboratory of Inf. Proc. Science) Helsinki University of Technology (198I) Mtintyl~i, M. and R.Sulonen" 
"GWB -A Solid Modeller With Euler Operators") to appear in IEEE Computer Graphics &#38; Applications 
M~ntyl~i, M. and T.Takala; "The Geometric Work-bench (GWB) -an Experimental Geometric Modeling System", 
in- Encarnacao, 3. (ed.): "EU-ROGRAPHICS "81", North-Holland, Amsterdam (1981), pp. 205-215 Requicha, 
A.A.G.-"Representations of Rigid Solids -Theory, Methods) and Systems", ACM Computing Surveys, Vol. 12 
No. 4 (1980), pp. 437-z¢-64 Requicha, A.A.G. and H.B.Voelcker: "Constructive Solid Geometry", Production 
Automation Project, Tech. Memo. No. 25, Univ. of Rochester (1977) ACKNOWLEDGEMENTS Thanks are due to 
Reijo Sulonen and Markku Tam-minen, who gave valuable comments during the preparation of this paper. 
The work described in the paper has been supported by the Academy of Finland and the Helsinki Universi-ty 
of Technology Foundation.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801263</article_id>
		<sort_key>61</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Generation and display of geometric fractals in 3-D]]></title>
		<page_from>61</page_from>
		<page_to>67</page_to>
		<doi_number>10.1145/800064.801263</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801263</url>
		<abstract>
			<par><![CDATA[<p>We present some straightforward algorithms for the generation and display in 3-D of fractal shapes. These techniques are very general and particularly adapted to shapes which are much more costly to generate than to display, such as those fractal surfaces defined by iteration of algebraic transformations. In order to deal with the large space and time requirements of calculating these shapes, we introduce a boundary-tracking algorithm particularly adapted for array-processor implementation. The resulting surfaces are then shaded and displayed using z-buffer type algorithms. A new class of displayable geometric objects, with great diversity of form and texture, is introduced by these techniques.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Fractal dimension]]></kw>
			<kw><![CDATA[Invariant surface]]></kw>
			<kw><![CDATA[Iteration]]></kw>
			<kw><![CDATA[Quaternions]]></kw>
			<kw><![CDATA[Surface determination]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31088777</person_id>
				<author_profile_id><![CDATA[81100065198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Thomas J. Watson Research Center, Yorktown Heights, NY]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807461</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Artzy, E., Friedan, G., and Herman, G., The Theory, Design, Implementation and Evaluation of a Three-dimensional Surface-Detection Algorithm, Comp. Graph. &amp; Im. Proc., 15, 1981, pp. 1-24.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Carpenter, L.C., Fournier, A., and Fussel, D., Display of fractal curves and surfaces, to appear, Comm. ACM.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hamilton, Sir W. R., Elements of Quaternions, Vols. I and II, Reprinted by Chelsea Publ. Co., New York, 1969.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Julia, G., M&#233;moire sur l'iteration des fonctions rationnelles, J. Math. Pure Appl. 4:47-245, 1918. Reprinted in Oeuvres de Gaston Julia, Vol. I, Gautier-Villars, Paris, 1968.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lin, H. K., Two and Three Dimensional Boundary Detection, Comp. Graph. and Im. Proc., 6, 1977, pp. 123-134.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Mandelbrot, B. B., Fractals: Form, Chance, and Dimension, Freeman, San Francisco, 1977.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Mandelbrot, B. B., Fractal Aspects of the Iteration of z&#8594;&#955;z(1&minus;z) for complex &#955; and z, Ann. N. Y. Acad. Sci. 357, 1980, pp. 249-259.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Mandelbrot, B. B., The Fractal Geometry of Nature, Freeman, San Francisco, 1982.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Mandelbrot, B. B., and Norton, A., Fractal Surfaces Defined by Iteration of Rational Functions in the Quaternions, to appear.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M., and Sproull, R. F., Principles of Interactive Computer Graphics, McGraw-Hill, New York, 1973.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I., Sproull, R., and Schumacker, R., A Characterization of Ten Hidden-Surface Algorithms, Computing Surveys, Vol. 6, No. 1, 1974.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generation and Display of Geometric Fractals in 3-D Alan Norton IBM Thomas J. Watson Research Center 
Yorktown Heights, NY 10598 ABSTRACT We present some straightforward algorithms for the generation and 
display in 3-D of fractal shapes. These techniques are very general and particularly adapted to shapes 
which are much more costly to generate than to display, such as those fractal surfaces defined by itera- 
tion of algebraic transformations. In order to deal with the large space and time requirements of calculating 
these shapes, we introduce a boundary-tracking algor- ithm particularly adapted for array-processor imple-mentation. 
The resulting surfaces are then shaded and displayed using z-buffer type algorithms. A new class of displayable 
geometric objects, with great diversity of form and texture, is introduced by these techniques. CR Categories 
and Subject Descriptors: 1.3.5 [Computer Graphics]: Computational Geometry and Object Modeling -Curve, 
surface, solid, and object representations; 1.3.7 [Computer Graphics]: Three-Dimensional Graphics and 
Realism -Color, shading, shadowing, and texture. General Terms: Algorithms, theory. Additional Key Words 
and Phrases: fractal dimen- sion, surface determination, iteration, invariant surface, quaternions. Permission 
to copy without fee all or part of this material is granted providedthat the copies are not made or distributed 
for direct commercial advantage, the ACM copyright notice and the title of the publication and its date 
appear, and notice is given that copying is by permission of the Association for Computing Machinery. 
To copy otherwise, or to republish, requires a fee and/or specific permission. (~) 1982 ACM 0-89791-076-1/82/007/0061 
$00.75 Introduction Until recently the geometric shapes of interest in engineering, science, and mathematics 
generally were constructed from the simple, smooth objects of classi- cal geometry. However, a new approach 
to geometry has now arisen, largely through the efforts of its origi- nator, B. Mandelbrot. He singled 
out a class of shapes, named them fractals, and developed nonrandom and random fractal models to simulate 
(and in some cases, to explain) the roughness and fragmentation of diverse aspects of nature. His fractal 
models of terrain prove of wide application in computer graphics [2,6,8]. Of special interest are the 
nonrandom fractals which are obtained by the iteration of algebraic func- tions. The mathematics of iteration 
has a long history, dating back to the work of Poincare, Fatou, and Julia [4]. However, the striking 
beauty and complexity of the resulting shapes in the complex plane was not re-vealed until the recent 
investigations of Mandelbrot [7]. The mathematical study of these shapes has led us to consider their 
counterparts in three-and four-dimensional space. In some cases a 1-parameter family of planar fractals 
may be better understood by regard- ing the whole family as a fractal object in 3-space. The major impetus 
for this work however was the dis- covery of a class of geometric shapes which result in three and four 
dimensions from iteration of algebraic operations in the quaternions [9]. Such shapes can properly be 
visualized only by producing images as close as possible to the 3-D visual experience. With the use of 
high-resolution computer graphics one simu- lates the effect of viewing such fractal objects as if they 
were modeled from clay and illuminated from outside. This article describes a system developed'for gener- 
ating and displaying such shapes in 3-D. The techni- ques discussed are generally applicable in the genera-tion 
and display of connected surfaces which form the boundaries between regions defined by different arith- 
metic or logical conditions. That is, the surface in Computer Graphics Volume 16, Number 3 July 1982 
question is required to have an inside and an outside, and it is assumed possible to distinguish in which 
set a given point lies by performing a series of calculations on the coordinates of the point. The techniques 
dis-cussed may be used to determine and display any such surface. However, the most interesting applications 
are on surfaces whose determination requires a great deal of calculation, and the display techniques 
were particu-larly designed for very irregular fractal surfaces. The mathematical principle behind the 
generation of these fractal shapes involves the iteration of algebra- ic transformations. Consider a 
mapping T: R n ~ R n of n-dimensional Euclidean space into itself. If x is a point in R n, successive 
applications of T define a se-quence of points in Rn: Xo=X, Xl=T(x), x2=T(T(x)), x3=T(T(T(x))) .... 
There are several possibilities for such a sequence; for example the points may diverge to infinity; 
the sequence of points could converge to a finite limit; the sequence could repeat a cyclic series of 
points; and so on. The result of such an iteration ordinarily depends on the starting point x. When that 
is the case, points in R n may be classified according to the results of such an iteration. For example, 
T(x) = x 2 defines a map-ping of the complex plane into itself. If [ x [ >1, suc-cessive applications 
of T result in a sequence tending to infinity. If [ x I <1 the sequence converges to 0. (Zero and infinity 
comprise the attractor set of T.) If x is in the circle I xl=l, the sequence remains in that circle. 
In this case the three sets defined by Ixl<l,lxl=l, and Ixl>l are each preserved byT. One in fact gains 
considerable understanding of the transformation T by considering only what it does to the invariant 
set Ix I---l; The algebraic symmetries present in the for-mula for T are exhibited in the geometry of 
this invari- ant set. This set can be described as the boundary between the points attracted to zero 
and those attract-ed to infinity. The surfaces illustrated in this article are all de- fined by analogous 
phenomena, but where the transfor- mation T may be given on R 3 or R 4 by any series of algebraic operations 
on the underlying real coordinates of the points. The algebraic symmetries involved in the transformations 
can be more complex than in the ex-ample above and the resulting invariant surfaces usual- ly are fractals. 
To check whether a given point is in- side or outside a given invariant surface, one calculates as many 
as 1000 iterates in the sequence T(x),T(T(x)),T(T(T(x))) ..... testing whether the points satisfy the 
appropriate conditions. Fractal dimension and surface modeling We refer the reader to [6,8] for a thorough 
discus-sion of fractals. Fractals are defined (see [6,8]) as mathematical objects whose topological dimension 
differs from their Hausdorff (fractal) dimension. This dimension is also related to the computer-graphic 
dis-play of complex objects. Suppose given a complex real-world environment which is to be displayed 
by computer graphics; e.g., a complex mechanical device or a natural scene. Such an environment must 
be ap-proximated when it is represented internally in the computer. Typically one can specify a resolution 
or tolerance and then represent such an environment in terms of primitive objects which are no smaller 
than the specified tolerance. An estimate of the complexity of such an environment is given by the number 
of such primitives required at a given resolution. See [11] for a discussion of how various hidden-surface 
algorithms perform as a function of this type of complexity. More generally, one may ask how the number 
of primitive elements varies as the tolerance is decreased. If for example a surface is represented by 
planar poly- gons, one would expect the number of such primitives to increase by at least a factor of 
four when the resolu- tion is doubled. The fractal dimension of the environ- ment can be related to this 
rate of increase. Suppose the surface of the environment is represented by a set of cubes of a given 
size (tolerance) which contains the surface. Let N(r) denote the number of such cubes of diameter r which 
are required. If N(r) grows as a pow- er r -D of the diameter, then D is the fractal dimension of the 
surface in question. Note that D will generally be between 2 and 3. The fractal dimension is therefore 
directly related to the number of sample points re-quired to describe the boundary of an object. The 
boundary representation used here is an approximation of the above cubic description. The rate of growth 
of the size of the surface models may be used to give a rough approximation of the fractal dimension. 
Other representations, using for instance planar polygons, can be similarly related to the fractal dimension. 
Another measure of the difficulty of graphically representing a complex environment is given by the depth 
complexity associated with a given viewer posi- tion and line of sight. The depth complexity is the number 
of visible surfaces intersecting the ray from the viewer in the direction of his line of sight. Depth 
com-plexity may be considered an indicator of the sorting or priority-checking which is required to determine 
visibil- ity in a hidden-surface algorithm. Typically a mathe-matical object in R 3 of dimension D intersects 
a straight line in a set of dimension D-2. Thus the num-ber of surfaces with which the hidden surface 
algorithm must deal is of the order of r 2-D, depending on the diameter r of primitives. Surface Determination 
The basic technique of calculating these invariant surfaces involves iterating a function repeatedly 
and keeping track of the points which satisfy certain crite-ria after many iterations. Generally the 
result of such a calculation is a determination of whether the starting point is interior or exterior 
to a specified invariant set. When such a starting point is interior, it is presumed (for purposes of 
approximation) that the entire cube associated with the point is interior. Nevertheless, these "point 
determinations" provide only a sampling of the surface and one must maintain a distinction be-tween grid 
points and the volumes they correspond to. The surfaces could in principle be determined by evalu- ating 
every point on a large 3-dimensional grid, then selecting boundary points according to known techni-ques 
(see [1,5].) However, the amount of calculation and data involved would prohibit such a determination 
on a large grid (say 1000xl000xl000). Instead, the surface is followed throughout the grid, without calcu-lating 
points far from the surface. The interior points which adjoin exterior points are retained (as proved 
boundary points) and their neighbors become candi-dates for the boundary, to be tested in the next cycle. 
The number of function evaluations is reduced consid-erably by this technique. (Exactly how many function 
evaluations depends on the fractal dimension of the surface in question.) This technique also provides 
in-formation about connectivity of the surface: One can separately calculate and examine different connected 
components of a given surface. An array processor was employed to do floating-point arithmetic offline. 
Because the point evaluations can be done by repeatedly performing the same opera-tions on lists of real 
numbers, the array processor is ideally suited for this type of application. However, these arithmetic 
calculations still require much more computer time than any other aspect of the surface determination. 
The array processor was programmed to input a list of up to 2000 points and output a list of as many 
codes, indicating whether each point is inside or outside the given geometric shape. This process is 
most efficiently performed when the number of input points approaches its maximum. Therefore, in follow- 
ing the surface, it was important to deal efficiently with large stacks, rather than keeping the stacks 
short. What is presented here is a systematic way of effective- ly keeping track of the results of calculations, 
and en-suring that most of the computer time involved is spent on function evaluation, not on manipulation 
of points. These methods were applied on an IBM 3033 with a Floating Point Systems AP190L array processor. 
Ordi-narily the function evaluation on the array processor used more than 90% of the total CPU time. 
The basic requirements of the surface determina-tion are as follows: 1. Repetition of point evaluations 
should be avoided, as this is the most time-consuming operation. 2. The algorithm should make no assumptions 
about the extent of the surface; it may be extremely convoluted (even approaching space filling) or it 
may be very smooth. 3. No sorting or comparison of stacks should be done; all stack manipulations should 
be linear in com-plexity, so that there be no penalty for using large stacks.  The final output of this 
process is a list of points P (boundary points) on the grid, satisfying the following: 1. P is inside 
the shape in question 2. At least one of the neighbors of P is outside the shape in question. 3. P 
is connected to one of a set of starting points via a path which follows the grid and consists only of 
boundary points.  Note that the above 3 conditions provide a recur-sive definition of the desired set 
of boundary points, depending only on the mathematical shape in question, the grid used, and the set 
of starting points. To deter- mine all boundary points, maintain a stack of newly-discovered boundary 
points, and perform the following until this stack is exhausted: 1. List all neighbors of newly-discovered 
boundary points. 2. Determine which of these neighbors are interior. Those which are interior are candidates 
for the boundary (since they satisfy conditions 1 and 3 above). 3. Check the neighbors of the candidates. 
If a candidate has a neighbor which is exterior, the candi-date is a boundary point, and is listed with 
the newly-discovered boundary points.  In order to be precise, one must define the notion of adjacency 
which is to be used in determining the "neighbors" found in steps 1 and 3. It was in fact useful to use 
different notions of adjacency, depending on the fractal characteristics of the surface being inves- 
tigated. The notion of adjacency used in step 1 deter- mines the extent of the surface; therefore surfaces 
which contain long, thin strands are more thoroughly delineated by using a liberal notion of adjacency, 
for instance considering cubes which touch only at a cor-ner to be adjacent. The notion of adjacency 
used in step 3 determines how thoroughly the surface is to be covered. In deciding whether a given interior 
cube was on the boundary, it was usually required to share a face with an exterior cube. In order to 
begin the surface determination, one must specify a starting list of boundary points. (The choice of 
starting points will establish which compo-nents of the surface are to be determined.) The starting points 
can be easily specified by choosing a line seg-ment which joins the interior and exterior of the given 
shape. At least one point on such a segment will be a boundary point, and can serve as a "seed" for the 
sur-face determination. It should be observed that the names "interior" and "exterior" are chosen only 
to distinguish two alterna-tive properties, not necessarily describing bounded and unbounded sets. In 
fact, the algorithm allows one to follow either the interior or exterior of the given math- ematical 
boundary. There are often interesting qualita- tive differences between these two sets, as shown in the 
illustrations. Display Methods The output of the preceding calculation is a sorted file of grid vertices; 
typically such a file will contain over one million points. This list of grid points pro-vides at best 
inner and outer limits to the extent of the surface; at worst a biased sampling of points near the surface. 
To each point on the list corresponds a cube which could be regarded as the primitive for the display 
process. However, one only can conclude that the cube in question intersects the given volume, not that 
the cube's surface coincides with part of the fractal surface being displayed. Because the surfaces to 
be displayed are fractal, the display techniques were chosen so as to avoid the customary preference 
for smoothness. The details one sees at a given resolution should suggest higher resolu- tion will reveal 
more detail. It was therefore consid-ered inappropriate to model the surfaces with planar or smooth primitives; 
the points themselves become the primitives of the display process. Conceptually the presence of a point 
on the list represents only the fact that a certain portion of 3-space is occupied by some opaque material; 
this should not imply the presence of a smooth surface bounding that opaque material. The display process 
is in two stages. In the first stage, illumination intensities are assigned to each ver-tex, depending 
on the relationship of th~ vertices to an imagined light source. During the second stage, an image is 
produced, depending on the direction to the viewer. The first stage is more time-consuming, requir- ing 
two passes through the data points. The second, image-producing stage, involves only one sequential pass 
through the data. The second stage may be per-formed repeatedly, to obtain a series of views of the object 
from different angles. The algorithms for both processes are based on z-buffers; that is, the model is 
projected into a two-dimensional buffer in memory, the projection being performed in such an order so 
as to handle shading and hidden surface elimination. Be-cause of the simplicity of the primitive display 
ele-ments, no sorting is required for either illumination and display projections. If the data initially 
is sorted on increasing model coordinates, then the z-buffer projec- tion can for most directions be 
achieved by a forward or reverse pass through the data. Projection in the remaining directions can be 
achieved by a simple res-tucturing of the initial sort, again done in linear time. Alternatively, the 
visible or illuminated surfaces can be found by determining the distance of each point from the viewer 
or light-source at the time of the projection, retaining closest points. This also is done in linear 
time, but may require additional space to retain visible z-coordinates. The use of z-buffers makes this 
an image-space rather than object-space algorithm; this is not a serious drawback, because the surfaces 
are only calculated to a prespecified accuracy. The primary difficulty encoun-tered with z-buffer techniques 
was in determining the illumination of surfaces which are nearly parallel to the light direction. This 
is a situation where some of the information ordinarily lost in a z-buffer projection must be recovered. 
Our solution to this problem was to regard the surface cubes as somewhat translucent; cubes immediately 
behind illuminated cubes receive a portion of the light of the illuminated ones. This tech- nique is 
particularly useful for eliminating staircasing when the light direction is along one of the grid axes. 
Fractal surfaces are not differentiable. This implies that the surface normal vector at a given point 
is gen- erally not defined. In order to simulate light reflec-tance one can however determine a "typical" 
normal direction at a given point by comparing its coordinates with those of nearby points. To obtain 
an approximate normal vector at a given illuminated point (which has projected into the z-buffer), the 
z-coordinates of its neighbors in the z-buffer are compared. The differ-ence in z-coordinate between 
two points divided by their separation in the z-buffer is used to estimate the tangent of the angle between 
the light source and the surface normal. The average of several such surface-normal approximations can 
be used to define the re-flectivity at a point. The figure below shows how such a normal vector is calculated: 
> Points labeled P and Q are projected to a z-buffer in the direction indicated by parallel arrows. 
The line perpendicular to PQ forms an angle theta with the illumination direction. Cos(0) may be calculated 
as the ratio a/c, where a is proportional to the distance be-tween the projections of P and Q in the 
z-buffer, b is the difference in z-coordinates of the two points, and c is the corresponding hypotenuse. 
Two of these calcula- tions al~ right angles determine a normal vector. The final part of the picture 
display consists of projecting the points in the direction of an imagined viewer, using the intensities 
determined above to de-cide how bright a given point will appear, this process is a straightforward z-buffer 
algorithm and is per-formed with an orthogonal projection. In order to ensure that the visible surfaces 
completely conceal the hidden surfaces, individual points in the model may be projected to several nearby 
positions in the image z-buffer. The result is a half-tone picture in which each visible point in the 
model produces one or more dots on the picture. Illustrations The illustrations have been chosen as 
examples of the diversity of form and texture which result from these techniques. The first two illustrations 
show 1- parameter families of fractals in the complex plane. Each horizontal slice is the shape in the 
complex plane corresponding to choosing one value of the parameter, and the whole figure shows the effect 
of changing the parameter. Shapes in the complex plane upon which these figures were based were first 
displayed graphical- ly by B. Mandelbrot. See plate 187 in [8] for another illustration of this type. 
The surfaces in figures 1 and 2 are defined as follows: For each complex number of absolute value 1, 
iterate the transformation T(z) = z(1-z). The set of complex numbers z for which the successive iterates 
do not tend to infinity defines a subset of the complex plane. The illustrated 3-d shapes show how this 
planar shape varies for ~ on the circle I 2, I = 1. By a change of variables, the shape in figure 1 is 
"unwound", producing figure 2. The two objects were followed on grids of size 6003 . Each is represented 
by approximately two million points. They are displayed at resolution 600x800 using two light sources. 
Figures 3 th/'ough 6 illustrate some invariant shapes determined by the iteration of quadratic polynomials 
in the four-dimensional quaternions [3]. (See [9] for a discussion of the mathematics involved in iteration 
of rational functions in the quaternions). For our present purposes, it suffices to observe that quaternion 
multi-plication and addition provide a 4-dimensional general- ization of complex arithmetic, based on 
vector algebra. Models in R 3 are produced by starting the iteration in a three dimensional subspace 
of the quaternions spanned by 1, i, and j. The resulting shapes were followed on a grid of size 12003 
. They were displayed at resolution 1024x1280 using two light sources. Figure 3 is defined by iteration 
of a quadratic po-lynomial in the quaternions, of the form k z(1-z), with a complex number k. The parameter 
~, was chosen so as to obtain an attractive cycle of length 7, using tech- niques developed in [7]. The 
shape shown is a con-nected component of the set of points attracted to that cycle. The shape is invariant 
under a 7-fold iteration of the defining transformation, and is one of an infinite number of components 
defined by that invariance. This component is represented by about 5.6 million points. Figure 4 is defined 
by the iteration of another po-lynomial of the form k z(1-z). In this case, the shapes illustrated are 
different components of the set attracted to a cycle of length four. Figure 5 is defined by the iteration 
of a polynomial of the form z2+t~, with t~ a complex number chosen so as to produce an attractive cycle 
of length 4. The illus- trated shape is the boundary of the set attracted to that cycle. Four colors 
are used to distinguish invariant sets defined by 4-fold iteration of the transformation. Figure 6 is 
defined by the polynomial i(z2+l). The illustrated shape is one of two components which together comprise 
the set of points attracted to a cycle of length two.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801264</article_id>
		<sort_key>69</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[A new general triangulation method for planar contours]]></title>
		<page_from>69</page_from>
		<page_to>75</page_to>
		<doi_number>10.1145/800064.801264</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801264</url>
		<abstract>
			<par><![CDATA[<p>The problem of approximating the surface spanning a given set of 3D points as a polyhedron of triangular faces (&#8220;triangulation&#8221;) is a significant one, and has many applications in the fields of computer graphics and computer vision. In this paper, several solutions to this problem are reviewed. These solutions can be grouped into two classes, and particular emphasis is given to the class of surfaces spanned by parallel planar contours. For a contour pair <italic>P<subscrpt>0</subscrpt>,P<subscrpt>1</subscrpt>,...P<subscrpt>m&minus;1</subscrpt></italic> and <italic>Q<subscrpt>0</subscrpt>,Q<subscrpt>1</subscrpt>,...Q<subscrpt>n&minus;1</subscrpt></italic>, a graph theoretic approach can be used to arrive at a class of solutions, each requiring exactly <italic>m+n</italic> steps to triangulate the pair. Existing methods (both rigorous and heuristic) for extracting a particular solution from this group are reviewed, and a new heuristic based on inter-contour coherence is proposed. This heuristic is being used in the field of Ultrasonic Non-destructive Evaluation to produce images of flaws in pressure vessels, and its performance is shown to compare favorably with methods of greater computational complexity. It is believed that this heuristic can also be used with success in industrial vision systems where similar contours are obtained using a laser range finder.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Graph]]></kw>
			<kw><![CDATA[Heuristic methods]]></kw>
			<kw><![CDATA[Object modeling]]></kw>
			<kw><![CDATA[Tree searching]]></kw>
			<kw><![CDATA[Triangulation]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>G.1.2</cat_node>
				<descriptor>Approximation of surfaces and contours</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.2.8</cat_node>
				<descriptor>Heuristic methods</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010205.10010206</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Search methodologies->Heuristic function construction</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003624.10003633.10010918</concept_id>
				<concept_desc>CCS->Mathematics of computing->Discrete mathematics->Graph theory->Approximation algorithms</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14177193</person_id>
				<author_profile_id><![CDATA[81100508884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ganapathy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ultrasonic Imaging Laboratory, Department of Electrical and Computer Engineering, University of Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333912</person_id>
				<author_profile_id><![CDATA[81536809956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[G.]]></middle_name>
				<last_name><![CDATA[Dennehy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ultrasonic Imaging Laboratory, Department of Electrical and Computer Engineering, University of Michigan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Boissonnat, J.D. and Faugeras, O.D., "Triangulation of 3D Objects," PROCEEDINGS OF THE 1981 INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, 658-660.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[O'Roarke, Joseph, "Triangulation of Minimal Area as 3D Object Models," PROCEEDINGS OF THE 1981 INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, 664-666.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359846</ref_obj_id>
				<ref_obj_pid>359842</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., etal., "Optimal Surface Reconstruction from Planar Contours," COMMUNICATIONS OF THE ACM, XX-10 (October 1977), 693-702.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Keppel, E., "Approximating Complex Surfaces by Triangulation of Contour Lines," IBM JOURNAL OF RESEARCH AND DEVELOPMENT, XIX (January 1975), 2-11.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807388</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Christianson, H. and Sederberg, T.W., "Conversion of Complex Contour Line Definitions into Polygonal Element Mosaics," COMPUTER GRAPHICS, XIII,2 (August, 1978), 187-192.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Ganapathy, S., etal., "Ultrasonic Imaging Techniques for Real-time In-service Inspection of Nuclear Power Reactors", Nuclear Regulatory Commission Report NUREG/CR-2154, September, 1981.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>30425</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Nilsson, Nils, PRINCIPLES OF ARTIFICIAL INTELLIGENCE, (Palo Alto: Tioga Publishing Co.) 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Horn, Berthold K. P., "Hill Shading and the Reflectance Map," PROCEEDINGS OF THE IEEE, LXIX, 1 (January, 1981) 14-47.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A NEW GENERAL TRIANGULATION METHOD FOR PLANAR CONTOURS S. Ganapathy &#38; T. G. Dennehy Ultrasonic 
Imaging Laboratory Department of Electrical and Computer Engineering University of Michigan Abstract 
The problem of approximating the surface spanning a given set of 3D points as a polyhedron of triangular 
faces ("triangulation") is a significant one, and has many appli- cations in the fields of computer graphics 
and computer vision. In this paper, several solutions to this problem are reviewed. These solutions can 
be grouped into two classes, and particular emphasis is given to the class of surfaces spanned by parallel 
planar contours. For a con-tour pair Po, PI,,Pm_I and Q0,Q1, Qn-], a graph theoretic approach can be 
used to arrive at a class of solutions, each requiring exactly m+n steps to triangulate the pair. Existing 
methods (both rigorous and heuristic) for extracting a particular solution from this group are reviewed, 
and a new heuristic based on inter-contour coherence is proposed. This heuristic is being used in the 
field of Ultrasonic Non-destructive Evaluation to produce images of flaws in pressure vessels, and its 
performance is shown to compare favorably with methods of greater computational complexity. It is believed 
that this heuris-tic can also be used with success in industrial vision sys-tems where similar contours 
are obtained using a laser range finder. Keywords: Computer Graphics, Triangulation, Heuristic Methods, 
Object Modeling, Graph and Tree Searching. CR Categories: F.2.2, 1.2.8, 1.3.5. 1. INTRODUCTION In this 
paper we describe an efficient heuristic for tri- angulation of the 3D surface formed by spanning a set 
of planar contours. These contours can have arbitrary geometry, but are constrained to be planar. The 
algorithm imposes no restriction on the orientation of the contours in relation to each other, but seems 
to work best when the contours are nearly parallel. Such contours are typical of those one might obtain 
with a computer vision system that uses a laser beam to determine cross sectional data of arbi- trary 
curved objects. In determining the spanning surface formed by these contours there is an inherent assumption 
that the contours belong to a single object. The algorithm as presented in Section 4.3.3 has been used 
for obtaining the surface of flaws in the field of Non-destructive Evaluation. However, the heuristic 
is fairly gen- eral and robust, and is useful in other applications as well, notably industrial vision 
systems. 2. STATEMENT OF PROBLEM The general triangulation problem can be stated as fol- lows. Given 
a set of points in space, determine the most reasonable model for the 3D surface approximated by these 
points. Certainly the simplest model from a computational view would be the approximation of the surface 
by a polyhedron of triangular faces. A set S of n unique 3D points Po,P],..Pn-1 can be clas- sified in 
(at least) one of two ways. Each of these classifi-cations has properties not present in the other, and 
thus pose two distinct triangulation problems. First, S can be composed of n points distributed arbitrarily 
in space with no assumed connectivity between them, as illustrated in Figure 1. This is the general case, 
and presents a very difficult problem in triangulation. Two methods for solving this case have recently 
been pro-posed[1,2], and are reviewed in section 3.  2 o o + Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct 3 ° commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. 0 5 Figure 1. (~) 1982 ACM 0-89791-076-1/82/007/0069 
$00.75 Figure 2. Alternately, S can be composed of n points distri-buted onto a number of parallel 
planar contours, as seen in Figure 2. This is a very important special case, one which has been studied 
extensively[3,4,5,6]. A methodology for defining a class of solutions for this case is given in section 
4, along with a review of methods, both rigorous and heuris- tic, for extracting a particular solution 
which has certain desirable features. A new triangulation method has been developed by the authors, and 
is being used in the field of Ultrasonic Non-destructive Evaluation. A detailed presentation of this 
method is given in section 4.3.3. 3. GENERAL SOLUTION A set S of distinct 3D points placed arbitrarily 
in space can be represented by a graph TIV.A], where the set of ver- tices V corresponds to S, and the 
set of arcs A represents the edges of some polyhedron formed by these points, as shown in Figure 3. Two 
algorithms for triangulating a set represented in this way, i. e., to determine a set of arcs A corresponding 
to a polyhedron of triangular faces, have recently been published. One method, proposed by Boissonnat 
and Faugeras[1 ], takes an iterative approach, making progressively higher-order approximations to the 
surface by recursively sub-dividing the graph and independently triangulating these smaller surfaces 
until an acceptable error level is reached. Another method, proposed by O'Roarke[2] is to shrink the 
convex hull enclosing a set of points S to a polyhedral approximation of the surface. Furthermore, a 
simple polyhedron formed in this way is the minima/ surface area polyhedron for S if no other simple 
polyhedron whose vertex set is precisely S has a smaller surface area. Two justifications could be given 
for choosing the con-vex hull as the starting point. First, the convex hull is the only simple polyhedron 
for its vertices, and is hence the minimum surface area polyhedron for its vertices. Second, physical 
objects have large convex areas which would coin- cide with the hull, so an algorithm starting from the 
hull would be fast and accurate on these objects. The above algorithms cannot be called "optimal" in 
the sense that they include a heuristic step to remove edges of the polyhedron after they have been added 
if such a step will "improve"[21 the surface. Indeed, it has been postu-lated that the problem of finding 
the optimal polyhedral approximation of the object described by a set of 3D points is NP-hard" [2]. However, 
restricting the kind of surfaces considered will not only make the task of finding the best triangular 
fit possible, but also can make available heuristic 1 2 4 1 Figure 3. methods which, while not finding 
the "optimal" surface, are fast and reasonably accurate. 4. SURFACE BETWEEN PLANAR CONTOURS A very practical 
and useful restriction is that points in S be distributed onto parallel planar closed contours. A methodology 
for solving the triangulation problem for this case is presented here, along with two "optimal" solutions. 
Heuristic methods which provide rapid, but not optimal, solu- tions are also presented. 4.1. METHODOLOGY 
The method for describing the outer surface spanning two adjacent planar contours is as follows. Let 
the "upper" contour be described as a series of m distinct points Po,PI.,.,Pm_]; the "lower" contour 
is described as a series of n distinct points QO,Q],..Qn_I. Unlike the general case described earlier, 
connectivity of the points is implied by their ordering. Furthermore, the contours can have no points 
in common. A contour segment is a linear approximation of the curve connecting consecutive points. An 
e/ementary tile is a triangular face composed of a single contour segment and two spans connecting the 
endpoints of a contour segment with a common point on the adjacent contour. The spans will be designated 
as "left" and "right" for obvious reasons (Figure 4). The set of elementary tiles which define a surface 
will, by definition, satisfy two constraints: (1) Each contour segment will appear in exactly one ele-mentary 
tile. The set will therefore contain exactly m+n tiles. (2) If a span appears as the left (right) span 
of some tile in the set, it will appear as the right (left) span of exactly one other tile in the set. 
   Po' . p'~-.~.~ contour / ~'~.~ segment/ n-1 ~  +o,,j/+..,. ; Figure 4, It has been shown]-3] 
that a set of tiles not consistent with these criteria cannot completely describe the outer surface between 
adjacent planar contours. A set of tiles which satisfies them is called an acceptable surface. The set 
of all acceptable surfaces for the contour pair Po,P]..+Pm_1 and QO,Q], Qn_j can be represented by a 
directed graph GIV,A ~. The set of vertices I/ corresponds to the set of all possible spans between the 
upper contour and the lower contour. The set of arcs A corresponds to all pos- sible elementary tiles. 
An arc will be incident from the ver-tex which represents the left span of the tile, and will be incident 
to the vertex which represent the right span of that tile. An arc Ivij,vij+l] joining columns j and j+l 
is called horizontal. An arc [vij,vl+],j] joining rows i and i+ 1 is called vertical. Such a graph is 
given in Figure 5. An acceptable surface is a path from vertex Voo to ver- tex Vmn. The number of acceptable 
surfaces for a contour pair of m and n points, respectively, is a function A~m,n], which can be reoursively 
determined as follows: Aim,n] = A[(m-l),n] + A[m,(n-1)] Aim,l] = i for allm All,n] = 1 for alln Hence 
i 0 1 2 m-1 1 ,,,.+~ --++ 2 v v vi j -# i ~ I "~ I T.? T T Aim,n] = ~(m-1) + (n-1)l~ /m-l~! (n-l)! 
A plot of Aim,n] is given in Figure 6 illustrating just how rapidly this function increases with m and 
n. With such a large number of acceptable surfaces avail-able, a method for selecting the "most acceptable" 
surface must be determined, and clearly an exhaustive search is prohibitively slow, even for exceedingly 
simple contour pairs. Consider a function p:A -* R mapping the set of elemen- tary tiles onto the real 
numbers. Associated now with every arc a is a weight ~a, which is the the value of ~ for that tile. The 
path of an acceptable surface will have exactly m+n arcs al,a2,...am+ n. The path chosen will be such 
that the function rn+n k= satisfies some predefined constraint(s). Two classes of functions are presented: 
First, functions which "optimize" some characteristic of the surface generated; second, functions which 
are computationally expedient and are rea-sonable approximations to the optimal case. 4.2. OPTIMAL METHODS 
Fuchs, etal.[3] defined the optimal surface between two parallel planar contours to be an acceptable 
surface such that ~ is minimal. Such a path is the minimal weighted path. Fuchs defined ~ to be the surface 
area of an accept- able surface, the optimal surface being such that this area is minimal. This is, for 
the important special case considered here, the minimal surface area polyhedron sought by thB O'Roarke 
method. Alternately, Keppel[4] proposed that the optimal sur-face be an acceptable surface such that 
~ is maximal. Such a path is the maximal weighted path. Keppel defined to be the enclosed volume of an 
acceptable surface. Sur- faces defined in this way have many applications, particu- larly in the fields 
of radiation therapy and plastic surgery. i + Jl la Figure 5. Figure 6. Neither the maximal nor the 
minimal weighted path can be found using local decision-making only. Efficient methods of global graph 
searching are needed, and many have been proposed[3,7]. However, approximations to these optimal methods, 
based on local heuristic decisions and never requiring more than m+n steps to triangulate a contour pair, 
have been shown to work very well on a large class of sur-faces. 4.3. HEURISTIC METHODS If some knowledge 
of the nature of the object being modeled is obtainable, then heuristic methods of triangula-tion, never 
needing more than m+n steps for a contour pair can be used. These methods come into play often when speed 
of computation is of higher importance than "optimal" results. Heuristic methods can be described in 
the same way as the optimal algorithms in the previous section. These methods still seek an acceptably 
weighted path from Vo0 to Vmn, but do not require that this path be maximal or minimal in any sense. 
The weighting function, however, must be: (a) based on local constraints only; (b) Computationally inexpensive. 
  Two known heuristic methods are reviewed in sections 4.3.1 and 4.3.2, and a new heuristic is introduced 
in section 4.3.8. 4.3.1. Keppel's Approximation An approximation for the maximal volume method[4] can 
be determined as follows. Consider a function ;c h defined only for horizontal arcs and another function 
~v defined only for vertical arcs. We will define these functions as follows. Associated with every horizontal 
arc [vij,v/j+l ] is a tetrahedron  T~" IPjPj+ iQlOq} where Oq is any point lying on the interior of 
the lower con-tour. The value of ~h for this arc is defined to be the volume of T/~. Similarly, every 
vertical arc [vij,vi+l,j] has a corresponding tetrahedron T~j {PjQ/Q/+ 10/~ where Op is any point lying 
on the interior of the upper con-tour. The value of ~v for this arc is defined to be the volume of T/~.. 
The path of an acceptable surface will have m hor-izontal arcs ho,h],...hm_ 1 and n vertical arcs Vo,Vl,...Vn_ 
]. Define m--1 n--1 ~= j~=o~hJ+/~=o~V I This function is locally maximized as follows. At any vertex 
v U, compute ~h = VOI T~j and ~o v = vol T~" If ~h>~v mark the horizontal arc, and move on to consider 
vertex v/,j+ 1. Else mark the vertical arc and move down to consider vertex vl+lj. In this manner, a 
locally maximal weighted path can be found in exactly m+n steps. This method must be performed on sequences 
of con-tour points forming exclusively convex or concave seg-ments. Consequently, significant preprocessing 
must be per- formed on a contour pair before triangulating. Furthermore, the algorithm seeks to maximize 
the tetrahedral volumes for convex segments, while minimizing these volumes for concave segments. These 
implementational issues will be addressed further in section 4.3.4. 4.3.2. Christianson Heuristic It 
has been pointed out by Christianson and Seder-berg[5] that heuristic methods perform best on contour 
pairs which are coherent in size and shape, and mutually centered. More importantly, contour pairs not 
satisfying these criteria can be transformed such that they do, and the application of heuristics to 
such a transformed pair pro- duces good results. One method of achieving size, shape, and orientation 
coherence is to transform a contour pair to lie within a unit square centered at the origin. An approximation 
for the minimal weighted path, operating on contours transformed in this manner is as fol-lows. The weight 
~a for an arc is defined to be the length of the span, and the tile with the shorter span is selected. 
In this way, the function ~ is locally minimized at each vertex. 4.3.3. A New Heuristic A further improvement 
of coherence between a contour pair can be achieved by transforming the contours such that the perimeter 
of each is exactly equal to 1. A new triangu-lation method proposed by the authors is designed to operate 
on transformed contours of this type. This method is currently being used in Ultrasonic Non-destructive 
Evalua-tion of pressure vessels. Sample images are presented in Section 5. ~a of an elementary tile is 
defined here to be the length of the contour segment included in the tile, divided by the perimeter of 
the contour on which that segment lies. Two facts follow immediately from this definition. First, for 
the m horizontal arcs and n vertical arcs of an acceptable surface; m ~h = ~ahl~-1 ---- 1~ /= and n 
~v = j~=l~aVj = 1 Second, ~ = ~h+~ v is exactly equal to 2 for all acceptable surfaces. While ~ has 
equal value for all acceptable surfaces, all such surfaces are not "equally acceptable". The order in 
which the contour segments are visited is the determining factor, and has physical significance. For 
a pair of contours of close size and shape coherence, triangulation should proceed along each contour 
at approximately the same rate, eliminating the formation of surfaces possessing artificially exaggerated 
surface tension. Left to its own devices, a physical body will always assume the shape of least ten-sion, 
justifying such an approach. With this in mind, the heuristic can be stated as fol-lows: Tiles are added 
to the surface in such a way that the absolute difference between sum of the horizontal weights and the 
sum of the vertical weights is minimized at all times. When an acceptable surface is reached, this difference 
will be O. The converse is not necessarily true. A demonstration of a typical step in this method is 
illus- trated in Figure 7. #~h represents the normalized distance along the upper contour visited. ~v 
is similarly defined for the lower contour. The next tile to be added to the surface can only be PjPj+IQi 
or PjQ/Qi+1. PjPj+IQi is chosen if l~h'+~h--~v'[ < I~v"l-~v--ff#h ' ] 4.3.3.1 In this case, j is incremented, 
and we proceed along the upper contour. Otherwise, PjQJQi+] is chosen, i is incre-mented, and we proceed 
along the lower contour. This method will require exactly m+n steps to perform the Computer Graphics 
Volume 1 6, Number 3 July 1982 Figure 7, complete triangulation of a contour pair. Another heuristic 
conceptually similar to this one (and seemingly simpler) is to add tiles based on cumulative perim- eter 
visited only, rather than on the absolute difference. The two methods can be related, and justification 
for choosing one over the other can be made, in the following manner. If cumulative perimeter is the 
only criterion, the tri-angulation process can be stated thus, using the same notation as above. PjP.i+~Qi 
is chosen if ~h'+~Ch < ~v'+~v; else choose PjQ/QI+p Rearranging terms and taking abso-lute value yields 
the inequality I~h "l-~h--~2v' l < I~vI+ 4.3.3.2 Rearranging the right hand side of inequality 4.3.3.1 
pro-duces the new inequality  t't'h'+~h--'+,,"l < l~,,+C'+,;-">h')l+ 4.a.3.a Clearly inequalities 4.3.3.2 
and 4.8.3.3 are identical up to the term ~v'--~Ph'. In the ideal case, when the formation of the outer 
surface proceeds along each contour at exactly the same rate, this term is precisely zero, and the heuristics 
are identical. However, in triangulation this ideal case is hardly, if ever, obtainable, and this term 
must be incor-porated into the decision. Hence the method of absolute difference is chosen. The choice 
of which pair of points from which to begin the process must also be given consideration. Truly optimal 
methods regard the graph G[V,A] as a toroid[3], wrapping around in both the horizontal and vertical directions 
. In this way, any point pj on the upper contour and qj on the lower can form the anchor span for the 
triangulation. An expedient alternative to such a procedure is to select that point on each contour having 
the minimum x value (in the transformed coordinate system) to be the endpoints of the anchor span. 4.3.4. 
Comparison of Methods Some very desirable features of this new heuristic become evident when it is compared 
to other methods. The most striking of its advantages is that, while per- forming computation using local 
information only, previous actions influence present decisions. Specifically, a local tri- angulation 
decision is based not only on the lengths of the contour segments involved, but also on the distance 
previ-ously traveled along each contour. It is a heuristic with memory, in contrast to all other heuristic 
methods hitherto published. The amount of contour preprocessing inherent to any method must also be considered. 
The complexity of such a step should not be dependent on any feature of a contour other than the number 
of points approximating it. If this is so, preprocessing may be done in one traversal of a contour, as 
it is with this new heuristic. However, the Keppel method, whose segmentation of a contour into concave 
and convex runs is a function not only of the number of points approximating a contour, but also of the 
shape of that contour, requires multi-pass preprocess-ing. Decomposition of a contour into concave and 
convex "runs" is a well-defined, but tedious operation. First, the convex hull enclosing a contour must 
be obtained by elim-inating concave segments. Then each of these concave segments must be recursively 
subdivided into convex sub-sets, each of which must be subdivided in turn, if neces- saP/. The Fuchs 
method, while being a truly optimal method requiring no special handling of contours, requires a large 
number of steps [3]+The number of steps is of the order of ~rnn [ log2m ] (m<n) and is bounded by ~log2m](2mn 
+ m) + 3ran + m 2. This is significantly more than the m+n steps required to tri- angulate a pair using 
heuristic methods, and hence is not appropriate for applications where speed is of primary importance. 
Finally, this new heuristic imposes no restriction on how contours must be oriented with respect to one 
another. The constraint of having contours mutually centered, or even parallel, is eliminated, for the 
triangulation decision is based solely on features of the contours themselves, and not on the spans between 
them. This is clearly an advantage, as non-parallel contours would be a more natural surface approximation 
than parallel contours for a large class of objects. Consider the object in Figure 8. The most natural 
cross-sectional description of it is a set of contours fanning out in constant incremental angle with 
respect to a pivot located at the base, as in Figure 8a. This new heuristic would function well without 
modification on such a set. However, all other existing triangulation methods would require a set of 
contours such as that of Figure 8b. If such contours were not chosen properly, the pivot point would 
be missed, producing a poor description (and image) of the object. 5. EXAMPLES Figure g illustrates a 
set of contours typical of a flaw volume produced during ultrasonic non-destructive evalua-tion in the 
laboratory. These contours are obtained by thresholding envelope detected data after it has been pro- 
cessed using a synthetic aperture[6]. The inter-contour separation is typically 1/S of a wavelength. 
This interval is approximately 0.0125" for lOMhz, sampling. Contours obtained in this manner exhibit 
close size and shape a. b. Figure 8.   Computer Graphics Volume 16, Number 3 July 1982 coherence, 
and are natural candidates for triangulation. A complete triangulation of the outer surface of this flaw 
as obtained with this new heuristic is shown in Figure 10. A shaded picture of the newly-acquired surface 
is shown in Figure 11. Another application of this method in NDE is triangula- tion between successive 
lines of a raster data grid [u.v,f(u.v)], as shown in Figures 12-14. A pseudo-color image of an XZ slice 
through an inspection volume, showing the front and back surface echoes, is given in Figure 12. Figures 
1 3 and 14 show the image of bottom-drilled holes in shading and pseudo-color, respectively. The shading 
in Fig- ure 1 a is based on unbiased gradient estimation[8]. Figures 11-14 were produced on a RAMTEK 
9351 (512 x 512 x 12) display controlled by a VAX 11/780 running UNIX. 6. CONCLUSIONS We have presented 
the general triangulation problem, and have presented a methodology for defining a class of J Figure 
9. solutions for the practical case of planar contours. A new heuristic incorporating memory features 
hitherto present only in so-called "optimal" algorithms has been described. Unlike the optimal methods, 
however, this new heuristic requires only m+n steps to triangulate a contour pair. This new heuristic 
has been shown to perform well in Non-destructive Evaluation, and it is believed it would work equally 
well in industrial vision systems using non-parallel contours. 7, ACKNOWLEDGEMENT This work has been 
supported in part by the Nuclear Regulatory Commission under contract NRC-05-75-182.  REFERENCES [1] 
Boissonnat, J.D. and Faugeras, O.D., "Triangulation of 3D Objects," PROCEEDINGS OF THE 1981 INTERNATIONAL 
JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, 658-660. [2] O'Roarke, Joseph, "Triangulation of Minimal 
Area as 3D Object Models," PROCEEDINGS OF THE 1981 INTERNA-TIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLI-GENCE, 
664-666. [3] Fuchs, H., etal., "Optimal Surface Reconstruction from Planar Contours," COMMUNICATIONS 
OF THE ACM, XX-10 (October 1977), 69,3-702. [4] Keppel, E., "Approximating Complex Surfaces by Tri-angulation 
of Contour Lines," IBM JOURNAL OF RESEARCH AND DEVELOPMENT, XIX (January 1975), 2- 11. [5] Christianson, 
H. and Sederberg, T.W., "Conversion of Complex Contour Line Definitions into Polygonal Element Mosaics," 
COMPUTER GRAPHICS, XIII,2 (August, 1 g78), 187-192. Ce] Ganapathy, S., etal., "Ultrasonic Imaging Techniques 
for Real-time In-service Inspection ot Nuclear Power Reac- tors", Nuclear Regulatory Commission Report 
NUREG/CR-2154, September, 1 g81. [7] Nilsson, Nils, PRINCIPLES OF ARTIFICIAL INTELUGENCE, (Palo Alto: 
Tioga Publishing Co.) 1 gso. [8] Horn, Berthold K. P., "Hill Shading and the Reflectance Map," PROCEEDINGS 
OF THE IEEE, LXIX, 1 (January, 1981) 14-47. Figure 10.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801265</article_id>
		<sort_key>77</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Creating volume models from edge-vertex graphs]]></title>
		<page_from>77</page_from>
		<page_to>84</page_to>
		<doi_number>10.1145/800064.801265</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801265</url>
		<abstract>
			<par><![CDATA[<p>The design of complex geometric models has been and will continue to be one of the limiting factors in computer graphics. A careful enumeration of the properties of topologically correct models, so that they may be automatically enforced, can greatly speed this process. An example of the problems inherent in these methods is the &#8220;wire frame&#8221; problem, the automatic generation of a volume model from an edge-vertex graph. The solution to this problem has many useful applications in geometric modelling and scene recognition.</p> <p>This paper shows that the &#8220;wire frame&#8221; problem is equivalent to finding the embedding of a graph on a closed orientable surface. Such an embedding satisfies all the topological properties of physical volumes. Unfortunately graphical embeddings are not necessarily unique. But when we restrict the embedding surface so that it is equivalent to a sphere, and require that the input graph be three-connected, the resulting object is unique. Given these restrictions there exists a linear time algorithm to automatically convert the &#8220;wire frame&#8221; to the winged edge representation, a very powerful data structure. Applications of this algorithm are discussed and several examples shown.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>Representations, data structures, and transforms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332801</person_id>
				<author_profile_id><![CDATA[81100482576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Hanrahan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Image Processing Laboratory, University of Wisconsin, Madison, Wisconsin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baumgart, B. G. Geometric Modelling for Computer Vision. Stanford Artificial Intelligence Project, Research Memo No. 249. Computer Sciences Department, Stanford University. 1974.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Catmull, E., and Clark, J. Recursively generated B-spline surfaces on arbitrary meshes. Computer Aided Design. 10(6) 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Demoucron, G., Malgrange, Y., and Pertuiset, R. Graphes plainaires: reconaissance et constructien de representations planares topologiques. Rev. Francaise Recherche Operationelle. 8 pp. 33-47. 1964.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Edmonds, J. A Combinatorial Representation for Polyhedral Surfaces. Notices Amer. Math. Soc. 7 p. 646 1960.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Falk, G. Computer Interpretation of Imperfect Line Data as a Three Dimensional Scene. Stanford Artificial Intelligence Project, Research Memo No. 132. Computer Sciences Department, Stanford University. 1970.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804395</ref_obj_id>
				<ref_obj_pid>800135</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Filotti, I. S., Miller, G. L. and Reif J. On Determining the Genus of a Graph in O(vO(g)) Steps. Eleventh Annual Symposium on the Theory of Computing. pp. 27-37 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Ganter, M. A. Techniques for Converting Wire-Frame to Solid-Geometric Representations. Masters Thesis. University of Wisconsin - Madison. 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Liardet, M. PICAX - Polyhedron Input to the Computer Using an Axonometric Drawing. Interactive Techniques in Computer Aided Design. pp. 344-355 1978.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Markowsky, G. and Wesley, M.A. Fleshing out Wire Frames. IBM J. Res. Develop. 24(5):582-597 1980.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Rubin, F. An Improved Algorithm for Testing Graph Planarity. IEEE Trans. Comp. C24:113-121 1975.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Waltz, D. L. Generating Semantic Descriptions from Line Drawings of Scenes with Shadows. Ph.D. thesis. Department of Electrical Engineering, Massachusetts Institute of Technology. Cambridge, Mass.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Creating Volume Models from Edge-Vertex Graphs Patrick M. Hanrahan Image Processing Laboratory University 
of Wisconsin Madison, Wisconsin ABSTRACT The design of complex geometric models has been and will continue 
to be one of the limiting factors in computer graphics. A careful enumeration of the properties of topologically 
correct models, so that they may be automatically enforced, can greatly speed this process. An example 
of the problems inherent in these methods is the "wire frame" problem, the automatic generation of a 
volume model from an edge-vertex graph. The solution to this problem has many useful applications in 
geometric modelling and scene recogni- tion. This paper shows that the "wire frame" problem is equivalent 
to finding the embedding of a graph on a closed 6r~entable surface. Such an embedding sa- tisfJes all 
the topological properties of physical volumes. Unfortunately graphical embeddings are not necessarily 
unique. But when we restrict the embedding surface so that it is equivalent to a sphere, and re- quire 
that the input graph be three- connected, the resulting object is unique. Given these restrictions there 
exists a linear time algorithm to automatically convert the "wire frame" to the winged edge representation, 
a very powerful data structure. Applications of this algorithm are discussed and several examples shown. 
 Computer Review Categories: 1.2.10 [Ar- tificial Inteligence] Vision and Scene Understanding -Representations, 
data structures and transforms; 1.3.5 [Computer Graphics] Computational Geometry and Ob- ject Modelling 
-Curve, surface, solid and object representations. Permission to copy without ~e all or part of this 
material is granted provided that the copies are not made or distributed mr di~ct commercial advantage, 
the ACM copyright notice and the title of the publicatio~and iB date appor, and notice is given that 
copying is by ~rmission of the Association mr Computing Machinery. To copy otherwise, or to republish, 
requires a ~e and/or specific permission. (~) 1982 ACM0-89791-076-1/82/007/0077 $00.75 i. INTRODUCTION 
 The increasing realism of computer generated images depends on the sophisti- cation of the models used 
to represent physical objects. Today there are basi- cally two types of solid modelling sys- tems. The 
first specifies volumes by boolean combinations of primitive volume elements. The second forms a volume 
by patching together surfaces to form a net- work that encloses the volume. These basic methods are independent 
of the geometry of the building blocks. Algebraic functions are particularly good for forming the volume 
elements needed in the first ap- proach whereas polygons or parametric sur- faces are better suited for 
the second ap- proach. In this paper we will concentrate on the second, or graph-based approach. [Throughout 
this paper'the word "graph" is interpreted in the graph theoretical sense and not in the pictorial sense. 
Similar- ly, "vertices" and "edges" refer to com- ponents of a graph, not to points or line segments.] 
We will use several results from topological graph theory to develop criteria for establishing a topologically 
well-formed model of a solid object. In particular criteria are presented for determining when an edge-vertex 
graph will uniquely specify a three dimensional volume model. A linear time algorithm is then presented 
for determining the faces comprising an object without holes from its graph. The major virtue of this 
algo- rithm is that no geometric information is used to construct the surfaces. Therefore, although the 
results have been applied to a polyhedral modelling system, these same notions are applicable to systems 
based on different geometric surfaces. Not only do these results provide in- sight into what conditions 
are necessary for complete and well formed models but they also provide techniques whereby com- plicated 
geometric objects can be created simply and efficiently. Topological descriptions are especially important 
in object recognition where general proper- ties such as adjacency and connectedness provide more useful 
information than the geometric information. This was the basis Computer Graphics Volume 16, Number 3 
July 1982 Figure I. Wire frame reconstructions of twosimilar objects. of an elegant method for inputting 
po- lyhedron from a single view described in Falk[1970] and further developed by Liar- det[1978]. The 
basic idea of these algo- rithms is to infer the topological infor- mation from a graph constructed from 
a line drawing. The faces of the graph, if assumed be be planar, provide constraints that can be used 
to propagate geometric information throughout a model. However, inferring the faces has remained a diffi- 
cult problem. Liardet's[1978] face con- struction algorithm is novel but relies on an unproved heuristic. 
Another different, but also purely topological approach based on a different group of heuristics was 
developed by Ganter[1981]. Taking a some- what different approach, Markowsky and Wesley[1980] have published 
a "wire frame" to surface algorithm which explicitly uses geometric information by forcing the final 
faces be planar. The above investigators all have published usable algorithms but do not address directly 
whether the faces that are generated are unique. The com- plexity of their algorithms also have not been 
analyzed, but in' all cases they ap- pear to be nonlinear functions of the problem size. Figure i. shows 
example "wire frame" diagrams of two similar objects. Both ob- jects are formed from a graph obtained 
by deleting several edges from an otherwise ambiguous graph, that of a hypercube (see the top figure). 
The resulting graph is then converted to a set of faces. With this surface information it is possible 
to remove hidden portions of the objects Furthermore, the surfaces of the objects have been defined such 
that a solid model is created. This is demonstrated by our ability to calculate the volume of each object. 
 2. VOLUMES, SURFACES AND GRAPHS. A solid or volume model defines an object by a set of points in three 
dimen- sional space. All points are classified into one of three categories: those points that are considered 
part of the object, or inside of it; the set of points that are not part of it, or outside of it; and 
those points which lie on the boundary between points inside and outside of the volume. Of course not 
all possible volumes correspond to physically realiz- able objects. Any physical object will occupy only 
a fixed volume. Also for our purposes individual objects are separate and distinct. That is, for each 
object, all points within it can be connected by a path which does not leave the interior of the object. 
Finally, for physical and mathematical reasons, this path should never become infinitesimally thin. This 
prevents the construction of objects by joining two volumes along a single edge or at a vertex (see Figure 
2). In terms of topology, the volume is said to be com- pact, connected and regular. The simplest representation 
of volumes is a three- dimensional array. Each element would correspond to a position in space and would 
indicate to what object it belongs. This array would embody both the topology and the geometry of the 
object at the ex- pense of occupying an incredibly large of amount memory. Much of the current research 
in solid modelling involves find- ing more economical methods to represent volumes. One way to limit 
the amount of stored information is to reduce the dimensionali- ty of the description. The most common 
method is to describe the volume as small- er surface patches joined along their lim- its. Fortunately, 
it is possible to join surface patches in ways that guarantee the above topological notions. To insure 
that the volume is finite, enclose it with a surface of finite area. A surface can be closed in several 
ways; Figure 3. shows how to close a surface to form a sphere and a torus. Matched edges (both labelled 
with the same letter in Figure 3.) are al- ways joined by placing the head of one Figure 2. The volumes 
of these objects are not regular, their surfaces are not two dimensional manifolds and their graphs are 
not three-connected.  G Q G Q Figure 3. The normal form of the outer edge labellings of a sphere 
(left) and a torus (right). alongside the tail of the other and vice versa. The direction of an edge 
is very important and is denoted by assigning to each edge label a "sign" based on whether it is aligned 
or antialigned relative to a counterclockwise traversal of the outer perimeter of the surface. Edges 
which are aligned in the same direction are labelled a, and edges which lielin the opposite direction 
are labelled, a-. The simplest outer labelling of a surface is called its normal form. The famous classification 
theorem of topology allows surfaces to be grouped by their outer edge labels. Every surface is topologically 
equivalent to one of following three types: ala 1 alblalbl ...  apbpapbp -i -i -i ala I a2a 2 ... 
aqaq  The first form is like a sphere. The second like a sphere with p handles (a handle is equivalent 
to a hole in a sphere --so a torus is classified as a sphere with a single handle). The third form is 
like a sphere with a crosscap (a crosscap can be pictured by pulling a section of the surface through 
another part --as in a Klein bottle). The first and second classifications are examples of orientable 
surfaces; the third an nonorientable sur- face. The most important trait common to all orientable surfaces 
is that they are two-sided, as a result space is divided into an interior and an exterior region. Nonorientable 
surfaces do not have this property and therefore should not be used to describe physical volume models. 
 Finally, the condition that a volume be regular is satisfied if the surface surrounding it is a two 
dimensional mani- fold. Two dimensional manifolds have the property that a small sphere centered at any 
point on the surface cuts from the surface a region topologically equivalent to a disc (this means that 
any non self- intersecting curve on the disc can be shrunk to a point without disconnecting the surface). 
This condition is not sa- tisfied if the surface contains singular points at any edges and vertices. 
Figure 2. shows volumes which are not regular, in all cases the surrounding surfaces are not two dimensional 
manifolds.  In summary, a surface which is fin- ite, closed, orientable, and a two- dimensional manifold 
will enclose a volume that is compact, connected and regular. The final step in reducing the dimen- 
sionality of the object description is to formulate conditions when a surface can be represented as a 
simple graph of edges and vertices. Perhaps it is best first to show that we can make more interesting 
graphs than that given by the normal form of the surface. At this point the surface has only a single 
patch (which we will often call a face), 2p edges and 2p vertices. But the total number of edges, vertices 
and faces can be increased by subdividing either an edge or a face. Subdividing a face results in a new 
face and a new edge, subdividing an edge results in a new ver- tex and a new edge. Although these subdi- 
visions increase the total number of edges, vertices, and faces they do not change the classification 
of the surface. This invariance is elegantly expressed with the Euler equation: V-E+F = 2(M-P) Where: 
 F is the number of faces. E is the number of edges. V is the number of vertices. M is the number 
of surfaces. P is the number of handles.  The right hand side is determined by the outer perimeter 
of edges alone and is called the characteristic of the surface. The quantity, M-P, is called the genus 
of the surface. With these constructions then, it is possible to construct a more complicated graph of 
edges and vertices. But more importantly this graph has, in the process of being formed, been laid out 
on a particular surface. In graph theoret- ical terms we have formed an embedding of the graph on a surface. 
Intuitively, a graph is embedded on a surface if it is possible to draw the graph so no two edges cross 
except at a vertex. The simplest examples are graphs embeddable on a sphere (or equivalently in a plane). 
Figure 7. shows a graph of a cube and its embedding in a plane. (This type of drawing is called a Schlegel 
diagram, it is capable of showing an embedding in a plane by stretching one face to occupy the entire 
background plane.) Once a graph has been embedded we can define faces as circuits that do not contain 
interior edges. Some other properties of embeddings are that all the edges are part of only two faces 
and that the number of faces that meet at every vertex is equal to the number of edges that are incident 
to that vertex. In summary, an embedding of a graph on an orientable surface represents a to- pologically 
correct volume model. 3. GRAPHICAL EMBEDDINGS. To convert from a surface represented by patches to 
a graph representation is straightforward, since the edges and ver- tices are already given. But to convert 
from a graph representation to a surface representation involves constructing an embedding of the graph 
on a surface. This is considerably more difficult. There are two major problems. The first is to con- 
struct candidate faces from closed cycles of edges that are consistent with the to- pology of embeddings. 
The second problem is to show when and if the embedding gen- erated from the graph is unique. All the 
embeddings of a graph can be enumerated by the following construction developed by Edmonds[1960]. Take 
a graph, G, consisting of a set of vertices, V(G), and a set of edges, E(G), and assign a number, 1..n, 
to each vertex. Represent each edge as two directed edges, (i,k) and (k,i). For each vertex, i, denote 
the set of all vertices neighboring it as: V(i) = (k(=V(G) : (i,k) (=E(G)) Denote the number of elements 
in each of these sets by n.. This set can be arbi- trarily ordered; ~he total number of pos- sible orderings 
is (n. ~) !. For each or- dered set define an opelr~tor, o., which is a cyclic permutation of the ordered 
set, V(i~---E-dmonds" theorem states that there is a one-to-one correspondence between the possible order 
ings of the neighbors of each vertex and the embedding of the graph on an orientable surface. The faces 
of the graph are generated by forming orbits with the cyclic permutation operator From any edge, (i,k), 
the next edge in P~e orbit is (k,p~(i)). If the sets are fin- ite the orbits must be closed. These or- 
bits define faces of the graph which can be combined with others to create a sur- face. The resulting 
surface will be orientable if the faces are always joined along the two edges (i,k) and (k,i), and closed 
since there are no unmatched edges. Figure 4. shows these sets for an embed- ding of a tetrahedron. 
The Edmonds" description of a embed- ding can be used to generate other embed- dings by reordering the 
neighbors of each vertex. The characteristic of the surface can be determined by counting the number 
of unique orbits, and then substituting this along with the edge and vertex counts into Euler's relation. 
But there exists an embedding for each permutat~-o-n of the neighbor sets. For a graph with n ver- tices 
this-T~lies that there may be as many as n! n different embeddings. This has two consequences, any method 
of generating the surfaces that searches over all plau- sible embeddings is not efficient, and second, 
and more importantly, a graph does not specify a unique embedding. This may The Vertex Set I v(G) 
{i, z, 3, 4} The Edge Set E(G). {(I,2), (t,~), (I,4), (2,3), (2,4), (~,,4)) The Ordered~Neighbors Sets 
v(,l~ {4,3, 2} 2 4 v~2).{i, ~, 4} v(3)o {i,4, 2} v(4). {i,2, 3) The Foce Orbit Set F- {(I, 2,3), (t,:5,4), 
(t, 4,~), (2,4,3)} Figure 4. Edmonds" combinatorial description of a tetrahedron. The sets, V(i) contain 
the neighbors to each ver- tex. Corresponding to each ordering of these sets there exists a set of faces, 
F, formed with the cyclic permutation operator.  result in several different volumes being created 
by the same graph. For a graph to uniquely specify an object some constraints must be placed on the 
surfaces on which it is embedded. One set of constraints can be derived from the genus of the surface. 
It is not difficult to prove that for a given graph there are always well defined minimum and maximum 
characteristic surfaces that allow proper embeddings. Finding embeddings on the minimum characteristic 
surface has the ad- vantage that it is the simplest surface (in terms of holes or handles) that the 
graph can define. Edmond's permutation technique allows this surface to be found by searching through 
all the possible ord- erings of neighbor set of each vertex -- but this involves searching n! different 
permutations. More recent ~esearch by Filloti et al. [1979] suggest alternative algorithms for testing 
if a graph can be embedded upon aO~u~face with a given genus in time of O(n -9;), where g is the genus 
of the surface. Both these algorithms will generate embeddings; the first will also test whether that 
embedding is unique; but neither directly address the important question as to when a single embedding 
exists. The general theoretical problem of generating graphs with a single embedding is largely unsolved. 
But in the case of planar graphs, that is graphs that can be embedded on a sphere and therefore whose 
corresponding volumes do not contain any holes or handles, there exists a very sim- ple criterion: Except 
for an eversion every planar graph has a unique set of faces if and only if it is three-connected.  
Singly connected Doubly connected Triply connected Figure 5. The possible planar emheddings of singly, 
doubly and triply connected graphs. An "eversion" will turn an object inside- out. This ambiguity can 
always be removed if the object of finite volume is chosen. N-connectedness forces there to be n in- 
dependent paths (i.e. sequences of edges that do not share any vertices) between any two vertices. Figure 
5. contains ex- amples of singly, doubly and triply con- nected graphs. Notice that the embedding of 
singly connected graph, Figure 5a., ex- ists in two forms generated by rotating the edge about the cut 
vertex. Similarly the biconnected graph, Figure 5b., also has multiple embeddings. However once the graph 
becomes triply connected, Figure 5c., there is only a single correct embed- ding in the plane. This condition 
on con- nectedness also removes several potential singularities associated with irregular volumes. As 
mentioned previously, volumes should not be joined along regions that are infinitesimally thin. This 
can occur if two volumes are joined at a vertex or along an edge. But if a graph is formed by either 
of these operations then it is not three-connected since there are at most two independent paths between 
the two ori- ginal graphs. 4. EMBEDDING A PLANAR GRAPH We have shown for certain graphs a unique set 
of faces exist and can be gen- erated by embedding the graph in the plane (forming a volume similar 
to a sphere). The problem of testing for planar embed- dings has received a great deal of atten- tion 
in the literature. Fortunately there are linear time algorithms that test for graph planarity. The algorithm 
presented here is a modification of one due to Demo- croun et. ai.[1964] (see also Ru- bin[1975]). Their 
algorithm inputs a graph and determines whether it is planar by constructing all its faces. The modifi- 
cations implemented in this work also in- sure the the graph is triply connected and further generate 
a data structure, the winged edge, which can be used for addi- tional processing. The winged edge (Baumgart[1974], 
and see Figure 6.) is a particularly good choice for embedding problems and volume modelling since it 
models the topology of a volume enclosed by an orientable sur- face. The topological information is con- 
tained in each edge record. An edge con- nects two vertices, separates two faces and contains forward 
and backward pointers to the next clockwise and counterclockwise edge of each face it separates. The 
ver- tex and face records contain pointers to an edge connected to them. Orientability is preserved locally 
by forcing the edge to be followed in opposite directions as its two neighboring faces are traversed 
in a consistent direction. Any number of edges can emanate from a vertex; succes- sive edges can be accessed 
by repeatedly skipping from one to the next. To build models a powerful set of topological operators 
are provided. The first and most basic builds a primitive object which is classified as a sphere (commonly 
named mbfv for "make a body of multiplicity i, containing a single vertex and face"). Other operators 
are based on invariances of Euler's relation and include mfe (for "make a face and an edge" by subdividing 
another face) and mev (for "make an edge and a vertex" by subdlviding an edge). The winged edge representation 
was originally designed by Baumgart to model polyhedra in three dimensions. But is also capable of representing 
arbitrary graphi- cal embeddings. This can be proved by showing that it is equivalent to Edmonds" permutation 
representation. Clearly the winged edge can be used to represent the interconnection matrix of a graph. 
The crucial point is that the ordering of the nccwedge pcwedge EDGE .edge : A edge pedge : ^edge nccwedge 
: ^edge pccwedge : ^ edge pcwedge : ^edge ncwedge : ^edge nv : ~ vertex pv ; ^ vertex nf ; ^ face pf 
: ^ face ¢wedge pecwedge MODEL -VERTEX FACE nmodel : ^model nver tax : ^vertex nfoce : ^face pmodel 
; *model pver tax : ^vertex place : ^face retiring : ^vertex vedgelisf : ^edge fedgellst : ^edge edgering 
: ^vertex xw, yw, zw, w : real aw, bw, cw, d : real focering : ~face Figure 6. The winged edge representa- 
tion. The schematic at the top graphi- cally shows the various pointers associ- ated with each edge. 
The majon data structures in the author's system are shown in the boxes.  I z   ,Izl I I I i i , 
~--~ I   IF i VlG)-[l, 2, 3, 4, 5, 6, 7, 8 } ~'/7 8",, ....... K ...... 4 3  E(G) , {(t,21, (I,41, 
(I,51, (2,3), (2,8), (3,4), 4 (s,7), (4,e). (e,e), ~_.... 2~J (s,s), (6,7), (7,el} ~ Figure 7. The 
intermediate stages in the embedding of a cube in the plane. The edge-vertex graph is shown in the upper-left. 
Dotted lines represent the final embedding. Light lines represent subgraphs that are formed by the embed- 
ding algorithm on its way to the final mbedding. Bold lines represent paths Lnat are added to split 
the faces. The first diagram shows the initial cycle. Each succeeding diagram shows the choice of a path 
(shown in bold) and the new faces (these are numbered as they are formed). Figure 8. An example of a 
more compli- cated wire frame model which is automat- ically converted to the winged edge representation 
by the program described in this paper.  neighbors at each vertex is represented by the "wings" of edges 
incident to that ver- tex. The permutation operators are pro- vided by a function which returns the next 
edge clockwise about a vertex given the incoming edge. These features make it essentially equivalent 
to Edmonds representaton and furthermore imply that constructing graph-based models with the winged edge 
will enforce the topological notions associated with volume models. This along with the ease by which 
dif- ferent elements are accessed is what makes it such a powerful modelling system. The algorithm 
of Demoucron et. ai.[1964] proceeds to construct a sequence of graphs, G~, G., ..., Gf ~, each of . 
I . Z.  which is an admissible subgrap~ of the fi- nal embedding. The first graph is any cy- cle contained 
in the input graph. Each successive graph is formed from its prede- cessor by splitting a face in the 
process forming a new face. The splitting is per- formed by embedding additional edges and vertices from 
the original graph. If at any point in this sequence a path of edges is encountered which cannot split 
some face then by the Jordan curve theorem this path must cross some edge on the graph. The existence 
of such a crossing implies that this embedding of the graph is non- planar. The key to the efficiency 
of this al- gorithm is to insure that at no point in the construction of the embedding a path is added 
that will require a rearrangement of the embedding in the future. Remember that the embeddings of the 
singly or dou- bly connected components are not unique, even though the embedding of the final graph 
will be unique. If the wrong embed- ding was chosen for subgraphs then the procedure would have to backtrack 
and correct the choice. To prevent this time consuming backtracking, only paths that can be embedded 
in a single face are used to split faces. Since this is the only face this path can split now, there 
is no other possible embeddings for this part of the graph and therefore no need to back- track to this 
point. If all the paths al- low multiple embeddings then the choice of path to embed is arbitrary since 
all paths can be embedded in a least two faces now, so in the future there will always be at least one 
face to embed the remaining paths. Figure 7. shows the intermediate stages in the construction of the 
embed- ding of a cube. The first drawing shows the Schlegel diagram for the cube, the correct embedding 
in the plane. The first step in the algorithm is the formation of a cycle. This cycle is formed using 
a depth first search and then embedded into the winged edge representation using the Euler operators 
mkpfb, mkev and mkfe. The resulting subgraph has multiplicity equal i, no holes or handles, 2 faces, 
4 edges and 4 vertices. Subsequent subgraphs are formed by searching from a vertex already contained 
in the subgraph until a path back to the subgraph is found. (This pro- cess is shown in the various diagrams 
in Figure 7.) At each step the number of faces common to the two anchoring vertices is determined (this 
is easily done using the winged edge). When a path is found that has a single embedding it is added to 
the data structure using the operators mkev and mkfe and the process of forming the embedding continues. 
 The above algorithm has been imple- mented in the programming language C run- ning under the UNIX operating 
system on a VAX 11/780. For the models shown at the beginning of the paper the running time is on the 
order of 0.4 seconds. For the model of surface display algorithms and advanced numerical algorithms, 
both of which re- quire topologically correct volume models. With the above algorithm these older data- 
bases can now be updated easily and effi- ciently. shown in Figure 8, which contains approxi- mately 
64 vertices, the total running time is 2.0 seconds. Empirical tests and theoretical analysis (Rubin[1975]) 
show that the expected running time of this al- gorithm is linear in the number of ver- tices. 5. APPLICATIONS 
 5.1 GEOMETRIC MODELLING The basic result of this work is to show what conditions must be placed on 
an edge-vertex graph to convert it to a prop- er surface and eventually to a volume model. Many of these 
conditions have been implicitly known or assumed for some time. For example, it is well known that any 
po- lyhedral solid can be built from tetrahedral primitives. Since a graph of a tetrahedron is triply 
connected and the operation of glueing two triangular faces together is a three vertex operation the 
resulting solid will be triply connected. Therefore almost all known polyhedral models are classic three-connected 
graphs. But because of the uniqueness of the face representation the topology of the faces need not be 
stored explicitly (in contrast to almost all current computer graphics systems data structures) since 
they may be constructed from the edges with the algo- rithm described. The above discussion of volumes, 
sur-  faces and graphs emphasizes the topology of models. Perhaps the greatest advantage of the purely 
topological approach is that it does not constrain itself to a particu- lar geometry. Models with curved 
edges and smooth faces are allowed and do not influence the performance of the algo- rithm. The success 
of the topological ap- proach lies in the constraints it places on the possible geometric interpretations. 
 In the case of polyhedra all the geometry is uniquely specified from the knowledge of the positions 
of the vertices since line and plane equations can be derived from this and the topological information. 
 This basic sequence can be extended to more complicated volumes by introducing more complicated templates 
for curves and surfaces. Alternatively, new topologies may be introduced by subdivision algo- rithms 
which generate additional points (see Catmull and Clark[1978]) on arbitrary topological meshes. Aside 
from its theoretical interest this approach has several potential uses. Historically, many CAD/CAM databases 
have been based on "wire frame" descriptions. This has become inadequate with the advent Besides updating 
older databases, this approach also is very useful in designing new models. For example, con- structing 
the faces of a model is a tedi- ous and error prone process. A particu- larly troublesome problem is 
to guarantee that the parity of the faces is con- sistent. Inconsistent parity may cause otherwise solid 
areas to appear as holes in the resulting model when it is viewed. A model formed by an embedding algorithm 
will automatically have the correct pari- ties assigned to its faces. Finally, an interesting use of 
the above algorithm is in the design of a "smart" hidden line processor. It is pos- sible to pipeline 
all the line segments in a scene to a postprocessor which con- structs a graph from the segments and 
from that graph creates the faces. Since lines segments are interpreted as a components of graph they 
can be sent in any con- venient order without regard to their direction, and without having to be expli- 
citly tagged as part of a face. The con- struction of the faces is a fast linear time algorithm that 
processes the graph into the occluding surfaces which are then used to remove the invisible edges. Such 
 a graphics post-processor simplifies the task of the application programmer im- mensely. 5.2 SCENE RECOGNITION 
 Several workers[Liardet, 1978 and Falk, 1970] have noticed that knowledge derived from its topology 
can be used to infer the geometry of an object. From a single view of an object it is possible to input 
the edge-vertex graph forming the polyhedron and the two dimensional projec- tion of all the vertices. 
If the faces of the object can then be determined and they are assumed to be of a particular geometry 
then they provide a set of constraints that can be used to infer the z coordi- nate. For example, suppose 
the surface patches are assumed to be planar. If we are given the plane equation of a face and the two 
dimensional projection of border points of that face then the three- dimensional coordinates can be determined 
by solving the plane equation. But subse- quent plane equations can be determined whenever the coordinates 
of three coplanar points are known. Thus, by iterating, first solving for plane equations, and then solving 
for more vertex coordinates, the geometry of the object is propagated through the model. Finally, we 
will mention some simi- 8. REFERENCES larites and differences between the above planar embedding algorithm 
and the scene labelling program of Waltz. The goal of each is to label each edge in a manner that is 
consistent with realizable geometries and topologies. Briefly, Waltz's algorithm generates a set of all 
possible edge labels in the scene and, us- ing constraints that occur at the ver- tices, iterates until 
an unambiguous in- terpretation is reached. The algorithm in this paper can be described in similar terms. 
A simple interpretation of the scene is produced that is correct. At each stage edges are added that 
maintain the consistency of the interpretation. If at any point a situation occurs that is ambi- guous 
the scene is unrealizable. Finally if all the edges are added, a final correct interpretation has been 
produced. 6. SUMMARY A valid representation of a solid three dimensional object is through its surfaces. 
Furthermore, a surface may be represented as the embedding of a graph. Most graphs have many possible 
embeddings, but if the graph is planar and three con- nected then there will exist a single unique embedding. 
This is demonstrated by presenting an algorithm to find all the faces of a planar, three connected graph 
in O(n) steps. Applications of this algo- rithm arise in geometric modelling and scene recognition. These 
considerations explain the versatility of the winged edge representation for topological models, since 
it one of the few representations capable of representing arbitrary embed- dings of a graph. A very interesting 
line of further research is the determination of other classes of graphs which specify unique embeddings. 
 7. ACKNOWLEDGEMENTS The author would like to thank the following people for their contributions. Mark 
Ganter and.John Uicker for introduc- ing me to the "wire frame" problem and in particular for emphasizing 
the importance of the purely topological approach. I would also like to thank Len Uhr and Tony Stretton 
for their support and encourage- ment while this work was being done. Baumgart, B. G. Geometric Modelling 
for Computer Vision. Stanford Artificial In- telligence Project, Research Memo No. 249. Computer Sciences 
Department, Stanford University. 1974. Catmull, E., and Clark, J. Recursively generated B-spline surfaces 
on arbitrary meshes. Computer Aided Design. 10(6) 1978. Demoucron, G., Malgrange, Y., and Per- tuiset, 
R. Graphes plainaires: reconais- sance et constructien de representations planares topologiques. Rev. 
Francaise Re- cherche Operationelle. 8_-p-p. 33-47. 19~. Edmonds, J. A Combinatorial Representa- tion 
for Polyhedral Surfaces. Notices Amer. Math. Soc. 7 p. 646 1960. Falk, G. Computer Interpretation of 
Im- perfect Line Data as a Three Dimensional Scene. Stanford Artificial Intelligence Project, Research 
Memo No. 132. Computer Sciences Department, Stanford University. 1970. Filotti, I. S., Miller, G. L. 
and Reif J. On -Determining the Genus of a Graph in O(v U[g)) Steps. Eleventh Annual Symposium on the 
Theory of Computing. pp. 27-37 1979. Ganter, M. A. Techniques for Converting Wire-Frame to Solid-Geometric 
Representa- tions. Masters Thesis. University of Wisconsin -Madison. 1981. Liardet, M. PICAX -Polyhedron 
Input to the Computer Using an Axonometric Drawing. Interactive Techniques in Computer Aided Design. 
pp. 344-355 1978. Markowsky, G. and Wesley, M.A. Fleshing out Wire Frames. IBM J. Res. Develop. 24(5):582-597 
1980. Rubin, F. An Improved Algorithm for Test- ing Graph Planarity. IEEE Trans. Comp. C24:113-121 1975. 
 Waltz, D. L. Generating Semantic Descrip- tions from Line Drawings of Scenes with Shadows. Ph.D. thesis. 
Department of Electrical Engineering, Massachusetts In- stitute of Technology. Cambridge, Mass.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801266</article_id>
		<sort_key>85</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Presidents' forum]]></title>
		<subtitle><![CDATA[Current issues in Computer Graphics]]></subtitle>
		<page_from>85</page_from>
		<doi_number>10.1145/800064.801266</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801266</url>
		<abstract>
			<par><![CDATA[<p>Computer Graphics has been an integral part of the computer environment since the mid-1950's with origins in military, academic and industrial applications.</p> <p>Presidents of several leading computer graphics companies will discuss some of the issues they see facing this dynamic industry. One president believes that the 1980's will belong to the systems integrators because of the rapid rate of change in both technology and the competitive environment. Vertical integration will not be a successful strategy for the 1980's.</p> <p>Another president is concerned that industry research analysts are misusing the term &#8220;CAD/CAM&#8221; and that, in fact, most of today's CAD/CAM systems are simply productive drafting systems. He is concerned about the confusion between realistic pictures and solid modelling and about the proper mix of functionality and interactivity.</p> <p>Still another president believes the crucial issues in computer graphics are those that concern users; those that have access to the tools and those of future development of hardware and software to meet their needs. He suggests that only about one to five percent of the potential users have adopted computer graphics. He is concerned about providing increased access to computer graphics facilities and about removing the widespread psychological barrier of &#8220;I can't use the computer to do what I want!&#8221;.</p> <p>Other current issues will also be addressed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40027810</person_id>
				<author_profile_id><![CDATA[81100488149]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Carl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machover]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Machover Associates]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329905</person_id>
				<author_profile_id><![CDATA[81100196787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Feddersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Applicon Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333166</person_id>
				<author_profile_id><![CDATA[81100439940]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ralph]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Linsalata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lexidata Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332903</person_id>
				<author_profile_id><![CDATA[81100261403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Preuss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ISSCO GRAPHICS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333318</person_id>
				<author_profile_id><![CDATA[81543708156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Spann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adage, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Presidents' Forum: Current Issues in Computer Graphics CHAIR: Carl Machover Machover Associates Computer 
Graphics has been an integral part of the computer environment since the mid-1950's with origins in military, 
academic and industrial applications. Computer Graphics in the early 80's has matured from the early 
role of often being a cure for no known disease, to being an indispensable element in many applications. 
By the end of 1982, fore- casters expect computer graphics to be a 4.6 billion dollar industry, with 
250,000 graphic devices in place, with a user community of perhaps 400,000, and with about 90~000 people 
employed in the design, manufacture, selling and support of computer graphics hardware, software, systems 
and services. By the end of the decade, the indus- try is expected to grow to 15 billion dollars a year. 
 Presidents of several leading computer graphics companies will discuss some of the issues they see facing 
this dynamic industry. One president believes that the 1980's will belong to the systems integra- tors 
because of the rapid rate of change in both technology and the competitive environment. Vertical integration 
will not be a success- ful strategy for the 1980's. Another president is concerned that industry research 
analysts are misusing the term "CAD/CAM" and that, in fact, most of today's CAD/CAM systems are simply 
productive drafting systems. He is con- cerned about the confusion between realistic pictures and solid 
model- ling and about the proper mix of functionality and interactivity. Still another president believes 
the crucial issues in computer graphics are those that concern users; those that have access to the tools 
and those of future development of hardware and software to meet their needs. He suggests that only about 
one to five percent of the potential users have adopted computer graphics. He is concerned about providing 
increased access to computer graphics facilities and about removing the widespread psychological barrier 
of "I can't use the com- puter to do what I want!". Other current issues will also be addressed. PANELISTS: 
Donald Feddersen Applicon Corporation Ralph T. Linsalata Lexidata Corporation Peter Preuss ISSCO GRAPHICS 
 Richard N. Spann Adage, Inc. 85 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801267</article_id>
		<sort_key>87</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>16</seq_no>
		<title><![CDATA[FLAIR - user interface dialog design tool]]></title>
		<page_from>87</page_from>
		<page_to>98</page_to>
		<doi_number>10.1145/800064.801267</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801267</url>
		<abstract>
			<par><![CDATA[<p>To aid the system designers in achieving early involvement of the users, FLAIR (a user interface dialog design tool) was conceptualized and developed. FLAIR allows the designer to rapidly prototype a system's Man-Machine Interface.</p> <p>A system designer can select a desired mix of input/output devices ranging from voice to high resolution graphics equipment. FLAIR prompts its users with a dynamic menu according to a pre-defined English-like syntax. Commands are entered by the designer's own voice. Entered commands are validated by voice pattern recognition and command language-gating. Pointing devices are used to locate, place, and/or pick objects from the RAMTEK 9400 25&#8221; high-resolution color monitor.</p> <p>The graphics display portion of FLAIR is handled by a Core Standard graphics package. This particular package also fits the specific needs of FLAIR in that it makes available most of the RAMTEK 9400's hardware features.</p> <p>A relational DBMS has been integrated into FLAIR in order to manage system and user-defined data relationships. The user-defined data can be associated with a particular on-screen graphics symbol, and can then be queried at a later time.</p> <p>In addition to single graphics snapshots (or &#8220;static frames&#8221;), FLAIR allows the system designer to create command menu hierarchies for &#8220;dynamic scenarios.&#8221; This allows the designer to simulate, through client menu item selection, the system control flow. He can, in effect, create a tree of menus, traverse the tree at will, and select more menus or system actions as desired. All menus appear as sensitized areas on the graphics screen, and can be selected via any of the available input devices.</p> <p>It is estimated that a system designer can greatly improve his dialog design productivity by using FLAIR. The designers need not code any formal programs. The designer need not master the usage of the host computer, the graphic input/output systems, the menu controls, or the database programming before his designs are realized. FLAIR is there to assimilate, assemble, save and exercise his instructed operator dialog.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[MMI testbed]]></kw>
			<kw><![CDATA[Rapid prototyping]]></kw>
			<kw><![CDATA[Spatial data management]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>User interfaces</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor>Human information processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.2.7</cat_node>
				<descriptor>Speech recognition and synthesis</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Picture description languages**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011050.10011023</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Context specific languages->Specialized application languages</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010179.10010183</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Natural language processing->Speech recognition</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010216.10010217</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Philosophical/theoretical foundations of artificial intelligence->Cognitive science</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332914</person_id>
				<author_profile_id><![CDATA[81332536228]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[C. S.]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TRW Defense Systems Group, Redondo Beach, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330157</person_id>
				<author_profile_id><![CDATA[81100282584]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Reid]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TRW Defense Systems Group, Redondo Beach, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>356748</ref_obj_id>
				<ref_obj_pid>356744</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bergeron, R. D., Bono, Peter, Foley, James, "Graphics Programming Using the Core System." Computing Surveys Dec. 1978]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Newman, W., Sproull, R., "Principles of Interactive Computer Graphics," McGraw-Hill, 2nd Edition 1979]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1500029</ref_obj_id>
				<ref_obj_pid>1499949</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Stonebraker, M., Held, G., Wong, E., "INGRES-A Relational Data Base System," Proc. AFIPS, Volume 44]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[ACM-SIGGRAPH, "Status Report of the Graphics Standards Planning Committee," Computer Graphics, Vol. 13, No. 3, August 1979]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1024281</ref_obj_id>
				<ref_obj_pid>1024273</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Treu, Siegfried, "A Framework of Characteristics Applicable to Graphical User-Computer Interaction," P61-71, User-Oriented Design of Interactive Graphics Systems. Based on ACM/SIGGRAPH Workshop Oct 14-15, 1976, 1977]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Smith, Sidney, "Man-Machine Interface (MMI) Requirements Definition and Design Guidelines - A Progress Report," P39-81, ESD-TR-81-113 Feb.1981]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D., Chan, P., Wallace, V. L., "The Human Factors of Graphic Interaction: Tasks and Techniques," Tech. Rep. 508, US Army Research Institute for the Behavioral and Social Science, Sept. 1981]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807503</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Bolt, T., "Put-That-There Voice Gesture at the Graphics Interface;" ACM/SIGGRAPH '80 Conference Proceedings, July 1980]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Parson, H., "Man-Machine System Experiments," The Johns Hopkins Press, 1972]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Deaton, B., "Dialog Design Language Functional Requirements Specification," TRW 1980 Man-Machine Interface IR&amp;D Final Report, Dec. 1980]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Kloster, G. V., "Draft and Guidelines for the Development of the MMI Design Guidebook": TRW 1980 Man-Machine Interface IR&amp;D Final Report, Nov. 1980]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807504</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Hanau, P., Lenorovitz, D., "Prototyping and Simulation Tools for User Computer Dialogue Design"; ACM/SIGGRAPH '80 Conference Proceedings, July 1980]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 FLAIR -USER INTERFACE DIALOG DESIGN TOOL Peter C. S. Wong Eric R. Reid TRW Defense Systems Group Redondo 
Beach, CA 90278 ABSTRACT To aid the system designers in achieving early involvement of the users, FLAIR 
(a user interface dialog design tool) was conceptualized and deve- loped. FLAIR allows the designer to 
rapidly pro- totype a system's Man-Machine Interface. A system designer can select a desired mix of 
input/output devices ranging from voice to high resolution graphics equipment. FLAIR prompts its users 
with a dynamic menu according to a pre-de- fined English-like syntax. Commands are entered by the designer's 
own voice. Entered commands are validated by voice pattern recognition and command language-gating. Pointing 
devices are used to lo- cate, place, and/or pick objects from the RAMTEK 9400 25" high-resolution color 
monitor. The graphics display portion of FLAIR is handled by a Core Standard graphics package. This 
particular package also fits the specific needs of FLAIR in that it makes available most of the RAMTEK 
9400's hardware features. A relational DBMS has been integrated into FLAIR in order to manage system 
and user-defined data relationships. The user-defined data can be associated with a particular on-screen 
graphics symbol, and can then be queried at a later time. In addition to single graphics snapshots (or 
"static frames"), FLAIR allows the system designer to create command menu hierarchies for "dynamic scenarios." 
This allows the designer to simulate, through client menu item selection, the system control flow. He 
can, in effect, create a tree of menus, traverse the tree at will, and select more menus or system actions 
as desired. All menus appear as sensitized areas on the graphics screen, and can be selected via any 
of the available in- put devices. Permission to copy without fee all or part of this material is granted 
provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright 
notice and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. &#38;#169; 1982 ACM 0-89791-076-1/82/007/0087 $00.75 It is estimated that a system 
designer can greatly improve his dialog design productivity by using FLAIR. The designers need not code 
any for- mal programs. The designer need not master the usage of the host computer, the graphic input/ 
output systems, the menu controls, or the data base programming before his designs are realized. FLAIR 
is there to assimilate, assemble, save and exercise his instructed operator dialog. CR Categories and 
Subject Descriptors: 0.2.2 (Software) Tools and Techniques -User Interfaces; H.I.2 (Information Systems) 
User/Machine Systems - Human Information Processing; 1.2.7 (Computing ~ethodologies) Natural Language 
Processing -Speech Recognition and Understanding; 1.3.4 (Computer Graphics) Graphics Utilities -Picture 
Description Languages; 1.3.6 (Computer Graphics) Methodology and Techniques -Interaction Techniques 
General Terms: Dialog Design Language, Man-Machine Interface Additional Key Words and Phrases: MMI testbed, 
spatial data management, rapid prototyping INTRODUCTION The personnel who will be the ultimate users 
of a system are the most important clients for de- signers of that system. An operator dialog inter- 
face that can promote the productivity of the user clients should be one of the goals of the system design. 
This goal can be accomplished by the clients' early involvement in the dialog ~lanning, design, simulation 
and development (ll). Dialog streamlining via simulations and final agreement, with the client to the 
functional dialog, should be achieved before production codes are generated. To meet this objective, 
a ~an-Machine Inter- face (MMI) testbed has been built at TRW. The testbed is fully operational. Included 
in the system are some of the latest technologies in com- puter graphics (software and hardware), voice 
rec- ognition and synthesis units, a relational data base management system, picture capture equipment 
and interactive input devices. Elements of the MMI hardware are grouped into "workstations." Each station 
consists of a full complement of in- put/output peripherals that may be associated with an operator station. 
This is illustrated by Figure I. The system designer, using one of these stations, can "mix and match" 
any of the devices to form the project's conceptualized console. The workstations themselves are connected 
to a VAX II/780 computer. The MMI workstation control is provided by a software system generically re- 
 ferred to as Dialog Design Language (DDL). The DDL is the software cornerstone of the testbed. It enables 
the users of the system to design operator dialog in a simple, straightforward manner. Fol- lowing is 
a description of the DDL implementation in the testbed. MOTIVATION AND CONCEPTS FOR THE DDL The Dialog 
Design Language (DDL) is a comprehen- sive software tool that enables the system designers to express 
operator/computer interactions and the human performance researchers to conduct MMI re- search. The DDL 
is constructed so that a user can take advantage of all the available hardware in- stalled in the Man-Machine 
Interface (MMI) testbed; yet, users do not have to master the intricacies of the host system before becoming 
a proficient user of the testbed. In this way, the designer can con- centrate on the dialog designs and 
demonstrate various MMI subsystem concepts before a proven MMI subsystem is actually implemented. By 
providing the end users with a chance to inter- act with a portion of the 'deliverable system' through 
scenario simulation, the DDL can lead to more accurate definition of system requirements. Various formats, 
content and sequence of inter- actions, as well as combinations of interactive devices to be used, can 
easily be considered. The usefulness of the DDL does not end with the estab- lishment of system requirements, 
but continues throughout the design and development process. Figure 2 illustrates the concept of using 
the DDL in the testbed, including customer interaction. For the human performance researchers, the 
DDL offers an easy method to construct experiments. Given the simple manner in which a chart or flow 
of charts can be constructed and replayed under computer control, experiments can be constructed and 
modified without the cost and delay of using professional artists to illustrate the displays. Since 
the DDL can logically control the sequence of the displays, man-machine and human performance experiments 
will be more realistic and the data gathering activity more uniform from subject to subject. CONCEPT 
 USER REQUIREMENTS To establish the requirements for the Dialog De- sign Language, system designers 
from a number of TRW projects were consulted. In addition, the de- sign has been reviewed with potential 
users, who plan to incorporate the DDL into their project work. No matter how sophisticated and complete 
a MMI test- bed may be, it is only as powerful as the system designer's ability to use it. Since the 
DDL is the system designer's tool for in- voking the powers of the testbed, it should be a model of utility. 
Out of this emerge two major re- quirements: l) The DDL must be easy for the system designer to use. 
If system designers are to be en- couraged to use new tools, they must be convinced that their jobs will 
be simplified. 2) The DDL should be flexible enough to simulate a broad range of scenarios. Much effort 
and expense is involved in building a good MMI development facility. It would be a w~ste of resources 
if the DDL accomo- dated only a very limited set of applications. The philosophy behind the DDL, then, 
is to keep it user-oriented, natural in its structure, yet powe ful enough to be useful. Most significant, 
the DDL should itself be a good example of MMI. The DDL is not strictly a language, but a menu-driven 
system that aids and directs the designer through a coherent and orderly translation of his scenario 
 into a form that is executable in the MMI testbed. The following major requirements form the basis 
for the initial structure of the Dialog Design Lan- guage (lO~. A. Display Static Frames -be able to: 
 I. Construct a picture 2. Edit a picture  3. Save and retrieve a picture  A picture can be in color. 
It can consist of back- ground maps or other non-erasable data, symbols, geometric elements (points, 
lines, circles, etc.), and textual information. B. Frames Scenario Dialog -be able to: I. Display a 
sequence of frames 2. Control the sequence of frames logically as a result of operator inputs  3. Control 
the sequence of frames via pre- defined control hierarchy of menu re- sponse or time  4. Save~ edit, 
retrieve the control hierarchy  The input devices shall be interactive. These de- vices shall be able 
to interchange with each other so that the system designer can choose the devices he wishes to include 
in the final system. C. Dynamic Frame with Scenario -be able to: I. Modify frames as a function of 
operator/ computer interaction 2. Attach relations with individual symbols  3. Include user algorithms 
to control the scenario. Figure 3 represents a functbnal view of the Dialog Design Language  D. Multi-Workstation 
Coordination -be able to: I. Effectively utilize the MMI testbed re- sources from multiple workstations 
 2. Simulate multiple workstation communi- cations with each station interacting with certain portions 
of a scenario E. Code Transportability and Standards: Since it is envisioned that DDL will be hosted 
 and used at other installations, the tools should be able to: I. Move the host independent modules 
easily from one machine to another 2. Have configuration management for updates THE DDL USER'S INTERFACE 
REQUIREMENTS The initial frame construction, subsequent editing and control is designed so that a system 
designer or human performance researcher can fully utilize the testbed without a major learning effort. 
The Dialog Design Language is menu-driven so that it can prompt a user for possible courses of action 
 at a given time. A TEST BED WORK STATION INCLUDES: F I MOP I CHR VOICE JOYSTICK I DISF LAY I SYNTHESIS 
N FIGURE I. DDL UTILIZATION IN MMI TEST BED [ " o~Mj'~,'L~I%F'?'ZOL " ~,~.~_ "'~,,~_ ,,~-----: '"~[. 
/ SCENARIO I H ~]!I .... I~-" i ~t'~';l~'~--L SYSTEM ~ "~-J DESIGNER ILl W DIALOG/DLYSPLAY G DI m !!,7~, 
,~ ~ I ~.,,~,,I --_-I DEVELOPMENT ~ "~ ~ ~.~$~ l .... I I z ~'~;°~ L___J ' ' ' J1 ~--~ ~>f~ ~I COSTOME.,.TERACT.O. 
~!11 ~' IJk~ hlE ~.~Jl W,T. TEST.ED -II FIGURE 2. 89 DYNAMIC SIMULATION OF OPERATOR DIALOGS (SCENARIOS) 
 CONTROL HIERARCHY SYMBOL DATA MENU DEFINITIONS OPERATOR DIALOGUE SIMULATION ANALYTICAL FUNCTIONS FIGURE 
3. GENEALOGY OF MMI TESTBED SOFTWARE I I III 1979 1980 1981 | 1982 GRAPHICS i PLOT-IO(,cqutml) DISSPLA 
(consNer~f) GIOS (acquired) u,u r,~,,Ar, im.,qv,,~l)   GIOS (newlfunctt ons ) SXOS (upd,~ to UX) 
 SAIGRAoH (acquired) DI-3000 O~ERS:  DECMg'r (net:wO~lng) DIALOGDIS IGN L'qNGUAG[ (DOL) i MMAGEI~T 
SYSTEM  OPERATOR DIALOG RESEARCH DOL R[OUII~rlM"I(TSPEC  DOLl JrL~IR) l~qG w4s (acquired)  d lm-~oo 
(tnto~race) FIGURE 4. Computer Graphics Volume 16, Number 3 July 1982 The choices for user interface 
devices include soft function keys, light pens, joystick, trackballs, voice input/output, touch panels, 
keyboards, valu~ tors, tablets, multiple high-resolution (I024 lin~ color graphic displays, hard-color 
copy camera units, etc. CONCEPT VALIDATION A formal requirement specification was published in the 
1980 TRW Man-Machine Interface IR&#38;D Final Report (lO). The report was sent to internal  reviewers 
and outside consultants. To further re- fine the concepts and gather additional ideas about the implementation 
of the DDL, works mentioned in  references five through twelve were studied. Rele- vant ideas and techniques 
from these publications were considered. IMPLEMENTATION  Two major DDL designs were developed in 1980 
(Figure 4 -Genealogy of Software). Each repre- sented an approach to constructing a Dialog Design Language. 
The first design approach utilizes a menu and state table-driven schema to control the graphics manipulation 
and, eventually, the operator dialogs. It relies heavily on the data base sys- tems to reference and 
update the state table as well as for data entries and retrievals. This de- sign is totally flexible 
because the actual direc- tion to do anything is found in the state tables. By changing or adding to 
these tables, new func- tions can be accommodated. The second approach for the DDL design is to use 
a dynamic menu coupled with "hard programmed func- tions" to control the graphics and other input/out- 
put manipulations and the operator dialogs. To e~ hance the speed of interaction, the data base is only 
used for user data entries and inquiries. Additional subroutines and their related menus must be added 
when new functions are required. Basic DDL contro~ mechanisms also differ between these approaches. 
The first concept utilizes the light pen or graph tablet extensively; first to pick a high-level instruction, 
such as a picture construction, picture editing, etc., and then to  subsequently pick the sub-menus. 
The second ap- proach utilizes voice recognition for all commands, a graph tablet to control the cursor, 
and a key- board for text string entries. The command struc- ture is more loosely defined; there are 
no ~'macro" instructions to segregate building and editing functions.  These two approaches proceeded 
from design to actual coding for the first phase of dialog design demonstrations. The first concept using 
the state table methods proved to be very slow, both in the programming of the software and the execution. 
The time to build any usable dialog design language using this method was projected to be long ~-at 
 least three man-years before the product could have any impact for simple dialog design. The second 
concept, using hard-coded functions, proved to be a very fast method o~ producing an operational Dialog 
Design Language system. Using  this method, a DDL can be in operation in one man- year with more functions 
than with the first approach. The second concept was adopted for the full DDL implementation. This 
does not connote that the experience learned in concept one will be lost. It will be incorporated later 
on in the system when better methodology can be found to input and manage the state tables. The second 
concept,which became known as FLAIR, is described below. SYSTEM DESCRIPTION -FLAIR CONCEPT FLAIR (Functional 
Language Articulated Interactive Resource) is a voice menu-driven Dialog Design Language which embodies 
all relevant hardware and software in the testbed. Through use of a variety of input devices (such as 
tablet, light pen, track ball, and voice recognition) FLAIR controls a Her- archy of user command menus. 
To date, actual FLAIR commands are entered via voice recognition, but the other devices can be used 
for user's menu se- lection and coordinate location. Additionally, the keyboard is used to enter long 
strings of in- formation, such as labels and file names. FLAIR FUNCTIONAL FLOW Functionally, the parts 
of FLAIR are shown in Figure 5. The FLAIR main routine acts as the over- all traffic router. It initiates 
the voice recog- nition unit and the voice synthesizer, the DBMS, graphics package, and tablet. In addition, 
it acts as the upper level menu dispatcher. The in- put section validates the voice command input by 
voice "command-gating" and cursor locations by window frames. Voice command-gatin~ is done by specifying 
the a11owed set of commands to be used in the context instance. The input validator then filters the 
signals from the voice recognition unit until one of the operative words is found. The screen cursor 
locator is validated and trans- lated by window displacement. The computational portion performs the 
necessary context switching to perform the various functions, program management, calculations, user's 
menu de- tection translations, inquiries, and retrievals to/from the relational DBMS. The resultant out- 
put operations can take the form of: l) calls to graphics to display objects on specified windows; 2) 
requests for more information by prompting the user; 3) transmission of a synthesized voice out- put 
to the user. FLAIR CAPABILITIES FLAIR provides the user the basic services to manipulate graphics objects 
or primitives using ACM Core Standard graphics. These include: drawing of lines, boxes, circles; writing 
of text in stroke or hardware character sets, in any size or orientation; selection of colors (including 
blinking configurations); fill of any enclosed area by a color; hardware pan and zoom of the screen; 
read or write a pixel image to/from the screens; erasure of entire display (or single items); freedrawn 
lines controlled by the cursor movement. HIGH-LEVEL FEATURES In addition to the basic features, FLAIR 
provides macro facilities to perform the following voice commands: --construct a shell for the FLAIR 
user to define and control a menu hierarchy of his own design  Computer Graphics Volume 16, Number 
3 July 1982 Tn his operator's dialog.  --access to a world-wide geographic data base. By describing 
two points to form a rectangular box, a geographic outline on the area can be dis- played with progressive 
detail. --generate full color dynamic barcharts and pie- charts --construct new symbols from existing 
symbol building blocks --compute arithmetic expressions by voice entry. This calculator is complete with 
six "registers." --access the relational data base for graphical entity attribute storage and retrieval 
--generate a grid system which is used to perform precise cursor positionings for lines, charts, and 
text --access the voice synthesizer unit USER INTERFACE The user interface utilizes relevant input/output 
 devices for any action. The two screens in the MMI workstation work in tandem to prompt and aid the 
users. The human voice is currently used to input all commands which manipulate FLAIR. Urgent or exceptional 
messages are announced by the voice synthesizer. Pick or point actions are handled by the graphics tablet 
or comparable input devices. SCREEN LAYOUT  FLAIR currently makes use of six separate graphical "windows" 
located on the two workstation monitor screens. These are (Figure 6): a) The Auxiliary Information Window, 
which is reserved for large-scale display of the sym- bol menu as well as the textstyle and line- style 
menus. This window covers the entire 19" monochrome screen.  b) The Primary Display Window, which contains 
all user-defined graphical data (such as lines, barcharts, maps, and user-built menus). It can be thought 
of as the "window" into the user's world. c) The Calculator Result Window, which contains all intermediate 
and final results from FLAIR's voice-activated desk calculator. d) The Error/Information Window, which 
is used for display of messages to the user, in- cluding error messages. This window only contains relevant 
text if the message is current. e) The Dynamic Command Window, which contains, at any given time, the 
list of words which the user can "say" to FLAIR for it to perform an operation. The command lists are 
dynamic because the meanings of the commands are accepted for only the allowable actions at that particular 
sequence. As the command is accepted, the list changes or refreshes after acknowledgement of receipt 
of a word. f) The Current Color Window, which graphically portrays the current color. It also contains 
an hourglass-shaped symbol, which appears when the system is too busy to accept input at that time. 
INPUT TO FLAIR As previously stated, FLAIR itself is primarily controlled by voice commands. In addition, 
the operator uses the graphics tablet for graphic primitive location and graphical picks. ~Jhen it comes 
to operator-designed scenarios (menu hierar- chies), the operator has his choice of joystick, light pen, 
trackball, or graphics tablet for menu item picks. VOICE SYNTHESIS The voice synthesis unit is used 
by FLAIR to flag operator errors and major events, to enable or disable certain FLAIR modes, and to prompt 
the operator in tandem with messages in the Error In- formation Window. The operator can also utilize 
the voice unit in his dynamic scenarios. COMMAND HIERARCHY FLAIR is controlled via voice commands, 
which se- lect paths from a tree-of-action items. Every command node may lead to another branch or con- 
clude with a leaf. in general, the top command level allows selection of an action (verb), level two, 
an object (noun), and level three, a modifier (adjective or noun). Regardless of context and depth in 
the tree, a set of global words is constantly monitored by FLAIR. These words (and some phrases) control 
overall FLAIR operation and voice recognition performance. These are: --ABORT: Returns user to topmost 
command level,re- gardless of current position in tree. --STOP: Halts the execution of the FLAIR program 
and returns to the operating system command mode. It must be repeated to avoid accidental exiting. --FINISHED: 
Returns the user to the previous (next highest) command level. This has no effect at the top command 
level. --RESET: Directs FLAIR to issue commands which re- set the voice recognition unit (VRM). All 
VRM internal flags are cleared, but the current vo- cabulary remains intact. --READ THRESHOLD: Queries 
the VRM unit and dis- plays the current voice "threshold"; that is, how accurate the user must be in 
order to be recognized. Default threshold is set at the VR~ during training time within the ranges from 
0 to 128. --SET THRESHOLD: Sets the VRM threshold to some- thing other than the present threshold. 
FLAIR COMMAND SYNTAX DESCRIPTION At the topmost (root) level of the FLAIR syntax, there currently exist 
fifteen root words which, in turn, activate the other 85 commands to accomplish a task. These root words 
are: - MAKE allows construction of barcharts, piecharts and composite symbols SAVE a) saves the Primary 
Display Window as a - digital image, and b) initlate/terminate construc- tion of a frame or scenario 
ERASE allows erasure of selected elements, or - the entire Primary Display Window DRAW enables graph|c 
primitive construction - COMPUTE engages the voice-activated calculator - GEOGRAPHICS allows definition 
and display of - some portion of the world, with or without a lon- gitude/latitude grid. ZOOM enables 
hardware zoom and pan features to -  FUNCTIONAL FLOW FOR THE DDL (FLAIR) FLAIR CO~94ANDS  J ] SYNTHESIZED 
 WOICE GENERATION TERMINAL GET &#38; VALIDATE  COHPUTAT IONS MESSAGES OPERATOR INPUTS  ;ervices 
GRAPHICS (Text Included', DISPLAYS RELATIONAL CORE k GENERATION DBMS GRAPHICS CALLS r FIGURE 5. FLAIR 
WINDOW AREAS MAIN DISPLAY AREA Menu Display  Area Cursor Depending on context, displays f the symbols 
list the line styles the character sizes Arithmetic Calculation Current Display Area Color Secondary 
display area  Selected System Message Area Area MONOCHROME MONITOR COLOR MONITOR FIGURE 6. enlarge 
and scan features on the screen GRAPHICS -DATABASE BINDING VOICE OUTPUT -prompts for line of text, then 
attempts to have the voice synthesis unit pro- nounce it COLOR -allows selectlon of current color QUERY 
-allows user to query the data base attri- bute of a specific symbol UNZOOM -resets zoom factor and pan 
position to normal modes CONTROL -(available only if frame construction mode is enabled) -permits construction 
of saved static frames and dynamic scenarios FILL -allows user to specify the objects he would like filled, 
or to enable default fill mode LIGHT PEN -enables light pen (and other RAMTEK devices) for user-defined 
menu picking. It auto- matically disables the tablet at time of pick Chart I shows the current structure 
of the FLAIR command. As one can see, functions such as Color (with asterisk) appear throughout the structure. 
These "tunnels" allow the user to rapidly and tem- porarily change the context so that some basic en- 
vironment changes can be accommodated without going back to the root. Function words with dou- ble asterisks 
are treated differently depending on the context in which they are used. Just as in the English language, 
where a word can have a dif- ferent meaning based on the text of the sentence, FLAIR utilizes this scheme 
to ensure that the commands are as natural as possible to the user. FLAIR CONVENTIONS/FEATURES The 
following is a brief description of some major features, conventions and practices inherent in FLAIR. 
 CURRENT GRAPHICAL ATTRIBUTES Rather than specifying such things as color every time a user manipulates 
something in his window, a number of graphical attributes are defined to be implicit in these actions. 
Each has its default. These defaults are: Attribute Default Color White Linestyle Solid (Style l) Text 
Size 90 Characters/line Text Direction Zero Degrees Text Generation Stroke (as opposed to hardware) Polygon 
Fill Off Pick Device Tablet Attributes are latched until a change is com- manded. SYMBOL BUILDING~BLOCK 
DEFINITION Contained with FLAIR is a menu to access 30 basic symbols. The menu contains rectangles, 
circles, crosses, radar disks, etc., and can be drawn on the screen by themselves or combined to form 
com- posite symbols. These composite symbols, once formed, are independent of the basic symbols, and 
can be used as new symbols. Definitions of the 30 symbol building blocks are kept in the FLAIR data 
base. However, there are another 120 basic symbols that FLAIR can access via an off-line process through 
the DBMS update utilities. FLAIR provides the capability to manipulate user- defined data, and associate 
it with graphical items portrayed on the screen. In this way, he can query the item in some fasion to 
retrieve all or part of its data. FLAIR provides graphics-database binding schema using the INGRES DBMS. 
Any symbol, whether it be a building block or composite, can be associated with database "attribute" 
-in this case, a string of text. If, at some later time, the operator wishes to inquire as to the string 
with a par- ticular symbol, he can pick that symbol from the primary display window with the tablet. 
The cor- responding attribute (if any) is then displayed in the message area. Future implementations 
will allow an operator to not only associate symbols with more complicated database structures, but 
to define these struc- tures as needed. DYNAMIC SCENARIO CONSTRUCTION AND PLAYBACK When the operator 
wishes to construct a dynamic scenario, he enables what is known as "save"mode . From that point on, 
all FLAIR commands and loca- tor input are recorded under an operator-specified scenario file. The actual 
scenario file is the list of FLAIR commands stored in the sequence specified. If an error occurs during 
construction or the user wants to update the scenario, these files can be edited with a regul~r text 
editor. To "play back" a scenario file, the operator then enters "display file" mode. FLAIR not only 
can create, store and retrieve static frames, it can allow the user to define and control a hierarchy 
of menus. This is the feature which makes FLAIR a dynamic scenario gen- erator. Through the definition 
of the user's menus, an operator dialog can be constructed. A client can interact, as he traverses through 
the selection of the menus, with the sequence and the path of the scenario controlled by the client. 
 Individual menu items are defined on the screen as pick-sensitive boxes or circles. If an opera- tor 
"picks" within the boundaries of a menu item, a previously defined action, as specified by the writer 
of the operator dialog, takes place. Actions can be the following: a) Output of some line of text via 
voice syn- thesizer b) Transfer of control to another scenario --in effect, traversing down another 
level in the menu hierarchy c) Return to the previous menu level (if any) -- can be thought of as "popping" 
one level upwards d) Return to the highest menu level e) Exit scenario mode and return to FLAIR control 
 f) Exit scenario mode, but allow entrance back to the scenario, if desired, at the point exited g) 
Definition of global menus -any levels can access this particular menu item SUPPORTING CAST The current 
supporting cast in the testbed is as follows:  Computer Graphics Volume 16, Number 3 July 1982 I I 
I t I . . @ o I I . i i I i i I i i~ L_L i_~ IIII . ~ o o ~ I ill-Ill ~ z ~z @ ,=<~,~. , ~ <~ ¥ J ..._1 
ILl-- 0 --g III i i ~ I / i i i I I I z z o o I I i i c_) x o m g,N~ I I I ~0 ~T~V N ~ .... ....... m 
"°°- ~ 0 1.!. z~ 95 Hardware Host: VAX II/780 Graphics Processor: RAMTEK 9400 -high resolution graphics 
system Input/Output Devices 25" (I024 line) color monitors 19" I024 line monochrome monitors Voice 
recognition unit Voice synthesizer Keyboard Joystick Light pen Graphic tablet Matrix color camera system 
 Software DBMS: INGRES (VMS) by Relational Technology Basic Graphics: SAIGRAPH by SAI, Inc. EXAMPLES 
Some examples of using FLAIR's syntax: S = Say P = Point T = Type Drawing a Circle: S -DRAW S -CIRCLE 
P -to the desired location for the center point S -CENTER P -to the desired location for the radius S 
-RADIUS Drawing a Symbol: S -DRAW S -SYMBOL S -a number or set of numbers one digit at a time P -to 
the desired location for the symbol S -HERE Drawing a Line: S -DRAW S -LINE P -to the location of 
the first point S -HERE P -to the location of the second point S -HERE Construction of a scenario with 
a user defined menu: S -SAVE S -DISPLAY T -in file name --do the desired graphics primitive S -CONTROL 
S -DETECT S -BOX P -location of lower left corner S -FILE S -LO~ER LEFT P -Location of upper right corner 
S -UPPER RIGHT T -the file name for the next level's frame Convention is ---.DIS S -Finished S -Finished 
S -Save S -Finished Display a Scenario: S -Display S -File T -File name CONCLUSION We have presented 
the current configuration and the functions of a Dialog Design Language (FLAIR) developed by TR~. The 
language combines the power of ACM Core-standard computer graphics, a rela- tional data base management 
system, voice recog- nition, and MMI input/output devices to form a tool for system designers. This tool 
will enable system designers to realize and manipulate con- ceptualized operator dialog. In this way, 
various approaches to the design can be tried so that the best concept or a combination of concepts will 
be implemented in the real system. FLAIR has been in operation since August 1981. The preliminary results 
are very promising. To generate a frame with functional menus takes very little time. One typical scenario 
which contained maps, symbols with attributes, various colors, a maximum of six separate functional menu 
frames, and a series of five hierarchy selection levels took three days to complete. Based on experience, 
 if the same scenario had been done using tradi- tional programming, it would have taken at least one 
and one-half to two weeks. FUTURE PLANS We intend to develop more advanced features for FLAIR, such 
as a multitasking DDL. In this way, under the same execution node, multiple work- stations can share 
the logic and provide multi- workstation communications to simulate two or more workstations communicating 
with each other. An- other major task will be to provide an interface for the user's own algorithms. 
This shell will provide for external simulations not easily or necessarily incorporated into FLAIR. 
In this wau, the user need not gain access to the in- ternals of FLAIR. ACKNOWLEDGEMENT The authors 
would like to thank Dr. Irene Grief of MIT and Mr. Bruce Monroe of INSGROUP, INC., who served as consultants 
in the concept of the DDL. The authors would also like to acknowledge the support and encouragement 
of Mr. J. J. Rosati and Mr. W. P. Mancina. REFERENCES l) Bergeron, R. D., Bono, Peter, Foley, James, 
"Graphics Programming Using the Core System." Computing Surveys Dec. 1978 2) Newman, W., Sproull, R., 
"Principles of Inter- active Computer Graphics," McGraw-Hill, 2nd Edition 1979 3) Stonebraker, M., Held, 
G., Wong, E., "INGRES- A Relational Data Base System," Proc. AFIPS, Volume 44 4) ACM-SIGGRAPH, "Status 
Report of the Graphics Standards Planning Committee," Computer Graphics, Vol. 13, No. 3, August 1979 
  5) Treu, Siegfried, "A Framework of Characte- ristics Applicable to Graphical User-Computer Interaction," 
P61-71, User-Oriented Design of Interactive Graphics Systems. Based on ACM/ SIGGRAPH Workshop Oct 14-15, 
1976, 1977  6) Smith, Sidney, "Man-Machine Interface (MMI) Requirements Definition and Design Guidelines 
- A Progress Report," P39-81, ESD-TR-SI-II3 Feb.1981 7) Foley, J. D., Chan, P., Wallace, V. L., "The 
Human Factors of Graphic Interaction: Tasks and Techniques," Tech. Rep. 508, US Army Research Institute 
for the Behavioral and Social Science, Sept. 1981  8) Bolt, T., "Put-That-There Voice Gesture at the 
Graphics Interface;" ACM/SIGGRAPH '80 Conference Proceedings, July 1980 9) Parson, H., "Man-Machine 
System Experiments," The Johns Hopkins Press, 1972 lO) Deaton, B., "Dialog Design Language Functional 
Requirements Specification," TRW 1980 Man-Machine Interface IR&#38;D Final Report, Dec. 1980 ll) Kloster, 
G. V., "Draft and Guidelines for the Development of the MMI Design Guidebook": TRW 1980 Man-Machine Interface 
IR&#38;D Final Report, Nov. 1980 12) Hanau, P., Lenorovitz, D., "Prototyping and Simulation Tools for 
User Computer Dialogue Design; ACM/SIGGRAPH '80 Conference Proceedings, July ]980  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801268</article_id>
		<sort_key>99</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>17</seq_no>
		<title><![CDATA[A user interface management system]]></title>
		<page_from>99</page_from>
		<page_to>106</page_to>
		<doi_number>10.1145/800064.801268</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801268</url>
		<abstract>
			<par><![CDATA[<p>The design and construction of the user interface to interactive systems is receiving increased attention. This paper describes a user interface management system that allows a designer/developer to focus on the logical functionality of an application without the usual bookkeeping associated with a conventional programming language. The user interface management system contains two components: a special purpose, application independent dialogue specification language and a run-time interpreter that provides a number of interaction extensions not possible with procedure libraries.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[User interface management]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Software support</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Device independence**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011074</concept_id>
				<concept_desc>CCS->Software and its engineering->Software creation and management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P62287</person_id>
				<author_profile_id><![CDATA[81100541084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Kasik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Boeing Computer Services, P.O. Box 24346, Seattle, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[J. H. Bleher, P. G. Caspers, H. H. Henn, and K. Maerker; "A Graphic Interactive Application Monitor"; IBM Systems Journal; vol. 19, no. 3, 1980; pp. 382-402]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801765</ref_obj_id>
				<ref_obj_pid>800049</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[H. B. Feldman and G. T. Rogers; "Toward the Design and Development of Style-independent Interactive Systems"; Human Factors in Computer Systems, Mar, 1982; pp. 111-116]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Status Report of the Graphics Standards Planning Committee; Computer Graphics; vol. 13, no. 3; August 1979, pp. II-71 - II-90]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807504</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[P. R. Hanau and D. R. Lenorovitz; "Prototyping and Simulation Tools for User/Computer Dialogue"; Computer Graphics; vol. 14, no. 3; July 1980; pp. 271-278]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563296</ref_obj_id>
				<ref_obj_pid>965143</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[D. J. Kasik; "Controlling User Interaction"; Computer Graphics; vol. 10, no. 2; July 1976; pp. 109-115]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. J. Kasik; "STAGING User's Guide"; Air Force Final Report for contract F 33615-75-C-3096; March 1977]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. J. Kasik and H. W. Ramsey; "Interface Oriented Software Design"; Internal Boeing report; to be released]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>260999</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[D. E. Knuth; The Art of Computer Programming, Volume 1; 1968; pp. 423-429]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[S. S. Lamb, G. L. Smith, and W. L. Warren; "SAMM: A Modeling Tool for Requirements and Design Specification"; Proceedings 2nd Computer Software and Applications Conference; November 1978; pp. 48-53]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[J. C. Michener and J. V. Sroka; "Software for Realtime Graphics"; 1980 SID Symposium; vol. 11; p. 34]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1096492</ref_obj_id>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[L. J. Peters; Software Design: Methods &amp; Techniques; Yourdon Press; 1981]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988422</ref_obj_id>
				<ref_obj_pid>988420</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[D. S. H. Rosenthal; "Methodology in Computer Graphics Reexamined"; Computer Graphics; vol. 15, no. 2; July 1981; pp. 152-162]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804557</ref_obj_id>
				<ref_obj_pid>800139</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[G. L. Smith, S. A. Stephens, L. L. Tripp, and W.L. Warren; "A Tool to Support Design Automation in Batch Manufacturing"; Proceedings 17th Design Automation Conference; June 1980]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[D.A. Thompson and P. Kirschner; "Decision Tree Display Parameters for Interactive Graphics Computers"; Interactive Techniques in Computer Aided Design; September 1978]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A USER INTERFACE MANAGEMENT SYSTEM David J. Kasik Boeing Computer Services P.O. Box 24346 Seattle, Washington 
98124 Abstract The design and construction of the user interface to interactive systems is receiving 
increased attention. This paper describes a user interface management system that allows a designer/developer 
to focus on the logical functionality of an application without the usual bookkeeping associated with 
a con-ventional programming language. The user interface management system contains two com- ponents: 
a special purpose, application independent dialogue specification language and a run-time interpreter 
that provides a number of interaction extensions not possible with procedure libraries. CR Categories 
and Subject Descriptors: 1.3.4 Computer Graphics : Graphics Utilities -software support; 1.3.6 Computer 
Graphics Methodology and Techniques -device indepen-dence, ergonomics, interaction techniques, languages 
General Terms: Human Factors, Languages Additional Keywords and Phrases: user in-terface management 1.0 
INTRODUCTION The role of interactive computing has con- tinually increased as the cost of more powerful 
terminals and computing hardware has decreased. The advent of improved interac- tive computing tools 
places additional demands on a system's software capabilities. A successful interactive- system must 
provide expected functionality that an end user can access conveniently and data integrity for the results 
of the functions. Standard pro- gramming languages contain constructs well suited for algorithm specification; 
data base Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
management systems provide both data indepen- dence and integrity. Analogous capabilities for design 
and construction of interactive dialogue sequences that allow convenient function access must be embedded 
within con-ventional programming languages. Currently, the system designer is burdened not only with 
constructing functions to meet end user requirements, but also with screen layout, interrupt handling, 
and extensive legality checks. In other words, the designer must completely specify both the logical 
function-ality of the application (e.g., construct a line between two points) and the physical techniques 
for accomplishing that function (e.g., pick the line between two points phrase and then pick two point 
entities that have been previously displayed). This paper discusses a user interface management system 
that removes the burden of physical interaction handling from the system designer. It has been designed 
to give the end user powerful interaction techniques without adversely affecting overall system performance. 
Decoupling physical interaction handling from logical function performance is the foundation for a consistent 
tool for the construction of interactive dialogue sequences. The user interface management system described 
in this paper is part of The Interactive Graphics Engineering Resource (TIGER), which provides a framework 
for the development of engineering applications at the Boeing Company. The user interface contains its 
own programming language to define inter-active dialogue sequences distinct from the application and 
a run-time module that specifically handles an end user's physical interactions. 2.0 BACKGROUND A high 
level view of an interactive ap-plication is contained in Figure 2-1. USER ~" ~ APPLICATION ~ DATA Figure 
2-1. High level view of application &#38;#169; 1982 ACM 0-89791-076-1/82/007/0099 $00.75 In this figure, 
the application is seen to be a window between the user and his data. It is the responsibility of the 
application to provide techniques for the creation, de-letion, and manipulation of the data. Individual 
functions must be straightforward and responsive to allow their effective use within an application. 
A user must under- stand not only the function performed but also the underlying data manipulated. For 
example, the user of a text editor understands that he is working with a set of characters, and the functions 
he is provided let him manipulate those characters. The designer/developer of an application must perform 
three basic tasks. He must con- struct a user interface for the functions to be provided, develop algorithms 
to perform each function, and determine efficient storage techniques for the data. Both the user interface 
and data base aspects of the system are dependent on the functions to be performed. Interactive computer 
graphic ex-pression of an application significantly affects these aspects. To promote emphasis on algorithm 
(logical) development requires that the corresponding physical activities and data management be handled 
separately. Tools are currently available to allow a designer to concentrate on t~e logical design of 
a data base FPeters 8!] to support an application. Dat~ base management systems relieve much of the tedium 
of physical manipulation of mass storage from the appli-cation while providing additional capabilities 
like recovery, simultaneous use of data, and security. Such systems allow a data base to be described 
as a schema imposed on the abstract model of data (the models currently available are hierarchical, which 
uses a tree model, CODASYL or network, which uses sets as a model, and relational, which uses tables 
as a model) that is manipulated by system supplied functions. In Figure 2-2, the programmer has the responsibility 
of pro-viding the logical view that is application specific, and the data base management system supplies 
the abstract model and physical manipulation functions. ABSTRACT PHYSICAL MASS LOGICAL VIEW ~ ; ~ (SCHEMA) 
MODEL FUNCTIONS STORAGE Figure 2-2. Data base management system overview It should be noted that other 
system components can involve the specification of abstract models with which a programmer works rather 
than forcing the programmer to have knowledge of specific devices. A good example of th~s philosophy 
is the SIGGRAPH Core package |GSPC 79 1 for the support of device indepenBent con~uter graphics. Core 
has defined an abstract model of a graphics terminal as a virtual device. Commands to the device are 
managed in an auxilliary display file. The capabilities for display of and in- teraction with a picture 
are based on levels of abstract graphics terminals that contain different levels of native capabilities. 
In this way a program can be re-targeted for a new device without necessarily impacting the high level 
application code. SIGGRAPH Core supports the separation of physical graphics input device handling from 
an application as a set of procedures. More elegant interfaces to such procedures are pro-vided via precompilers, 
r A system ~or real-time avionics control LMichener 80J uses a precompiler to embed proper control procedure 
calls into an application. An interactive dialogue test bench that embeds procedure in- vocations inside 
an application for dialogue!esting Rurposes has also been developed Hanau 80 I. An integrated graphics/interface 
~t~ ~a~ri~v~eCBlt~d~tl. control within Some work has been done to allow an ap-plication designer to 
construct interactive dialogue sequences outside the algorithmic flow of an application. One modul~ wit~in 
a finite element application |Kasik 76]andlKasik 77| provided some degree oT explicit lo~ical-phgsical 
separation. The implementation allowed operation on either a high-speed re-fresh or a storage tube terminal 
with little change in the application. Further work with this user interface module produced applications 
for~the design and development o~ SAMM [Lamb 78] and IDEF-O [Smith 80] structured design charts. _Two 
other authors [Rosenthal 81] and[Feldman 82] have suggested that applicatio~ algorithms anB dialogue 
sequences should be physically eparated. ~ The IL language proposed in Feldman 821 also enforces logical 
separation ot contained in [Rosenthal 81T. Both systems suggest architectures simflar to that described 
in Section 3.0. However, neither have a language for interaction specification like the user interface 
TICCL language (Section 4.1) nor the extended interaction heuristics provided in the run-time interpreter 
(Section 4.2). In summary, a number of software tools and techniques have been constructed that help 
the designer enforce the separation of logical and physical operations within an application. Data base 
management systems present a programmer with a logical model of data and procedures that allow manipulation 
of the data within the confines of the model. Graphics management systems like SIGGRAPH Core present 
a standard set of operations for a conceptual terminal that are automatically mapped to different physical 
graphics devices. This same technique of logical and physical separation of interactive dialogue sequences 
from graphics devices has been adopted in the TIGER user interface management system. 3.0 OPERATING ARCHITECTURE 
Both data base management systems and graphics support packages are implemented traditionally as utilities 
invoked by proce- dure calls within an application. Physical procedure calls can be masked through the 
use of a precompiler and thus provide one more level of independence in the application. PHYSICAL DBMS 
STORAGE OTHER UTILITIES GRAPHICS USER PACKAGE Figure 3-1. Traditional Application architecture In the 
architecture shown in Figure 3-1, the application controls when and where calls are made to graphics 
utilities to collect input from a user. The location of these calls is often embedded directly within 
the code that is performing the function. Those commands are generally presented sequentially and do 
not allow a user to arbitrarily back up to modify a value that he has already changed or to skip steps 
without extensive special purpose code. A different view of an application is one in which the user dialogue 
is separated from the algorithmic portions of each function. The user specifies a set of information 
to begin execution of a process. The process can insure that it has necessary and sufficient information 
for its execution. Any interactive function can then be under-stood to contain an input collection phase, 
a function execution phase, and an output modi- fication phase. Output modification is needed if the 
function determines that the given input results in multiple or ambiguous solutions or is in error. For 
example, intersecting two curves may result in a number of different intersection points and the user 
may not wish to keep all of them. User dialogue is required only during the input collection and output 
modification phases. Note that the ultimate inter-pretation of the data that results from a function 
is the responsibility of the user. He may need other functions to aid in the in- terpretation process 
for fine-tuning the output in a design environment. In short, the user must be recognized as a key part 
of the system and that the output data is his ultimate product. The tradeoff between performance and 
capability plays a large role in the design of an architecture to accommodate the input-execution-output 
phases of a function. A system suffers if performance is degraded by increased functionality. The philosophy 
em-ployed in the design of the TIGER user interface management system is radically dif-ferent from the 
application calling a graphics input procedure as in Figure 3-1. Instead, the user interface management 
system is active when an application is being processed and is responsible for invoking application pro-cedures 
in response to specific user actions. The physical interactions a user performs are handled immediately 
by the user interface manager without application intervention or interpretation. This allows extended 
functionality with no performance degradation. Figure 3-2 contains an overview of this ex-ternal control 
philosophy. APPLICATION SPECIFIC MENUS   usER F- ",.I RAI PHYSICAL STORAGE Figure 3-2. Alternate (TIGER) 
Application Architecture There are two major differences between Figures 3-i and 3-2. First, the graphics 
module of Figure 3-2 contains output only capabilities. All interaction handling is ac-complished through 
the user interface. Device independence is preserved in the current im-plementation which currently supports 
the Evans and Sutherland Multi Picture System and PS300. Second, the user has become the controller of 
the application. The concept of external control of an application has made a number of interaction enhancements 
possible that would have been extremely difficult to implement in an application independent manner with 
the more traditional embedded control philosophy. 4.0 SOFTWARE COMPONENTS The interface between the user 
interface management system and the application is des-cribed in this section. The responsibility for 
physical interrupt handling and menu display has been given to a special module. This module operates 
independently of and supplies formatted information to the logical application procedures that contain 
the algo-rithmic flow of the complete system. The actual commands available to an end user are defined 
as part of the application and contain the user's logical interface to application functions. The application 
programmer needs consistent techniques to define the specific dialogue sequences to be displayed to the 
user and to receive user inputs at run-time. The TIGER user interface management system contains two 
components to address these needs. First, a special purpose programming language allows an application 
programmer to define dialogue sequences. The language is processed similarly to a schema definition language 
in a data base management system. A preprocessor compiles the dialogue sequences into a formatted menu 
file that is read by a run-time interpreter. Second, application procedures invoked by the run-time interpreter 
contain specific parameter lists. The parameter lists supply information to the application and provide 
a mechanism ~by which the application can modify the execution flow of an interactive dialogue se-quence 
in the event of an error condition. The separation between interaction language and parameter lists lets 
a programmer construct and maintain dialogue sequences in a language designed for that purpose and the 
actual processing algorithms in a suitable programming language.  4.1 TICCL Description The TIGER Interactive 
Command and Control Language (TICCL) gives a programmer a tool to define and organize interactive dialogue 
sequences. For example, a function may be required to construct a line between two points. TICCL allows 
the programmer to specify the function name and indicate that the user is to enter the two points by 
picking them on the screen. Figure 4-1 con-tains a TICCL fragment and associated PASCAL processing procedures. 
(Bold print denotes TICCL keywords.) Figure 4-1 contains examples of the principal TICCL keywords. The 
command con- tains a text string (Line Btwn 2 Pts) that will be disp]ayed to a user. If the run-time 
interpreter detects that the user has selected this phrase, it will follow the action_vector, which controls 
traversing the hierarchy. The ]eve] begins a new set of in-formation. The header contains a prompt message 
the run-time interpreter will display. The pick 9~ab]e field defines an application procedur~ that contains 
a speci-fication of what entities a user can pick at this point. The pick enable record contains a number 
of parameters-to control addition of information to a queue. The pick~rocessor is invoked upon conditions 
defined by the pick_enable routine; in Figure 4-1, N LnBt2Pt 1PS is called if exactly two enti- tTes 
are picked. The user interface provides the application the pick information through the first two parameters, 
and the application tells the user interface if there is an error through the errors parameter. TICCL 
CODE  command display = Line Btwn 2 Pts; definition = Line Between Two Points ; action vector = down 
1; end conmand l~vel header prompt = Select end points of line; end header enable pick enab]e= N 
LnBt2Pt 1ES ; pick-processor ~ N LnBt2Pt 1PS ;-J-action vector = up-l; -end enable  end_level PASCAL 
CODE P procedure N LnBt2Pt IES q (Far pick~enable : u_pickenrec); begin (**Set up parameters for picking 
entities.**) with pick_enable do begin autoexecflag := auto; minaccept := 2; maxaccept := 2; dups := 
lastc~eckqd; dupcheck := |xyzJ ; pickstyle := forget; intenstyle := intensify; end; end; procedure 
N LnBt2Pt 1PS ( queuelen :- u~acceptlim; var pickprocrec : u_pickprarr; var errors : integer); var end1, 
end2 : gpickidtype; ptl, pt2: Nt_Ent; Ln: Nt_Ent; begin (*** Retrieve data base key of picked points 
from user interfa#e~input array. ***) end1 := pickprocrecL1 j .pickid; end2 := pickprocrec[2] .pickid; 
 (*** Fetch point data from data base. ***) N Ent Fetch ( end1, ptl ); N Ent Fetch ( end2, pt2 ); (*** 
Generate an empty line entity. ***) N Ent Ln ( Ln ); (*** Calculate canonical form of line. ***) mlnent_2pts(ptl.ptdata, 
pt2.ptdata,Ln.cvdata); (*** Store line definition in data base.***) N Ent Create ( Ln ); (*** Draw the 
line on the display. ***) N Ent Draw ( Ln ); end; Figure 4-1. TICCL and PASCAL Fragments The TICCL language 
is strictly block structured and i) compiled into a triply-linked tree mKnuth 69|. Commands (delimited 
by co~mand/end_dommand) are organized into a set delimited bylevel/end level. The commands in a set have 
a specifi-c header/end header that describes a prompt message. Th-e simple hierarchy is well-suited for 
command organization that ~s convenient for the end user [Thompson 78|, the designer, and the implementer. 
In addition to the overall organization, the language provides techniques to selectively enable input 
devices for interactive parameter specification with enable/end enable. The model used is concep-tually 
si~lar to virtual input devices in SIGGRAPH Core and provides similar device in-dependence. A difference 
occurs in the distinction between Core's valuators and lo- cators and TICCL's output-oriented categorization. 
In TICCL, input devices like dials and tablets are categorized by the di- mensionality of their output. 
TICCL users seem to grasp the one, two, or three value categorization more readily than the Core locator/valuator 
distinction. The difference is one of style and not philosophy. The types of data the application can 
request at run-time are: --Entity identification (picking) --Alphanumeric data --Simple list of alternatives 
--One value (for example, dial output) --Two values (for example, tablet position information) --Three 
values (for example, sonic pen position information). A full description of TICCL syntax is beyond the 
scope of this paper. Other key- words provide more specialized command specification techniques and programmer 
com- fort features like macro definition and in-vocation, separate compilation, and inclusion of other 
source files. 4.2 Run-time Interpreter The run-time interpreter portion of the user interface management 
system assumes both input and output responsibility for all interactive dialogue. It uses the results 
of the compiled TICCL language as input. The interpreter displays all command sequences to the user. 
The form, positioning, and style of display are all contained within the interpreter. The current implementation 
pre- sents all information to the user in the form of menus. After a set of menus (all commands and prompt 
messages on a TICCL level) is displayed, the interpreter collects interrupts and processes them according 
to the constraints imposed by the static TICCL structure and dynamic inputs provided by the application 
at run-time. Receipt of a specific interrupt may cause the invocation of an application procedure for 
further pro-cessing. Interrupts that define input para-meters to a function are queued by the interpreter 
without application intervention until a specific command is requested or a pseudo-command condition 
(for example, a car-riage return in type-in mode) is received. The interpreter passes specific information 
to the application via parameter lists that are specifically tailored to a particular input. PASCAL record 
structures are defined as part of the run-time interpreter for use in the application. The run-time 
interpreter is responsible for imposing application defined constraints on dynamically entered information. 
For example, pick processing mode provides feedback, insures that the proper number of entities have 
been selected, and lets the user remove as well as add information to the pick queue without application 
intervention. The type-in processor lets the application programmer specify type, range, and default 
information about each field to be entered and insures that the entered values are parsed correctly. 
After a specific application procedure is invoked, the run-time interpreter examines the menu file to 
determine which set should be displayed next. The programmer has methods available to dynamically override 
the compiled path if additional information is needed or an error is detected. For example, a function 
may be provided to let the user change attribute values for a large number of entities. It is natural 
to let the user pick the entities before letting him enter new values. During normal processing, the 
user will enter a correct value and be able to execute a new function. However, if he makes an error 
during value entry, it is more de- sireable to allow him to re-enter the erroneous value without repicking 
the entities to be modified. The construction of an independent interpreter that controls application 
execu-tion rather than being embedded as utility procedure calls within the application offers new capabilities 
without extensive programming effort within the application itself. Examples of the extended capabilities 
within the user interface management system are: --command set display --implied reject processing --recursive 
command re-entry --situation dependent commands. Command Set Display. One of the more significant problems 
with a menu approach to display of dialogue sequences is that the user is often constrained to enter 
one parameter at a time in a predefined order. While good as a tutorial aid, one-at-a-time command processing 
is limiting for the more experienced user who understands the subtleties of the system. Embedded control 
of menus promotes this con-straint because code execution is required at the same time inputs are gathered. 
The TICCL technique of menu presentation attempts to strike a balance between one-at- a-time and batch 
command processing. TICCL allows the programmer to identify functions and sets of parameters as new levels 
in the language hierarchy. The run-time interpreter applies heuristics to examine the parameters and 
determine how far it can look ahead on a default path. All information on each level is displayed to 
the user with the default clearly identified. The specific heuristics currently implemented allow default 
path tracing to continue until (1) a leaf node on the tree is reached, (2) there is more than one default 
command on a level, (3) the user must dynamically enter information such as picking an entity, or (4) 
the next level to display overflows the menu area. The user can choose any command below his current 
level, and the interpreter automatically executes the proper procedures to skip unselected levels. The 
default path itself is constructed in one of two ways. First, the application can mark a command as a 
default through situation dependent tests as described later in this section. Second, the interpreter 
remembers the last path a user took down any one branch of the tree. If no explicit defaults are de- 
tected during lookahead processing, the in-terpreter uses the last command the user picked as default 
and continues looking. The first command on a level is assumed to be de- fault if the level has not been 
entered previously. In an interaction sequence like three-dimensional transformation of entities, the 
user may set up a complex translate, scale, and rotation and choose an entity. He may then realize that 
he needs to do the same operation on another entity. Because of de-fault processing, he will only need 
to choose the entity he wishes to transform without having to reconstruct the transformations. Implied 
Reject Processing. If a user is presented with a number of sets of commands and can pick a command below 
a specific level to imply acceptance of a default path, he can obviously pick a command above that level 
as well. During normal interaction processing, this implies that the user is rejecting any already executed 
commands and wishes to follow a path that may or may not be dis-played. The user is thus saved a step 
be- cause he does not have to specify an abnormal exit from the command sequence. If a user is transforming 
entities, he may have entered a translation and scale but decides that the translation value is incorrect. 
He need only select the translation option a second time to enter a new value. The scale value pre-viously 
entered will stay in effect because of default processing. Similarly, the user may have selected a function 
that he did not wish to execute and can exit by selecting another function. The programmer may have to 
supply code to allow for this abnormal termi- nation (or ABEND) condition to clean up data specific to 
a particular step. Recursive Command Re-entry. The natural decomposition of functions and parameters 
pro- vided in TICCL provides a reasonable path for most tasks. On occasion, an end user wants to leave 
his current path and return to the same spot with no information loss. Examples of recursion are rotating 
the view of a model to make identification of the second point to de-fine a line easier or temporarily 
constructing a line to aid in the construction of a point. In some cases he may even need to execute 
the function he is currently executing to do his task. For example, if the user is con-structing a line 
between two points, one of the points may be the result of the intersection of two temporarily constructed 
lines. The user must be not be prohibited from temporarily creating a line between two points because 
he is already in the function. This amounts to recursive re-entry of command sequences and is difficult 
to program in an embedded control architecture. In the TICCL environment, however, the programmer must 
only save data local to a specific function to allow recursive re-entry of that code. From an end user's 
point of view, recursion is invoked by picking a command at a higher level than the one he is currently 
on. If the command is defined as re-enterable, a recursive call to the chosen function is generated rather 
than an implied reject. The programmer can mark specific commands (generally those that define function 
boundaries) as re-enterable. Re-entry status can be turned on or off dynamically so a user can choose 
a command and have it re-entered or cause rejection of the current command. The interpreter automatically 
saves its current context, including all previously queued in- formation, when a function is recursively 
re-entered and restore the same context when recursion is exited. Situation Dependent Commands. A technique 
that is commonly used in an application using embedded user interface control is presenting commands 
to a user that are legal in a specific context. For example, some geometry construction functions such 
as line parallel to a line at a given distance are applicable only when a user is doing planar or two 
dimen- sional construction. By not presenting these functions to a user when he is working in three dimensional 
construction mode, many errors can be prevented before they occur. TICCL allows the programmer to associate 
a specific logical test that is executed before a command is displayed. In this way, values that vary 
during the execution of the appli-cation can be examined to determine if a command is legal or not. In 
the example above, the test associated with the line parallel to a line at a given distance command in 
TICCL would be: If two d construction then display the command else blank the command. 5.0 IMPLICATIONS 
The presence of a user interface management system that is external to an ap-plication forces some changes 
in the way interactive systems are currently designed and constructed. A language that allows a designer 
to specify dialogue sequences out- side the application makes it more natural to design the dialogue 
before (or at least in parallel with) specific algorithms. The end user is provided a more powerful and 
consistent interaction capability. The pro- grammer sees a correlation between dialogue sequences and 
code that allows direct trace-ability to the end user interface. Current program design techniques are 
oriented to al-gorithm design, and new techniques that allow specific interactive ~ialogue ~equence design 
have been constructed tKasik 82]. The end user, the application programmer, and the system designer all 
benefit from the introduction of a user interface management system. The end user is presented with con-sistent 
display and processing style for all dialogue sequences. If he must access a dif- ferent application 
with different function-ality, the format and basic interaction techniques remain the same. The end user 
can thus access more applications without extensive retraining to learn new key posi- tions and the like. 
Two different appli-cations have been implemented with TICCL. The first allows construction and manipulation 
of curves and surfaces in three space. The second is experimental and pro- vides construction, query, 
and manipulation functions for planar polyhedra. Both systems have the same interface appearance even 
though the functions provided are radically different. The application programmer has the burden of interrupt 
processing removed because much of the bookkeeping required in queueing and parsing interrupts is assumed 
by the run-time interpreter. The system designer is provided a consistent set of capabilities for specifying 
functions and parameters that is independent of terminal type. He can concentrate on design of easy to 
use rather than easy to implement dialogue sequences.  6.0 CONCLUSION The TIGER user interface management 
system is being developed as part of an effort to provide significant computer aided engineering design 
capabilities at Boeing. It contains unique concepts to increase consistency for the end user through 
the in-troduction of a dialogue sequence specification language. The external control of the application 
by the run-time inter-preter portion of the system provides extensive interaction improvements through 
the use of default path following and recursive command execution at no penalty to the application programmer 
and positive benefit to the end user. ACKNOWLEDGEMENTS. The design and construction of the specific 
user interface management system within TIGER has had many contributors. Specific thanks go to E. C. 
Edwards who acted as a sounding board early in the design pro- cess, D.T. Sanford for suggesting default 
path tracing, L.C. Carpenter for an excel-lent phase one implementation that provided many of the path 
tracing heuristics, W.T. Hubbard for constructing the phase one TICCL compiler, J.R. Houser for assuming 
phase two implementation which introduced recursion, and R. Katz for his editorial contributions to this 
paper. Early investigations into the practical use of the user interface management system and construction 
of methodologies that make it useable were conducted by H.W. Ramsey, L. B. Kelso, and J. A. Jaech. REFERENCES 
[Bleher 80] J. H. Bleher, P. G. Caspers, H. H. Henn, and K. Maerker; "A Graphic Inter- active Application 
Monitor"; IBM Systems Journal; vol. 19, no. 3, 1980; pp. 382-402 [Feldman 82] H. B. Feldman and G. T. 
Rogers; "Toward the- Design and Development of Style-independent Interactive Systems"; Human Factors 
in Computer Systems, Mar, 1982; pp. 111-116  [GSPC 79] Status Report of the Graphics Standards Planning 
Committee; Computer Graphics; vol. 13, no. 3; August 1979, pp. II-71 -II-90 [Hanau 80] P. R. Hanau and 
D. R. Lenorovitz; "Prototyplng and Simulation Tools for User/Computer Dialogue"; Computer Graphics; vol. 
14, no. 3; July 1980; pp. 271-278  [Kasik 76] D. J. Kasik; "Controlling User Interaction"; Computer 
Graphics; vol. 10, no. 2; July 1976; pp. 109-115  [Kasik 77] D. J. Kasik; "STAGING User's Guide"; Air 
Force Final Report for contract F 33615-75-C-3096; March 1977 [Kasik 82] D. J. Kasik and H. W. Ramsey; 
"Interface Oriented Software Design"; Internal Boeing report; to be released [Knuth 69] D. E. Knuth; 
The Art of Computer Programming, Volume 1; 1968; pp. 423-429 [Lamb 78] S. S. Lamb, G. L. Smith, and W. 
L. Warren; "SAMM: A Modeling Tool for Requirements and Design Specification"; Pro- ceedings 2nd Computer 
Software and Applications Conference; November 1978; pp. 48-53 [Michener 80] J. C. Michener and J. V. 
Sroka; "Software for Realtime Graphics"; 1980 SID Symposium; vol. 11; p. 34 Computer Graphics Volume 
16, Number 3 July 1982 I Peters 81] L. J. Peters; Software Design: ethods &#38; Techniques; Yourdon Press; 
1981 [Rosenthal 81] D. S. H. Rosenthal; "Methodology in Computer Graphics Re- examined"; Computer Graphics; 
vol. 15, no. 2; July 1981; pp. 152-162 [Smith ~I G. L. Smith, S. A. Stephens, L. L. Tripp, W.L. Warren; 
"A Tool to Support Design Automation in Batch Manufacturing"; Proceedings 17th Design Automation Conference; 
June 1980 [Thompson 78] D.A. Thompson and P. Kirschner; "Decision Tree Display Parameters for Interactive 
Graphics Computers"; Interactive Techniques in Computer Aided Design; September 1978  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801269</article_id>
		<sort_key>107</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>18</seq_no>
		<title><![CDATA[The device model of interaction]]></title>
		<page_from>107</page_from>
		<page_to>114</page_to>
		<doi_number>10.1145/800064.801269</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801269</url>
		<abstract>
			<par><![CDATA[<p>Any interactive system can be described in terms of the devices it involves, and their interconnections. Similarly, each device can be described in terms of simpler devices and their interconnections. Such descriptions are strictly modular, and well structured.</p> <p>This observation allows any system to be described, at all levels, by the same language. Such descriptions have intuitive appeal for hardware as well as software components, and for process control applications as well as human-machine interaction. The Device model of interaction, as described here, can ease the job of designing user- friendly interactive systems, and can be adapted for automatic compilation.</p> <p>As an example, the design of an actual system component is discussed. The design is presented, at several levels, in a Pascal-like notation. It represents a module created to provide a human-machine interface via a graphic tablet, keyboard and video monitor.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>D.3.3</cat_node>
				<descriptor>Abstract data types</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Device independence**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.3.3</cat_node>
				<descriptor>Control structures</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10011027</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Control structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011008.10011024.10003202</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->General programming languages->Language features->Abstract data types</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31027575</person_id>
				<author_profile_id><![CDATA[81100109760]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Mathematics, Northeastern University, Boston, Massachusetts]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>807432</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anson, E. The semantics of graphical input. Proc. SIGGRAPH 79 (Computer Graphics) 13, 2 (Aug. 1979), 113-120.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357147</ref_obj_id>
				<ref_obj_pid>357146</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Borning, A. The programming language aspects of ThingLab, a constraint-oriented simulation laboratory. ACM TOPLAS 3,4 (Oct. 1981), 353-387.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Brinch Hansen, P. EDISON: A Multiprocessor Language. Computer Science Department, University of Southern California, 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brinch Hansen, P. The Design of EDISON. Computer Science Department, University of Southern California, 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807394</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Britton, E.G., Lipscomb, J.S., and Pique, M.E. Making nested rotations convenient for the user. Proc. SIGGRAPH 78 (Computer Graphics) 12,3 (Aug. 1978), 222-227.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Dahl, O.-J., and Hoare, C.A.R. Hierarchical program structures. In Structured Programming, Dahl, O.-J., Dijkstra, E.W., and Hoare, C.A.R. (Eds.), Academic Press, New York, 1972, pp. 175-220.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, J.D., and Wallace, V.L. The art of natural man-machine conversation. Proc. IEEE 62,4 (April 1974), 462-471.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kay, A. Smalltalk. In Methodology of Interaction, Guedj, R.A., et al (Eds.), North-Holland, Amsterdam, 1980, p. 7-11.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Liskov, B.H., and Zilles, S.N. Specification techniques for data abstractions. IEEE Trans. on Software Engineering 1, 1 (March 1975), 7-19.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359789</ref_obj_id>
				<ref_obj_pid>359763</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Liskov, B.H., Snyder, A., Atkinson, R., and Schaffert, C. Abstraction mechanisms in CLU. Comm. ACM 20,8 (Aug. 1977), 564-576.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Plasmeijer, M.J. Input Tools: A Language Model for Interaction and Process Communication. PhD Thesis, Katholieke Universiteit, Nijmegen, The Netherlands, 1981.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988422</ref_obj_id>
				<ref_obj_pid>988420</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Rosenthal, D.S.H. "Methodology in Computer Graphics" Re-examined. Computer Graphics 15, 2 (July 1981), 152-162.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[SIGGRAPH-ACM GSPC. Status report of the Graphic Standards Planning Committee. Computer Graphics 13,3 (Aug. 1979).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Tesler, L. The Smalltalk environment. BYTE 6,8 (Aug. 1981), 90+.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807367</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Van den Bos, J. Definition and use of higher-level graphics input tools. Proc. SIGGRAPH 78 (Computer Graphics) 12,3 (Aug. 1978), 38-42.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>357141</ref_obj_id>
				<ref_obj_pid>357139</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Van den Bos, J., Plasmeijer, R., and Stroet, J. Process communication based on input specifications. ACM TOPLAS 3,3 (July 1981), 224-250.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 EdAnson Department of Mathematics Northeastern University Boston, Massachusetts ABSTRACT Any interactive 
system can be described in terms of the devices it involves, and their interconnections. Similarly, each 
device can be described in terms of simpler devices and their interconnections. Such descriptions are 
strictly modular, and well structured. This observation allows any system to be described, at all levels, 
by the same language. Such descriptions have intuitive appeal for hardware as well as software components, 
and for process control applications as well as h~an- machine interaction. The Device model of interaction, 
as described here, can ease the job of designing user- friendly interactive systems, and can be adapted 
for automatic compilation. As an example, the design of an actual system component is discussed. The 
design is presented, at several levels, in a Pascal-like notation. It represents a module created to 
provide a human- machine interface via a graphic tablet, keyboard and video monitor. CR Categories and 
Subject Descriptors: 1.3.6 [Computer Graphics]: Methodology and Techniques - Device independence; Interaction 
techniques; Languages; D.3.3 [Progran~ing Languages]: Language Constructs -Abstract data types; Control 
structures. General Terms: Design, Languages, Theory. INTRODUCTION Design and implementation of complex 
interactive systems, with graphics, continues to increase in importance. This can be demonstrated by 
a trip to any arcade, or any of the many Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the industries now becoming dependent on the tech- nology. Unfortunately, 
progress in this area is impeded by the lack of adequate models of inter- active systems. The Device 
model describes an interactive system (or device) in terms of its component devices and the data paths 
(channels) between them. Such a description is complete, yet concise, and is closely related to the 
user's intuitive understanding of the system. A previous paper [i] introduced a model for simple devices. 
Here, we expand and adapt that model to include composite devices and entire systems. Then we apply the 
new model to the design of a user- friendly interactive interface. Finally, we compare the Device model 
with several other approaches to interaction, with particular attention to clarity, modularity and completeness. 
 SIMPLE DEVICES A simple device is one whose description does not include other devices. That is, it 
is the atomic unit from which composite devices are built. It performs its role in a system by interacting 
with other devices in a controlled manner. A detailed description, with examples, is provided in [i]. 
This section briefly describes simple devices, and the next section extends the theory to include composite 
devices. Description of any device is from two perspec- tives: the external behavior ("outside") and 
the implementation ("inside"). The outside of a device is the way it inter- acts with other devices. 
A device acts on other devices by signalling events, and by making part of its state visible to other 
devices. A device is affected by other devices when it acts on events, or when it examines the visible 
state of other devices. The outside thus consists of: i) Visible state, 2) Events, and 3) Actions.  
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
(~) 1982 ACM0-89791-076-1/82/007/0107 $00.75 In principle, an event is equivalent to a change of state. 
When an action occurs, its effect is described as a state change, or an event, depending on what is more 
convenient. ~,ese strongly interrelated elements are separated here to si~plify descriptions.  107 
For example, an ASCII keyboard is a simple device. It responds to an external event (finger pressing 
a key) by signalling an event with an ASCII character which represents the key pressed. The event corresponds 
to the change in state (of one key) from up to down. The reverse change (releasing the key) is not signalled 
by most hardware. The inside of a device is the way it implements the outside. It consists primarily 
of I) Variables and functions to implement the state, and 2) Procedures to implement actions, which may 
signal events. Additional procedures and variables my be included inside a device, which are not visible 
to other devices. The device's state remains unchanged between successive actions. The following Pascal-like 
notation describes a simple device called an unbounded valuator [5]. It uses the visible state of a valuator 
with a fixed range, together with a user-controlled clutch, to provide an unbounded value. Typically, 
this device would be connected to a knob (which gives an integer in a fixed range), but it can work with 
any device that yields an intege[. TYPE UnboundedValuator(InitialValue:integer) = DEVICE VARClutch~gaged: 
Boolean; bias: integer; EXTERNAL BoundedValue: integer; INITIAL BEGIN ClutchEngaged :-- false; bias 
:= InitialValue END; * Function value: integer; BEGIN IF ClutchEngaged THEN value := BoundedValue + 
bias ELSE value := bias END; ON Increment( amount: integer ) bias := bias + amount; ON ClutchDisengage 
BEGIN bias := value; ClutchEngaged := false'  E~D; ON ClutchEngage BEGIN bias := value - BoundedValue; 
ClutchEngaged := true END;  END;  The key word Ek'r~NAL indicates that the name BoundedValue refers 
to part of another device's state. The key word INITIAL indicates an action which takes place only once, 
when the device is created. The asterisk (*) before the function definition indicates that its value 
is visible outside the device. The procedures, beginning with ON, implement actions, which respond to 
events signalled by actions in other devices. A user's actions, to change the value (knob twisting, 
clutching, etc.), need not be synchro- nized in any way with the device that queries the value. The value 
may be queried any number of times before it is changed, or may be changed any number of times before 
it is queried. The only limitation is that it cannot be queried during the few microseconds required 
to effect the change, which may cause sc~e small delays. In short, it acts like a real (hardware) device. 
 This device description can be stored in a library, and used for many applications, or can be implemented 
directly by hardware. The next section shows how to arrange devices into complex devices or systems. 
 COMPOSITE DEVICES An interactive system is a complex device, which responds to a user's behavior (events), 
observes the positions of things like knobs (external state), and displays information on a screen (visible 
state). It is, itself, composed of several devices, and behaves as it does because of the way those devices 
interact with one another. This section shows how to describe such interactions of devices. Whether 
simple or composite, a device is an object very much like a program variable. That is, it has a value 
(state) which can be observed, and can be manipulated by means of specific operations (actions). The 
only significant difference is that variables do not affect other variables directly. By extending the 
concept of abstract data type, to include event signalling, we can incorporate devices within the formalism. 
The following description is thus an extension of the modular type definition schemes used in languages 
such as CLU [9, I0], SIMULA [6], and EDISON [3,4]. Device Declaration Declaration of a device may be 
direct, as in: VAR UnboundedValuator: DEVICE or indirect, as in: TYPE UnboundedValuator = DEVICE 
VAR Vl,V2: UnboundedValuator; D: array[l..10] of UnboundedValuator; Devices may be used as variables 
within any procedure, or within another device. In either case, a special statement is required to specify 
the interconnections of the devices. This is described below. The usual dynamic scope rules for variables 
apply to devices and their interconnections. That is, the devices declared in a procedure exist, and 
are connected as specified, during a single execution of the procedure. This provides for dynamic mode 
 108 changes. The devices declared within a device are initialized prior to the outer device's INITIAL 
action, and exist as long as the outer device does. The actual hardware devices available to the system 
are implicitly declared as global. As a consequence, they may be referenced anywhere in the system. Similarly, 
any software device or device type is "available within the usual static scope of its declaration. A 
device type declared in a library has global scope. Device ConfiGuration One device communicates with 
another by refer- encing part of its outside. When devices are declared within a fixed environment, it 
may be convenient to refer to their elements by fully qualified names (e.g., V2.value). However, when 
a device type is to be included in a library, the actual names and types of connected devices cannot 
be known in advance. The CHANNEL statement resolves the difficulty, by enabling late binding of devices. 
 The binding of an event to an action, or a visible state element to an external state element, is a 
channel. Using a fully qualified name implicitly defines a channel. Unqualified names are dun~nies, requiring 
that channels be explicitly defined by the CHANNEL statement. Each channel is an ordered pair, with the 
source (event or visible state) given first, and the destination (action or external state) given last. 
 Events, actions and external state elements can be grouped, with the implication that any of the sources 
will go to all of the destinations. Of course, the source must be unique if it is a state. The following 
example illustrates the composition of a compound device, using a previ- ously declared device. TYPE 
UnboundedLocator(InitialLocation:Location) = DEVICE E~TKF~NAL BoundedLocation: Location; VARVx(InitialLocation.X), 
Vy(InitialLocation.Y): UnboundedValuator; *Function unboundedLocation: Location; BEGIN UnboundedLocation 
:= (Vx.value, Vy.value) END; CHANNEL (*Engage, [Vx.ClutchEngage, Vy.ClutchEngage]); (*Disengage, [Vx.ClutchDisengage, 
 Vy.ClutchDisengage]); (*BoundedLocation.X, Vx.BoundedValue); (*Boundedlxx~ation.Y, Vy.BoundedValue); 
 ON RelMove( dX,dY: integer ) BEGIN Signal Vx.Increment(dX); Signal Vy.Increment(dY) END END; { 
unboundedLocator }  This device definition assumes that a type Location is defined globally, and has 
fields X and Y. Its input is a Location, which is assumed to be bounded, and its output is also a Location 
but without bounds. It recognizes events Engage and Disengage. The component devices each provide one 
element of the Unbounded Location. The CHANNEL statement implies that the Engage event concurrently 
invokes the ClutchEngage events in Vx and Vy. It is thus equivalent to: ON Engage BEGIN Signal Vx. ClutchEngage; 
Signal Vy. ClutchEngage END; Since execution of events in separate devices is concurrent, it is necessary 
to provide mutual exclusion during the access to their respective data structures. This is accomplished 
by permit- ting only one action to be in progress, in any device, at any time. Any reference to a device 
which is active, implicitly delays the current device a short while. However, nested devices may be active 
concurrently with the enclosing device. These rules, together with a prudent system design, allow considerable 
parallelism without undesirable race conditions or deadlocks. Sufficient power is provided, by this 
notation, to describe any device or system. However, greater convenience could be obtained by introducing 
higher-level constructs for defining groups of channels. This is a topic for further research. AN EXAMPLE 
 At Northeastern University, we are developing interactive tools to support computer mediated instruction 
and testing in Mathematics. Part of the system, now in use, is a graphics editor. Its user can create 
canned displays consisting of lines, polygons, circles, sketches, plots of functions, and text. The displays 
are stored in an indexed file, and can be shown by an application program. The editor also allows additional 
data to be associated with each display, so considerable flexibility can be put into a lesson or test, 
without any complicated programming. We considered a machine independent design. However, we observed 
that "machine independent" usually means that, no matter which machine you run it on, it doesn't work 
particularly well. We chose instead to make the system work as well as possible on our equipment, taking 
advantage of its unique combination of features. Neverthe- less, the design, as presented here, limits 
machine dependence, and confines it to a few modules. This section briefly describes some interest- 
ing aspects of the design. Many important details are omitted. The intent is to show how the Device model 
contributes to a good design. 109 The ~ystem Environment The editor is embedded in a system consisting 
of a Terak 8510 microcomputer and a Stmm~graphics BitPad One graphic tablet. The Terak includes (among 
other things) a keyboard, and a video monitor which refreshes from processor memory. We assume here 
that these devices are pre- defined, with enly their outsides known: The Tablet signals an event, whenever 
it senses the stylus near its surface: Signal StylusSense( Loc:Location; z:Boolean ); The X and Y fields 
of Loc give the stylus position (ranging 0..2199), and z indicates whether the tip switch is closed. 
When the stylus is held near the tablet's surface, this event occurs at least every ten milliseconds. 
When the stylus is lifted, no events occur. The Keyboard signals an event whenever a key is pressed: 
 Signal KeyPressed( CH: char ); The value of CE indicates which key was pressed. If a key is held down 
for more than one second, the event repeats, every i00 ms., until the key is released. A system Clock 
signals an event 60 times per second: Signal tick; The video monitor continually refreshes its display 
from a frame buffer (external state), which is part of the system's visible state. The User Interface 
 The application program interacts with its user by means of button presses, cursor position, and graphic 
images. The user interface trans- lates between these abstract notions and the physical realities of 
the devices being used. Buttons that can be pressed include the keys on the keyboard, and virtual buttons 
(rectangles) drawn on the graphic tablet's surface. However, some of these buttons are not interesting 
to the application program, since they are used only to control the cursor. The cursor can be moved by 
either the graphic tablet, or by certain keys on the keyboard. It is represented, to the applica- tion 
program, simply as a Location. It is repre- sented, to the user, as a graphic image. The user may change 
the shape of the cursor, at any time, by simply pressing a button (either on the key- board or the tablet). 
The user interface sorts out the physical events caused by the user, and maintains the cursor, reporting 
only interesting (synthesized) events and values to the applica- tion program.  The user interface 
includes four embedded devices, each of which represents part of the user's view of the interface. The 
devices are called ButtonTablet, FunctionKB, Cursor, and Display. Omitting lower-level details, the user 
interface is described as follows: UserInterface: DEVICE TYPE CursorStyle = (Dot, DetCircle, CrossHairs, 
Invisible); *ButtonCode = ( codes for virtual buttons ); VAR ButtonTablet: TabletWithOverlay; FunctionKB: 
KeyboardWithFunctions; Cursor((0,0)): UnboundedLocator; Display: DEVICE CHANNEL (cursor.UnboundedLocation, 
[*cursorPosition, Display.CursorPosition]); (ButtonTablet.PenPosition, Cursor.BoundedLocation); (ButtonTablet.PenDown, 
Cursor.Engage); (ButtonTablet.PenUp, Cursor.Disengage); ([ButtonTablet.ApplicationButton, FunctionKB.ApplicationKey], 
*Button); ([ButtonTablet.Style, FunctionKB.Style], Display.SetCursorStyle); (FunctionKB.RelMove, Cursor.PelMove); 
 ~D; ButtonTablet corresponds to the graphic tablet with the overlay (including function buttons) added. 
FunctionKB is the keyboard, with some keys interpreted as function keys. ButtonTablet and FunctionKB 
are described more fully below. Cursor is an unbounded locator. Because of the channel specifications, 
the cursor follows the direction of the pen when it is in the drawing area, and can be moved by keys 
on the keyboard. Lifting the pen disengages the cursor's clutch, so that the pen can be moved to another 
part of the drawing area without affecting the cursor. A sweeping motion of the pen moves the cursor 
any distance desired, including off the screen. This proves useful for stretching rubber lines and circles, 
etc. (The actual implementation also includes an absolute cursor, which is not included here.) The Display 
device maintains an image of the cursor, and the images specified by the application program, in a frame 
buffer made available to the video monitor. It finds the cursor's location in an external variable called 
CursorPosition, and responds to requests (i.e., the SetCursorStyle event) to change the cursor's appearance. 
Details of this device are ~nitted, for the sake of brevity, as are several features of the other devices. 
 ii0 Button Tablet The physical graphic tablet is fitted with an overlay, which divides its surface 
into a drawing area and several function buttons. Some of the function buttons control the cursor style, 
and others are for use by the application program. The stylus, when in the drawing area, controls a bounded 
locator. The PenDown and PenUpevents signal the stylus moving into and out of the drawing area. The device 
includes simpler devices, to represent the function buttons and the drawing area. It also makes use 
of a simpler type, called TabletWithSense. Since the actual tablet signals an event only when it senses 
the stylus (and does nothing otherwise), this device type is required to synthesize knowledge about whether 
the stylus is being sensed by the tablet. It signals a PenAt event whenever the stylus is sensed at a 
new position, or the tip switch is opened or closed, and signals NoPen whenever the stylus is lifted. 
We thus have a device which reflects everything the user does with the stylus. The details of this device 
are not given here. TYPE TabletWithOverlay = DEVICE TYPE TabletButton(LOC: screen region; ID) = DEVICE 
 VARdown: Boolean; INITIAL down := false; ON PenPress( PenPosition:Location; z:Boolean ) IF z and 
(PenPosition is in LOC) THEN IF down THEN { ignore } ELSE BEGIN down := true; Signal ButtonPress( 
ID ) END ELSE down := false;  ON PenRelease down := false END; { TabletButton } VARTablet: TabletWithSense; 
StyleControl: array[a:CursorStyle] of TabletButton(iOCation of a, a); UserButton: array[a:ButtonCode] 
of TabletButton(location of a, a); DrawingArea: DEVICE VARPenInArea: Boolean; *DrawingLocation: Location; 
 INITIAL BEGIN PenInArea := false; DrawingLocation := (0,0); Signal PenUp END; PrOCedure PenLost; BEGIN 
 IF PenInArea THEN BEGIN PenInArea := false; Signal PenUp; ELSE { skip } ~D; ON PenChange( PenPosition:Location; 
 z:Boolean ) IF PenPosition is within drawing area THEN BEGIN DrawingLocation := PenPosition; IF PenInArea 
~HEN { skip } ELSE BEGIN PenInArea := true; Signal PenDown END END ELSE PenLost; { from drawing area 
}  ON NoPen PenLost END; { DrawingArea } CHANNEL (Tablet.PenAt, [DrawingArea.PenChange, StyleControl(*).PenPress, 
UserButton(*).PenPress] ); (Tablet.NoPen, [DrawingArea.NoPen, StyleControl(*).PenRelease, UserButton(*).PenRelease] 
); (StyleControl(*).ButtonPress, *Style); (UserButton(*).ButtonPress, *ApplicationButton); (DrawingArea.DrawingLocation, 
 *PenPosition); (DrawingArea. PenUp, *PenUp) ; ~D; (DrawingArea.Penlk)wn, { TabletWithOverlay } *PenDown); 
 Function Keyboard The actual keyboard is a simple alpha- numeric, ASCII- coded keyboard. Since the 
user interface operates in terms of a keyboard with function keys, the KeyboardWithFunctions type is 
included to simulate the desired hardware. Cursor moving keys have an acceleration feature. In addition, 
this device provides the ability to type characters in an alternate font (normally used to provide underscores), 
toggled by a function key. TYPE KeyboardWithFunctions = DEVICE SaxSpeed = 5;  VAR LastKey: char; AltCharset: 
Boolean; TimeElapsed: integer; Speed: integer; EVENT ApplicationKey( CH: ButtonCode ); Style( NewStyle: 
CursorStyle ); RelMove( dX,dY: integer ); INITIAL BEGIN LastKey := NUL; AltCharset := false; TimeElapsed 
:= 0 END; iii ON Keyboard.KeyPressed( C~: char ) VARdX,dY: integer; BEGIN CASE (class of CH) of 
cursor movement: BEGIN set dX,dY according to direction; IF (TimeElapsed < 8) and (CH=LastKey) THEN 
Speed := min(Speed+l, MaxSpeed) ELSE Speed := i; Signal RelMove( dX*Speed, dY*Speed ) END; cursor style: 
Signal Style(requested style); font toggle: AltCharset := notAltCharset; alpha-numeric: IF AltCharset 
THEN Signal ApplicationKey(chr(ord( C~)+128)) ELSE Signal ApplicationKey( CH ) E~D; LastKey := C~; 
 TimeElapsed := 0 ~gD; ON Clock.tick TimeElapsed := TimeElapsed + 1 END;  Implementation and Evaluation 
 Although this design is presented here in an extended notation, the actual implementation was in UCSD 
Pascal. Since the implementation language does not support events or channels, the design was hand translated 
into a rough function- al equivalent. The i~plementation works well, but not as smoothly as designed 
here. Events were neces- sarily simulated by polling of device status, with a resulting loss of power 
and responsiveness. For example, the keyboard is momentarily locked whenever drawing is in progress. 
 Unfortunately, the modular structure of the design bad to be sacrificed in the translation. The result 
is a program which is less easily understood, and more machine dependent, than this design would indicate. 
Inversion of the control structure was required, to simulate events with device polling, and devices 
had to be combined in non- intuitive ways. We take this as one indica- tion that the Device model of 
interaction provides a better modular structure than the usual language extensions, such as the CORE 
[13]. OTHER APPROACHES The Device model of interaction is a new  approach to the description of interactive 
systems. It has been designed to permit greater clarity, modularity and completeness, than are possible 
with other approaches. The next few paragraphs compare these other approaches, including their relative 
strengths and weak- nesses. In the many interactive graphics systems available, three distinct approaches 
appear. These include i) the graphics package, 2) the linguistic model, and 3) object-oriented inter- 
action. Graphics PackaGes Graphics packages are typified by the CORE proposal [13]. They provide procedures 
for displaying graphics, and for obtaining graphical input in a device-independent form. They provide 
no special representation for the structure of the interaction. Since the interactive protocol is distributed 
throughout a large number of subroutine calls, it is not easy to determine whether it is consis- tent, 
or exactly what the pattern is. The CORE's virtual device concept pushes the input devices into modules, 
but does not provide for modularity at higher levels, and does not permit the use of all useful characteristics 
of the physical devices. A more complete criticism of this approach may be found in [1,12]. Linguistic 
Interaction Patterns of interaction are modelled as linguistic exchanges by Foley and Wallace [7], who 
also advocate virtual devices. The linguis- tic model treats the interactive dialog as a finite state 
controller, augmented by program variables, with graphic feedback added to the state transitions. Since 
a finite controller defines a specific grammar, the pattern of interaction is clearly stated, and consistency 
can be checked with a little effort. Modularity of this approach was improved by the Input Tool Model 
(ITM) of van den Bos [15, 16], which also allows actual devices to be used. Interchangeable modules, 
called tools, each associate a small granm~r with an action to be performed when the user completes a 
phrase of its language. Although the notation used for the grammar allows only regular expressions, it 
is augmented by conditions on program variables. It is thus isomorphic to the finite controller model, 
and provides a more convenient notation. Linguistic modelling has the advantage of providing a clear, 
compact representation for an interactive language. The ITM system allows modular representation of allowable 
sequences of user actions. It has the disadvantage that it cannot ade- quately represent the features 
of real inter- active devices. For example, it cannot simulate a device (such as a knob) which retains 
its value between uses and which can be changed by the user at any time. Instead, any state element which 
is retained between invocations (by the program), must be stored outside the tool. Furthermore, a simulated 
tool can be manipulated by the user only when the program explicitly requests input from it. This limits 
the user's options. On the other hand, the Device model uses the same nota- tion to describe hardware 
and software, and 112  stores the device's state within its defining module, allowing the user to manipulate 
it at any time. Another disadvantage is intrinsic to the linguistic approach. It requires the user 
to think in terms of allowable sequences of actions, or a grammar. For many applications, it seems much 
better to allow the user to think in terms of direct action on some objects or devices. The linguistic 
approach results in large numbers of modes, which tends to limit the options of the user. An object- 
oriented model is preferred, since it encourages the user to view the system as an object, with simple, 
consistent rules for use. One might ask, would electronic arcade games be as popular as they are, if 
they included large numbers of modes, and required their users to model the interaction as a sequence 
of requests? The illusion of direct, manual control of objects seems to be an essential component of 
their com- pelling attraction. Similar observations might be made, regarding more serious applications. 
 A further objection to the linguistic approach, is that it does not meaningfully show what happens in 
each mode. The content of a granm~r is oriented about the allowable sequences of modes (or sub-modes). 
The state diagram for a modeless system would consist of one state and a multitude of transitions pointing 
hack to it. To the extent that actions within a mode are reflected by a granm~r, the approach resembles 
a degenerate form of the Device model. That is, the transitions represent actions on the state of the 
system. The Device model extends this concept, and makes it modular, so that we can see more clearly 
what is happening. Object-OrieDted Interaction Alan Kay's Smalltalk [8, 14] is the prototype object- 
oriented interactive system. Of all approaches currently in use, it is the most like the Device model. 
It builds a system out of independent objects, which send messages to one another. Each object has a 
method for responding to each type of message it can receive, and is responsible for maintaining its 
own state. Interactive devices are objects, whose methods have side effects in the physical world. An 
extension of Smalltalk, called ThingLab [2], provides for automatic adjustment of objects, in response 
to changes in others, controlled by constraints. Messages are very similar, in intent, to events. Visible 
state can be simulated by the value returned from an object in response to a message. Differences from 
the Device model include modularity and flexibility. Binding of messages to receivers is part of a method 
in Smalltalk, and is thus dynamic. Indeed, message sending and routing are the essence of the language. 
The Device model treats event binding as a static, structural issue. Corsnunication is separated from 
action, where in Smalltalk all action is  implemented by means of con~nunication. The advantage to Smalltalk 
is uniformity. The advan- tage to the Device model is a separation of concerns, and greater clarity 
of the patterns of interaction. The Device model recognizes that devices may be implemented on separate 
processors. Devices are expected to operate concurrently. On the contrary, Smalltalk requires an object 
to function sequentially with the destinations of its messages. A secondary difference relates to implementa- 
tion. Smalltalk appears to require an inter- preter, or a high-level microprogram, in order to implement 
its objects. Although the Device model has not yet been implemented, it appears that compilation into 
a conventional machine language is feasible. Suitable optimization of compiled code can result in systems 
which provide much better response times than interpreted code can. The Device model exchanges only a 
litt[~ of Smalltalk's power for simplicity, modularity and reduced demands on the hardware. CONCLUSIONS 
 The Device model of interaction can be used to describe interacting objects and systems of any kind 
or conplexity. It can be used to describe the behavior of a graphical input (or output) device, or to 
i~lement it as a program. Since it deals with states, events and actions, it has intuitive appeal for 
process control applications as well. This flexibility is particularly appeal- ing, since h~aan- machine 
interaction is often a vital part of process control. The Device model facilitates interleaving of multiple 
tracks of interaction. A user can manip- ulate two devices simultaneously, and pause to do something 
else before reseming. Contrast this with the ubiquitous ping- pong style of interac- tion encouraged 
(or required) by most systems. Devices are objects a person can manipulate. Such a model of an interactive 
system facilitates the illusion that the user is manipulating things, rather than asking a computer to 
do something. When a system's users are people who don't understand computers, this is a useful illusion. 
In many applications, it is useful even to hide the fact that a computer is used. The model is directly 
applicable to descriptions of hardware, as well as software. Channels can translate to wires, bus addresses, 
or procedure bindings, with equivalent validity. Strict modularity, as provided by the model, permits 
any software device to be replaced by an equivalent hardware device, regardless of whether it is part 
of another device. Similarly, hardware can often be simulated effectively by software. The notation used 
here can thus be used as a precise form of hardware documentation. More research is needed before the 
Device model becomes a real tool for implementing interactive systems. A cor~plete language must 113 
  Computer Graphics Volume 16, Number 3 July 1982 be designed, and several details must yet be resolved. 
Of course, implementation as a conpiler would be desirable. Nevertheless, even in its present state, 
the model appears useful as a conceptual tool to aid in designing interactive systems. ACKNOWLEDGEMENTS 
 The work described in this paper was supported, in part, by NSF Comprehensive Assistance to Undergraduate 
Science Education (CAUSE) grant number SER79-07499. BIBLIOGRAPHY i. Anson, E. The semantics of graphical 
input. Proc. SIGGRAPH 79 (Computer Graphics) 13,2 (Aug. 1979), 113-120. 2. Borning, A. The programaing 
language aspects of ThingLab, a constraint-oriented simulation laboratory. ACM TOPLAS 3,4 (Oct. 1981), 
353- 387. 3. Brinch Hansen, P. EDISON: A Multiprocessor Language. Computer Science Department, University 
of Southern California, 1980.  4. Brinch Hansen, P. The Design of ~9ISON. Computer Science Department, 
University of Southern California, 1980.  5. Britton, E.G., Lipscomb, J.S., and Pique,  M.E. Making 
nested rotations convenient for the user. Proc. SIGGRAPH 78 (Computer Graphics) 12,3 (Aug. 1978), 222-227. 
 6. Dahl, O.-J., and Hoare, C.A.R. Hierarchical program structures. In Structured Programming, Dahl, 
O.-J., Dijkstra, E.W., and Hoare, C.A.R.  (Eds.), Academic Press, New York, 1972, pp. 175-220.  7. 
Foley, J.D., and Wallace, V.L. The art of natural man- machine conversation. Proc. IEEE 62,4 (April 1974), 
462-471.  8. Kay, A. Smalltalk. In Methodology of Interaction, Guedj, R.A., et al (Eds.), North- Holland, 
Amsterdam, 1980, p. 7-11.  9. Liskov, B.H., and Zilles, S.N. Specification techniques for data abstractions. 
IEEE Trans. on Software ~gineering i, 1 (March 1975), 7-19.  10. Liskov, B.H., Snyder, A., Atkinson, 
R., and Schaffert, C. Abstraction mechanisms in CLU. Comm. ACM 20,8 (Aug. 1977), 564-576.  ii. Plasmeijer, 
M.J. Input Tools: A Language Model for Interaction and Process Communica- tion. PhD Thesis, Katholieke 
Universiteit, Nijmegen, The Netherlands, 1981. 12. Rosenthal, D.S.H. "Methodology in Computer Graphics" 
Re-examined. Computer Graphics 15,2 (July 1981), 152-162. 13. SIGGRAPH-ACM GSPC. Status report of the 
Graphic Standards Planning Co, m~ittee. Computer Graphics 13,3 (Aug. 1979).  14. Tesler, L. The Smalltalk 
environment. BYTE 6,8 (Aug. 1981), 90+.  15. Van den Bos, J. Definition and use of higher- level graphics 
input tools. Proc. SIGGRAPH 78 (Computer Graphics) 12,3 (Aug. 1978), 38-42.  16. Van den Bos, J., Plasmeijer, 
R., and Stroet,  J. Process communication based on input speci- fications. ACM TOPLAS 3,3 (July 1981), 
224- 250. 114 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801270</article_id>
		<sort_key>115</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>19</seq_no>
		<title><![CDATA[Star graphics]]></title>
		<subtitle><![CDATA[An object-oriented implementation]]></subtitle>
		<page_from>115</page_from>
		<page_to>124</page_to>
		<doi_number>10.1145/800064.801270</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801270</url>
		<abstract>
			<par><![CDATA[<p>The XEROX Star 8010 Information System features an integrated text and graphics editor. The Star hardware consists of a processor, a large bit-mapped display, a keyboard and a pointing device. Star's basic graphic elements are points, lines, rectangles, triangles, graphics frames, text frames and bar charts. The internal representation is in terms of idealized objects that are displayed or printed at resolutions determined by the output device. This paper describes the design and implementation of a graphics editor using an object-oriented technique based on a Star-wide subclassing method called the Trait Mechanism.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Business graphics]]></kw>
			<kw><![CDATA[Subclassing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>D.2.2</cat_node>
				<descriptor>User interfaces</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.4.1</cat_node>
				<descriptor>Word processing</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.7.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010510</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document preparation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10011187.10011188</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Personal computers and PC applications->Word processors</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10011006.10011066.10011069</concept_id>
				<concept_desc>CCS->Software and its engineering->Software notations and tools->Development frameworks and environments->Integrated and visual development environments</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329710</person_id>
				<author_profile_id><![CDATA[81100456341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Lipkie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Corporation, El Segundo, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333841</person_id>
				<author_profile_id><![CDATA[81539779556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Evans]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Corporation, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331535</person_id>
				<author_profile_id><![CDATA[81100626345]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Newlin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Corporation, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333480</person_id>
				<author_profile_id><![CDATA[81100560537]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Weissman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Corporation, Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>806468</ref_obj_id>
				<ref_obj_pid>800210</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[G. Curry, L. Baer, D. Lipkie and B. Lee, "Traits - An Approach to Multiple-Inheritance Subclassing," Conference on Office Automation Systems, Philadelphia, Penn., June 1982.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807780</ref_obj_id>
				<ref_obj_pid>800254</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[E. Harslem and L.E. Nelson, "A Retrospective on the Development of Star," submitted to 6th International Conference on Software Engineering; Tokyo, Japan, Sept, 1982.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[C. Irby, L. Berginsteinsson, T. Moran, W. Newman and L. Tesler, "A Methodology for User Interface Design", Systems Development Division, Xerox Corporation, January 1977.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[J. Mitchell, W. Maybury and R. Sweet, "Mesa Language manual," Technical Report CSL-79-3, Xerox Corp., Palo Alto Research Center, Palo Alto, CA, April 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[J. Seybold, "Xerox's 'Star'" in The Seybold Report, Media, Pennsylvania: Seybold Publications, v. 10, no. 16, 1981.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1500840</ref_obj_id>
				<ref_obj_pid>1500774</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[D. Smith, C. Irby, R. Kimball and E. Harslem, "The 7tar User Interface: An Overview," NCC '82.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[D. Smith, C. Irby, R. Kimball, B. Verplank, and E. Harslem, "Designing the Star User Interface," Byte, v. 7, no. 4, 1982.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[L. Tesler, "The Smalltalk Environment", Byte, V. 6, no. 8, 1981.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[C. Thacker, E. McCreight, B. Lampson, R. Sproull and D. Boggs, "Alto: A Personal Computer," Computer Structures: Principles and Examples, D. Siewiorek, C. Bell and A. Newell, editors, McGraw-Hill, 1982.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Star Graphics: An Object-Oriented Implementation Dr. Daniel E. Lipkie Xerox Corporation, El Segundo, 
California Steven R. Evans, John K. Newlin, Robert L. Weissman Xerox Corporation, Palo Alto, California 
 Abstract : The XEROX Star 8010 Information System features an integrated text and graphics editor. The 
Star hardware consists of a processor, a large bit-mapped display, a keyboard and a pointing device. 
Star's basic graphic elements are points, lines, rectangles, triangles, graphics frames, text frames 
and bar charts. The internal representation is in terms of idealized objects that are displayed or printed 
at resolutions determined by the output device. This paper describes the design and implementation of 
a graphics editor using an object-oriented technique based on a Star-wide subclassing method called the 
Trait Mechanism. CR Categories and Subject Descriptors: D.2.2 [Soft- ware Engineering]: Tools and Techniques 
-User interfaces; H.4.1 [Information Systems Applica-tions]: Office Automation -Word processing; 1.3.6 
[Computer Graphics]: Methodology and Techniques -Interaction techniques; 1.7.2 [Text Processing]: Document 
Preparation General Terms: Design Key Words: business graphics, subclassing I. The Star Workstation 
In 1975 Xerox started an effort to transfer research from the Xerox Palo Alto Research Center (PARC) 
into mainline office products. Central to this strategy was the development of a top-of-the-line professional 
workstation, subsequently named Star, that was to XEROX ® , 8010 and Star are trademarks of XEROX CORP. 
Permission to copy without fee all or part of this material is granted provided that the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; 
1982 ACM 0-89791-076-1/82/007/0115 $00.75 provide a major step forward in several different domains of 
office automation. A retrospective on the development of Star is presented in [2]. A unique aspect of 
Star is its user interface (UI) and the role it played in the development of Star [5, 6, 7]. About 30 
work years of effort were expended in designing the UI before the functionality of the system was fully 
decided and before the computer hardware was even built. The hardware that supports this UI (figure 1) 
Ethernet .... i[ii............................................. '1 ii~iil Display with y 3 icons and 
I! 1 open document ~.:.:~..:~..1~ Keyboard with 3 function groups Mouse with !!!!!!]~] 2 buttons ~- Processor 
with 29MB disk drive Figure 1 Star Workstation Schematic  consists of a microprogrammable 22-bit virtual, 
18-bit real address space processor, an 808 by 1024 pixel (11" x 14") bit-mapped display, a keyboard, 
a pointing device called a mouse and a 10 or 29M byte disk. The workstation may be attached to a 10M 
bits-per-second Ethernet for access to remote printing, filing, communication and electronic mail services. 
The mouse has a ball on the bottom that turns as the mouse slides over a flat surface. Electronics sense 
the ball rotation and displaces a cursor on the screen in corresponding motions. There are two buttons 
on the mouse, called SELECTand ADJUST, used to make and adjust a selection as described below. The keyboard 
has a conventional central part and three groups of function keys. The left function group contains the 
generic commands: MOVE, COPY, DELETE, SHOW PROPERTIES, COPY PROPERTIES,and AGAIN. Their meaning is defined 
only in a generic sense; it is up to the currently selected element to further define them as explained 
below. The keys in the upper function group are referred to as soft keys. Their meanings and use are 
discussed below. The right function group includes the command KEYBOARD and others that are not of interest 
in this paper, When KEYBOARDis pressed, the soft keys allow the user to assign a new interpretation to 
the central keyboard and to display a window that shows the meaning of each keytop. Keyboards supported 
are Japanese, various European keyboards, Dvorak and keyboards with useful office and mathematical symbols. 
Central concepts to the Star UI are what you see is what you get, visibility (don't hide things under 
CODE+key combinations) and a physical office metaphor. One of the functional areas of the office addressed 
by Star is document creation, which encompasses text editing and formatting, figure editing (graphics), 
mathematical formula editing and page layout. These are all integrated. As an example of what you see 
is what you get, the Star user edits on the display both text and graphic figures, which appear exactly 
as they do when the document is printed. This document was prepared using Star; no special step was needed 
to merge the figures and text. Visibility and the office metaphor are discussed in the next section. 
The design of the Star software began in the spring of 1978 and the first release, containing 255,000 
lines of code, was completed in Oct. 1981. Over the 3.5 years approximately 93 work years of effort were 
expended and in excess of 400,000 lines of code were written. This effort was aided by the adoption of 
an object-oriented style of coding right from the start and by the use later of a multiple-inheritance 
subclassing mechanism, Traits [1], as the basis for defining and implementing objects. An object-oriented 
imple-mentation was chosen because it corresponded closely to the UI model of interacting screen elements. 
In this paper we use the term element to refer to user perceived entities and reserve the term object 
for the corresponding internal implementations. As explained below, there is no graphics editor per Be, 
but of the 255,000 lines of code in the release about 28,000 are associated with editing figures in documents. 
In Section II we describe the Star user interface. The Trait mechanism is presented in Section HI. Its 
application in the Star implementation is discussed in Sections IV and V. II. The Star User Interface 
The Star UI differs from that of other computer systems through its heavy use of the graphics capabilities 
of a bitmap display, its adherence to a physical office metaphor, and its rigorous application of a small 
set of design principles [3]. The graphics capabilites reduce the amount of typing and remembering required 
to operate the system; the office metaphor makes the system familiar and friendly; the design principles 
unify the nearly two dozen functional are as of Star. One important principle is to make objects and 
actions in the system visible. The system should not hide things under obscure CODE + key combinations 
or force the user to remember a lot of conventions. When a choice had to be made between easy novice 
use and efficient expert use, Alan Kay's maxim was followed: "Simple things should be simple; complex 
things should be possible". As you make everything visible, the display becomes reality, and the user 
model becomes identical with what is on the screen. Using the physical office metaphor Star creates electronic 
counterparts to the physical elements in an office: paper, folders, file cabinets, mail boxes and so 
on. The Star screen represents a desktop on which are placed small (-1" x 1") pictograms or ~cons that 
represent these elements, e.g. the document (paper) and file drawer (file cabinet) icons in figure 2. 
| | I ! Bar Chart Old Example Memos Figure 2 Document and File Drawer Icons Within a illustration the 
currently implemented graphics elements are points, lines, rectangles, triangles, graphics frames, text 
frames and bar charts. Examples are shown in figure 3. A graphics frame is a Computer Graphics Volume 
16, Number 3 July 1982 Points ° ° Lines ) Bung ~ ~i o...ooo.~ N i i J ~ n In m m m n o m m i Rectangles 
®N Triangles .NJ Text Frame with | Text Frame with J invisible border I visible dashed border I Figure 
3 Examples of Graphic Elements rectangular area reserved for figures. Text frames allow the user to 
put labels in figures. Iconic, text and graphic elements are selected by pointing at them with the mouse 
and clicking (pressing and releasing) one of the buttons on the mouse. Icons show that they are selected 
by highlighting (video reversing) their image. Character selections highlight themselves by inverting 
a rectangular region around the characters. The user selects a graphics element by pointing anywhere 
along one of its lines or edges. When a graphic element is selected, it inverts a small square region 
around each of its control points. Lines have control points at each end, rectangles (figure 4) and L 
Guiding Point Figure 4 Rectangle with Inverted Control Points (Expanded Scale)  frames at each corner 
and midpoint of each side and triangles at each vertex. The inverted region around the control point 
closest to the cursor is slightly larger. This control point is called the guiding point and becomes 
attached to the cursor when the element is moved, copied or stretched. An element highlights as if selected 
when the mouse button is depressed, but it is selected only when the button is released. The user may 
change the candidate selection by moving the mouse with the button still depressed until the desired 
element is highlighted. After an icon, character or graphics element has been selected, it may be manipulated 
by one of the generic operations. To move a document to a file drawer, select the document icon, press 
MOVE,point at the file drawer icon and click the SELECT button. Elements may also be manipulated in other 
ways described below. The meaning of the operation is determined by the selected element. Moving a document 
icon to a file drawer icon sends the document over the Ethernet and stores it on a file server; moving 
it to an out-basket icon sends the document via electronic mail; moving it to a printer icon makes a 
hardcopy of it. Copying and deleting have similar straightforward semantics. The OPENcommand in the left 
function group may only be applied to an icon and creates a window through which the icon's contents 
are displayed and edited. Star has a modeless editing style; there are no start edit or end edit commands. 
The user merely selects a character in a window displaying a document and begins typing and the text 
is appended to the selected character. The page content is reformatted as the user edits. The generic 
operations also may be applied inside a window; e.g. text may be moved, copied or deleted by merely selecting, 
pressing the function key and pointing to the destination. Before discussing the other editing actions, 
we will explain how graphics elements are entered into text. To enter a figure into a document the user 
selects a character in the document and types a character that represents a graphics frame. (The character 
is found on a keyboard accessible through the KEYSOAR0 key.) This non-printing, but screen-displayable, 
character is inserted after the selected character. It looks like a boat anchor and represents an anchoring 
point for the graphics frame. The frame appears between two lines of text at the same time the anchor 
character is typed. As the textual content of the document is reformatted during subsequent editing this 
anchor character is shifted as any other character and in addition its associated graphics frame is also 
repositioned, e.g. the anchor character acts like a footnote reference mark, 117 and the graphics frame 
moves from page to page as its reference mark is moved. Once the user has a graphics frame in a document, 
other graphics elements may be moved or copied into the frame. Star graphics has only two creation actions, 
inserting a graphics frame as described above and MAKE LINE described below. All other graphics elements 
are made by copying. Every desktop has a directory icon that contains a blank document and a graphics 
document that has all the graphics elements. The directory's documents can only be copied, not moved 
or deleted, so the user always has a source of documents and graphics elements. Pressing the SHOW PROPERTIES 
key opens a small window in which a property sheet appropriate to the current selection is displayed. 
The property sheet displays the property values for the currently selected element. The properties are 
changed by setting them to the desired values and clicking at the Done command which applies the new 
properties and closes the property sheet window. For each property the property sheet either displays 
an enumeratation of all possible values or provides a box into which the value is typed. The property 
sheet for a graphics line is shown in figure 5. .5": ~::"~:':':,'-i:-:"i~9~:~:*:::'~ ::~:ii:=':" ~~=-~y 
~ ~ --~.~ ........................................................................................................ 
 Line Property Sheet ~%efaults .~.Apply ~ Width n roll Structure  m-m I Left (Upper) Line End , I Right 
(Lower) Line End Constraint I FIXED ANGLE I Figure 5 Line Property Sheet  The ? command provides access 
to online documentation about the llne property sheet; Defaults sets the properties to system defined 
values; Apply applies the properties but does not close the property sheet; Reset sets the properties 
to the values they had when Show Properties was invoked. There are three kinds of properties: choice, 
state and text. A choice-type property displays a set of mutually exclusive values for the property which 
are shown immediately adjacent to each other; e.g. a line's width, structure and line endings are each 
choice properties. Exactly one is on at any one time and is video reversed. To change it the user points 
at the desired value and clicks a mouse button. A state-type property may be either on or off. Pointing 
at and clicking a mouse button toggles its setting. A line may be constrained to be at a fixed angle 
so that its length but not its direction may be changed during the stretching action described below. 
An unconstrained line may have its length and/or direction changed. A text-type property displays a box 
into which a text value is typed. None of the properties on the line property sheet are text-type. But 
an example is the text-type property on the document property sheet which determines the name of the 
document. The properties of a rectangle are the width and style of its bounding box, its interior shading 
and a fixed shape constraint. The properties of text and graphics frame include the width and style 
of the border. Text frames may be constrained to be fixed or flexible. A flexible text frame will change 
shape as its text contents are edited. Graphics frames may be positioned horizontally within a column 
(flush left, centered, flush right) and vertically within a column (top, centered, bottom or floating). 
Graphics frames also have a grid that may be displayed as dots or plus marks at each grid point or as 
ticks around the edge of the frame. Grid spacing is also variable. Another way to change a selected element's 
properties is to press COPY PROPERTIES and then point at an element that is the source of the desired 
properties. Associated with every text selection is a multi-click level: character, word, sentence or 
paragraph. Clicking at an unhighlighted character with the SELECT mouse button selects the character 
at the character level; clicking again with the SELECT mouse button at the same character selects the 
enclosing word; clicking at any character in a selected word selects the enclosing sentence; clicking 
at any character in a selected sentence selects the enclosing paragraph; clicking at a character in a 
selected paragraph brings the selection back to the character level and selects that character. Clicking 
at a character with the ADJUST mouse button expands or shrinks the selection at the current level to 
minimally span the pre-existing selection and the character pointed at. There is no selection level associated 
with a graphics selection, but the ADJUST button has a graphics interpretation that is used to extend 
the selection to include multiple elements. Clicking the ADJUST button at a graphics element toggles 
it in/out of the current selection. The ADJUST button may also be used to extend the selection by adding 
all elements properly contained in a bounding box. The user presses the ADJUST button, which fixes one 
corner of the bounding box, and moves the mouse with the button depressed. The current mouse position 
defines the opposite corner of the bounding box. As long as the ADJUST button is depressed, a box is 
drawn on the screen from the fixed point to the current mouse postion and all elements properly contained 
are highlighted. When the button is released (at button up) these elements are added to the selection. 
An extended selection may be moved, copied, deleted, joined, stretched or the elements may have their 
properties changed. The elements of an extended selection may be of different types, e.g. lines, rectangles 
and text frames. Whenever there is a graphics selection the soft keys at the top of the keyboard take 
on graphics meanings: STRETCH, MAGNIFY, GRID, MAKE LINE, JOIN/SPLIT and ToP/BOTTOM. When the current 
selection is textual the soft keys take on meanings that allow the appearance of the charaters to be 
changed, e.g. bold, italic, underlined, superscript, subscript, larger font size and smaller font size. 
When STRETCH is pressed the selection is de-highlighted and the control point furthest from the guiding 
point is replaced by an X and is considered pinned. The guiding point becomes attached to the mouse when 
a button is pressed. As the mouse is moved the selection is horizontally and vertically scaled to conform 
to the pinned and guiding points and redisplayed. On button up the element retains the new size, the 
X is removed, and the selection is rehighlighted. MAGNIFYis simlar to STRETCH except that the same scaling 
factor is applied in the horizontal and vertical directions, i.e. aspect is maintained. The GRID soft 
key toggles the grid on/off for the frame containing the selection. If the grid is active, it controls 
the placement of the guiding point during move, copy, stretch and magnify. MAKE LINE creates a line between 
two successive mouse click positions. JOIN combines an extended selection of graphics elements into a 
cluster element. Once joined, all of the original elements behave as a single element for purposes of 
selection and editing. This allows users to define their own graphics symbols. The SPUT function acts 
on a cluster and reverses the effect of JOIN. Graphics and text frames are opaque, that is they obscure 
elements that are under them. In figure 6a the  !Above Rec,a.=,e / I =o,ow oc,a°=lo I (6a) (6b) Figure 
6 Overlappm" g Elements text frame is above the rectangle, while in figure 6b it is below. The soft 
keys TOP and BOTTOMallow the user to move the current selection to the top or bottom level in a frame. 
 In keeping with Star's style of modeless editing, the graphics editor is not invoked in the traditional 
sense. In fact, as we shall see later, there is no graphics editor in the traditional sense. All graphics 
editing capabilities are available whenever there is a document open. The Star user may pause during 
document editing and read incoming mail or use the records processing feature or any of the other Star 
functions. The responsibility for making the transition between these editing environments resides with 
the elements on the desktop, not the user. This is a major difference between Star and other information 
systems, including the Alto system [9] where the user explicitly invokes BRAVO for text editing, SIL 
or DRAW for figure editing, and LAUREL for electronic mail. The full text editing capabilities are available 
for editing the contents of text frames within graphics frames, e.g. text frames may contain anchor characters 
and graphics frames. This means that Star must support the virtual nested invocation of editors. [II. 
Traits - The Star Subclassing Mechanism Object-orientation is a method for organizing software such that, 
at any time, computation is performed under the aegis of a particular object, not a centralized program 
that handles every case from one place. The nature of the Star UI and tl~e user model it fosters led 
to the adoption of an object-oriented method from the beginning of the software development. Subclassing 
is a refinement of the basic object-oriented methodology that constructs objects out of more primitive 
behaviors. Initial Star subclassing efforts were in the SIMULA-67 and Smalltalk [8] style where the specialization 
relations form a tree. We found it necessary to generalize this concept to allow specialization relations 
that are represented by directed acyclic graphs. A full description of the Trait mechanism and the generalized 
concept of multiple- inheritance subclassing is beyond the scope of this discussion but may be found 
in [1]. Subclassing as a way of implementing objects was not used during initial development of Star. 
This was partly because the designers had had little experience with subclassing as a methodology for 
large production software systems where performance is a primary consideration. It was also believed, 
incorrectly, that an extensible design based on subclassing would necessitate a violation of the typing 
mechansim of the implementing language, Mesa [4]. But as implementation progressed, it became clear that 
signil~cant code-sharing was possible since we were dealing with objects that were more similar than 
different and we re-examined the subclassng problem. We first present some of the central concepts of 
the trait mechanism and then describe how it has been applied in graphics. The initial graphics implementation 
was about 17,000 lines of code and space does not permit a full presentation of the graphics traits and 
their interaction. During this description we will refer to trait definitions summarized in Section VI. 
A trait is a characterization of a behavior and is the primitive abstraction used to define objects. 
A trait used to define an object is said to be carried by the object, e.g. the trait TreeElement is carried 
by objects Trait definition: trait name state component traits fixed operations replaceable operations 
 that live in tree-like data structures. To implement a behavior, an object carrying a trait remembers 
information in a state defined by the trait. For example objects carrying TreeElement have 3 state variables, 
next, parent and eldest, that are pointers to the corresponding objects or are the special value objectNil, 
pointer to no object. In a departure from SIMULA-67, traits may carry other component traits where the 
carry relationship is represented as an acyclic directed graph. This permits behaviors to be built on 
multiple lower-level abstractions. The basic imaging trait, Schema, carries TreeElement because all imaging 
objects are part of an imaging tree rooted at a Desktop Object that manages the Star display (see figure 
9 below). A trait defines operations as a means of presenting information to or extracting information 
from an object, e.g. the operations GetParent and SetParent fo~ TreeElement. Operations also may be invoked 
for effect, e.g. the Schema operation Paint is a request to an object to paint its image on the display. 
An operation is invoked on an object by specifying a trait carried by the object, an operation defined 
by the trait and the object. In Mesa, an operation invocation is implemented as a procedure call with 
the object handle as the first parameter and other parameters as needed, e.g. Schema.Paint[ object .... 
] Operations that extract information are implemented as procedures that return values. A trait operation 
has a specification (name, parameters, return type) and a realization (an implementing piece of code). 
Fixed operations are those for which the trait supplies the realization, e.g. the implementing code for 
GetParent in TreeElement is the same for all objects carrying the trait, it merely accesses the state 
variable parent and returns its value. Replaceable operations are analogous to SIMULA- 67 VIRTUAL procedures. 
The trait defines the specification and each class supplies its own realization that is used by all objects 
in the class, e.g. the Schema trait provides the specification for Paint but the classes Line and Rectangle 
each provide their own realizations that access the object's state to display the appropriate image. 
A class trait is a trait that provides fixed operations for creating and destroying objects in the class. 
Associated with each class is a replaceable operations vector that is the composition of its own and 
its component trait's replaceable operations. The realizations of replaceable operations are assigned 
to the vector elements. The vector for the Line class is shown in figure 7. Size Schema Operations Paint 
PointedAt Edit RelLocChild Contend ~-GSchema Operations CountCp HasCp Operations LocCp Figure 7 Replaceable 
Operations Vector for Line Class  Each object created by a class trait has an object state vector that 
is the composition of the class's state and the class's component trait's states. The vector for a Line 
object is shown in figure 8. Line Class Id next TreeElement State parent eldest size GSchema State Ioc 
in parent width Line State styleline endings constraint Figure 8 Object State Vector for a Line Object 
 IV. Applying the Trait Mechanism to Star The first release of Star defined 169 traits, 129 of which 
were class traits. 99 traits required state variables and 39 had replaceable operations. Non-class traits 
we will discuss are: TreeElement, Schema, GSchema, ListSchema and HasCp. Class traits we will discuss 
are AnchoredFrame, Line and TextBlock. Traits definitions for these traits are summarized in Section 
VI. The TreeElement trait allows objects to be organized into tree data structures. The tree structure 
corresponding to a 3 page, 3 column document containing graphics and text is shown in figure 9. This 
Desktop I I Window Window Window Document I I Page Page Page I I Heading Body Footing I I Column Column 
Column I 1 Text Block Anchored Frame Text Block Graphics Frame I I I Graphics Element1 Graphics ElementN 
Figure 9 Desktop Imaging Tree structure will be explained more fully after we have introduced the Schema 
and ListSchema traits. The Schema trait forms the basis for imaging, pointing resolution, selecting and 
editing within Star. It defines 22 replaceable operations but for the purposes of this discussion we 
are only be concerned with those shown in Section VI. These operations allow an object to be asked its 
size (Size), to honor a request that it paint its image (Paint), to handle a pointing action by the mouse 
(PointedAt), to respond to an editing action when it is selected (Edit), to return the location of a 
child relative to itself (Rel kocChild) or relative to the upper left corner of the screen (Screen kocChild). 
GSehema is an extension to Schema to meet the needs of graphics objects and is the basic trait carried 
by all graphics objects. It provides state variables for its size and location within its parent. Of 
the 39 replaceable operations it defines we are concerned only with Contend which is used during pointing. 
ListSchema is a trait carried by an object that has non-overlapping children that are arranged either 
vertically with left edges aligned (pages in a document) or horizontally with top edges aligned (columns 
on a page), see figure 10. These two I I I I  I I' L margin Figure 10 Horizontal ListSchema  arrangements 
are embodied in the ListSchema trait that is carried by an object that wishes to arrange its children 
in this manner. The state defines the inter- child spacing, the margin between the children and the parent 
carrying this trait and the color for the areas not covered by children. A list with color black and 
non-zero spacing and margin values is a common method for drawing lines around objects. HasCp is a trait 
carried by all graphics objects that have control points. The only graphics object that does not have 
control points is the cluster object created and destroyed by the JOIN and SPUr functions. For a given 
class the number of control points is the same so a replaceable operation, CountCp, is defined to return 
this value, e.g. 2 for line objects, 8 for rectangles. The replaceable operation LocCp returns the object-relative 
location of a control point. The fixed operation HighlightCp highlights a control point in one of the 
styles: default (small square), guiding (larger square) or pinned (an X). ClosestCp and FurthestCp are 
fixed operations that enumerate the control points for an object and use LocCp to determine the control 
point closest and furthest from a particular coordinate. They are used to determine g~.iding and pinned 
control points. Rectangles, graphics frames and text frames have the same number and arrangement of control 
points and so use the same realizations for CountCp and LocCp. This increases code sharing so that only 
2 realizations for 2 separate replaceable operations are needed to implement all the control point behaviors 
for 3 classes of objects. This is typical of the code sharing benefits of the trait mechanism. AnchoredFrame 
is the class trait for the graphics frame that is associated with the anchor character. There is no screen-visible 
element for this object. The keyboard insert of an graphics frame actually creates two objects, an anchored 
frame with a graphics frame inside it. It is the graphics frame that is the visible box. Anchored frame 
objects are also used to anchor equations in text. An anchored frame object forms the boundary between 
the non-graphics and graphics domains. A page column is a vertical list of left edge aligned text blocks 
and anchored frames. The one and only child of the anchored frame is a graphics frame that may be aligned 
flush left, centered or flush right within the anchored frame as determined by a property on the graphics 
frame property sheet. Within the graphics frame there are no restrictions on object arrangement. Line 
is the class trait for graphics lines. Its state retains the properties shown in figure 5. TextBlock 
is the class trait for objects that have textual content. Further details about this trait are beyond 
the scope of this discussion. Text blocks and anchored frames are the only objects that exists in a document 
column. Note that the Schema trait defines operations for asking an object its size and location but 
does not define corresponding state variables. Also note that the replaceable location query is a request 
to a parent object for the location within the parent of a child object, i.e. a parent-relative location. 
This is done, as we discuss below, for flexibility and economy of storage and for performance. It was 
felt best not to force all classes to store their size in the same manner at the Schema level because 
the trait is used as a component trait for a large number of classes each with possibly quite different 
behavior, e.g. a horizontal list-like object may determine its size by summing the widths of its children 
and use the height of its tallest child as its own height while a graphics object may store this information 
in its state. This judgement has been shown correct by the diversity of methods for determing size that 
now exist as the Star software has matured and new features, objects and behaviors have been implemented. 
It is quite common for a trait to define a behavior, such as Schema Size, that requires the cooperation 
of all objects that carry it in order to complete the behavior. For performance reasons the fundamental 
location query is in terms of location within parent. Displaying an object or changing its location on 
the screen should not require changing its state. For example, the Star workstation processor has instructions 
that support moving bits from one part of the screen to another. Scrolling a page upward is merely a 
matter of moving existing screen bits and painting new bits into the vacated portion of the window; none 
of the scrolled objects needs to be told to modify any of their state. This processor support also aids 
performance because it is not necessary to invoke the Paint operation for objects that already have their 
image on the screen. Also, changing the size of an object or deleting an object near the front of a document 
does not require changing the state of all following objects in the document. When the screen location 
of an object is needed for an operation the object is passed its screen location as a parameter or it 
invokes the operation ScreenLocChild on it parent. V. Two Examples Star graphics was the first major 
piece of Star software designed in terms of traits and that used the full generality of the mechanism. 
Pieces of software designed or implemented prior to graphics have subsequently been converted to the 
traits mechanism. In this section we will describe two interactions between the tr-aits presented in 
section III. We first show how the GSchema trait completes the Schema size and location behaviors and 
second show how it extends the Schema trait for pointing behavior. The GSchema state records a size and 
parent-relative location. Fixed GSchema operations allow this information to be accessed and changed. 
All GSchema objects use the same realization for the Schema replaceable operation RelLocChild which invokes 
the fixed GSchema operation GetRel LocSelf on the child object. Note that for a graphics object GSchema.GetRel 
LocSelf[ object ] returns the same value as Schema.RelLocChild[ TreeElement.GetParent[ object ], object 
] Objects that carry the Schema trait are responsible for a rectangular patch of the screen. Among sibling 
objects this may be sole responsibility, as is the case between page objects, or may be a shared responsibility, 
as is the case between overlapping graphics objects. Sole vs shared responsibility has interesting implications 
for the implementation of imaging behaviors. We will look at the pointing and selecting behavior of objects. 
The document object carries the ListSchema trait and is the parent of page objects. It is quite easy 
for a document to determine which page contains the cursor and then pass the buck for processing the 
pointing action to that page. As long as the cursor remains within the bounds of the window displaying 
the document and within the bounds of the page, the page object has sole responsibility for tracking 
the movement of the cursor, for providing user feedback in the form of highlighting and for making a 
selection when the user releases the mouse button. If the cursor leaves these bounds with the button 
still depressed the page passes responsibility back to the document object for continued processing. 
A page satisfies its obligation by passing the buck to the column containing the cursor, etc. When the 
user releases the button, the currently pointed-at object registers itself as the current selection with 
a central mechanism. Subsequent user editing actions are sent to it via the Schema Edit operation. It 
is up to the object to decide how to respond to the editing action, e.g. graphics lines ignore typing. 
This method for button processing is embodied in the Schema replaceable operation PointedAt. Parameters 
for the operations are the object being asked to process the pointing action, its screen location, a 
tracking region, the current cursor location and the state of the mouse buttons. The return value for 
the operation is an updated cursor location and an updated mouse button state. The object must track 
the cursor as long as it is in the tracking region and must return control when the cursor leaves the 
region. The semantics of PointedAt were designed for the non-graphics domain where nested list-like arrangements 
predominate, e.g. pages in documents, columns in pages. List-like arrangements also predominate outside 
windows but a description of their uses there is beyond the scope of this discussion. The ListSchema 
trait provides a buck-passing realization for PointedAt that is used by almost all objects carrying the 
ListSchema trait. Sibling graphics objects must share responsibility for pointing, eg. pointing inside 
the boundary of a rectangle may really lead to the selection of some other object. If the user points 
at the letter "x" in "Text" in figure 6a the text frame does not allow the user to point through to the 
rectangle under it. If the user points at the upper left corner of the text frame in figure 6b the user 
is pointing through the rectangle. This sharing behavior is implemented by the GSchema operation Contend 
described below. If the cursor is positioned inside a graphics frame and a mouse button is pushed, the 
list-like PointedAt realization behavior described above resolves the cursor to the window containing 
the cursor, the document within the window, the proper page, the proper column and finally to the anchored 
frame within the column. The anchored frame's realization for PointedAt is to enumerate it descendants 
and ask each how much interest it has in the current cursor position. The child with the greatest interest 
is passed the buck for processing the pointing action by invoking its PointedAt realization. The tracking 
region it is passed is very small, a box about 1/8" square. This allows the anchored frame to regain 
control and re-poll its descendants if the user moves the cursor any significant amount. The GSchema 
operation Contend is the operation used to ask a graphics object how much interest it has in the current 
cursor position. The descendants are enumerated top-down and enumeration stops when all have been enumerated 
or one of the descendants says stop, e.g. the text frame in figure 6a when the cursor is pointing at 
the letter"x". Rather than change the semantics of PointedAt for graphics objects, or replacing it completely 
with a new set of operations to do pointing resolution, we merely added a pre-processing phase by adding 
Contend. The extending of behaviors by addition, not replacement, of operations is a capability offered 
by the traits mechanism and used widely throughout Star. Note also that the user is allowed to button 
down near a graphics element and see it highlight, move the mouse with the button still down out of the 
graphics frame and point to a letter in the main document text and see the graphics element de-highlight 
and the letter highlight, continue dragging the mouse out of the document window and point to an icon 
and see the letter de-highlight and see the icon highlight and then select the icon by releasing the 
button. All this is possible as a single user action. In the traditional sense this may be thought of 
as automatically invoking three editors in succession, the graphics editor, the text editor and the desktop 
editor, and passing control between them when in reality we are traversing a tree of objects and asking 
each to exhibit its own behavior. The implementation corresponds to this model and for this reason there 
is no graphics editor per se that is invoked by the Star user, there are only graphics behaviors that 
are exhibited in response to user actions and these behaviors are available at all times.  Computer 
Graphics VI. Trait Summary The following trait summary is in the order they were introduced above. Additional 
state variables and operations beyond the scope of this discussion are represented as "...". trait name: 
TreeElement state: next, parent, eldest component traits: none fixed operations: GetNext, SetNext, GetParent, 
 SetParent, GetEIdest, SetEIdest .... replaceable operations: none trait name: Schema state: none component 
traits: TreeElement fixed operations: ScreenLocChild .... replaceable operations: Size, Paint, PointedAt, 
Edit, RelLocChild, ... trait name: GSchema state: size, location in parent, ... component traits: Schema 
fixed operations: GetSize, SetSize, GetRelLocSelf, SetRelLocSelf .... replaceable operations: Contend 
.... trait name: ListSchema state: margin, spacing, color component traits: Schema fixed operations: 
... replaceable operations: ... trait name: HasCp state: none component traits: none fixed operations: 
HighlightCp, ClosestCp, FurthestCp .... replaceable operations: CountCp, LocCp .... trait name: Line 
state: width, style, line endings, constraint component traits: GSchema, HasCp fixed operations: none 
replaceable operations: none trait name: TextBlock state: text contents, ... component traits: Schema 
.... fixed operations: ... replaceable operations: ... VII. Conclusions Adopting an object-oriented 
implementation and the traits mechanism has been a success. The initial graphics design and implementation 
(without bar charts and text frames) was done in one work year by a new hire who knew nothing about the 
Mesa language or the Star object-oriented methodology or the traits mechansim. This was in large part 
due to the building block nature of the methodology. Also the graphics functional  Volume 16, Number 
3 July 1982 specification had already been written, and one of the authors had validated the graphics 
user interface by prototyping on the Xerox Alto using a different implementation technique. Subsequent 
to graphics, most of Star has been converted to this methodology, and three other major pieces of software 
have been undertaken: an equations editing capability, a 3720 emulations window, and a table editing 
capability. All are having equally good results. The trait mechanism has allowed a rather straightforward 
mapping of Star UI elements to internal implementing objects. REFERENCES 1. G. Curry, L. Baer, D. Lipkie 
and B. Lee, "Traits -An Approach to Multiple-Inheritance Subclassing," Conference on Office Automation 
Systems, Philadelphia, Penn., June 1982. 2. E. Harslem and L.E. Nelson, "A Retrospective on the Development 
of Star," submitted to 6th International Conference on Software Engineering; Tokyo, Japan, Sept, 1982. 
 3. C. Irby, L. Berginsteinsson, T. Moran, W. Newman and L. Tesler, "A Methodology for User Interface 
Design", Systems Development Division, Xerox Corporation, January 1977. 4. J. Mitchell, W. Maybury and 
R. Sweet, "Mesa Language manual," Technical Report CSL-79-3, Xerox Corp., Palo Alto Research Center, 
Palo Alto, CA, April 1979. 5. J. Seybold, "Xerox's 'Star'" in The Seybold Report, Media, Pennsylvania: 
Seybold Publications, v. 10, no. 16, 1981.. 6. D. Smith, C. Irby, R. Kimball and E. Harslem, "The 7tar 
User Interface: An Overview," NCC '82. 7. D. Smith, C. Irby, R. Kimball, B. Verplank, and E. Harslem, 
"Designing the Star User Interface," Byte, v. 7, no. 4, 1982. 8. L. Tesler, "The Smalltalk Environment", 
Byte, V. 6, no. 8, 1981. 9. C. Thacker, E. McCreight, B. Lampson, R. Sproull and D. Boggs, "Alto: A 
Personal Computer," Computer Structures: Principles and Examples, D. Siewiorek, C. Bell and A. Newell, 
editors, McGraw-Hill, 1982.  124  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801271</article_id>
		<sort_key>125</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>20</seq_no>
		<title><![CDATA[The graphics CAD/CAM industry(Panel Session)]]></title>
		<subtitle><![CDATA[Financial perspectives]]></subtitle>
		<page_from>125</page_from>
		<doi_number>10.1145/800064.801271</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801271</url>
		<abstract>
			<par><![CDATA[<p>Richard N. Spann</p> <p>Companies supplying graphics CAD/CAM components and systems form an important high technology business segment. Panelists representing four financial perspectives will discuss market entry, financing, segment performance and shareholder's expectations. Development of the graphics CAD/CAM industry in the 80's will be the unifying theme.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Financial (e.g., EFTS)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333318</person_id>
				<author_profile_id><![CDATA[81543708156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[N.]]></middle_name>
				<last_name><![CDATA[Spann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adage, Inc.]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330301</person_id>
				<author_profile_id><![CDATA[81100058762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frederick]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Adler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adler & Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334050</person_id>
				<author_profile_id><![CDATA[81100214787]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Kurlack]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Merrill Lynch]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331656</person_id>
				<author_profile_id><![CDATA[81332515136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Joseph]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[McNay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Essex Investment Management Co.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332288</person_id>
				<author_profile_id><![CDATA[81100182244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Margaret]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Reichenbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[L.F. Rothschild, Unterberg, Towbin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801272</article_id>
		<sort_key>127</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>21</seq_no>
		<title><![CDATA[The Geometry Engine]]></title>
		<subtitle><![CDATA[A VLSI Geometry System for Graphics]]></subtitle>
		<page_from>127</page_from>
		<page_to>133</page_to>
		<doi_number>10.1145/800064.801272</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801272</url>
		<abstract>
			<par><![CDATA[<p>The <italic>Geometry Engine</italic>[1] is a special-purpose VLSI processor for computer graphics. It is a four-component vector, floating-point processor for accomplishing three basic operations in computer graphics: matrix transformations, clipping and mapping to output device coordinates. This paper describes the Geometry Engine and the Geometric Graphics System it composes. It presents the instruction set of the system, its design motivations and the Geometry System architecture.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Arithmetic processing]]></kw>
			<kw><![CDATA[Geometric processing]]></kw>
			<kw><![CDATA[Real-time graphics]]></kw>
			<kw><![CDATA[VLSI]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Geometric algorithms, languages, and systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>C.5.4</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010633</concept_id>
				<concept_desc>CCS->Hardware->Very large scale integration design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010633.10010659</concept_id>
				<concept_desc>CCS->Hardware->Very large scale integration design->VLSI system specification and constraints</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39070428</person_id>
				<author_profile_id><![CDATA[81332493735]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Clark]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Laboratory, Stanford University and Silicon Graphics, Inc., Palo Alto, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H. "A VLSI Geometry Processor for Graphics." Computer 13, 7 (July 1980), 59-68.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clark, J. H. Graphic Display Processing System and Processor. Patent Pending.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>892734</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clark, J. H. Parametric Curves, Surfaces and Volumes in Computer Graphics and Computer Aided Geometric Design. Tech. Rept. 221, Computer Systems Laboratory, Stanford University, November, 1981.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman, W. and Sproull, R. F., Principles of Interactive Computer Graphics. Addison-Weseley, Reading, Mass., 1980.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Geometry Engine: A VLSI Geometry System for Graphics by James 11. Clark Computer Systems Laboratory 
Stanford University and Silicon Graphics, Inc. Palo Alto, California  Abstract The Geometry Engine [1] 
is a special-purpose VLSI processor for computer graphics. It is a four-component vector, floating- point 
processor for accomplishing three basic operations in computer graphics: matrix transformations, clipping 
and mapping to output device coordinates. ]'his paper desribes the Geometry Engine and the Geometric 
Graphics System it composes. It presents the instruction set of the system, its design motivations and 
the Geometry System architecture. Keywords: VLSI, Geometric processing, real-time graphics, arithmetic 
processing CR Categories: 3.3, 3.4, 3.7 Geometry System Overview The Geometry System is a floating-point, 
geometric computing system for computer graphics constructed from a basic building block, the Geometo' 
Engine. Twelve copies of the Geometry Engine arranged in a pipeline compose the complete system in its 
most general form. In its present form, the Geometry Engine occupies a single. 40-pin IC package. The 
notable characteristics of the system are: General Inslruction Sel - It executes a very general 2D and 
31) instruction set of utility in all engineering graphics applications. This instruction set includes 
operations for matrix transformations, windowing (clipping), perspective and orthographic projections, 
stereo pair production and arbitrary output device coordinate sealing.  Cun'e Generation - The system 
will generate quadratic and cubic curves and all of the conic sections, i.e. circles, parabolas, hyperbolas, 
etc.  Defice Independent -The system is independent of the output device used and works equally well 
in either vector- based or raster-based systems. It allows color or black and white polygons, lines and 
characters.  Flexible Input Format The system accepts input coordinates in either integer or floating 
point format.  Permission to copy without fee all or part of this material is granted provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. &#38;#169; 1982 ACM 0-89791-076-1/82/007/0127 $00.75 127  iligh Performance Floating Poini 
Its effective computation rate is equivalent to 5 million floating-point operations, per second, corresponding 
to a fully transformed, clipped, scaled coordinate each 15 microseconds.  Reconfigurable -Each Geometry 
Engine is "softly" configured; that is, one device with a single configuration register serves in twelve 
different capacities.  Selection/ltit-Testing Mechanism - The Geometry Engine has a "hit-testi,g" mechanism 
to assist in "pointing" functions, such as are required for a fast, interactive graphics system with 
a tablet, mouse or other input devices.  Scales to a Single Chip - The system can be put in a smaller 
number of 1C packages as soon as the technology for fabrication reduces the size of the Geometry Engine 
design. Ultimately, the entire 12 Engine system will fit on in one IC package, be a factor of 4 faster 
and be correspondingly reduced in cost.  The Geometry Engine is a four-component vector function unit 
whose architecture is best illustrated by the chip photograph shown in Figures 1 and 2. Fach of the four 
function units along the bottom two-thirds of the photo consists of two copies of a computing unit, a 
mantissa and characteristic. The chip also has an internal clock generator, at the top lzft corner, and 
a microprogram counter with push.down subroutine ~aek, shown at the top right. The upper third of the 
chip is the control store, which holds the equivalent of 40k bits of control ~ore. This control ~ore 
contains all of the mtcrocode that implements the instructions and floating-point computations described 
below. Clock P A D S PC/Stack Control Store ( 40 K bits equivalent ) X X Exponent Mantissa ¥ Exponent 
Mantissa Y I/0 Registers Z Z Exponent Mantissa w W Exponent Mantissa PADS Figure 1: A Block Diagram 
of the Geometr). Engine corresponding to the photo in Figure 2. Computer Graphics Volume 16, Number 
3 July 1982 Geometry System Functions The Geometry System [2] is designed for high-performance, low-cost, 
floating-point geometric computation in computer graphics applications. It is composed of three subsystems, 
each of which is composed of Geomeu3' l-ngines. These subsystems are illustrated in Figure 3. The particular 
position of a Geometry Engine in the pipeline determines its particuar function in the whole system. 
Each Engine has a configuration register that is loaded when the system is powered on, after a Reset 
command is issued. Until the system is reset again, the Engine behaves according to the configuration 
code. Matrl), Engines I I > I ! J Scaler Engines Chpper Engines I I] I < Figure 3: Geometry System; 
each block is a Geometry Engine. The subsystems are: Matrix Subsystem -A stack of 4x4 floating-point 
matrices for completely general, 2D or 3D floating-point coordinate transformation of graphical data. 
 Clipping Subsystem -A windowing, or clipping, capability for clipping 2D or 3D graphical data to a 
window into the user's virtual drawing space. In 3D, this window is a volume of the user's virtual, floating-point 
space, corresponding to a truncated viewing pyramid with "near" and "far" clipping.  Scaling Subsystem 
Scaling of 21) and 31) coordinates to the coordinate system of the particular output device of the user. 
In 31), this ,waling phase also includes either orthographic or perspective projection onto the viewer's 
virtual window. Stereo coordinates are computed and optionally supplied as the output of the system. 
  The characteristics of each of these subsystems follows. Matrix Subsystem The matrix subsystem provides 
arbitrary 2D and 3D transformation ability, including object rotations, translations. scaling, perspective 
and orthographic projection. Using this matrix, it is possible to define a completely arbitrary 2D or 
3D viewing window and accomplish all affine transformations. The matrix transformation subsystem is the 
first four Geometry Engines in the pipeline. Distributed over these Engines is a 4x4 matrix and an eight-deep, 
4x4 matrix stack to accommodate picture subroutine structure. The top element of the stack is the current 
matrix that is used to multiply all incoming coordinates. Full floating-point transformation of all incoming 
coordinates is done by this subsystem in 15 microseconds. Transformed points are supplied by this subsystem 
to the clipping subsystem. The matrix stack allows the use of picture subroutines. All incoming matrix 
multiplication commands cause the current matrix {top-of-stack) to be multiplied by the incoming matrix. 
"lhis allows a graphic object to be attached to a "parent" graphic object, thereby providing for a hierarchical 
drawing. This is done in the same way that a push-down szack ks used in a general- purpose computer for 
storing arguments to subroutines. The I,oadMM command causes a new matrix to be loaded onto the top of 
the s~ack, while a MulIMM command causes the current top of the stack to be multiplied by the supplied 
matrix. The matrix stack can be manipulated by user instructions. In addition to the normal Push and 
Pop commands, there is also a StoreMM command to provide for overflow handling of the stack if picture 
structure depth exceeds the eight levels allowed by the depth of the stack. This subsystem also generates 
cubic and rational cubic curves [3]. An incremental difference matrix for forward difference curves 
can be loaded onto the top of the stack, and a special Iterate command causes the forward differences 
of this matrix to be computed, with the result that a new coordinate on the curve is output to the clipping 
subsystem. Conic curves are generated using rational cubics. New points on the curve are generated in 
10 microseconds. Clipping Subsystem The four to six Geometry Engines following the Matrix Subsystem 
comprise the Clipping Subsystem. Each Geometry Engine in the clipping subsystem clips the objects to 
a single boundary (plane) of the viewing window. Thus, if there is no need or desire to clip objects 
to the near or far clipping boundaries, either or both of the corresponding Engines may be eliminated, 
with no undesired side effects. This might be clone to decrease the cost of the system. The clipping 
subsystem gets all input data after it has passed through the matrix subsystem, so that only transformed 
coordinates are supplied to it. It has no explicit registers that the user may manipulate. It always 
clips transformed coordinates to specific boundaries. The boundaries are made to correspond to particular 
boundaries of the user's drawing space by altering the transformation matrix so that the desired portion 
of the environment to be within the window is scaled to be within the standard clipping boundaries. As 
an assistance in testing objects for intersection with the viewers window, a special hit. testing mode 
is included in the clipping subsystem. This mode disables output of certain data from the Geometry System. 
For example, to select an object on the screen that is being pointed to by the input device cursor, hit. 
testing is enabled and a special hit-testing matrix is loaded into the current matrix. This matrix is 
computed from the screen coordinates of the cursor: it might correspond to a tiny window centered at 
the pointing device's screen coordinates. If anything comes out of the geometry system in this mode. 
it signifies that an object has passed within the tiny window near the cursor position. Of course, the 
hit-testing window may be of any size, so that this feature can be useful m area-select functions, as 
well. To provide further information useful in identifying how objects pass through the hit-window, each 
drawing instruction gets from one to six bits set in it to signify which of the one to six clipping boundaries 
were intersected by the line-segment drawn. To assist in identifying the object, a special object naming 
convention is used, thereby providing a completely general selection and hit-testing mechanism. Scaling 
Subsystem Ste reo Computation The last two Geometry Engines in the pipeline are the Scaling Subsystem. 
This subystem converts output from the Clipping Subsystem to the coordinate system of the output device. 
This process causes the window on the user's drawing space, which is specified by loading the appropriate 
matrix into the Matrix Subs)stem, to be mapped onto a viewport of the output device, which is specified 
by loading the .scaling subsystem's viewport registers. The viewpon registers allow up to 24-bit integer 
values, depending upon the coordinate system of the output device: they arc the only device-dependent 
pan of the system. In 3D, the mapping process includes an orthographic or perspective projection and 
stereo pair production. Because the Geometry System is a homogeneous system that treats all three coordinates 
(x,y,z) the same, the Scaling Subsystem also maps the z coordinate. Thus, by loading the z viewport registers 
with appropriate values, either perspective depth values or intensity depth-cue values will be supplied 
by the Geometry System, according to the manner in which the output device interprets the z values. Of 
course, if no depth values are needed in the particular application, they may be discarded. Either two 
or four integer values are output by the Scaling Subsystem for each coordinate point that comes out of 
the system. When two values come out, they are X and Y, in screen coordinates. If the Scaler Engines 
are configured properly, these four values are: X right - the x screen coordinate for the right eye. 
 X left - the x screen coordinate for the left eye.  Y - the y screen coordinate for both eyes.  Z 
- the perspective depth value for both eyes.  Geometry System Computations The matrix system does the 
computation: Ix" y" z' w'l = Ix y zw]M, where M is the top of the matrix stack and [x y z w] is the 
input vector to be transformed. The coordinates [x' y' z' w'] are supplied to the clipping subsystem, 
which clips them ~ that they satisfy -w'< x'< w', -w'< y'< w', and -w' < z" < w'. Note that these clipping 
boundaries are somewhat different from those used in most homogeneous clipping systems [4], in that the 
z coordinate is treated identically to the x and y coordinates. This simplifies the system, and is equivalent 
to all other homogeneous clipping systems if the correct matrix is used and the proper viewpon sxrale 
factors are used. After clipping, since all points coming out of the clipper satis~ these inequalities, 
the scaler does the final mapping to output device coordinates with the following computations: D = (z'/w')*Ss 
+ Cs, Z = (z'/w')*Sz + Cz, X = (x'/w')*Sx + Cx, and Y = 0"/w')*Sy + Cy. The coefficients Sx and Cx are 
the X half-size and X center of the vie~q~ort in the coordinate system of the output device. Similarly 
for the Y and Z values. The Ss and Cs ~alues are explained in the next section. The Geometry Engine can 
be used to obtain stereo pair pictures at no extra computational cost Consider first the ordinary monographic 
case.  Monographic Case In a system where the origin is the perspective projection point, the ordinary 
projection for 3 dimensional .scenes [4] is to divide both x and y by z. That is, the screen coordinates 
of the point are given by X = (x/z)*Sx + Cx and Y = (y/z)*Sy + Cy, where (Cx,Cy) is the center of the 
"viewport" and (Sx,Sy) is its half-size. If homogeneous coordinates are used, these equations are modified 
to compute perspective depth. The transformation on [x,y,z] is modified to compute homogeneous coordinates 
as follows: [x'y' z' w'] = [x yzl]M. M is chosen to yield Ix', y', z',w'] = [x, y, az+b, z], where a 
= (1+ N/F)/(1-N/F) and b = -.2N/(1-N/F). N and F are the respective distances of the Near and Far clipping 
planes from the projection point. With these definitions, the projected coordinates are computed from 
X = (x'/w')*Sx + Cx,(l) Y = (x'/w')*Sy + Cy, and Z = (z'/w')*Sz + Cz = (a + b/z)*Sz + Cz, where we have 
substituted the values ofz' = az+b and w' = z, from above. This yields the same values for X and Y as 
before. In addition, however, it computes perspective depth, which can be useful in hidden-surface computations. 
With this computation, points at the Near clipping plane will be mapped into Cz-Sz and points at the 
Far clipping plane will be mapped into Cz + Sz. Stereographic Case For proper stereo, we wish to compute 
two different views, one for the left eye and one for the right eye. In other words, there are two different 
projection points that differ in a displacement in the x direction only: Xright = ( (x' + dx)/w' )*Sx 
+ Cx.right, and Xleft = ( (y'-dx)/w")*Sx + Cx.left where dx is half the distance between the two projection 
points (distance from the center of the head to each of the eyes), Cx.left is the center of the left 
projection viewport and Cx.right is the center of the right projection viewport. The Y and Z coordinates 
are unaffected. Defining Cx.offset to he the offset of the right and left vie~q~orts from a "center" 
viewport, Cx, we have Cx.left = Cx -Cx.offset and Cx.right = Cx + Cx.offset. Instruction Set Summary 
The foregoing equations then become The instruction set for the geometry engine partitions into three 
types: Xright = (x'/w')*Sx + Cx + { (dx/w')*Sx + Cx.offset } and Xleft = (x'/w')*Sx t- Cx - { (dx/w')*Sx 
+ Cx.offset }, or Xleft=X+ D, and Xright = X- D, where X is the "normal" X computation in Equation 1 
and D is the quantity in brackets. Note that D is a computation like that of X,Y and Z in Equation 1. 
In other words, it involves a division, a multiplication and an add. Inspection of the third of Equation 
1 suggests that we define "stereo viewport" parameters as follows: Ss = dx*Sx/b, and Cs = Cx.offset - 
a*(dx*Sx/b). Then the quantity D is computed to be D = (z'/w')*Ss + Cs,  giving the required result 
for D when these substitutions are made. The Geometry Engine has four floating-point function units; 
two are required to accomplish one computation of the sort A=(B/C)*E+ F. Therefore, one Engine will 
perform two of these computations, for example for the X and Y coordinates. Since another Engine is required 
to compute Z, it has two free units that can be computing D as well, using the Ss and Cs values defined 
above. If the l'ngine computing I) and Z is put in the pipeline before the X and Y Engine, the X-Y Engine's 
microcode can compute X+D and X-D, outputing the four values IX + D,X-D,Y,Z]. Of course, if no ~ereo 
is desired, but Z is still needed, the coefficients Ss and Cs can be zero. The Geometry F.ngine implements 
this stereo computation, and when properly configured, will output these four quantities. Programming 
the Geometry System The Geometry System is a slave processor. It has no instruction fetch unit: it must 
be given every instruction and data value by a controlling processor. Likewise, the display controller 
must take each value that comes out of the Geometry System. The instruction/data stream supplied to the 
system is a high- level graphics instruction set mixed with coordinate data. Instructions and data are 
supplied to the system via its input port, which is the set of input pins of the first Matrix Subsystem 
Engine, and output data and instructions are taken from its output port, which is the set of output pins 
of the last Scaling Engine. A convenient view of the system is as a hardware subroutine: in fact, this 
is precisely the first way it will be used. as a hardware subroutine to the IRIS processor/memory system, 
which is based on the Motorola 68000 and IEEE Multi-bus. Input data must always be in user's virtual-drawing 
(integer or floating-point) coordinate system, and except in special non-display circumstances such as 
hit-testing, output data is always in the coordinate system of the user's output device. Register Manipulation 
- These instructions alter the matrix, matrix s~ack, or viewport registers. They are used to set the 
window for a particular view of the virtual drawing, load the viewport registers, change the matrix or 
matrix .stack to draw a different object, orient a particular object (rotate, translate, etc.) or ~ve 
the state of the matrix stack for later restoration. Instructions in this category are: o LoadMM -Load 
the following 16 floating-point data values onto the top of the stack, destroying the current matrix. 
The 16 floating-point numbers are the 4x4 matrix. o MultMM -Multiply the current matrix on the top of 
the stack with the following 4x4 matrix. o PushMM -Push all matrices on the stack down one position, 
leaving the current top of stack unaltered. (After this operation the second stack position is a copy 
ofthe top of the stack.) o PopMM -Pop all matrices in the stack up one position. o StoreMM -Store the 
top of the matrix stack. This instruction input to the geometry system causes the StoreMM instruction, 
followed by the 4x4 matrix (16 floating-point numbers) to come out of the Geometry System at its output 
port. It can be used to save the complete state of the matrix stack. o LoadVP -Load the viewport registers. 
Following this instruction, eight 32-bit numbers describing the viewport parameters must be supplied. 
  Drawing Instructions -These instructions actually cause graphic objects to be drawn. All drawing 
instructions are followed by four 32-bit floating-point numbers, representing the (x,y,z,w) coordinates 
of the point being supplied to the Matrix Subsystem for transformation. Each drawing command assumes 
that there is a current point in the drawing, for example the current pen position in a virtual-space 
plotter. Certain instruc.tions update that position, while others cause things to be drawn from that 
point. We refer to this position as the Current Point. Assuming clipping does not eliminate them, each 
of the following instructions except Curve comes out of the Geometry System at its output port, followed 
by the device coordinates. o Move -Move the Current Point to the position specified by the floating-point 
vector that follows. o Morel -Same as Move, but integer data is supplied. o Draw -Draw from the Current 
Point to the position specified by the following data. Update the Current Point with this value after 
drawing the line segment. o DrawI- Same as Draw, except that integer data is supplied. o Point and 
PointI - Cause a dot to appear at the point specified in the following data. Update the Current Point 
with this value after drawing the point. o Curve -Iterate the forward differences of the matrix on the 
top of the matrix stack: issue from the Matrix Subsystem to the Clipping Subsystem a Draw command followed 
by the computed coordinates of the point on the curve. The Current Point is updated just as with the 
Draw command. This command should not be followed by data as with the other drawing commands. o MovePoly 
and MovePolyl -In Polygon mode, move   the Current Point to the position supplied by the following 
data. This command must be used rather than Move ifa closed polygon is to be drawn. o DrawPoly and DrawPolyl 
-In polygon mode, same as Draw command. o CIosePoly -Close the currently open polygon, flushing the 
polygon from the clipping subsystem.  Miscellaneous Commands- o SetHil -Set Hit Mode. This causes the 
state of the Clipping Subsystem to change so that only commands, and not data, are output. Refer to the 
"Selection and Hit-testing" section for a complete description. o ClearHit -Clear Hit Mode. This restores 
the state of the Clipping Subsystem to normal. Refer to the "Selection and ttit-testing" section for 
a complete description. o PassThru -This instruction allows the passing of a variable number of 16-bit 
words through the geometry system unaltered and uninterpreted. It is useful for passing instructions 
and data that are unique to the display controller and that have no meaninng to the Geometry System.The 
number of words to be passed through is specified by a 7-bit field in the instruction.  Selecting and 
Hit-testing In an interactive computer graphics environment it is frequently necessary to select certain 
objects that appear in the display for special attention. This is usually done with the aid of some type 
of input device, such as a light-pen, mouse, tablet or joy-stick. If the input device being used is a 
light-pen, the common selection mechanism varies, but involves detecting in hardware when the "beam" 
of the CRT is under the field of view of the light-pen. This approach is good for pointing at objects 
on the screen but poor for entering new objects into the drawing. because a tracking mechanism must be 
drawing some type of tracking object that the light-pen must be sensing. Because of the extra expense 
of the light-pen tracking mechanism and because many people no longer believe it necessary to actually 
point to objects directly on the screen, the light-pen is not feasible in low- cost systems. The alternatives 
to the light-pen, the tablet and mouse (we chose to ignore the joy-stick), are useful for entering new 
data into drawings, but without an extra mechanism, they are poor for pointing at existing objects in 
a drawing. The hit-testing mechanism in the Geometry System solves this problem. The common software 
mechanism for doing this selection task is to check each object to see if it is in the selection area. 
This selection area might be an area specified by identifying some portion of the drawing ,space to check 
objects against or it might be a small neighborhood around the cursor, which is tracking the position 
of the mouse or tablet. Intelligent operations can be done to reduce the amount of time spent in checking. 
For example, the bounding box around an object can be tested to see if any portion of the object is in 
the selection area: if it is not. then none of the object is in the selection area and therefore need 
not be further tested. This selection task is basically a clipping task, and the Geometry System has 
a special mode for handling it. The Hit-testing mode disables all data from coming out of the Geometry 
System. However, specific drawing instructions still come out of the system, missing their corresponding 
data. Thus. in hit-testing mode, if anything comes out of the output port of the system, this means that 
there was a "hit." In other words, something was in the selection area established by loading the selection 
matrix into the Matrix Subsystem. For a completely general selection mechanism, one might not only like 
to know whether an object passes through the selection window, but also which boundaries it intersects, 
or whether it is completely contained within the selection area, or perhaps completely surrounds the 
area. To accommodate these needs, the Geometry System provides information in the form of "hit-bits" 
that tell which of the six clipping boundaries are intersected by each drawing command, in this way, 
the device that is receiving Geometry System output may assemble the necessary information by "integrating" 
the various "hit-bits" from successive drawing commands used in drawing the object. Hit-testing is useful 
only when combined with a naming mechanism for identifying the objects being drawn. This can be done 
by loading a name register in the display controller before drawing each object that ks to be identified 
with a hit. This can be done using the PassThru instruction. Character Handling Characters provide a 
special problem for any geometric transformation subsystem. Of course, characters may be defined as strokes, 
or vectors, and supplied just as all other data to the Geometry System, but since the number of strokes 
to make up a character might be quite large, we ordinarily do not wish to draw characters in this way. 
On the other hand, any other approach will not provide for complete, general rotations, etc. of 3D characters. 
As a result, most systems must make a compromise and provide characters as a special ease. The usual 
problem with characters is that if they are a special case, then clipping them is a special case. The 
Geometry System clips characters only if they are defined as strokes, just like all other data. However, 
since it must make possible the clipping of special-case characters and character generation in the display 
controller, the LoadVl' instruction and corresponding data is always passed on to the output port of 
the system. The reason for this is that this data defines the boundaries of the character clipping window 
in the display controller. Mixing special-case characters and graphics presents another problem. There 
are two cases: Putting characters in a drawing -this is handled by combining special sentinels to the 
display controller via the PassThru command with the Point command. The Poinl command is used to position 
the beginning of the character string. The Raster Subsystem, which is designed as a companion to the 
Geometry Subsystem, does the actual character clipping. Completely general character clipping is accomplished 
by proper use of these subsystems together.  Putting a drawing with characters This case is straightforwardly 
handled by properly modifying the tranformation matrix to reflect the character clipping window position. 
Then drawing can proceed as usual. The particular modifications for each case are handled by the software 
package mentioned above.  The IRIS Graphics System The Geomeu3' System is being implemented on the 
a system called the Integrated Raster Imaging System, IRIS, which consists of the following components: 
A processor/memor).' board with the Motorola 68000 and 256k bytes of RAM: the memory can be expanded 
to 2M bytes. ]'he 68000 microprocessor executes instructions in the on-board memory at 8 MHz. This memory' 
is fully mapped and segmented for 16 processes. Additional memory is accessed over the Multibus at normal 
Multibus rates.  A Geometry Subsystem, with a multibus interface, lqFO's at the input and output of 
the Geometry System and from ten to twelve copies of the Geometry Engine.  A custom 1024x1024 Color 
Raster Subsystem, with high- performance hardware for polygon fill, vector drawing and arbitrary, variable-pitch 
characters. The hardware and firmware provide for color and textured lines and polygons, character clipping, 
color mapping of up to 256 colors and selectable double or single-buffered image planes.  A 10 Megabit 
EtherNet interface board.   Summary The Geometry System is a powerful computing system for graphics 
applications. It combines a number of useful geometric computing primitives in a custom VLSI system that 
has a future because of its .scalable nature. It is quite likely that within 5 years the system will 
be implemented on one, 1/2.million transistor, integrated-circuit chip, with a correspondingly reduced 
cost and increased speed. Acknowledgements Many people provided advice and suggestions during the two 
years over which this project has been done. Marc Hannah's masterful ability with VLSI Design Tools and 
UNIX and his graphics understanding were indispensible. Professor John Hennessy provided an indispensible 
microcode development tool in SLIM. and his willingness to help us when in need is appreciated. I,ynn 
Conway of Xerox PARC made resources available during the fomaative stages of the project, and without 
them, it probably would not have been carried out: we are indebted to her for this. Forest Baskett of 
Stanford made it possible by supporting us in the early stages. Dick Lyon was an important first advisor 
on 1C design. Martin Haeberli was very helpful in the testing phase. Valuable conversations were had 
with Chuck Thacker, Bob Sproull, Alan Bell, Martin Newell, Ed Chang, Danny Cohen, Doug Fairbairn, John 
Warnock, Chuck Seitz, Carver Mead, and Lance Williams. Hewlett-Packard Corporation fabricated the first 
copy of the first part of the data- path, and Bob Spencer and Bill Meuli of Xerox PARC's Integrated Circuits 
I,aboratory fabricated the first fully functional copy of the entire chip. We are especially grateful 
for the enthusiasm and support of Xerox Corporation's Palo Alto Research Center: this project could not 
have been done without the support of the insightful people there. The research was supported by the 
Advanced Research Projects Agency of the Department of Defense, DARPA, under contract number MDA 903-79-C-0680. 
 References 1. Clark, J.H. "A VLSI Geometry Processor for Graphics." Computer 13, 7 (July 1980), 59-68. 
 2. Clark, J. H. Graphic Display Processing System and Processor. Patent Pending. 3. Clark, J. H. Parametric 
Curves, Surfaces and Volumes in Computer Graphics and Computer-Aided Geometric Design. Tech. Rept. 221, 
Computer Systems Laborator3.', Stanford University, November, 1981.  4. Newman, W. and SprouU, R. F.. 
Principles oflntemctive Computer Graphic~ Addison-Weseley, Reading, Mass., 1980.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801273</article_id>
		<sort_key>135</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>22</seq_no>
		<title><![CDATA[A contour display generation algorithm for VLSI implementation]]></title>
		<page_from>135</page_from>
		<page_to>146</page_to>
		<doi_number>10.1145/800064.801273</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801273</url>
		<abstract>
			<par><![CDATA[<p>Recent articles have discussed the current trend towards designing raster graphics algorithms into VLSI chips. The purpose of these design efforts is to capture some of the real-time animation capability found in vector graphics systems. Currently, real-time vector graphics animation is limited primarily to operations involving coordinate transformations. In order to enhance this animation capability, frequently encountered vector graphics algorithms that require the high speed, parallel computation capability of VLSI must be identified. Real-time contour display generation from grid data is one such algorithm. This paper describes the specifics of a contour display generation algorithm, the architectural framework of a processor that performs this algorithm and the architectural requirements of such a processor.</p> <p>The contouring algorithm is based on a data structure, the contouring tree, whose regularity and amenability for parallel computation make it an ideal candidate for VLSI. The architectural framework for a contouring processor chip that performs this algorithm for the real-time environment of interactive graphics is discussed, particularly the issues of memory size and data distribution. A model of the contouring process is created in order to determine the necessary physical parameters of the contouring processor in this architectural framework. Conclusions are drawn concerning the feasibility of producing a VLSI chip that performs this contouring algorithm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Vector display devices**</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P199384</person_id>
				<author_profile_id><![CDATA[81100549374]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Zyda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Washington University, St. Louis, Missouri]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>578775</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aho, Alfred V., Hopcroft, John E., and Ullman, Jeffrey D. The Design and Analysis of Computer Algorithms. Reading, Massachusetts: Addison-Wesley Publishing Company, 1974, Chapters 1-5.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Barry, C.D. and Sucher, J. A. "Interactive Real-Time Contouring of Density Maps," ACA Winter Meeting, Honolulu, March 1979, Poster Session.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806792</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Clark, James H. "A System Design Revolution," Computer Graphics: A Quarterly Report of SIGGRAPH-ACM, Volume 15, Number 3 (August 1981), pp. 79-80.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Dammkoehler, R.A. Personal communication. October 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>578480</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mead, Carver and Conway, Lynn, Introduction to VLSI Systems, Reading, Massachusetts: Addison-Wesley Publishing Company, 1980, Chapter 8.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Myers, Ware "Algorithms, Ergonomics, and Solid Modeling Highlight SIGGRAPH '81," Computer, Volume 14, Number 10 (October 1981), pp. 126-127.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Newman, William H. and Sproull, Robert F. Principles of Interactive Computer Graphics. Second Edition. New York: McGraw-Hill, 1979, Chapter 1.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806792</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Sproull, Robert "Custom VLSI Chips for Graphics," Computer Graphics: A Quarterly Report of SIGGRAPH-ACM, Volume 15, Number 3 (August 1981), p. 79.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Stritter, Edward and Gunter, Tom "A Microprocessor Architecture for a Changing World: The Motorola 68000," Computer, Vol. 12, No. 2 (February 1979).]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807442</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Wright, Thomas and Humbrecht, John "ISOSRF&#8212;An Algorithm for Plotting Iso-Valued Surfaces of a Function of Three Variables," Computer Graphics: A Quarterly Report of SIGGRAPH-ACM, Volume 13, Number 2 (August 1979), pp. 182-189.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Zyda, Michael J. "Multiprocessor Considerations in the Design of a Real-Time Contour Display Generator," Technical Memorandum 42, St. Louis: Department of Computer Science, Washington University, October 1981.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Zyda, Michael J. "Joystick Driven Display Rotation and Control Console Management," Technical Memorandum 24, St. Louis: Department of Computer Science, Washington University, November 1980.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Contour Display Generation Algorithm for VLSI Implementation Michael J. Zyda Department of Computer 
Science Washington University St. Louis, Missouri 63130 Abstract Recent articles have discussed the 
current trend towards designing raster graphics algorithms into VLSI chips. The purpose of these design 
efforts is to capture some of the real-time animation capability found in vector graphics systems. Currently, 
real-time vector graphics animation is limited primarily to operations involving coordinate transformations. 
In order to enhance this animation capability, frequently encountered vector graphics algorithms that 
require the high speed, parallel computation capability of VLSI must be identified. Real-time contour 
display generation from grid data is one such algorithm. This paper describes the specifics of a contour 
display generation algorithm, the architectural framework of a processor that performs this algorithm 
and the architectural requirements of such a processor. The contouring algorithm is based on a data 
structure, the contouring tree, whose regularity and amenability for parallel computation make it an 
ideal candidate for VLSI. The architectural framework for a contouring processor chip that performs 
this algoritkm for the real-time environment of interactive graphics is discussed, particularly the issues 
of memory size and data distribution. A model of the contouring process is created in order to determine 
the necessary physical parameters of the contouring processor in this architectural framework. Conclusions 
are drawn concerning the feasibility of producing a VLSI chip that performs this contouring algorithm. 
 CR Categories and Subject Descriptors: 1.3.1 ~omputer Graph--i-cs~: Hardware Architecture - vector 
display devices; 1.3.3 [Computer Graphical: Picture/Image Generation display algorithms; Permissiofl 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. &#38;#169; 
1982 ACM 0-89791-076-1/82/007/0135 $00.75 1.3.5 [Computer Graphics~: Computational Geometry and Object 
Modeling -Curve, surfaces, solid and object representations; Geometric algorithms, languages and systems; 
1.3.7 [Computer Graphical: Three-Dimensional Graphics and Realism - Animation; B.7.1 [Integrated CircuitsJ: 
Types and Design Styles -VLSI (very large scale integration); E.I [Data StructuresT: Trees General Terms: 
contouring, VLSI This work has been supported by the following grants: NIHI P01 GM 24483-01AI and NIH5 
T32 GM07564. 1.O INTRODUCTION Recent articles have discussed the current trend towards designing graphics 
algorithms into VLSI chips [3,6,8]. Most of these efforts have concerned limited functions, such as frame 
buffer control. Of these efforts, the majority are directed towards raster graphics because of the simplicity 
and regularity of pixel operations. These projects have aimed at capturing some of the real-time animation 
capability found in currently produced vector graphics systems. In vector graphics, real-time animstion 
is seen primarily as object rotation and translation. This is because most vector graphics systems sre 
based on minicomputers that have little hardware for special graphics other than a matrix multiplier. 
Sophisticated examples of vector graphics animation are usually relegated to the non real-time environment 
of motion picture film. The trend towards the design of graphics algorithms into VLSI chips can change 
this situation for vector graphics systems. VLSI offers the possibility of multiple, computational units 
operating in parallel within the boundaries of a single chip. Because both processing elements and memory 
elements can be easily implemented in VLSI, one is encouraged to find structures formed from these elements 
that can use this available concurrency [5J. In order to take advantage of these VLSI capabilities with 
respect to vector graphics, one must identify graphics algorithms that are partitionable into relatively 
simple subproblems capable of the type of paralleI 135 solution VLSI has to offer. Candidates for this 
treatment are the graphics algorithms frequently encountered in graphics programs and, of this group, 
those that require more than one-thirtieth of a second to compute. One algorithm that has both of these 
characteristics is contour display generation from grid data [11]. This paper discusses the specifics 
of this contouring algorithm, the architectural framework of a processor that performs this algorithm 
and the architectural requirements of such a processor. As a graphics algorithm, contour display generation 
is frequently used in X-ray crystallography, computer-aided tomography, and other applications for which 
grid data is collected. It is generally depicted as a computationally slow operation whose output is 
sent to a plotter or film recorder. A number of papers have been written documenting "breakthroughs" 
that increase the speed of such contouring algorithms. One author has recently reported that his contouring 
subroutine used one second of central processor time on NCAR's Control Data 7600 [10]. Although a contour 
generation program of this speed is useful for static situations, it is found to be lacking when user 
 interaction is important and the succession of images caused by contour level changes is meaningful. 
 One application in which real-time animation is important is the determination of molecular structures 
from the electron density data generated by X-ray crystallography [2]. Such an operation is executed 
interactively by using a computer graphics program that displays a Dreiding (stick) model of the working 
molecule, inside a contour display of the corresponding region of the molecule's electron density grid. 
In addition to the graphics function, the computer program monitors a series of signals generated by 
the user, while turning the various knobs on a control console [12]. The values read from these knobs 
are interpreted by the program as modifications to either the working molecule or the contour display. 
Modifications to the molecule cause flexible bonds to be rotated and bonds to be lengthened; modifications 
to the contour display produce an increase or decrease of the contour level. The goal of this process 
is to produce the stick model of the molecule that best fits inside the given electron density data set. 
The user can determine whether or' not the model fits the density grid by modifying the contour level, 
shrinking the contour surface to the working molecule. Similarly, the user can expand the contour surface 
from the stick model for better visibility. This function requires that the hardware have the capability 
to rapidly change the contour display as its contour level changes. Another application that requires 
the real-time animation of contour displays is connected with the systematic search procedures used in 
Drug Design research at Washington University, St. Louis, Missouri [4]. In this project, all possible 
conformers of a molecule are generated by a systematic, incremental rotation of the flexible bonds hypothesized 
for the molecule. Each molecular conformation generated by this process is checked against steric (Van 
der Waals's) constraints and various user set, geometric constraints. The molecular conformations of 
the molecules that pass the constraints during a run of this systematic search process are visually examined 
by a drug designer/bench chemist for classification and verification. One of the steps in verification 
is to generate an energy surface for passing molecular conformations. This surface is constructed through 
a contouring process. Because it outputs several thousand passing conformations, the process must be 
performed rapidly. A VLSI chip that generates contour displays for such demanding applications must 
be able to produce and distribute a new picture in the amount of time it takes the graphics hardware 
to change display frames. This is less than one-thirtieth of a second. Any greater amount of time is 
discernable by the viewer, either as a flicker or a hesitation in the picture update. In fact, one-thirtieth 
of a second is diseernable to many people, making one-sixtieth of a second a more desirable time for 
the change of display frames.  [7]. 2.0 CONTOURING ALGORITHM 2.1 Contour Display The contouring algorithm 
is best described within the context of a contour display. A contour display is a graphic representation 
of all the isovalued points in a given region of space. The region of space for this algorithm is a 
two-dimensional grid of data values. Its graphic representation is a set of line segments that run through 
interpolated equivalued points on this two-dimensional sheet. A three-dimensional grid can be contoured 
by this algorithm by graphically combining the line segments generated from all possible orthogonal, 
two-dimensional sheets of the three-dimensional grid. This produces a chicken-wire-like view of the three-dimensional 
surface at a particular contour level. The contour level is the value at which a particular sheet is 
contoured. A given, two-dimensional sheet has a continuous series of contour displays between its minimum 
and maximum grid values. The difference between contour displays from one level to the next is not large 
if the difference in levels is not large. This is the basis for the formulation of a data structure that 
represents the continuum of contour displays--the contouring tree [11~. The contouring algorithm proposed 
in this paper is based upon a reduction in the scope of the contouring tree. 2.2 Contouring Data Structure 
 For this discussion a contouring tree is a data structure that represents a 2 x 2 grid region in a form 
that permits the user to easily retrieve the contour display for any given contour level. A contouring 
tree is generated for every 2 x 2 subgrid of a larger, two-dimensional sheet. The creation and use of 
the contouring tree is best described with an example of a small grid. Figure I depicts the line segments 
for contour levels 50 and 1OO. The contour at level 100 is a closed contour that forms a single, connected 
loop. The contour at level 50 is an open contour. Figure 2 presents a contouring tree for the lower, 
lefthand 2 x 2 subgrid of Figure I. The edges of the contouring tree correspond to the directed, downhill 
edges inscribed on the 2 x 2 subgrid. The edges of the tree are ordered, maintaining the same counterclockwise 
ordering as in the original grid. The dashed lines in Figure 2 indicate the order in which the coordinates 
are generated from the contouring tree for the display at levels 100 and 50. The boxed "[]" under nodes 
2 and 5 indicates that a setpoint display command should be generated for any coordinate that is created 
along the edges I-2 and 2-5, respectively. We can best describe the features of the contouring tree in 
the course of the following description of the processes of tree creation and display generation.  2.3 
Creation Of Contouring Trees The first step in the process of creating contouring trees is to choose 
which of the four points that border the 2 x 2 region is to be designated the maxima. The purpose of 
a maxima point is to have a point to serve as the root of the contouring tree. In this context, the maxima 
can be the point that has: (I) the maximum value for the entire two-dimensional grid, or (2) the maximum 
value only for the concerned 2 x 2 subgrid, or (3) the first encountered point of multiple equal maximum 
values for the 2 x 2. This latter condition can be basis for the maxima because, in an ordered consideration 
of grid points, with the maxima set to the first encountered maximum valued point, all equal and maximum 
valued points appear as roots of contouring trees in other 2 x 2 regions. For the selection of maxima 
in this illustration, we have considered the four grid points in counterclockwise order. The selection 
of a local maxims for the 2 x 2 region determines a large part of the configuration of the contouring 
trees. The root of the tree is the maxima and the remaining three points are the immediate descendent 
nodes of that root. The selection of the root also determines the order in which the three descendent 
nodes are added onto the root. In fact, once the root is chosen the descendent pointers are easily indexed 
from a small table.  At this point in the procedure, only two edges of a total of five remain unattached 
to the descendent nodes of the root. The first of the two remaining edges can be attached as the descendent 
of either the first node attached to the root or the second node attached to the root in counterclockwise 
order. To select the attachment for the first edge, one compares the grid values on both ends of the 
free edge. The free edge is attached to the node that has the highest value. This attaches the fourth 
edge of the 2 x 2. The fifth and remaining free edge is attached like that the fourth. It can be added 
onto either the second node attached to the root or the third node attached to the root in counterclockwise 
order. The conditions for this final step differ from the latter only in the possibility that this fifth 
edge would be the second descendent edge added to the second descendent node of the root. The addition 
of this edge completes the formation of the contouring tree for the 2 x 2 grid section. Display pen 
command information must be placed in the contouring tree when the edges are attached during tree construction. 
Two display commands are required for drawing the contour display: setpoint and drawto. The setpoint 
command causes the display pen to be moved in a non-drawing mode and set on a specified coordinate. The 
drawto command causes the display pen to draw from the current position of the pen to a specified coordinate. 
One must place a setpoint command in the contouring tree on the lower valued node of each perimeter edge 
whose downhill direction is counterclockwise. This command appears in Figure 2 as a boxed "~3" on nodes 
2 and 5, with respect to edges I-2 and 2-5. All other nodes have drawto commands. The display pen command 
information indicates when a line enters the 2 x 2 from a neighboring 2 x 2. This portion of the algorithm 
is described in greater detail in [117. After this information has been entered, the contouring tree 
can be used for the generation of coordinates and display pen commands at the selected contour level. 
 2.4 Display Generation From A Contouring Tree 0nly four configurations of the ordered tree can be 
created for any 2 x 2 subgrid (Figure 3). Because of this limited number, one can select the tree configuration 
during the tree construction process and use that configuration information to create a list containing 
the order in which the tree's nodes should be considered for display generation. This list is termed 
the "enumeration list." The order of the nodes on this list is top-down from the root and counterclockwise. 
Generated with the enumeration list is a second list that specifies the "next node" to consider if one 
places a coordinate on an edge of the tree. With this second list one can skip over nodes lower on the 
path formed by the connected set of nodes from the root to an external node. The edge under consideration 
list of Figure 2 serves to remind the user that when one is considering node 2 one is determining if 
a coordinate is to be 137 generated along the edge I-2. The architectural framework of this The nodes 
on the enumeration list are sequentially examined. If the grid value contained at a node is less than 
or equal to the currently selected contour level, a coordinate is generated, via linear interpolation, 
for the edge under consideration. At the same time a display pen command is issued. This display pen 
command is for the operation contained at the lower valued node of the edge from which the coordinate 
was generated. After the generation of the coordinate and display pen commands, the position in the enumeration 
list is advanced according to the value of the next node list element for that lower valued node. If 
the grid value contained at a node is greater than the currently selected contour level, consideration 
is given to the next node on the enumeration list. This process continues until either the enumeration 
list is exhausted or the next node list causes it to be exhausted.  2.5 Algorithm Parallelism  To 
this point we have described the method for the generation and use of the contouring tree for a single 
2 x 2 grid region. In order to generate the contour display for a larger plane, this algorithm must be 
executed for every 2 x 2 subgrid of this plane. The processing involved in the display generation for 
each of these 2 x 2 subgrids is independent of that performed for any of the neighboring 2 x 2 grid regions. 
One may think of the computation for each 2 x 2 as occurring in a "cellular" processor only concerned 
with that 2 x 2 subgrid. Synchronization during the contour generation process is not required, nor is 
complex data communication. The only communication necessary is that of transmitting grid endpoints, 
contour levels, and control signals to each 2 x 2 cellular processor and that of retrieving the display 
coordinates and commands from each cellular processor. Because data communication is minimal and there 
is no requirement for synchronization of the contouring procedures for each 2 x 2 subgrid, the potential 
for concurrency is quite large. 3.0 ARCHITECTURAL FRAMEWORK  From the above discussion of algorithm 
parallelism, we can determine the type of VLSI layout necessary for the 2 x 2 cellular processor and 
its interconnections (see Figures 4 and 5). Before describing this VLSI architecture, we must first 
understand how the capability for real-time contouring is to be used. For this discussion, we describe 
the necessary characteristics of a "contouring processor." For initial consideration, we should think 
of this processor as a single VLSI chip, although examination may show that the processor might require 
multiple chips because of VLSI density limitations. contouring processor is that of a device used in 
conjunction with the typical display processor/minicomputer system. In this system, the host minicomputer 
initiates the contouring processor whenever a new contour level is detected or a new grid is delivered. 
The contouring processor computes the new display according to the algorithm discussed and deposits the 
resulting coordinates and display pen commands into the picture memory of the display processor. A display 
system can contain several contouring processors. The number of contouring processors required depends 
on two factors: the maximum size of the grids that one chooses to contour in real-time and the total 
number of the maximum size grids that must be contoured by each processor. In order to specify these 
factors for a particular display system, we must choose a problem requiring contouring. Using the molecular 
modeling program presented in the introduction as the typical application, we find that the largest three-dimensional 
grid of concern is a cube of 30 units on each side [2]. As discussed, a three-dimensional grid is contoured 
by generating the display for all of the possible orthogonal planes that compose the grid. One must therefore 
contour 90, 30 x 30 planes in order to complete the picture representing the 30 x 30 x 30 cube.  3.1 
Architectural Modeling  After the selection of a target application and a figure for its maximum grid 
size, one must compute a value for the maximum contouring capability of a single contouring processor, 
i.e., the number of 30 x 30 sheets it can contour in one-thirtieth of a second. To obtain this value 
for the contour generation process, one constructs a model of this process and monitors the behavior 
of this model while it simulates the performance of the contouring processor. Aho's [I] discussion of 
algorithm analysis methods is pertinent to this formulation. In Aho's computational models the key to 
analyzing the time complexity of an algorithm is the ability to assign a time cost on an instruction 
by instruction basis for the process under consideration. Central to his modeling methodology are the 
assumptions of a uniform time cost for each instruction and a varying time cost, depending on the operands 
required at each instruction. Although the computational models of [I] are generally used for determining 
the time-order of magnitude of simple algorithms, they can be extended to produce a modeling methodology 
for more complex processes. The design under consideration in this paper takes this approach, rejecting, 
however, Aho's analytical method of determining time costs in favor of an explicit totaling of the memory 
reference costs for a series of randomly generated grids. This choice is based on the assumption that 
the execution of a single memory reference takes the same time or longer than the execution of a single 
instruction. 138 For this model, the memory reference totals are recorded in three parts. These are 
the grid to contouring processor transfer time, the actual display generation time, and the coordinate/display 
pen command delivery time. Upon the assumption that the transfer is accomplished serially, the grid transfer 
time is computed by totaling the number of grid elements to be transferred to the contouring processor 
from the host minicomputer. The display generation time is computed by totaling the memory references 
required to compute the display for a single 2 x 2 subgrid. In this case, all the 2 x 2 subgrid displays 
are understood to be computed in parallel. The time for the delivery of the coordinate/display pen commands 
is computed by totaling the maximum number of coordinates possible from a single 30 x 30 sheet and multiplying 
this value by four. The value of four was determined upon the assumption that each coordinate/display 
pen command set is representable by a quadruple of memory locations. One can easily compute the first 
memory reference total--the transfer of a single 30 x 30 grid from the host minicomputer to the contouring 
processor. Assuming that each element of this grid is representable by one memory location and allowing 
additional references for overhead, one finds 1OO0 memory references to be a reasonable approximation 
of the reference total for this transfer operation. It is more difficult to compute the second memory 
reference total--the actual time for display generation. In order to accomplish this task within the 
modeling framework discussed, one must program the contouring algorithm as it would be written for the 
contouring processor. One then evaluates the probable memory reference contribution at each branch point 
of that program. Having performed these two steps, one executes the contouring program with imbedded 
memory reference counters. The number of references, approximately 2500 per 2 x 2 subgrid, is generally 
constant for all 2 x 2 subgrid configurations. This number is constant because the algorithm consists 
primarily of table lookups derived from the initial maxims choice. The third memory reference total--the 
coordinate/display pen command transfer from the contouring processor to the picture memory of the display 
processor, is the largest part of the 30 x 30 memory reference count. This reference total depends on 
the maximum number of coordinates and display pen commands that can be generated for a single 30 x 30 
grid. The maximum number of coordinates and display pen commands for a single 2 x 2 subgrid is four. 
Given 841 2 x 2 subgrids in a single 30 x 30 sheet, there are a maximum of 3364 coordinate/display pen 
command quadruples. Tests with randomly generated grids, however, have shown 3364 to be too large a number. 
The largest number encountered is 2600 coordinate/display pen commands. Using 2600 coordinate/display 
pen commands as the limit and multiplying by the size of the quadruple, we find that approximately 11,OOO 
references are needed to transfer the largest display for a single 30 x 30 grid.  Summing the three 
reference counts, the memory reference total for contouring a single 30 x 30 grid is approximately 15,OOO. 
Assuming a rather fast memory and comparable processor and using IOO,0OO references as the maximum number 
of references to be allowed in one-thirtieth of a second, one finds that six, 30 x 30 grids can be contoured 
by each contouring processor. This means that fifteen contouring processors are needed to contour the 
90, 30 x 30 grids that make up the display for the 30 x 30 x 30 cube.  3.2 Architectural Problems 
This model poses two serious architectural problems: (I) There are difficulties of memory contention 
in the delivery of the contour display to the picture memory; and (2) assuming that the data can be delivered, 
there are possibly more vectors than currently available vector graphics systems can draw. Because the 
second problem is more serious, we consider it first. The maximum number of vectors that we expect to 
generate for the 90, 30 x 30 grids is 234,000. The computer graphics manufacturer Evans and Sutherland 
claims for its latest product, the PS-3OO, the capability to display 95,000 vectors. The difference between 
the two numbers, although not an order of magnitude, is sufficiently large for concern. The total number 
of vectors produced by the contouring processor must be reduced to a quantity that the display device 
can handle. The ideal location for this reduction is in the contouring processor, before the delivery 
of the coordinate and display pen commands. One method by which to reduce the total number of vectors 
is based on the actual visibility of the object after it has been transformed by a viewing matrix. In 
this method, coordinates that are output from the contouring process would be transformed by that matrix 
and examined to determine whether they were visible within the boundaries of the display screen. Vectors 
entirely out of range of the screen's boundaries would not be passed on, and vectors inside the boundaries 
would be passed on. Vectors that cross the screen's boundaries would be clipped inside the contouring 
processor and the clipped version of the coordinates would be sent to the display device. This method, 
however, provides only a partial solution. A problem occurs when the viewing matrix projects the entire 
picture within the screen's boundaries. Under these conditions, it would be necessary to make a second 
visibility check in order to determine if the vectors generated from the transformed coordinates could 
be plotted as single points, i.e. degenerate vectors. The procedure for making this check would depend 
upon the limitations of the display device. If the display device could not handle the required number 
of vectors, it might be necessary to provide a degeneracy window value as a tuning device. This window 
value would allow one to map small lines into single points. The test for degenerate vectors would be 
performed at  139 the same time as the check for visibility within the screen's boundaries. The most 
difficult part of this solution would be the actual coordinate transformation operation. Assuming that 
floating point operations were performed as part the contouring process, this reduction would not be 
difficult. The additional capabilities of coordinate transformation and the check for vector visibility 
and degeneracy could be accommodated in a total of approximately 4000 references. The other architectural 
problem is the memory  contention during the delivery of the contour display to the picture memory of 
the display processor. This problem arises from the original architectural framework. The framework 
is that of a typical display processor/minicomputer system. Current display processor systems have only 
one picture memory. This picture memory is accessed by the display processor, in order to refresh the 
screen, and by the host minicomputer, in order to deposit the latest display update. A problem occurs 
when fifteen contouring processors access a single picture memory. Under these conditions the contention 
for memory is significant, preventing the delivery of the latest picture and the display of the current 
picture. One solution to the problem of memory contention would be to partition the picture memory. 
A memory partition would be provided for each contouring processor, insuring that each contouring processor 
only shares its portion of display memory with the display processor. The memory could be organized so 
that it logically appeared as one memory to the display processor. To resolve any remaining contention 
between the single contouring processor and the display processor one would provide a precedence mechanism 
that favors the display processor, because display refresh must occur on time. The architectural problems 
encountered in the design of the contouring processor appear resolvable. The addition of coordinate transformation 
as an integral part of the contouring processor is an easily compartmentalized and isolated system change. 
It can be imbedded in the contouring processor with little impact on the framework of the display system. 
The only effect foreseen occurs in passing the viewing matrix to the contouring processor. By contrast, 
the architectural problem of display memory partitioning significantly affects the design of the display 
system. This  was expected, because experience with current vector graphics systems has shown that the 
most frequent bottleneck in updating a display is the transfer of data from the host computer to the 
display memory via a single, serial pathway. This is the most serious impediment to the animation capability 
of current vector graphics systems. The advent of special graphics processors, such as the contouring 
processor, will require the reconsideration and redesign of the commonly used display processor system. 
 3.3 VLSI Perspectives In the preceding sections we have defined the architectural framework and required 
capabilities of the contouring processor. With this overview of the architectural requirements, we can 
begin to describe how such an architecture can be implemented in VLSI. Figure 4 is a schematic view of 
the 2 x 2 cellular processor. The interconnection scheme for a set of these 2 x 2 cellular processors 
is shown in Figure 5, which is derived from the discussion of algorithm parallelism in Section 2.5. In 
reference to these two figures, the VLSI decision that must be made is how much can be placed on a single 
VLSI chip. It would be ideal to be able to place all the cellular processors for all six planes of the 
30 x 30 grids described at the end of the modeling discussion in Section 3.1 onto a single chip. This 
would require a single chip total of 5,O46 cellular processors, each of the complexity shown in Figure 
4. This is clearly impossible for reasons of VLSI density limitations. The practical question, then, 
is how many 2 x 2 cellular processors can be put on a single chip. In order to answer this, we must first 
consider the hardware requirements, as sketched in Figure 4.  3.4 Hardware Requirements There are 
5 functions that the 2 x 2 cellular processor has to be able to perform on external command: (I) collect 
grid data from the system bus, (2) collect the view matrix from the system bus, (3) receive the contour 
level from the system bus, and execute the contouring/ coordinate transformation procedure, (4) output 
the generated coordinates to the system bus for transfer to the picture memory of the display processor, 
and (5) reset and count. Three of the five functions require very little in the way of special hardware 
because they are only operations that transfer data to/from the cellular processor's memory from/to the 
system bus. The only special requirement for this data transfer operation is that it be capable of addressing 
each cellular processor. When collecting grid data from the system bus, the cellular processor needs 
to be able to ignore all grid data not specifically addressed for it. The cellular processor also needs 
to be able to ignore output commands on the system bus. This implies that each cellular processor has 
knowledge of which 2 x 2 subgrid it represents and, further, that the external system bus control line 
indicates, at some time, which cellular processor is being addressed. The external address presentation 
to the cellular processor is containable within 15 bits, given the 30 x 30 x 30 limitation discussed 
earlier. This external address is placed into the External Data Register of each cellular processor upon 
the initiation of each grid data delivery cycle and each coordinate retrieval cycle. The actual data 
delivery/retrieval then follows the address indication. The next question is how does the cellular processor 
receive and maintain its copy of this address. There are two possibilities.  The first possibility is 
to have the address of each cellular processor permanently contained within the cellular processor in 
a small ROM. This would require a slightly different VLSI layout for each cellular processor. This is 
clearly unsuitable because we can benefit the most from VLSI if we strive towards "regularity." Regularity, 
at least for VLSI, means many copies of the same thing. The best mechanism, then, for assigning addresses 
to the cellular processors is one in which the layout for each cellular processor is identical. The 
second possibility for cellular processor address assignment is that of a chain of "count enabling" wires, 
one input to and one output from each cellular processor. With this wire, each processor is sequentially 
addressed in the following fashion: simultaneous to the initiation of the external reset function, the 
first cellular processor in the chain has its Enable In line set high. The reset/enable combination causes 
the cellular processor to increment the address held in the External Data register by one. This address 
is then returned to the system bus at the same time that the Count Enable Out line is set high. This 
enables the neighboring cellular processor, which follows the same procedure. The end result is that 
each cellular processor has a unique "address" which can be used, through judicious upper level control, 
to deliver~retrieve data on an individual cellular processor basis. In addition to meeting the regularity 
criterion mentioned above, this mechanism is one that can easily travel across the multiple chip boundaries 
necessary in the composition of the contouring processor. Having shown that four of the five functions 
the cellular processor performs on external command can be accomplished with a minimum of special hardware, 
we now examine the requirements of the final function. The hardware requirements for this function, 
that of executing the contouring and coordinate transformation procedures, are not substantial, although 
the execution sequence for this hardware is not trivial. The cellular processor needs an ALU capable 
of integer multiplication, division, addition, and subtraction. From the description of the algorithm 
it appears as if floating point arithmetic were required. The contouring procedure, however, was originally 
implemented using entirely integer arithmetic and should therefore present no problem for such a limited 
function ALU. Coordinate transformations are likewise capable of simulation via integer arithmetic. 
Another piece of hardware important when considering the contouring~transformation function is that of 
memory. Figure 4 shows a 128 word (16 bit) RAM that should suffice for all contouring tree construction, 
grid collection, coordinate generation, and coordinate transformation operations expected in the cellular 
processor. There are other pieces of hardware that take up space on the VLSI chip, such as the ALU input 
and output registers, the ALU and Cell flags, and the previously mentioned External Data register. But 
 the hardware that takes up the most space is that concerned with control. The control section is made 
up of the External Instruction register, the Microprogrsm counter logic, the Decoder, and the Microcode 
Memory. The amount of chip space taken up by the External Instruction register, the Microprogram counter 
logic, and the Decoder is not large in comparison to that taken up by the Microcode memory. The Microcode 
memory holds the sequence of hardware execution instructions for the 2 x 2 contouring tree creation, 
coordinate generation, and coordinate transformation procedures. The Microcode memory also holds the 
 instruction sequences for the other externally initiated functions previously mentioned. Using the figures 
from the architectural modeling section, it is estimated that approximately 4096 16 bit words are needed 
to accommodate the operations necessary for all of the functions of the cellular processor. Making comparisons 
on a byte-by-byte basis with devices such as the MC68000 [9] or the available 64K RAMs, we estimate from 
two to four cellular processors on a single chip. Without actually completing the VLSI design, there 
is no adequate way to estimate the amount of space it requires on a VLSI chip other than to roughly compare 
the hardware requirements with already existing VLSI devices. The presentation of a complete VLSI design, 
however, is not within the scope of this paper. We have been concerned here to examine the extent to 
which the presented contour display generation algorithm can benefit from the highly concurrent capabilities 
suggested by the VLSI technology. 4.0 CONCLUSION This paper has presented an algorithm whose VLSI implementation 
can greatly increase the animation capability of current vector display systems. The algorithm for contour 
display generation was selected for examination because of its frequent use in computer graphics programs. 
Two applications have been introduced in which the need for a real-time contour display generation capability 
is evident. For such applications we have discussed the development of a contouring algorithm that can 
use the highly parallel computation capability available in the VLSI technology. In connection with this 
algorithm, we have examined the architectural framework for a contouring processor that performs the 
contouring algorithm. This discussion serves to highlight features of the contouring algorithm that are 
not easily recognized from its serial description. The architectural requirements of the contouring processor 
have been formulated with respect to the hardware necessary for its VLSI implementation. A question of 
the feasibility of using multiple cellular processors on a single VLSI chip has also been addressed. 
We have concluded from our research that the number of cellular processors per chip, though low, is sufficiently 
large to warrant further development.   5.0 REFERENCES I. Aho, Alfred V., Hopcroft, John E., and Ullman, 
Jeffrey D. The Design and Analysis of Computer Al~orithms. Reading, Massachusetts: Addison-Wesley Publishing 
Company, 1974, Chapters I-5. 2. Barry, C.D. and Sucher, J. A. "Interactive Real-Time Contouring of Density 
Maps," ACA Winter Meeting, Honolulu, March 1979, Poster Session.  3. Clark, James H. "A System Design 
Revolution," Computer Graphics: A Quarterl 7 Report of SIGGRAPH-ACM, Volume 15,--Number 3 (August 1981~V, 
pp. 79-80.  4. Dammkoehler, R.A. Personal communication. October 1981.  5. Mead, Carver and Conway, 
Lynn, Introduction to VLSI Systems, Reading, Massachusetts: Addison-Wesley Publishing Company, 1980, 
Chapter  8. 6. Myers, Ware "Algorithms, Ergonomics, and Solid Modeling Highlight SIGGRAPH '81," Computer, 
Volume 14, Number 10 (October 1981), pp. 126-127. 0 (i, 3) (1,2) 70 (l,l) 20 (i,i) 7. Newman, William 
H. and Sproull, Robert F. Principles of Interactive Computer Graphics. Second Edition. New York: McGraw-Hill, 
1979, Chapter I. 8. Sproull, Robert "Custom VLSI Chips for Graphics," Computer Graphics: A Quarterly 
Report of SIGGRAPH-ACM, Volume 15, Number 3 (~ 1981), p. 79.  9. Stritter, Edward and Gunter, Tom "A 
Microprocessor Architecture for a Changing World: The Motorola 68000," Computer, Vol. 12, No. 2 (February 
1979).  10. Wright, Thomas and Humbrecht, John "ISOSRF -- An Algorithm for Plotting Iso-Valued Surfaces 
of a Function of Three Variables," Computer Graphics:  Quarterly Report of SIGGRAPH-ACM, Volume 13, 
Number 2 (August 1979), pp. 182-189. 11. Zyda, Michael J. "Multiprocessor Considerations in the Design 
of a Real-Time Contour Display Generator," Technical Memorandum 42, St. Louis: Department of Computer 
Science, Washington University, October 1981.  12. Zyda, Michael J. "Joystick Driven Display Rotation 
and Control Console Management," Technical Memorandum 24, St. Louis: Department of Computer Science, 
Washington University,  November 1980. Figure 1  30 60 ~0 Contour Levels 50 + i00  50 9O (2,1) 
(3,1) Figure 2 Sample Contouring Tree for 2 x 2 Subgrid 70 l/20 150 50 i00 .... ~ 150 50 [] 20 Order 
i 2 3 4 5 6 # Enumeration i 2 5 3 4 6 List Next Done 3 3 4 Done Done Node List Edge Under Consideration 
root alone i -2 2 -5 I-3 i -4 4 -6 143  Figure 3 All Possible Configurations of the Contouring Tree 
Generated for a 2 x 2 Subgrid J>b  Figure 4 Schematic View of 2 x 2 Cellular Processor Control Inputs 
code Memory  (4096, 16 bitwd)  A, i l ecoer r I I (128~wwds) I ~ 1 ~Logic " 16 blts/wd [ ALU 
 ALU- IN@ ALU- IN 1 I  F Cell Bus I, l~tr Oa~a ~eg I IExt. Instr. Reg. 1 I System Bus Count Enable 
 In/Out Figure 5 2 x 2 Cellular Processor Interconnection Scheme o= t i  II I ' I i 1 I i I 
i .... I L_ J i--I .i __J .... I I "T I + I, -i I (i,i+2) (,i+2) I I l(l,i+l) (2, i+l)   I I" __ 
 i I I L' I I ,I ,I , ÷ I I j J I J L___I I .1_ _. ----"1 T -I "i F l + 1 (l,i+l) (i,i+Z) <2,1+i>?,~i>. 
 (l,i) (2,~) (2,i) (3,i) I No'te: Each cellular processor has a copy of the 4 grid points and grid 
point coordinates it represents. The cellular processor is responsible for generating the coordinates 
and drawing instructions for its assigned 2 x 2 subgrid on each receipt of a new contour level. 146 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801274</article_id>
		<sort_key>147</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>23</seq_no>
		<title><![CDATA[A rectangular area filling display system architecture]]></title>
		<page_from>147</page_from>
		<page_to>153</page_to>
		<doi_number>10.1145/800064.801274</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801274</url>
		<abstract>
			<par><![CDATA[<p>A display system architecture which has <italic>rectangular area filling</italic> as its primitive operation is presented. It is shown that lines can be drawn significantly faster with this architecture than with a pixel display system. The rendition of filled boxes is also faster showing an <italic>O(n<supscrpt>2</supscrpt>)</italic> speed improvement. Furthermore filled polygons can be rendered with an <italic>O(n)</italic> speed improvement.</p> <p>The design and implementation of this rectangular area filling architecture are discussed and refined. A custom VLSI integrated circuit is currently being designed to implement this rectangular area filling architecture and at the same time reduce the display memory system video refresh bandwidth requirements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>B.6.1</cat_node>
				<descriptor>Memory control and access**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.6.1</cat_node>
				<descriptor>Memory used as logic**</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.7.1</cat_node>
				<descriptor>Memory technologies</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.7.1</cat_node>
				<descriptor>VLSI (very large scale integration)</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>B.3.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010607</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Semiconductor memory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010607</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Semiconductor memory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010607</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Semiconductor memory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010633</concept_id>
				<concept_desc>CCS->Hardware->Very large scale integration design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010600.10010607</concept_id>
				<concept_desc>CCS->Hardware->Integrated circuits->Semiconductor memory</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P59510</person_id>
				<author_profile_id><![CDATA[81100452270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Whelan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, California Institute of Technology, Pasadena, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J.E. "Algorithm for computer control of a digital plotter" IBM Systems Journal, Vol. 4, No. 1, 1965, pp. 25-30.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clark, J.H. "Structuring a VLSI System Architecture" LAMBDA, Vol. 1, No. 2, 1980, pp. 25-30.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Henry Fuchs and John Poulton "PIXEL-PLANES: A VLSI-Oriented Design for a Raster Graphics Engine" VLSI Design, Vol. 2, No. 3, 1981, pp. 20-28.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806791</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Satish Gupta, Robert F. Sproull and Ivan E. Sutherland "A VLSI Architecture for Updating Raster-Scan Displays" Computer Graphics, Vol. 15, No. 3, 1981, pp. 71-78.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bart Locanthi "Object Oriented Raster Displays" Proceedings of Caltech Conference on Very Large Scale Integration, 1979, pp. 215-225.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Takashi Ohzone, Juro Yasui, Takeshi Ishihara and Shiro Horiuchi "An 8K&#215;8 Bit Static MOS RAM Fabricated by n-MOS/n-Well CMOS Technology" IEEE Journal of Solid-State Circuits, Vol. SC-15, No. 5, October 1980, pp. 854-861.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Toshio Wada, Hiroshi Yamanaka, Mitsuru Sakamoto, Hirohiko Yamamoto and Shigeki Matsue "A 16 DIP, 64 kbit, Static MOS-RAM" IEEE Journal of Solid-State Circuits, Vol. SC-16, No. 5, October 1981, pp. 488-491.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Gary S. Watkins "A real-time visible surface algorithm" University of Utah Computer Science Department UTECH-CSc-70-101, June 1970.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Rectangular Area Filling Display System Architecture Daniel S. Whelan Computer Science Department 
California Institute of Technology Pasadena, California 91125 Abstract A display system architecture 
which has rectangular area filling as its primitive operation is presented. It is shown that lines can 
be drawn significantly faster with this architecture than with a pixel display system. The rendition 
of filled boxes is also faster showing an O(n 2) speed improvement. Furthermore filled polygons can be 
rendered with an O(n) speed improvement. The design and implementation of this rectangular area filling 
architecture are discussed and refined. A cus- tom VLSI integrated circuit is currently being designed 
to implement this rectangular area filling architecure and at the same time reduce the display memory 
system video refresh bandwidth requirements. CR Categories and Subject Descriptors: 1.3.1 [Com- puter 
Graphics]: Hardware Architecture --raster display devices; 1.3.3 ]Computer Graphics]: Picture/Image Gener- 
ation --display algorithms; B.3.m [Memory Structures]: Design Styles -- miscellaneous; B.6.1 [Logic Design]: 
Design Styles -- memory control and access; B.6.1 [Logic Design]: Design Styles --memory used as logic; 
B.7.1 ]Integrated Circuits]: Types and Design Styles -- Memory technologies; B.7.1 [Integrated Circuits]: 
Types and Design Styles --VLSI Introduction Presently, most computer graphics systems are of two architectures. 
Calligraphic displays render lines as their primitive operation whereas raster scan displays render pix- 
els as their primitives. This paper discusses a new type of Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. &#38;#169; 1982 ACM 0-89791-076-1/82/007/0147 
$00.75 display that rehders filled rectangular areas as its primitive operation. This type of display 
can be considered a sub- class of raster scan displays and as such it retains all of the attributes of 
such display systems. This paper examines algorithms for rendering lines, filled boxes and filled polygons 
and compares time com- plexities for these algorithms on conventional raster scan displays with those 
on rectangular area filling displays. This analysis will show that the design and implementation of a 
rectangular area filling display system is desirable. Very Large Scale Integration (VLSI) is beginning 
to show itself in graphics hardware. VLSI has made com-puter graphics possible by providing large inexpensive 
ran- dom access memories (RAMs) and several researchers have demonstrated its use for realizing high-performance 
hard- ware in a low-cost medium ]Clark 1980, Gupta 1981]. Fur- thermore major manufacturers will be providing 
such in- tegrated circuits in the near future (NEC and Intel). These uses of VLSI technology will make 
high performance graph- ics systems low cost propositions in the near future. Other researchers have 
chosen to use the flexibility of VLSI to design display systems that have to be considered radically 
different architectures [Fuchs 1981, Locanthi 1979]. This paper uses VLSI technology to implement a "processor 
per pixel" display system. This device is different from pre- vious attempts in that it attempts to ride 
the coat tails of the RAM manufacturers to provide a large number (4096) of processors per device. Applications 
exist for which a rectangular area filling display system would provide substantial system improve- ments. 
One such application area is Computer Aided Design (CAD), which may gain the most from such a display 
since most CAD applications involve the display of non-shaded filled polygons most of which are rectangles. 
Another ap- plication area is real time animation systems since the dis- play system can provide the 
nice property of robustness. That is, as the system becomes overloaded, its performance degrades smoothly. 
 Algorithms This section will analyze algorithms for the rendition of lines, filled boxes and filled 
polygons on traditional pixel oriented displays and on rectangular area filling display systems. The 
primitive operation that pixel oriented display systems perform is the setting of a 1 X 1 rectangular 
area to some intensity i in some constant time rp. The primi- tive operation that a rectangular area 
filling display system performs is the setting of an axis-aligned n X m rectan-gular area to some intensity 
i in some constant time ra. Thus, rectangular area filling display systems have all the generality of 
pixel oriented display systems since we can set n = m = 1. The relative performance of the two systems 
is determined by rp/ra. In some cases, ra may be smaller than rp since some inexpensive pixel based systems 
require software to select the pixel from within a word of pixels. The following discussions of algorithms 
will use ra and rp but the reader should consider ra ~ rp. For the purpose of discussing line drawing 
algorithms, a line can be described independently of its endpoints by the differences in x and y between 
the endpoints. These differences will be referred to as A x and Av. Efficient line drawing algorithms 
for pixel display devices [Bresenham 1965] require max([A~[, IAy[)+ 1 pixel write operations. Such algorithms 
can be directly implemented on rectan-gular area filling display systems, thus the worst case line drawing 
performance of the two systems is similar. Of course, horizontal and vertical lines are rectangles with 
a height or width of a single pixel and can be drawn in time ra. In fact, on a rectangular area filling 
display system, any line can be composed of min(lA~[, [Avl ) + 1 boxes, so it is possible to render a 
line in time ra. (min(IA=l, IANI) + 1). Worst case behavior for this algorithm occurs when [Ax[ = ]AN[ 
and the algorithm requires lAx[ + 1 display write operations which is the same behavior as algorithms 
on pixel displays. Average behavior of these two line drawing algorithms can be calculated by considering 
an uniform angular distribution of lines of unit length. If the line length is much larger than one pixel, 
then the constant term can be dropped and the mean relative performance, P, of these two algorithms is: 
max(]A~], IlXNI) dO  = I0 f0 mi-(la= h la l) Symmetry allows us to only consider the range from 0 
to ~. In this range, max(]A~], IAN]) reduces to cos0 and mill(]A~ ], ]AN I) reduces to sin 0 yielding: 
-fi = fo~ rp.cosOdO __ rp-sinO[~ If ra = rp average line drawing time is 241% faster using a rectagular 
area filling display system than with a conventional pixel based system. Furthermore, if we con- sider 
that in many applications, the angular distribution of lines is not unitbrm but tends to emphasize horizontal 
and vertical lines, the rectangular area filling display performs even better. Consider filling a l X 
w axis-aligned rectangular area such that lw --~ n 2. This requires n 2 write operations on a pixel based 
display system thus requiring time rp. n 2. On a rectangular area filling display system, this operation 
is a primitive and as such requires one display write operation and requires time ra which can be considered 
as an O(n 2) improvement. Filling times for convex polygons are more difficult to compare since polygons 
can differ in both area and shape. In general though, convex polygons can be thought of as having a minimum 
bounding box of A~ X Av pixels. On a pixel based display system the worst case polygon fill will require 
Az AV write operations whereas on a rectan-gular area filling display system, the worst case behavior 
is min(Ax, Av) write operations. Such performance can be achieved by using ordinary scan-line algorithms. 
 Consider a distribution in shape of polygons of area n 2 such that the shorter dimension of the minimum 
bounding boxes of these polygons is uniformly distributed between 0 and n. Pixel based display system 
always require n 2 display write operations while a rectangular area filling display system requires 
at the worst only n display write operations, an order n improvement. The mean relative performance, 
P, is: : r v n 2 __ rp n 2 r~ ± fo min(/, n2/l) dl ran! fo l dl r if2" r-" --2n rp an 2 0 a2 Non-convex 
polygons can be decomposed into convex polygons. In practice convex and non-convex polygons are often 
fractured into axis-aligned trapezoids. The time re- quired to fill polygons that have be fractured into 
trapezoids still shows an order n performance improvement over pixel based systems since an order n performance 
improvement is made for each trapezoid. In summary, algorithms can be found that make rec-tangular area 
filling display systems perform better than pixel based display systems at line drawing, box filling 
and general polygon filling operations. Display Memory System Architecture Pixel based raster scan display 
systems are common place because of the availability of high density random access memories. Unfortunately, 
RAM manufacturers are striving to build larger and larger n X 1 RAMs without considering that other RAM 
organizations might yield vi- able products. It is because of this n X 1 memory or-ganization that we 
have pixel based display systems. In this paper, another RAM organization is presented that en- ables 
rectangular area filling display systems to be built. Furthermore since this RAM design is very similar 
to cur-rent RAMs being produced today, it should be able to be produced as a high density part at low 
cost. Conventional pixel based display systems are built up of n X 1 memory circuits that are organized 
into a m X z linear array where m is usually some integral multiple of n and z is the the number of bits 
per pixel. Figure 1 il- lustrates how such a system is built up from n X 1 RAMs. A typical display system 
might have m ~ 1,000,000 and z --24. The important thing to realize is that this or-ganization imposes 
a restriction that only one pixel may be modified in any memory cycle. That is, out of 24,000, 000 bits 
of RAM only 24 may be altered at any one time. This is the bottleneck that conventional RAM devices impose 
upon the graphics system designer. There are ways to make this bottleneck appear less severe but usually 
this means that the designer has built ADDRESS 40:19 n-l> SELM- I I I RAM RAM RAM M,O M,I M,Z-I RESS 
In: w o O rm SELI I i I im_,)i ~ _- RAM RAM RAM I,O I,I I,Z-I SEL 0 I I I RAM RAM RAM 0,0 O, I 3,Z-I 
 a system that allows more than one pixel to be accessed at a time. For example, Sutherland [Gupta 1981] 
has built a display system that allows 64 neighboring pixels to be modified at once and although it is 
true that the system performance may be 64 times better, such a system is only altering 24.64 bits out 
of 24,000,000 which is still a small fraction. Figure 2 illustrates why the conventional RAM or-ganization 
causes this bottleneck. A static RAM contains a rectangular array of memory cells each similar to the 
cell shown in Figure 3. A single cell is selected by first select- ing a row of the memory array and 
then selecting a bit within that row. Both selections are made by the binary decoders labeled row select 
and column select. The RAM cell works by writing its contents on the bit lines when selected. During 
a read cycle, sense amplifiers detect the state of the RAM cell by detecting voltage changes on the bit 
lines. During a write cycle, one of the bit lines is pulled to ground and the RAM cell is set to a 1 
or a 0. Since n X 1 RAMs use binary address decoders and thus hide all but one memory element from the 
outside ROW W--.J--W-- 4~ x ,/h- ADDRESS MEMORY ARRAY O--   IIIIIIIIIIIIIIII I SENSE AMPS I I I IIII 
IIIIII I ] II I COLUMN SELECT I DATA I COLUMN ADDRESS Figure 2: Floorplau of a typical ~ by 1 static 
RAM. R0W --L DO DI DZ-I Figure 1: Composition of an m by z memory from a by 1 BIT BIT memory chips. Figure 
3: A six transistor static memory cell. world, the only practical way to make a large improvement in 
display performance is to design a RAM chip expressly for use as a graphics display memory. The previous 
section illustrated that rectangular area filling is a desirable primi- tive operation. Because we want 
to deal with rectangles, it seems that the display memory system ought to be able to address rectangles 
in a natural way. Another desirable feature would be if the memory organization could decrease the video 
refresh overhead that is usually associated with high resolution displays. It is often the case that 
the computer or graphics processor can only get every other memory cycle or may not even be able to access 
the video memory during active scan periods. This means that video refreshing often requires between 
fifty and eighty percent of the display bandwidth. Double buffering can be used to decrease the video 
refreshing overhead but it is an expen-sive solution. Addressing rectangles within a rectangular memory 
array can be easily accomplished by providing the x and y coordinates of the lower left corner and upper 
right corner points. The problem is to design a decoding scheme that is cascadable, that is, one that 
will allow an arbitrary size rectangular array to be built. As was pointed out earlier, both conventional 
RAMs and display memory systems use binary address decoders and the address decoding process can be though 
of as a hierarchy of binary decoders. Such a simple cascading scheme is desirable and for this reason, 
the decoders within our rectangular area filling RAM ought to be the same type of decoder that is used 
to but together an array of these RAM chips. In order to provide two dimensional addressability, the 
memory array within the RAM will have to be modified slightly. First, notice that the static RAM of Figure 
2 has only row select signal lines running across the array. The rectangular area filling RAM will need 
column select signal lines running across the memory array also. Of course, the basic RAM cell of Figure 
3 will have to be modified slightly to make use of these column select lines. This involves the addition 
of two pass transistors as is illustrated in Figure 4. The operation of an array of such memory cells 
is as follows. A memory cell is selected if its column and row select lines are both 1. When selected, 
the memory cell behaves as it did before, forcing its state onto the bit lines. Notice however, that 
if a band of column select lines and a band of row select lines are set to 1, a rectangle of memory cells 
are simultaneously selected. These cells may be written in the same way that one memory cell is written 
in a conventional RAM. Reading poses a problem in that cells in different selected rows may have had 
different values prior to being selected and for this reason, this architecture only allows single rows 
to be read. This memory array architecture provides us with a way of implementing a rectangular area 
filling RAM only requiring us to design a banded address decoder. In addition, the extra wiring channels 
and pass ROW SELECT ~-]  ~ V?C  BrTCOLUMN SELECT BiT Figure 4: Static memory cell with row and column 
select capability. transistors have not resulted in an extremely larger memory cell, enabling the implementation 
of large memory arrays on a single die. The design of a banded address decoder is straight- forward. 
A banded decoder requires two address, a lower address and a upper address, and selects the band of out- 
puts between and inclusive of the two addresses. Assume that one can build a n bit decoder that takes 
an address i where i < 2 n and selects all outputs in the range [i... 2n]. Call that the lower address 
decoder. Similarly construct another decoder that selects all outputs in the range [0... i] and call 
that the upper address decoder. The logical AND of these two decoders selects the band [la... ua] where 
la is the lower address and ua is the upper address. Either type of decoder can be constructed by building 
a binary decoder and adding a propagation chain to it as is illustrated in Figure 5. A propagation tree 
can be used to reduce the propagation time from O(n) to O(log n). A complete band-ed decoder is illustrated 
in Figure 6. Notice that it provides select and carry inputs that are used in the cascading of the decoders. 
Decreasing video refresh bandwidth requirements is also rather straightforward. If we construct a display 
mem- ory out of these RAM chips such that rows in the memory o SELECT 0 > SELECT I SELECT 2 SELECT 5 
 'C> D i CARRY IN Figure 5: Binary decoder with a propagation chain. Computer Graphics Volume 16, Number 
3 July 1982 o- o- 1~55 I#~g u CARRY rN --SELECT 0 --SELECTq --SELECT 2 )--SELECT L CARRY IN ; t J LOWER 
ADDRESS DECODER UPPER ADDRESS DECODER Figure 6:A Complete 2 bit banded decoder implemented with a propagation 
tree. array map onto scan lines on the display then a whole scan line of video refresh data can be read 
in one memory cycle. Since we don't have the pins to bring all of the bits out of the chip in parallel, 
read data is shifted out serially, as is appropriate for video refreshing and only requires the addition 
of a shift register to each RAM chip. Adding a latch between the RAM array and the shift register makes 
the device easier to design into a system since the one video refresh cycle per scan-line time doesn't 
have to come at any time tightly synchronized with the video refresh operation but has to occur in a 
window of one scan-line time. Figure 7 illustrates the complete architecure of a rec-tangular area filling 
RAM. The memory array is surrounded on two sides by the row and column banded decoders. The decoder strobe 
input is used to gate the outputs of the decoders. The bottom of the memory array is connected to sense 
amplifiers which detect the small voltage variations on the bit lines during read operations and convert 
these signals to a logic level representing the value of the stored bit. The outputs of the sense amplifiers 
are inputs to a 64-bit wide latch. When data is read, it is loaded into the latch and, sometime later, 
the outputs of the latch are loaded into a 64-bit shift register and shifted off the chip to provide 
video data. The final component is labeled write drive circuitry and consists of the logic necessary 
to pull the proper bit line to ground during write cycles. Figure 8 illustrates how these RAM chips may 
be com- posed to form a larger display memory system. This figure illustrates a single plane and thus 
for a full display system, the RAM chips must be arranged in a three-dimensional array instead of a two-dimensional 
array as in the case of ordinary systems. Decoding for the array, is done with banded decoders. A decoder 
output is used for the select input while neighboring decoder outputs are used for the two carry inputs. 
A multiplexer is needed to select the proper serial video data stream. It should be noted that the rectangular 
area filling RAM can be considered a "processor per pixel" display sys- tem. Each RAM cell can be considered 
as a processor that LLX SELECT I~ --URX SELECT LLX CARRY [ --URX CARRY  LLXADORESSB 1 IF-- URX ADDRESS 
IIIIlilllllllllllII LLY SELECT~ E -- w -- LLY CARRY ~ Q -- O -- LLY ADDRESS~ o -- w -- D -- a --MEMORY 
ARRAY w -- Z -- URY SELECT~ < URY CARRY ~ URY ADDRESS'----I--  IIIIllllllllll]llll SENSE AMPS I Ilrlllll{[lllllllll 
READ LATCH I"-LATCH CLOCK SHIFT REGISTER - I -DATA OUT DATA IN " III]111 IIIIII Ill lip WRITE DATA --WRITE 
DRIVE CIRCUlTR~ Figure 7: Rectangular Area Filling RAM architecture. LLY URY 0-- uJ £3 o URY-- 
o uJ a VIDEO Q DATA tu a LLY--Z ROW SELECT O OUT LLX URX 0----0 I X BANDED DECODER I I I LLX URX 
Figure 8: Composition of a display memory system out of Rectangulax Area Filling RAMs. sets its state 
bit if both column and row selects are present. In this sense, on the "processor per pixel" spectrum, 
the rec- tangular area filling RAM belongs at the minimal processor end. In general, schemes that utilize 
minimal processors will be able to place larger numbers of these processors on a chip, making it more 
feasible to construct systems out of such devices. Implementation Refinements A 4096 device rectangular 
area filling RAM is cur- rently being implemented in n-MOS. Several refinements have been made to the 
architecture to reduce the circuitry, power dissipation and the package pin count. One of the most notable 
changes is that the RAM is being implemented as a pseudo-static RAM instead of  Computer Graphics Volume 
16, Number 3 July 1982 as a static RAM. This change changes the memory cell CAS / from an eight transistor 
circuit to the six transistor circuit 64 illustrated in Figure 9. Because of the reduction in the STROBE 
,I BOOTSTRAPPED    BUFFERS/ number of transistors and the elimination of a power line, the pseudo-static 
RAM cell is much smaller than the static cell. Furthermore since ordinary static RAMs use six tran- sistor 
cells, comparable size devices should be achievable. Currently 16K static RAMs are available and many 
64K devices have been reported in the literature [Ohzone 1980, Wada 1981]. Pseudo-static implies that 
the device is static in one mode of operation and dynamic in another mode. This means that the device 
must be periodically refreshed but in a display memory, refreshing occurs naturally through the video 
refresh accesses and thus imposes no additional overhead. In certain applications such as pixel-replicating 
zoom, the display memory may need to be refreshed and can be refreshed through explicit refresh cycles. 
Pseudo-static devices also draw less current than static devices so the device power dissipation is expected 
to be much less. In addition, current surges that result from large numbers of bits changing in one memory 
cycle can be easily controlled. In pseudo-static RAMs, the bit cells behave as capacitors during write 
cycles and not as flip-flops, allowing the rate at which the bits change states to be determined by the 
series resistance of the pullup transistors used in the write circuitry. By adjusting the series resistance 
of these transistors, the amplitude of current surges can be limited to an acceptable value. In an effort 
to reduce the number of pins required by the RAM, 12 address pins are used and are time multiplexed to 
read in all 24 bits. This time multiplexing has the side affect of allowing one of the two banded address 
decoders in Figure 7 to be discarded since a single banded decoder can be time multiplexed and the outputs 
latched at the appropriate times by the select driver circuitry. These changes are illustrated in Figure 
10. Conclusions The performance improvements that rectangular area filling display systems are able to 
provide justifies their im- plementation. Architectures such as this one, which can be COLUMN SELECT 
ROW __ SELECT L L BIT BIT Figure 9: A psuedo-static RAM cell with row and column selects. LOWER ~ i ADDRESS~ 
N ! UPPER ADDRESS ~-64 x 64 SELECT ~. PSEUDO-STATIC CARRY ~ I up ~'~ ~ I--- I MEMORY ARRAY CARRY 
  DOWN~ 0 I gl -V  ~ 64 RAS PCH --~ PRECHARGE I ~ 64 WRtTE~ t WRITE CIRCUITRY 1 WRITE DATA -~ 64 READ 
~ SENSE AMPLIFIERS I¢64 STORE-~ LATCH I VIDEO 64 I = VtDEOSHIFT REGISTERDATA IN DATA OUT T T CLOCK LOAD 
Figure 10: Floorplan of the 4090 bit Rectangular Area Filling RAM currently being implemented. described 
as "processor per pixel" architectures, necessitate implementations in silicon and since these architectures 
can provide notable improvements in system performance, this is an area where manufacturers should concentrate 
their efforts. The system described here does have some drawbacks. While it can fill polygons fairly 
fast, it is not significantly faster at filling shaded polygons since shading usually im- plies that 
neighboring pixels have slightly different values. For the same reason, the architecture does not aid 
in the rendition of anti-aliased lines. However, both shading and anti-aliasing can be performed by this 
system since the rec- tangle size can be set to 1 X 1 causing the system to behave like an ordinary pixel 
system. Further work might concentrate on a scan-line filling RAM. The previous analysis indicates that 
scan-line fi!ling can offer significant performance improvements but will not be as good as rectangular 
area filling since scan-lines are always horizontal. Shading algorithms might be easyily implemented 
on a scan-line fillingRAM. Both scan-line and rectangular area filling RAMs allow scan-lines to be filled 
and therefore can be used in implementing real-time visible surface systems using a Watkins [Watkins 
1970] algorithm. Acknowledgments The author would like thank Jim Kajiya and Carver Mead for their many 
helpful discussions. In addition, he would also like to acknowledge conversations with Henry Computer 
Graphics Fuchs. This research was sponsored by the Defense Ad- vanced Research Project Agency, ARPA Order 
number 3771, and monitored by the Office of Naval Research under contract number N00014-79-C-0597.  
References 1. Bresenham, J.E. "Algorithm for computer control of a digital plotter" IBM Systems Journal, 
Vol. 4, No. 1, 1965, pp. 25-30. 2. Clark, J.H. "Structuring a VLSI System Architecture" LAMBDA, Vol. 
1, No. 2, 1980, pp. 25-30. 3. Henry Fuchs and John Poulton "PIXEL-PLANES: A VLSI-Oriented Design for 
a Raster Graphics Engine" VLSI Design, Vol. 2, No. 3, 1981, pp. 20-28. 4. Satish Gupta, Robert F. Sproull 
and Ivan E. Sutherland "A VLSI Architecture for Updating Raster-Scan Dis- plays" Computer Graphics, Vol. 
15, No. 3, 1981, pp. 71-78. 5. Bart Locanthi "Object Oriented Raster Displays" Pro- ceedings of Caltech 
Conference on Very Large Scale Integration, 1979, pp. 215-225. 6. Takashi Ohzone, Juro Yasui, Takeshi 
Ishihara and Shiro Horiuchi "An 8KX8 Bit Static MOS RAM Fabricated by n-MOS/n-Well CMOS Technology" IEEE 
Journal of Solld-State Circuits, Vol. SC-15, No. 5, October 1980, pp. 854-861. 7. Toshio Wada, Hiroshi 
Yamanaka, Mitsuru Sakamoto, Hirohiko Yamamoto and Shigeki Matsue "A 16 DIP, 64 kbit, Static MOS-RAM" 
IEEE Journal of Solid- State Circuits, Vol. SC-10, No. 5, October 1981, pp. 488-491. 8. Gary S. Watkins 
"A real-time visible surface algorithm" University of Utah Computer Science Department UTECH-CSc-70-101, 
June 1970.  Volume 16, Number 3 July 1982  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801275</article_id>
		<sort_key>155</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>24</seq_no>
		<title><![CDATA[Approaches to solid modelling (Panel Session)]]></title>
		<page_from>155</page_from>
		<doi_number>10.1145/800064.801275</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801275</url>
		<abstract>
			<par><![CDATA[<p>Solid modelling systems in current use are based on approaches that are fundamentally different. The panelists will discuss the advantages and disadvantages of the different approaches, especially from the user's point of view.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332047</person_id>
				<author_profile_id><![CDATA[81100445698]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Malin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MAGI]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330267</person_id>
				<author_profile_id><![CDATA[81100405005]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bliss]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ford Motor Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334383</person_id>
				<author_profile_id><![CDATA[81100274894]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Carmody]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IBM Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14159273</person_id>
				<author_profile_id><![CDATA[81332507110]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Consultant]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332344</person_id>
				<author_profile_id><![CDATA[81100339443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schloessel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Applicon, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331574</person_id>
				<author_profile_id><![CDATA[81100278235]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Swarbrick]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[General Electric Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801276</article_id>
		<sort_key>157</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>25</seq_no>
		<title><![CDATA[Path specification and path coherence]]></title>
		<page_from>157</page_from>
		<page_to>166</page_to>
		<doi_number>10.1145/800064.801276</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801276</url>
		<abstract>
			<par><![CDATA[<p>This paper presents an interactive method for specifying a path in space and time through a three-dimensional environment. A sequence is generated by showing the series of views along the path. The sequence is previewed on a vector scope, and after it is interactively refined, each frame is rendered on a raster device. The path is represented by a B-spline to provide smooth, continuous motion. The timing along the path is also defined by a B-spline so that changes in velocity are smooth. The use of &#8220;path coherence&#8221; is introduced. The utilization of the available data from the a priori temporal and spatial path definition holds great promise for frame to frame coherence. The path coherence can be used to reduce the number of polygons which need to be considered in a viewed environment. This reduction makes the previewing of complex environments appear less cluttered. Furthermore, the computational expense of the culling and sorting operations in the visible line/surface determination is reduced. One sample usage of this is a tree-structured partitioned environment where the priority ordering of the environment must be changed only when the path crosses a partition boundary.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Viewing algorithms</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P331852</person_id>
				<author_profile_id><![CDATA[81100553189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kim]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Shelley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baecker, R., "Picture-driven Animation," Spring Joint Computer Conference, 1969, AFIPS Press.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Clark, J., "Hierarchical Geometric Models for Visible Surface Algorithms", Communications of the ACM, volume 19, number 10, Oct., 1976.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cook, R., "Polygonal Database User's Guide," Program of Computer Graphics, Cornell University, Ithaca. New York, unpublished, 1981.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cosman, M., Schumacker, R., "System Strategies to Optimize CIG Image Content," Proceedings Image II Conference, Scottsdale, Arizona, June, 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807507</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Feibush, E., Levoy, M., Cook, R., "Synthetic Texturing Using Digital Filters," SIGGRAPH/80 Conference Proceedings, Seattle, Wash., July, 1980.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Kedem, Z., Naylor, B., "On Visible Surface Generation by A Priori Tree Stuctures", SIGGRAPH/80 Conference Proceedings, Seattle, Wash., July, 1980.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806788</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hubschman, H., Zucker, S., "Frame To Frame Coherence and the Hidden Surface Computation: Constraints for a Convex World", SIGGRAPH/81 Conference Proceedings, Dallas, Texas, August, 1981.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., "FILM ANIMATION: Presenting Both Historical and Technical Aspects of the Subject and Introducing: Nereus a Computer-Aided Key-Frame Animation System", B.Arch. Thesis, Cornell University, Ithaca, New York, 1976.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Newman, W., Sproull, R., Principles of Interactive Computer Graphics, McGraw-Hill Book Co., New York, New York, 1979.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Robertz, W., "Graphical Input System for Computer-Aided Architectural Design", Proceedings CAD 80, IPC Science and Technology Press, Guildford, UK, March 1980.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Schumacker, R., "A New Visual System Architecture," Proceedings Second Interservice/Industry Training Equipment Conference, Salt Lake City, Utah, November, 1980.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Sutherland,I., Sproull, R., and Schumacker, R., "A Characterization of Ten Hidden-Surface Algorithms", Computing Surveys, volume 6, number 1, March 1974.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806813</ref_obj_id>
				<ref_obj_pid>800224</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Wallace, B., "Merging and Transformation of Raster Images for Cartoon Animation," SIGGRAPH/81 Conference Proceedings, Dallas, Texas, August, 1981.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Wein, M., Burtnyk, L., "Computer Generated Key-frame Animation," Journal SMPTE, March, 1971.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807386</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Wein, M., Tanner, P., Bechthold, G., Burtnyk, N., "Hidden Line Removal for Vector Graphics", SIGGRAPH/78 Conference Proceedings, Atlanta, Ga., August, 1978.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Wild, E.C., Rougelot, R.S., Schumacker, R.A., "Computing Full Color Perspective Images," Electronics Laboratory, Syracuse, New York, 1971.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359848</ref_obj_id>
				<ref_obj_pid>359842</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Wu, S.C., Abel, J., Greenberg, D., "An Interactive Computer Graphics Approach to Surface Representation", Communications of the ACM, volume 20, number 10, October, 1977.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 16, Number 3 July 1982 PATH SPECIFICATION and PATH COHER~CE Kim L. Shelley 
and Donald P. Greenberg Program of Computer Graphics Cornell University Ithaca. New York 14853  ABSTRACT 
 This paper presents an interactive method for specifying a path in space and time through a three-dimensional 
environment. A sequence is generated by showing the series of views along the path. The sequence is previewed 
on a vector scope, and after it is interactively refined, each frame is rendered on a raster device. 
The path is represented by a B-spline to provide smooth, continuous motion. The timing along the path 
is also defined by a B-spline so that changes in velocity are smooth. The use of "pabh coherence" is 
introduced. The utilization of the available data from the a priori temporal and spatial path definition 
holds great promise for frame to frame coherence. The path coherence can be used to reduce the number 
of polygons which need to be considered in a viewed environment. This reduction makes the previewing 
of complex environments appear less cluttered. Furthermore, the computational expense of the culling 
and sorting operations in the visible line/surface determination is reduced. One sample usage of this 
is a tree-structured partitioned environment where the priority ordering of the environment must be changed 
only when the path crosses s partition boundary.  CR Categories and Subject Descriptors: 1.3.3 [Computer 
Graphics]: Picture/Image Generation -display algorithms; viewing algorithms; 1.3.6 [Computer Graphics]: 
Methodology and Techniques -interaction techniques; 1.3.7 [Comput@r Graphics]: Three-Dimensional Graphics 
and Realism -visible line/surface algorithms  INTRODUCTION To properly evaluate space in terms of scale 
and the experience of moving through that space, one Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. @ 1982 ACM 0-89791-076-1/82/007/0157 $00.75 must deal with 
a four dimensional problem: movement through three-space over time. Thus it is necessary to specify both 
the positional and temporal characteristics of a path. For proper simulations, the spatial and time 
curves should guarantee continuous motion. A predefined path determines a sequence of dynamic images 
by specifying a sequence of observer positions. Therefore one can use a type of frame to frame coherence 
called "path coherence", which can reduce the computational expense of sorting and culling in visible 
line/surface determination. This paper discusses a method for interactive path definition and the use 
of path coherence in a polygonal environment.  PATH DEFINITION A well known method for defining a linear 
path and the rate of motion along that path is a computer-aided animation technique known as key-frame 
in-betweening ([8, 14]). This technique allows the user to specify the rate of motion by defining the 
number of frames to generate between each pair of key frames. It is also possible to specify a two-dimensional 
spatial path for the key-frame interpolation. The method of path definition used in %his research allows 
the user to interactively draw the three-dimensional path in the three-dimensional environment, using 
the technique of inking. This method allows the user to have complete control of exactly where each point 
of the path is placed in three space. The rate of motion is interactively specified by defining a velocity 
for each point on the path with a three dimensional "timing curve". The timing curve provides for continuous 
changes in velocity so that one does not see sudden changes in velocity as can be discerned in key-frame 
techniques when the sequence passes through a key-frame.  Positioning the Path The path is interactively 
defined by inking on the plan view of the environment. Initially the path is defined by a two dimensional 
array of X,Y coordinates that are returned from a tablet. The definition of the path in this manner is 
limited   Computer Graphics Volume 16, Number 3 July 1982 oZ < 5° t= >-b-(J O J LU > 10° / TIMING CURVE 
W L/ Z ,< co [Z] 200 16. 12° 80 / 5.a. Zero 0° 0=0 0°8 1°5 2=2 3°0 3°7 o°0 0°8 1°5 2.2 3o0 3,7 0°0 0°8 
t°5 2°2 3°0 3°7 TIME TIME TIME Z 5= >'-}--. lO. J J ~ ~° 2> 4° < 1° 2. 5.b. Positive Oo O° 0°0 0°7 I=3 
I°9 2°S 0.0 0°7 t°3 1°9 2°6 TIME TIME bJ 12. L) Z < tO. CO 7. Eh 5. 2./ 0° 0.0 0.7 1°3 I.9 2.6 
TIME Z 5° >-ii° b] 39= ~_< 32=  L] J O0 24° j k~ 7° i i > 16= r_3 5° f,D , < 0°0 i°I 2°3 3°4 4°S 
5=7 8° 5.e. Negative -3 ........... o° O° 0°0 l.l 2°3 304 4°6 5°7 0=0 l°L 2°3 3°4 4°6 5°7 TIME TIME 
 Figure 5. Acceleration, velocity, and distance curves used to match acceleration at the timing curve. 
 coherence algorithms. For example, the new Evans and Sutherland CT-5 system uses frame-to-frame PATH 
$OHERENCE coherence in several ways to remove extraneous dynamic images by specifying a sequence of 
portions of the database prior to image observer positions. Therefore a type of computation ([4, 11]). 
Active data is maintained frame-to-frame coherence called "path coherence" in buffer memories after culling 
operations are can be used which reduces the computational performed. These culling operations include 
expense of culling and sorting in visible field-of-view testing (clipping to the viewer's line/surface 
determination. frustum), level of detail thresholds, removal of bac~-facing polygons and image size 
testing, and The predefined path determines a sequence of rely heavily on frame-to-frame coherence. 
respect to frame-to-frame coherence. Hubschman and Zucker described one restricted method for predicting 
visible scene elements and occlusion Back-face Removal. relationships for static environments composed 
of convex polyhedra ([7]). Specialized hardware in Back-facing polygons are removed from the list of 
computer image generation equipment, such as polygons to be displayed. The determination of flight simulators, 
also rely on frame-to-frame which polygons are to be culled is normally Very little information has 
been published with achieved by taking the dot product of the view vector and the polygon normal. The 
operation is thus linear with the number of polygons. For sequences through static environments, this 
back-face polygon culling must be performed for each successive change in the line of sight (view vector) 
relative to the object environment. Since the line of sight changes for each observer position, or for 
each change in view direction, in general, the culling operation must be performed for each frame in 
a motion sequence. The use of path coherence can greatly reduce the computational effort of the cumulative 
culling operations. There are two cases to consider for culling; one occurs during the preliminary planning 
of a sequence, and the other occurs during the final previewing or rendering stage. When initially planning 
a sequence, the first step is to specify the path through the environment. At this stage, the direction 
of the line of sight for each observer position is unspecified. In order to dynamically preview test 
sequences, it is useful to cull from the environment at least all of those polygons which can never be 
seen from the path, regardless of the view direction finally specified. This preliminary culling procedure 
reduces the clutter of complex environments when viewed on dynamic vector refresh displays without hidden 
line removal. This rough culling operation, which is not an exact method, is accomplished as follows. 
 Since the spatial definition of the path is known, a bounding (min-max) box enclosing the path is automatically 
generated. Any view from the path must obviously also be taken from within the Dounds of this box. Four 
"critical view vectors", whose origins are at the corners of the box are also automatically defined. 
The direction of these critical sight lines are sequentially ninety degrees apart from one another (Figure 
6). For each critical view vector, an orthographic window is used to remove back-facing polygons, and 
the front-facing polygons are clipped to the hither plane only. This effectively removes all polygons 
behind the observer without specifying a frustum of vision. The remaining group of front-facing polygons, 
called a "critical group" is stored for each critical view vector. The  vector I vl .... ctor 21 / 
 union of these four critical groups is determined, and during the preliminary sequence planning, this 
union is displayed for all arbitrary viewing angles along the path. Note that because of the unique selection 
of the critical view vectors, polygons within the bounding box are not clipped or culled. For the final 
previewing and rendering stage, path coherence can be used to substantially improve on the rough approximation 
obtained for the preliminary planning. At this stage, all of the view directions along the path are known. 
 Specification of the view vectors occurs in two ways. The default condition is to assign view vectors 
tangent to the spatial path. The user can also interactively specify any view direction at any position 
on the path. This option provides the opportunity to simulate an observer walking down a path, stopping, 
turning around and tilting one's view, and then continuing along the path. In this mode. all critical 
view vectors must be interactively specified (Figure 7.a). An alternative method, which has not been 
implemented, would be to automatically extract critical view vectors at the start and end points, all 
local maximum and minimum positions (first derivative equals zero or changes sign), or points of contraflexure 
(second derivative equals zero or changes sign) (Figure 7.b ). Once the view vectors for the entire 
sequence have been defined, path coherence can be used very effectively to reduce the culling expense, 
and thus improve the efficiency of the visible surface computations. A segment is defined as the portion 
of a path between two critical view vectors. The critical view vectors bound the interpolated view vectors 
for all segments of the path. Thus, for each path segment it is only necessary to consider those polygons 
remaining after the culling and hither plane clipping operations have been performed at Figure 7.a. 
Critical view vectors interactively specified. Figure 6. Bounding (min-max) box and critical view Figure 
7.b. Critical view vectors automatically vectors for a path. specified. ~JJ JJ _j.~ J ...a _) > _) Figure 
8.a. Path with two critical view vectors Figure 8.b. Critical group for critical view defined. vector 
I. Figure 8.e. Critical group for critical view vector 2. the bounding positions. For continuity, a 
critical group is created for each segment. The group consists of the union of the remaining polygons 
from each the the two bounding views. When a critical view point is defined, that point is flagged 
in a bit vector. This bit vector is of the same length as the array of points that define the path. Therefore, 
there is a bit which corresponds to each point on the path. To begin the sequence, the first critical 
group on the list is defined to be the group of polygons for the first segment along the path. As the 
sequence is being shown, the bit vector is checked for each frame. If the bit vector is "on" it represents 
the first frame in the next segment, and the group of polygons from which views are taken along the path 
is re-defined to be the next critical group on the list. This process is followed until the sequence 
is complete. Sorting In any visible line/surface algorithm, sorting is Figure 8.d. Union of critical 
groups I and 2. the major computational expense. This expense can be greatly reduced using path coherence. 
 Any environment can be partitioned by separating planes, as set forth in the Schumacker algorithm ([12]). 
These separating planes are a property of the environment. Any environment can be divided into partitioned 
areas, but the number of separating planes required depends on the complexity of the environment (Figure 
9.a). The separating planes are static in a static environment but must be dynamic in a dynamic environment. 
 A binary tree can be built which defines the separating planes and the clusters on either side of them 
(see Figure 9.b). "A cluster," as defined by Sutherland et. ai.,([12]) in their description of the Schumacker 
algorithm, "is a collection of faces that can be assigned a fixed set of priority numbers which, after 
back edges are removed, provide correct priority from any viewpoint." If the environment contains concave 
polyhedra, then the priority of polygons within the polyhedra (clusters) can be determined using Schumacker's 
face priority algorithm ([16]). However it is Computer Graphics Volume 16, Number 3 July 1982 t f I 
i " t~/;\ F ; ": t f I,, i I ,,i \ 5-- t f i .... i t I . i. I i! ¸ I I I I I I t t I t FI~ 9.a. Plan 
view separating planes. of an environment with Figure 9.b. Binary separating planes environment. tree 
and representing polyhedra of the the significant to note that the procedure can be 6. Push polygons 
of the leaf onto the generalized to include the results of a polygon stack and mark the parent node indicating 
visible surface algorithm if the polygons can be that this child has been traversed. output in sorted 
order. 7. Move up tree, resetting CURNODE and In this implementation the branches of each node mark 
the parent node indicating that this of the binary tree point to either a cluster or child has been traversed 
until a node is another separating plane. Leaf nodes contain only found that has a child that has not 
been clusters. Each separating plane has a "true" traversed. (positive normal) side and a "false" side; 
therefore each internal node has a true child and 8. If no node is found that has a child a false child. 
The subtree of the true child which has not been traversed, then all contains all clusters on the true 
side of the leaf nodes have been reached and the top separating plane of the node and likewise for the 
of the stack contains the cluster with subtree of the false child. lowest priority. For any viewpoint 
in the environment, the tree can Thus a priority of the partitioned areas is be traversed to find the 
partitioned area with established for the viewpoint. highest priority. The traversal algorithm is as 
follows: The depth and shape of the binary tree depend on the order of insertion of the separating planes 
 I. Initialization. The current node. and where separating planes are placed in the CURNODE. is the node 
of the binary tree environment in relation to one another. Two that is currently being considered. binary 
trees of different shapes and depths can CURNODE is initially the root node. represent the same set of 
separating planes in an environment and produce the same priority 2. Do steps 3 through 7 while at 
least ordering. one child of CURNODE has not been traversed. When a path is defined in the partitioned 
 environment, the cluster priority can be  3. If one child of CURNODE has been determined for the first 
point on the path. This traversed, set CURNODE equal to the other priority will not change until the 
path crosses a child of CURNODE. separating plane into another partitioned area. A  new priority ordering 
is determined for the path  4. Repeat step 5 until a leaf is in this partitioned area and is valid until 
 reached. another separating plane is crossed. Thus the priority must be reordered only when the path 
 5. Test view point with plane equation crosses a separating plane. of CURNODE. Set CURNODE to child 
indicated by result. It is important to note here a vital distinction.  When building the binary tree 
and determining what is on each side of a separating plane, each   SORT,NG separating plane is defined 
to be bounded by all previously defined separating planes. Thus in Figure 9.a., separating plane 2 is 
bounded by separating plane I. However when determining when the path crosses a separating plane, the 
semi-infinite separating plane through the environment (shown by the dashed lines in Figure 9.a.) is 
used. Sorting is done only once for the entire environment: when the binary tree is built. The number 
of times the priority ordering must be recomputed for a sequence of frames along a path depends on the 
number of separating planes crossed (which depends on the complexity of the environment) and characteristics 
of the path such as its length, position and shape. The graph in Figure 10 shows the amount of CPU time 
required for creating the binary tree and for determining the priority ordering for an environment consisting 
of twenty-two separating planes and twenty-three clusters. The path for this sequence of frames crosses 
five separating planes. The initial peak is the creation of the binary tree; the other, smaller, peaks 
are when the path crosses a separating plane and a new priority ordering is established. Rendering 
If a painter's algorithm were used for rendering, the polygons would be rendered ("painted") from lowest 
priority to highest priority. The output from the polygon renderer could be sent directly to the screen. 
In this scheme, each scan line segment would be antialiased into the current background on the screen. 
As polygons with higher priority are rendered, they would tend to obscure the lower priority polygons. 
 There are two disadvantages to this scheme of rendering. First, much time can be wasted rendering polygons 
that will subsequently be covered up (painted over) by higher priority polygons. This is especially true 
in environments with great depth complexity. A second disadvantage in rendering from back to front (painting) 
is that erroneous colors may appear in the completed image. This is a result of each polygon being antialiased 
into the current background rather than the final display. For correct rendering of antialiased polygons 
it is necessary to render from highest priority to lowest priority (front to back). One effective way 
to render from highest to lowest priority, without antialiasing, is to use a l-bit mask which contains 
an element corresponding to each pixel on the display screen ([15]). The visibility of each pixel is 
determined by the contents of the mask. If the corresponding mask bit is set, it indicates that a nearer, 
and hence blocking polygon covers the pixel. To employ this technique with antialiasing, instead of 
using a l-bit mask, a mask of floating point numbers can be used. (This is a similar technique to the 
one used by Wallace in cel E a. o ~'~.IORIT ¥ ORDERING H~ "~ [I __ ~~-~CUL LING e.rCLIP PING , , 
i  frame number Figure 10. CPU time in milliseconds for sorting and priority ordering operations for 
a sequence of frames. The initial peak is the creation of the binary tree; other, smaller, peaks are 
when the path crosses a separating plane and a new priority ordering is established. merging for cartoon 
animation [13].) The values in this mask range from 0.0 to 1.0. A value of 0.0 for a pixel means that 
no polygons contribute color to the pixel. A value of 1.0 means that all polygons contributing color 
to the pixel have been accounted for, thus subsequent values for the pixel are ignored. Thus, for each 
pixel, two values are stored in a virtual frame buffer. One is the color value which is actually an 
RGB triplet. The other is the mask which contains the cumulative percentage for that pixel, and ranges 
from 0.0 to 1.0. After all polygons have been sent through the polygon renderer, the contents of the 
virtual frame buffer are rendered into the background. DISPLAYING THE SEOUENCE A pilot program has 
been implemented to test these theories. The sequence of moving along the path over time can be dynamically 
previewed on a vector display device so that refinements to the sequence can be made interactively and 
tested quickly. An example previewed frame (with back planes culled) is shown is Figure 11.a. Each frame 
of the refined sequence is then rendered on a raster device which can then be output to film or video 
tape. Two examples of rendering using antialiased polygons are shown in Figures 11.b and 12. CONCLUSIONS 
 Dynamic previewing of a sequence on a black and white vector display before it is rendered is valuable 
both in choosing the correct sequence and in reducing computation time. Evaluating space and the experience 
of moving through that space is a four dimensional problem. The solution presented uses a B-spline representation 
to define both the spatial curve and the timing curve. B-splines are used to guarantee the smooth and 
continuous motion through the space.  [1] Baecker, R., "Picture-driven Animation," Spring Joint Computer 
Conference. 1969, AFIPS Press. [2] Clark, J., "Hierarchical Geometric Models for Visible Surface Algorithms", 
Communications of the ACM, volume 19, number 10, Oct., 1976. Cook, R., "Polygonal Database User's Guide, 
 Program of Computer Graphics, Cornell University, Ithaca. New York, unpublished, 1981. [3] [4] Cosman, 
M., Schumacker, R., "System Strategies to Optimize CIG Image Content," ~roceedin~sIg~geII Conference, 
Seottsdale, Arizona, June. 1981.  [5] Feibush, E., Levoy, M., Cook, R., "Synthetic Texturing Using 
Digital Filters," SIGGRAPH/80 Qonference ~oceedin~s, Seattle, Wash., July, 1980.  [6] Fuchs, H., Kedem, 
Z., Naylor, B., "On Visible Surface Generation by A Priori Tree Stuctures", ~IGGRAPH/80 Conference proceedings, 
Seattle, Wash., July, 1980. [7] Hubsehman, H., Zueker, S., "Frame To Frame Coherence and the Hidden 
Surface Computation: Constraints for a Convex World", SIGGRAPH/81 Conference proceedings, Dallas, Texas, 
 August, 1981. [8] Levoy, M., "FILM ANIMATION: Presenting Both Historical and Technical Aspects of the 
Subject and Introducing: Nereus a Computer-Aided Key-Frame Animation System", B.Arch. Thesis, Cornell 
University, Ithaca, New York, 1976.  [9] [10] [11] [12] [13] [14] [15] [16] [17] Newman, W., 
Sproull, R., Principles of Interactive Computer Granhics~ McGraw-Hill Book Co., New York, New York, 
1979. Robertz, W., "Graphical Input System for Computer-Aided Architectural Design", Proceedings CAD 
80, IPC Science and Technology Press, Guildford, UK, March 1980. Schumaeker, R., "A New Visual System 
Architecture," Proceedings Second Interservice/IndustrY Trainin~~ Conference, Salt Lake City, Utah, 
November, 1980. Sutherland,I., Sproull, R., and Schumaeker, R., "A Characterization of Ten Hidden-Surface 
Algorithms", ComDutin~ Surveys, volume 6, number I, March 1974. Wallace, B., "Merging and Transformation 
of Raster Images for Cartoon Animation," SIGGRAPH/81 Conference Proceedings, Dallas,Texas, August, 
1981. Wein, M., Burtnyk, L., "Computer Generated Key-frame Animation," Journal SMPTE, March, 1971. 
 Wein, M., Tanner, P., Bechthold, G., Burtnyk, N., "Hidden Line Removal for Vector Graphics", SIGGRAPH/78 
Conference proceedings, Atlanta, Ga., August, 1978. Wild, E.C., Rougelot, R.S., Schumacker, R.A., "Computing 
Full Color Perspective Images," Electronics Laboratory, Syracuse, New York, 1971. Wu, S.C., Abel, J., 
Greenberg, D., "An Interactive Computer Graphics Approach to Surface Representation". Communications~ 
the ACM, volume 20, number 10, October, 1977.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801277</article_id>
		<sort_key>167</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>26</seq_no>
		<title><![CDATA[Special purpose automatic programming for hidden surface elimination]]></title>
		<page_from>167</page_from>
		<page_to>178</page_to>
		<doi_number>10.1145/800064.801277</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801277</url>
		<abstract>
			<par><![CDATA[<p>In many applications of three dimensional computer graphics, the appearance of the same scene must be computed repeatedly for many different positions of the viewer. This paper concerns a scheme for exploiting this property of an application for the purpose of improving the efficiency of the hidden surface computation. The scheme involves a kind of automatic programming: for each scene to be considered, a different special purpose program is automatically constructed. The special purpose program then takes the position of the viewer as input, and generates a suitable description of the scene with hidden surfaces removed as output. Since the special purpose program has a very limited task to perform - it is adapted to handling just one scene - it can be much faster than any general purpose algorithm would be for the same scene. The paper describes a method by which special purpose programs for hidden surface elimination can be constructed in a fully automatic manner. The method has been implemented, and results of experiments are given.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.2</cat_node>
				<descriptor>Program synthesis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Visible line/surface algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010377</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Visibility</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P46070</person_id>
				<author_profile_id><![CDATA[81100632233]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Stanford University, Stanford, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Beckeman,L., Haraldsson, A., Oskarsson,O., and Sandewall, E.{1976}, A partial evaluator and its use as a programming tool, Artificial Intelligence Journal 7,1976, pp. 319-357]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>321996</ref_obj_id>
				<ref_obj_pid>321992</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Burstall R.M., and Darlington, J.{1977}, A transformation system for developing recursive programs, JACM, Vol. 24, No. 1, January 1977]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807481</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fuchs, H., Kedem, Z.M., and Naylor, B.F.{1980}, On visible surface generation by a priori tree structures, Computer Graphics, Vol. 14, No. 3, July 1980,p. 124]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>260999</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Knuth, D.E.{1968}, The art of computer programming, vol 1: Fundamental algorithms, Addison-Wesley, Reading Mass., 1968, pp. 258-268]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Newell, M.E., Newell, R.G., and Sancha, T.L.{1972}, A new approach to the shaded picture problem, Proc. ACM National Conference, 1972]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Robson, J.{1979}, The height of binary search trees, The Australian Computer Journal, 11(1979), pp 151-153]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356626</ref_obj_id>
				<ref_obj_pid>356625</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I.E.,Sproull, R.F., and Schumacker, R.A.{1974}, A characterization of ten hidden-surface algorithms, Computing Surveys, Vol. 6, No. 1, March 1974]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Yao, F.F{1980}, On the priority approach to hidden surface algorithms, Proc. 21st Symposium on Foundations of Computer Science, October 1980]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Special Purpose Automatic Programming for Hidden Surface Elimination Chris Goad Department of Computer 
Science Stanford University Stanford, California 94306 Abstract In many applications of three dimensional 
com- puter graphics, the appearance of the same scene must be computed repeatedly for many different 
positions of the viewer. This paper concerns a scheme for exploiting this property of an application 
for the purpose of improving the efficiency of the hidden surface computation. The scheme involves a 
kind of automatic programming: for each scene to be considered, a different special purpose program is 
automatically constructed. The special purpose program then takes the position of the viewer as in- put, 
and generates a suitable description of the scene with hidden surfaces removed as output. Since the special 
purpose program has a very limited task to perform -it is adapted to handling just one scene -it can 
be much faster than any general purpose algorithm would be for the same scene. The paper describes a 
method by which special purpose pro-grams for hidden surface elimination can be con-structed in a fully 
automatic manner. The method has been implemented, and results of experiments are given. CR Categories 
and Subject Descriptors: 1.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism -visible line/surface 
algorithm; 1.2.2 [Artificial Intelligence]: Automatic Programming -program synthesis General Term: Algorithms 
This research was supported in part by the National Science Foundation under Grant MCS81-04873 and in 
part by the Advanced Research Projects Agency of the Department of Defense under Contract MDAg03-80- 
C-0102 Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
1. Introduction In computer animation and flight simulation, and in many other applications of three 
dimen-sional computer graphics, the appearance of the same scene must be computed repeatedly for many 
different positions of the viewer. Several algo-rithms for hidden surface elimination have been devised 
which exploit this property of an application (which is one aspect of "object coherence" in the sense 
of [7]). Among these algorithms are those of Schumacker(see [7]), and Fuchs[3]. In each case, the algorithms 
proceed by first constructing a suitable data structure describing the scene;this is done "off- line". 
Then the data structure is used at runtime for performing the hidden surface computation for each position 
of the viewer. We will be concerned here with a different scheme for exploiting object coherence. Rather 
than automatically constructing a different data struc-ture for each scene s to be considered, we instead 
automatically construct a different program P, for each scene; the program then takes the position of 
the viewer as input, and generates a suitable repre- sentation of the scene with hidden surfaces removed 
as output. P, is referred to as a special purpose pro- gram for hidden surface elimination - special 
purpose in that it performs the hidden surface computation not for arbitrary scenes but just for the 
one scene s. Since Ps has a very limited task to perform, it can be much faster than any general purpose 
algorithm for hidden surface elimination would be for the same scene. We use the priority sorting method 
introduced by Newell[5], and Schumacker[7] as the general scheme for the hidden surface computation; 
what is new is the way in which priority sorting is carried out. The specifics are as follows. The scene 
to be dis- played is represented by a set of faces in three dimen- sional space; a face, as usual, is 
a convex oriented polygon. We rcstrict ourselves to static scenes, that is, scenes in which only the 
viewer -and no object &#38;#169; 1982 ACM 0-89791-076-1/82/007/0167 $00.75 - moves. Hidden surface elimination 
is carried out by first finding a priority ordered list of the faces, that is a list L such that whenever 
face i occludes face j from the current viewing position, i appears after j in L. L, then, is a kind 
of "back-to-front" ordering of the faces. Once the priority list has been computed, the actual generation 
of the image may be carried out according to a variety of schemes, the simplest of which involves "painting" 
the faces in the priority order given. (Back faces are removed from the priority list before painting.) 
We will not be con- cerned here with the image generation process, but only with the priority sorting 
phase of the computa- tion. (In any case, real time applications such as flight simulation require that 
much of image genera- tion process be implemented in hardware.) We have developed and implemented a synthesis 
method M for the priority sorting problem. To avoid any possibility of misunderstanding, let us describe 
the function performed by M in precise terms. M is a program (written in LISP) which takes an arbitrary 
scene s given by a set of faces as input, and produces a program P~ as output. P, in turn takes an arbitrary 
viewing position p as input, and produces a priority sorted list of the faces in the scene as output. 
M was tested on one large scale example, namely, a description of a hilly landscape containing over 1000 
faces provided to the author by the Link division of Singer cor-poration. The results of the test indicate 
that, on this example, the automatically synthesized special purpose program is about an order of magnitude 
faster than existing algorithms such as those of Schumacker and Fuchs mentioned earlier. In addi- tion, 
the automatic programming scheme possesses an advantage in flexibility over Schumacker's algo-rithm, 
since it does not place restrictions on the structure of the scenes to be treated. 2. Relation to other 
work in automatic pro-gramming The reader who is familiar with work in the field of automatic programming 
will recognize that the kind of automatic programming with which we are concerned here is very different 
from the tradi- tional kind. According to the usual formulation, the automatic programming problem is 
that of pass- ing automatically from specifications of programs (expressed in some particular formal 
language) to programs which satisfy those specifications. Thus the class of problems to be solved, or 
at least attacked, by an automatic programming system of the traditional kind is a broad one, unless 
the specification language is extremely restricted. Our enterprise, in contrast, involves automatically 
generating programs for a narrow class of tasks given by an ordinary mathematical rather than metamathematical 
or logical definition -namely, just the class of priority sorting tasks for arbitrary scenes. We do not 
require that our synthesis method for priority sorting work on any other class of prob- lems. In this 
sense our aims are less ambitious than those popular in the field of automatic programming as whole. 
3. Outline of presentation The method which we use for synthesis of spe- cial purpose priority sorting 
programs was developed and will be presented in several stages. We will start by describing a simple 
scheme for automati- cally generating special purpose programs, namely the scheme of specializing general 
purpose programs. If this scheme is applied in a direct way to the priority sorting problem, the results 
are unsatisfac~ tory in that the special purpose programs P~ which are generated for any but very small 
scenes s are intractably large. However, having formed a picture of what the programs P, are like, we 
are able analyse their defects. The result of this analysis is a series of modifications to the P, which 
finally yield us-able programs Q~. More precisely, the modifications apply not to the synthesized programs 
P~, but to the method by which they are synthesized, so that we end up with a direct method for synthesis 
of usable special purpose programs. We have chosen to present the synthesis method in a step-by-step 
manner in order to increase clarity, but also for a more important reason. Namely, tile method by which 
the P, are produced is directly applicable to a wide variety of problems in graphics and elsewhere, whereas 
the subsequent modifications are particular to the priority sorting problem. Thus the multi-stage presentation 
has the virtue of exhibiting the method which we use for priority sorting as an instance of a general 
scheme for exploiting sweep coherence; the scheme is: "take a general purpose algorithm and specialize 
it, analyse the results, and introduce appropriate modifications." The scheme applies in a direct way 
to geometrical problems in which the shape of objects is fixed, but their posi- tions are not. Examples 
include a variety of prob- lems in three dimensional graphics and simulation, such as hidden line (rather 
than surface) elimina-tion, collision detection between moving bodies of known shape, and hidden surface 
elimination for scenes with moving objects. We are currently inves- Computer Graphics Volume 16, Number 
3 July 1982 tigating other applications of the general kind indi- cated by these examples. The first 
stage of the presentation includes a preliminary part. Namely, in section 6, we intro-duce methods for 
specialization of general purpose programs in an extremely simple setting which has nothing in particular 
to do with graphics. 4. Remark on data strugtures vs. programs There is no convincing way to formally 
dis-tinguish between the "data structure scheme" (as employed by Schumacker, Fuchs, and others for hid- 
den surface elimination), and the "automatic pro-gramming scheme" which we employ. The reason for this 
is that automatically constructed special purpose programs may be regarded as data struc-tures which, 
together with their arguments, are passed to the interpreter for the language in which the programs are 
expressed. However, the fact that the distinction between a program and a data struc- ture which is not 
a program cannot be convincely formalized does not make it a useless distinction. For the purposes of 
this paper, the word "program" is to be read in its ordinary informal sense: a program is a description 
of a method of computation which is expressible in a programming language of the usual kind. The programs 
which we shall automatically construct have a particularly simple form -they are almost decision trees. 
If the reader prefers to think of these simple programs as data structures instead, then that is his 
priviledge. 5. A formal statement of the special purpose automatic programming problem Before proceeding 
to an example, it will be use- ful to state the special purpose automatic program- ming problem in its 
general form. Let ~(p,x,y) be a ternary predicate. ~(p, z,y) may be regarded as a definition of a class 
of programming problems parametcrized by p if we require that for each value of p, a program Ap be constructed 
which satisfies the input-output specification kzy.c~(p, x, y) in the usual sense; that is, we expect 
that for each x, c~(p,z, Ap(x)) holds. Now, the automatic program-ming problem for a is just that of 
automatically passing from any value for p to a program for Xz y.c~(p, x, y). That is, we want a program 
S~ such that for each p, and each z, c~(p, z, Sa(p)(z)) holds. We will refer to the programs S'~(p) as 
"special pur- pose programs", and to S~ itself as a synthesis pro- gram for ~. A synthesis program Sa 
for a will be useful under the following conditions. (1) Values of y with a(p, x, y) are repeatedly computed 
from values of p and x, where, in the sequence of p's and x's to be treated, p changes slowly, and x 
rapidly. (2) The synthetic program S~(p) is faster than any general purpose program which computes V 
from p and x in one stage. (3) The construction of the synthetic programs S,,(p) is not inordinately 
expensive. Of course, the way in which one makes use of Sa in this situation is to compute S~(p) only 
when a new value of p presents itself. Terminology: in the computational situa-tion described by (1) 
above, where values Yi are repeatedly computed from inputs (Pi, Xi), and where x changes rapidly and 
p slowly, we will say that the sequence of inputs exhibits sweep coherence. The picture behind this term 
is that of a ray performing a circular sweep about a point, with z "attached" farther out than p, so 
that the position of x changes more rapidly than that of p. Evidently, object coherence in three dimensional 
graphics is an in-stance of sweep coherence. One way of constructing special purpose pro-grams for c~(p, 
x, y) is to specialize a general purpose program f(p, z) which computes y from p and x in one stage. 
An example illustrating the technique follows. 6. A simple example: list membership As indicated earlier, 
the purpose of this sec-tion is to introduce some of the techniques used for automatic construction of 
special purpose programs in a simple setting. The example is essentially a reformulation of the construction 
of a binary search tree by random insertion [6]. Let u be a list of ele- ments from a linearly ordered 
set, and let n be an individual member of that set. The computational problem which we wish to solve 
is that of determin- ing whether n is a member of u. We.may set this up as a problem of the kind described 
in the introduc- tion by defining M(u, n, r) as "r = TRUE if n E u; r = FALSE otherwise". We assume that 
the sweep coherence criterion for the utility of performing a computation in two stages is met; that 
is, we assume that membership of n in u must be determined for many different values of n, but few values 
of u. What is wanted is a synthesis program for M -a program SM which will take a list u as input and 
produce as output a fast program SM(u) for determining mem- bership in that list.  Computer Graphics 
Volume 16, Number 3 July 1982 The appropriate "data stucture scheme" for ex- ploiting sweep coherence 
in the current example in- volves sorting and binary search. Since this method is provably optimal, no 
improvements by the use of automatic programming are possible in this case. With this warning in mind, 
let us proceed. The synthesis method for M consists of taking a slightly peculiar algorithm for computing 
list membership and specializing it to the list at hand. The algorithm to be specialized is given by 
the recursive definition: f(u, n) ---- if empty(u) then FALSE else if n < head(u) then f(tail(u),n) else 
if n > head(u) then f(tail(u), n) else TRUE The specialization to a concrete list L is carried out by 
a conventional scheme: first symbolically ex- ecute f as applied to L, then simplify the resulting decision 
tree by removing redundant decision nodes. (Other terms for symbolic execution include partial evaluation 
[1], and repeated unfolding [2]). A redun- dant decision node is one whose outcome is predeter- mined 
by the outcomes at nodes along the path lead- ing from the root of the decision tree to the node in question. 
More precisely, if PI'" "Pk are the decision predicates appearing on the path leading to node N, and 
Q is the predicate at node N, then N is redundant if At A A2" -Ak ~ Q is valid, or if A1 A A2...Ak ~ 
~Q is valid, where Ai ~ Pi if the true branch is taken out of Pi's node in the path leading to N, and 
A, = ~Pi if the false branch is taken. The removal of a redundant decision node is carried out by replacing 
the subtree of which it is a root by the subtree rooted at its left or its right son, depending on whether 
the predicate at the node is predeter- mined to be true or false. Note that determining whether a node 
in a decision tree is redundant in-volves deciding whether certain formulas are valid. In the current 
instance, all such formulas are im-plications between conjunctions of inequalities con-taining n as the 
only variable, so that the decision problem has an easy automatic solution. Here is an example of the 
behavior of SM, the synthesis method for M just described, in a concrete case. Suppose that the underlying 
ordered set for the membership computation is the integers, and that we wish to compute SM((2 , 5)). 
We proceed by symbolically executing f((2, 5), n), arriving at the decision tree: if n < 2 then (if 
n < 5 then FALSE else if n > 5 then FALSE else TRUE) else if n > 2 then (if n < 5 then FALSE else if 
n > 5 then FALSE else TRUE) else TRUE The decision node represented by the second line of the above program 
is redundant, since n < 2 D n < 5. Its removal yields: if n < 2 then FALSE else if n > 2 then (if n < 
5 then FALSE else if n > 5 then FALSE else TRUE) else TRUE No redundant nodes remain. Hence the above 
program represents the final result of computing SM((2, 5)). By the analysis given in [6], the behavior 
of SM exhibits the following general properties. First, the expected maximum depth of the decision tree 
generated by SM when applied to a list L of length n (assuming that L is given in random order with equal 
probability assigned to each ordering) is k log2(n) for a constant k between 7.26 and 8.62 (the exact 
value of k is not known). Second, the number of decision nodes in SM(L) is exactly 2n, where again n 
is the length of L. (If we adopt a formulation in which the decision nodes have three branches [for <, 
=, and >], then the number of nodes and depth are halved.) Thus the worst case running time of the programs 
generated by SAc, measured by the max-imum number of decision nodes to be traversed on the path to the 
result, may be expected to exceed the optimal worst case running time obtained by sorting and binary 
search by only a small constant factor. Also the size of the programs generated is linear in the length 
of L, so that in this respect as well SM is only a small constant factor worse than optimal. Finally, 
note that the expected worst case running time of SM(L) is immensely better than that of the one stage 
algorithm given by f; the former is logarithmic in the length of L, while the latter is linear. At this 
stage, it should be evident why we chose the somewhat peculiar definition of f, involv- ing a three-way 
case analysis according to whether n is greater than, less than, or equal to head(u), rather than the 
usual definition, in which only the  Computer Graphics Volume 16, Number 3 July 1982 cases of equality 
and non-equality are treated. The reason is that implications between inequalities are common, whereas 
one non-equality never implies another. Consequently, a list membership decision tree built of inequalities 
will contain many redun-dant nodes, while the corresponding tree built of equalities and non-equalities 
will not. The list membership example illustrates the fol- lowing points. (1) In the case of list membership, 
the automatic programming approach failed to produce results which were as good as those produced by 
the right data structure. But on the other hand, our approach did almost as well, and in a sense required 
less in the way of intellectual resources than the conventional scheme. For, in constructing special 
purpose pro-grams for individual lists L, all that was required was the unwinding and optimization cf 
a slight variant of the ordinary one stage program for computing mem- bership. On the other hand, the 
construction of the sort and binary search method requires additional ideas -ideas which are not already 
implicit in the one stage program. There are many computational problems exhibiting sweep coherence where 
a one stage program exists, but where no fully satisfactory data structure for exploiting the coherence 
is avail- able -the required additional ideas have not been found. Such problems are promising candidates 
for the automatic programming approach. One such problem is that of hidden surface elimination. (2) 
Exploiting sweep coherence of one kind or another is a major enterprise within computer science. For 
example, the sorted list is just one of many data structures devised for this purpose, but even taken 
alone sorting has been the target of a large amount of effort. Any new tool for exploiting sweep coherence 
may be expected to have wide ap- plication.  7. The synthesis method for hidden surface elimination 
Now we turn to the our principal subject, the synthesis of special purpose programs for priority sorting. 
Let P(F,p,L) denote the predicate, "L is a i~riority sorted list of the faces F from viewpoint p". Our 
aim is to devise a synthesis program for P in the formal sense described in section 5. That is, we want 
a program Sp which will take F and generate a spe- cial purpose priority sorting program Sp(F); Sp(F) 
then takes the position p as input, and generates a priority sorted list L for p. (As is well known, 
not every scene has a priority ordering for its faces from every point of view. The priority sorting 
methods which we will consider will perform the priority sort if possible, and will indicate its impossibility 
other- wise.) We will proceed by first considering the result of applying the direct approach illustrated 
in sec-tion 6 -that of specializing a simple one stage al- gorithm and then removing redundancies from 
the resulting decision tree. Then modifications of the direct method will be introduced one by one until 
a usable final result is obtained. Here is the brute force algorithm for priority sorting: (1) Compute 
the entire occlusion relation; that is, determine for each pair of faces i, j with i ~A 3" whether or 
not i occludes j from p. (2) Topologically sort the faces according to the occlusion relation computed 
in step (1). If there is a cycle in the occlusion relation, then no topological sort is possible, and 
in this case the out- come of the computation is an indication of failure. (Priority sorting consists 
exactly of finding a linear order which is consistent with the occlusion rela-tion; the task of extending 
an acyclic binary relation to a linear order is the topological sorting problem. Algorithms for topological 
sorting may be found in standard references such as [4]).  Now, consider the decision tree which results 
from unwinding this algorithm for a particular set F of faces. The decision tree To which we have in 
mind may be described in precise terms as follows. Let n be the number of faces, and let (it, ]1), (i2, 
]2)'" "(i,~(,~-l), ],~(,~-~))be an enumera- tion of the set of pairs of faces from F given in the order 
in which they are considered in step (1) of the brute force algorithm. To, then, is a full binary tree 
of depth n(n -1) (with 2 n(=-l) nodes!). Let occ(p, i, j) denote the occlusion predicate: occ(p, i, j) 
holds iff i occludes 3" from p. Then the predicate appearing at each of the 2 k-1 decision nodes at the 
kth level of the tree is occ(p, ik,jk). Evidently, each leaf of the tree corresponds to a particular 
truth as-signment to the occlusion predicates, that is, to a particular occlusion relation on the faces. 
Following step 2, the computational result appearing at each leaf is the result of topologically sorting 
the faces according to the occlusion relation associated with that leaf, or an indication of failure 
if that relation contains a cycle. The next stage in the process consists of the removal of redundant 
nodes from the decision tree. However, this requires that we automatically decide whether assertions 
of the form occ(p,i,]) or ~occ(p, i, ]) follow from sets of other assertions of the same form. As they 
stand these decision problems do not have any emcient solution. This difficulty can be overcome by making 
use of the following ob- servation. Namely, for any two faces i,] in three dimensional space, the set 
of viewer positions from which i occludes j constitutes a simplex -that is, a convex region in space 
bounded by planes. The number of such boundary planes is the sum of the number vertices in i and the 
number of vertices in j, plus 2. Now, in order to test whether an occlu-sion between two given faces 
i and ] takes place, one may proceed by checking in turn whether the viewer position lies "inside" or 
"outside" the planes which make up the boundary of the simplex. The asser-tions which we have just made 
are not difficult to prove, but because of space limitations, the proofs will not be included here. Putting 
matters more formally, the predicate occ(p,i,]) can be expressed as a conjunction of simpler predicates 
occl(p., i, j).. "OCCr(p, i, j), each of which is a "plane test" on the viewer position - a test which 
can be written as a linear inequality in the coordinates p~,pu,pz of the viewer position. Thus, we can 
rewrite the decision tree T o to get a decision tree T1 in which the predicate at each decision node 
is a linear inequality. This is done by replacing all occlusion decision nodes in To according to the 
fol- lowing scheme: Replace "if occ(p, i, j) then tt else t2" with:  if occl(p, i, j) then if occ2(p, 
i, ]) then if OCCr(p, i, j) then ti else t2 else t2 else t2 We are in a good position to automatically 
remove redundant nodes from Tl,.since (1) the predi- cate at each of its nodes is a linear inequality, 
(2) the negation of a linear inequality is a linear inequality, and (3) the question of whether a given 
inequality follows from a set of inequalities can be efficiently decided by use of the simplex algorithm. 
Further, on intuitive grounds, it is to be expected that the removal of redundant nodes will reduce the 
size of T1 by a very large factor, since, once the fifth level -say -of the decision tree has been reached 
along a given path, the tests met so far along the path will have severely constrained the position of 
the viewer, and accordingly it is likely that the outcomes at the great majority of nodes below that 
level will in fact be predecided. Of course, the problem with TI from the prac- tical standpoint is that, 
even for very small numbers of faces, its size is intractably large. In practice, however, one will not 
proceed by first generating the decision tree T1, and then optimizing it, but instead will optimize the 
tree while it is being generated. This can be done by constructing the tree from top down. When a new 
decision node is to be generated, we ask first whether its outcome is predetermined. If so, then we need 
not add the node to the tree; instead, we proceeds directly with the generation of its left or right 
subtrees, depending on whether the predicate at the node is predetermined to be true or false. If this 
scheme is used, then no redundant nodes are ever generated; instead, the optimized tree is produced in 
one pass. So, we have shown how to automatically con-struct an optimized decision tree T2 for doing the 
priority sort for a fixed set of faces, but variable viewpoint. Although we expect that 712 will be much 
faster than the brute force method from which it sprang, it is still too large to be of practical use. 
8. Further improvements The following observations will allow us to get much better results. (1) The 
entire occlusion relation need not be determined in order to do the priority sort. (2) A partial determination 
of the occlusion relation which is insumcient to do the entire sort may still allow a part of the sort 
to be carried out. The former observation will allow us to shorten the tree, while the latter will make 
it possible to diminish the size of the results appearing at the leaves, by moving as much information 
as possible about the output of the computation to nodes closer to the root of the tree. Let I be a set 
{Ii.. "Ik} of linear inequalities in the the coordinates p~, Pu, P* of the viewer position. 172 Computer 
Graphics Volume 16, Number 3 July 1982 We may think of I as representing the simplex of points in three 
dimensional space which satisfy all of the inequalities/1.. "Ik. Let canoce(I,i, j) denote the predicate, 
"there is some point satisfying each of the inequalities in I from which the face i occludes the face 
j". Now, consider an arbitrary node N in T2. Let IN be the set of inequalities which are assumed to hold 
at N -that is to say, the set of inequalities which must hold if N is to be reached in the course of 
executing the decision tree. Let G be the graph of canoec(IN, i, j) viewed as a binary relation on the 
set of faces - we take the nodes of G to be the set F of faces, and "draw" an edge from i to ] if there 
is a viewer position satisfying IN from which i occludes j. The construction of canocc graphs is illustrated 
by figures 1 - 4. For the sake of simplicity, the figures portray two dimensional rather than three dimen-sional 
occlusion relationships. The occluding objects are oriented line segments rather than faces, and the 
position of the viewer is given by a point on the plane rather than a point in space. One line segment 
sl is said to occlude another ~2 from a point p if there is a ray originating frmn p which crosses sl 
first, and then s2, and if p lies on the "front" side of both s1 and s2. One way of visualizing this 
reduction in dimension is to imagine that the line segments rep- resent projections in the plane of a 
set of rectangular vertical faces, all which possesss a common extent in the vertical dimension. When 
viewed from this perspective, it is evident that any occlusion relation which can arise in the two dimensional 
case can arise also in three dimensions, but of course the converse does not hold. See [8] for a discussion 
of occlusion and priority sorting in two dimensions. Figure 1 represents a two dimensional scene. The 
orientations of the segments are not indicated in the figure, so we stipulate here that the "front" sides 
of the segments are, in each case, their up-per sides. Figure 2 represents the graph of the rela- tion 
canocc(nil, i, j) on the set {A, B, C, D, E, F} of segments appearing in figure 1, where the set of in- 
equalities which the viewer position must satisfy is empty. Thus an edge from node i to node j appears 
in figure 2 if there is any point from which i occludes ]. Figure 3 represents the graph of the relation 
canocc(Zl,i,j) "for the same set of segments, with the difference that the viewer position is restricted 
to the region above the dashed line; the set ZI consists of the single inequality expressing the assertion 
that the position of the viewer lies above the dashed line. Figure 4 represents the graph of cannocc(Z2, 
i,j), I A ,. Figure 1 E c,z\   I t ? F/ B B Figure 2 Figure 3 / E C A %/ B Figure 4 where Z2 asserts 
that the viewer lies on or below the dashed line. Now let us return to the general discussion. Again, 
let N be a node in an occlusion decision tree, let 1N be the set of inequalities in force at the node, 
and let G be the graph of canocc(IN, i, j) viewed as a binary relation on the set of faces. If G is acyclic, 
then a topological sort of G will yield a priority or-der which is valid for all view points in the simplex 
IN, since only occlusions given in G can occur from viewpoints within the simplex, and since the order 
given by the topological sort of G will be consis-tent with all of these occlusions. It may be that from 
each particular point in the simplex, the set of occlusions observed is a proper subset of those occuring 
in G, but nonetheless, the topological sort of G will be consistent with this smaller occlusion set. 
In the acyclic case, the subtree rooted at N is not needed at all; a correct priority order can be 173 
 Computer Graphics Volume 16, Number 3 July 1982 generated without further case analysis. (Example: 
the graph in figure 3 is acyclic, so that its topologi- cal sort-eg the ordering (B,E,D,F,C,A) -is a 
valid priority ordering for all viewer positions above the dashed line.) Suppose, on the other hand, 
that G contains cycles. Let $1" 'Sk be the strongly con-nected components of G. (Recall that a strongly 
con- nected component of a directed graph is a maximal set S of points from the graph having the property 
that, for any two points p,~ E S, there is a path from p to q. In the graph given in figure 2, there 
are three strongly connected components, two of which are singleton sets. These are {A, C, F, D}, {E}, 
and {B}.) As long as there is more than one strongly connected component -that is, as long as G is not 
itself strongly connected -a part of the priority or-der can he determined at the current stage. Let 
G' be the (acyclic) graph which results from collaps- ing the strongly connected components of G into 
single vertices. (Formally: the vertices of G' axe the strongly connected components Sl" .Sk of G; ~Si 
-+ SiN is an edge in G' iff for some p E Si, q 6 Sj, ~p --* q]] is an edge in G.) The result of topologi- 
cally sorting G' gives an ordering to the St-' "Sk which constitutes a partial priority sort, in the 
fol- lowing sense. Let G" be the graph of any occlusion relation which is consistent with inequalities 
IN. If G" has a priority ordering at all, then it has one of the form append(pl(Sil), p2(S,,).. "pk(Si,)) 
where Si~.-'Si, is the topologically sorted ordering of G', and where pj(Sij) is some permutation of 
Sis. (By "append" here is simply meant the operation on lists of numbers which takes the lists which 
it is given, and appends them together to create a longer list.) Thus, the set of faces has been partitioned 
into subsets $1-. "Sk such that the members of each Si may be listed consecutively in any final order- 
ing, and the order in which the Si appear has also been decided. It is only the orderings within the 
Si that remain to be determined. All of this has two consequences: (1) the occlusion or lack of occlusion 
between faces in di[fer~nt strongly connected com-ponents need not be considered in the decision tree 
rooted at N. Further, the strongly connected com-ponents may be considered separately; if desired, a 
different decision tree may be generated for each. (2) The decision tree rooted at N needs to specify 
only the ordcrings within, and not between the strongly connected components, provided that the ordering 
Sq.. "Sik is stored in one way or another at the node N. The observations of the last paragraph indicate 
in a fairly direct way how T.2 may be improved on. The result T3 of this improvement has a structure 
which is a bit more complicated than that of an ordinary decision tree, in that it has internal nodes 
which are not decision nodes. One way of describing T3 is as a simple loop-free program which is built 
up from constants denoting lists of faces by use of (1) the conditional operator: "if P then tl else 
t2" where P is a linear inequality in P~,Pu,P*, and (2) the append operator: "append(Q,...tk)". Thus, 
:/'3 differs from a decision tree only in that T3 makes use of two operators ("if" and "append"), rather 
than just one ("if") in constructing its result. The method by which T3 is built follows closely the 
method used to build T2. The difference is that the canocc graph is employed to guide the selection of 
face pairs for case analysis, and also to split the computation of the priority ordering into separate 
computations for separate strongly connected com-ponents. The method for synthesizing T3 is given below 
by the recursive program £(I, F). Here, I is a set of inequalities, and F a set of faces. The result 
returned by R(I,F) is a program which computes an ordering for the faces in F; under the assumption that 
the inequalities in I hold, this ordering will be" a correct priority ordering of the faces. ~(I, F): 
(1) Compute the graph G of canocc(I, i, j) for i~i]GF.  (2) If G is acyclic, then topologically sort 
G, and return the constant representing this sorted list of faces. (3) If G is strongly connected then: 
 (a) choose i,] such that occ(p,i,j) is not decided by I. If there are no such i, j then return "FAIL". 
 (b) choose an occlusion subtest occk(p, i, ]) which is not decided by I. Generate a case analysis: 
return the program, "if" ocek(p, i, j) "then" R(IU{occ~(p, i, ])}, F) "else" ~(r U {~oeek(p, ~,/)}, F) 
  (4) Otherwise:  (a) Let $1...Sk be the strongly connected components of G. Topologically sort the 
graph G' gotten by collapsing the Sj, getting an ordering S~I.-.S~ k of the S i. Compute the programs 
Pi --~ ]~(I, Si) for priority sorting the Sj. Return the pro-gram, "append"(P~,, . . "Pi,) 9. Failure 
flags In the text of ~(I,F) above, part 2,line (a) contains the instruction, "if there are no such i,], 
then return('FAIL')". This needs explanation. For any particular scene, it may happen that there are 
viewer positions for which no correct priority order- ing of the faces in the scene exists. In this situa-tion, 
the ease analysis generated by ~ will proceed until the viewer position is restricted to a simplex within 
which the occlusion relation on the current strongly connected component is completely deter- mined -that 
is, a simplex with the property that every point within the simplex gives rise to exactly the same occlusion 
relation. The occlusion relation in question will contain cycles, so that at this stage in the case analysis, 
a particular simplex of viewer positions will have been found such that no viewer position within the 
simplex admits a correct priority ordering. It is in this situation that the synthesis method places 
the flag "FAIL'at the offending node. Two uses can be made of failure flags. First, the presence or absence 
of flags in the result produced by the synthesis program ~ allows the designer of a scene to determine 
whether priority orderings are possible from all viewer positions, and if not, what parts of the scene 
and what parts of viewer space can lead to problems. If problems do exist, the scene might be modified 
so as to eliminate them. Second, a synthesized program containing failure flags can be implemented in 
such a way that the flags are replaced by code that takes appropriate action -most likely, giving an 
incorrect ordering in the hope that the user will not notice. 10. An example As an illustration, let 
us trace the behavior of when applied to the scene represented by figure 1. In the initial call to ~, 
the set of faces F is {A, B, C, D, E, F}, while the set of inequalities I is nil. The resulting canocc 
graph appears in figure 2. This graph has three connected com-ponents: {A,C,F,D}, and the two singleton 
sets, {B}, and {E}. Clause (4) of £ pertains, so that is called recursively on each of the three strongly 
connected components. The singleton components immediately yield singleton orderings. For the com-ponent, 
{A, C, F, D}, clause (3) pertains, so an oc-clusion test and subtest must be chosen. How this choice 
is made does not affect the central properties of the algorithm; we may assume that the choice is made 
at random. Suppose that, in tbis case, the occlusion subtes~ which is chosen is the one which tests whether 
the viewer position lies above the dashed line. Let us designate this test "Q(p)". Reeursive calls, ~({Q(p)}, 
{A, C, F, D}) and ~({-~Q(p)}, {A, C, F, D}) are made to generate the two sides of the case analysis. 
Both calls im- mediately generate priority orderings without fur-ther case analysis, since the eanoce 
graphs aris-ing from either of the assumptions Q(p), -,Q(p) are acyclic, as can be seen from figures 
3 and 4. Thus the final result of the computation will have the form, append(( B), { E), if Q(p) then 
(n, F, C, A) else (A, D, F, C)) (Since the acyclic graphs which arise in the com- putation do not have 
unique topological sorts, the above is not the only possible outcome.) 11. Results of experiments A program 
for synthesis of special purpose priority sorting programs has been implemented on a PDP-10 computer 
in MacLisp. The program has been tested on one large scale example so far, namely, a description of a 
hilly landscape derived from a data base provided to the author by the Link division of Singer corporation. 
(Link is a manufac- turer of flight simulators). The description consisted of a set L of 1135 faces making 
up, roughly speaking, a triangulation of the landscape. The implemented program is based directly on 
the method described in section 8, but includes the following important refinement. (There are also a 
number of less impor- tant refinements and implementation details whose description is beyond the scope 
of the current general presentation of the method). In almost all three dimensional computer graphics 
applications, the field of view covered by the image to be generated is limited. For example, in several 
of the flight simulators manufactured by Link, the field of view or %vindow" spans 48 de- grees horizontally, 
and less than 40 degrees verti- cally. We exploit this fact by performing an initial case analysis according 
to the direction in which the viewer is looking. Specifically, this is what is done: Consider the projection 
of the viewing direction v onto the z,y plane. We divide the "pie" consist-ing of the set of all possible 
such projections into ten equal "slices", each 36 degrees wide. Now, one face can visibly occlude another 
only if there is a ray from the viewer:s eye which passes through face i and face j and whose direction 
lies within a certain angular distance the viewing direction v. So, the as- sumption that the z, y projection 
of v lies within a given 36 degree slice reduces the number of possible occlusions between faces, since 
it places limitations on the relative positions of visibly occluding pairs of faces. In any case, the 
ten slices of the pie are considered separately, with field of view parameters of 48 degrees horizontal 
and 40 degrees vertical, and with the additional assumption that the angle of the viewing direction to 
the x,y plane (that is, the an-gular deviation from horizonta! flight) is less than 30 degrees. The other 
cases, where the angle of view is steeply up or steeply down, are handled separately. For each slice, 
a separate initial canocc relation is computed; it is this restricted canocc relation which forms the 
starting point for the method ~ described in section 8. In the experiments performed so far, the syn- 
thesis method has been applied to the landscape L for only one of the ten pie slices. The features of 
the landscape (mountains and valleys) are oriented ill more or less random directions, so there is reason 
to believe that similar results would be obtained for each of the pie slices. Also, the steeply up and 
steeply down orientations should yield results which are better, and not worse, than the horizontal orien- 
tations. Tile synthetic program T..~ produced for priority sorting for the landscape within one slice 
of viewer directions had the following attributes: Worst case number of decision nodes encountered during 
any execution of 7:3:53 Expected number of decision nodes encountered dur- ilLg an execution of T3, assuming 
that at each deci- sion node the two possible outcomes of the decision are equally likely: 27 Total number 
of decision nodes in T'3:85 For the flight simulation application, one would wish to implement not T..~ 
itself, but rather a variant of Ta which takes advantage of the fact that, in flight simulation and similar 
applications, the posi- tion of the viewer changes smoothly from frame to frame. This can be done by 
arranging to keep a record from each priority computation of the result of that computation, and also 
of which computation path was taken, that is, a record of which branch was followed at each decision 
node encountered in the course of the computation. Then, for following frames, the program checks whether 
the new viewer positions yield new computation paths; as long as the computation path does not change, 
there is no need to modify the priority sort computed at an ear- lier frame. Also, as long as the change 
takes place fairly late in the computation path, only a part of the priority sort will need to be recomputed. 
In any case, a machine language program of the kind just described can be automatically derived in a 
straight- forward manner from 7"3. The construction of a machine language program has not actually been 
carried out, nor is there any reason to do so for the present purposes, since the relevant parameters 
(size and speed) of the machine language program can be easily derived from 7"3. Here are those parameters. 
Size(estimate) : 1500 36 bit words Expected number of machine language instructions executed in order 
to verify that a given priority sort is still valid for a new frame: 8 instructions per plane times 27 
planes ----- 216 instructions The total size of the synthetic program, with all orientations accounted 
for, would be about 1500 times 12 z 18,000 words. The average number of instructions executed per frame 
should not be much higher than the expected number of instructions required for verification since, for 
most new frames, the com-putation path will not change. It is difficult to estimate in the general case 
how the size and speed of the special purpose program produced by our method for an arbitrary scene will 
depend on the characteristics of that scene. For one thing, it is not only the size of the scene that 
is relevant, but also the details of its structure. (To see this, note that there are scenes of arbitrary 
size with the property that one priority ordering works for all positions of the viewer.) However it 
may be use- ful if we give some extremely rough estimates based on our experience with synthetic programs 
derived for fragments of the landscape used for the experi- ment. It appears that for this kind of scene, 
both the size and the running time of synthetic programs are given by expressions of the form k * f(n) 
* n, where f(n) is a function which grows very slowly with n; perhaps f(n) ~ log(n). The constant k of 
propor- tionality for the running time (in machine instruc-tions executed) is small - something like 
.02, whereas for the size (in words of memory), k is something like 10. 12. Comparison to other algorithms 
for priority sorting There are two algorithms described in the litera- ture which exploit object coherence 
in performing ... _% Figure 5 priority computations, namely those of Schumacker [7], and Fuchs[3]. 
The two algorithms do not con-stitute two distinct ways of solving the priority prob- lem. Rather, the 
algorithm of Fuchs provides an automatic method for constructing the data struc-ture which is required 
by Schumacker's algorithm in the on-line phase of the computation. This data structure is called a "binary 
space partitioning tree" (BSP) in [3]. It consists of a hierarchical decomposi- tion of the scene by 
means of a collection separating planes. There is a superficial resemblance between the special purpose 
programs which we produce and the programs which use the BSPs at runtime to perform the priority sort. 
The resemblance is that in both cases, the method involves comput-ing the viewer's position relative 
to a set of planes. The resemblance is superficial because the following significant differences exist: 
(1) The BSP algorithms traverse the binary space partitioning tree - compar- ing the viewer position 
to the plane at every node -rather than ezecuting a decision tree (or the like), in which only a fraction 
of the nodes are visited. (2) The planes which appear at decision nodes in our synthetic programs are 
not separating planes in any significant sense (although they will separate one /ace from another); in 
any case, there is not the same kind of systematic geometric relationship between the planes and the 
priority orderings produced that exists for BSP algorithms. It is not hard to construct artificial scenes 
for which ~he automatic synthesis method desribed here is superior to the BSP methods. Consider the two-dimensional 
scene schematically pictured in figure 5, in which the configuration in figure 1 is replicated a large 
number of times. The number of nodes in any BSP for this scene will be at least O(n), where n is the 
number of "peaks" in the scene, so the traver-sal of the BSP performed by Schumacker's algo-rithm will 
require O(n) comparisons. The automatic synthesis method will also produce a special pur-pose program 
with O(n) decision nodes. However, the expected worst case running time of the spe-cial purpose program 
(under the assumption that the selection of occlusion tests in the construction of the program is done 
in a random way) will be O(log(n)). (The O(log(n)) result here follows from the same considerations that 
were invoked in sec-tion 6 to arrive at the O(Iog(n)) expecteddepth for list membership decision trees.) 
Various elaborations of the BSP method, such as the precomputation scheme described in [3], when used 
in conjunction with a clever (and probably not automatically realiz- able) choice of separating planes, 
can yield improved BSP results for this scene. However, each such elaboration can be defeated by slight 
modifications of the scene -modifications which do not affect the results gotten by use of the automatic 
programming method. The above assertions are not difficult to prove, but again for reasons of space limitation, 
we will omit the proofs. Now we turn to a comparison in performance for Link's scene L, described in 
the last section. The algorithm of Schumacker was used by Link for dealing with the landscape on which 
we per-formed our experiments. The separating planes for the landscape were generated by a semiautomatic 
method differing from that of Fuchs. The number of separating planes was about 200 (exact fgures are 
not available), so that the number of plane com- parisons needed for a priority sort in this case is 
200/27 = 7.4 times the expected number required by our synthetic program. Also, the overhead per plane 
for keeping track of the traversal of a BSP is higher than the overhead per plane for our synthetic program; 
at least 12 (rather than 8) instructions per plane are needed. So all in all, the synthetic program is 
about 1.5 * 7.4 ~ 11 faster than Schumacker's for this example. This comparison is not to be regarded 
as more than a preliminary indication of how well our method will perform in practice, since one ex-ample 
is not a sufficient test in any case, and since several of the numbers involved in the comparison have 
tile status of estimates, and not measurements. Also, note that this comparison applies to a par-ticular 
choice of separating planes, and not to BSP methods in general. Still, the evidence available so far 
does indicate that our method is substantially faster than the BSP methods. Further aspects of the comparison 
are memory space and off-line computation time requirements. The space requirements of Schumaker's algorithm 
are smaller by about a factor of ten than ours. This is not as bad as it sounds for our method, since 
the space needed to store a synthetic program for priority sorting, though large, is still of the same 
or- der as the space needed for storing other data about the scene, such as smooth shading and color 
informa- tion. Another disadvantage of our approach is that the amount of computer time required to construct 
synthetic programs is quite large. The synthesis of T3 (which deals with only one of twelve orientations) 
took about one hour of cpu time on a PDP-10/KL- 10 computer. Still, this is not prohibitive if the scene 
for which the synthesis is being carried out is to be used for many simulations. The main advantage 
of our method over BSP methods is not speed, but flexibility. Although the trial landscape L is separable 
into clusters in the way required by Schumacker's algorithm, we make no use whatever of that fact; our 
results would be similar for another landscape of the same general kind for which no set of separating 
planes existed. The restriction to separable scenes needed for BSP methods is a serious one in practice, 
since scenes which are constructed so as to present a natural ap- pearance with a minimal number of faces 
are very unlikely to be separable. A scene which is not linearly separable as it stands can be modified 
so as to meet the separability condition, but not without penalty. One way of meeting the condition involves 
splitting faces, and this is the approach taken by Fuchs. The difficulty with the method of Fuchs is 
that as the size of scenes to be considered grows, the number of new faces introduced by splitting, and 
the size of the BSP itself, can increase rapidly. In [3], an upper bound on the number of nodes in the 
BSP is given which is cubic in the number of faces. If this bound is approached for the scene L, then 
Fuchs' al- gorithm is not usable. Of course, there is no reason to expect a priori that the upper bound 
is a relevant estimate for situations which arise in practice. For the considerably smaller examples 
which Fuch's con- sidered, the number of nodes in the BSP did not grow at a rapid rate. So, without further 
data, it is difficult to estimate exactly what the result of ap- plying Fuchs' algorithm to the landscape 
L would be. 13. Conclusion We have described a new method for solving the priority sorting problem which 
arises in hidden sur-face eliminatiom Preliminary evidence suggests that the method has advantages of 
speed and flexibility over methods currently in use for real-time applica- tions. The new method was 
developed according to a very general scheme for the automatic generation of fast special purpose programs. 
This scheme ~s ap- plicable to other problems in computer graphics and elsewhere. References 1. Beckeman,L., 
Haraldsson, A., Oskarsson,O., and Sandewall, E.[1976], A partial evaluator and its use as a programming 
tool, Artificial Intelligence Journal 7,1976, pp. 319-357 2. Burstall R.M., and Darlington, J.[1977], 
A trans-formation system for developing reeursive pro-grams, JACM, Vol. 24, No. 1, January 1977 3. Fuchs, 
H., Kedem, Z.M.,and Naylor, B.F.[1980],  On visible surface generation by a priori tree structures, 
Computer Graphics, Vol. 14, No. 3, July 1980,p. 124 4. Knuth, D.E.[1968], The art of computer program- 
ming, vol 1: Fundamental algorithms,Addison- Wesley, Reading Mass., 1968, pp. 258-268 5. Newell, M.E., 
Newell, R.G., and Sancha, T.L.[1972], A new approach to the shaded picture prob-lem, Proc. ACM National 
Conference, 1972 6. Robson, J.[1979], The height of binary search trees, The Australian Computer Journal, 
11(1979), pp 151-153 7. Sutherland, I.E.,Sproull, R.F., and Schumacker, R.A.[1974], A characterization 
of ten hidden-surface algorithm%Computing Surveys, Voh 6, No. 1, March 1974 8. Yao, F.F[1980], On the 
priority approach to hid- den surface algorithms,Proc. 21st Symposium on Foundations of Computer Science, 
October 1980   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801278</article_id>
		<sort_key>179</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>27</seq_no>
		<title><![CDATA[Speech recognition as a computer graphics input technique (Panel Session)]]></title>
		<page_from>179</page_from>
		<page_to>180</page_to>
		<doi_number>10.1145/800064.801278</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801278</url>
		<abstract>
			<par><![CDATA[<p>Richard Rabin</p> <p>Interactive graphics systems typically require intense &#8220;hands busy/eyes busy and brains busy&#8221; activity on the part of the system user/operator.</p> <p>Voice input by means of automatic speech recognition equipment, offers major potential for improving user/operator productivity. It is the only input technique which does not require the direct use of hands and eyes. Voice input can replace or complement keyboards, function keys, tablets and other types of input devices typically employed for entering commands, alpha and numeric data.</p> <p>Alan R. Strass</p> <p>Effective human interfaces are an essential element of plant information systems and computer integrated processes, such as graphics. In the past, data gathering choices in the factory have been generally dominated by clipboards and travelling punched card decks. Similarly, complicated keystroke sequences have often been required to evoke appropriate computer controlled functionality. Today, a number of advanced human interface techniques can be used to improve both source data capture and the selection of appropriate computer controlled operations. These techniques are becoming an integral part of many emerging on-line/real-time engineering and manufacturing applications.</p> <p>Speech recognition, in particular, is emerging as an important interface technology. Speech input can reduce the amount of attention the user has to spend on the mechanics of recording information of selecting functions and allows users to concentrate on their primary task. Some of the benefits include: (1) reduced user training time, (2) increased worker productivity, (3) reduced secondary key input, and (4) improved timeliness and accuracy of information made available via voice.</p> <p>Currently available speech recognition products have already been used to demonstrate these benefits. These advantages will only increase as the next generation of speech products deliver improved recognition performance.</p> <p>Mark Robillard</p> <p>There is an increasing awareness of the potential for improving operator productivity for graphic systems by the use of voice input. We have analyzed a number of applications in conjunction with potential users to determine how best to use this capability. Initial conclusions are that voice input can be effective, providing that the capabilities of the speech recognition equipment utilized to input voices are matched to the requirements of the applications. Key factors to be considered are vocabulary size and types, and the use of isolated words versus continuous speech utterances.</p> <p>Sue Schedler</p> <p>The addition of voice input to Calma's GBSll System provides users with a fast, accurate means by which they can execute commands. A customized Interstate Electronic VRM System has been integrated into the hardware configuration of all microelectronics products, Chips, Sticks and CARDSII. System software recognizes the input from the Harmony VRM in the same manner as a keyboard or menu button input. All three input methods may be used separately or in conjunction with each other. All command inputs are buffered, so the user need never wait for a command to complete before entering the next. A voice file contains up to 50 words, and any number of files may be uploaded or downloaded by one user. Both standard system commands or GPLII Programs can be executed by voice. On-line design productivity by menu input can be increased by 50% when the operator uses voice.</p> <p>Operator training is unaffected by the addition of voice to the system. Voice is trained as a separate module in the class. Its greatest impact appears once the users know operation syntax and can communicate commands quickly.</p> <p>Calma provides standard voice menu to be used by customers or the customer may modify the file as needed. Most commonly used commands are put into the voice file leaving less frequently used commands for the onscreen menu. By implementing the combination of voice and menu, we can completely eliminate the need for keyboard input.</p> <p>Operators initially are skeptical of the &#8220;bell and whistle&#8221; feature, but find after using voice input that they do not feel uncomfortable &#8220;talking&#8221; to the computer, and find it an enjoyable and productive tool.</p> <p>Matthew Peterson</p> <p>The use of voice recognition in a &#8220;mature&#8221; graphics application raises a number of issues that must be faced. &#8220;Mature&#8221; refers specifically to the maturity of the user interface in the graphics application. Commercial CAD/CAM systems provide such an application: they have been evolving for over ten years, they possess robust user interface features, and their features are subject to the test of the marketplace.</p> <p>Use of voice recognition in such applications raises a number of challenges to the technology:</p> <p>(1) How does it compare to existing features (e.g., tablet menuing) for servicing typical user interface needs?</p> <p>(2) Are there atypical or emerging needs for which it is particularly suitable?</p> <p>(3) How attractive is its price/productivity offering compared to alternative user investment strategies for increasing productivity?</p> <p>(4) In what directions should it be driven to better service the needs of commercial graphics applications?</p> <p>Material on present and future industry needs, other user interface features, and industry price trends will be presented to help assess these issues.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.2.7</cat_node>
				<descriptor>Speech recognition and synthesis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010179.10010183</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Natural language processing->Speech recognition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333307</person_id>
				<author_profile_id><![CDATA[81100510613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rabin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[V.P. Marketing/Business Development, Verbex Division, Exxon Enterprises]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P328923</person_id>
				<author_profile_id><![CDATA[81100420821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alan]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Strass]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program Manager - Voice Data Entry, General Electric Company]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332326</person_id>
				<author_profile_id><![CDATA[81545290256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Robillard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Manager, New Display Technology, Sanders Associates]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333874</person_id>
				<author_profile_id><![CDATA[81100528987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sue]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schedler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[I.C. Market Manager, G. E./Calma]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14176359</person_id>
				<author_profile_id><![CDATA[81538600856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Manager, Software Technology Products, Computervision]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL: Speech Recognition as a Computer Graphics Input Technique CHAIR: Richard Rabin V.P. Marketing/Business 
Development Verbex Division, Exxon Enterprises Interactive graphics systems typically require intense 
"hands busy/eyes busy and brains busy" activity on the part of the system user/operator. Voice input 
by means of automatic speech recognition equipment, offers major potential for improving user/operator 
productivity. It is the only input technique which does not require the direct use of hands and eyes. 
Voice input can replace or complement keyboards, func- tion keys, tablets and other types of input devices 
typically employed for entering commands, alpha and numeric data. Panelist: Alan R. Strass Program Manager 
-Voice Data Entry General Electric Company. Effective human interfaces are an essential element of plant 
information systems and computer integrated processes, such as graph- ics. In the past, data gathering 
choices in the factory have been generally dominated by clipboards and travelling punched card decks. 
Similarly, complicated keystroke sequences have often been required to evoke appropriate computer controlled 
functionality. Today, a number of advanced human interface techniques can be used to improve both source 
data capture and the selection of appropriate computer con- trolled operations. These techniques are 
becoming an integral part of many emerging on-line/real-time engineering and manufacturing applica- tions. 
 Speech recognition, in particular, is emerging as an important interface technology. Speech input can 
reduce the amount of attention the user has to spend on the mechanics of recording information of selecting 
functions and allows users to concentrate on their primary task. Some of the benefits include: (i) reduced 
user training time, (2) increased worker productivity, (3) reduced secondary key input, and (4) improved 
timeliness and accuracy of information made available via voice. Currently available speech recognition 
products have already been used to demonstrate these benefits. These advantages will only increase as 
the next generation of speech products deliver improved recognition performance. Panelist: Mark Robillard 
Manager, New Display Technology Sanders Associates There is an increasing awareness of the potential 
for improving operator productivity for graphic systems by the use of voice input. We have analyzed a 
number of applications in conjunction with poten- tial users to determine how best to use this capability. 
Initial con- clusions are that voice input can be effective, providing that the capabilities of the speech 
recognition equipment utilized to input voices are matched to the requirements of the applications. Key 
fac- tors to be considered are vocabulary size and types, and the use of isolated words versus continuous 
speech utterances. 179 Computer Graphics Volume 16, Number 3 July 1982 PANEL (continued): Speech Recognition 
as a Computer Graphics Input Panelist: Sue Schedler I.C. Market Manager  G. E./Calma  The addition 
of voice input to Calma's GBSII System provides users with a fast, accurate means by which they can execute 
commands. A customized Interstate Electronic VRM System has been integrated into the hardware configuration 
of all microelectronics products, Chips, Sticks and CARDSII. System software recognizes the input from 
the Har- mony VRM in the same manner as a keyboard or menu button input. All three input methods may 
be used separately or in conjunction with each other. All command inputs are buffered, so the user need 
never wait for a command to complete before entering the next. A voice file con- tains up to 50 words, 
and any number of files may be uploaded or down- loaded by one user. Both standard system commands or 
GPLII Programs can be executed by voice. On-line design productivity by menu input can be increased by 
50% when the operator uses voice. Operator training is unaffected by the addition of voice to the system. 
Voice is trained as a separate module in the class. Its greatest impact appears once the users know operation 
syntax and can communicate commands quickly. Calma provides standard voice menu to be used by customers 
or the customer may modify the file as needed. Most commonly used commands are put into the voice file 
leaving less frequently used commands for the onscreen menu. By implementing the combination of voice 
and menu, we can completely eliminate the need for keyboard input. Operators initially are skeptical 
of the "bell and whistle" feature, but find after using voice input that they do not feel uncom- fortable 
"talking" to the computer, and find it an enjoyable and pro- ductive tool. Panelist: Matthew Peterson 
Manager, Software Technology Products Computervision The use of voice recognition in a "mature" graphics 
application raises a number of issues that must be faced. "Mature" refers specif- ically to the maturity 
of the user interface in the graphics applica- tion. Commercial CAD/CAM systems provide such an application: 
they have been evolving for over ten years, they possess robust user inter- face features, and their 
features are subject to the test of the mark- etplace. Use of voice recognition in such applications 
raises a number of challenges to the technology: (i) How does it compare to existing features (e.g., 
tablet menuing) for servicing typical user interface needs?  (2) Are there atypical or emerging needs 
for which it is particularly suitable?  (3) How attractive is its price/productivity offering compared 
to alternative user investment strategies for increasing produc- tivity?  (4) In what directions should 
it be driven to better service the needs of commercial graphics applications?  Material on present 
and future industry needs, other user inter- face features, and industry price trends will be presented 
to help assess these issues. 180 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801279</article_id>
		<sort_key>181</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>28</seq_no>
		<title><![CDATA[Context-sensitive, graphic presentation of information]]></title>
		<page_from>181</page_from>
		<page_to>188</page_to>
		<doi_number>10.1145/800064.801279</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801279</url>
		<abstract>
			<par><![CDATA[<p>We begin by reviewing spatial data management&#8212;the technique of accessing and organizing information via its graphical representation in an organized spatial framework.</p> <p>We describe an operational prototype system that exceeds the capabilities of other spatial data management systems in two ways: (1) the graphical presentation of data is tailored to the user's identity, task, and database query; and (2) the system has the capacity for large databases. These capabilities are possible because the system dynamically generates its graphics environment.</p> <p>Our technique for dynamically generating the graphics environment relies on modeling of user context, semantic modeling of the underlying database, and direct use of knowledge about design layout and the utility of pictures. We briefly describe our current research efforts to extend this knowledge-based approach to automatic graphics environment synthesis.</p> <p>A videotape demonstrating the system accompanies this paper.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>H.1.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>H.2.m</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.2.1</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002951.10003227</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003241.10003243</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Decision support systems->Expert systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952</concept_id>
				<concept_desc>CCS->Information systems->Data management systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Management</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39037176</person_id>
				<author_profile_id><![CDATA[81100311571]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Friedell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America and Case Western Reserve University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331284</person_id>
				<author_profile_id><![CDATA[81331488682]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barnett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329779</person_id>
				<author_profile_id><![CDATA[81100095620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kramlich]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Corporation of America]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brachman, R.J., "What's in a concept: Structural foundations for semantic networks," International Journal of Man-Machine Studies 9, 127-152.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988424</ref_obj_id>
				<ref_obj_pid>988420</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Brown, D. and B. Chandrasekaran, "Design Considerations for Picture Production in a Natural Language Graphics System," Computer Graphics, 15, 2.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807391</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Donelson, W., "Spatial Management of Information," Proceedings of ACM Siggraph '78, Atlanta, Georgia.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>911150</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Friedell, M., "Automatic Graphics Environment Synthesis," Forthcoming Ph.D. Dissertation, Department of Computer Engineering and Science, Case Western Reserve University, Cleveland, Ohio 44106.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Greenfeld, N. and M. Yonke, "An Information Presentation System for Decision Makers," Artificial Intelligence Department, Bolt, Beranek, and Newman, Inc., Cambridge, Massachusetts 02138.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807470</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Herot, C., R. Carling, M. Friedell, and D. Kramlich, "A Prototype Spatial Data Management System," Proceedings of ACM Siggraph '80, Seattle, Washington.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Herot, C., G. Brown, R. Carling, M. Friedell, D. Kramlich, and R. Baecker, "An Integrated Environment for Program Visualization," Proceedings of the IFIP WG 8.1 Working Conference on Automated Tools, New Orleans, Louisiana, January, 1982.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Kahn, K., "Creation of Computer Animation from Story Descriptions," A.I. Technical Report 540 (Ph.D. Dissertation), Massachusetts Institute of Technology, Cambridge, Massachusetts 02139.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Context-sensitive, Graphic Presentation of Information Mark Friedell Computer Corporation of America 
and Case Western Reserve University Jane Barnett and David Kramlich Computer Corporation of America 
 Abstract We begin by reviewing spatial data management-the tech- nique of accessing and organizing information 
via its graphical representation in an organized spatial framework. We describe an operational prototype 
system that exceeds the capabilities of other spatial data management systems in two ways: (1) the graphical 
presentation of data is tailored to the user's iden- tity, task, and database query; and (2) the system 
has the capacity for large databases. These capabilities are possible because the system dynamically 
generates its graphics environment. Our technique for dynamically generating the graphics environ- ment 
relies on modeling of user context, semantic modeling of the underlying database, and direct use of knowledge 
about design layout and the utility of pictures. We briefly describe our current research efforts to 
extend this knowledge-based approach to automatic graphics environment synthesis. A videotape demonstrating 
the system accompanies this paper. CR Categories and Subject Descriptors: H.1.2 [Models and Princi- ples]: 
User/Machine Systems; H.2.m [Database Management]: Miscellaneous; 1.2.1 [Artificial Intelligence]: Application 
and Expert Systems; 1.3.3 [Computer Graphics]: Picture/Image Gen- eration; 1.3.4 [Computer Graphics]: 
Graphics Utilities; 1.3.6 [Computer Graphics]: Methodology and Techniques--Interaction techniques. This 
research was supported by the U.S. Naval Electronics Sys- tems Command under contract N00039-80-C-040 
and by the U.S. Defense Advanced Research Projects Agency under contract N00014-81-C-0456. The views 
and conclusions contained in this document are not necessarily those of the U.S. Government. Authors' 
address: Computer Corporation of America, 575 Technol- ogy Square, Cambridge, Massachusetts 02139 Permission 
to copy without fee all or part of this material is granted provided that the copies are not made or 
distributed for direct commercial advantage, the ACM copyright notice and the title of the publication 
and its date appear, and notice is given that copying is by permission of the Association for Computing 
Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. (~) 1982 ACM 
0-89791-076-1/82/007/0181 $00.75 1. Introduction In previous papers, researchers at the Massachusetts 
Institute of Technology [DONELSON] and at Computer Corporation of Amer- ica [HEROT, et al.] have explored 
the concept of spatial data management-the technique of accessing and organizing informa- tion via its 
graphical representation in an organized spatial frame- work. Prototype spatial data management systems 
have been built that successfully exploit computer graphics as a database access medium. However, this 
success is limited to a small number of applications using relatively small databases. These limitations 
are due principally to the large investment required to generate and store graphics environments. The 
costs of graphically presenting a large database are extremely high. Maintaining several different graphic 
representations of a single database-each tailored to a different task--is also impractical. Hence, the 
technique of spatial data management has been applied only to tasks that could benefit from the typical 
presenta- tion of a relatively small database. To move spatial data manage- ment out of the laboratory 
and into widespread use, the technical obstacles of graphics environment synthesis and management must 
be overcome. This paper reports on the development of the View System, a newly operational spatial data 
management system that offers an interesting solution to the problems of environment synthesis and management. 
The View System defines and generates its graphics environment dynamically, in response to the user's 
viewing task, database queries, and motion through space. This technology gives the View System the capacity 
to graphically present large databases in a way that is tailored to the user's identity and task. 2. 
The Spatial Data Management Concept Spatial data management is the technique of accessing data through 
their graphical representations. These representations are referred to as icons. Icons are arranged in 
two-dimensional infor-mation spaces (Ispaces). A typical spatial data management system is based on an 
interactive computer graphics system composed of color, raster-scan displays, touch-sensitive screens, 
and a joystick. Such a system allows the user to traverse an Ispace to arrive at information of interest, 
and to zoom in on individual icons to obtain greater detail. Spatial data management is motivated by 
the needs of a growing community of people who want to access information through a database management 
system (DBMS) but are not trained in the use of such systems. A database viewed through a spatial data 
management system is more accessible and its structure is more apparent than when viewed through a conventional 
DBMS. Users of conventional DBMSs can access data only by asking questions in a formal query language. 
In contrast, users of spatial data management systems benefit from the ability to access computer- resident 
information while retaining a familiar, visual orientation. Many types of questions can be answered without 
using a key- board and a conventional query language. By organizing information in a natural, spatial 
framework, spatial data management encourages browsing and requires less prior knowledge of the contents 
and structure of the database. A user 181  can find the information he needs without having to specify 
it pre- cisely or knowing exactly where in the database it is stored. 3. The View System The View System 
is a newly operational, spatial data manage- ment system developed at Computer Corporation of America. 
Unlike earlier spatial data management systems that present predefined Ispaces, the View System dynamically 
generates all Ispaces as they are required. The information used to create Ispaces--information about 
the database, user context, and graphi- cal presentation-is contained in an internal knowledge base. 
Since Ispaces are generated in real-time on an ad hoc basis, the View System is able to select from the 
knowledge base represen- tational graphics and layout conventions that are preferred by the user for 
his task. Furthermore, while the entire Ispace is always logically defined, only a small portion is physically 
instantiated at any time. This means that there is essentially no upper limit on Ispace size. In earlier 
systems, the need to store all Ispaces physi- cally limited both the size and number of available Ispaces 
and contributed substantially to the overall cost of the system. The View System is composed of two major 
subsystems: the Answer Space Generator and the View Generator. The Answer Space Generator interacts with 
the user through a graphical query interface to produce a description of each query's answer space. The 
term answer space refers to the data that the system selects to respond to the user's query. In addition, 
an answer space description includes the user's identity, overall task goal, and specific database query. 
The View Generator is then responsible for using this description and the data resulting from the query 
to create an Ispace. The user specifies a query to the Answer Space Generator and then views the Ispace 
created by the View Gen- erator as the query response. The user may repeat the process as desired. Below, 
Section 3.1 describes the model used by the entire View System to embody knowledge about user tasks and 
the database domain. Sections 3.2 and 3.3 describe the two subsystems comprising the View System. 3.1 
The Conceptual Model To ensure that the user need not be familiar with the database's structure, all 
knowledge about the nature of the data contained in the database is consolidated in a unified conceptual 
model. The conceptual model also contains (1) information that the View Sys- tem uses to decide how to 
look at and present the knowledge in the model under different circumstances and (2) advice for the View 
Generator about graphically displaying the data represented in the model. The conceptual model is based 
on an object-oriented knowledge representation, KL-ONE [BRACHMAN], and takes the form of a semantic network. 
Each node or concept in the network is a ge- neric description of a set of data entities contained in 
the data- base. A structured, spatial map of all the concepts in the network is presented to the user 
by the View System's graphical interface. This graphical display of the network is called the concept 
map. It is discussed further in Section 3.2. The definition of a concept is fully specified by the nodes 
related to it within the network. The nodes involved in describing a concept fall into two categories: 
those defining a concept's characteristics and those outlining how a concept should be viewed and operated 
on by various modules within the system. The nodes that define a concept's characteristics contain informa- 
tion about the concept's name and attributes, such as its parts, functions, and properties. The nodes 
that outline how a concept should be viegeed describe three aspects of the concept: 1. How it should 
be offered on the query menu. These nodes contain graphical and textual information that is used to de- 
pict the concept in the formatted menu displays. 2. How it should be instantiated. These nodes contain 
informa- tion indicating where and how to retrieve the set of database entities represented by the concept 
node. 3. How it should be displayed. These nodes contain graphics knowledge that is applied to the retrieved 
data values to form the icons that are displayed in response to the user's query.  The structure of 
the semantic network can best be understood by considering the two classes of relationships in which 
every con- cept may participate: 1. Inheritance Relationships: relationships imposed by the superclass/subclass 
hierarchy or hierarchies embodied in the semantic network 2. Definitional Relationships: relationships 
between a concept node and those nodes that have a part in directly describing the nature of the concept 
  Each concept in the network is a member of at least one hierar-chy of nodes. Because the conceptual 
model is a network, a con- cept can participate in more than one hierarchy. This flexibility in the model 
makes possible alternative representations of a data entity without requiring duplication of the central 
nodes involved in representing that entity. Each concept within the semantic network can be classified 
as simple or complex. Simple concepts do not have attributes. Sim-ple concepts generally are used to 
represent entities within the database that have a value. Complex concepts do have attributes, which 
can in turn be simple or complex. Complex concepts are the ones that the user can select for viewing. 
Although not fully illustrated in the example used for this paper, the rich network structure used in 
the View System's knowledge representation allows a wide variety of relationships between data entities 
to be modeled and presented to the user. 3.2 The Answer Space Generator The Answer Space Generator is 
divided into three modules: the Menu Manager, the Answer Space Analyzer, and the Data Manager, as shown 
in Figure 3.1. The Menu Manager provides the user with a spatially oriented view of the types of information 
contained in the database. It also provides a query menu that allows queries to be specified without 
the use of a formal query language. The Answer Space Analyzer combines the user's query specification 
with knowledge about the user's viewing preferences, task goals, and other information gathered throughout 
the query session. Its output is a description of the answer space as it should be presented to the user. 
This description, along with the data retrieved and organized by the Data Manager, is passed to the View 
Generator. Answer Space Generator Figure 3.1 The Answer Space Generator Modules The graphical menu interface 
allows the user to move about in the conceptual model's network structure and to specify queries in terms 
of the concepts described in the network. This interface consists of formatted displays describing the 
data available to the user for retrieval and indicating the functions for specifying a query. The displays 
are presented on two color monitors, part of the View System workstation shown in Figure 3.2. The complete 
workstation consists of three color, raster scan, frame buffer displays; a joystick; a data tablet; and 
a keyboard. The displays show different kinds of information depending on whether the user is specifying 
a query or viewing an Ispace. The menu monitors--the left and right displays in the figure-are equipped 
with touch-sensitive screens. By touching the screens, the user 182   Plan applicability patterns are 
used by the Icon Plan Matcher to choose the best icon plan for the current icon creation task. The procedure 
for choosing the best plan is based on best-first search through a graph-like structure referred to as 
a situation space. The situation space is a performance-oriented, knowledge-indexing scheme developed 
for the View System. The structure of the situation space facilitates efficient search for the knowledge 
sources--in this case, icon plans-that are most relevant to the current problem-solving situation. The 
situation space takes the form of a directed graph. Each node, or situation, is defined by the following 
4-tuple: 1. User identity 2. User task 3. Entity type 4. Attribute list  Each situation indexes one 
or more plans. A plan is indexed by the situation that is prescribed by its applicability pattern. There 
is an edge in the situation space from situation A to situa- tion B if exactly one of the following conditions 
is met: 1. Entity type A is a subconcept of entity type B 2. Attribute list A is a superset of B with 
exactly one additional attribute 3. User identity A is not user identity B 4. User task A is not user 
task B  Edge-traversing costs are assigned to represent the differences between the two situations connected 
by an edge. An example situation space is shown in Figure 3.12. The Icon Plan Matcher uses the following 
algorithm to search for the best plan in the situation space: Step h Define the candidate plan as the 
null plan and assign the "'cost" of using it to be infinite. Define the home situa- tion as the situation 
that most uniquely characterizes the current icon creation task. Assign the base cost of the home situation 
to be zero, and place it in the open set. Step 2: If the open set is empty, return the candidate plan. 
Captain | Watch Officer I I ~ nltlonll~y I mime 2* tl, nat&#38;onlllly SHIP I SHIP CRU,SE___.._%. CRt.SE_.__.__~R 
" Cmt.SE__.__a. CRt.SJ_. I I Figure 3.12 An Example Situation Space Step 3: Define the active situation 
as the situation in the open set with the lowest base cost. Remove the active situation from the open 
set. Step 4: If the base cost of the active situation is greater than or equal to the cost of the candidate 
plan, return the candi- date plan. Step 5: Compute the cost of each plan indexed by the active situation, 
using the following heuristic: The cost of each plan is the base cost of the active situation plus the 
cost of mismatching attribute importance. This mismatching cost is the sum of (1) the priority rankings 
of all attributes required by the icon creation task but not part of the plan's applicability pattern, 
plus (2) the number of inter- changes needed to bubble sort the plan's attribute prior- ity list into 
the sequence specified by the icon creation task. If the cost of using any plan is less that that of 
the candidate plan, it becomes the new candidate plan. Step 6: Compute the base cost of each successor 
of the active situation as follows: The base cost of each successor situa- tion is the base cost of the 
active situation plus the cost of traversing the edge from the active situation to the suc-cessor situation. 
If the base cost of any situation is less than the cost of the candidate plan, add it to the open set. 
Step 7: Go to step 2. The edge-traversing costs are the key elements of the plan-matching heuristic. 
They provide a means for computing the degree of compromise associated with using a plan intended for 
a different situation. 3.3.3 The leon Creator The Icon Creator constructs icons by interpreting icon 
plans. Its operation is directed by the Layout Manager, as discussed previ- ously in Section 3.3.1. Each 
request issued to the Icon Creator includes an alphanumeric description of the entity to be represented 
by the icon, the desired location of the icon in the Ispace, and an icon plan (provided by the Icon Plan 
Matcher) for constructing the icon. The output of the Icon Creator is an icon description encoded for 
processing by the Stager/Navigator. The Icon Creator is composed of two co-routines: the monitor and 
the filter. The monitor interprets instructions in the icon plan for determining those portions of the 
icon that depend on specific data values being represented. The filter copies precomputed icon segments 
from the plan to the icon under construction. These segments are portions of the icon that are determined 
by data identity (not data value) and are therefore invariant for all icons constructed from a single 
plan.  3.3.4 The Ispace Manager The Ispace Manager is a database system for maintaining the Ispace as 
a logical graphics environment. It is responsible for maintaining the instantiated Ispace cache as required 
to present the illusion that the entire Ispace is physically available. Access routines are provided 
for installing and retrieving icons anywhere in the logical Ispace, at any time. The Ispace Manager is 
used by the Icon Creator and the Stager/Navigator. After each icon is constructed by the Icon Creator, 
it is installed in the Ispace database. As the View System user moves through the Ispace, icons are retrieved 
by the Stager/ Navigator, images of these icons are computed, and the images are fed to the frame buffer 
displays. The Ispace Manager partitions the logical Ispace into a grid of square Ispace blocks. The Ispace 
cache area is a rectangular region of the Ispace aligned on the grid. Each Ispace block within the cache 
has associated with it an inverted list of identifiers of icons that overlap the block. Icons that cross 
Ispace block boun- daries exist in two or more inverted lists. This is shown in Figure 3.13. Before an 
icon is inserted into an Ispace, it is copied into a storage area and assigned a unique identifier. This 
identifier is added to the inverted lists associated with the blocks in which the icon exists. 186 
A B ~,01 A . ir I I I C D "i Figure 3.13 An Icon in Multiple Ispace Blocks Icon retrieval is specified 
in terms of the rectangular extents of a query window. A list of all retrieved icons is computed as the 
union of the inverted lists of all Ispace blocks that the query win- dow overlaps. The size and location 
of the Ispace cache changes to meet cach- ing requirements: if an icon is to be inserted into an Ispace 
block that is not part of the cache, the appropriate vertical and/or hor- izontal cache boundary is extended 
to include that block. Extend-ing the cache area is shown in Figure 3.14. If an attempt to insert an 
icon fails due to a lack of storage space for the icon, the trailing boundaries of the cache area are 
retracted until the necessary space becomes available. Retraction of cache area boundaries takes place 
in one row or column of blocks at a time. After each row or column is retracted, storage space is reclaimed 
for each icon that is now completely outside the cache area. Retracting the cache area is shown in Figure 
3.15. Trailing boundary of cache retracted to reclaim space for icon A Figure 3.15 Retracting Cache Area 
Boundaries    3.3.5 Stager/Navigator The Stager/Navigator is responsible for merging the icons con- 
tained in the Ispace cache with a background and feeding the result to the display screen. Backgrounds 
can be blank or can contain information, such as a map. As the user scrolls over an Ispace, the Stager/Navigator 
feeds the appropriate portion of the Ispace background, together with any icons in that portion, to the 
display screen. It queries the Ispace cache for any icons in the area that it is feeding. If there are 
icons in the query window, the icon descriptions, as generated by the Icon Creator and stored in the 
cache, are passed to the Image Generator. The Image Generator interprets the graphics primitives generated 
by the Icon Creator from the icon plan and produces pixels that are merged with the background. The merged 
foreground/background is then fed to the display. A B C Cache boundaries extended from B to C to accomodate 
icon A Figure 3.14 Extending Cache Area Boundaries 4. Future View System Research Knowledge engineering 
techniques, even such basic ones as those employed in this work, are relatively new to computer graphics. 
Only a few examples can be readily found in the litera- ture [KAHN] [BROWN &#38; CHANDRASEKARAN] [GREENFELD 
&#38; YONKE]. Nonetheless, much of the View System's ability to gen- erate graphical information spaces 
on demand is the result of direct representation and use of knowledge about user identity, user task, 
database semantics, and the utility of different types of representational graphics and layout schemes. 
The Man-Machine Interfaces Section at Computer Corporation of America is currently engaged in a research 
project to fully automate the synthesis of icon plans through a knowledge-engineering approach. This 
work involves techniques specially developed for computer graphics applications [FRIEDELL]. We hope that 
this research, in conjunction with the work reported here, will lead to a widely applicable technology 
for automatic graphics environment synthesis.  5. Other Applications for Dynamic Environment Generation 
Spatial data management is not the only application in which large graphics environments must be generated 
and managed. In almost all such graphics applications, the costs of describing, stor- ing, and managing 
the environment represent a major portion of the overall investment. New technologies for automating 
the syn- thesis and management of graphics environments are needed if computer graphics is to be fully 
exploited. We believe our approach to dynamically generating environments is one of these technologies. 
 187 In the immediate future, we plan to apply the techniques described in this paper to the problem 
of generating animated illustrations of computer processes. This work will be part of a long-term effort 
to develop a program visualization system-a sys- tem for graphically examining and manipulating large, 
executing computer programs [HEROT, BROWN, et al.]. We also expect to consider the problems associated 
with gen- erating large, realistic, three-dimensional environments for simula- tion systems, and the 
problem of specifying long sequences of frames for lengthy animation segments. 6. Acknowledgements This 
paper reflects research contributions by Gretchen Brown, Richard Carling, Chris Herot, Diane Smith, and 
Gerry Wilson at Computer Corporation of America. Richard Carling also contrib- uted substantially to 
the implementation of the View System. The authors wish to thank Ronni Rosenberg and Gretchen Brown for 
their assistance in editing this paper for both technical content and style. 7. Referenees [BRACHMAN] 
Brachman, R.J., "'What's in a concept: Structural foundations for semantic networks," International Journal 
of Man-Machine Stu- dies 9, 127-152. [BROWN &#38; CHANDRASEKARAN] Brown, D. and B. Cbandrasekaran, "Design 
Considerations for Picture Production in a Natural Language Graphics System," Computer Graphics, 15, 
2. [DONE, SON] Donelson, W., "Spatial Management of Information," Proceedings of ACM Siggraph "78, Atlanta, 
Georgia. [FRIEDELL] Friedell, M., "'Automatic Graphics Environment Synthesis," Forthcoming Ph.D. Dissertation, 
Department of Computer Engineering and Science, Case Western Reserve University, Cleveland, Ohio 44106. 
[GREENFELD &#38; YONKE] Greenfeld, N. and M. Yonke, "An Information Presentation Sys- tem for Decision 
Makers," Artificial Intelligence Department, Bolt, Beranek, and Newman, Inc., Cambridge, Massachusetts 
02138. [HEROT, et at.] Herot, C., R. Carling, M. Friedell, and D. Kramlich, "A Proto- type Spatial Data 
Management System," Proceedings of ACM Siggraph "80, Seattle, Washington. [HEROT, BROWN, et al.] Herot, 
C., G. Brown, R. Carling, M. Friedell, D. Kramlich, and R. Baeeker, "'An Integrated Environment for Program 
Visualiza- tion," Proceedings of the IFIP WG 8.1 Working Conference on Automated Tools, New Orleans, 
Louisiana, January, 1982. [KAHN] Kahn, K., "'Creation of Computer Animation from Story Descrip- tions,"A.I. 
Technical Report 540 (Ph.D. Dissertation), Mas-sachusetts Institute of Technology, Cambridge, Massachusetts 
02139.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801280</article_id>
		<sort_key>189</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>29</seq_no>
		<title><![CDATA[Graphical tools for interactive image interpretation]]></title>
		<page_from>189</page_from>
		<page_to>198</page_to>
		<doi_number>10.1145/800064.801280</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801280</url>
		<abstract>
			<par><![CDATA[<p>This paper describes BROWSE, an interactive raster image display facility which is a major component of a larger integrated Map Assisted Photo-interpretation System (MAPS), being developed as a prototype interactive aid for photo-interpretation. Application areas for this research include image cartography, land use studies and reconnaissance, as well as image database organization, storage, and retrieval.</p> <p>BROWSE is a window-oriented display manager which supports raster image display, overlay of graphical data such as map descriptions and image processing segmentations, and the specification and generation of 3D shaded surface models. Digitized imagery from black and white and color aerial mapping photographs is displayed by BROWSE at multiple levels of resolution and allows for dynamic positioning, zooming, expansion or shrinking of the image window. Map data represented as vectors and polygons can be superimposed on the imagery through image-to-map registration. Access to collateral map databases and terrain models may be accomplished using the BROWSE graphical interface. Finally, the window representation gives a convenient communication mechanism for passing image fragments to image interpretation programs, which generally run as separate processes. The results of such processing can be returned to BROWSE for further processing by the user.</p> <p>We will discuss the rationale behind the design of BROWSE as well as its application to domains including aerial photo-interpretation and 3D cartography.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Cartography]]></kw>
			<kw><![CDATA[Image database]]></kw>
			<kw><![CDATA[Map representations]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
			<other_category>
				<cat_node>I.4.9</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
			<gt>Design</gt>
			<gt>Experimentation</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329793</person_id>
				<author_profile_id><![CDATA[81100056878]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Mckeown]]></last_name>
				<suffix><![CDATA[Jr.]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Carnegie-Mellon University, Pittsburgh, Pa.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331369</person_id>
				<author_profile_id><![CDATA[81332495851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jerry]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Denlinger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, Carnegie-Mellon University, Pittsburgh, Pa.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ball, J. Eugene. Canvas: the Spice graphics package. Tech. Rept. Spice Document S108, Carnegie-Mellon University, Pittsburgh, PA., 1980.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Grant, Eric. AGP: A Graphics Package. Unpublished project report, Carnegie-Mellon University, Pittsburgh, PA., 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[IEEE Workshop on Picture Data Description and Management, Asilomar, Ca., August, 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[IEEE Workshop on Computer Architecture for Pattern Analysis and Image Database Management, Hot Springs, Va., November, 1981.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[IEEE Computer Magazine, Special Issue on Pictorial Information Systems, November, 1981.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Lukes, G. E. Computer-assisted photo interpretation research at United States Army Engineer Topographic Laboratories (USAETL). Techniques and Applications of Image Understanding III, Society of Photo-Optical Instrumentation Engineers, Washington, D.C., April, 1981, pp. 85-94.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[McKeown, D.M., and J.L. Denlinger. BROWSE: A Generalized Image Display Facility Users Guide. Image Understanding Group Memo 6, Carnegie-Mellon University, Pittsburgh, PA., June, 1980.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[McKeown, D.M. Knowledge Structuring in Task Oriented Image Databases. Proceedings of the IEEE Workshop on Picture Data Description and Management, Asilomar, Ca., August, 1980, pp145-151.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[McKeown, D. M. and T. Kanade. Database Support for Automated Photo Interpretation. Techniques and Applications of Image Understanding III, Society of Photo-Optical Instrumentation Engineers, Washington, D.C., April, 1981, pp. 192-198.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[McKeown, D.M. Generation of a Map Database for Aerial Photo-Interpretation. Proceedings of the IEEE Workshop on Applied Imagery Pattern Recognition, September, 1981.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[McKeown, D.M. Concept Maps. Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA., 1982. in preparation]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Reddy, R., and G. Gill. Representation Complexity of Image Data Structures. Proceedings: DARPA Image Understanding Workshop, May, 1978, pp. 28-30.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Shafer, S. The CI Command Interpreter. Computer Science Department, 1980. Entry ci, chapter 3. CMU Update: UNIX Programmer's Manual (c) Bell Laboratories, 1980.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Smith, D. R. CMU Image Format and Paging System. Image Understanding Group Memo 3, Carnegie-Mellon University, Pittsburgh, PA., November, 1980.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Sproull, R. F. Raster Graphics for Interactive Programming Environments. Tech. Rept. CSL79-6, Xerox Palo Alto Research Center, 1979.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Teitelman, W. A Display Oriented Programmer's Assistant. Tech. Rept. CSL77-3, Xerox Palo Alto Research Center, 1977.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Graphical Tools for Interactive Image Interpretation David M. McKeown, Jr. Jerry L. Denlinger Department 
of Computer Science Carnegie-Mellon University Pittsburgh, Pa. 152131 A bst ract This paper describes 
BROWSE, an interactive raster image display facility which is a major component of a larger integrated 
Map Assisted Photo-interpretation System (MAPS), being developed as a prototype interactive aid for photo-interpretation. 
Application areas for this researchinclude image cartography, land use studies and reconnaissance, as 
well as image database organization, storage, and retrieval. BROWSE is a window-orientcd display manager 
which supports raster image display, overlay of graphical data such as map descriptions and image processing 
segmentations, and the specification and generation of 3D shaded surface models. Digitized imagery from 
black and white and color aerial mapping photographs is displayed by BROWSE at multiple levels of resolution 
and allows for dynamic positioning, zooming, expansion or shrinking of the image window. Map data represented 
as vectors and polygons can be superimposed on the imagery through image-to-map registration. Access 
to collateral map databases and terrain models may be accomplished using the BROWSE graphical interface. 
Finally, the window representation gives a convenient communication mechanism for passing image fragments 
to image interpretation programs, which generally run as separate processes. The results of such processing 
can be returned to BROWSE for further processing by the user. We will discuss the rationale behind the 
design of BROWSE as well as its application to domains including aerial photo-interpretation and 3D cartography. 
CR Categories and Subject Descriptors: 1.3.4 [Computer Graphics]: Graphics Utilities -application packages; 
graphics packages; 1.4.8 [Image ProceSsing]: Scene Analysis; 1.4.9 [Image Processing]: Applications General 
Terms: Algorithms, Design, Experimentation Additional Key Words and Phrases: cartography, image database, 
map representations Permission to copy without fee all or part of this material is granted provided that 
the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and 
the title of the publication and its date appear, and notice is given that copying is by permission of 
the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. (~ 1982 ACM 0-89791-076-1/82/007/0189 $00.75 1. Introduction There is increasing research 
activity in the area of digital image interpretation and image understanding, particularly in pictorial 
database management and map data processing [3, 4, 5]. A common research theme is the extraction of information 
from digital imagery (image interpretation), the maintenance of a large number of raster images and associated 
vector descriptions (pictorial database management), and the creation and/or updating of cartographic 
information (map data processing). While research in these areas straddles several disciplines, such 
as image processing, database and information systems, computational geometry, and artificial intelligence, 
computer graphics plays a major role in the display of remotely sensed imagery and cartographic data, 
and in image processing and interpretation. In this paper we describe the computer graphics aspects of 
a prototype interactive photo-interpretation workstation, MAPS (Map Assisted Photo-interpretation System). 
The raster graphics component, BROWSE [7], has been designed as a user front-end for interactive photo-interpretation 
of high resolution aerial photographs, but the design principles and implemented facilities have been 
used by researchers in a variety of application areas, such as sonar image analysis, natural indoor scene 
interpretation, and robotics tasks. We will mainly discuss how BROWSE provides raster and vector graphic 
display facilities that allow the user to display, manipulate, and process digitized imagery and map 
data from a large scale database centered over Washington, D.C. [8, 9]. 2. Window-O riented System BROWSE 
is a window-oriented display facility. The concept of a window-based raster display system is not new. 
It has been used in several systems to display text, line drawings, and binary bit mapped images [16, 
1]. In thcse systems the advantage of multiple windows, with arbitrary size and position, each usually 
maintaining the context of a different processing task, is in the ease with which they allow a user to 
switch his attention between the tasks at hand. Features such as rapid refresh for window movement and 
update, automatic reduction of window size while inactive, and the maintenance of partial information 
concerning the "state" of the window process, combine to provide a powerful and flexible man-machine 
interface. In Teitclman's system[16] asynchronous communications,, such as notification of mail arrival, 
messages from other users on the system, ]This research w~ sponsored by the Defense Advanced Research 
Projects Agency (DOI)). ARPA Order No. 3597, and monitored by the Air Force Avionics Laboratory under 
Contract F33615-78-C-1551. "lhe views and conclusions in this document are those of the author and .should 
not be interpreled as representing the official policies, either expressed or implied, of the Defense 
Advanced Research Projects Agency or the U.S.Government. and output from background jobs, were nearly 
packaged and displayed. 2.1. Windows Increase Spatial Resolution We have applied and extended the window 
paradigm to handle high resolution, multi-hit per pixel digitized images. Our objective is to allow for 
the simultaneous display of multiple windows into different digitized images. As in previous work, such 
a systcm provides for the maintenance &#38;multiple contexts, and is an effective medium for man- machine 
communication. However, due to nearly an order of magnitude difference in the amount of data needed to 
perform screen updates and due to processing limitations found in most frame-buffer architectures, many 
of the solutions used for single bit per pixel displays [15] are not suitable for direct implementation. 
Given current frame-buffer technology, it is not possible to view even major portions of an entire image 
at full resolution. 2 In our system typical raster image sizes are between 2048x2048 and 4096x4096 pixels. 
However, in some application areas even larger rasters are common. Black and white aerial mapping imagery 
is usually digitized to 8 bits per pixel, so an image requires between 4 and 16 megabytes of online disk 
storage. Color or multi-spectral data multiplies .the file size requirement by the number of additional 
spectral bands. More irnportantly, one generally would like to review several images side by side, with 
enough context to observe how the images overlap or relate to each other. Enough detail must be provided 
to perform meaningful analysis, such as detection of small temporal changes of image features. These 
requirements arise in a variety of image processing application areas, including interactive stereo analysis, 
time sequence analysis, change-detection tasks, and image selection from a menu of database image fragments. 
We believe that the image window paradigm increases the effective spatial resolution of the frame buffer 
by allowing the user to dynamically manipulate the size, level of detail, and visibility of imagery. 
 2.2. Multi-Resolution Imagery Since feature size (buildings, roads, airports, etc.) varies greatly, 
it is often the case that lower resolution images, constructed by convolution filtering, are adequate 
to provide the necessary spatial resolution. Hence, an important characteristic for an interactive photo-interpretation 
system is the ability to image zoom through a hierarchy of image resolutions, tn MAPS each of our 40 
Washington, D.C. images is stored in a hierarchy of reduced resolutions (2:1, 4:1, 8:1, etc.) which allows 
the full image context to be displayed at some resolution. BROWSE allows a user to establish quickly 
windows with appropriate levels of detail, while maintaining a spatial context in low resolution windows. 
3. Frames, Windows, Images, Segmentations The BROWSE display manager allocates and manipulates four entities: 
frames, windows, images, and segmentations. In this section we will ~lescribe the user's view and the 
function of each entity.  3.1. Frames Users view frames as containers of image windows that aggregate, 
in a single display, features or landmarks of interest from many different images. Users give frames 
symbolic names, which may describe the image being displayed (ie. DC38617), the name of a cartographic 
feature (KENNED Y CENTER), or simply allow for unique naming (A FRAME, B FRAME....). 2Of course, there 
are frame buffe~ capable of 1024x1024 resolution. We are mainly concerned with the limitations of standard 
video compatible 525-line display systems. In the BROWSE implementation, a frame is an allocation of 
bit planes from the frame-buffer memory, which has 32 bit planes dimensioned as 512x512 pixels. The number 
of active frames is limited only by frame buffer memory; however, only one frame may be displayed at 
a time. Frame depth is variable, and can range from a single bit to a maximum of eight bits, effectively 
limited by the 8x8 video lookup tables and efficient utilization of our VAX-11/780 DMA interface. Assuming 
digitized images of 8 bits per pixel, we can simultaneously allocate four frames. Variable depth color 
frames u.p to 24 bits per pixel (8 bits per primary RGB) are also fully supported. BROWSE allows the 
user to control which frame is currently displayed, or to specify a frame display sequence. Frame toggling 
is accomplished by manipulating the frame buffer lookup table hardware.  3.2. Windows A wOldow is a 
rectangular portion of an image stored in frame buffer memory and is displayed in a frame. Each window 
is referred to by a symbolic name assigned by the user when the window is created. Windows are the basic 
unit that BROWSE manipulates through its command structure. Some typical window operations are shrink, 
expand, zoom, adjust, mot,~ and copy. We will describe user-level window command syntax in section 5 
and associated data structures in section 4. 3.3. Segmentations Segmentation files are used to store 
image segmentations which are organized as a list of features. Each feature is described by a list of 
vectors fomling an outline of the fcature in the image. The file also contains information such as the 
number of features and their symbolic feature names. Feature names, supplied by the user, are used to 
index into a database of textual fcature descriptions. Segmentations are generated in a variety of ways: 
interactive human segmentation, machine segmentation, and use of image-to-map correspondence to project 
existing segmentation descriptions onto other images. Segmentations are normally associated with a particular 
image, although several of our databases also use segmentation files to define map features. For map 
fcatures, the coordinate system is latitude, longitude, and elevation rather than pixel coordinates. 
The typical output of automatic image-segmentation programs is a rcgion patch image or mask image where 
the pixel value represents the region number associated with the pixel. Using commands available in BROWSE, 
it is possible to convert these mask images to a vector boundary list and superimpose the outlines of 
segmentation features on top of the original image. Using the frame-buffer overlay memory, up to four 
unique segmentation colors may be used and displayed concurrently.  3.4. Image Organization The Image 
Understanding Group at Carnegie-Mellon has adopted a block paged representation for large raster images 
after careful study of alternative representations, such as two dimensional array, dope vector, and row 
paged organizations. An analysis of representation complexity is given in Reddy and Gill [12] and a detailed 
implementation description is given by Smith [14]. Block structuring partitions the image into a collection 
of non-overlapping subimage rectangles. The size of each subimage block is chosen to be equal to (or 
a multiple of) the memory page size. In our case, page size is 1024 bytes, organized as 32 horizontal 
rows by as many pi~cels as will fit in 32 bytes. Pixels are bit packed in the row, and blocks are padded 
out to use whole pages only. For low-level image processing, such as histogramming and convolution operations, 
simple access methods, such as dope vector and row paged may show performance advantages over the block 
 in this manner so we could support a different frame buffer or alternative user interfaces such as 
menus. Further, the BROWSE subroutine package forms a library of routines that may be used in whole or 
in part to configure special purpose BRO WSE-like programs. ]'he user interface routines retrieve commands 
and arguments from the user and make calls to the BROWSE subroutine package to modify the screen or 
perform other functions such as printing a list of the windows contained in a frame. When screen coordinates 
are required, they are retrieved by these rou tines using any one of several devices set by the user, 
including tablet, keyboard, and joystick. The BROWSE subroutine package is called by the user interface 
routines and in turn calls the lower level graphics primitives, Decoupling the user interface from the 
BROWSE subroutine package allows these routines to be used where parameters for a command are known in 
advance, such as, when creating a window containing the intermediate results of some image-processing 
operation. These routines are designed so that it is transparent as to whether they are invoked through 
user interaction, from another program, or from a command file, Window package routines save windows 
in temporary files and maintain the low-level display state for re-generation of the screen. This state 
information includes the relative depth of each window, the number of windows in a frame and the positions 
of windows on the screen. The window package routines are written to support general purpose windowing 
for arbitrary graphics. The window package includes routines for defining a rectangular portion of the 
screen as a window and for simple window operations such as move and delete. The lowest level of software 
is the frame-buffer dependent graphics primitives, These routines perform frame allocation and manipulation, 
hardware cursor control, graphic overlay manipulation and vector drawing. These routines also make calls 
to image access routines to read data from image files and write it to the frame-buffer. User Interface 
Browse Subroutine oclooe Window Package Graphic! Primitives Image Access Package Figure 2 shows the 
software layering of the BROWSE graphics package at each level for the command addwindow. 4.2. Data 
Structures Data structures and window operations have been designed to give interactive users a large 
amount of freedom in deciding which images should be windowed and where the windows should be displayed 
on the screen. Users will typically create windows that contain an entire image at a low resolution, 
then zoom smaller fragments to a higher resolution to examine some particular area more closely. It is 
often desirable to view different images of the same area side by side, possibly with the results of 
some image processing overlaid. Since the display screen is only large enough to show a small amount 
of the information contained in several images, it is usually necessary to adjust the size and position 
of windows until all the necessary information is visible. Display State Information. In order to move 
and delete windows, BROWSE must record the state of the display. This state includes the image and window 
names, the position of the windows, and their relative depth within the frame. Each window in a frame 
has a unique depth which is higher for windows that have been added or re-drawn more recently. Window 
depth is a metric that determines which window is occluded when there is overlap. The depth information 
is needed in order to re-generate the portion of the screen that becomes visible whenever a window is 
moved or deleted. Additional information, such as which image is in the window, image bounds, and any 
associated segmentations, must also be maintained to allow for updating the display. Frame Structures. 
Figure 3 shows how the frame, window, image, and segmentation structure information is organized. Frame 
structures contain the frame name, the number of windows, and a list of the windows in the frame, sorted 
according to depth. The lowest depth B_addwindow(command string) Retrieves the name for the new window, 
the image to use, the image bounds, and the screen position from the user. b_addwindow(frame, name. image, 
srow, erow. scol, ecol, row. col) Sets up high level structure information such as window name and image 
displayed for the new window and calls lower level graphics routine to add the window. w_addwindow (frame, 
stow, scol, emw, ecoi) Saves the window and sets up window structure information such as depth in the 
frame and maintains low level window information such as the number of windows in a frame. g_rdimg (image, 
frame, srow, scol, erow, ecoL row, col) Calls g_getbox to get a rectangular portion of an image file 
and writes it to the screen. ggetbox (#nage, srow, erow. scol, ecol, buffer) Reads a rectangular portion 
of an image file into a buffer. Figure 2: Levels of Routines Used for Addwindow window in the frame is 
the first window in the list. When windows are added, they are put at the end of the list to eliminate 
the need to re-sort the list of windows or move all of the window pointers when a window is added. In 
window operations such as raiscwindow, which change the relative depth of a window, the list of windows 
must be re-ordered to put the newly re-drawn window at the end of the list. Window Structures. The data 
structure for a window contains the window name, a pointer to the image data structure, the row and column 
bounds of the image fragment which is visible in the window, and any segmentations associated with the 
window. Since images are associated with windows, not frames, it is possible to display fragments from 
more than one image in the same frame. Associating segmentations with windows instead of images makes 
it possible to view different segmentations of the same image in different windows and to select whether 
a segmentation will be displayed in a particular window. Different segmentations may be the result of 
different methods of processing on the same image. A list of overlay colors to use in drawing each feature 
is also kept for each segmentation. An overlay color can be associated with each feature in the segmentation 
file allowing users to highlight a particular important feature within a window. However, users normally 
use a single overlay color for all features in the same segmentation file. Window structures contain 
the frame that contains the window and the depth of the window in the frame. While the frame and depth 
are rcdundant with information in the frame structure, they simplify access in cases where it is necessary 
to determine the frame or depth of a given window. For example, many BROWSE commands take windows as 
arguments but also need the frame. Rather than searching through the frames to find the one that contains 
the given window, the frame is extracted from the window structure. Image Structures. The structures 
maintained by BROWSE for images contain the image file name and a symbolic name supplied by the user. 
A pointer to an image file structure is also stored in the image structure. The image file structure 
contains information such as the number of rows and columns in the image, the operating system descriptor 
for the open file, and an in-core page table used in accessing image files. Segmentation Structures. 
The structure for a segmentation contains the file name of the segmcntati.on, a symbolic name supplied 
by the user, and a list of the features in the segmentation. Features consist of a feature name and a 
polygon, line, or point description that defines the feature. 4.3. Driving the Frame-Buffer When a window 
is updated, the entire screen area occupied by the window is rewritten with a single block transfer to 
minimize the number of writes to the frame buffer. In section 5.3, an example of window deletion describes 
the restoration of the display behind the window as the composition of image fragments. The process of 
composing the buffer in memory is considerably faster than writing the intersecting portion of each image 
fragment to the frame-buffer individually. The primary motivation is aesthetic. Under normal timesharing 
loads a full screen display update takes less than 2 seconds of real-time. Small window updates appear 
to refresh instantly. Current frame-buffer communication bandwidth limitations, scan-line oriented addressing, 
and time-sharing operating system overhead make the performance of sequential update of image fragments 
unsatisfactory. Thus, the low-level graphics and image access packages support composition of in-core 
window buffers and block transfer I/O both for aesthetic and performance reasons. FRAME WINDOW name numwindows 
window window window t -me image -image bounds ,frame depth in frame display position segmentation , 
feature colors segmentation feature colors IMAGE fi le name symbolic name image file pointer SEGMENTATION 
file name tTatu re Figure 3: Organization of Structure Information 5. User Interface Commands In this 
section we discuss the user-interface commands used to manipulate frames, windows, images, and segmentations. 
There are some forty commands currently implemented in the BROWSE library, we will discuss the most frequently 
used subset. By convention, the frame currently being displayed is used as the default frame, however 
a window-name command argument may be in any active frame. Specification of windows can be done by naming 
the window or by tracking hardware cursors in the frame-buffer by using a tablet, joystick, or keyboard 
interface in the current display frame 5.1. Frame Operations define frame-name frame-type byte-size Allocate 
a new frame. The name to give the frame, the type (BW or RGB) and the number of bits per pixel are required. 
After a frame has been defined, windows can be added and manipulated. untie fine frame-name The specified 
frame is freed. All windows in the frame are deleted. The frame buffer memory that had been allocated 
for the frame is released. chgdisp frame-name The specified frame is displayed. Users can toggle back 
and forth between different frames with chgdisp. 5.2. Image Operations openimgfile-name symbolic-name 
image-type A new image file is opened. A symbolic name is associated with the image for future, use since 
file names tend to be long, cumbersome, and difficult to remember. closeimg symbolic-name An image file 
is closed. Since UNIX limits the number of file descriptors available to a process (~30), image files 
are closed and re-opened as necessary by BROWSE to increase the number of images that can be maintained, 
q%e number of images is limited only by internal data structures (-150), and thus, images are rarely 
closed by the user. 5.3. Window Operations addwindow [frame] window-name image-name left-corner right-corner 
display-position A window is added to a frame. The user must specify the symbolic name of the image to 
be associated with the window, the portion that should be visible, and the desired display position. 
If the display position specified would put part of the window off of the screen, the window position 
is adjusted so that the edge of the window is flush with the edge of the display screen. When a window 
ks added, it becomes the highest window in the frame and is piat at the end &#38;the window list for 
the frame. In displaying the new window, the portion of the screen that will contain the window is first 
erased. The image fragment is then written to the display by a block raster transfer from the image file 
to the frame buffer. A border several pixels wide is left around the window to distinguish it from any 
other windows it may overlap. The name of the window is written in a space above the image fragment reserved 
for that purpose. After a window is displayed, the entire window is read back from the frame buffer and 
saved in a temporary file for use in future windowing operations such as delwindow. Saving the entire 
window simplifies the process of re-drawing when a window is deleted or moved. delwindow window-name 
Delete a window. In order to delete a window, the portion of the screen information that was behind the 
window must be re-drawn. This is done by piecing together portions of the temporary files that are saved 
for each window. A buffer containing the new screen information for the area that will become exposed 
is constructed. This is done by first zeroing the buffer and then writing into the buffer the portion 
of windows that intersect the deletion area. Windows are written in order according to the depth in the 
frame. Once the buffer is constructed, it is written to the display, overwriting the window. Figure 4 
shows how a typical window is deleted. The upper portion of the figure depicts schematically how the 
screen would look before and after the deletion of window C. The lower part of the figure shows how the 
buffer which overwrites the window is created. In W^, a buffer the size of window C is allocated and 
zeroed. The pUortions of all windows that intersect the deletion area must be copied into the buffer. 
The depth of windows A, B, and D determines the order in which they are written into the buffer. In W 
1 the portion of window A intersecting window C is written into the buffer at the appropriate place. 
In W 2 the portion of B intersecting C is written into the buffer. Finally, in W., the intersecting portion 
of window D is written into the buffer. ~ince B is below D, part of B is overwritten by D in the buffer. 
 mvwindow window-name[destination-frame] display-position To move a window, the user must specify the 
window to be moved and the new display position. Windows can be moved to another frame by specifying 
an optional destination frame. In moving a window, the portion of the screen information behind the window 
is updated, as in delwindow, and the window is re- drawn at the new position. This is similar to deleting 
the window and adding it again at a new position except that the temporary file does not need to be re-computed 
since it does not change. cpwindow window-name new-name [destination-frame] display-position Make a 
copy of a window. Copying a window is essentially the same as adding a new window except that it is not 
necessary to supply the image fragment since it is the same for the new window. The window to be copied 
must be specified, along with the new position. Windows can be copied to another frame by specifying 
an optional destination frame. raisewindow window-name Bring a window to the top in the frame. This is 
done by re-drawing the window. The window must be moved to the the end of the list of windows in the 
structure information to indicate its new depth in the frame. expand window, name[percent] Expand a 
window. To expand a window, a percentage of the window size is added to the upper bounds and subtracted 
from the lower bounds of the image fragment, and the window is re- drawn with the new bounds and with 
the center of the new window at the same screen pOsition as the center of the old window. Since the new 
window will completely overlap the screen area covered by the old window, it is not necessary to re- 
generate any portion of the screen except to draw the new window. shrink window-name[percent] Shrink 
a window. To shrink a window, a percentage of the window size is added to the lower bounds and subtracted 
from the upper bounds of the image fragment, and the window is re-drawn with the new bounds. Since the 
new window will be smaller, it is necessary to first update the screen information that is behind the 
old window, as in delwindow. before deletion after deleting C ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
 ::::::::::::::::::::::::::::::::::::::::::::::::::::::: Wo Wi W2 W3 Figure 4: Deleting a Window adjust 
window-name display-position Adjust a window. In adjusting a window, the user specifies a point that 
is to become the new center of the window and the window is re-drawn with that point at the center. Any 
valid screen coordinate can be used to calculate an image position with respect to the window. For points 
outside the window the image point used is the point that would be at the screen position entered if 
the image fragment were larger. Adjust is useful for bringing an area of interest to the center of a 
window and for "panning" around in a high resolution image which can not all fit on the display screen. 
autozoom [zoom-factor][new-window-size] display-position Create a new window of the image at a higher 
resolution by specifying an image point in another window. This is similar to adding a window except 
that it is not necessary to specify the image or the image bounds. A new window is created at the higher 
resolution with the point specified at its center. The image coordinate specified is used to calculate 
the image bounds for the new window. This command allows users to hnage zoom through a hierarchy of image 
resolutions, quickly establishing windows with appropriate level of detail, while maintaining a spatial 
context. 5.4. Segmentation Operations segopen file-name symbolic-name Open a segmentation file. As with 
images, a symbolic name is also associated with the segmentation for future reference. setseg[type] window-name 
.symbolic-segmentation-name Associate a segmentation with a window. The symbolic name of the segmentation 
and the window with which it is to be associated are required. The o'pe field, when given, is used for 
database applications to specify hand segmentations, machine segmentations, or map segmentation descriptions. 
The segmentation is drawn in the window by clipping all segmentation vectors against the window bounds. 
Due to possible occlusions, it may be necessary to re-draw any segmentations in higher windows. Window 
depth is used to order the drawing of scgmentations. rmseg window-name symbolic-segmentation-name Remove 
the association between a window and a segmentation. The window name and the segmentation are required. 
The segmentations are then re-drawn as in setseg. showseg window-name overlay-color feature-name Set 
the color of a particular feature in a segmentation associated with a window. The window name, the new 
color, and the name of the feature are required. After the color of a feature is changed, the segmentations 
are re-drawn as in setseg. 5.5. Command Files An interactive command interpreter, CI [13], is used by 
BROWSE to provide flexible parsing and substring matching. The user may create command files, which are 
text files containing a valid BROWSE command with its arguments, with each command on a separate line. 
Figure 5 shows a command file dc1420 maintained for general use that allows the user to open a Washington, 
D.C. database image using its generic name. In this case, the command file can be used both to reduce 
the interaction time and to conceal the actual file system location of the imagery. When invoked by the 
user (<dc1420) the contents of the file are passed directly to the CI command interpreter and are echoed 
to the user's terminal. BROWSE> <dc1420 open Ivisc/washdc/asc/dc1420/Ibw.im9 Ibw1420 map open /visc/washdc/asc/dc1420/2bw.img 
2bw1420 map open /visc/washdc/asc/dc1420/4bw.img 4bw1420 map open /visc/washdc/asc/dc1420/8bw.img 8bw1420 
map Figure 5: BROWSE Command File Users find it comfortable to move between typing some commands intcractivcly 
and using command files for frequently repeated opcrations such as image file opening, frame allocation, 
and window creation. The command file facility is also used to set up complicated contexts for image 
processing and to "can" demonstrations. 6. Applications In this section we will briefly discuss applications 
of BROWSE windowing capabilities to image database retrieval, to communication for image interpretation, 
and to 3D map graphics generation. 6.1. Database Retrieval For many photo-interpretation tasks, it is 
important to be able to examine an area of interest using several windows to contain images obtained 
over a period of time. In MAPS this can bc accomplished through queries to the map database component, 
CONCEPTMAP [9] [11]. A "high level" query such as "Display all images taken after 1974 containing 'Dept. 
of Commerce building'" can be directly implemented as a procedure or program by using the BROWSE subroutine 
package as primitives to create and display a new frame composed of windowed image fragments centered 
around the map feature, qqae photograph in Figure 6 shows the result of the previous query. The sequence 
of database and window operations to create Figure 6 is as follows: 1. Look up "Dept. of Commerce building" 
geographical map coordinates (latitude,longitude,elevation) in the map database. 2. Search the image 
database using map coordinates (minimum bounding rectangle) for images containing the geographical area. 
Return a list ofnonempty intersections. 3. For each image in the list obtained in step (2), convert 
map coordinates into image (row,col) vector coordinates. Clip polygons to image raster bounds and discard 
"empty" polygons. Return a list of nonempty "image feature" polygons. 4. Partition the frame and calculate 
window size (maximal) based on the number of images in the list obtained in step (3). Display the image 
at the highest resolution that fits within thc window sizc calculated above. 5. Display "image features", 
automatically generating window names based on the generic image name.  Once displayed, any of the 
windows can be manipulated using the 1}P, OWSE commands previously described. Thus, the user can select 
one or more of the image fragments, expand the size of the window to obtain more image context, move 
a window for side-by-side comparison, zoom in for more detail, or adjust the center of the window. An 
analogous database access can be accomplished by simply indicating an area el interest in an existing 
window using a tablet-controlled cursor. The display window coordinates are first converted to image 
coordinates, then to map coordinates (latitude,longitude, elevation). The image database is then searched 
for intersecting areas as described above. 6.2. Windows as a Communication Mechanism So far we have discussed 
the use of image windowing for interactive display in tightly coupled man-machine interactions. The window 
representation is also quite useful as a communication mechanism to invoke image processing programs 
and to retrieve and display the results of such processing. BROWSE uses the standard UNIX 3 process forking 
mechanism to run image processing algorithms such as edge detection, smoothing, and filtering operations, 
and to run image interpretation programs such as linear feature extraction, region growing, and image 
segmentation. The image fragment to be processed is described by a window which may have been created 
manually by the operator or generated automatically as a result of a database query [9, 10]. In many 
image-processing systems, users are required to specify image coordinates manually or to create a cropped, 
temporary image for the analysis program to operate on. Then the results of such processing, another 
image or feature boundary descriptions, are returned as files. The user must then examine the resulting 
image, or the extracted image feature description, using his favorite display program. This somewhat 
tcdious procedure is in sharp contrast to the BROWSE philosophy which encourages the implementer of the 
image processing program to display the results in a window created by the forked process. This new window 
can be inherited back by the parent process after the forked process has terminated. Thus the interactive 
user can easily compare the processed image window side-by-side with the original image without the need 
for running another program, and while maintaining processing context. There are two methods generally 
used to accomplish this communication. In the first, the window data structure containing the image file 
name and subimagc boundary information is used to dynamically create a command line that defines the 
portion of the image to be processed. In addition to the window, other program specific parameters can 
be passed to the processing program using the UNIX command line facility, or the user can provide them 
directly through terminal interaction. The following are command line arguments for generic smoothing 
and edge extraction programs. They are typical of others written at Carnegie-Mellon. smooth [mode-switch] 
in-image [subimg] out-image edgeop [mode-switch] in-image [subimg] out-image To run thc program the 
user specifies the file name of the input and output image, and an optional sub-image range. In each 
program the mode-switch specifies the type of smoothing (median, average, sample) or edge operator (sobel. 
laplacian, nagao). While this provides a flexible interface, it also requires a great deal of typing 
by the user to specify image file path-names, and sub-image pixel ranges. The BROWSE user interface to 
thcse routines is of the form: BROWSE> median window-name BROWSE> sobel window-name where a temporary 
output file is created and the image processing 3UNIX is a Trademark of Bell Laboratories  program is 
invoked with the appropriate command line arguments generated by BROWSE. When the processing is complete, 
the user is prompted to display the results in a new window and may save the temporary file for future 
use. The second method of communication is to create BROWSE-like programs that communicate through shared 
files that remain open across the process fork. The forked process maintains and can modify the frame-buffer 
state. The frame, window, and image descriptions inherited by the forked process are identical to those 
of the parent process. Thus, the display state maintained by BROWSE is available to child processes, 
and modifications can be passed back to the parent. We have used dais mechanism in our MAPS database 
system to create task-specific BROWSE-like programs to perform image-to-map correspondence, landmark 
generation, interactive human segmentation, map-guided machine segmentation, and map database access. 
The user may invoke these domain experts in any sequence to perform an image processing or. cartographic 
task. Each program contains only the basic subset of commands loaded from the BROWSE library necessary 
for image manipulation within its task domain. Specialized commands are not duplicated, and users move 
from one processing step to another without leaving the BROWSE environment. 6.3.3D Map Display A central 
problem for a variety of cartographic tasks is flexible access to 3D map databases[6]. Tasks include 
inspection and verification of spatial databases, incremental update, and feature enhancement. BROWSE 
pruvides tools for the selection of ground area either through image-to-map correspondence (ie. describing 
the area to be portrayed ~(ia digital imagery) or direct specification of map coordinates. The photograph 
in Figure 7 shows a full frame window containing a two dimensional map image of an area around Washington 
D.C.. This 13 color-class thematic image shows areas such as forest and park (green), water (blue), residential 
(yellow), and high density urban (brown). It was generated by scan conversion of a polygon map database 
provided by the Defense Mapping Agency (DLMS Level 1). In this application, the user indicates a rectangular 
area of interest in the map image, specifies the center point (west of National airport), viewing position 
(from the southeast), and view angle. This is done by tracking a cursor on the display to minimize the 
amount of knowledge that the user must have of the actual 3D coordinate system. The photograph in Figure 
8 shows the result of the 3D map generation. For each image point in the area specified by the user, 
a map coordinate is calculated (latitude, longitude, elevation). A 3D surface description is generated 
using the thematic color from the map image, and this description is passed to a 3D shaded raster graphics 
display program [2]. The resulting map image is then displayed by BROWSE. 7. Conclusions In this paper 
we have shown the usefulness of windowing as a mechanism for display and interactive prucessing of digital 
imagery. We have described how one system, BROWSE, uses multiple windows to aid users in various aspects 
of image interpretation tasks. We have also discussed the syntax of a subset of BROWSE commands and the 
basic data structures used in manipulating the display. BROWSE is used regularly as a front-end for image 
processing and database programs in the MAPS system and also as a general purpose image display facility. 
8. Acknowledgements We are pleased to acknowledge the members of the CMU Image Understanding Systems 
group who have provided a demanding but friendly user community. Special mention is due to David Smith 
and Steve Shafer for implementations of the image paging package and CI command interpreter package, 
respectively. Thanks to Rich Korf and Bob Sproull for editorial and technical comments on earlier drafts. 
References 1. Bail, J. Eugene. Canvas: the Spice graphics package. Tech. Rept. Spice Document $108, Carnegie-Mellon 
University, Pittsburgh, PA., 1980. 2. Grant, Eric. AGP: A Graphics Package. Unpublished project report, 
Carnegie-Mellon University, Pittsburgh, PA., 1981.  3. IEEE Workshop on Picture Data Description and 
Management, Asilomar, Ca., August, 1980. 4. 1EEE Workshop on Computer Architecture for Pattern Analysis 
and Image Database Management, Hot Springs, Va., November, 1981. 5. IEEE Computer Magazine, Special 
Issue on Pictorial Information Systems, November, 1981.  6. Lukes, G. E. Computer-assisted photo interpretation 
research at United States Army Engineer Topographic Laboratories (USAETL). Techniques and Applications 
of Image Understanding Ill, Society of Photo-Optical Instrumentation Engineers, Washington, D.C., April, 
1981, pp. 85-94.  7. McKeown, D.M., and J.L. Denlinger. BROWSE: A Generalized lmage Display Facility 
Users Guide. Image Understanding Group Memo 6, Carnegie-Mellon University, Pittsburgh, PA., June, 1980. 
 8. McKeown, D.M. Knowledge Structuring in Task Oriented lmage Databases. Proceedings of the IEEE Workshop 
on Picture Data Description and Management, Asilomar, Ca., August, 1980, pp145-151.  9. McKeown, D. 
M. and T. Kanade. Database Support for Automated Photo Interpretation. Techniques and Applications of 
Image Understanding Ill, Society of Photo-Optical Instrumentation Engineers, Washington, D.C., April, 
1981, pp. 192-198.  10. McKeown, D.M. Generation of a Map Database for Aerial Photo- Interpretation. 
Proceedings of the IEEE Workshop on Applied Imagery Pattern Recognition, September, 1981.  11. McKeown, 
D.M. Concept Maps. Computer Science Department, Carnegie-Mellon University, Pittsburgh, PA., 1982. in 
preparation  12. Reddy, R., and G. Gill. Representation Complexity of Image Data Structures. Proceedings: 
DARPA Image Understanding Workshop, May, 1978, pp. 28-30.  13. Shafer, S. The CI Command Interpreter. 
Computer Science Department, 1980. Entry ci, chapter 3. CMU Update: UNIX Programmer's Manual (c) Bell 
Laboratories, 1980.  14. Smith, D. R. CMU Image Format and Paging System. Image Understanding Group 
Memo 3, Carnegie-Mellon University, Pittsburgh, PA., November, 1980.  15. Sproull, R. F. Raster Graphics 
for Interactive Programming Environments. Tech. Rept. CSL79-6, Xerox Palo Alto Research Center, 1979. 
 16. Teitelman, W. A Display Oriented Programmer's Assistant. Tech. Rept. CSL77-3, Xerox Palo Alto Research 
Center, 1977.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801281</article_id>
		<sort_key>199</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>30</seq_no>
		<title><![CDATA[MAPQUERY]]></title>
		<subtitle><![CDATA[Data base query language for retrieval of geometric data and their graphical representation]]></subtitle>
		<page_from>199</page_from>
		<page_to>207</page_to>
		<doi_number>10.1145/800064.801281</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801281</url>
		<abstract>
			<par><![CDATA[<p>The storage of geographical data is a growing type of application within data processing.</p> <p>Users want to retrieve parts of the data stored and to have them shown in graphical form. They need an easy and flexible query language to select the data needed and to describe the form of representation. The existing query languages do not suit this purpose without amendments. This paper presents a proposal for adaptation and gives some examples for query formulation. Moreover, it discusses the special problems of graphical input and output.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.2.3</cat_node>
				<descriptor>Query languages</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Government</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.2</cat_node>
				<descriptor>Earth and atmospheric sciences</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010437</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Earth and atmospheric sciences</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010476.10010936</concept_id>
				<concept_desc>CCS->Applied computing->Computers in other domains->Computing in government</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10002952.10003197</concept_id>
				<concept_desc>CCS->Information systems->Data management systems->Query languages</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14131692</person_id>
				<author_profile_id><![CDATA[81100370657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Frank]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Geodesy and Photogrammetry, Swiss Federal Institute of Technology (ETH), Zurich, Switzerland]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blaser, Albrecht, Lehmann, Hubert: Abfragesprachen in Datenbanken, in: IBM Nachrichten 30 (1980) Heft 251.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>811515</ref_obj_id>
				<ref_obj_pid>800296</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chamberlin, D.D., Boyce, R.F.: SEQUEL: A Structured English Query Language, in: Proc. of 1974 ACM SIGFIDET Workshop on Data Description, Access and Control, Ann Arbor Michigan, 1974, p. 713-775.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Chamberlin, D.D. et al.: Sequel 2: A unified approach to data definition, manipulation and control, in: IBM J. Res. Develop. 20 (1976) p. 560-575.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Chang, N.S., Fu, K.S.: A Query Language for Relational Image Database Systems, Proc. IEEE Workshop Picture Data Description and Management, Aug. 1980, pp. 67-73.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Chang, N.S., Fu, K.S.: Picture Query Languages for Pictorial Data-Base Systems, Computer Nov. 1981, p. 23]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Chang, S.K., Fu, K.S. (Ed.): Pictorial Information Systems, Springer-Verlag 1980]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Chang, S.K., Lin, B.S., Walser, R.: A Generalized Zooming Technique for Pictorial Data Base Systems, AFIPS Conf. Proc., 1979 NCC, Vol. 48, pp. 147-156.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Chang, S.K, Kunii, T.: Pictorial Data-Base Systems, in: Computer Nov. 1981, p. 13.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Childs, D.L.: Extended Set Theory. A general model for very large, distributed, backend information systems, in: 3rd Internat. Conf. on Very Large Data Bases, Tokyo 1977.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Codd, E.F.: Relational Compleneness of data base sublanguages, Courant Computer Science Symposium 6,"Data Base Systems", R. Rustin (Ed.) Prentice Hall, 1972, IBM RJ 987.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Codd, E.F.: Seven Steps to Rendezvous with the casual user, in"Data Base Management" (Ed.: Klimbie/Koffeman) North Holland, Amsterdam 1974.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Conzett, R.: Landinformationssystem, Vortrag an Tag der offenen T&#252;r, 12. April 1980, Institut f&#252;r Geod&#228;sie und Photogrammetrie, ETH Z&#252;rich, 1981]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>4198</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Date, C.J.: An introduction to database systems, Addison Wesley, Reading 1979, 2nd ed.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Dieckmann, E.M.: Three relational DBMS (Oracle, Ingres, SQL/DS), Datamation Sept. 1981.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Dutton, G. (Ed): First international advanced study symposium on topological data structures for geographic information systems, Harvard Papers on Geographic Information Systems, Harvard University, Cambridge, March 1978.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Eichhorn, G.: Land Information Systems. Symposium of the F&#233;d&#233;ration Internationale des G&#233;om&#232;tres, October 16 - 25, 1978, Darmstadt]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Frank, A.: Applications of DBMS to Land Information Systems. 7th Internat. Conf. on Very Large Data Bases, Cannes (France), 1981, p. 448.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Frank, A.: MAPQUERY: Design and implementation, Problems of realization of Land Information Systems, Part 2, Report 56, Institut f&#252;r Geod&#228;sie und Photogrammetrie, Swiss Federal Institute of Technology, Zurich, Switzerland.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Frank, A.: Data Structure and Data Description for Geo Data Systems, Problems of realization of Land Information Systems, Part 3, Report 57, Institut f&#252;r Geod&#228;sie und Photogrammetrie, Swiss Federal Institute of Technology, Zurich, Switzerland.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Go, A., Stonebraker, M., Williams, C.: An approach to implementing a Geo-Data System. Electronics Research Laboratory, College of Engineering, University of California, Berkeley, Meno ERL-M 529, 25 June 1975.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807470</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Herot, Christofer; F., Carling, Richard; Friedel, Mark; Kramlich, David: A Prototype Spatial Data Management System. Siggraph '80, ACM Computer Graphics, Vol. 14, No. 3, p.63, July 1980.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Hooper, K.: Experimental Mapping. The perceptual representation of environments. University of California, Santa Cruz, Technical Report.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Johnson, J.: Intellect on Demand: INTELLECT, Datamation Nov. 1981, p.73.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Jones, P.F.: Four principles of man-computer dialogue, in: Computer Aided Design Journal, Vol. 10, No. 3, May 1978.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Krause, J., Lehmann, H.: User Speciality Language. A natural language based on information system and its evaluation. Int. Conf.on Literary and Linguistic Computing, Bonn, Dec. 1979.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Krause, J.: Natural Language Access to Information Systems. An Evaluation Study of its Acceptance by End Users, in: Inform. Systems, Vol. 5, No. 4, May 1980, pp 297-318.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[M &amp; S: The use of computer graphics for utilities mapping and records, Report 78-090 A, M &amp; S Computing Inc. 1978.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>743174</ref_obj_id>
				<ref_obj_pid>647953</ref_obj_pid>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Mantey, P.E., Carlson, E.D.: Integrated Geographic Data Bases: The GADS Experience, in: Data Base Techniques for Pictorial Applications, ed. by A. Blaser, Springer-Verlag, 1979, pp. 173-198.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Meier, A., Zehnder, C.A.: Fl&#228;chenmodell-Register wichtiger geographischer Daten-sammlungen der Schweiz. Institut f&#252;r Informatik ETH Z&#252;rich, Bericht Nr. 39, 1980.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Meister, D.: Behavioral foundations of system development. New York: Wiley, 1976.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>805902</ref_obj_id>
				<ref_obj_pid>800194</ref_obj_pid>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Montgomery, C.A.: Is Natural Language an Unnatural Query Language, Proceedings of the ACM Annual Conference, New York, 1972, p.1075.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Newman, W.M., Sproull, R.F.: Principles of Interactive Computer Graphics, 2nd ed. McGraw-Hill 1979.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356837</ref_obj_id>
				<ref_obj_pid>356835</ref_obj_pid>
				<ref_seq_no>33</ref_seq_no>
				<ref_text><![CDATA[Reisner, P.: Human factors studies of Data-base Query Languages, a survey and assessment, ACM Surveys Vol. 13, No. 1, March 1981, p.13.]]></ref_text>
				<ref_id>33</ref_id>
			</ref>
			<ref>
				<ref_obj_id>743042</ref_obj_id>
				<ref_obj_pid>647953</ref_obj_pid>
				<ref_seq_no>34</ref_seq_no>
				<ref_text><![CDATA[Schmutz, H.: The Integrated Data Analysis and Management System for Pictorial Applications, in Data Base Techniques for Pictorial Applications, ed. by A. Blaser, Springer-Verlag, 1979, pp. 475-493.]]></ref_text>
				<ref_id>34</ref_id>
			</ref>
			<ref>
				<ref_seq_no>35</ref_seq_no>
				<ref_text><![CDATA[Senko, M.E.: Data description language in the concept of multilevel structured description: DIAM II with FORAL. Douqu&#233; BCM. Nijssen GM. Ed., Data Base Description 1975.]]></ref_text>
				<ref_id>35</ref_id>
			</ref>
			<ref>
				<ref_seq_no>36</ref_seq_no>
				<ref_text><![CDATA[Senko, M.E.: FORAL LP: design and implementation, VLDB 1978, Berlin.]]></ref_text>
				<ref_id>36</ref_id>
			</ref>
			<ref>
				<ref_seq_no>37</ref_seq_no>
				<ref_text><![CDATA[Senko, M.E.: A query-maintenance language for the data independent accessing model II, in: Inform. Systems, Vol. 5, 1980.]]></ref_text>
				<ref_id>37</ref_id>
			</ref>
			<ref>
				<ref_obj_id>320295</ref_obj_id>
				<ref_obj_pid>320289</ref_obj_pid>
				<ref_seq_no>38</ref_seq_no>
				<ref_text><![CDATA[Shneiderman, B.: Improving the human factors aspects of database transactions. ACM Transaction on Database Systems, Vol. 3, No. 4, December 1978]]></ref_text>
				<ref_id>38</ref_id>
			</ref>
			<ref>
				<ref_obj_id>320476</ref_obj_id>
				<ref_obj_pid>320473</ref_obj_pid>
				<ref_seq_no>39</ref_seq_no>
				<ref_text><![CDATA[Stonebraker, M.; Wong, E.; Kreps, P.; Held, G.: The Design and Implementation of INGRES, ACM Transaction on Database Systems, Vol. 1, No. 3, September 1976, p. 189.]]></ref_text>
				<ref_id>39</ref_id>
			</ref>
			<ref>
				<ref_seq_no>40</ref_seq_no>
				<ref_text><![CDATA[Stonebraker M.; Rowe, L.A.: Observations on Data Manipulation Languages and their Embedding in General Purpose Programming Languages. Proceedings 3rd Int. Conf. on Very Large Data Bases, IEEE, New York, 1977, p. 128.]]></ref_text>
				<ref_id>40</ref_id>
			</ref>
			<ref>
				<ref_seq_no>41</ref_seq_no>
				<ref_text><![CDATA[Synercom: Synercom Information and Mapping System, User manual. Synercom Technology, Inc., 1980.]]></ref_text>
				<ref_id>41</ref_id>
			</ref>
			<ref>
				<ref_seq_no>42</ref_seq_no>
				<ref_text><![CDATA[Tamminen, M.: Management of spatially referenced data. Conceptual study of system requirements and structure. Report, Helsinki University of Technology, Finnland.]]></ref_text>
				<ref_id>42</ref_id>
			</ref>
			<ref>
				<ref_seq_no>43</ref_seq_no>
				<ref_text><![CDATA[Thomas, John C.: Psychological Issues in Data Base Management. Proceedings 3rd Int. Conf. on Very Large Data Bases, IEEE, New York, 1977, p. 169.]]></ref_text>
				<ref_id>43</ref_id>
			</ref>
			<ref>
				<ref_seq_no>44</ref_seq_no>
				<ref_text><![CDATA[Thompson F.B.; Thompson B.H.: Practical natural language processing. The REL system as proto-type, in: Advances in computers, Vol. 13, New York 1975.]]></ref_text>
				<ref_id>44</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807471</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>45</ref_seq_no>
				<ref_text><![CDATA[Tsurutani, T.; Kasakawa, Y; Naniwada, N.: ATLAS: A geographic data base system, in: ACM Computer Graphics, Vol. 14, No. 3, July 1980. SIGGRAPH 80, Conf. Proceedings.]]></ref_text>
				<ref_id>45</ref_id>
			</ref>
			<ref>
				<ref_obj_id>538906</ref_obj_id>
				<ref_seq_no>46</ref_seq_no>
				<ref_text><![CDATA[Ullman, J.D.: Principles of Data Base Systems. Pitman London 1980.]]></ref_text>
				<ref_id>46</ref_id>
			</ref>
			<ref>
				<ref_seq_no>47</ref_seq_no>
				<ref_text><![CDATA[Wild: Wildmap, Interactive Photogrammetric Data-Base and Mapping System. Operators Procedure Manual, Wild (Heerbrugg) 1980.]]></ref_text>
				<ref_id>47</ref_id>
			</ref>
			<ref>
				<ref_seq_no>48</ref_seq_no>
				<ref_text><![CDATA[Zloff, M.: Query by Example, AFIPS Conference Proceedings, 1975 National Computer Conference, Vol. 44, AFIPS Press, 1975, p. 431.]]></ref_text>
				<ref_id>48</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 16, Number 3 July 1982 MAPQUERY: Data Base Query Language for Retrieval of 
Geometric Data and their Graphical Representation Andr@ Frank Institute of Geodesy and Photogrammetry 
Swiss Federal Institute of Technology (ETH) Zurich, Switzerland Abstract The storage of geographical 
data is a growing type of application within data processing. Users want to retrieve parts of the data 
stored and to have them shown in graphical form. They need an easy and flexible query language to select 
the data needed and to describe the form of rep- resentation. The existing query languages do not suit 
this purpose without amendments. This paper presents a proposal for adaptation and gives some examples 
for query formulation. Moreover, it dis- cusses the special problems of graphical input and output. 
CR Categories and Subject Descriptors: H.2.3 IDatabase Managementl: Languages -Query languages; 1.3.6 
IComputer graphicsl: Methodology and Techniques - Interaction techniques; J.l IAdministrative Data Processingl: 
Government J.2 IPhysical Sciences and EngineeringI: Earth and athmospheric sciences General Terms: 
Human Factors, Languages Figure i :~'o'L o °',a~/; ' o ° ° ~S °°° i. Introduction  Large amounts of 
data are stored in computer systems. Very often, graphical presentation is pre- ferred; it seems that 
human beings understand pic- torial information better than large tables of alphanumeric items. An important 
problem in connection with the application of computer graphics is the form in which a casual user with 
little training can express his information needs. A query language adapted to the special requirements 
of graphical represen- tation is apparently needed 1121. This paper proposes a formal language for data 
retrieval and presentation. Formal, because a for- mal language seemed easier to implement in our environment 
lacking experience with natural language systems. It is, furthermore, uncertain whether it is worthwile 
to implement a natural language interface for this application. Although some working systems are reported 
Iii, 1231, 1261, it is not absolutely certain whether a natural language is appropriate as a query language 
1311, 1381. As users are not conversing frequently about maps and their appear- ance, there is no fixed, 
widely used vocabulary on which to base anatural language query system 1221, 125  Permission to copy 
without fee all or part of this material is granted publication and its date appear, and notice is given 
that copying is by provided that the copies are not made or distributed for direct permission of the 
Association for Computing Machinery. To copy commercial advantage, the ACM copyright notice and the 
title of the otherwise, or to republish, requires a fee and/or specific permission. 199 &#38;#169; 
1982 ACM 0-89791-076-1/82/007/0199 $00.75 A number of substantially different types of data storage and 
graphical representation appli- cations can be differentiated:  - retrieval of previously stored complete 
pic- tures usually in raster format (references may be found in 151, 161, 181), - representation in 
graphical form of data which is usually shown in tabular form (spatial data management system) 1211, 
 -  retrieval and representation of data, usually presented in the form of geographic maps 1151,  
1161, 1291.  This paper will treat the latter case only; it is, however, felt that part of the conclusions 
may easily be extended to the area of spatial data management system, too. Based on a concrete simplified 
application, the needs of users for retrieval and representation of information are described. The well 
known SEQUEL language 121, 131 is used as a starting point to develop MAPQUERY which is believed to be 
especially suited for retrieval of geometric data. 2. The Application Land Surveyors traditionally 
gather data related to objects in space and represent these data on maps of different style, size and 
scale. These maps are used by a wide variety of users to many different purposes. The gathering of 
geometric data is a costly process and closely related to the representation of data in the form of maps. 
The drawing of clear, easily understandable maps is a tedious and very expensive operation demanding 
highly qualified personnel (fig. I). In our society land has become a resource of utmost importance, 
requiring very careful management. All planning activities in connection with e.g. land use or environment 
protection need data about objects in space as they are traditionally shown on maps. But for different 
planning purposes different data are needed and often the form should be specially adapted. Traditional 
surveying techniques make it difficult to transfer information represented on one map into a new map 
of different style. The process requires expensive and time consuming manual work. More complicated -if 
not impossible -is very often to combine in one drawing data represented on different maps. The possibilities 
of modern electronic data processing induced the idea to store land related data in a computer and to 
draw the desired maps on demand, exactly suited to the use intended. 3. Limitations of Available Specialised 
Systems For different types of users -e.g. public utility companies -such systems have been developed 
to fulfil certain needs of these users -e.g. drawing of plans of the actual situation of pipes and lines 
in underground. Similar examples can be shown for planning applications. 1151, 1271, 1411, In the past 
these systems have been based on storage and retrieval of the graphic features of the plans. The different 
map sheets are usually attri- buted to different files and in each file the lines and other graphical 
features of the map are stored. These systems do normally not provide a powerful interactive user query 
language. The selection of the objects on the screen is usually done by indi- cating the name of the 
map-sheet and then 'zoom' and 'pan' may be used to choose the desired window. It is obvious that such 
systems allow only for a very limited flexibility of adaptation to users needs, i.e. different query 
strategies, contents and representations. But systems should be built to fulfil the needs of a wider 
group of users -a requirement that is certainly reasonable if we compare the costs of constructing such 
a system (which are very likely to be in the order of millions of dollars) to the costs of data gathering 
and maintenance (for a larger city a complete survey will cost tens of millions of dollars) 1421. We 
call these systems Land Information Systems (LIS) because they are intended to store land re- lated (i.e. 
spatially referred) data. Such Land Information Systems may not be based on stored graphical representations 
of objects but must store an internal (abstract) model of objects, from which model different graphical 
representations can be derived upon demand. 4. Data Base Management Systems The theory concerning data 
base systems 1131,  1461 and the available Data Base Management Systems are very valuable for implementing 
Land Information Systems. The data model which has to be stored is to a certain extent similar to the 
data in the commer- cial environment where such systems have been succesfully applied. In order to minimize 
the problems of updating stored information, data base theory states certain rules for splitting up information 
into independent parts (often called entities). Generally the goal is to minimize redundancy by storing 
one piece of information in one place only -for example, even if a person owns several houses, the address 
of the person is not stored as a data element in connec- tion with each house but as an attribute of 
the entity "person". With the houses only a logical connection to the owner is stored; using this schema 
of storage it is evidently much simpler to update the owner's address. 5. Difference between Ordinary 
Commercial Data and Geo-Data There is a great difference between ordinary commercial data and geometric 
data i~ a Land In- formation System. All data in a Land Information System are related to objects in 
space; they are, therefore, also related to a position in space. This is in contrast to commercial and 
administra- tive systems, in which data are hardly ever ex- ploited in connection with position in space. 
 This relation to space and geometry is impor- This typical query may be generalized into the tant to 
consider. Between the different data el- form ements geometric and topological relations exist, e.g. 
a house is situated on a parcel, a parcel is adjacent to a street. show all <object-type-list> within 
<window>[ There are so many potential relationships that it is impossible to store all of them explicitly; 
but they can be stored implicitly in the form of coordinate geometry. However, the deduction of topological 
information from geometry may be quite expensive. The second difference is in the form of access to 
the data. Commercial data are very often re- quested in queries yielding one or more selected data elements, 
presented in the form of a list. As an example: List all employees with age higher than 55~ which yields 
fig. 2 as an answer. f Name Age Ares, Jan Myer, Don Orlos, Pit Pier, Mark 57 60 58 57 J Figure 2 It 
is natural to represent objects in Land In- formation Systems geometrically, for which reason these systems 
are usually queried in order to get a map on the screen: Show all houses, streets, parcels within 'Lohzelg' 
area! which yields fig. 3 as an answer. f Lohz where <window> specifies a subset of the two- dimensional 
space (range in X and Y) and is used to select a great many of data elements, which are shown as a 
map. It is very important that this type of query can be answered fast. The storage structure should 
support fast access to data in spatial neighbourhood. This fast access is important not only for the 
queries, but also for fast geometrical and topo- logical algorithms (e.g. in case a topological re- lation 
must be deduced from the co-ordinate geo- metry). Such a storage structure has been implemen- ted at 
our institute 1171. It can be based on a commercially available Data Base Management System, i.e. DBMS-10 
from DEC, a CODASYL network type, and should form the base for building a Land Information System. 6. 
A Query Language for Geo-Data The query languages in commercial Data Base Management Systems are often 
based on relational calculus 1141; many are similar to SEQUELI21, J31, which will serve as a background 
for the following discussion. It has been shown that these languages are 'relationally complete' i.e. 
all queries that can be formulated in first order predicate cal- culus may also be formulated in this 
language !I01 . 'Completeness' is regarded as a minimal require- ment for the expressive power of a 
query language and the need for extensions is generally felt 111,   191, i40i.  Expressing topological 
(e.g. 'a is within b') or geometrical (e.g. 'a is nearer to b than c') con- ditions in relational calculus 
is difficult if not impossible 1381. it will therefore be necessary to extend the language to include 
such specific func- tions, as e.g. WITHIN: object x window ÷ boolean which is true, if the object or 
part of it extends  into the window. WITHIN evokes therefore an operation like 'clipping' and cannot 
easily be expressed by boolean expression (c.f. fig. 4). PI (xl'Yl) / 11 i j  Figure 3 P2 (x2'Y2) 
"'''~ Figure 4  The special type of information the user expects makes other changes necessary It is 
not simply a change in the form of representation of retrieved data from alphanumeric to graphical, but 
a far- reaching change in the form of man-machine communi- cation. The most important points to consider 
in this respect are: - context needed (environment) - selection through geometric or topological relationship 
(e.g. window) - form of presentation - graphical input 6.1 Environment If a user wishes a point 
to be represented, he will accept the output of the co-ordinate values of this point in alphanumeric 
form (fig. 5). But he will certainly not accept the point represented graphically on the otherwise empty 
screen (fig. 6) - he is simply unable to interprete the data. f Point-mr x _y 435 936 51 574.73 Figure 
5 f 435 J Figure 6 Therefore, in the graphical representation not only the object asked for but a 
suitable environ- ment for interpretation must be presented. This is probably one of the reasons for 
the superiority of graphical communication.  'In general, personnel gather too little information prior 
to making a diagnostic decision. However, when more information is available, subjects are more ef- ficient 
in selecting data items' 1301. 'This suggests that it may be wiser to provide the user with more information 
than he or she appears to ask for in answer to a query' 143 I. Two different aspects of the environment 
needed for interpretation have to be differentiated: - the contents (e.g. houses and/or streets) - 
the spatial extension (the window). 6.2 Window In a query which should yield an answer in gra- phical 
form the user must set -implicitly or explicitly -a window 1321 indicating the part of the world which 
is to be shown on his screen To set the window explicitly, the user must be given a command to indicate 
the limiting co-ordinate values (x.min, x.max, y.min, y.max). In many cases, this would be too troublesome 
for the user; he wishes to see a certain house of which he knows the postal address but not the co-ordinates. 
 It is the system's task to find out these co- ordinates~ The user should be allowed to state a query 
like: Show house with ad~ess = '23 Parkland Drive' and expect the system to choose a suitable window 
 to show the desired house with a reasonable part of the environment. The window cannot be determined 
by the extreme coordinates of the object desired alone, but must be extended in order to include some 
of the sur- rounding objects. Some experiments show that users prefer the drawings not to be in an arbitrary 
scale, but are more at ease with some standard values. To decide whether a map is in the scale of I:i000 
or 1:500 is easy (based on e.g. width of streets) and unconsciously done. The user may then interpret 
the whole contents in accordance to his preexisting concept of a 1:500 scale map. This seems preferable 
to a scale adjusted to a maximum size on the screen, arriving at arbitrary scales, which make interpretation 
difficult. First Example Query The user desires a traditional map showing streets and street names, 
houses with numbers, parcel boundaries (fig. 7) within a window given by two corner points.  d y ,e, 
s3 0 ~ ~ ~ t Figure 7  Although an abbreviation should be prepared for this query, we will discuss it 
here in detail. The concept of extensibility of a language to let the user define a query with the normal 
query language and then store this query and let him call it later by an abbreviation is certainly very 
useful Iiii, 1441. The data base provided for answering this query contains the following data-types 
(fig. 8). area area-name geometry parcel l street parcel-nr street-name area-mr area-mr geometry 
geometry house / house - mr street-name I  pareel-nr l geometry / Figure 8 This schema naming the 
entities with their data fields and indicating possible connections by arrows is intended for ease of 
presentation only. The prob- lem of designing optimal data description for geo- metric data is treated 
in 1191, 1421. We can now express this first query in a SEQUEL like language: DEFINE window (x.min, 
x.max, y.min, y.max) SHOW street-name, geometry FROM street WHERE geometry WITHIN window SHOW house-mr, 
geometry FROM house WHERE geometry WITHIN window SHOW geometry FROM parcel WHERE geometry WITHIN window 
 Certain liberties have been taken with SEQUEL to express this query but the result is not very promising 
for daily users. SEQUEL was developed for the selection of data through complex criteria -it seems less 
useful for the selection of several groups of data using always the same very simple criteria, i.e. the 
window. It is certainly adequate -as it is usual in computer graphics languages 1321 -to extend the 
effects of the WINDOW-statement from the place where it is given until it is explicitly recalled or over- 
 ridden by a new WINDOW-statement. The WINDOW command  sets, as understood by the user, a context which 
must be used to interpret his further commands [241. Second, it seems superfluous to restrict each ob- 
ject selected by the condition that it extends into the window -this is from the user's point of view 
natural, otherwise it cannot be shown (and the user evidently does not care whether it is retrieved or 
not). This would give: \ IN WINDOW (x.min, x.max, y.min, y.max)  SHOW street-name, geometry FROM 
street SHOW house-mr, geometry FROM house SHOW geometry FROM parcel Second Example Query  The user 
desires a similar map as before but with a larger scale and featuring the house with address '7 Hill 
Drive' in the center (fig. 9). Figure 9  SEQUEL (and most other query languages) requires the user 
to write two different query statements: one for selecting the window and another for selec- ting the 
contents within the window. A specialized language for retrieval of geometric data should offer a simpler 
formulation: SHOW IN WINDOW THE geometry WHERE str FROM house =eet-name 'Hill Drive' AND house-mr = 
7 ALL ALL ALL geometry, geometry, geometry, house-mr street-name FROM FROM FROM house street parcel~ 
 This query would effectively select the desired house twice, firstly in the 'THE' statement where the 
window is built, and secondly in the 'ALL...FROM house' phrase. This seems no real problem for CRT displays 
-for plotters it would be necessary to select first the window and check the output on a display and 
then ask for a new drawing with the previously set window: SHOW ALL geometry, house-nr FROM house ALL 
geometry, street-name FROM street ALL geometry, FROM parcel~ The same construct allows also to build 
a map gradually, first selecting an entity which sets the window and then adding further data to the 
picture. Third Example Query The user needs the same type of map as before, but showing streets, houses 
and parcels for the area 'Talacher' (fig. i0). f J Figure i0 SHOW IN WINDOW THE geometry FROM area 
WHERE area-name = 'TALACHER' ALL geometry, street-nr FROM street WHERE area-nr SELECT area-nr FROM area 
WHERE area-name = 'TALACHER' ALL geometry FROM parcel WHERE area-nr SELECT area-nr FROM area WHERE 
area-name = 'TALACHER' ALL geometry, house-nr FROM house WHERE parcel-nr = SELECT parcel-nr FROM parcel 
WHERE area-nr = SELECT area-nr FROM area WHERE area-name = 'T&#38;LACHER'~ An easier solution is obviously 
needed: The natural hierarchical structure of area con- taining streets and parcels, and parcels containing 
houses should be exploited.  The user starts with his naming of 'area' a con- text for the rest of the 
query. He can now ask to see the streets thereof (understood: the streets of this area), the parcels 
(understood: the parcels of this area) and the houses thereof (understood: from the parcels selected 
-the context has been moved). This context can be interpreted, given the connec- tions between area, 
street and parcel and between parcel and house shown in fig. 8. To stress this context, it seems better 
to change the 'field FROM entity' phrase in 'entity (field)' 133 I . This pro- posal exploits mainly 
ideas from 1351, ]37 I with different keywords and some minor differences in syntax.  SHOW IN WINDOW 
THE area (geometry) WHERE name = 'TALACHER' THEREOF ALL streets (geometry, street-nr) ALL parcels (geometry) 
THEREOF ALL houses (geometry, house-nr)~ This is a unique description as long as there are not two 
different connections between two object-types. During the design of the user's view of the sorted data 
such problems can be catered to. Fourth Example Query The user needs a map showing the street named 
'Hill Drive' and all the adjacent parcels. In a schema in which a connection between street and parcel 
already exists (meaning the parcel which is formed by the street), we have in order to pre- pare this 
type of query to include a new object in the user's view: adjacent parcel to a street (fig. Ii). With 
this solution we can dispense with in- cluding commands for selection based on topological relationship 
(c.f. 1451). _~!~_~---~ street p are e------~ [~'~"~~ street-name parcel-nr area-nr area-nr geometry 
 geometry Figure II This allows the query  SHOW IN WINDOW THE street (geometry) WHERE street-name 
= 'Hill Drive' THEREOF ALL adjacent-parcels (geometry)~ 7. Form of representation For each application 
a basic list of conven- tional signs has to be prepared. This list should include signs with gradual 
differences to represent different classes of similar objects (fig. 12). These conventional signs are 
implemented as procedures, using the routines of a general graphics package. Provisions for adding new 
signs should be made. The query language must include a command to choose a certain sign for a class 
of objects. We propose SHOW ... ALL streets (geometry) AS street-sign-l! house-house-house- sign-i 
sign-2 sign-3 Figure 12 Further, the query language should allow to separate one group of objects into 
several classes depending on the values in a certain field. An example how to graphically differentiate 
houses depending on their ages: SHOW IN WINDOW THE area (geometry) WHERE area-name = 'TALACHER' THEREOF 
ALL streets (geometry) ALL houses (geometry) WITH year-built <1850 AS Hous-sign 1 WITH year-built 
>1850 AND year-built > 1900 AS Hous-sign-2 WITH year-built > 1900 AS Hous-sing-3: There a very interesting 
problem appears: which objects can be represented by which conventional signs. Traditionally, conventional 
signs in carto- graphy are classified into points, lines and areas with distinct or open limits. Further 
research will show what types of signs can be used to represent which objects. Two different aspects 
must be con- sidered. the possibility of interpretation of the result by the human user - the interfacing 
between the data producing routines of the data base system and the routines for drawing. 8. Conventional 
Form and Contents of Output In daily routine the user must be able to state in the shortest possible 
form that he wishes a certain representation. It is customary to see cer- tain things in a map, as customary 
as to use certain signs to show the objects. It is therefore necessary that with evoking a prestored 
map type the user not only selects the cartographic signs to be used for the different en- tities, but 
also a standard selection of contents. In these cases, the formulated query rather des- cribes the window 
to be chosen (e.g. by sta~ing a uniquely selected object) or indicates some ad- ditional (preferably 
highlighted) information asked for.  In our proposition for a query language, after SHOW the name of 
the type of presentation needed can be included. In this short form the query from the second example 
is only: SHOW map-type-i IN WINDOW THE house (geometry) WHERE street-name = 'Hill Drive' AND house-nr 
= 7~ but the result shows also houses, parcels and streets, i.e. all the details needed for interpre- 
tation, because the query is automatically completed with the prestored selections. 9. Graphical Input 
 If some pictures are already drawn on the screen, it seems most natural to formulate a query with graphical 
input. There is little difficulty to permit the selec- tion of the key words of a query with a menu 
and a pointing device 1361. More important is to be able to include the variable parts of a query. The 
last query would be simpler, replacing the description of the house by pointing on the screen: SHOW 
map-type-I IN WINDOW THE house (geometry) X where the 'X' indicates that the user will indicate the 
house desired by pointing on the screen (this is similar to 'Query by Pictorial Example' 141). In case 
where more than one object can be identified through pointing at one point (e.g. house or parcel), this 
requires a subtle inter- action between the query analyzer and the PICK function. Depending on the syntactical 
context, the result must be the address of a house or the number of a parcel. i0. Related Work In 151, 
181 an overview of pictorial and geo- graphical data bases and their query languages has been given. 
Therefore, we can restrict our dis- cussion to the main features of the present proposal. The language 
proposed here is designed for 'skilled frequent user' 1381. It seems of prime importance that simple 
daily problems can be solved with short queries easy to formulate: nevertheless, extensions to specify 
more complex retrievals should be possible. We therefore opted for a formal language, permitting well 
chosen default values and did not consider systems like Query-by-Example 1481 or derivates extended 
to graphical applications QPE 141,1DAMS 1341. Some systems (GEO-QUEL 1201, CADS 1281, GRAIN 171) separate 
the selection of a map's contents from its display. This allows for an easy implemen- tation of manipulation 
routines treating the re- trieved map-data, but we preferred to hand a single step solution to the daily 
user. Nevertheless,  such a splitting-up could easily be introduced in References the design here proposed. 
 i. II. Conclusion 2. A query language for retrieval of geometric data and their graphical representation 
has to be adapted to the special needs of such systems. Traditional query languages for commercial systems 
cannot be used without changes. 3. The special problems are - within the limits of the screen a multitude 
of objects have to be shown, -window is selected either with coordinate  4. values or 'reasonable' 
in respect to an object of interest, - language must allow for constructs yielding several object types 
shown independently on the same screen,  5. - user should be able to evoke with one name a prestored 
style of representation, - this style includes also a standard choice  6. of object types, - differentiation 
of object classes based on an attribute and their differentiated  7. representation must be possible, 
 - graphical input must be acceptable wherever the user prefers this form. A proposal for such a query 
language has been  8. described. This proposal features: - automatic adjustment of a window including 
9. the object of interest, - use of context during selection of objects in order to shorten query specifications, 
 - including selection of cartographic signs  i0. for representation, - recall of standard selection 
and standard representation. It is implemented in a restricted fashion in  ii. a prototype of a Land 
Information System at ETH Jl81. Acknowledgments 12. I thank Prof. R. Conzett and Prof. C.A. Zehnder 
(Institut for Informatik) and their assistants for many fruitful discussions. Special thanks go to Beat 
Sievers and Bruno Sp~ni who 13. during years of practical work came to know the needs of the users and 
looked to the appropriate- ness of this query language. 14. Comments on an earlier version of this 
paper by Lothar GrOndig (Heidelberg) and Markku Tamminen (Helsinki) helped much to clarify the ideas. 
 15. The reviewers' comments contributed much to the readability of this paper. The implementation 
is done on the DEC-IO computer of the 'Zentrum for interaktives Rech- hen' and I am grateful for the 
permission to use 16. these computer facilities. Blaser, Albrecht, Lehmann, Hubert: Abfragespra- chen 
in Datenbanken, in: IBM Nachrichten 30 (1980) Heft 251. Chamberlin, D.D., Boyce, R.F.: SEQUEL: A Structured 
English Query Language, in: Proc. of 1974 ACM SIGFIDET Workshop on Data Description, Access and Control, 
Ann Arbor Michigan, 1974, p. 713 -775. Chamberlin, D.D. et al.: Sequel 2: A unified approach to data 
definition, manipulation and control, in: IBM J. Res. Develop. 20 (1976) p. 560 -575. Chang, N.S., 
Fu, K.S.: A Query Language for Relational Image Database Systems, Proc. IEEE Workshop Picture Data Description 
and Manage- ment, Aug. 1980, pp. 67 -73. Chang, N.S., Fu, K.S.: Picture Query Languages for Pictorial 
Data-Base Systems, Computer Nov. 1981, p. 23 Chang, S.K., Fu, K.S. (Ed.): Pictorial Infor- mation Systems, 
Springer-Verlag 1980 Chang, S.K., Lin, B.S., Walser, R.: A Gener- alized Zooming Technique for Pictorial 
Data Base Systems, AFIPS Conf. Proc., 1979 NCC, Vol. 48, pp. 147 -156. Chang, S.K, Kunii, T.: Pictorial 
Data-Base Systems, in: Computer Nov. 1981, p. 13. Childs, D.L.: Extended Set Theory. A general model 
for very large, distributed, backend information systems, in: 3rd Internat. Conf. on Very Large Data 
Bases, Tokyo 1977. Codd, E.F.: Relational Compleneness of data base sublanguages, Courant Computer 
Science Symposium 6, "Data Base Systems", R. Rustin (Ed.) Prentice Hall, 1972, IBM RJ 987. Codd, E.F.: 
Seven Steps to Rendezvous with the casual user, in "Data Base Management" (Ed.: Klimbie/Koffeman) North 
Holland, Amsterdam 1974. Conzett, R.: Landinformationssystem, Vortrag an Tag der offenen THr, 12. 
April 1980, Institut fHr Geod~sie und Photogrammetrie, ETH ZOrich, 1981 Date, C.J.: An introduction 
to database systems, Addison Wesley, Reading 1979, 2nd ed. Dieckmann, E.M.: Three relational DBMS 
 (Oracle, Ingres, SQL/DS), Datamation Sept. 1981. button, G. (Ed): First international advanced study 
symposium on topological data structures for geographic information systems, Harvard Papers on Geographic 
Information Systems, Harvard University, Cambridge, March 1978. Eichhorn, G.: Land Information Systems. 
Symposium of the F~d~ration Internationale des G~om~tres, October 16 -25, 1978, Darmstadt 206 17. Frank, 
A.: Applications of DBMS to Land Information Systems. 7th Internat. Conf. on Very Large Data Bases, Cannes 
(France), 1981, p. 448.  18. Frank, A.: MAPQUERY: Design and implementation, Problems of realization 
of Land Information Systems, Part 2, Report 56, Institut for Geo- dgsie und Photogrammetrie, Swiss Federal 
Institute of Technology, Zurich, Switzerland.  19. Frank, A.: Data Structure and Data Description for 
Geo Data Systems, Problems of realization of Land Information Systems, Part 3, Report 57, Institut fdr 
Geod~sie und Photogrammetrie, Swiss Federal Institute of Technology, Zurich, Switzerland.  20. Go, A., 
Stonebraker, M., Williams, C.: An approach to implementing a Geo-Data System. Electronics Research Laboratory, 
College of Engineering, University of California, Berkeley, Meno ERL-M 529, 25 June 1975.  21. Herot, 
Christofer; F., Carling, Richard; Friedel, Mark; Kramlich, David: A Prototype Spatial Data Management 
System. Siggraph '80, ACM Computer Graphics, Vol. 14, No. 3, p.63, July 1980.  22. Hooper, K.: Experimental 
Mapping. The per- ceptual representation of environments. University of California, Santa Cruz, Technical 
Report.  23. Johnson, J.: Intellect on Demand: INTELLECT, Datamation Nov. 1981, p.73.  24. Jones, P.F.: 
Four principles of man-computer dialogue, in: Computer Aided Design Journal, Vol. i0, No. 3, May 1978. 
 25. Krause, J., Lehmann, H.: User Speciality Language. A natural language based on infor- mation system 
and its evaluation. Int. Conf.on Literary and Linguistic Computing, Bonn, Dec. 1979.  26. Krause, J.: 
Natural Language Access to Information Systems. An Evaluation Study of its Acceptance by End Users, in: 
Inform. Systems, Vol. 5, No. 4, May 1980, pp 297-318.  27. M &#38; S: The use of computer graphics for 
utilities mapping and records, Report 78-090 A, M &#38; S Computing Inc. 1978.  28. Mantey, P.E., Carlson, 
E.D.: Integrated Geographic Data Bases: The GADS Experience, in: Data Base Techniques for Pictorial Appli- 
cations, ed. by A. Blaser, Springer-Verlang, 1979, pp. 173 -198.  29. Meier, A., Zehnder, C.A.: Fl~chenmodell- 
Register wichtiger geographischer Daten- sammlungen der Schweiz. Institut for Infor- matik ETH ZOrich, 
Bericht Nr. 39, 1980.  30. Meister, D.: Behavioral foundations of system development. New York: Wiley, 
1976.  31. Montgomery, C.A.: Is Natural Language an Unnatural Query Language, Proceedings of the ACM 
Annual Conference, New York, 1972, p.i075.   32. Newman, W.M., Sproull, R.F.: Principles of Interactive 
Computer Graphics, 2nd ed. Mc Graw-Hill 1979.  33. Reisner, P.: Human factors studies of Data- base 
Query Languages, a survey and assessment, ACM Surveys Vol. 13, No. I, March 1981, p.13.  34. Schmutz, 
H.: The Integrated Data Analysis and Management System for Pictorial Applications, in Data Base Techniques 
for Pictorial Appli- cations, ed. by A. Blaser, Springer-Verlag, 1979, pp. 475 -493.  35. Senko, M.E.: 
Data description language in the concept of multilevel structured description: DIAM II with FORAL. Douqu~ 
BCM. Nijssen GM. Ed., Data Base Description 1975.  36. Senko, M.E.: FORAL LP: design and implemen- tation, 
VLDB 1978, Berlin.  37. Senko, M.E.: A query-maintenance language for the data independent accessing 
model II, in: Inform. Systems, Vol. 5, 1980.  38. Shneiderman, B.: Improving the human factors aspects 
of database transactions. ACM Transaction on Database Systems, Vol. 3, No. 4, December 1978  39. Stonebraker, 
M.; Wong, E.; Kreps, P.; Held, G.: The Design and Implementation of INGRES, ACM Transaction on Database 
Systems, Vol. i, No. 3, September 1976, p. 189.  40. Stonebraker M.; Rowe, L.A.: Observations on Data 
Manipulation Languages and their Embedding in General Purpose Programming Languages. Pro- ceedings 3rd 
Int. Conf. on Very Large Data Bases, IEEE, New York, 1977, p. 128.  41. Synercom: Synercom Information 
and Mapping System, User manual. Synercom Technology, Inc., 1980.  42. Tan~inen, M.: Management of spatially 
referenced data. Conceptual study of system requirements and structure. Report, Helsinki University of 
Technology, Finnland.  43. Thomas, John C.: Psychological Issues in Data Base Management. Proceedings 
3rd Int. Conf. on Very Large Data Bases, IEEE, New York, 1977,  p. 169.  44. Thompson F.B.; Thompson 
B.H.: Practical natural language processing. The REL system as proto- type, in: Advances in computers, 
Vol. 13, New York 1975.  45. Tsurutani, T.; Kasakawa, Y; Naniwada, N.: ATLAS: A geographic data base 
system, in: ACM Computer Graphics, Vol. 14, No. 3, July 1980. SIGGRAPH 80, Conf. Proceedings.  46. Ullman, 
J.D.: Principles of Data Base Systems. Pitman London 1980.  47. Wild: Wildmap, Interactive Photogranmletric 
Data-Base and Mapping System. Operators Procedure Manual, Wild (Heerbrugg) 1980.  48. Zloff, M.: Query 
by Example, AFIPS Conference Proceedings, 1975 National Computer Conference, Vol. 44, AFIPS Press, 1975, 
p. 431.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801282</article_id>
		<sort_key>209</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>31</seq_no>
		<title><![CDATA[The challenge of CAD/CAM education]]></title>
		<page_from>209</page_from>
		<page_to>211</page_to>
		<doi_number>10.1145/800064.801282</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801282</url>
		<abstract>
			<par><![CDATA[<p>Colleges and universities are not meeting industry needs for graduates trained in the use or implementation of computer-aided design and manufacturing systems since few schools have experience in teaching CAD/CAM. Furthermore, many Bachelor's-level graduates are going directly into industry, rather than pursuing graduate degrees, thereby compounding this problem. Members of this panel represent both schools and industry and together attempt to outline approaches to developing more extensive CAD/CAM emphasis in education, including</p> <p>- the role colleges and universities can or should play in addressing industry needs in this area</p> <p>- user- and implementor-oriented CAD/CAM education</p> <p>- CAD/CAM education as an integral part of university engineering curricula</p> <p>- problems encountered in organizing and implementing CAD/CAM curriculum</p> <p>- universities' relationships with industry.</p> <p>It is our hope that panelists' experiences will help guide schools in structuring CAD/CAM programs to best insure that the United States remain at the forefront of technology and compete industrially world-wide.</p> <p>Frank Puhl</p> <p>Victor Langer</p> <p>Donald P. Greenberg</p> <p>Mark S. Shepard</p> <p>Herb Voelcker</p> <p>Universities' Relationships With Industry</p> <p>Frank Puhl</p> <p>American industry needs engineers who have been brought up with the realization that computers are intimately involved in all aspects of design, analysis, and manufacturing. Industry, of course, needs engineers who are already trained in fundamental CAD/CAM principles and in solving real world problems using computers, as well as those who have been taught to be productivity- and cost-conscious.</p> <p>To help meet these needs, we must now provide answers to several significant questions:</p> <p>What part should industry play in encouraging and sponsoring research and education in CAD/CAM?</p> <p>How can we resolve conflicts between industries' &#8220;trade secrets&#8221; and universities' &#8220;open research&#8221;?</p> <p>How can industry and educational institutions work together to increase U.S. productivity so as to regain our competitive edge?</p> <p>Lockheed Corporation and CADAM, Inc. are addressing these issues by supporting selected schools, installing CADAM systems, and providing fellowships for education and research at upperclass and graduate levels. In addition, we are involved in various joint research projects with universities, and we look optimistically toward continuing these projects.</p> <p>CADD CAM User EducationCADD CAM User Education</p> <p>Victor Langer</p> <p>General Electric Medical Systems Division in Milwaukee experienced a severe shortage of CADD CAM operators and encouraged MATC to develop a training program which started in 1980. A three-year NSF- CAUSE grant and a partial donation of a Computervision CADDS 3 (now CADD 4 Designer V six-station) system resulted in a program for upgrading employed designers and for two-year associate degree full-time students. The program enrolls 200 students per semester, with 70% continuing education students and 30% full-time students. Before students can effectively apply CADD CAM education, they must have drafting and design experience or at least a year of engineering training, plus ability to use spatial relationships and a course in descriptive geometry. With this background, students learn to create geometry for a 2-D drawing database and gain sufficient experience in the first course to be as productive as employees with six months' full-time experience. In the final semester or in the second advanced course, students create 3-D geometry and apply analytical computer capabilities for design, specializing for uses in mechanical, electrical, structural, architectural, and graphics arts applications. The defined geometry is also used in CAM for generating numerical control machining and flame-cutting paths, and for robotic control.</p> <p>Each course has been evaluated, follow-up studies have been completed, and an advisory committee has guided development. Employees have verified success and ease in transferring geometric skills to many different CADD CAM systems in the market. Beginning in the Fall of 1982, Apple microcomputers will be used to teach all 2-D computer graphics skills previously taught on the Computervision system, and CADD CAM education is now becoming available on an economical basis to all users.</p> <p>Some Problems in CAD Education</p> <p>Donald Greenberg</p> <p>It is imperative that universities educating the next generation of engineers introduce computer-aided design courses into their curricula. There are many obstacles in accomplishing this within a university structure. This presentation describes the facilities and operation at Cornell University and discusses the potential benefits and difficulties.</p> <p>CAD/CAM Education in an Engineering Curriculum</p> <p>Mark S. Shepard</p> <p>Today there is a large industrial demand for engineering graduates that understand CAD/CAM techniques and computer graphics. Therefore, many colleges and universities already have or are planning to introduce computer graphics and CAD/CAM concepts into their curriculum. The major questions to be addressed in integrating these techniques into the curriculum include type and amount of hardware, development and maintenance of software and method of introduction into the curriculum.</p> <p>In 1977, RPI's school of engineering established the Center for Interactive Computer Graphics which is charged with integrating interactive computer graphics into the entire undergraduate engineering curriculum and providing a facility for graduate instruction and research. With heavy industrial support, the Center has also developed a research program in computer graphics and CAD/CAM. This presentation will discuss RPI's overall approach to integrating interactive computer graphics into the engineering curriculum.</p> <p>A Postgraduate Program in &#8220;Programmable Automation&#8221;</p> <p>Ari Requicha</p> <p>Herb Voelcker</p> <p>&#8220;Programmable Automation&#8221; designates the emerging body of knowledge surrounding CAD/CAM and industrial robotics. Graduate study in the field is aimed at (1) understanding the informational aspects of design and production in the discrete goods industries, and (2) developing new technologies for producing goods automatically with programmable, general-purpose tools. Some of the knowledge and techniques used in Programmable Automation are drawn from established fields (computer science, material science, control theory, ...), but the distinctive character of Programmable Automation is set mainly by the pervasive roles played by <underline>geometry</underline> and <underline>computation</underline>.</p> <p>A postgraduate program in Programmable Automation is being launched at the University of Rochester to train MS/level systems engineers for industry, and Ph.D-level researchers and teachers. The program's evolution reflects a &#8220;trickle-down&#8221; philosophy of education, wherein major new fields enter engineering education through on-going research; research begets seminars, seminars sometimes evolve into graduate courses, and graduate courses sometimes spawn undergraduate courses. (Put differently, the process starts with mature minds grappling with poorly understood concepts and ends with immature minds assimilating tightly codified concepts.)</p> <p>The Rochester program is sited in Electrical Engineering and draws heavily on the staff and facilities of the Production Automation Project; it also has strong links with Mechanical Engineering and Computer Science. The initial curriculum is based on two core courses in computational geometry, a graphics lab, and a systems seminar; these are supplemented with established courses in computer science, digital systems, finite-element analysis, control theory, and so forth. An NC Systems course and lab will be introduced a year hence. Plans for linking the program with Rochester's VLSI program, and for launching robotics research and teaching, are still in an embryonic stage.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.6</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>J.1</cat_node>
				<descriptor>Education</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010432.10010439</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P332494</person_id>
				<author_profile_id><![CDATA[81100608271]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michel]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Melkanoff]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UCLA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330278</person_id>
				<author_profile_id><![CDATA[81332522191]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Puhl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[President, CADAM, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334189</person_id>
				<author_profile_id><![CDATA[81100288025]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Langer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Project Director, Milwaukee Area Technical College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Director, Program of Computer Graphics, Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332328</person_id>
				<author_profile_id><![CDATA[81100539173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mark]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Shepard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rensselaer Polytechnic Institute]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330776</person_id>
				<author_profile_id><![CDATA[81332533732]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Herb]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Voelcker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Production Automation Project, University of Rochester]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Challenge of CAD/CAM Education CHAIR: Michel A. Melkanoff UCLA Colleges and universities are not 
meeting industry needs for gra- duates trained in the use or implementation of computer-aided design 
and manufacturing systems since few schools have experience in teach- ing CAD/CAM. Furthermore, many 
Bachelor's-level graduates are going directly into industry, rather than pursuing graduate degrees, thereby 
compounding this problem. Members of this panel represent both schools and industry and together attempt 
to outline approaches to developing more extensive CAD/CAM emphasis in education, including - the role 
colleges and universities can or should play in addressing industry needs in this area - user-and implementor-oriented 
CAD/CAM education - CAD/CAM education as an integral part of university engineer- ing curricula - 
problems encountered in organizing and implementing CAD/CAM curriculum - universities' relationships 
with industry. It is our hope that panelists' experiences will help guide schools in structuring CAD/CAM 
programs to best insure that the United States remain at the forefront of technology and compete industrially 
world- wide. (Edited by Larry Lichten, UCLA, panel coordinator.) PANELISTS: Frank Puhl Mark S. Shepard 
President Rensselaer Polytechnic Institute CADAM, Inc. Victor Langer Herb Voelcker Computer Graphics 
Project Director Production Automation Project Milwaukee Area Technical College University of Rochester 
 Donald P. Greenberg Director, Program of Computer Graphics Cornell University Abstract: Universities' 
Relationships With Industry Frank Puhl CADAM, Inc. American industry needs engineers who have been 
brought up with the realization that computers are intimately involved in all aspects of design, analysis, 
and manufacturing. Industry, of course, needs engineers who are already trained in fundamental CAD/CAM 
principles and in solving real world problems using computers, as well as those 209 PANEL (continued): 
The Challenge of CAD/CAM Education who have been taught to be productivity- and cost-conscious. To 
help meet these needs, we must now provide answers to several significant quesulons: What part should 
industry play in encouraging and sponsoring research and education in CAD/CAM? How can we resolve zonflicts 
between industries' "trade secrets" and universities' "open research"? How can industry and educational 
institutions work together to increase U.S. productivity so as to regain our competitive edge? Lockheed 
Corporation and CADAM, Inc. are addressing these issues by supporting selected schools, installing CADAM 
systems, and providing fellowships for education and research at upperclass and graduate lev- els. In 
addition, we are involved~n various joint research projects with universities, and we look optimistically 
toward continuing these projects. Abstract: CADD CAM User Education Victor Langer Milwaukee Area Technical 
College General Electric Medical Systems Division in Milwaukee experi- enced a severe shortage of CADD 
CAM operators and encouraged MATC to develop a training program which started in 1980. A three-year NSF- 
CAUSE grant and a partial donation of a Computervision CADDS 3 (now CADD 4 Designer V six-station) system 
resulted in a program for upgrading employed designers and for two-year associate degree full- time students. 
The program enrolls 200 students per semester, with 70% continuing education students and 30% full-time 
students. Before students can effectively apply CADD CAM education, they must have drafting and design 
experience or at least a year of engineering training, plus ability to use spatial relationships and 
a course in descriptive geometry. With this background, students learn to create geometry for a 2-D drawing 
database and gain sufficient experience in the first course to be as productive as employees with six 
months' full-time experience. In the final semester or in the second advanced course, students create 
3-D geometry and apply analytical computer capabilities for design, specializing for uses in mechanical, 
electri- cal, structural, architectural, and graphics arts applications. The defined geometry is also 
used in CAM for generating numerical control machining and flame-cutting paths, and for robotic control. 
 Each course has been evaluated, follow-up studies have been com- pleted, and an advisory committee has 
guided development. Employees have verified success and ease in transferring geometric skills to many 
different CADD CAM systems in the market. Beginning in the Fall of 1982, Apple microcomputers will be 
used to teach all 2-D computer graphics skills previously taught on the Computervision system, and CADD 
CAM education is now becoming available on an economical basis to all users. Abstract: Some Problems 
in CAD Education Donald Greenberg Cornell University It is imperative that universities educating the 
next generation of engineers introduce computer-aided design courses into their curri- cula. There are 
many obstacles in accomplishing this within a univer- sity structure. This presentation describes the 
facilities and opera- tion at Cornell University and discusses the potential benefits and difficulties. 
 210 PANEL (continued): The Challenge of CAD/CAM Education Abstract: CAD/CAM Education in an Engineering 
Curriculum Mark S. Shepard Rensselaer Polytechnic Institute Today there is a large industrial demand 
for engineering gradu- ates that understand CAD/CAM techniques and computer graphics. There- fore, many 
colleges and universities already have or are planning to introduce computer graphics and CAD/CAM concepts 
into their curricu- lum. The major questions to be addressed in integrating these tech- niques into the 
curriculum include type and amount of hardware, development and maintenance of software and method of 
introduction into the curriculum. In 1977, RPI's school of engineering established the Center for Interactive 
Computer Graphics which is charged with integrating interactive computer graphics into the entire undergraduate 
engineer- ing curriculum and providing a facility for graduate instruction and research. With heavy industrial 
support, the Center has also developed a research program in computer graphics and CAD/CAM. This presentation 
will discuss RPI's overall approach to integrating interactive computer graphics into the engineering 
curriculum. Abstract: A Postgraduate Program in "Programmable Automation" Ari Requicha and Herb Voelcker 
University of Rochester "Programmable Automation" designates the emerging body of knowledge surrounding 
CAD/CAM and industrial robotics. Graduate study in the field is aimed at (I) understanding the informational 
aspects of design and production in the discrete goods industries, and (2) developing new technologies 
for producing goods automatically with programmable, general-purpose tools. Some of the knowledge and 
tech- niques used in Programmable Automation are drawn from established fields (computer science, material 
science, control theory, ...), but the distinctive character of Programmable Automation is set mainly 
by the pervasive roles played by aeometrv and computation. A postgraduate program in Programmable Automation 
is being launched at the University of Rochester to train MS/level systems engineers for industry, and 
Ph.D-level researchers and teachers. The program's evolution reflects a "trickle-down" philosophy of 
education, wherein major new fields enter engineering education through on-going research; research begets 
seminars, seminars sometimes evolve into graduate courses, and graduate courses sometimes spawn undergraduate 
courses. (Put differently, the process starts with mature minds grap- pling with poorly understood concepts 
and ends with immature minds assimilating tightly codified concepts.) The Rochester program is sited 
in Electrical Engineering and draws heavily on the staff and facilities of the Production Automation 
Project; it also has strong links with Mechanical Engineering and Com- puter Science. The initial curriculum 
is based on two core courses in computational geometry, a graphics lab, and a systems seminar; these 
are supplemented with established courses in computer science, digital systems, finite-element analysis, 
control theory, and so forth. An NC Systems course and lab will be introduced a year hence. Plans for 
linking the program with Rochester's VLSI program, and for launching robotics research and teaching, 
are still in an embryonic stage. 211 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801283</article_id>
		<sort_key>213</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>32</seq_no>
		<title><![CDATA[3D Galatea]]></title>
		<subtitle><![CDATA[Entry of three-dimensional moving points from multiple perspective views]]></subtitle>
		<page_from>213</page_from>
		<page_to>222</page_to>
		<doi_number>10.1145/800064.801283</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801283</url>
		<abstract>
			<par><![CDATA[<p>We describe an interactive graphics system for the entry of three-dimensional moving points from multiple perspective views. This work represents a major extension of Galatea, our system for graphics-assisted 2D motion analysis. 3D Galatea permits reconstruction of 3D time-dependent positions from 2D entries in two or more perspective views.</p> <p>The system supports a general approach for calibrating perspective views. This method, based on work of Sutherland, uses a known 3D reference object to calibrate completely arbitrary perspective projections. A somewhat restricted class of perspective views may be calibrated without an explicit calibration object using another approach developed from photogrammety.</p> <p>In 2D Galatea, we have used an animated graphics overlay onto the source image to give the analyst feedback regarding current and previous data entries. This capability is extended in 3D Galatea by overlaying auxiliary lines, which are the backprojections of previous 2D entries from one view into other views. This concept amounts to a fourth interpretation of the well-known Roberts homogeneous matrix equation describing perspective projections of 3D space into a 2D image. The auxiliary line is useful in locating a point which is obscured in one of the images, or in determining the correspondence of projected points as seen in different views, which may be ambiguous or easily confused.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Range data</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Stereo</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.4.8</cat_node>
				<descriptor>Time-varying imagery</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.3</cat_node>
				<descriptor>Health</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010449</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Health informatics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010446</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Consumer health</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010225.10010227</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision tasks->Scene understanding</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245.10010248</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems->Video segmentation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010245</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision problems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P333853</person_id>
				<author_profile_id><![CDATA[81100236432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Steven]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[MacKay]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Biophysics and Theoretical Biology, Department of Radiology, University of Chicago, Chicago, Illinois]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333312</person_id>
				<author_profile_id><![CDATA[81100416288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Sayre]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Biophysics and Theoretical Biology, Department of Radiology, University of Chicago, Chicago, Illinois]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31092844</person_id>
				<author_profile_id><![CDATA[81100352220]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Potel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Biophysics and Theoretical Biology, Department of Radiology, University of Chicago, Chicago, Illinois]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aisen, A. M., Al-Sadir, J., MacKay, S. A., Potel, M. J., Rubin, J. M., and Sayre, R. E. Quantitative ventricular wall motion analysis from biplane coronary angiograms. Proc. IEEE Computers in Cardiology, (1980), 443-446.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cullen, J. M., Shaw, E., and Baldwin, H. A. Methods for measuring the three-dimensional structure of fish schools. Animal Behaviour 13, (1965), 534-543.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R. P. and Potel, M. J. The system design for Galatea, an interactive real-time graphics system for movie and video analysis. Computers and Graphics 1, (1975), 115-121.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Graybill, F. A. Introducton to Matrices with Applications in Statistics. Belmont, CA.: Wadsworth Publishing Co. (1969).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Longuet-Higgins, H. C. A computer algorithm for reconstructing a scene from two projections. Nature (London) 293, (1981), 133-135.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[MacKay, S. A. A system for graphics assisted entry of 3D points. Proc. IEEE 4th Int'l. Computer Software and Applications Conf, (COMPSAC'80), (1980), 128-134.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Merchant, J. Exact area registration of different views of a common object scene. Optical Engineering 20, (1981), 424-436.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. and Sproull, R. F. Principles of Interactive Computer Graphics, (2nd edition). New York: McGraw-Hill, (1979).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Partridge, B. L. and Cullen, J. M. A low cost interactive coordinate plotter. Behavior Research Methods and Instrumentation 9, (1977), 473-479.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563285</ref_obj_id>
				<ref_obj_pid>965143</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J. and Sayre, R. E. Interacting with the Galatea film analysis system. ACM Computer Graphics 10, (1976), 52-59.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J. and Sayre, R. E. Data environment for a laboratory film analysis system. Proc. IEEE 1st Int'l. Computer Software and Applications Conf. (COMPSAC '77,) (1977), 800-806.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J., MacKay, S. A., and Sayre, R. E. Galatea, an interactive computer graphics system for movie and video analysis. Proc. 15th Int'l. Cong. on High Speed Photography and Photonics, in press, (1982).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J., Sayre, R. E., and MacKay, S. A. Graphics tools for interactive motion analysis. Comput. Graphics Imag. Proc. 14, (1980), 1-23.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J., Sayre, R. E., and Robertson, A. A system for interactive film analysis. Comput. Biol. Med. 9, (1979), 237-256.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Potel, M. J., Rubin, J. M., MacKay, S. A., Aisen, A. M., Al-Sadir, J., Sayre, R.E. Evaluation of heart wall motion in 3D using biplane coronary angiograms. Investigative Radiology 16, (1981), 5423.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108781</ref_obj_id>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K. Digital Image Processing. New York: Wiley, (1978).]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Roberts, L. G. Homogeneous matrix representation and manipulation of n-dimensional constructs. Document MS 1405, Lincoln Laboratory, MIT, Cambridge, MA., (1965).]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>63448</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Rogers, D. F. and Adams, J. A. Mathematical Elements for Computer Graphics. New York: McGraw-Hill, (1976).]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Rubin, J. M. and Sayre, R. E. A computer-aided technique for overlaying cerebral angiograms onto computed tomograms. Investigative Radiology 13, (1978), 362-367.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sayre, R. E., Rubin, J. M., Duda, E. E., and Patronas, N. J. Quantitative three-dimensional angiograms: application, including augmentation of computed tomograms. Proc. IEEE Conf. on Computer Applications in Radiology, (1979), 95-102.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Slama, C. C., Theurer, C., Henrikson, S. W., (eds). Manual of Photogrammetry. American Society of Photogrammetry, Falls Church, VA., (1980).]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. E. Three-dimensional data input by tablet. Proc. IEEE 62, (1974), 454-461.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Ullman, S. The Interpretation of Visual Motion. MIT Press: Cambridge and London, (1979).]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Underwood, E. E. Quantitative Stereology. Reading, MA.: Addison-Wesley, (1970).]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Galatea: Entry of three-dimensional moving points from multiple perspeetive views Steven A. MacKay, 
Richard E. Sayre, and Michael J. Potel Department of Biophysics and Theoretical Biology Department of 
Radiology University of Chicago Chicago, minois 60637  Abstract We describe an interactive graphics 
system for the entry of three-dimensional moving points from multiple perspective views. This work represents 
a major extension of Galatea, our system for graphics-assisted 2D motion analysis. 3D Galatea permits 
reconstruction of 3D time-dependent positions from 2D entries in two or more perspective views. The system 
supports a general approach for calibrating perspective views. This method, based on work of Sutherland, 
uses a known 3D reference object to calibrate completely arbitrary perspective projections. A somewhat 
restricted class of perspective views may be calibrated without an explicit calibration object using 
another approach developed from photogrammety. In 2D Galatea, we have used an animated graphics overlay 
onto the source image to give the analyst feedback regarding current and previous data entries. This 
capability is extended in 3D Galatea by overlaying auxiliary lines, which are the backprojections of 
previous 2D entries from one view into other views. This concept amounts to a fourth interpretation of 
the well-known Roberts homogeneous matrix equation describing perspective projections of 3D space into 
a 2D image. The auxiliary line is useful in locating a point which is obscured in one of the images, 
or in determining the correspondence of projected points as seen in different views, which may be ambiguous 
or easily confused. CR Categories and Subject Descriptors. 1.3.6 [Computer Graphics] : Methodology and 
Techniques-Interaction techniques. 1.4.8 [Image Proecssing] : Scene Analysis-Range data; Stereo; Time-varying 
imagery. J.3 [Life and Medical Sciences] -Biology; Health. Permission to copy without fee all or part 
of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. (~) 1982 ACM 0-89791-076-1/82/007/0213 $00.75 
 Introduction Many scientific problems require the acquisition and analysis of three-dimensional (3D) 
x,y,z coordinate positions. Such application arise in biomechanics (joint movement), medicine (blood 
vessel or tumor positions), anatomy (bone structure), behavioral ecology (fish schools or bird flocks), 
and many engineering fields (automotive, aerospace, and aerial survey). Three-dimensional input devices 
are useful if the object of interest can be measured directly, and is of appropriate size. However, most 
scientific problems requiring 3D data entry have as their primary source planar images (photographs, 
movie films, X-ray films, video images, etc.). These planar images are two-dimensional perspective projections 
of the original 3D objects, and the input problem becomes one of inferring three-dimensional positions 
from these planar projections. In various application areas, ad hoc solutions to this problem are the 
rule. Techniques of photogrammetry or stereology [ 24,21] and devices such as stereo comparators are 
adapted to work for particular multiple view situations, such as stereo pairs, orthogonal pairs or other 
fixed geometry imaging situations. However, a general solution requires the ability to support arbitrary 
perspective views arising from non-specific, possibly moving camera positions, inference of view descriptions 
subsequent to imaging, reconstructions from more than 2 views, and high quality interactions and well-defined 
procedures for the analyst. The work reported here describes an interactive animated computer graphics 
system for tracking points through time in three-dimensions from multiple perspective views. This system, 
called 3D Galatea, is an extension of our work with the Galatea system for entering time dependent 2D 
data [3,10,14,13,12]. The principal mathematical formulations on which the system is based are familiar 
to computer graphics practitioners from their use in data output, in the generation of perspective displays. 
Our system illustrates how these same formulations are the basis for data input techniques, for 3D data 
describing phenomenon sampled through perspective views. Our data acquisition system provides several 
advances on previous entry systems. First, it provides a completely generalized approach to the calibration 
of the perspective view [ 22,19,20]. Our system makes use of a calibration object which is imaged in 
the same system as the unknown object. When data entry begins, positions in the calibration object image 
are entered and used to infer a mathematical description of the perspective views. The resulting calibrated 
views may be completely arbitrary -orthogonal, stereo, oblique, whatever. The views are inferred a posteriori, 
that is, the imaging system may be  positioned arbitrarily, with the resulting perspective views calibrated 
after-the-fact. Moreover, the reconstruction geometry is not restricted to two views, but may be comprised 
of any number of views, with more views increasing the accuracy of the inferred 3D positions. A somewhat 
restricted class of perspective views can be calibrated up to a scale factor without an explicit calibration 
object if special conditions exist in the imaging system. If two views are produced with identical cameras, 
a single moving camera, or a moving object, and the location of the projection of the optical axis on 
the image is known, then the relationship between the views may be determined by entering the corresponding 
pairs of image points from a number of unknown object points [ 5]. A second advance of our system is 
that it makes use of an animated computer graphics display superimposed onto the film image, using a 
projection crt and series of mirrors [13]. The computer is interfaced to both the film projector and 
a data tablet, and the generated graphics display shows changes in both devices. Thus, when the analyst 
moves the cursor on the data tablet, he sees its location displayed on the film image. Also, as the film 
projector runs, the animated display changes in frame-by-frame synchrony. In effect, the system acts 
like "dynamic tracing paper" [i0]. This allows the accuracy of the entries to be constantly checked and 
corrected if necessary. The graphics overlay of previously entered pointsmakes it easy for the analyst 
to avoid omission or duplication of points and to resolve ambiguities in the orientation and correspondence 
of points. The most powerful capability provided by the graphics overlay is the display of backprojections 
of positions from one view onto other views. When the position of a point is entered in one 2D perspective 
view, it is constrained to lie on a line in 3D space (Figure la). Using our 3D reconstruction mathematics, 
the equation of this line can be computed and its 2D projection superimposed onto other views using the 
graphics overlay. The point in question must lie along these "auxiliary lines" in the other views. The 
auxiliary lines greatly facilitate locating the corresponding positions in the other views. For example, 
one of the primary applications of this system is the study of the 3D motion of the heart. The points 
of interest are the branch points of large and small coronary arteries, and these bifurcations are occasionally 
on-edge or obscured in one view. But, as long as the bifurcation can be entered in any of the views, 
its position can be entered in the other views as the intersection of the auxiliary line and the large 
vessel. This significantly increases the number of usable points, since points that cannot be entered 
for just a few frames in any view are rendered useless for other data entry methods. The auxiliary line 
technique also is of immense value in identifying which are the corresponding points in the two views 
even when they are clearly seen. Without the auxiliary line, this determination is usually the most time- 
consuming aspect of multi-view data entry and very easy to confuse, especially in applications where 
there is little external information (anatomy, connectivity, or other structure) to guide the correspondence. 
 Possibly the most important aspect of our system is that it provides a high quality interactive graphics 
context for the difficult problem of working with multiple views of time-dependent 3D objects. The analyst 
is presented with 2D moving points, 3D moving points, and perspective view descriptions as graphical 
data types [ 13] to be manipulated. By supporting these abstractions as primitives, the system provides 
a natural conceptual framework for manipulating image views and seeing the relationship between his 2D 
interactions and the 3D object space. In this paper, we will first discuss the overall methodology of 
3D data entry, including the mathematical basis for the 3D reconstructions. Then, we will describe the 
details of how the system is used to calibrate the 3D perspective views and reconstruct 3D positions 
from 2D entries in the perspective views. Finally, we will summarize some of our applications of the 
resulting 3D time-series data. Methodology Our method of entering 3D positions by marking 2D locations 
in multiple perspective views involves four major steps (shown in Fig. 1 a-e). (1) The perspective views 
are calibrated. In the general approach, a calibration object containing points whose 3D positions are 
known is imaged with the same system used to image the unknown object. The analyst enters the 2D projected 
points of this object using the computer system. Fig. lb shows what the analyst sees while he is calibrating 
the view. In the center of the figure is the source image, a frame from an X-ray movie, with an image 
of a typical calibration object, a plexiglas cube with imbedded metal shot and rods at known 3D positions. 
Superimposed on the source image is the computer graphics image, containing command menus and status 
information which are described below. The superimposed cross shows the position of the cursor, which 
is on top of one of the cube points. (2) After determining the view transformations, the analyst enters 
points from the first view. Fig. lc shows what the analyst sees during first view data entry. The source 
image is a frame from a X-ray movie film of a human heart. Radiopaque dye is injected into the coronary 
arteries to make them visible as the light curves in the film. The overlaid cross indicates a point being 
entered at the bifurcation of two vessels at the center of the image. (3) After entering points from 
the first view, the analyst enters points from subsequent views, aided by the auxiliary line, which is 
calculated from the view transformations and the coordinates of a 2D point in the first view. Fig. id 
shows a second view of the same coronary arteries as in Fig. lc, with the auxiliary line and other computer 
graphics overlaid. The auxiliary line intersects the bifurcation near the right edge of the image, indicating 
to the analyst the point entered from the first view. (4) Once projected points of at least two views 
of an unknown 3D point are entered, the 3D coordinates of the point can be reconstructed. The geometric 
relationship between the 2D points [UlVl] , [u2v 2] and the 3D point [xyz] appears in Fi~. la. ~mce much 
of our interest lies in examining the movement of points in three dimensions, the above process is carried 
out for sequences of 2D points entered over many frames of film, resulting in many time series of 3D 
points. These line series can be utilized in a number of analyses. Fig. le is a frame from a stereo pair 
animation of heart wall motion, with the 3D points connected by lines which approximate the coronary 
arteries, and vectors representing the 3D velocity of each of the wall points.  Theory For a particular 
perspective view, the relationship between a 3D point [x,y,z] and its projected point [u,v] is given 
by the matrix equation: [xyz 1] [T] =k [uv 1] , (I) where k is a scaling factor, [T] is the 4 x 3 transformation 
matrix which describes the perspective view, and [x y z 1] and [u v 1] are homogeneous coordinate representations 
of the 3D and 2D points, respectively [17,22,18,8]. The view described by [T] can represent arbitrary 
linear operations on 3D space. Any nonlinear aspects such as projection onto a curved surface (e.g., 
the face of a cathode ray tube) must be handled by polynomial corrections (see below). But otherwise, 
the perspective matrix can describe the complete transformation from actual 3D positions imaged from 
any arbitrarily placed camera to an arbitrarily oriented film plane. Furthermore, the method will also 
correctly handle the subsequent projection of this film image onto an arbitrarily oriented screen, and 
data entry with an arbitrarily oriented 2D digitizing device. Roberts' [17] initial interpretation of 
equation (1) was that it described the projection of known 3D points into 2D. Sutherland [22] provided 
two additional interpretations. If many values of Ix y z] and the corresponding [u v] are known, then 
IT] can be computed in a process called view calibration. Sutherland's second interpretation was that 
if the projected [u v] coordinates of a 3D point are measured for at least two calibrated views, then 
the 3D coordinates of that point can be reconstructed. We provide a fourth interpretation of (1) [ 19,20], 
which allows us to calculate the position of the backprojected auxiliary line if one [u v] and two views 
are known. View ealibration To determine the perspective transformation for a view, the 2D projected 
positions [u:,v:] of points in the calibration object with known 3D lo~a{ions [xl,y~,z i] are entered. 
Each point entered must satisfy equatlor~ (5): [xiYiZi 1] IT] =k [u iv i I] (2) This is a series of 
three linear equations, which can be reduced to two equations by solving for the scaling factor k and 
substituting: (tll -t13ui)xi + (t21 - t23ui)Y i + (t31 - t33ui)z i + (t41 - t43u i) = 0 (t12 - t13vi)xi 
+ (t22 - t23vi)Yi + (3) (t32 - t33vi)z i + (t42 - t43v i) = 0 th th where t.. is the element of [T] 
in the i row and the j column. II Each of the equations (3) represents a plane in three dimensions, and 
the intersection of the two planes in (3) is a 3D line passing through li,.Yi,Z i and ui, vi, shown as 
one of the dashed lines in Fig. If the 2D projections of n 3D points in the calibration object are measured, 
the n pairs of equations of the form of (3) may be solved simultaneously for the unknowns t... This system 
of equations can be represented [~ matrix form as: [A] [T c] = t43[B] (4) where [A] is a 2n x 11 matrix, 
[T c] is an 11 element column matrix containing eleven of the twelve elements of the transformation matrix 
[T] , and [B] is a 2n column matrix. The twelfth matrix element t.^ is an overall scale term and may 
be taken to be equal ~o J 1 without loss of generality. This system of equations is in ii unknowns (the 
elements of [T_], therefore, if 6 or more 2D points are measured, then ~he system is overdetermined and 
can be solved in a least squares sense by using a generalized inverse [4] : [T c] = ([A]t[A])-I[A]t[B] 
(5) which gives the elements of the transformation matrix [T] [22,18]. Several methods are available 
for calibrating a smaller class of perspective views without resorting to a special calibration object 
[23]. In the particular method we have used, the principal restriction imposed upon the views is that 
view 1 can be transformed into view 2 by simply a 3D translation of the center of projection followed 
by a 3D rotation about the new center of projection [5,7]. Any scale changes brought about by different 
camera systems, or subsequent projection must be known independently. Also, the principal point, the 
point on the image plane where the optical axis of the system falls, must be known, as is the case in 
photogrammetric systems. In spite of these restrictions, this method is useful for situations, such as 
surveying, where it is not possible to place a calibration object in the field of view. The method requires 
that 8 corresponding pairs of image points be entered from any unknown object points. The 2D coordinates 
of these 8 points in each of the two views allows the translation and rotation operations to be calculated. 
This calibrates the two perspective views to within a scale factor, which can be resolved if some absolute 
distance is known in the object of interest. 3D Point Reconstructions After the transformation matrices 
[S] for view 1 and [T] for view 2 have been determined, the projected 2D points [ul,v I] and [u2,v 2] 
in each of the two views correspondin~to-the 3D pomt of interest can be measured and the coordinates 
of the 3D point [x, y, z] reconstructed. It is possible to write 2 pairs of equations like (3), which 
determine two lines in three dimensions (the two dashed lines in Fig. la). If the elements of [S] are 
denoted by s i. and those of [T] by tij, then these equations can be v~ritten in matrix form as: s11-s13u 
I s21-s23u1 s31-s33u 1 xl s43u1-s41 s12-s13vl s22-s23v1 s32-s33v 1 yJ s43v1-s42 (6) t11-t13u 2 t21-t23u 
2 t31-t33u 2 zl t43u2-t41 t12-t13v 2 t22-t23v 2 t32-t33v2 This matrix equation represents a system of 
4 equations in 3 unknowns [x,y,z] and thus, as in (5) above, the solution is found in a least squares 
sense by a generalized inverse. If more than 2 views are available, the additional views just add more 
rows to equation (6), and the least-squares solution for [x,y,z] is that much better determined.  Computer 
Graphics Calculation of auxiliary lines Fig. la shows another way equation (6) can be interpreted. If 
the two views [S] and [T] are known, and a 2D projected point [ul,vl] in view I ([S]) are entered, then 
a line is defined*in'three dimensions which passes through [ul,vl] and the 3D point [x,y,z], as shown 
by one of th6 d~shed lines in Fig. la. This 3D line can then be backprojected into the second view ([T]) 
as a 2D line on which [u~,v~] must lie. This projected line can also be calculate0 fFom equation (6), 
which represents an overdetermined system of 4 equations in 3 unknowns [x,y,z]. If one of the rows of 
(6) was eliminated, it would still be possible to solve for [x,y,z]. If u I and v I have been specified 
in the first view, then u 2 ~ selected as, say, the u coordinate of the left edge of the film image. 
This system of 3 equations is solved for [x,y,z] , which is projected through transform [T] to produce 
[ug, vg], the intersection of the auxiliary line with the left e~gd'of the film image. This process is 
repeated for any of the other three edges of the film image to give the position of the auxiliary line 
projected into view 2. Each additional view generates another auxiliary line. All the auxiliary lines 
will, given correct data entry, intersect at the projection of the 3D point of interest. Use of graphical 
overlays From the above discussion, one can see that to calibrate a view, the analyst must enter the 
locations of the projections of several 3D points, and must define at least 2 such views in order to 
correctly reconstruct unknown 3D points from the locations of their 2D projections. Clearly, the positions 
of 2D projected points must be entered correctly in both the calibration and reconstruction phases in 
order to compute the 3D points correctly. Thus, the system should provide information to the analyst 
about the accuracies of the calibration and of the 3D reconstructions as the 2D data entries proceed. 
The other requirement for correct 3D reconstruction is that the 2D projected points entered from different 
views must correspond to the same 3D point. Our experience has shown that in many instances the points 
of interest are obscured or ambiguous in one of the views. Therefore, it is extremely useful for the 
system to provide information about the possible positions of a 2D projected point in one view given 
the location of a 2D projected point from another view. To solve both the accuracy problem and the correspondence 
problem, we have used an animated computer graphics display superimposed onto the film image and synchronized 
with the film projector [13,6]. The source image (movie film, photographic slide, etc.) is front-projected 
onto a large (110 x 120 cm) screen, and a specially-constructed projection crt (based on a RCA 4862 projection 
kinescope and Schmidt reflective optics) is used to overlay the graphics image directly onto the film 
image. The projection crt is driven by a vector graphics display processor (DEC VT-11), which produces 
a points- and--lines graphics image. This image is updated in real- time by the system minicomputer (DEC 
PDP-II/40 with 56 kilobytes of memory and two 2.4 megabyte disk cartridges). The analyst uses a data 
tablet (Summagraphics) to move a cursor in the graphics overlay for entering positions and controlling 
the system. As the film projector runs, the animated display changes in frame-by-frame synchrony with 
the film image. For example, as a moving point is tracked by the analyst, the graphics overlay superimposes 
the corresponding moving data point being recorded in each frame. Additionally, the entire collection 
of previously entered points are  Volume 16, Number 3 July 1982 continually displayed and updated as 
the film is advanced, helping the analyst enter more points without inadvertant omission or duplication. 
The graphics overlay permits other useful displays such as connectivity of points, allowing the analyst 
to use the structure of the object of interest as an aid in locating, identifying, and tracking points. 
The graphics overlay provides a major contribution toward resolving the correspondence problem by superimposing 
auxiliary lines onto the source image. After a point is entered in one view, the location of the auxiliary 
line is calculated as described above, and is used by the analyst in locating the position of the corresponding 
projected point in the other view (see Fig. id). Frequently, the auxiliary line passes through an obvious 
point and the correspondence is known immediately. Sometimes there may be more than one candidate point 
on the auxiliary line, but the correct one can usually be resolved easily by gross structural considerations, 
or by running the film and observing what point stays on the auxiliary line. If a third view is available, 
it can be used to resolve such ambiguities, since the two auxiliary lines will intersect at or near the 
correct point. Of course, the auxiliary line is of further help to the analyst in indicating whether 
an accurate 3D position is being obtained. User interface In addition to serving as an aid to the data 
entry, the graphics overlay provides the medium by which the analyst interacts with and controls the 
execution of the system. The graphics display is divided into two areas, a large data area and a small 
strip along the top edge which contains the command menu (Fig. 1b-d). The bottom part of the display 
contains continually updating information regarding the system status, including the location of the 
digitizing cursor, the number of points already entered, the frame number of the film projector, the 
current view, the current data instance, and other items of interest to the analyst. This status information 
is drawn in a part of the data area so this region is still open for data entry. The analyst initiates 
actions and controls the execution of the system through the command menu. The menu contains 35 buttons 
on 6 rows, but to reduce the amount to space taken from the data area, only 1 row is visible at a time. 
The other rows are accessed by hitting the leftmost button in each row (marked ROLL). The placement of 
buttons on rows is organized so that all buttons on a row have a similar function or are used during 
the same phase of data entry. During the view calibration phase, row 1 (visible in Fig. ib) is used. 
The buttons on the row allow the analyst to select the current calibration point (INC CP#, DEC CP#), 
to delete mistaken entries (DEL CP#, DEL PT), and to use the entered 2D points to infer a view transformation 
(INFER). When the analyst hits the button marked INFER, information about the 2D points just entered 
and the transformation matrix just calculated is displayed. The system will also reproject all the 3D 
calibration points according to the inferred transformation matrix and superimpose them onto the film 
image using the graphics overlay. This aids the analyst in assessing the error associated with individual 
point entries and allows detection of the most common types of mistakes made in the calibration process, 
which are misidentification of 3D points and poor entry of a specific point. If these mistakes are committed, 
then specific points will show large individual errors, which the analyst corrects by reentering the 
offending point. The reprojection of all the 3D calibration points after a subset  of them has been 
entered is often of great value in helping the analyst locate and properly identify the remaining, possibly 
more difficult to find, calibration points. Row 2 allows the analyst to control the transition between 
view calibration and unknown point entry. Command buttons in this row enable the analyst to turn on and 
off the graphics display (BLANK/SHOW), to save the current transformation matrix (NEXT VIEW), and to 
terminate calibration entry (CAL DONE). The command buttons in rows 3-6 are used during the actual entry 
of 3D points. The data entry is organized by instances [ ii]. A 2D data instance consists of one u,v 
coordinate pair in each frame of a particular view. Thus, the analyst uses one 2D instance to track one 
projected point as it moves in one view and uses the instance number to distinguish that moving point 
from all others. Two or more corresponding 2D instances are then used along with the view calibration 
information to reconstruct a 3D moving points instance [ 13]. The system supports two tools for entering 
instances. The first is a marking tool: data is entered on cursor depressions in the data area, and is 
used for entering static points. The second tool is a tracking tool, used for following moving objects. 
This tool is first turned on (using a command button) and the point of interest is followed with the 
cursor as the film moves. A u,v coordinate pair will be entered on every film projector frame change 
until the tool is turned off (using another command button). The graphics display changes from blinking 
to steady and the cursor changes its appearance when the tool is turned on to inform the analyst of the 
change. Row 3 of the menu (visible in Fig. ic) deals mainly with the active instance, the 2D instance 
currently being entered. By hitting buttons in this row, the analyst is able to: turn the active instance 
on or off (INST ON/OFF); write the active instance to a file and prepare the system for entering a new 
instance or re-entering an old instance (NEXT INST, PREV INST); delete all data previously entered for 
the active instance (DEL INST); and terminate system execution (EXIT). For entry of time dependent data, 
a temporal calibration must be performed in addition to the spatial calibration discussed above. If the 
views are obtained on the same piece of movie film or videotape (e.g., by using a mirror), then the temporal 
calibration is trivial. But, if the views are recorded on different pieces of film (e.g., by different 
cameras), then temporal calibration consists of determining when the same real time point occurs in each 
view so that a time base can be assigned to each film clip. The system monitors projector frame changes 
to determine the time to associate with each data entry, so control of the temporal aspect of data entry 
amounts to control of the projector frame number. Row 4 of the menu gives the analyst this control by 
allowing him to: set the current frame number to zero, or to one of the time bases (FR#=0), FR#=BI, FR#=B2); 
and, increment or decrement the frame number (INC FR#, DEC FR#), which is useful for changing the frame 
number without moving the film in the projector. Row 5 of the command buttons deals mainly with the auxiliary 
instances, which are previously entered 2D instances from other views. Data from these are used to calculate 
the position of auxiliary lines which will appear across the screen. In addition to the auxiliary lines, 
a continuously updating single line (visible below the command buttons in Fig. ld) shows the 3D coordinates 
(as determined from the position of one of the auxiliary points and the current cursor position) and 
an error measure (the distance from the cursor position to one of the auxiliary lines). If an auxiliary 
line is off the screen, or if there is no auxiliary data for a certain frame, warning messages to that 
affect will appear on the screen in place of this single line display. By hitting buttons in row 5, the 
analyst is able to: activate the auxiliary line features (AUX ON); change which auxiliary instances are 
being used (INC AUX, DEC AUX); and, produce a set of 3D points (MAKE 3D). Row 6 of the command buttons 
allows the analyst to select the view being entered. In the default configuration, the view number is 
determined by the 2D instance number (for example, if two views are being used, all odd 2D instances 
are assumed to be from view 1, and all even instances from view 2). The analyst can override the default 
using the command buttons on row 6 and: establish a relationship between frame number and view number 
(FR# = VIEW) (e.g., all data in a certain range of frames is assumed to be from a particular view); or 
independently set the view number (INC VIEW, DEC VIEW, SET VIEW); or restore the default relationship 
between instance numbers and view numbers (INST# = VIEW). Aeeuraey of entered data The accuracy with 
which 3D points can be entered using this system was checked with the following experiment. X-ray images 
of an 8 cm calibration cube with 15 pieces of lead shot (diameter 0.125 cm), were taken from two approximately 
orthogonal views. The two views of this cube were calibrated using some of the visible points as known 
points and two transformation matrices were constructed. The rest of the 15 points were then entered 
as unknown points and 3D coordinates were calculated. The average ~istance between the points as measured 
directly on the cube and as entered using this system was less than 0.03 cm, smaller than the size of 
the shot. Although this experiment indicates that the accuracy of this data entry system can be very 
high, several sources of error have been identified in this method, and steps have been taken to reduce 
their impact. One principal problem arises from geometric distortions introduced by the projection crt 
in the film analysis system, and by certain types of imaging equipment (e.g. video image intensifiers). 
These devices introduce pincushion distortions into the image, causing straight lines to appear curved 
inward. This distortion can be corrected by placing a grid on the large front screen or in the imaging 
system. The relation between the measured distorted coordinates and the known undistorted coordinates 
is estimated by a 3rd order polynomial, where the coefficients of the polynomial are chosen to minimize 
the squared distance between the polynomial estimate of the undistorted position and the actual position 
[16]. This polynomial is then used to compute the undistorted positions of 2D points from the distorted 
coordinates entered using the system. At its maximum, in the corners of the projection crt, the distortion 
is less than 7% of the size of the image. Another potential source of error involves registration of 
the film image from frame to frame. Slight shifts in frame registration could be magnified into large 
errors in the space of the unknown object. Some of our preliminary work was performed using a 16mm film 
projector without pin registration, and such registration shifts were noticed. We solved this problem 
by acquiring a pin-registered 35mm film projector (Vanguard Film Analyzer), which has great spatial resolution 
and registration accuracy. Another solution supported by the data entry system is to enter fiducial marks 
and correct for the frame-by-frame 2D misregistration. Applieations Because our 3D data entry system 
permits the use of arbitrary perspective projections, a number of different imaging systems can be utilized 
which generate the multiple views in a variety of ways, depending on the requirements of the application. 
For example, it is not necessary to use two camera systems to record two views simultaneously. By using 
a mirror (Figure 2a), two views of the object can be recorded by one camera, eliminating the need to 
synchronize multiple movie cameras or to provide timing events in the film. A similar technique to the 
use of a mirror is to use the shadow of the object as the second view, where the sun or a light source 
provides a different center of projection [2,9]. If the user supplies a connectivity between the points 
in the object, the system can produce an animated stick figure, which is a useful characterization of 
human or animal figures for locomotion studies. Figure 2b shows the stick figure as a stereo pair from 
an overhead view, which is different from either original view. The data entry system also supports the 
use of more than two views. Two views of any point are all that are required to reconstruct 3D coordinates, 
but often some object points are obscured in one or more of the views, and multiple views may be necessary 
to visualize all the points. If multiple views are used, easier verification of entries is possible, 
since the data entry system will produce more than one auxiliary line. If the entries of the previous 
views are correct, the auxiliary lines will intersect at the point of interest. If the auxiliary lines 
do not agree, the different 2D positions can be reentered, and their auxiliary lines examined in the 
other views, with an overall consensus reached through successive refinement. The use of multiple views 
is illustrated in Figure 3. These four images were produced by moving a single camera around this static 
object. Figure 3a shows the entry of the first view, 3b-d the second, third, and fourth views. Each new 
view has an additional auxiliary line to aid entry. The analyst can verify that previous views were entered 
correctly because the auxiliary lines intersect at the point of interest (clearly seen in Fig. 3c). Once 
2D entries in the three views have been made, each view can be examined to confirm that the 2D point 
entered on the point of interest in the film falls on the intersection of the two auxiliary lines. The 
object used for calibrating the views need not be specially constructed. If the 3D coordinates of some 
points on the object of interest are known, they may be used as calibration points, and the calibrated 
views then may be used to measure the 3"19 locations of other points on the object. In Figure 4, the 
object of interest is the city of Chicago, specifically the 3D coordinates of the tops of the city's 
major buildings. The 3D coordinates of 8 downtown buildings were determined using a street map (for the 
x-y position) and an almanac (for the building height, or z coordinate). The tops of these buildings 
as seen in photographs taken from five locations around Chicago were entered as calibration points, and 
the calibrated views were used to find the 3D coordinates of the tops of other buildings. Figs. 4a-e 
are five views of the city showing 4 auxiliary lines intersecting at the op of Sears Tower, Chicago's 
tallest building. For more than two years, the primary application of this data entry system has been 
the analysis of 3D motion of human and animal hearts. In this case the imaging system is a pair of X-ray 
source/image intensifier/movie camera trains which produce two synchronized, approximately orthogonal 
views. In human hearts, branch points of the coronary arteries (Fig. lb,c) have been the principal objects 
tracked, typically one to three dozen points for one hundred to three hundred film frames [ 1,15]. The 
animal studies have been performed in dogs, using surgically implanted endocardial and epicardial metallic 
markers. We have been able to study the speeds and directions in which different regions of the heart 
wall move (Fig. le), and show that these motions are affected in diseased hearts and by drug interventions. 
 Summary We have described an interactive computer graphics system used for entering the 3D positions 
of moving points from arbitrary multiple perspective views. This system has several features which allow 
large amounts of 3D data to be gathered quickly and accurately. The graphical overlay, especially the 
auxiliary line, have proven essential for the recognition of corresponding points in the different views 
which is the most difficult task for the human analyst. By supporting 2D data instances, 3D data instances, 
and perspective view descriptions as basic system primitives, 3D Galatea provides a rich graphics environment 
for relating 2D interactions and 3D data input. Aeknowledgments This work has been supported by grants 
from the Goldblatt Cancer Research Foundation, NIH grants CA- 14599 and HD-07136, and by the Galatea 
Computer Graphics Facility of the University of Chicago. The assistance and comments of Dr. Jonathan 
M. Rubin and Dr. Alex M. Aisen, Department of Radiology, University of Chicago, are appreciated.  Referenees 
 1. Aisen, A. M., A1-Sadir, J., MacKay, S. A., Potel, M. J., Rubin, J. M., and Sayre, R. E. Quantitative 
ventricular wall motion analysis from biplane coronary angiograms. Proc. IEEE Computers in Cardioloffy, 
(1980), 443-446. 2. Cullen, J. M., Shaw, E., and Baldwin, H. A. Methods for measuring the three-dimensional 
structure of fish schools. Animal Behaviour 13, (1965), 534-543. 3. Futrelle, R. P. and Potel, M. J. 
The system design for Galatea, an interactive real-time graphics system for movie and video analysis. 
Computers and Graphics 1, (1975), 115-121. 4. Graybill, F. A. Introducton to Matrices with Applications 
in Statistics. Belmont, CA.: Wadsworth Publishing Co. (1969). 5. Longuet-Higgins, H. C. A computer algorithm 
for reconstructing a scene from two projections. Nature (London) 293, (1981), 133-135. 6. MacKay, S. 
A. A system for graphics assisted entry of 3D points. Proc. IEEE 4th Int'l. Computer Software and Applications 
Conf~ (COMPSAC'80), (1980), 128-134.  7. Merchant, J. Exact area registration of different views of 
a common object scene. Optical Engineering 20, (1981), 424-436. 8. Newman, W. M. and Sproull, R. F. 
Principles of Interactive Computer Graphics, (2nd edition). New York: McGraw-Hill, (1979).  9. Partridge, 
B. L. and Cullen, J. M. A low cost interactive coordinate plotter. Behavior Research Methods and Instrumentation 
9, (1977), 473-479. 10. Potel, M. J. and Sayre, R. E. Interacting with the Galatea film analysis system. 
ACM Computer Graphics i__00 , (1976), 52-59. 11. Potel, M. J. and Sayre, R. E. Data environment for 
a laboratory film analysis system. Proc. IEEE 1st Int'l. Computer Software and Applications Conf. (COMPSAC 
'77 t) (1977), 800-806. 12. Potel, M. J., MacKay, S. A., and Sayre, R. E. Galatea, an interactive computer 
graphics system for movie and video analysis. Proc. 15th Int'l. Cong. on High Speed Photography and Photonics, 
in press, (1982). 13. Potel, M. J., Sayre, R. E., and MacKay, S. A. Graphics tools for interactive motion 
analysis. Comput. Graphics Imag. Proc. 1_44, (1980), 1-23. 14. Potel, M. J., Sayre, R. E., and Robertson, 
A. A system for interactive film analysis. Comput. Biol. Med. _9, (1979), 237-256. 15. Potel, M. J., 
Rubin, J. M., MacKay, S. A., Aisen, A. M., Al-Sadir, J., Sayre, R.E. Evaluation of heart wall motion 
in 3D using biplane coronary angiograms. Investigative Radiology 1__66, (1981), 5423. 16. Pratt, W. 
K. Digital Image Processing. New York: Wiley, (1978). 17. Roberts, L. G. Homogeneous matrix representation 
and manipulation of n-dimensional constructs. Document MS 1405, Lincoln Laboratory, MIT, Cambridge, MA., 
(1965). 18. Rogers, D. F. and Adams, J. A. Mathematical Elements for Computer Graphics. New York: McGraw-Hill, 
(1976). 19. Rubin, J. M. and Sayre, R. E. A computer-aided technique for overlaying cerebral angiograms 
onto computed tomograms. Investigative Radiology 13, (1978), 362-367. 20. Sayre, R. E., Rubin, J. M., 
Duda, E. E., and Patronas, N. J. Quantitative three-dimensional angiograms: application, including augmentation 
of computed tomograms. Proc. IEEE Conf. on Computer Applications in Radiology, (1979), 95-102. 21. Slama, 
C. C., Theurer, C., Henrikson, S. W., (eds). Manual of Photogrammetry. American Society of Photogrammetry, 
Falls Church, VA., (1980). 22. Sutherland, I. E. Three-dimensional data input by tablet. Proc. IEEE 
62, (1974), 454-461. 23. Unman, S. The Interpretation of Visual Motion. MIT Press: Cambridge and London, 
(1979). 24. Underwood, E. E. Quantitative Stereology. Reading, MA.: Addison-Wesley, (1970).  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801284</article_id>
		<sort_key>223</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>33</seq_no>
		<title><![CDATA[A morphological study of the form of nature]]></title>
		<page_from>223</page_from>
		<page_to>232</page_to>
		<doi_number>10.1145/800064.801284</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801284</url>
		<abstract>
			<par><![CDATA[<p>A process of recreating some forms of nature, including shells, horns, tusks, claws, and spiral plants, is herein described. The forms of nature based on spirals and ramification are generated not through the use of object data calculated by measurement, but through the use of an algorithmic structure based on the laws of nature. Although there are a myriad of forms to the shapes of nature, they are represented on the basis of one common principle, which can be expressed by means of the same mathematical expressions. The graphic software which is called &#8220;GROWTH&#8221; (Growth Rationale Object Work THeorem) has now been designed to create these forms automatically. GROWTH incorporates the rational, common principle of geometrical series which are found in the structute of biological objects.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Biological law]]></kw>
			<kw><![CDATA[Computer animation]]></kw>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Growth]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Similarity transformation]]></kw>
			<kw><![CDATA[Spiral Structure]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>J.3</cat_node>
				<descriptor>Biology and genetics</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.4.7</cat_node>
				<descriptor>Size and shape</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010242</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Shape representations</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010935</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Genetics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010095</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Systems biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010444.10010087</concept_id>
				<concept_desc>CCS->Applied computing->Life and medical sciences->Computational biology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39050442</person_id>
				<author_profile_id><![CDATA[81100605028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawaguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Art, Nippon Electronics College, Hyakunin-cho, Shinjuku-ku, Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Eiichi Izuhara, "A Unified Model for Generative Process of Plants," Bulletin of the Japanese Society for the Science of Design, September, 1978]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yoichiro Kawaguchi, "Spiral Structure in Image Composition," Bulletin of the Japanese Society for the Science of Design, September, 1978]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[D'Arcy W. Thompson, On Growth and Form I, II Cambridge University Press, 1968]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A MORPHOLOGICAL STUDY OF THE FORM OF NATURE Yoichiro Kawaguchi Department of Art Nippon Electronics 
College Hyakunin-cho, Shinjuku-ku, Tokyo 160 ABSTRACT A process of recreating some forms of nature, 
including shells, horns, tusks, claws, and spiral plants, is herein described. The forms of nature based 
on spirals and ramification are generated not through the use of object data calculated by measurement, 
but through the use of an algorithmic structure based on the laws of nature. Although there are a myriad 
of forms to the shapes of nature, they are rep- resented on the basis of one common principle, which 
can be expressed by means of the same mathematical expres- sions. The graphic software which is called 
"GROWTH" (Growth Rationale Ob- ject Work THeorem) has now been design- ed to create these forms automatically. 
GROWTH incorporates the rational, com- mon principle of geometrical series which are found in the structute 
of biological objects. CR categories: Animation/Dynamics/ Art Key words and phrases: Biological law, 
GROWTH, spiral structure, similarity transformation, computer graphics, computer animation, Raster 
graphics Permission to copy without fee all or part of this material is granted provided that the copies 
are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of 
the publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
INTRODUCTION We often find various forms which can be regarded as the form of design in the natural 
environment. A form which exhibits such regularity and order at- tracts our interest. The laws under 
which these forms operate are rules. These laws are rules which are physical factors regulating the process 
of form- ation. If these laws are assumed to be the principles whereby forms are created, we can anticipate 
that things having a similar form are the result of a common principle in operation. This program can 
create spiral fig- ures which are composed on a set of sim- ilar, but smaller, units, by entering the 
direction of growth on 3-D coordi- nate axes and accumulating these smaller figures in proportion to 
the growth rate, which is expanding in a series. In the exercise designed to simulate the growth of spiral 
plants, a more cen- tripetal structure is generated by in- creasing the number, the angles, and the length 
of the branches in a series at any branching point. The use of GROWTH to generate natural veins and tendrils 
stemming from various plants can be de- scribed as follows. Shaded images ren- dered by this process 
are included, as are scientific illustrations and animat- ed pictures explaining the morphological organization 
of living things. In 1977, Eichi Izuhara advised me that the growth of shells occurs based upon simple 
and rational principles. In this paper, we will explain the principle of the spiral form mathematically 
and discuss the plants, snails, and horns which are herein used as models. The form of nature will be 
proven not by measuring three-dimensional data of models but by explaining various rules of formation 
mathematically and by re-creating the models based on that explanation. &#38;#169; 1982 ACM 0-89791-076-1/82/007/0223 
$00.75 Shells' Growth Model By observing the forms of fossils and various shells, one can see that shells 
share a common morphological structure and plan, or mechanism. Be- cause they are governed by simple 
math- ematical formula, the explanation of the formation technique by computer graphics became possible. 
 In this paper, emphasis is placed upon shells' morphology starting from the principle of the accumulation 
of shells rather than setting the living object's internal tissue's chemical parameter. Also, the research 
was done without considering genetic factors. There are various formation factors which contribute to 
the growth of shells; however, the purpose of this paper is to focus on the principle of morphological 
formation, which, in natural science, is considered to have a great influence, and to develop that morphological 
fOrm- ation algorithm in computer graphics. Therefore, the research does not end only in a biological 
morphological study -it extends to the application of the research to the exploitation of new techniques 
in the formative arts. The following is a description of shells' growth simulation. Thus, as a model 
for biological growth principle (formula), its application to computer graphics is explained. Tz T_~x 
< Y Figure 1 -Mouth of Shell STRUCTURE ANALYSIS Basically, shells have spiral struc- tures supported 
by numerous small chamb- ers which are arranged regularly. Overall shell structure consists of small 
chambers. Each chamber is similar New chambers are accumulated at the posi- tion of previously accumulated 
old small chambers Accumulation of the small chambers occurs locally. In other words, newly- formed 
small chambers are similar to those just previously formed. Therefore, small chambers are formed in an 
orderly and similar manner, and accumulation be- gins by geometic ratio to form a logari- thmic spiral. 
 In order to determine these shells' overall morphological structure, only the small chambers' morphological 
structure has to be determined. Small chambers are polyhedrons which have similar top and bottom bases. 
 -OVERVIEW - Do' TOP Do -- BOTTOM " i -Qy x Do y SIDE VIEW FRONT VIEW ) < ) Tx Tz Figure 2 - Formation 
of Polyhedron  Algorithm of Three-Dimensional Small Chambers I. Bottom base  Both top and bottom bases 
are polygons, which consist of N number apexes. Formation of Polygons i. Ellipse  The user has the 
option to input in- formation by tablet and/or keyboard to vary the radius of the ellipse (see figure 
i) 2. Non-elliptical configuration  The mouths of the shells which exist in nature have not only the 
shapes of el- lipses but also have unique morphological structures. Through the use of the above inter- 
actively, the concave/convex protuber- ance of the ellipse described above can be modified to construct 
the optimum shape of the shell'S mouth. II. Side View  Construct the mouth form which was constructed 
in a two-dimensional figure into 9 three-dimensional figure. At this point, the determined three-dimensional 
objects influence the overall shape of the shells. This, as I have described above, is composed of polyhedrons, 
which consist of two similar flat surfaces - bottom and top bases. Fi'gure 3 (above) Accumulated structure 
shell Figure 4 (right, top) Horn; views three Figure 5 (right, bottom) Horn, spective view per- Relationship 
Between Two Flat Surfaces I) Identical Ratio Make an identical figure of the en- larged or reduced 
scale of the bottom base, as was done at the top base. Supposing the long diameter of the ellipse to 
be D__oo, and the scaled form of Do (either enlarged or reduced) to be Do'. The ratio can thus be described 
as follows: Do' r - Do This ratio (r) is the growth ratio of accumulation which takes place in each 
small chamber in the process of shell growth. 2) The Width The width is the distance between the top 
and bottom bases. Supposing the thickness to be the value of H,as the figure of H is bigger, the shells 
with a rapid growth rate and a long and slen- der shape are constructed. The age of the shells is indicated 
by the number of small chambers. The relationship between the width and the position is a matter of the 
same level. That is, the width, H, is the distance between the top and bottom bases, and the position 
is nothing but the transferring of axis x and [, which reaches the top and bottom bases. Again, the top 
base is identical to the figure of the bottom base, in identical scale to it.  3) Inclination (Slope) 
 Inclination is indicated by the angle of the top and bottom bases of the shell. On a three-dimensional 
angular axis (x,y,z) , it takes a direction either towards the right, left, hither or yon. Actual shells 
have both right-twist and left-twist; however, they are not limited to either right or left twists. 
To check the twist of shells in the process of data input, besides a three-dimensional picture consisting 
of a front-view picture, a side-view picture, and an overview, a picture done by perspective transformation 
is prepared (see figures 2, 3, 4). The reason for doing so is that there is a change in the direction 
of growth due to a change in the direction of the an- gle between the top and bottom bases. If the angle, 
2, between the top and bottom bases of the shells were small, the shells twirl a gentle, sloped curve 
spiral. On the other hand, if is big, shells twirl rapidly. Q : (Qx,Qy,Qz)  KNOT i?_ .......... TRUNK 
/// BOTTO~I BASE ............. ~ < 4) Positional Relationship If one observes shells in nature, he 
will find that the direction of the position is not necessarily vertical. Suppose the transfer distance 
against each direction of length, width, and depth to be T x, T_~, and T z. The relationship between 
the bottom base and the top base is indicated by the condition in which the distance of the height direction, 
i.e., the width relationship, T z, is fixed and moved in a certain direction. When the positional relationship 
of both bottom and top bases shifts from a vertical position to anoth- er position, each small chamber 
does not adjoin, as in the case of some shells, and it starts to form in the shape of horns of sheep 
or goats (see figure 5) Helght ~\ > DIAMETER D O FIGURE 6 -Branching Structure  Rules of Generating 
Model  Simulation of the spiral growth of a plant is basically similar to the sim- ulation of the growth 
of a horn, fang, nail, etc. The fundamental formation of the simulation contains a theory of the growth 
and some new, additional parameter which are factors in growth. i) A trunk and branches deriving from 
it have diameters ( D ) and heights (H). 2) A top base and a bottom base of a trunk are parallel. 3) 
Two adjacent trunks are connected by a knot. 4) Between the top and bottom base of a knot is a certain 
angle (~) 5) The top and bottom base of a knot always contact each other at one point (K). 6) A branch 
always derives from a knot. 7) The diameter of a branch is always affected by the height of the knot 
from which it derives. 8) When the height of a branch or knot becomes smaller than one pixel, the branch 
stops growing.  i = the number i = 0, i, 2... of n-l, trunks n Ho Do - H1 D1 - Hn Dn S (constant (See 
figure 6) 3.2 Tendril Plants and Their Modification A tendril plant can be generated by the application 
of the principle, which rolls a branch of a plant into a spiral. A tendril of a plant does not grow in 
a certain direction like the shell of a pearly nautilus, but a force which turns the grawth to the other 
direction is gen- erated at a certain rolling angle. We called this phenomenon the "self-adj- ustment 
law which a plant holds within itself" and made a model of a tendril plant by setting a limit to the 
angle of rolling and by operating the force which has the same strength and the op- posite direction 
upon the growth of the plant. We could generate different types of tendril plant models according to 
the number of accumulation of bending. It is not necessary to input data like "twin- ing around" or 
"sunward" of, for in- stance, a morning glory, to this sim- ulation model. A plant itself generates new 
tendrils successively by self-multi- placation. If we shift not only the limitation of the angle for 
changing direction but also the value of the self-adjustment power, we will be able to get a plant which 
has a more complex curve. When we adjust the values at the number of bending to less than those of a 
tendril, a completed model more closely resembles a natural plant. Extension of Generating Model The 
following process is a morpho - genic model by which a plant successive- ly repeats self-adjustment processes 
so as to generate a complex formation. i) The basic structure shall follow the law of a growth of a 
tendril plant. 2) A rolling angle ~ shall gradually be increased or decreased at the rate of a certain 
value @ O((i) = <X(i) ± e 3) A tendril derived from a knot shall receive a force ~ which adjusts the 
rolling angle of a trunk CX  (X(i) = O((i) ± e 7 4) ~ shall have an effect upon C~ only when ~ is out 
of the provided limit- ations.  <d< kl 5) After has been adjusted, will again come out of the provided 
limit- ations  ko <d<kl when accumulation reaches a certain number. Then the self-adjustment force 
 will again operate upon to the opposite direction and try to re- cover a stable balance. 6) If we set 
the provided limitation extremely narrowly, we cannot see the growth of tendrils but will see the growth 
of crowded branches and leaves. Thus we found it possible to gen- erate a model which was very similar 
 to the existing plant by means of intro- ducing a self-adjustment factor and other physical factors 
in addition to the basic principles of a spiral formation. From the research reported this time, it 
is anticipated that, adapting the above result as a common theorem of the branching of plan~s, we will 
be able to construct various systematic models which express the evolution of plants from the past to 
the future, including p|ants existing at present. CONCLUSION Since this program possesses an al- gorithic 
structure based on the similarity growth that takes place in the natural world, it can create shells 
and horns of intended shape. Shapes thus created are formed by curved surfaces obtained not through 
com- plex measurements but by the principles of regular logarithmic spiral. By expanding these principles, 
we have succeeded in freely generating such complex shapes as corals and tendril plants, of which dimension 
is hard to measure by methods currently available. The process to generate these shapes which might 
consist of hundreds of thou- sands, or even millions of faces, is an application of biological and mathemati- 
cal principles by branching. 6. Acknowledgements I started this research in 1977, with the advice of 
Professor Eiichi Izuhara at the Industrial Product Research Institute, and the first step was a growth 
model of a shell by line drawing. At that time, Mr. Tomohiro Ohira, also at the Industrial Research Institute, 
gave me suggestions concerning the design of three-dimension- al curved surfaces. I would like to ex- 
press my sincere thanks to both of them. I would also like to express my apprecia- tion to Mr. Dirk 
Binder and Mr. Osamu Fukuda of Tokyo Foreign Language College for their cooperation in the translation 
of this report, and Mr. Takafumi Okubo of Nippon Electronics College for the oppor- tunity to continue 
my research in this field. 7. References [i] Eiichi Izuhara, "A Unified Model for Generative Process 
of Plants," Bulle- tin of the Japanese Society for the Science of Design, September, 1978 [2] Yoichiro 
Kawaguchi, "Spiral Structure in Image Composition," Bulletin of the Japanese Society for the Science 
of Design, September, 1978 [3] D'Arcy W. Thompson, On Growth and Form I, II Cambridge University Press, 
1968 The equipment used in this research are as follows: E &#38; S Picture System II and Melcom 700 
for line drawings AED 512 and Micro -11/23 for shaded images The language was FORTRAN A background 
picture and a picture of a shell or a group of spiral plants are made separately  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801285</article_id>
		<sort_key>233</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>34</seq_no>
		<title><![CDATA[An interactive graphics environment for architectural energy simulation]]></title>
		<page_from>233</page_from>
		<page_to>241</page_to>
		<doi_number>10.1145/800064.801285</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801285</url>
		<abstract>
			<par><![CDATA[<p>An interactive computer graphics system has been developed for the architecture profession which provides a &#8220;design environment&#8221; for the evaluation of building energy consumption. The system includes an integrated set of graphic input tools which generate the geometric and attribute data necessary for the determination of thermal load in buildings. In addition, a comprehensive set of graphical output routines has been created to allow the designer to visually interpret the results of alternative design strategies.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.0</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Application packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>J.6</cat_node>
				<descriptor>Computer-aided design (CAD)</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010432.10010439.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Physical sciences and engineering->Engineering->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010469.10010472.10010440</concept_id>
				<concept_desc>CCS->Applied computing->Arts and humanities->Architecture (buildings)->Computer-aided design</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Performance</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14160944</person_id>
				<author_profile_id><![CDATA[81332521468]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jon]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Pittman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P68459</person_id>
				<author_profile_id><![CDATA[81100196982]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Donald]]></first_name>
				<middle_name><![CDATA[P.]]></middle_name>
				<last_name><![CDATA[Greenberg]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Program of Computer Graphics, Cornell University, Ithaca, New York]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Andersson, Brandt. "An Architect's Energy Program Using Interactive Graphics". Computer Graphics, Vol. 2, No. 8. September 1979, Cygnus Publications, San Francisco. pp 38-42.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Automated Procedures for Engineering Consultants. "Energy Simulation Program II". 1981, Automated Procedures for Engineering Consultants, Dayton, Ohio.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[ASHRAE Task Group on Energy Requirements for Heating and Cooling of Buildings. Procedure for Determining Heating and Cooling Loads for Computerizing Energy Calculations. 1976, American Society of Heating, Refrigeration, and Air Conditioning Engineers, Inc., New York.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807403</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Atherton, Peter R., Weiler, Kevin and Greenberg, Donald, "Polygon Shadow Generation". Computer Graphics, Vol. 12, No. 3. August 1978. pp. 275-281.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Atwood, Charles L., "SDMS, A Large Scale Structural Engineering POL using BASIC plus". Spring 1978 DECUS Proceedings. 1978, Digital Equipment Corporation, Maynard, Mass.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Braden, Spruille. Graphic Standards of Solar Energy. 1977, CBI Publishing, Boston.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807422</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Brown, B.E., "Computer Graphics for Large Scale Two-and Three-Dimensional Analysis of Complex Geometries", Computer Graphics Vol. 12, No. 2. August 1979, pp. 33-40.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Christiansen, H.N. "The Emerging Role of Color Graphics", Finite Element Methods in the Commercial Environment. Edited by John Robinson, Papers presented at the Second World Congress on Finite Element Methods, Dorsett, England. Vol. 1. 1978, Robinson and Associates, Dorsett, England.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Cook, R. Polygonal Database User's Guide. Unpublished. 1981, Program of Computer Graphics, Cornell University, Ithaca, New York.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563901</ref_obj_id>
				<ref_obj_pid>965141</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Crow, Franklin C. "Shadow Algorithms for Computer Graphics". Computer Graphics Vol. 11. No. 2. August 1977, pp. 242-248.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Diamond, Stephen C. et. al. DOE-2 Programmer's Manual 1979, Los Alamos Scientific Laboratory, Los Alamos, New Mexico.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806801</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Dill, John C. "An Application of Color Graphics to the Display of Surface Curvature". Computer Graphics. Vol. 15. No. 3. August 1981, pp. 153-161.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Feibush, Eliot Alan and Greenberg, Donald P. "A Geometric Input and Editing System for Architectural Design". 1981, CAD'82, Spring 1982, Brighton, England. pp. 164-171.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Haber, Robert B. "Computer-aided Design of Cable Reinforced Membrane Structures", 1981, Cornell University, Ithaca, New York.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Hittle, Douglas C. The Building Loads Analysis System Thermodynamics (BLAST) Program, Version 2.0 Users Manual. Army report CERL-TR-E-153. 1979, U.S. Army Construction Engineering Research Laboratory, Champaign, Illinois.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Horak, Henery L. et. al. DOE-2 Reference Manual. 1979, Los Alamos Scientific Laboratory, Los Alamos, New Mexico.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Lopez, L.A., POLO II Programmer's Guide. 1973, Civil Engineering Systems Laboratory, University of Illinois at Urbana-Champaign, Urbana, Illinois.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Milne, Murray and Yoshikawa. S. "SOLAR-5, An Interactive Computer-Aided Passive Solar Building Design System". Computer Graphics, Vol 2. No. 8. September 1979. Cygnus Publications, San Francisco. pp. 44-46.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Olgyay, Victor. Design With Climate a Bioclimatic Approch to Architectural Regionalism. 1963. Princeton University Press. Princeton, New Jersey.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Ramsey, Charles G. and Sleeper, Harold R., Architectural Graphic Standards. 1970, John Wiley and Sons, New York.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Rogers, Richard J. "An Interactive Graphics Input System for Building Energy Analysis". 1976, Cornell University, Ithaca, New York.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Roos, D. ICES System: General Description. MIT Department of Civil Engineering report R67-49. 1967, Cambridge, Mass.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Schulman, Michael A. "The Interactive Display of Parameters on Two- and Three-dimensional Surfaces". 1981, Cornell University, Ithaca, New York.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Robertz, Wayne. and Greenberg, Donald P. "A Graphical Input System for Computer-Aided Architectural Design". CAD'80 Brighton, England, Spring 1980. pp. 715-723.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807443</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Yessios, Chris I., "Computer Drafting of Stones, Wood, Plant,and Ground Materials". Computer Graphics Vol. 13. No. 2., August 1979, pp. 190-198.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AM INTERACTIVE GRAPHICS ENVIRO~MEF~ FOR ARCHITECTURAL ENERGY SIMULATION Jon H. Pittman and Donald P. 
Greenberg Program of Computer Graphics Cornell University Ithaca, New York 14853 CR Categories and Subject 
Descriptors: 1.3.0 [Computer Graphics]: General; 1.3.4 [Computer Graphics]: Graphics utilities - application 
packages; J.6 [Computer Applications]: Computer-aided Design (CAD) General Terms: Architecture, Computer-aided 
 design, Computer Graphics, Energy Analysis ABSTRACT An interactive computer graphics system has been 
developed for the architecture profession which provides a "design environment" for the evaluation of 
building energy consumption. The system includes an integrated set of graphic input tools which generate 
the geometric and attribute data necessary for the determination of thermal load in buildings. In addition, 
a comprehensive set of graphical output routines has been created to allow the designer to visually interpret 
the results of alternative design strategies.  INTRODUCTION It is necessary that the architectural 
profession seek new ways to make buildings efficient in their use of energy. The evaluation of a building's 
energy performance requires the determination of heat loss / heat gain for each component of the structure 
and is dependent on the thermal envelope, the orientation, the external climate, and the internal operation 
conditions. The tools currently available for analysis of building energy performance do not lend themselves 
to use in the architectural design process. Furthermore, although architects work and think in visual 
and spatial terms, the current energy analysis programs are all numerically oriented. Computer graphics 
can be used to help bridge the gap between the visual, synthesis-oriented world of the architect and 
the necessity for numerically-oriented analysis. Permission to copy without fee all or part of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. @ 1982 ACM 0-89791-076-1/82/007/0233 $00.75 The ENERGY design 
environment is designed to be a highly interactive, visual means for architects to easily create and 
manipulate large complex building descriptions. It consists of graphic input tools which allow the architect 
to create a volumetric description of the building geometry; create libraries of material, wall section, 
 operating schedule, and window data; and assign specific elements of these libraries to geometric entities 
in the building description. This information is stored in a database which relates geometric data, attribute 
data, and thermal performance data for a particular design problem. The design environment also provides 
output tools which the architect may use to graphically study the thermal performance of a building and 
its components. These include two and three-dimensional graphs of thermal performance, color-coded representations 
of heat loss and gain, and tools to allow the architect to study the relationship of solar orientation 
to shading patterns using shadow algorithms. These tools are structured to permit the architect to interactively 
view various components building's energy pe the and rforman relationships their effect ce. between on 
the LITERATURE SURVK~ Several thermal load determination programs have been in use for a number of years. 
These programs calculate the heat loss and heat gain for a building based on climate and operating conditions. 
The most advanced of these programs are ESP-2 [2], DOE-2 [11,16], and BLAST [15]. All currently depend 
on numerical input data making their usage cumbersome, expensive, and error-prone. Graphic preprocessors 
would substantially improve the use of such programs by eliminating these problems. The development 
of graphic energy analysis pre- and post-processors is analagous to the development of graphic finite 
element pre-and post-processors in structural engineering. The algorithms to perform finite element analyses 
were well-established, but the difficulties in data input and interpretation led to the development of 
interactive graphics programs to improve human-machine communication [7,8,14,23]. These programs used 
a variety of graphics techniques to prepare input data and display the output of finite element analysis 
programs. 233 There have been several programs which address the problem of preprocessing thermal load 
data using interactive graphics. These include [1,18,21]. There is no precedent, however, for a complete 
design environment which includes both pre-and post-processing as well as a set of design tools for building 
energy load analysis. The concept of a management program which controls and coordinates the activities 
of a number of applications has been previously explored in [5,17,22]. However, graphics versions of 
such programs have not been published. For post processing, as well as establishing a symbiotic design 
environment between the designer and the computer, it is necessary to rely on sophisticated raster graphics 
algorithms for color displays. Shadowing algorithms such as those used in the ENERGY design environment 
are described in [4,10]. Methods for the color display of parametric data on two and three dimensional 
surfaces are described in [12,23]. OVERVIEM The ENERGY design environment acts as an interface between 
the architect and energy analysis tools. It allows the architect to deal with the energy implications 
of a design problem using a familiar graphical vocabulary. It manages information on building geometry, 
thermal performance, orientation, weather data, and scheduling data; all of which is necessary to perform 
a building energy analysis. Included in the ENERGY design environment are aids to design decision making, 
a preprocessor for energy analysis data, and a postprocessor and display system for the results of an 
energy analysis. Visual cues are used whenever possible to facilitate interaction with the computer. 
Attempts have been made to adapt to the architect's mode of working rather than forcing the architect 
to conform to the computer (or programmer's) mode of working. MAIN Structure of the system. The ENERGY 
design environment consists of four components : the ENERGY program, the project database, the generic 
database, and one or more analysis programs (Fig. I). Each of these components is described below. The 
primary component is the ENERGY program which forms the interface between the architect and the computer 
and coordinates the activities of the other components. The architect communicates with the ENERGY program 
via menus using graphic input techniques. The menu pages are organized into a hierarchical tree structure 
containing five major subsystems: design tools which provide design decision-making tools for the architect, 
a library manager to deal with the generic database, an attribute editor to assign project attributes 
in the project database, a preprocessor to prepare data for analysis and control execution of the analysis, 
and a postprocessor to interpret the results of the analysis (Fig. 2). In addition, a view page (Fig. 
3) is provided to allow the architect to display and manipulate images of the building geometry. Figure 
I. Components of the ENERGY design environment. /f DESIGN LIBRARY ATTRIBUTI ANALYSIS POST SUI~PAIN MATERIAL 
~OAR~/~ PBOOEC, ..... B°LE ........ O~XTI;~I,~N, SECTION CALE.OA ORA....... SECTION (2) SCHEDULE (2) 
WINDOW (2) Figum-e 2. The menu tree of the ENERGY design environment. 234    CONCLUSION REFERENCES 
 The ENERGY design environment described above provides a design tool which closely matches the needs 
of the architectural design process. Because of the graphic communication, the architect can work with 
the computer using a familiar graphic vocabulary. It thus allows architects to use the computer as a 
design tool without having to adapt their methods to the computer. Furthermore, the immediate feedback 
allows its use in the preliminary design phases since it can easily be used in an iterative fashion. 
The architect may evaluate a design solution and generate a new design solution based on that evaluation. 
Finally, it is evolutionary. The base of design information about a given project becomes more and more 
complete as the design progresses. There is clearly a close fit between the features included in the 
ENERGY design environment and those needed by architectural designers who are concerned with energy issues. 
 Interactive computer graphic energy programs would benefit the architectural profession in a number 
of ways. They would extend the capability of the architect by enabling parametric studies of thermal 
performance, relating numeric output to graphic representation of data, and allowing ease of data input 
for energy analysis. Designs would be evaluated on a rational basis, thereby allowing the architect to 
become more accountable for design decisions. Time for data input would be reduced and interpretation 
of analyses would be enhanced, thus making energy analysis more cost effective. Finally, it would force 
the improvement of energy analysis routines due to the availability of precise geometric input data. 
 In a more general sense, the ENERGY design environment is a prototype for the use of computer graphics 
in the architectural design process. It is focused on energy analysis but addresses concepts and issues 
that are relevant to all architectural problems. It attempts to provide a partnership between architect 
and computer which will extend the architect's capability to deal effectively with increasingly complex 
problems.  ACKNOWLEDGEMENTS We would like to thank Tom Mazzotta who programmed the calendar and schedule 
editors and Dan Ambrosi who modeled the geometry of the building used as an example. We would also like 
to thank Ted Crane, Wayne Robertz, and Marc Schiler for their comments and technical assistance and 
Emil Ghinger for his photography. In addition, we would like to thank Katherine Elliott for her editorial 
assistance. [I] Andersson, Brandt. "An Architect's Energy Program Using Interactive Graphics". Comnuter 
Graphics, Vol. 2, No. 8. September 1979, Cygnus Publications, San Francisco. pp 38-42. [2] Automated 
Procedures for Engineering Consultants. Ener~v2~ Program I I". 1981, Automated Procedures for Engineering 
Consultants, Dayton, Ohio. ASHRAE Task Group on Energy Requirements for Heating and Cooling of Buildings. 
procedure for D _ ~ Heatin~ and Coolin~ Loads f o r ~ Energy Calculations. 1976, American Society of 
Heating, Refrigeration, and Air Conditioning Engineers, Inc., New York. [3] [4] Atherton, Peter R., 
Weiler, Kevin and Greenberg, Donald, "Polygon Shadow Generation". Computer GraPhics, Vol. 12, No. 3. 
August 1978. pp. 275-281. [5] Atwood, Charles L., "SDMS, A Large Scale Structural Engineering POL using 
BASIC plus". Sorin~ 1978 DECUS Proceedings. 1978, Digital Equipment Corporation, Maynard, Mass. [6] 
Braden, Spruille. Graphic Standards of Solar Energy. 1977, CBI Publishing, Boston. [7] Brown, B.E., 
"Computer Graphics for Large Scale Two-and Three-Dimensional Analysis of Complex Geometries", ~omouter 
Graphics Vol. 12, No. 2. August 1979, Pp. 33-40. [8] Christiansen, H.N. "The Emerging Role of Color 
Graphics", Finite ~ Methods in the C ~ 2 g ~ . Edited by John Robinson, Papers presented at the Second 
 World Congress on Finite Element Methods, Dorsett, England. Vol. I. 1978, Robinson and Associates, Dorsett, 
England. Cook, R. YQlvgonal Database User's Guide. Unpublished. 1981, Program of Computer Graphics, 
Cornell University, Ithaca, New York. [9] [10] Crow, Franklin C. "Shadow Algorithms for Computer Graphics". 
Computer Graphics Vol. 11. No. 2. August 1977, Pp. 242-248. [11] Diamond, Stephen C. et. al. DOE-2 
2 2 ~ Manual 1979, Los Alamos Scientific Laboratory, Los Alamos, New Mexico. [12] Dill, John C. "An 
Application of Color Graphics to the Display of Surface Curvature". Computer Graohics. Vol. 15. No. 3. 
August 1981, pp. 153-161. [13] Feibush. Eliot Alan and Greenberg, Donald P. "A Geometric Input and Editing 
System for Architectural Design". 1981, CAD'82, Spring 1982, Brighton, England. pp. 164-171. 240 [14] 
 [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] Haber, Robert B. "Computer-aided Design 
of Cable Reinforced Membrane Structures", 1981. Cornell University, Ithaca, New York. Hittle. Douglas 
C. The Buildin~ Loads Analysis System~ (BLAST) 2_~ Version 2.0 Users Manual. Army report CERL-TR-E-153. 
1979, U.S. Army Construction Engineering Research Laboratory, Champaign, Illinois. Borak, Henery L. 
et. al. DOE-2 Reference Manual. 1979, Los Alamos Scientific Laboratory, Los Alamos, New Mexico. Lopez, 
L.A., POLO II2_y_Q~ Guide. 1973, Civil Engineering Systems Laboratory, University of Illinois at Urbana-Champaign, 
Urbana, Illinois. Milne, Murray and Yoshikawa. S. "SOLAR-5, An Interactive Computer-Aided Passive Solar 
Building Design System". _ ~ ~raohics~ Vol 2. No. 8. September 1979. Cygnus Publications, San Francisco. 
pp. 44-4b. Olgyay, Victor. Design With Climate ADDrOChtO Architectural Re~ionalism. 1963. Princeton 
University Press. Princeton, New Jersey. Ramsey, Charles G. and Sleeper, Harold R., GraPhic Standards. 
1970, John Wiley and Sons, New York. Rogers, Richard J. "An Interactive Graphics Input System for Building 
Energy Analysis". 1976, Cornell University, Ithaca, New York. Roos, D. /_~ System: General DescriPtion. 
MIT Department of Civil Engineering report R67-49. 1967, Cambridge, Mass. Schulman, Michael A. "The 
Interactive Display of Parameters on Two- and Three-dimensional Surfaces". 1981, Cornell University, 
Ithaca, New York. Robertz, Wayne. and Greenberg, Donald P. "A Graphical Input System for Computer-Aided 
 Architectural Design". CAD'80 Brighton, England, Spring 1980. pp. 715-723. Yessios, Chris I., "Computer 
Drafting of Stones, Wood, Plant,and Ground Materials". ComDuter_G_~aphics Vol. 13. No. 2., August 
1979, Pp. 190-198.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801286</article_id>
		<sort_key>243</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>35</seq_no>
		<title><![CDATA[Graphic input interaction techniques(Panel Session)]]></title>
		<page_from>243</page_from>
		<doi_number>10.1145/800064.801286</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801286</url>
		<abstract>
			<par><![CDATA[<p>In the last decade most research and development in computer graphics has been in the output side. Input devices and techniques for communicating with graphic systems are now also becoming active research topics. To communicate the state-of-the-art and to identify the key research issues, SIGGRAPH sponsored a workshop on graphics interaction input techniques in June, 1982. The topics to be discussed include tools for building user interfaces, recent successes using those tools, human factors, generic interaction types, interaction techniques for raster systems and style-independent interaction techniques. The workshop will be summarized with an address on the current state-of-the-art and identification of the major research issues. In addition, selected participants will be asked to address the primary research issues.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP40036757</person_id>
				<author_profile_id><![CDATA[81100411412]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Thomas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Battelle Pacific Northwest Laboratory, Richland, Washington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL: Graphic Input Interaction Techniques CHAIR: James J. Thomas Battelle Pacific Northwest Laboratory 
Richland, Washington 99352 In the last decade most research and development in computer graphics has 
been in the output side. Input devices and techniques for communicating with graphic systems are now 
also becoming active research topics. To communicate the state-of-the-art and to identify the key research 
issues, SIGGRAPH sponsored a workshop on graphics interaction input techniques in June, 1982. The topics 
to be dis- cussed include tools for building user interfaces, recent successes using those tools, human 
factors, generic interaction types, interac- tion techniques for raster systems and style-independent 
interaction techniques. The workshop will be summarized with an address on the current state-of-the-art 
and identification of the major research issues. In addition, selected participants will be asked to 
address the primary research issues. Panelists: to be selected 243 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801287</article_id>
		<sort_key>245</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>36</seq_no>
		<title><![CDATA[Ray tracing parametric patches]]></title>
		<page_from>245</page_from>
		<page_to>254</page_to>
		<doi_number>10.1145/800064.801287</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801287</url>
		<abstract>
			<par><![CDATA[<p>This paper describes an algorithm that uses ray tracing techniques to display bivariate polynomial surface patches. A new intersection algorithm is developed which uses ideas from algebraic geometry to obtain a numerical procedure for finding the intersection of a ray and a patch without subdivision. The algorithm may use complex coordinates for the (<italic>u, v</italic>)-parameters of the patches. The choice of these coordinates makes the computations more uniform, so that there are fewer special cases to be considered. In particular, the appearance and disappearance of silhouette edges can be handled quite naturally. The uniformity of these techniques may be suitable for implementation on either a general purpose pipelined machine, or on special purpose hardware.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Computer graphics]]></kw>
			<kw><![CDATA[Parametric patches]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[Ray tracing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Raytracing</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010374</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Ray tracing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P132648</person_id>
				<author_profile_id><![CDATA[81100653012]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Kajiya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[California Institute of Technology, Pasadena, Ca.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[APPEL, A., "Some Techniques for Shading Machine Renderings of Solids", 1968 SJCC, pp. 37-45.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[BLINN, J.F., "Computer Display of Curved Surfaces" Ph.D. Thesis, U. of Utah, Computer Science Department, Salt Lake City, Utah. 1978.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>507101</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[BLINN, J.F., "Simulation of Wrinkled Surfaces", Computer Graphics, v.12, August 1978, pp.286-292.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[BLINN, J.F., CARPENTER, L.C., LANE, J.M. AND WHITTED, T., "Scan Line Methods for Displaying Parametrically Defined Surfaces", Comm. ACM, v.23, January 1980, pp.23-34.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[CATMULL, E.E., "A Subdivision Algorithm for Computer Display of Curved Surfaces" Ph.D. Thesis, U. of Utah, Computer Science Department, Salt Lake City, Utah. 1974.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[LANE, J.M., AND CARPENTER, L.C., "A Generalized Scan Line Algorithm for the Computer Display of Parametrically Defined Surfaces", Computer Graphics and Image Processing, v.11, 1979, pp.290-297.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[LITTLEWOOD, D.E., A University Algebra, Dover, 1970.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[RALSTON, A., A First Course in Numerical Analysis, McGraw-Hill 1965.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[ROTH, S.D., "Ray Casting for Modeling Solids" Computer Graphics and Image Processing, v.18, 1982, pp.109-144.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807479</ref_obj_id>
				<ref_obj_pid>965105</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[RUBIN, S., AND WHITTED, T., "A Three-Dimensional Representation for Fast Rendering of Complex Scenes", Computer Graphics, v.14, 1980, pp.110-116.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[ULLNER, M., private communication, 1981.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[USPENSKY, J.V., Theory of Equations, McGraw-Hill, 1948.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[WALKER, R.J., Algebraic Curves, Springer-Verlag, 1950.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[WHITTED, T., "An Improved Illumination Model for Shaded Display", Comm. ACM, v.23, June 1980, pp.343-349.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RAY TRACING PARAMETRIC PATCHES James T. Kajiya California Institute of Technology Pasadena, Ca. 91125 
 ABSTRACT. This paper describes an algorithm that uses ray tracing techniques to display bivariate polynomial 
sur- face patches. A new intersection algorithm is developed which uses ideas from algebraic geometry 
to obtain a num- erical procedure for finding the intersection of a ray and a patch without subdivision. 
The algorithm may use complex coordinates for the (u, v)-parameters of the patches. The choice of these 
coordinates makes the computations more uniform, so that there are fewer special cases to be con- sidered. 
In particular, the appearance and disappearance of silhouette edges can be handled quite naturally. The 
uniformity of these techniques may be suitable for imple- mentation on either a general purpose pipelined 
machine~ or on special purpose hardware. KEYWORDS: computer graphics, raster graphics, ray trac- ing, 
parametric patches CR CATEGORIES: 1.3.3, 1.3.5, 1.3.7 From its inception, the method of ray tracing has 
al- ways been the technique of choice when ultimate rea- lism of computer generated images is the goal 
[Appel 1968]. This paper describes a new technique for generating ray traced images of piecewise polynomial 
bivariate parametric patches. In contrast to other algorithms to do this, the new algorithm does not 
repeatedly subdivide a patch, but rather calculates an intersection between a patch and a ray using more 
or less direct numerical procedures. This procedure has a number of pleasant characteris- tics. First, 
for patches of low degree, the algorithm Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. @ 1982 ACM 0-89791-076-1/82/007/0245 $00.75 proceeds more quickly. 
In fact, if the patch coefficients degenerate to a planar surface the amount of computa- tion needed 
to intersect a ray with it is roughly the same as for an algorithm tuned for ray-plane intersec- tions. 
Second, the algorithm is robust --many patch algorithms need preliminary subdivisions to satisfy some 
a priori approximation [Blinn, et. al. 1980], the algorithm presented here has no such requirement. This 
paper treats the intersection problem only. There are many other components to a ray tracing system, 
such as the lighting model calculation [Whirred 1980], the use of object coherence to mitigate scene 
com-plexity [Rubin and Whitted 1980], antialiasing in the ray tracing context [Whitted 1980], and the 
use of reflectance and normal vector perturbation mapping techniques to enhance realism [Blinn 1978b]. 
We study the intersection problem because it has been found to be the most time critical step. §1 Notation 
This section establishes the general notation of the paper. We also set up the basic equations for patch, 
line, and plane definitions. A point in RP3, real projective 3 space, is given by its homogeneous coordinates, 
i.e. a 4-vector z k, k = 0, 1, 2, 3. Usually ~x °, x 1 , x 2, x ~) is written as (Z, y, Z, ~.O) = (Z 
1,z 2,z 3,ZO). 245 A typical representation of a bicubic patch is as a set of 4 by 4 matrices, one :matrix 
for each homogeneous coordinate: k - Pli' %3, k=0,...,3. To calculate the 4D point corresponding to 
a chosen u, v-parameter value we use zk _k . 3--i. 3--j = pliu v k = 0,..., 3. (1) Throughout this paper 
we shall use the summation convention: if an index appears twice in a term then it is summed across the 
range of its values. The expres- sion above when written in full is actually 3 3 zk ZZ k a--i 3--i PijU 
V k = 0, ...,3 i~0 ./~0 or in the familiar matrix form x k --~ (u a u 2 u 1)(P '~) k -~ 0,...,3 where 
pk = (p~j). Note that u" is an algebraic n th power while x k is a coordinate indexed by k. No con- fusion 
should ever result since parameter values are al- ways multiplied and never indexed while spatial coor-dinates 
are always indexed and never multiplied. Now a ray has several co-ordinate representations. One can represent 
it as two points on the line, a point and direction cosines, or the intersection of two planes. For our 
purposes we choose the latter representation. Thus a line is represented by the equation: l~'z k = O, 
ct = O, 1. (2) §2 Intersecting A R~q with A Patch This section discusses the process we use to transform 
the ray-patch intersection problem in real projective 3 .~ace into the problem of intersecting two algebraic 
curves in complex 2 space. By performing such a trans- tbrmation we will be able to make use of tools 
available from algebraic geometry. To intersect a patch and a ray we substitute equation (1) into equation 
(2) to obtain: l~ k 3-i s-j kpiiu v = 0 a -~-O, 1 or, setting o k lkPli -~ ali 1 k IkPlj -~- blj we have 
the equations: aiiua-lv 3-j = 0 biju3-iv 3-i = O. Each of these two equations define the algebraic curves 
formed by the intersecting the patch with each of the two line defining planes. The intersection points 
of these two algebraic curves in turn give the u, v-param- eter values at which the ray intersects the 
patch. This proves our first theorem. Theorem 1. Let a patch be given by a ~ by ~ by array of coefficients 
p~y and a line be given by its 2 by array of coefficients l~. To find the parameter values u, v at which 
they intersect we may find the solution of two algebraic equations: alju3--iv 3-j = 0 biiua-lv 3-j -~- 
0 where 0 k ali ~ lkPff and 1 k blj ~ lkpij Proof: | The above two equations must be solved simultaneously. 
They represent degree six algebraic curves in the (u, v)- plane. They are not, however, completely arbitrary 
degree six curves. We can consider algebraic curves as points in. a vector space. This is done via their 
defining polynomial equations since the coefficients of the defining polynomial form a vector space. 
In our case above, the dimension of the vector space of curves is only half the dimension of the vector 
space of general degree six algebraic curves. As we shall see, the smaller space cuts down the number 
of possible solutions by half. §3 The Resultant We now present the key observation of this paper: that 
it is possible to intersect algebraic curves via a relatively straightforward mechanical procedure. We 
describe the actual intersection algorithm in section 5. This section and the next will justify the steps 
involved in the algorithm. The reader may safely skip to section 5 on a first reading --at the risk of 
being somewhat mystified by a few of the steps of the algorithm. In finding a procedure to intersect 
two algebraic curves given by bivariate polynomials, we use some results about univariate polynomials. 
These results may be found in most books treating algebraic geometry or the theory of equations [Walker 
1950, Littlewood 1970, or Uspenskii 1948]. We now state the central definition of this section. Definition. 
The resultant of two polynomials a(u) = aiu n-i and b(u) = bin m-j of degree n and m, reap. whose roots 
are al,..., an and fll,---,fl, is defined as: R(a, b) = a~b~(-1) m+"-I H (al -flj). (3) i~l,...,n 3"~1,...,m 
From this definition follows two obvious facts. Theorem 2. The resultant of two polynomials is zero iff 
they share a common zero. Proof: Obvious from the definition, since if a¢ ---- fly for some i, ] then 
and only then would the expression for R(a, b) vanish. | Theorem 3. The resultant R(a, b) is homogeneous 
and symmetric in each set of roots Oq,..., a n of a(u) and ill,..., fin of b(u) considered independently. 
Proof: Obvious from inspection of the definition of the resultant and the definition of a symmetric polynomial 
given immediately below. | Definition. An expression f(xl,..., Z n) of n vari-ables is called symmetric 
if its value is unchanged upon any permutation of its arguments. Example. Here are some symmetric functions 
for n=3: 81(zO, zl,Z 2) -~ Z 0 -[-Z 1 --I-Z 2 s2(z °,z i,z 2) = zoz 1 +z°z 2 +zlz 2 ss(z ° , x 1 , z 
2) = zOzlz 2" Why is it important to notice that the resultant is symmetric in the roots of the polynomials 
a and b? It is because of the next theorem. Theorem 4. Let a(u) = ao uk + al uk-1 +... + ak be a polynomial 
with roots al,...,ak. Then any ez-pression 4p symmetric in the ai can be rewritten as a polynomial in 
the coefficients ai of a(u) divided by ao and a factor of-I. Proof: The difference of two symmetric polynomials 
is symmetric. We form the basic symmetric polynomials given in the example below. It is possible to lower 
the degree of ¢ simply by matching the highest degree monomial with a power of one of the basic symmetric 
terms. Induction finishes the proof. | Example. Let a(u) = aiu 3-I be a cubic polynomial. Then from the 
previous example we have: (-1) la0sl(al,a2,a3) : al + a2 + a3 = al (-1)2aos2(al, a2, an) = alO:2 + ala3 
+ a2as = a2 (--1)Saos3(oq, a2, as) = OtlOt2a3 -~ as. Finally, we can now state the key fact that we need 
to intersect algebraic curves. Theorem 5. The resultant of two polynomials can be ezpre,aed as a polynomial 
in the coefficients of each. Proof: Use theorem 3 and theorem 4. | This important result allows us to 
determine whether two univariate polynomials share a common root by calculating directly from their coefficients 
without first factoring each polynomial into its roots. We merely calculate the resultant and test for 
zero. For univariate polynomials the resultant gives us a convenient method 247 for testing for common 
zeroes of two polynomials. For bivariate polynomials such a convenience becomes necessity: the resultant 
allows us to solve the curve intersection problem in u and v separately. The resultant R(a, b) may be 
expressed as a polynomial in the coefficients. What does this polynomial look like? We might multiply 
out the product in equation (3) and attempt t.o calculate the polynomial directly Fortunately, there 
is an easier way. §4 The Bezout Determinantal Form of the Resultant This section discusses a method for 
directly calculating the resultant R(a, b) as a polynomial in the coefficients of a and b. The resultant 
takes on a determinantal form whose appearance depends on the degrees of the polynomials a and b. ~We 
show how to generate a determinant for each degree and display determinants for two cases: the full degree 
six bicubic case and the case for the intersection of two lines. If a(u) and b(u) share a common root 
~ then a(~) = 0 and b(~) = 0. Furthermore, if a(u) and b(u) have degrees n, m we can without loss of 
generality assume that n ~_ m (or, in other words, that n ~ m-t-r, r > 0). It is then easy to see following 
set of equations also obtain:  b0a( ) - a0 rb( ) = 0 (b0~ + bl)a(~) -(a0~ + al)~rb(~) = 0 (b0~ ra-i 
"{- bi ~ m-2 +"" "t- bm-1)a(~) --(a0~ '~-1 + ale '~--2 +... + am-1)~'b(~) --~0 ~r-lb(~) = 0 = 0 = 0. 
If we multiply out these equations and collect like powers of ~ we get a system of algebraic equations. 
Now, if 1,~, ~2,...,~, are considered to be inde-pendent variables we then have a linear system of equa- 
tions. For that system to have a solution the deter-minant of the coefficients must be zero. This deter- 
minant is exactly the resultant R(a, b). Let us illustrate the case for n = 3, m = 3, and r ---- 0. We 
have the following determinant: F 0011 a0 al ao a2 a3 R(a,b)= bo b2 bo b3 + ao a2 ao a3 al a2 a3  bob3 
I I blb3 b2 b3[ aO a3 al 63 62 a3 If this determinant is zero then the two cubic polyno- mials have 
a root in common. Another example is if n = m = 1. The system of equations then reduces to R(a,b) bo 
bl = ao a l which is the ordinary determinant encountered when solving this particular linear system 
of equations ob-tained by intersect the two lines by more conventional means. The above argument shows 
that the vanishing of the Bezout determinant is a necessary condition for a com- mon root to occur, but 
not a sufficient condition. The argument for sufficiency is more involved: it involves a manipulation 
of the definition given in the previous section. §5 Calculating the Intersection of Two Curves We need 
to intersect the two algebraic curves given by the locus of solutions of two bicubic polynomials a(u, 
v), b(u, v). We use the resultant to find such inter- sections by a trick. This trick is to consider 
bivariate cubic polynomials in u and v to be univariate polyno- mials in v with coefficients polynomials 
in u, i.e. = Thus to calculate the intersection of the a(u,v) and b(u, v) curves we simply find common 
roots of the (uni- variate) cubic polynomials in v. By theorem 2, the vanishing of the resultant R(a, 
b) = 0 gives us a necessary and sufficient condition for an in- tersection to occur. From theorem 5, 
the resultant is a polynomial in the coefficients of a and b. Since each of the coefficients of these 
polynomials are again poly- nomials in u, the resultant is a polynomial expression in these polynomials. 
Upon substituting, we obtain for the resultant a polynomial r(u) in u, i.e. R(a,b).~r(u) The vanishing 
of this polynomial is the condition for a common root in v, i.e. The roots of the equation  r(u)=O gives 
the u-coordinates of the intersection points of the two curves. The roots #1,#2,...,#q of the polynomial 
r(u) give the values of the u parameter at which intersections occur. To find the values of the v parameter 
at which intersections occur we successively substitute #i into the coefficients to obtain polynomials 
in v. Since we already know that these polynomials have a common root it is unnecessary to solve each 
polyomial equation separately and match roots. Rather, we may simply compute the GCD of the two polynomials 
to obtain a polynomial which divides both polynomials. Often this GCD polynomial is of degree one allowing 
us to read off the v parameter value directly from its coefficients. Here is a summary of the intersection 
procedure: 1. Compute the Bezout determinantal form r(u) of the resultant R(a, b) of the two bicubic 
polyno- mials a(u, v),b(u, v) each considered as (univariate) cubic polynomials in v with coefficients 
in C[u]. 2. Solve r(u) ---- 0 to obtain #l,..-,#q the u param- eter values of the intersection points. 
 3. For each #i calculate via the Euclidean algorithm the GCD of the cubic polynomials a(#i, v), b(#i, 
v). 4. Find the roots u(i of the GCD to obtain the v parameter values of the intersection points.  
There are a number of special cases in the above algo- rithm. First, when calculating the Bezout determinant 
one of the polynomials, say a(u, v), may have degree 0 in v. In this case the determinant need not be 
com- puted at all since a(u, v) and b(u,v) have a common zero when and only when a(u) has a zero. Second, 
the resultant relies on the fact that a0, b0 ~ 0 for the two polynomials. When these coefficients are 
themselves polynomials we get spurious zeroes of the Bezout deter- minant at the roots of a0, b0. This 
is easily detected at the GCD stage: the the two substituted polyno-mials will be mutually prime. Third, 
the occurrence of multiple intersections will cause the GCD of the sub- stituted polynomials to have 
degree higher than one. This is easily treated by simply solving the resulting GCD polynomial for all 
roots. Finally, the presence of numerical error makes computing tee GCD polynomial somewhat delicate. 
We use the Euclidean algorithm modified by treating very small coefficients as zero. We caution however 
that a careful evaluation of the algo- rithm with respect to numerical error and stability has not yet 
been undertaken. Figures 1 and 2 show the results of applying this proce- dure to two different patches. 
We have not yet incor- porated the lighting models which include reflection, refraction, and shadows. 
§6 Bezout's Theorem How many different intersections can a ray have with a bicubic patch? We shall show 
that there are 18 possible intersections--they are, however, not necessarily real. The next theorem that 
tells us at how many points two distinct algebraic curves can intersect. It is a generalization of the 
fundamental theorem of algebra. Theorem 6. [Bezout). If two curves of degree m and n do not share a common 
factor, then they intersect in exactly mn points counted with multiplicities. Proof: See Walker (1950). 
II Note that if one of the curves is linear then it has degree 1 and we collapse to the fundamental theorem 
of algebra. This example points out that the theorem does not hold if we limit our attentions to the 
real case. Nor should we ignore the points at infinity. Thus to properly hold, Bezout's theorem is stated 
in the setting of complex homogeneous coordinates. Because bicubic polynomials are not general polyno- 
mials of degree six we may improve upon Bezout's theorem. Bezout's theorem implies that there may be 
up to 36 intersections of a ray with a patch, but for the case of bicubic polynomials many of these intersections 
are always multiple. Each a~j is obviously a point in 16-dimensional poly- nomial space. A general degree 
six curve is the locus of solutions of the equation Z ciiuS-~v s-i = 0 i+j_<s or as matrices, the general 
degree six looks like (u s uS . . . u 1) vs while )ur case appears as Theorem 7. Two bicubic curve$ 
intersect at ezactly 18 point8 in CP2, the co mplez projective plane. Proof: Computing the resultant 
of the two polyno- mials as in the intersectio:a procedure above we see that the resultant polynomial 
r(u) has degree at most 18. Each root of this polynomial gives a u-value at which the a and b polynomials 
:have a root in common. §7 Solving Polynomial Equations At the heart of the algorithm presented above 
is a re- quirement to repeatedly solve univariate polynomial equations. Indeed, as we shall see, the 
bulk of the com- putation time for the whole intersection calculation is taken by the root finding procedure. 
It is thus crucial to use a highly efficient root finder. The method we have chosen is Laguerre's method 
[Ralston 1965]. It is an iterative scheme similar to Newton's but with a more robust behavior. Laguerre's 
method iterates ac-cording to the following procedure: ~k+l = a'(~k) ± V~ where n is the degree of the 
polynomial a, and H(~) = (. -1)[(. -1)a'(~) ~ - .aC~)a"~)]. The sign of the square root is chosen to 
make the update term have smallest magnitude. The derivation of Laguerre's method proceeds essen-tially 
the same way as does the Newton procedure except that one additional term is preserved in the power series 
expansion. This, in effect, approximates the function by a parabola instead of a line. The Laguerre procedure 
has a number of advantages of the Newton. It is more robust. It is cubically convergent, trebling the 
number of digits of accuracy at each step. It converges in a single iteration for linear and quadratic 
equations. It naturally finds all the complex roots. The well known Newton procedure also possesses some 
well known instabilities when presented with initial guesses that do not satisfy certain criteria. In 
con-trast, for a polynomial a(u) with real roots ~1 < ~2 < '" < (n, the Laguerre procedure is unconditionally 
st- able. No matter what the initial guess, it will converge to the nearest real root. Unfortunately, 
for complex roots the behavior of the Laguerre procedure is not so thoroughly understood although it 
has been found in practice to work well. Certainly our experience with solving the polynomial equations 
encountered during the intersection process above has shown the procedure to be remarkably stable. Because 
of the cubic convergence of the Laguerre pro- cedure, roughly five iterations are sufficient for the 
ac- curacy required for graphics. Since most patches tend to have degree lower than the full cubic, it 
is an advantage to be able to naturally solve quadratics and linear equations in one step. The Laguerre 
algorithm does this within a single iteration. Finally, the ability of the Laguerre algorithm to find 
complex roots naturally is in contrast to the Newton procedure. For a real polynomial, if the initial 
guess is real then all subsequent Newton iterations will be so. To obtain complex roots one must resort 
to a modification of Newton's algorithm such as, for ex-ample, Bairstow's. For these reasons we have 
chosen the Laguerre pro- cedure to calculate roots of the resultant polynomial. However, we consider 
the selection of a suitable root finding procedure to be an open question --especially when considering 
hardware implementation. Once a root has been found, the polynomial is deflated by synthetically dividing 
by the monomial correspond- ing to the root. This step is exactly Hornet's rule whose intermediate results 
form the new deflated coefficients. If the root ~ is complex and the coefficients of the poly- nomial 
are real then it is more efficient to deflate by the root and its conjugate simultaneously. This is done 
by synthetically dividing by the quadratic: z 2 + 2Re{~}z + I~12.  §8 Algorithm Complexity Because 
the probability distribution of the degrees of the incoming bicubic patches are unknown, as well as of 
the number of iterations for the Laguerre procedure to converge (it is strongly dependent on the degree), 
we are unable to estimate the average number of opera- tions to compute a ray-patch intersection. We 
will, however, estimate the worst case number of floating point operations. These estimates appear in 
Table 1. What do these numbers mean? The following is a highly speculative discussion of the performance 
of this algorithm in the context of a full ray tracing system. We emphasize that we have as yet not constructed 
such a system so that the following estimates are very approximate. The reader is cautioned to regard 
this speculation with skepticism. However, by giving the reader a rough indication of the order of magnitude 
of computation involved, this discussion may have some small value. Estimate that there are perhaps 1 
million rays that need to be traced per frame at 30 frames per second. Assume also that the hierarchical 
representation tech- nique of Rubin and Whitted culls intersection com-putations to on the average of 
one patch per ray. Since the Laguerre root finding procedure is O(n 3) in the number of iterations, let 
us also assume that the degree of the patches is such that one tenth to one quarter the number of floating 
point operations is needed in the average as in the worst case. We then estimate that using this technique 
we would need a 200-1000 gigaFlop machine to make real time ray traced images possible. It should be 
mentioned that an independent calculation performed by one of the reviewers of this paper yielded a complexity 
of 30 Hr/Frame on a quarter megaFlop machine (VAX-11/780), which makes it approximately as fast as existing 
programs. Also it puts the real time figure at 810 gigaFlops. The state of the art for general purpose 
machines is roughly 200-1000 megaFlops. It is clear that in order to realize real-time ray traced images 
we need substantial advances both in algorithm development as well as in special purpose hardware design. 
§9 Comparison with Other Algorithms At present only one other procedure is available for computing the 
intersection of a ray with a patch, viz. Whitted's [1980]. 1 We can liken the current situation in ray 
tracing to the suite of algorithms for render- ing patches in scan-line order. There are roughly two 
camps: the numerical analysis camp, of which Blinn's algorithm is the sole resident; and the subdivi- 
1I understand that Michael Potmesil of Rensselaer Polytechnic Institute has also implemented a ray tracing 
scheme for bicubic patches that is a hybrid of subdivision and numerical tech- niques. Unfortunately, 
I haven't yet seen it. sion camp, populated by (another) Whitted's method as well as the Carpenter-Lane 
and Lane-Carpenter ap- proach, cf. Blinn, et. al. [1980]. The new method is similar in spirit to Blinn's, 
espe- cially his "slow-but-accurate" method, Blinn [1978a]. In fact, the new method may be used as a 
scan-line algorithm and represenl~s a new entry in the numeri- cal analysis camp. Our method differs 
from Blinn's in that we do not have the various silhouette edge track- ers and maxima finders needed 
in his algorithm. We never need worry when contours appear and disappear since we are not using ]Newton's 
algorithm which has a critical need for a good initial guess. Whitted's ray tracing :method repeatedly 
subdivides patches using a necessary condition to determine upon which subpatch to recurse. This method 
bears a strik- ing similarity to the Carpenter-Lane algorithm for dis- playing patches in scan-line order, 
except that subdivi- sion must proceed along both parameters since the ob- ject is to subdivide down 
to a point rather than a span. The subdivision algorithm uses quite a bit of storage and converges only 
at a linear rate to the desired point. Thus many more iterations are needed than with the new method. 
On the other hand, each iteration be- ing a Catmull subdivsion [Catmull 1974] plus bound- ing sphere 
calculation is far cheaper than the steps proposed here. We can liken the subdiviision algorithms to 
root finding by binary search along both co-ordinates. The method presented here calculates a more powerful 
iteration step. The method conw;rges more quickly but is more expensive at each step. Which method is 
preferable? Certainly in the numerical analysis field, experience has shown that iterative calculation 
is preferable to subdivision search. Whether the same tradeoffs apply in the computer graphics context 
--especially when considering special purpose hardware --remains a question to be answered by a more 
careful and detailed comparison than we have done here. Finally we mention an approach due to Ullner[1981] 
which is similar to the Lane-Carpenter algorithm. This method does not actually subdivide but rather 
simply evaluates at specified points, until a flatness criterion is satisfied, then it directly calculates 
the ray-plane intersection. §10 Speeding up the Computation There are several optimizations which may 
be carried out on the above procedure. First, when roots of the Bezout polynomial are either found to 
lie outside the parameter interval (usually [0, 1]) or are found to be complex, then we may simply delete 
them from the list of roots and deflate the resultant accordingly. This step saves the subsequent substitution 
and GCD steps, which are unnecessary since these roots do not con-tribute to the visible portion of a 
patch. Second, if we shoot rays from the observer in scan-line order, we may retain one of the plane 
equations and move only the second. This eliminates half the algebraic curve com- putations in the step 
which calculates the resultant. Third, we may pick up a small amount of coherence by retaining the solutions 
to the previous round of rays from past scan lines to predict where the new inter- section may be. Blinn 
[1978a] has found this to be a most effective heuristic, reducing the average number of iterations to 
less than one. If we retain the out of bounds and complex roots then we may capitalize on Blinn's optimization 
further, for a ray which just misses a patch actually intersects it at a pair of com- plex conjugate 
points. See figure 3. Finally, it may be that eliminating the Laguerre proce- dure in favor of some form 
of Sturm's algorithm (which finds real roots within some interval), would yield ad- vantages. We have 
not yet investigated this question. §11 Suitability for Hardware Implementation The lack of coherence 
in the ray tracing computation can be considered to be both unfortunate and an ad-vantage. Incoherence 
can be an advantage when seek- ing to parallelize an algorithm. The opportunity for virtually unbounded 
parallelism is afforded by the fact the each pixel computation proceeds independently of all others. 
Because it uses well known numerical pro- cedures, fast floating point pipelines -- both embedded within 
large scientific machines and implemented as quasi-autonomous units --will without any difficulty directly 
accelerate the speed of image generation. The algorithm is bound almost exclusively by floating point 
operation speed, not by storage management, not by dataflow complexity, and not by program control com- 
plexity. Hardware to implement this technique must perform two operations with facility. First, it must 
be able to swiftly calculate fixed length recurrence formulae such as Horner's rule. And second, it must 
be able to find the roots of polynomials very quickly and in a robust manner. The algorithm as presented 
here is free of all ad hoc heuristics which may complicate the implementation of it in hardware. There 
are no silhouette edge trackers, maxima finders, etc. involved. The algorithm is either straight line 
code or conditional code dependent upon the degree of the polynomials. Thus the algorithm con- sists 
of only feedforward datapaths with no feedback datapaths. There is one indeterminate WHILE itera- tion 
in the root finder, which because of the excellent convergence of the Laguerre method may be unrolled 
to say 5 stages. Upon loop unrolling everything may be totally pipelined. Ad,~itionally, the possibility 
of using a single unified representation for planes and patches presents itself, since aside from a penalty 
in data storage and a small computational overhead, the cost of solving ray- plane intersections via 
this method compares not un-favorably with the direct linear solution. §12 Disadvantages and Open Problems 
Because no significant ray-to-ray coherence is utilized, aside from a rather weak object coherence, the 
algo- rithm consumes a tremendous amount of computing bandwidth. Given that a typical frame requires 
trac- ing approximately 1 million rays, a real time processor would have to dispatch the computation 
for each ray in 33 ns. Thus, if only 10 floating operations are required per ray, real-time ray tracing 
requires a 300 megaFlop processor. As we have seen, we have at worst case ap- proximately 6000 floating 
operations per ray. Thus it would seem that incorporating very strong ray- to-ray coherence is a sine 
qua non for a practical ray tracing system. On the other hand, techniques such as normal perturbation 
destroy almost any coherence bet- ween neighboring reflected and transmitted rays within a patch. It 
is difficult to see how one can retain any but weak coherence when employing the various in-tensity and 
normal perturbation mapping techniques which are so vital to the convincing realism offered by parametric 
patches. Perhaps it is impossible to recon- cile the two. It may well be that high quality images are 
by their nature very expensive. Another open problem is how to properly anti-alias ray traced images. 
Whitted applies a Catmull sub-division to each pixel [Whirred 1980]. He also op-timizes by restricting 
the aliasing procedure to areas which have high luminance gradients--such as edges. Even so, this form 
of anti-aliasing triples the cost of ray-tracing. For heavily mapped patches the penalty for anti-aliasing 
simply staggers the imagination. We believe that another way may be found to anti-alias ray-traced images. 
We are currently investigating the possibility that Hamilton's point characteristic from geometrical 
optics may be of some help here. ACKNOWLEDGMENTS. ] would like to thank Mike Ullner for many discussions 
and ideas about the ray tracing process. Also, the reviewers were helpful in making many valuable suggestions. 
§13 References APPELj A., "Some Techniques for Shading Machine Ren- derings of Solids", 1968 SJCC, pp. 
37-45. BLINN~ J.F., "Computer Display of Curved Surfaces" Ph.D. Thesis, U. of Utah, Computer Science 
Department, Salt Lake City, Utah. 1978. BLINN~ J.F., "Simulation of Wrinkled Surfaces", Computer Graphics, 
v.l~, August 1978, pp.286-292. BLINNj J.F., CARPENTER, L.C., LANE, J.M. AND WHITTED, T., "Scan Line Methods 
for Displaying Parametrically Defined Surfaces",Comra. ACM, v.23, January 1980, pp.23-34.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801288</article_id>
		<sort_key>255</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>37</seq_no>
		<title><![CDATA[An algorithm and data structure for 3D object synthesis using surface patch intersections]]></title>
		<page_from>255</page_from>
		<page_to>263</page_to>
		<doi_number>10.1145/800064.801288</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801288</url>
		<abstract>
			<par><![CDATA[<p>There are several successful systems that provide algorithms that allow for the intersection of polygonal objects or other primitive shapes to create more complex objects. Our intent is to provide similar algorithms for intersecting surface patches. There have been contributions to this concept at the display algorithm level, that is, computing the intersection at the time the frame is generated. In an animation environment, however, it becomes important to incorporate the intersection in the data generation routines, in order that those parts of the intersected object that never contribute to an image are not processed by the display algorithm. This only increases the complexity of the object unnecessarily, and subsequently puts an additional burden on the display algorithms.</p> <p>An algorithm is described which uses a modified Catmull recursive subdivision scheme to find the space curve which is the intersection of two bicubic patches. An associated data structure is discussed which incorporates this curve of intersection in the patch description in a way suitable for efficient display of the intersected object. Sample output of these intersections are shown which serve to illustrate the capabilities and limitations of the described procedures.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Quadtree]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14134146</person_id>
				<author_profile_id><![CDATA[81100379189]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wayne]]></first_name>
				<middle_name><![CDATA[E.]]></middle_name>
				<last_name><![CDATA[Carlson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The Ohio State University, Computer Graphics Research Group, Columbus, Ohio]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baumgart, B.G., "Geometric Modeling for Computer Vision," AIM-249, STAN-CS-74-463, Stanford U. Computer Sci. Dept. (October 1974).]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, James F., "Computer Display of Curved Surfaces," PhD Thesis , University of Utah (December 1978).]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360727</ref_obj_id>
				<ref_obj_pid>360715</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Braid, Ian C., "The Synthesis of Solids Bounded by Many Faces," Comm. ACM Vol. 18(4) pp. 209-216 (April 1975).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Brown, C. M., "PADL-2: A Technical Summary," IEEE Computer Graphics and Applications Vol. 2(2) pp. 69-84 (March 1982).]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Catmull, Edwin E., "A Subdivision Algorithm for Computer Display of Curved Surfaces," UTEC-CSc-74-133, Salt Lake City, Utah (December 1974). University of Utah Dept of Comp Science.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807440</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Clark, James H., "A Fast Scan-Line Algorithm for Rendering Parametric Surfaces," Computer Graphics (SIGGRAPH 79 supplement) Vol. 13(3)(August 1979 ).]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cohen, Elaine, Lyche, Tom, and Riesenfeld, Richard F., "Discrete B-Splines and Subdivision Techniques in Computer-Aided Geometric Design and Computer Graphics," Computer Graphics and Image Processing Vol. 14(2) pp. 87-111 (October 1980).]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Forrest, A. R., "Computational Geometry -Achievements and Problems," in Computer Aided Geometric Design, ed. Richard F. Riesenfeld, Academic Press, New York (1974).]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908681</ref_obj_id>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Hunter, G.M., Efficient Computation and Data Structures for Graphics, Princeton U., Dept. of EE and CSc (1978). Ph.D. Dissertation.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Lane, Jeffrey M. and Carpenter, Loren C., "Scan Line Methods for Displaying Parametrically Defined Surfaces," Comm. ACM Vol. 23(1) pp. 23-34 (January 1980).]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Lane, Jeffrey M. and Riesenfeld, Richard F., "A Theoretical Development for the Computer Generation of Piecewise Polynomial Surfaces," IEEE Trans. Pattern Analysis and Machine Intelligence Vol. PAMI-2(1) pp. 35-46 (January 1980).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360355</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Levin, Joshua Z., "A Parametric Algorithm for Drawing Pictures of Solid Objects Composed of Quadric Surfaces," Comm. ACM Vol. 19(10) pp. 555-563 (October 1976).]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563884</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Parent, Richard E., "A System for Sculpting 3-D Data," Computer Graphics Vol. 11(2) pp. 138-147 Proc. Siggraph '77, (Summer 1977).]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Tilove, R. B., "Set Membership Classification: A Unified Approach to Geometric Intersection Problems," IEEE Trans. Computers Vol. C-29(10) pp. 874-883 (Oct. 1980).]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807363</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Whitted, J. Turner, "A Scan Line Algorithm for Computer Display of Curved Surfaces," Computer Graphics (SIGGRAPH 78 supplement) Vol. 13(3)(August 1978 ).]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Algorithm and Data Structure for 3D Object Synthesis Using Surface Patch Intersections Wayne E. 
Carlson The Ohio State University Computer Graphics Research Group Columbus, Ohio ABSTRACT There are 
several successful systems that pro- vide algorithms that allow for the intersection of polygonal objects 
or other primitive shapes to create more complex objects. Our intent is to provide similar algorithms 
for intersecting sur- face patches. There have been contributions to this concept at the display algorithm 
level, that is, computing the intersection at the time the frame is generated. In an animation environment, 
however, it becomes important to incorporate the intersection in the data generation routines, in order 
that those parts of the intersected object that never contribute to an image are not pro- cessed by the 
display algorithm. This only increases the complexity of the object unneces- sarily, and subsequently 
puts an additional bur- den on the display algorithms. An algorithm is described which uses a modified 
Catmull recursive subdivision scheme to find the space curve which is the intersection of two bicubic 
patrhes. An associated data structure is discussed which incorporates this curve of inter- section in 
the patch description in a way suit- able for efficient display of the intersected object. Sample output 
of these intersections are shown which serve to illustrate the capabilities and limitations of the described 
p~cedures. CR Categories and Subject Descriptors: 1.2.5 [Com- puter Graphics]: Computational Geometry 
and Object Modeling -Curve, sur.face, solid, and object This research was supported in part by the National 
Science Foundation, grant number MCS 7923670. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; 1982 ACM 0-89791-076-1/82/007/0255 $00.75 General Terms: 
combinatorial geometry, recursive subdivision, surface patch Additional Keywords and Phrases: quadtree 
 1. INTRODUCTION One of the most important aspects of complex image synthesis is the generation and 
description of the data comprising an object to be rendered. Many early techniques were developed to 
deal with this task, which might be referred to as computa- tional geometry [8] or computer-aided geometric 
~esign. The most interesting general purpose data generation techniques involve interactively specifying 
and successively modifying certain primitive structures in order to create an object for computer display. 
These primitive structures can be points, lines, polygons, or parametric surfaces. Some of the most 
complex three dimensional objects have been created in an environment of this type by using capabilities 
which lie in the realm of combinatorial geometry. This involves describing a three dimensional object 
by the com- bination of simpler geometric shapes or primi- tives. That is, certain operators (for example, 
union and intersection) are applied to two objects, resulting in a third object which is defined by the 
original two and the operator. Several systems have been developed [3,13,1,4] which combine two polyhedral 
objects by inter- secting their faces in order to define a third object. Levin [12] described a similar 
combina- torial technique for quadric surfaces. This paper attempts to extend the concept of combinatorial 
geometry to parametric (in particu- lar, bicubic) surface patches by developing an algorithm that finds 
the space curve which is the intersection of two patches, and uses this infor- mation to form a third 
object from two objects comprised of surface patches. The algorithm proceeds by using a Catmull patch 
subdivision scheme to recursively subdivide two patches until the intersection, if it exists, can be 
isolated. 2. CATMULL SUBDIVISION OF BICUBIC PATCHES A bicubic parametric surface patch can be defined 
in matrix notation as follows: V(u,v) (u 3 u 2 B t v 2 t = u 1) * B * P * * (v 3 v I) where P is a 
4x4 matrix representing the sixteen points comprising the defining polygonal network for the patch. B 
is the 4x4 matrix comprising the coefficients of the basis functions. In particu- lar, if we choose to 
use the Bernstein polynomi- als as our basis, these functions are f(x) = (I -x) 3 = I - 3x + 3x 2 -x 
3 g(x) = 3x(I -x) 2 = 3x -6x 2 + 3x 3 h(x) = 3x2(1 -x) = 3x 2 -3x 3 3 k(x) = x and the corresponding 
matrix is -I 3 -3 I -- 3 -6 3 0 B = -3 3 0 0 I 1 0 0 0 Using this formulation, Catmull[5] showed 
that such a patch could be subdivided into four sub- patches by finding the midpoint of the patch and 
the midpoint of each of the four boundary curves. By observing that the midpoint of a cubic curve is 
the average of its two endpoints less a correction term, he created a correction matrix C for the patch 
which contains the endpoints of all the boundary curves, or the corner points of the patch, and the correction 
values. C can be defined as C = S * B * P * B t * S t where 0 0 0 1 0 1 0 0 S = 1 1 1 1 3 I 0 I 
 is derived to obtain the correction terms for each curve. The center of the patch can be obtained by 
utilizing the values and the correc- tion terms of the midpoints of the boundary curves. 3. PATCH INTERSECTION 
 After two objects defined as a collection of bicubic patches defined with the Bernstein basis have been 
interactively positioned and oriented in space, the algorithm proceeds on a patch by patch basis. Each 
patch of the first object is compared against every patch of the second object, and the intersection 
is calculated. The algorithm has three main subalgorithms. First, the subdivision of the patches is 
per- formed. Second, the intersection is calculated, and finally the new object is described according 
to the combinatorial operator in effect.  3.1. SUBDIVISION ALGORITHM Since there are more (and more 
complex) steps as the algorithm progresses, it is desirable to determine if a patch might possibly participate 
in the intersection as soon as possible, and eliminate it from consideration if not. At the highest level, 
the test relies on the fact that the Bernstein basis possesses the convex hull property; that is, all 
points on the surface of the patch lie within the convex hull defined by the sixteen points of the defining 
polygonal net- work. Thus, it suffices to show that the convex hulls of the two patches under consideration 
are linearly separable to determine if the patches intersect. As it was discovered, the separability 
test for two convex hulls was more expensive than performing the more complex tests on the patch later 
in the algorithm, so a faster test was desired. After determining that a simple min-max enclos-  ing 
box test which used the defining polygon was inconclusive for too many cases where the patches were 
in fact separable, the following test was implemented. An arbitrary planar equation related to the 
defining polygonal network was calculated. In particular, three of the corner vertices of the network 
were used to obtain the planar equation. The maximum distance of each of the other network points from 
the plane, both above and below, were determined. This segmented the space into three parts, with the 
patch guaranteed to be totally within the middle seg- ment, since the convex hull is guaranteed to 
be totally included in this segment. (See Figure I) If any point in the network of the other patch 
 was found to lie within this center segment, no determination of separability could be made. In most 
cases that were tried, this test drastically cut the number of patches that needed to be con- sidered 
further. In many cases, there were over fifty percent fewer patches than were considered with the basic 
min-max enclosing box test, and this segmentation test was only slightly more expensive to perform. 
 The subdivision algorithm is a recursive algo- rithm. If two patches failed the separability test described 
above, then one is divided into four subpatches. The actual bicubic patch is divided according to the 
Catmull scheme presented in section 2. Then a new defining polygonal] net- work for each new subpatch 
is calculated. This is done relatively simply by using the inverses of some of the matrices that were 
used in the subdivision. If C is the matrix containing the subpatch values and correction information 
for the four corners, and S and B are the matrices defined in the section 2, then the matrix SP of subpolygon 
coordinates is given by -I -I (st)-1 (Bt)-1 SP = B * S * C * *  The separability test is then made 
for the sub- patch using this new subpolygon network and the second of the two patches. The subdivision 
is recursively performed on a subpatch until either its subpatches pass the separability test, or the 
faces of the defining polygonal networks for these suhpatches are coplanar to within a given epsilon. 
The planarity is measured by determining the distance from each point in the defining polygonal network 
to the plane containing three of the corner points, and testing to see if this distance is less than 
epsilon. When this happens, it can be shown that the patch will approximate very closely the quadrilateral 
containing the four corners of the network. Hence, this quadri- lateral is entered into the data structure 
to be considered in the intersection part of the algo- rithm. The second patch is then recursively sub- 
 divided, being tested against the first patch. 3.2. INTERSECTION ALGORITHM When the subdivision algorithm 
completes, we have two sets of (planar) polygons, one set from each patch. The intersection algorithm 
is an order n-squared algorithm that compares each polygon of one set against every polygon of the second 
set. Once again, an essential criterion is that processing will be aborted as soon as possible if no 
intersection between the two polygons exists. The following sequence of tests will guarantee that this 
criterion is met to an acceptable degree. First, a (min-max) enclosing box test is performed on the two 
polygons under considera- tion. If the test is successful, the two polygons are linearly separable and 
need not be considered any further. If it fails, then the first polygon is compared edgewise against 
the second. The planar equation of the second polygon is evaluated at both endpoints of an edge. If both 
evaluate to nonzero results of the same sign, they lie on the same side of the plane con- taining the 
quadrilateral, and hence can't inter- sect. If not, the edge is compared against the bounding box of 
the quadrilateral, and we only continue if they can't be shown to be separable. Next, the point of intersection 
between the edge and the plane containing the face is calcu- lated. Two tests are performed to determine 
if this point lies within the quadrilateral or not. The largest coefficient of the planar equation will 
determine which 2D plane will result in the 2D projection of the face having the largest area. Both the 
quadrilateral and the point of intersection are projected onto this 2D plane. A two dimensional min-max 
test is used on the projected quadrilateral and the projected point. If the point lies outside this 2D 
box, it cannot be inside the three dimensional quadrilateral. If it is inside the enclosing box, a vector 
emanat- ing from the projected point, and extending to infinity in some direction is considered. If it 
intersects the projected edges of the face an odd number of times, the point must be within the quadrilateral. 
(See Figure 2) These tests are done for each edge, and then the roles of the two quadrilaterals are reversed. 
Upon completion of this part of the algorithm, we have a collection of line segments in our data structure 
which are the intersections of the various quadrilaterals. Now they are organized, by using information 
about their relative orientation, into an ordered set that can faithfully be used as a good approx- imation 
to the intersection of the ~wo patches. 3.3. DATA STRUCTURE The subdivision process described above 
can logically be represented by a tree of degree four, or quadtree [9]. In this structure, the root of 
the tree represents an entire patch. The four children are the subpatches into which the patch is subdivided~ 
and the leaf nodes are those subpatches that either are found not to partici- pate in the intersection, 
or have satisfied the planarity termination condition. Each node of the tree is stored as a six field 
record. Included are pointers to the parent node, the four children, and a leaf pointer. If the node 
is itself the root of a subtree, the leaf pointer has the value NULL. Otherwise, it points to a record 
containing the following information relevant to the subdivision and subsequent inter- section: a pointer 
to the defining polygonal net- work for the subpat~h of the tree limb; the orientation of this subpatch 
relative to the other object; a pointer to a record containing information relevant to the actual intersection; 
links to records for subpatches 'outside' and subpatches 'inside' the other object. As the subdivision 
process progresses, it can determine if a subpatch absolutely does not con- tain the curve of intersection. 
If this is deter- mined, the relative orientation of the subpatch can also be easily calculated. In this 
case, the orientation is entered into the proper field of the record just described, and the intersection 
pointer is NULL. If, on the other hand, it cannot be determined without a doubt that the patch doesn't 
contain the curve of intersection, a link is established to the intersection information record, and 
the orientation field must wait until later to be filled in. If the link field to the intersection informa- 
tion record is not NULL, then the intersection algorithm considers the defined subpatch in its comparison 
to determine the space curve approxi- mation. This record contains the following fields: a count of the 
number of points of inter- section calculated and which are relevant to this particular subpatch (it 
is 0 if there are none); pointers to the coordinates of the intersection points; a pointer to the edge 
from which this point came; an orientation flag which indicates if the edge was 'entering' or 'leaving' 
the other subpatch (this is used to correctly recreate the new object in the combinatorial algorithm); 
the slopes of the subpatches at this point of inter- section. A quadtree was chosen as the internal 
data structure because it is relatively compact, it retains the information relative to the topologi- 
cal structures of the original patches and of the subpatches, and it lends itself quite well to the combinatorial 
operators available to the user. The implementation represents an entire object as an n-branch tree, 
where n is the number of patches in the description, and each branch links a quadtree, which is related 
to the corresponding patch. Figure 3 shows a patch subdivided until those subpatches whose polygonal 
approximations possi- bly participate in determining the curve of intersection were determined. Figure 
4 represents the associated quadtree. A black leaf node represents a non NULL node, with relevant infor- 
mation provided by the appropriate algorithms.  3.4. COMBINATORIAL OPERATORS The purpose of the combinatorial 
operators is to provide a description to the system as to how two patch defined objects are to be combined 
in the creation of a third object. When the inter- section subalgorithm completes, the tree is built 
and contains all the information necessary in order to apply these operators. That is, the approximation 
to the curve of intersection has been found and all subpatches have information regarding their new definition. 
They also have recorded the relative orientation with respect to the other object. This orientation is 
necessary to classify [14] the subpatches. For the purpose of exposition, suppose that we want to combine 
two objects A and B as shown in Figures 5 and 6. Figure 7 shows the two objects in the desired positions. 
The resulting shape is shown for each of the available operators using these two objects. UNION(01,02) 
--this operator results in a third object 03 which contains the entire 'outside' surfaces of objects 
01 and 02. The two objects can be con- sidered "welded" at the space curve(s) of intersection, and if 
they are solid objects, any surface area not relevant to the display (e.g., if part of 02 is inside of 
01) need not be included in the description of 03. See Figure 8. INTERSECT(01,O2) --this creates 03 
from the space that is common to both objects, or the 'inside' surfaces of the two objects. See Figure 
9. CUT(O1,O2,S) --this operator creates 03 by "cutting" 02 with the surface of 01. S is an option to 
determine which part of 02 will be retained as 03. If S = '+', that part of 02 outside of 01 is used. 
If S = '-', the complementary surfaces will be used. (See Figures 10- 13.) It can be seen that the operation 
INTERSECT and UNION can be derived directly from combinations of the set difference, or CUT operations. 
They were used as operators explicitly because they are more commonly used. Since the tree structure 
used to store the sub- division retains an indication of the topological relationship, it is very easy 
to determine the resulting surfaces of the above operators. It suffices to traverse the tree along the 
links that go to the orientation records defined by the operator. 4. SUMMARY The concepts of patch 
subdivision have been used several times before for computer display of surfaces [5,2,15,10]. The idea 
of using planar- ity as a termination condition for the subdivi- sion process was used in a display algorithm 
by Lane and Carpenter and one by Clark [10,6]. Lane and Riesenfeld [11] used planar approximations to 
the subpatches as a means to determine the inter- section of Bezier and b-spline patches (see also[7] 
for a thorough treatment of patch inter- section issues). The primary difference between the ideas in 
this paper and those in the above references is that we do the intersection at object generation time, 
and incorporate the intersection information in the description of the resulting object. This reduces 
the time spent in rendering the object at each frame. If this intersection information isn't retained 
as part of the object description, it is necessary to recalculate the intersection each time a frame 
is generated. In an animation environment, where many frames using a given object need to be created 
for an animation sequence, the overhead can be quite significant. The concepts included in this paper 
are currently incorporated in an integrated data gen- eration system in use at the Computer Graphics 
Research Group. This system currently runs on a VAX 11/780. The objects generated by the subdi- vision 
process are being converted to a polygonal description before being displayed in solid shaded format 
using a polygon tiler. They are also displayed in vector format on a Megatek cal- ligraphic display. 
Research is continuing to pro- vide a patch display algorithm that can utilize the object description 
directly to render the object, rather than combining a patch display with a polygonal display algorithm. 
Moreover, the subdivision algorithm is being used to provide an efficient method for providing a compact 
polyhedral approximation to a patch defined object. Since only areas of excessive curvature need to be 
described as a collection of very small polygons, those areas of little curvature need not have more 
complexity than is necessary. The slope values of the subpatches at the points of intersection are recorded 
in the leaf nodes as described in Section 6. We are  interested in utilizing these values together with 
the curve of intersection to calculate "fil- lets", or smooth transitions where the patches intersect. 
We are particularly interested in modeling the human body as a collection of sur- face patches, and these 
fillets would provide the smoothness at the joints (eg, elbow, wrist, knee) that is desired for an accurate 
representation. 5. ACKNOWLEDGMENTS The author would like to acknowledge the advice and motivation of 
Dr. Frank Crow. Prof. Charles Csuri provided the necessary support, and other members of the Computer 
Graphics Research Group added assistance, comments, and criticisms. Julian Gomez deserves special recognition 
for the effort he put forth to provide the electronic documentation software. References I. Baumgart, 
B.G., "Geometric Modeling for Com- puter Vision," AIM-249, STAN-CS-74-463, Stan- ford U. Computer Sci. 
Dept. (October 1974). 2. Blinn, James F., "Computer Display of Curved Surfaces," PhD Thesis , University 
of Utah (December 1978).  3. Braid, Ian C., "The Synthesis of Solids Bounded by Many Faces," Comm. ACM 
Vol. 18(4) pp. 209-216 (April 1975).  4. Brown, C. M., "PADL-2: A Technical Summary," IEEE Computer 
Graphics and Applications Vol. 2(2) pp. 69-84 (March 1982).  5. Catmull, Edwin E., "A Subdivision Algorithm 
for Computer Display of Curved Surfaces," UTEC-CSc-74-133, Salt Lake City, Utah (December 1974). University 
of Utah Dept of Comp Science.  6. Clark, James H., "A Fast Scan-Line Algorithm for Rendering Parametric 
Surfaces," Computer Graphics (SIGGRAPH 79 supplement) Vol. 13(3)(August 1979 ).  7. Cohen, Elaine, Lyche, 
Tom, and Riesenfeld, Richard F., "Discrete B-Splines and Subdivision Techniques in Computer-Aided Geometric 
Design and Computer Graphics," Com- puter Graphics and Image Processing Vol. 14(2) pp. 87-111 (October 
1980).  8. Forrest, A. R., "Computational Geometry - Achievements and Problems," in Computer Aided Geometric 
Design, ed. Richard F. Riesenfeld,Academic Press, New York (1974).  9. Hunter, G.M., Efficient Computation 
and Data Structures for Graphics, Princeton U., Dept. of EE and CSc (1978). Ph.D. Dissertation.  10. 
Lane, Jeffrey M. and Carpenter, Loren C., "Scan Line Methods for Displaying Parametri- cally Defined 
Surfaces," Comm. ACM vol. 23(I) pp. 23-34 (January 1980 ).  11. Lane, Jeffrey M. and Riesenfeld, Richard 
F., "A Theoretical Development for the Computer Generation of Piecewise Polynomial Surfaces," IEEE Trans. 
Pattern Analysis and Machine Intel- ligence vol. PAMI-2(1) pp. 35-46 (January 1980).  12. Levin, Joshua 
Z., "A Parametric Algorithm for Drawing Pictures of Solid Objects Composed of Quadric Surfaces," Comm. 
ACM Vol. 19(10) pp. 555-563 (October 1976).  13. Parent, Richard E., "A System for Sculpting 3-D Data," 
Computer Graphics Vol. 11(2) pp. 138-147 Proc. Siggraph '77, (Summer 1977).  14. Tilove, R. B., "Set 
Membership Classifica- tion: A Unified Approach to Geometric Intersec- tion Problems," IEEE Trans. Computers 
Vol. C- 29(10) pp. 874-883 (Oct. 1980).  15. Whitted, J. Turner, "A Scan Line Algorithm for Computer 
Display of Curved Surfaces," Com- puter Graphics (SIGGRAPH 78 supplement) Vol. 13(3)(August 1978 ). 
   Computer Graphics Volume 16, Number 3 July 1982 Figure I The shaded area represents (a 2D cross 
section of) direction. The defining polygons containing the the convex hull of the defining polygon. 
P is the points PI and P4 are separable, but no determina- plane derived from three of the four corner 
tion can be made with this test for those contain- points, dl is the maximum distance in the + direc- 
ing points P2 and P3. tion, and d2 is the maximum distance in the - P2 " P1 Figure 2 The: solid box 
is the quadrilateral under con-PI from turther consideration. Point P2 is inside sideration after being 
projected onto the plane the box, since the semiinfinite ray crosses the that will result in the largest 
area. The points projected edge an odd number of times, while the PI, P2, and P3 have also been projected 
onto the ray associated with P3 crosses an even number of same plane. The dashed lines are the min-max 
times, eliminating it from consideration. enclosing box, and this test obviously eliminates 12 23 34 
46 49 47 f 48 24 Figure 3 N~V N~ I 23 SW ~ SE 24 LI /I I \ [2 Li 35 36 38 $7 46 47 49 48 39 40 I[~% 4[ 
 Figure 4 Computer Graphics Volume 16, Number 3 July 1982 Figures 5 and 6 Ol and 02 Figure 8 03 = UNION(OI, 
O2); Figure 9 03 = INTERSECTION(Of, 02); O Figure 7 Figure ii 03 = CUT(OI,O2,'-'); Figure 12 03 
= CUT(O2,OI,'+') Figure 13 Figure l0 O3 = CUT(O2,OI,'-'); 03 = CUT(O1,O2,'+=);  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801289</article_id>
		<sort_key>265</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>38</seq_no>
		<title><![CDATA[Scanline rendering of parametric surfaces]]></title>
		<page_from>265</page_from>
		<page_to>271</page_to>
		<doi_number>10.1145/800064.801289</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801289</url>
		<abstract>
			<par><![CDATA[<p>A scanline algorithm is described which renders bicubic patches directly from the parametric description without producing a polygonal approximation. The algorithm is partially based on earlier work by Whitted. A primitive object, called a &#8220;curved-edge polygon&#8221;, is defined, and an algorithm for breaking down a bicubic patch into the primitive objects is described. A general surface intersection method is employed to provide a robust silhouette edge detector. Shades are computed by calculating a cubic approximation to the normal surface and performing either a cubic or a linear interpolation of the bounding edge normals across the scanline. Subdivision of parametric surfaces is used to reduce the complexity of the surfaces being rendered, providing dramatic improvement in the results of both the silhouette detector and the shading methods.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Color, shading, shadowing, and texture</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382.10010384</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation->Texturing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP39035166</person_id>
				<author_profile_id><![CDATA[81100265872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dino]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schweitzer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Utah, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330115</person_id>
				<author_profile_id><![CDATA[81100349130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Elizabeth]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Cobb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Science Department, University of Utah, Salt Lake City, Utah]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>563893</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. "Models of Light Reflection for Computer Synthesized Pictures". In Proceedings of SIGGRAPH '77, ACM, July 1977, pp. 192-198.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>908845</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. Computer Display of Curved Surfaces, Ph.D. Thesis, University of Utah, Salt Lake City, Utah, December 1978. Also Tech. Report No. 1060-126, Jet Propulsion Laboratory, Pasadena, California.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>907242</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. E. A Subdivision Algorithm for Computer Display of Curved Surfaces, Ph.D. Thesis, University of Utah, Salt Lake City, Utah, December 1974. Also Tech. Report No. UTEC-CSc-74-133, Department of Computer Science, University of Utah.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Cohen, E.; Lyche, T.; and Riesenfeld, R. F. "Discrete B-splines and Subdivision Techniques in Computer-Aided Geometric Design and Computer Graphics", Computer Graphics and Image Processing, Vol. 14, No. 2, October 1980, pp. 87-111. Also Tech. Report No. UUCS-79-117, Department of Computer Science, University of Utah, October 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>358815</ref_obj_id>
				<ref_obj_pid>358808</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Lane, J. M.; Carpenter, L. C.; Whitted, J. T.; and Blinn, J. F. "Scan Line Methods for Displaying Parametrically Defined Surfaces", Communications of the ACM, Vol. 23, No. 1, January 1980, pp. 23-34.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Riesenfeld, R. F.; Cohen, E.; Fish, R. D.; Thomas, S. W.; Cobb, E. S.; Barsky, B. A.; Schweitzer, D. L.; and Lane, J. M. "Using the Oslo Algorithm as a Basis for CAD/CAM Geometric Modelling". In NCGA '81 Conference Proceedings, National Computer Graphics Association, Inc., June 1981, pp. 345-356.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Scanline Rendering of Parametric Surfaces* Dino Schweitzer and Elizabeth S. Cobb Computer Science Department 
University of Utah Salt Lake City, Utah 84112 Abstract A scanline algorithm is described which renders 
bicubic patches directly from the parametric description without producing a polygonal approximation. 
The algorithm is partially based on earlier work by Whitted. A primitive object, called a "curved-edge 
polygon", is defined, and an algorithm for breaking down a bicubic patch into the primitive objects is 
described. A general surface intersection method is employed to provide a robust silhouette edge detector. 
Shades are computed by calculating a cubic approximation to the normal surface and performing either 
a cubic or a linear interpolation of the bounding edge normals across the scanline. Subdivision of parametric 
surfaces is used to reduce the complexity of the surfaces being rendered, providing dramatic improvement 
in the results of both the silhouette detector and the shading methods. CR Categories and Subject Descriptors: 
1.3.3 [Computer Graphics]: Picture/Image Generation Display algorithms; 1.3.5 [Computer Graphics]: Computational 
Geometry and Object Modeling -Curve, surface, solid and object representations; 1.3.7 [Computer Graphics]: 
Three-Dimensional Graphics and Realism - Color, shading, shadowing and texture. General Term: Algorithms 
*This work was supported in part by the National Science Foundation (MCS-8004116) and the U.S. Army Research 
Office (DAAG29-81K-0111). Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise,or to republish, requires a fee and/or 
specific permission. @ 1982 ACM0-89791-076-1/82/007/0265 $00.75 1. Introduction Full-color shaded raster 
images can often convey more useful information about the shape of parametric surfaces than the better 
established line- drawing displays. Shaded raster images have great potential for use in computer-aided 
design systems because of their higher information content and minimal ambiguity. However, there is no 
general class of algorithms for generating such images. The continued study of methods for generation 
of raster images of smooth surfaces is crucial if designers are to be able to use the full power of high-quality 
computer graphics. Scanline rendering methods are an important subset of rendering algorithms for raster 
devices because they make use of coherence to produce images faster than other raster methods. Many scanline 
rendering algorithms for polygons are well known, but there are few, if any, truly satisfactory methods 
for rendering parametrically defined ~urfaces in scanline order. This paper describes an algorithm which 
renders bicubic patches in a scanline manner directly from the parametric surface description and without 
recourse to polygonal approximation. Problems which are addressed include: -deciding what are the characteristics 
of a patch which can be rendered "easily" in scanline order -deciding how more complicated patches can 
be subdivided into this simpler type of patch -devising a method for tracing out curved edges in scanline 
order -providing a method for calculating silhouette edges -devising an algorithm for assigning shades 
at each point on the patch 2. Background Some background in the area of scanline rendering of parametric 
surfaces is useful in order to understand the relationship between this algorithm and existing ones. 
 2.1. Existing Methods One of the earliest raster rendering algorithms for parametric surfaces was that 
of Catmull [3]. The algorithm was not scanline oriented, but rather used an extremely fast subdivision 
technique to subdivide the patch until a patch that was smaller than a single picture element (pixel) 
was obtained. At this point, the (small) patch could be rendered easily. Another method, which could 
conceivably be scanline oriented but was not necessarily so, involved deriving a polygonal approximation 
to the smooth surface and rendering the polygons. In order to get a smooth-shaded image, this approach 
involved approximating surface normals with averaging techniques. Further, deriving the polygonal approximation 
was often a tedious task which was performed manually. The Alpha_l group at the University of Utah is 
developing an experimental CAGD system in which high-quality raster graphics plays an important role. 
They currently use a method of automatically generating a polygonal approximation to B-spline surfaces 
to within a specified tolerance using subdivision techniques, producing high-quality smooth shaded renderings 
from those polygons using a scanline algorithm [6]. Their scanline-order subdivision approach is a modification 
of the Lane-Carpenter algorithm [5]. The advantage of the polygonal approximation approach to parametric 
surface rendering is that polygon rendering is fast and fairly easy to implement. One drawback to polygonal 
approximations is the boundary edges of such a surface. A finer approximation is required to avoid polygonal 
edges than is necessary to produce smooth shading on the interior of the patch. Often, the edges of a 
surface rendered in this manner have visible linear segments, even when the shading in the interior of 
the surface appears quite smooth. 2.2. Scanline Methods In addition to these approaches, there have 
been several attempts to render surfaces in scanline order directly from the parametric description without 
generating an approximating polygonal mesh. Blinn's algorithm [5, 2] was primarily an algebraic approach. 
He solved the surface and silhouette equations on each scanline, creating a list of boundaries for that 
scanline. By sorting th6 boundaries in order of x value on the scanline, successive pairs of boundaries 
represented areas that required shading. The algorithm was very general, but had some numerical instabilities. 
Whitted's algorithm [5] developed a primitive object which will be referred to in this paper as a "curved-edge 
polygon", a term which will be defined shortly. His algorithm detected silhouette edges by solving the 
normal equations along each edge for locations where the z component was zero. A cubic hermite interpolant 
was used to describe the silhouette between the points along the edge. To obtain finer resolution, the 
patch was subdivided prior to solving for silhouette intersections with the boundaries. Once the silhouettes 
were determined, the resulting curved-edge polygons were rendered in a typical scanline fashion using 
Newton iteration to solve for successive intersections of the edges with the scanlines. The major disadvantage 
of Whitted's approach was the lack of generality in finding silhouettes. His silhouette detector would 
fail to find silhouettes which did not intersect the boundary of the patch. 3. Algorithm Overview The 
algorithm described in this paper is loosely based on Whitted's algorithm. "Curved-edge polygons" are 
used as a primitive object type and more complex patches are reduced to this primitive type. The term 
"polygon" may be somewhat misleading because it gives the impression that the primitive objects are planar. 
This is certainly not the case -planar curved-edge polygons are a special case of the objects referred 
to as curved-edge polygons. The term "curved-edge polygon" is chosen because the methods which are used 
to render such an object are quite analogous to the methods for rendering polygons. We define a curved-edge 
polygon to be any bicubic patch which: -does not contain any silhouette edges (with respect to the current 
view) -has only bounding edges which are monotonic in y -can be represented by univariate cubic equations 
of its bounding edges The reasoning behind the first of these restrictions is quite obvious, because 
silhouette edges are the places where the patch begins to curve behind itself. The hidden surface problem 
can be ignored within a single curved-edge polygon if this criterion is met. The second and third requirements 
have to do with the nature of scanline rendering. Y-monotonicity of bounding edges insures that the edge 
only crosses any given scanline a single time, so it is not necessary to find multiple values of an edge 
for a scanline. The expression of the patch in terms of its bounding edges allows the edges to be traced 
out as the algorithm progresses up the scanlines. Appropriately shaded pixels are drawn between pairs 
of edges which intersect the current scanline. The algorithm consists of essentially two parts. The first 
part is a series of preprocessing steps. Bivariate cubic surface descriptions are converted into a standard 
internal form and these patches are processed to derive the curved-edge polygons. This involves detection 
of silhouette edges and edges which are not monotonic in y, as well as piecing of the edges back together 
into curved-edge polygons. The second part of the algorithm does the actual rendering of the curved-edge 
polygons.  4. Preprocessing Steps The preprocessing steps consist of: -Subdividing the patch to some 
user-specified flatness criterion to improve the shading. -Converting the patch to the internal power 
basis representation. -Deriving position coefficients for univariate cubic curves along the boundaries. 
-Deriving an approximation to the normal surface and the coefficients for univariate cubic normal curves 
along the boundaries. -Locating any silhouette edges in the patch. - Subdividing edges which are non-monotonic 
in screen-space y value. -Piecing the various boundary and silhouette edges back together into coherent 
"curved-edge polygons." All of these steps will be described in the following sections. 5. Subdivision 
A quick method for simplifying the complexity of a parametric surface is to subdivide the patch until 
some flatness criterion is met, and then to evaluate (render) each resulting patch as an independent 
surface. If a Bezier or open B-spline representation is used, the desired tangency and continuity conditions 
between patches are maintained throughout the subdivision. This subdivision step makes certain other 
aspects of the algorithm much better behaved. In particular, normal surfaces to bicubic patches can be 
degenerate, and subdivision makes them more manageable because the subdivided pieces have less curvature. 
This in turn makes both the silhouette detector and the shading computations more accurate and the resulting 
image is significantly improved. Figure 1 shows a bicubic patch, and Figure 2 shows a typical subdivision 
of the same patch. Flatness testing for a surface defined by Bezier or B-spline techniques takes advantage 
of the convex hull properties of the surface control mesh. One can compute the plane defined by three 
of the four corner vertices of the control mesh very easily. From there, the distance of each point in 
the mesh to the plane may be calculated and compared with the user-specified flatness tolerance. If the 
control mesh lies within this tolerance, then the convex hull property guarantees that the surface will 
also be within the same tolerance. 6. Bicubic Patches and Boundary Equations The general form for a bivariate 
parametric cubic surface C(u,v) is: [u3u2ul] MPMT [v3v2v1]T where M and P depend on the defining data 
for the surface. P could contain points in a Bezier control mesh, the bicubic position, derivative, and 
cross-derivative data, or any of several other data forms. The M's are chosen appropriately, in order 
to convert the convenient data form to bivariate power basis representation. The power basis form is 
the basic internal surface representation used in this algorithm; many other representations can be converted 
easily to this form. Most commonly, the P matrix contains three components (x, y, and z) and so C(u,v) 
is a three-dimensional point. For scanline rendering, it is useful to have a univariate description of 
each bounding edge of the patch. This is easily derived from the standard bivariate coefficient matrix 
Q = M P MT as follows: C(u,0) = edgeo(t) =[t3t2tl] [Q] [0001]W C(u,1) = edgel(t) =[t3t2tl] [Q] [llll]T 
 The expressions for C(0,v) and C(1,v) are similar. Four coefficients are required to describe a cubic 
curve, and these can easily be computed from the Q matrix. The coefficients for edgel(t), for example, 
are summations of the rows of Q. 7. Normal Coefficient Calculation The four edge curves derived in the 
last section enable us to render a scanline image for a patch without shading. However, shading information 
is crucial; a raster rendering is useless without shading information. Shades depend on the normal to 
the surface at each point and so a method of determining or approximating the normal at each point on 
the surface is required. The development below follows Catmull [3]. In order to make the discussion simpler 
(in terms of notation), the following abbreviations are defined, together with similar ones for V: U 
=[u 3 u 2 u 1] U' = [3u2 2u i 0] U"= [6u 2 0 0] It is easy to obtain expressions for the derivatives 
of the x-component of the surface as follows (where Mx is the x component of the coefficient matrix for 
the surface): X = U MxV dX(u,v)/du = X u = U' Mx V dX(u,v)/dv = Xv = U Mx V' The equations for the y 
and z components are analogous to those for the x component. An expression can be derived for the surface 
which describes the surface normal at each point (u,v). This can be achieved by computing the cross-product 
of the u and v derivatives of the surface. Thus, [ Xn Yn Zn ] =[Xu, Yu, Zu]X[Xv, Yv, Zv] = [ YuZv -YvZu, 
ZuXv -XuZv, XuYv-YuXv] The first component of the normal vector can be rewritten as Xn(u,v) = YuZv - 
YvZu -(U'MyV)(UMzV') - (UMyV')(U'MzV) This can be seen to be a quintic expression in u and v. We chose 
to approximate the quintic normal surface by a cubic in order to simplify the computation, and so that 
the rendering system could treat positional and normal information in a uniform way. Note that this may 
not be appropriate for some surfaces in which there are major undulations in the shading across the patch. 
This is one of the points where the preprocessing subdivision step has a profound influence on the quality 
of the shading in the resulting image. A cubic approximation can be derived by using the Hermite data 
(corner positions, derivatives in each direction at the corners, and cross-derivatives at the corners). 
The matrix for this data is: dXn dXn/dv ................................... dXn/dv d2Xn/dudv where 
the elements in each quadrant are evaluations of the functions at the corners of the patch. 2-2 1 1 -3 
3 -2 -1 0 0 1 0 1 0 0 0 transforms the bivariate Hermite data into the internal form. This is the x-component 
of the matrix of coefficients for the cubic approximation to the surface normal at each point. Similar 
computations may be used for the y and z components. Univariate edge normal coefficients are derived 
in exactly the same manner as before for the positional data. This representation allows edges to be 
traced and normals to be interpolated between the edges to render smooth-shaded images in scanline order. 
 8. Silhouette Determination The silhouette edges of a surface, if it contains any, are defined as those 
curves on the surface along which the z-component of the normal vector is zero (assuming the eye is on 
the z axis). In image space, this corresponds to the normal vector lying in the plane of the display 
screen. The algorithm uses a set of intersection routines developed by the Alpha _1 group at the University 
of Utah for intersecting B- spline surfaces. The intersection procedure is based on the Oslo Algorithm 
[4], which is used to add knots to B-spline curves. The Oslo Algorithm can be applied to performing fast 
subdivision of B-spline surfaces. The use of subdivision to produce the intersection curve of two B-spline 
surfaces is outlined as "Application 3" in [4]. Expressions for Xn(u,v), dXn/du(u,v), dXn/dv(u,v), and 
d2Xn/dudv(u,v) are easily derived. Xn(u,v) -(U'MyV)(UMzV') - (UMyV')(U'MzV) dXn(u,v)/du = (U"MyV)(UMzV') 
+ (U'MyV)(U'MzV') -(U'MyV')(U'MzV) - (UMyV')(U"MzV) dXn(u,v)/dv = (U'MyV')(UMzV') + (U'MyV)(UMzV") -(UMyV")(U'MzV) 
- (UMyV')(U'MzV') d2Xn(u,v)/dudv = (U"MyV')(UMzV') + (U"MyV)(UMzV") + (U'MyV')(U'MzV') + (U'MyV)(U'MzV") 
(U'MyV")(U'MzV) - (U'MyV')(U'MzV') -(UMyV")(U"MzV) -(UMyV')(U"MzV') If these equations are evaluated 
at the points (0,0), (0,1), (1,0) and (1,1), then the components of the matrix listed above can be filled 
in. Computing C P CT where C is To compute a silhouette edge, the cubic approximation of the normal surface 
and the z = 0 plane are converted to the spline representation used by the Alpha_l group. This provides 
a surface representation of the normals at each point on the surface. Evaluation of the normal surface 
at a parametric point (u,v) yields the normal vector of the geometric surface at the point (u,v). The 
intersection algorithm then uses fast subdivision and the convex hull property of B-spline meshes to 
determine a set of points which lie on the intersection line of the two normal surfaces. The parametric 
(u,v) values are then evaluated with the surface equation to determine a corresponding set of points 
on the surface which represent the actual silhouette in screen space. Further, connecting these points 
with straight lines is guaranteed to be within a user-specified tolerance of the true silhouette. Figures 
3 and 4 show patches with silhouette edges. 9. Curved-Edge Polygon Creation After the silhouette edges 
are determined, it is necessary to organize the collection of cubic edges into closed "curved-edge polygons" 
for rendering. Intuitively, this involves dividing the patch into separate pieces which are single valued 
in the z component. Practically, the problem of pairing edges into closed sections is more difficult 
than one might first suppose. The algorithm used to accomplish this grouping is an "edge chasing" method 
which joins edges together until they form a closed curved-edge polygon and then outputs that polygon. 
One edge of the boundary or silhouette is selected first to start the curved-edge polygon. Then the edge 
(silhouette or boundary) which joins it next is added to the polygon. If the first edge were a boundary 
edge, the succeeding edge might be the next boundary edge, or a silhouette edge that intersects the boundary 
edge. The edge-chasing continues until all the boundary edges and silhouette edges have been added to 
closed polygons. This method is adapted from the philosophy that one can always get through a maze by 
keeping one's right hand on a wall and making turns accordingly (assuming the maze has only one entry 
and exit). Similarly, by always making "right" turns whenever a new curve is encountered, the algorithm 
is guaranteed to produce closed polygons. The definition of a "right" turn is dependent on which boundary 
edge one is turning on (e.g. on the v = 0 boundary, a "right" turn is toward the end with the larger 
u value). One assumption made in this approach is that all silhouettes intersect boundary curves on both 
ends. As stated earlier, this is a poor assumption to make for the general case. However, because the 
silhouette detector finds all intersecting and non-intersecting silhouettes, the silhouette list can 
be preprocessed to create artificial intersections for the non-intersecting silhouettes. An artificial 
intersection is an extension of the silhouette in an appropriate direction until a boundary is crossed. 
These artificial edges have no effect on the final image, but do provide a means of accurately subdividing 
a patch with internal silhouettes. 10. Monotonicity of Edges in Y One final preprocessing step remains 
to be performed on the set of edges which form the curved- edge polygons. Each edge in the curved-edge 
polygon is tested for y-monotonicity by taking the derivative of the cubic edge equation and solving 
the quadratic formula for the resulting coefficients. The results are parametric values of minima and 
maxima on the edge. If the resulting values lie within the parametric range of the edge, the edge is 
subdivided. The subdivision is extremely simple. The cubic positional and normal coefficients of the 
edge are merely copied and the parametric range of the two resulting edges are adjusted so that the two 
edges together cover the entire range of the original edge, but each new edge is monotonic in y. 11. 
Rendering Steps In this section, the basic rendering method for the simplified patch described above 
is discussed. The patch has no silhouette edges and the edges are monotonic in y. In order to render 
curved-edge polygon objects on a particular scanline, a standard scanline algorithm places the edges 
on the start-lists and active-list. Then, the edges on the active list are paired off from the left of 
the screen to the right of the screen and the pixels between them are drawn. In order to draw between 
two edges, the intersection with the current scanline must be determined. Since the y value of the edge 
is known (ie. it is just the value of the scanline), the cubic equation can be solved for the y-component 
to discover the parametric value (t) of the edge for that y. As in Whitted's algorithm, a Newton iteration 
is used for this. Although in general Newton iteration may be unstable, it is quite effective for cubic 
edges which are monotonic in y. Also, if the parametric (t) value which was discovered on the last scanline 
is saved, then the initial value can be a very good approximation. Once the parametric (t) value of the 
edge on the scanline is obtained, evaluation of the x value of the edge on the scanline is performed 
by using the parametric value with the stored edge coefficients. The same parametric value yields the 
x, y, and z components of the normal vector to the surface at that point. The beginning and ending positions 
of the curved-edge polygon on the current scanline have been determined, as have the normals at each 
endpoint. A simple method of drawing the scanline segment is to interpolate the normal vectors across 
the covered area of the scanline and evaluate the shading value at each pixel. Both a linear and a cubic 
interpolation scheme have been implemented. The linear normal interpolation between two edges of a curved-edge 
polygon turned out to be unsatisfactory in many cases, because the normal of a curved-edge polygon does 
not necessarily change linearly across a scanline. The preprocessing subdivision of the patch until it 
reaches some specified flatness criterion reduces the problem to a smaller scale but does not solve it. 
However, using a cubic rather than linear normal interpolation improved the shading of the surface considerably. 
The required information for performing a cubic Hermite interpolation across the scanline between two 
edges of the curved-edge polygon is fairly easily computed. Once a normal is determined for each pixel, 
it is converted to an intensity value using Blinn's lighting model [1]. Figures 5 and 6 show a surface 
shaded with linear and cubic normal interpolation. Each shows both a top view and a side view to better 
illustrate how the patch curves in z. Linear interpolation assumes a constant change in the normal direction. 
Thus Figure 5 (with the light source from the right) has the highlight in the wrong position since the 
normal is not changing linearly across the patch. Cubic interpolation makes use of derivative information 
along the edge, and so it better approximates the normal change. This can be seen by the position of 
the highlight in Figure 6. 12. Miscellaneous Implementation Remarks As mentioned before, the hidden surface 
problem may be ignored within each curved-edge polygon because it contains no silhouette edges. However, 
the hidden surface problem must still be solved between individual curved-edge polygons. Because an entire 
image is rendered in one pass through the scanlines, a scanline z-buffer is used to determine which surface 
is visible. The current implementation uses bicubic Bezier surfaces as the external interface to the 
system. There are several reasons for selecting the Bezier representation. The Bezier control mesh provides 
a rough approximation to the surface, and thus makes designing test cases relatively simpler and more 
intuitive than if some other representation had been chosen. A second factor is that the subdivision 
process which was described is very easily implemented for Bezier patches. Further, the Bezier representation 
is actually a special case of cubic B-splines. Extension of the algorithm to bicubic B-spline surfaces 
would only involve converting the control mesh to the internal power basis representation. 13. Conclusion 
One disadvantage to the rendering algorithm described is that it is limited to bicubic patches. This 
would not be a particularly serious constraint, because much design work is done with cubics (or even 
quadratics), except for the fact that the normal surface to a cubic surface is quintic. A cubic approximation 
to the normal surface may be unsuitable in some cases, but subdivision alleviates the problem to a great 
extent. A second disadvantage is that the silhouette detection is complicated by the fact that the normal 
surface of the patch is often degenerate, but again, the subdivision step is quite effective in minimizing 
this problem. Advantages of the implemented algorithm are of two types: those concerning the use of a 
scanline algorithm, and those dealing with the rendering of curved edges rather than polygonal approximations. 
Scanline algorithms are generally faster than other techniques by making use of scanline coherence to 
quickly calculate position and shading information. The previous scanline position is used as a first 
approximation for the Newton iteration solution. Curved edge rendering eliminates the polygonal boundary 
problem which plagues man.y surface rendering schemes. The problem arises from approximating curved edges 
with polygons, resulting in visible "corners" appearing along patch boundaries. The patches rendered 
with this algorithm do not have this problem. Another advantage of curved-edge polygons is the relative 
size of the patch that can be rendered without subdivision. In a polygonal approximation, it is necessary 
to attempt to model the surface as several flat pieces, often resulting in an extremely large number 
of objects for a highly curved area. Since the algorithm described in this paper uses a cubic to approximate 
the normal for shading, a single curved-edge polygon can represent a relatively large area with significant 
curvature. 14. Acknowledgement We wish to thank Spencer W. Thomas for the use of his B-spline surface 
intersector. His implementation of that algorithm comprises the essential part of our silhouette edge 
detector. References 1. Blinn, J. F. "Models of Light Reflection for Computer Synthesized Pictures". 
In Proceedings of SIGGRAPH '77, ACM, July 1977, pp. 192-198. 2. Blinn, J. F. Computer Display of Curved 
Surfaces, Ph.D. Thesis, University of Utah, Salt Lake City, Utah, December 1978. Also Tech. Report No. 
1060-126, Jet Propulsion Laboratory, Pasadena, California. 3. Catmull, E. E. A Subdivision Algorithm 
for Computer Display of Curved Surfaces, Ph.D. Thesis, University of Utah, Salt Lake City, Utah, December 
1974. Also Tech. Report No. UTEC-CSc-74-133, Department of Computer Science, University of Utah. 4. 
Cohen, E.; Lyche, T.; and Riesenfeld, R. F. "Discrete B-splines and Subdivision Techniques in Computer-Aided 
Geometric Design and Computer Graphics", Computer Graphics and Image Processing, Vol. 14, No. 2, October 
1980, pp. 87-111. Also Tech. Report No. UUCS-79-117, Department of Computer Science, University of Utah, 
October 1979. 5. Lane, J. M.; Carpenter, L. C.; Whitted, J. T.; and Blinn, J. F. "Scan Line Methods 
for Displaying Parametrically Defined Surfaces", Communications of the ACM, Vol. 23, No. 1, January 1980, 
pp. 23-34. 6. Riesenfeld, R. F.; Cohen, E.; Fish, R. D.; Thomas,  S. W.; Cobb, E. S.; Barsky, B. A.; 
Schweitzer, D. L.; and Lane, J. M. "Using the Oslo Algorithm as a Basis for CAD/CAM Geometric Modelling". 
In NCGA '81 Conference Proceedings, National Computer Graphics Association, Inc., June 1981, pp. 345-356. 
  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801290</article_id>
		<sort_key>273</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>39</seq_no>
		<title><![CDATA[A generalization of algebraic surface drawing]]></title>
		<page_from>273</page_from>
		<doi_number>10.1145/800064.801290</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801290</url>
		<abstract>
			<par><![CDATA[<p>The mathematical description of three dimensional surfaces usually falls in one of two classifications: parametric and algebraic. The form is defined as all points which satisfy some equation: F(x,y,z)&equal;0. This form is ideally suited for image space shaded picture drawing, the pixel coordinates are substituted for x and y and the equation is solved for z. Algorithms for drawing such objects have been developed primarily for first and second order polynomial functions. This paper presents a new algorithm applicable to other functional forms, in particular to the summation of several gaussian density distributions. The algorithm was created to model electron density maps of molecular structures but can be used for other artistically interesting shapes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Curve, surface, solid, and object representations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010399</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Parametric curve and surface models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396.10010401</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling->Volumetric models</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P131722</person_id>
				<author_profile_id><![CDATA[81100294395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Blinn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Jet Propulsion Laboratory, California Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 16, Number 3 July 1982 A Generalization of Algebraic Surface Drawing James 
F. Blinn Jet Propulsion Laboratory Calfornia Institute of Technology Abstract The mathematical description 
of three dimensional surfaces usu- ally falls in one of two classifications: parametric and algebraic. 
The form is defined as all points which satisfy some equation: F(x,y,z)=0. This form is ideally suited 
for image space shaded pic- ture drawing, the pixel coordinates are substituted for x and y and the equation 
is solved for z. Algorithms for drawing such objects have been developed primarily for first and second 
order polynomial func- tions. This paper presents a new algorithm applicable to other func- tional forms, 
in particular to the summation of several gaussian den- sity distributions. The algorithm was created 
to model electron den- sity maps of molecular structures but can be used for other artisti- cally interesting 
shapes. * This paper has been selected for publication in the July 1982 issue of Transactions on Graphics. 
 273 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801291</article_id>
		<sort_key>275</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>40</seq_no>
		<title><![CDATA[Business graphics(Panel Session)]]></title>
		<subtitle><![CDATA[What is it?]]></subtitle>
		<page_from>275</page_from>
		<doi_number>10.1145/800064.801291</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801291</url>
		<abstract>
			<par><![CDATA[<p>Much has been written about the explosive growth occurring and forecast for business graphics. Users and vendors alike are excited by these prospects. But, what is business graphics? Is it narrowly confined to slide making? Some with a broad view have suggested it is an extension of a management information system. Users search through journals and marketing literature eager to prove a salesman's claim that &#8220;a pretty color picture is really worth a thousand words&#8221; or at least several pages of numbers in a computer printout. Vendors seek the clues to developing a product strategy that will allow them to cash in on the forecasted shipments. One study [1] indicated 58,000 graphic terminals will be used in business graphics applications in 1986 sales, including computer graphics&#8212;capable personal computers, and dedicated computer graphics systems.</p> <p>This panel will present the thoughts and views of four business graphics practitioners&#8212;one associated with a large computer company, another a user from a large industrial concern, the third associated with an independent software company and the last, an industry consultant.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>J.1</cat_node>
				<descriptor>Business</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010406</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010406.10010412</concept_id>
				<concept_desc>CCS->Applied computing->Enterprise computing->Business process management</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P329789</person_id>
				<author_profile_id><![CDATA[81332513659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Luther]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lexidata Corporation]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P330877</person_id>
				<author_profile_id><![CDATA[81100336576]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Irwin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jarett]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Irwin M. Jarett, CPA, Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31092672</person_id>
				<author_profile_id><![CDATA[81100588657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jack]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Russell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[General Motors Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39073930</person_id>
				<author_profile_id><![CDATA[81332507284]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Howard]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Johnson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Digital Equipment Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14154260</person_id>
				<author_profile_id><![CDATA[81332531514]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thompson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Precision Visuals, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[The CRT Graphics Terminal Industry: A Strategic Analysis, 1981-1986. Venture Development Corporation, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Computer Graphics. The Yankee Group, October 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL: Business Graphics: What is it? CHAIR: David Luther Lexidata Corporation Much has been written 
about the explosive growth occurring and fore- cast for business graphics. Users and vendors alike are 
excited by these prospects. But, what is business graphics? Is it narrowly confined to slide making? 
Some with a broad view have suggested it is an extension of a management information system. Users search 
through journals and mark- eting literature eager to prove a salesman's claim that "a pretty color picture 
is really worth a thousand words" or at least several pages of numbers in a computer printout. Vendors 
seek the clues to developing a product strategy that will allow them to cash in on the forecasted ship- 
ments. One study [i] indicated 58,000 graphic terminals will be used in business graphics applications 
in 1986 sales, including computer graphics --capable personal computers, and dedicated computer graphics 
systems. If business graphics users are actively searching for information management solutions like 
business computer graphics and vendors are anxi- ous to supply it, then they better get together and 
decide what it is one side is selling and the other is buying. Graphic specialty companies currently 
dominate the computer graphics hardware market in shipments, dollars, and state-of-the-art products. 
To a large extent, these "pro- ducts" have kindled the business graphics fire, i.e., the ready availabil- 
ity of low-cost, color raster systems. The presence of general purpose data charting software and good 
promotion has helped. However, for a long time it appeared that the vendor community was doing more pushing 
rather than being pulled by customers. Recently, this has shifted. User demand has accelerated rapidly. 
The "business graphics" tag has become so popu- lar it is associated with every new product offering--from 
large mainframe computers to independent software packages to personal computers. What of the personal 
computer graphics system, the entrance of the large general systems vendor, the merger of business graphics 
and office automation, the need for a totally integrated data capturing, data manage- ment and data presentation 
system or what about "standards" for how infor- mation is stored and displayed? How do these topics fit 
into or formulate the definition of business graphics? This panel will present the thoughts and views 
of four business graphics practitioners--one associated with a large computer company, another a user 
from a large industrial concern, the third associated with an independent software company and the last, 
an industry consultant. i. The CRT Graphics Terminal Industry: A Strategic Analysis, 1981-1986. Venture 
Development Corporation, 1981. 2. Computer Graphics. The Yankee Group, October 1981. PANELISTS: Irwin 
Jarett Howard Johnson Irwin M. Jarett, CPA, Ltd. Digital Equipment Corporation Jack Russell John Thompson 
General Motors Corporation Precision Visuals, Inc. 275 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801292</article_id>
		<sort_key>277</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>41</seq_no>
		<title><![CDATA[The message is the medium]]></title>
		<subtitle><![CDATA[Multiprocess structuring of an interactive paint program]]></subtitle>
		<page_from>277</page_from>
		<page_to>287</page_to>
		<doi_number>10.1145/800064.801292</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801292</url>
		<abstract>
			<par><![CDATA[<p>An innovative design for an interactive paint program has been developed based on multiple processes and message passing. Traditional paint programs combine interrupt-driven support of a graphical input device, such as a mouse or tablet, with the coloring of pixels in a raster display. We advocate a different design methodology which is illustrated in our implementation. The multiple processes and message passing primitives provided by some real-time operating systems encourage the design of parallel-program architectures and anthropomorphic programming structures, analogous to artist procedures and the metaphors of Smalltalk.</p> <p>The Thoth operating system was used to experiment with such an anthropomorphic design. Thoth provides a hospitable environment in which to investigate the distribution of algorithms between software and microprogrammed hardware processes, the performance and responsiveness of a multiple-process interactive program, and experimental user interfaces using an Ikonas 3000 frame buffer.</p> <p>The paint program consists of processes which handle the graphics tablet, track an iconic cursor, paint a selection of brushes, fill regions of the image, draw lines, and implement the user interface. Some processes have been implemented both in software and microcode.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Administrator]]></kw>
			<kw><![CDATA[Agent]]></kw>
			<kw><![CDATA[Anthropomorphic programming]]></kw>
			<kw><![CDATA[Message passing]]></kw>
			<kw><![CDATA[Overseer]]></kw>
			<kw><![CDATA[Paint program]]></kw>
			<kw><![CDATA[Secretary]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Paint systems</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>D.4.4</cat_node>
				<descriptor>Message sending</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>D.4.1</cat_node>
				<descriptor>Multiprocessing/multiprogramming/multitasking</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010949.10010957.10010959</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Operating systems->Process management->Multiprocessing / multiprogramming / multitasking</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10011007.10010940.10010941.10010949.10010965.10010968</concept_id>
				<concept_desc>CCS->Software and its engineering->Software organization and properties->Contextual software domains->Operating systems->Communications management->Message passing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P241413</person_id>
				<author_profile_id><![CDATA[81407594230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Richard]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Beach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P142871</person_id>
				<author_profile_id><![CDATA[81100324790]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Beatty]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP14069878</person_id>
				<author_profile_id><![CDATA[81100170154]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kellogg]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Booth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329724</person_id>
				<author_profile_id><![CDATA[81100298953]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Darlene]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Plebon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Laboratory, Department of Computer Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39031557</person_id>
				<author_profile_id><![CDATA[81100188679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Eugene]]></first_name>
				<middle_name><![CDATA[L.]]></middle_name>
				<last_name><![CDATA[Fiume]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Systems Research Group, Department of Computer Science, University of Toronto, Toronto, Ontario, Canada, M5S 1A7]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Beach, R.J., Beatty, J.C., Booth, K.S., White, A.R., "Documentation Graphics at the University of Waterloo," International Conference on Research and Trends in Document Preparation Systems, Swiss Institute of Technology, Lausanne, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Beatty, J.C., Booth, K.S., Matthies, L.H., "Watkins Algorithm Revisited," CMCCS, Waterloo, 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Booth, K.S., and Gentleman, W.M., "Anthropomorphic Programming," Conference on Issues for Large Scale Computing, Salishan Lodge, Oregon, March 1982.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Booth, K.S., and MacKay, S.A., "Techniques for Frame Buffer Animation," Graphics Interface '82 Conference Proceedings, Toronto, 1982.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909544</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Cargill, T.A., "A View of Source Text for Diversely Configurable Software," PhD thesis, University of Waterloo, 1979.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909190</ref_obj_id>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Cheriton, D.R., "Multi-process Structuring and the Thoth Operating System," PhD thesis, University of Waterloo, 1979.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>359074</ref_obj_id>
				<ref_obj_pid>359060</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Cheriton, D.R., Malcolm, M.A., Melen, L.S., and Sager, G.R., "Thoth, a Portable Real-time Operating System," CACM, Vol 22, No 2, 1979.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Dyment, Doug, "A Corkscrew for the Software Bottleneck," Micros 1:2 (October 1980) pp 21-24.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>806794</ref_obj_id>
				<ref_obj_pid>965161</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Evans, K.B., Tanner, P.P., and Wein, M., "Tablet Based Valuators that Provide One, Two, or Three Degrees of Freedom," Computer Graphics, Vol 15, No 3, Aug. 1981.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Gentleman, W.M., "Message Passing Between Sequential Processes: the Reply Primitive and Administrator Concept," Software-Practice and Experience, Vol 11, pp. 435-466, 1981.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Goldberg, A., and Ingalls, D.H.H., "The Smalltalk-80 System," BYTE, Vol 6, No 8, Aug. 1981.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Gurd, R.P., "A Tiny C Compiler for a Bit-Sliced Microprocessor," Master's thesis, University of Waterloo (in preparation), 1982.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807416</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Kahn, K, and Hewitt, C., "Dynamic Graphics Using Quasi Parallelism," Computer Graphics, Vol 12, No 3, Aug. 1978.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Kay, A., Goldberg, A., "Personal Dynamic Media," Computer, IEEE, Vol 10, No 3, Mar. 1977.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Levoy, M., "Computer Animation Tutorial Notes," SIGGRAPH '81, 1981.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Malcolm, M.A., et al., Zed Reference Manual, Thoth Computer Research Foundation, University of Waterloo, 1980.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_obj_id>801293</ref_obj_id>
				<ref_obj_pid>965145</ref_obj_pid>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C.W., "Computer Animation with Scripts and Actors," Computer Graphics, this issue.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R,, "Paint," Tech. Memo. No. 7, Computer Graphics Lab, NYIT, Old Westbury, NY, July 1978.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807456</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Smith, A.R., "Tint Fill," Computer Graphics, Vol 13, No 2, Aug. 1979.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>965103</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Shoup, R.G., "Color Table Animation," Computer Graphics, Vol 13, No 2, Aug. 1979.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Tilbrook, D.M, "A Newspaper Pagination System," M.Sc. thesis, Department of Computer Science, University of Toronto, 1976.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Message is the Medium: Multiprocess Structuring of an Interactive Paint Program Richard J. Beach 
John C. Beatty Kellogg S. Booth  Darlene A. Plebon Eugene L. Fiume Computer Graphics Laboratory Computer 
Systems Research Group Department of Computer Science Department of Computer Science University of Waterloo 
University of Toronto Waterloo, Ontario, Canada N2L 3G1 Toronto, Ontario, Canada, M5S 1A7 (519) 886-1351 
(416) 978-2011 Abstract An innovative design for an interactive paint program has been developed based 
on multiple processes and message passing. Traditional paint programs combine interrupt-driven support 
of a graphical input device, such as a mouse or tablet, with the coloring of pixels in a raster display. 
We advocate a different design methodology which is illustrated in our implementation. The multiple processes 
and message passing primitives provided by some real-time operating systems encourage the design of parallel-program 
architectures and anthropomorphic programming structures, analogous to artist procedures and the metaphors 
of Smalltalk. The Thoth operating system was used to experiment with such an anthropomorphic design. 
Thoth provides a hospitable environment in which to investigate the distribu- tion of algorithms between 
software and microprogrammed hardware processes, the performance and responsiveness of a multiple-process 
interactive program, and experimental user interfaces using an Ikonas 3000 frame buffer. The paint program 
consists of processes which handle the graphics tablet, track an iconic cursor, paint a selection of 
brushes, fill regions of the image, draw lines, and imple- ment the user interface. Some processes have 
been imple- mented both in software and microcode. Permission to copy without fee all or p~irt of this 
material is granted provided that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, and notice is given that 
copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, 
requires a fee and/or specific permission. (~) 1982 ACM 0-89791-076-1/82/007/0277 $00.75 CR Categories 
and Subject Descriptors: D.2.2 [Software Engineering]: Tools and Techniques -- user interfaces; D.2.6 
[Software Engineering]: Programming Environments; D.3.3 [Programming Languages]: Language Constructs 
--concurrent programming structures; D.4.1 [Operating Systems]: Process Management --concurrency, deadlocks, 
multiprocessing / multiprogramming, scheduling, synchron- ization; 1.3.2 [Computer Graphics]: Graphics 
Systems- distributed / network graphics; 1.3.6 [Computer Graphics]: Methodology and Techniques --ergonomics, 
interaction techniques. General Terms: Algorithms, Design, Human Factors. Additional Key Words and Phrases: 
administrator, agent, an- thropomorphic programming, message passing, overseer, paint program, secretary. 
 Introduction An interactive paint program has been built using multi- ple processes and message passing. 
The program has an an- thropomorphic des!gn, assigning independent processes to roles which might otherwise 
be considered in a human context. This design contrasts with traditional paint program implementations 
using sequential and interrupt-driven procedures. In our experience, the multiple-process implementation 
does not degrade responsiveness. To the contrary, process priorities may be altered to fine tune the 
program's performance, providing added flexibility. An anthropomorphic design clarifies the interaction 
of processes, promoting better understanding of a complex program. We have found the resulting program 
to be more reliable and the design more amenable to enhancement than with more conventional methodologies. 
A goal of the multiple-process design is the eventual migration of parts of the program to microcode. 
The decomposition into processes leads easily to microprogram modules. The paint program has proved to 
be an effective tool for creating pictures, an absorbing demon-  Computer Graphics Volume 16, Number 
3 July 1982 stration program for visitors to the Computer Graphics Laboratory, and a testbed for research 
on user interface design and Telidon information-provider workstations. Our experience has been that 
graphics programs are easier to write, debug, and modify when structured as multi- ple processes. Anthropomorphism 
adds a clear understand- ing of this structuring to further simplify the task of system design.  Traditional 
Paint Programs Several paint programs have been described in the literature [18,20,15]. There is little 
discussion in these papers of program structure, but the following general observations have been gleaned 
from conversations with their designers. Most paint programs use interrupt-driven procedures for the 
painting algorithms, and use general looping-structure to repetitively test for user input actions. Quite 
often, overly "clever" code has been used to provide a more efficient implementation of the algorithms. 
General features of paint programs are described in the notes prepared by Smith [18]. Some pointing device, 
often a mouse or tablet, is used to locate the paint brush on the screen. When a button on the mouse 
or tablet is pressed the program modifies image pixels with the brush pattern. Several painting techniques 
can be implemented, such as rubber-stamping copies of the brush, inking the brush pattern along the path 
traced by the pointing device, or simulating an air brush by tinting pixels in a varied pattern about 
the brush. The user interface for paint programs frequently relies on an auxiliary screen upon which 
menu items may be shown, and selected, and a keyboard for entering names of brushes, picture filenames, 
and sometimes menu actions. The goal of our work was to implement an interactive paint program with similar 
features, but using a very different underlying program structure. Message-Based Operating Systems The 
design of multiple-process programs is influenced greatly by the architecture of the operating system 
upon which they are built [10]. With inexpensive primitives for creating and communicating among many 
independent processes, different views of program design are possible and practical. The following section 
describes the message-based operating system used in this project. An awareness of the process and message-passing 
primitives is necessary to appreciate their influence on the design of multiple-process programs. Thoth 
Operating System The portable Thoth real-time operating system [6,7] was developed at the University 
of Waterloo and serves as the development system for our work. Thoth was designed to meet real-time constraints, 
such as those of an interactive graphics program, and to be portable across a variety of mini- computer 
systems. It has been ported to the Texas Instru- ments TI990, Data General Nova, Modcomp IV, Honeywell 
Level 6, and Advanced Micro Devices AM16 systems. Por-tability was achieved by using a high-level BCPL-derivative 
implementation language, Zed [16], and a portable compiler and software development system [5]. Our multiple-process 
paint program was programmed in Zed on the Honeywell Level 6 system. Several features of Thoth promote 
the design of programs with many small processes. Process primitives are relatively cheap; the cost of 
a message communication (send- receive-reply) between two processes is about one millisecond. Since process 
management in Thoth is inexpensive, processes often have short life-times, being created and destroyed 
as needed. A group of cooperating processes is called a team and shares memory. Processes are executed 
in pseudo-paralM by the scheduler. Process communication and synchronization is accomplished entirely 
by sending and receiving messages. There are no semaphores or monitors in Thoth. It is important to note 
that although processes in a team do share memory, the memory is never used for synchronization. In fact, 
all synchronization of memory access is accomplished using message passing. The Thoth process scheduler 
provides a priority scheme for process execution. If all processes are created with identical priorities, 
then natural-break scheduling will occur. This permits the process designer to determine how processes 
will voluntarily relinquish the processor. All processes will execute until they block, normally due 
to a message communication primitive (which includes all input/output requests). If processes are allocated 
different priorities, then events which unblock processes will cause the highest priori- ty unblocked 
process to be executed. Response time is guaranteed in interactive programs by arranging that processes 
which control interactive devices have the highest priority, followed by user interface processes, and 
finally those processes executing algorithms such as area fill which may require large amounts of computation. 
 Graphics Hardware The paint program displays its images on an Ikonas 3000 frame buffer, which has 512 
x 512 resolution with 32-bit pixels. The Ikonas system includes four color lookup tables, a cross-bar 
switch for selectively routing bit planes to the color map, a direct memory access interface to the host 
computer, and a bit-slice microprocessor. The microprocessor can manipulate the frame buffer image at 
very high speed. It communicates with the host computer via a scratchpad memory. A "Tiny C" compiler 
for the Iko- nas microprocessor has been written at Waterloo [12]. Several microprocessor algorithms 
have been written in Tiny C including dynamic iconic tracker and line drawing algo- rithms. Graphics 
input is provided by a Summagraphics Bitpad One tablet. The tablet is operated with either a pen-like 
stylus or a four-button puck. Novice users quickly adapt to using the pen stylus, although almost always 
switch to the puck to avoid several physical complaints with the stylus The Administrator Process Concept 
cord and jitter. People with a fine arts background express a preference for the pen-like stylus, especially 
for free-hand sketching.  Anthropomorphic Design Several previous projects, particularly in Artificial 
Intelligence, have used multiple processes endowed with hu- man characteristics. Artist procedures in 
Kahn's Director program [13] provide 'a model of procedures which are told what to do, and then expected 
to perform those actions independently. The Smalltalk message-object programming system [14,11] provides 
a similar abstraction, in which an ac- tion that is described continues until instructed to change. A 
more recent use of this technique is reported by Reynolds in these proceedings [17]. This anthropomorphic 
design technique [3,8] provides a context in which independent program pieces can be built and interconnected 
in ways similar to the job classifications and reporting roles of a large organization. This analogy 
ex- tends to multiple-process program design by assigning to processes roles which they can perform, 
and by creating communication channels between these processes. The concepts of administrators, workers, 
secretaries, agents, and overseers provide role models for the processes in our paint program. An administrator 
process maintains a resource and relies on worker processes to manipulate that resource [6,10]. The worker 
process performs a computation and sends the results to its administrator process. In our paint program, 
a tablet administrator looks after the graphics tablet. Other processes may request the current tablet 
position, or ask to be notified the next time the tablet button is depressed. An administrator process 
receives messages from its worker processes and from other client processes which request information 
about the resource it administers. The general coding structure of an administrator is an infinite loop, 
as shown in Example 1. Figure 1 shows the organizational chart for an adminis- trator process and its 
relationship to both its worker processes and its client processes. It is important that administrator 
processes not be send-blocked, so that they may respond to requests as soon as they occur. In an organizational 
chart, this is evident because an administrator has only incoming arrows, representing received messages. 
The Overseer Process Concept An overseer process performs two functions. It relieves a worker process 
from the distractions of interruptions, and it Administrator() { repeat { requestor_id = .Receive( message 
); if ( message[TYPE] ~= FROM CLIENT ) { if ( info available ) Supply info and reply( message, requestor_id 
); else Queue_reply for later( message, requestor_id ); } else if ( message[TYPE] == FROM_WORKER ) { 
 Update_info( message ); .Reply( message, requestor_id ); if ( reply_queued ) Supply info and reply( 
message, remembered_id ); } } }  Example 1 Client= Worker I  Administrator Worker2  Worker.  
Figure 1. The organizational chart for an administrator process with several work- er processes and several 
client processes. An arrow is drawn between two processes to represent the direction messages are sent. 
acts as a watchdog in case the worker process runs out of control. The worker process is programmed simply 
to perform some task, such as filling a region of the painted im- age. The overseer process executes 
concurrently looking for any change in the job description for the worker process. In the paint program, 
the area fill process is aborted when an overseer process recognizes a panic signal from the user. Figure 
2 illustrates the process structure for an originat- ing process with the worker and overseer processes 
which it creates. Although only the messages passed between these processes are shown, the worker and 
overseer may interact with other processes to perform their functions. Design of the Paint Program Painting 
programs are complex. They combine computer graphics algorithms with real-time interaction. Several concurrent 
activities are exhibited: tracking the paint brush, providing timely user feedback, painting (which may 
lag behind the user), and user control of these actions. The paint program uses several process abstractions: 
hardware and resource administration processes, algorithmic worker processes, and user interface control 
processes. Administrator processes support the graphics input tablet, the auxiliary terminal screen, 
and the terminal keyboard. Since several processes will eventually require access to these administrator 
processes, they are designed to perform simple and well-defined actions. The message communication primitives 
provide synchronization between the parallel execution of all these processes. Thus, when several processes 
wish to request tablet coordinates, the tablet administrator sees only a sequence of requests. Worker 
processes implement the various painting Figure 2. The organizational chart for an overseer process 
and its associated worker process. Computer Graphics Volume 16, Number 3 July 1982 techniques, such as 
free-form brushing, rubber stamping, and connect-the-dots with straight lines. Area fill is also imple- 
mented in a separate process, an organization which simplifies the algorithm substantially. There is 
no need to check periodically for user actions as this is performed by an overseer or administrator, 
at the expense of creating another process. This relies upon process creation and destruction primitives 
being cheap to use. User interface processes control the actions of the paint program by creating transient 
processes to perform actions, such as filling a region, and by sending messages to awaiting worker processes, 
such as for drawing a line. The interaction sequence in the paint program becomes a question of inter- 
process communication. Processes which perform user-directed actions can be transient, continuously running, 
or blocked waiting for a work order. User feedback, such as tracking the tablet with a cursor, is an 
example of a continu- ously running process.   Graphics~ Tablet Frame Buffez~ Cursor TTY Screen 
 Keyboard Frame Buffer Display Memory Figure 3. Tablet ,Secretar' Cursor Tracker The organizational 
chart in Figure 3 represents an overview of the entire paint program, with processes identified as nodes, 
and arrows connecting two processes when one sends a message to the other. Hardware devices can be represented 
as fictitious processes that are sinks for messages. They are considered as processes which reply to 
requests sent by software processes. Tablet Administrator Process The interactive painting program requires 
input from a graphics pointing device [21,9], indicating where the user has positioned the paint brush. 
Several uses are made of the pointing device: painting pixels, selecting menu items, speci- fying the 
starting point for an area fill, and aborting the fill algorithm. As mentioned previously, a tablet administrator 
handles the graphics tablet. A tablet secretary is created to filter tablet coordinates to eliminate 
redundant coordinates and jitter (small deflections in the coordinate values due to Tablet Administrator 
 TTY FIll Tracker Overseer Paint User Interface Area Fill Keyboard Brushing[  ) ( ) ]C ''o" Listener 
, Drawing Draw Brush  The organizational chart for the multiple-process paint program. Arrows represent 
the directions messages are sent. 281  Computer Graphics Volume 16, Number 3 July 1982 the sensitivity 
of the tablet rather than motion of the tablet stylus). The tablet administrator accepts several message 
types: update the current coordinates from the tablet secretary, request the current tablet coordinates, 
request the next tablet coordinates which are different from the present position, request the coordinates 
for the next tablet hit when the stylus or button is pressed, and change the tablet sampling mode. The 
pseudocode in Example 2 illustrates the inter-process communication of the tablet administrator. The 
Cursor Tracker Process An important user feedback is the tracker which echos the motions of the pointing 
device. In order to guarantee responsive behavior, a free-running process is created to track the tablet 
by positioning a screen cursor. This tracking process will be created whenever the user is permitted 
to paint on the screen. At other times, for example, when an picture is being saved or restored, the 
tracking process is not needed and is destroyed. The tracker process is a simple loop, as shown in Example 
3. The start-up parameter Tablet_administrator identifies the process which can provide the current tablet 
coordinates upon request. The request for dhanged coordinates ensures that as soon as the pointing device 
is repositioned, the tracking cursor will also be repositioned. The Draw Brush Process A basic action 
of the paint program is to copy an arbitrarily shaped and colored brush into the frame buffer display. 
A separate process to perform this drawing action has been implemented for two reasons. First, it was 
intended that this process would migrate from the host computer to the microprocessor in the frame buffer, 
and that a message- based protocol would enable the two processors to interact reliably. Second, it was 
realized that for complex brushes, the painting algorithm might be significantly slower than the hand 
motion of the user. When the user was ahead of the Tablet_administrator() { repeat { requestor_id = 
.Receive( message ); if ( message[TYPE] == UPDATE_COORDINATES ) {  Update_info( message ); .Reply( 
message, requestor_id ); if ( request_queued &#38;&#38; tablet_hit_occurred ) Supply_info_and_reply( 
message, remembered_id ); }  else if ( message[TYPE] == REQUEST_COORDINATES ) Supply_info_and_reply( 
message, requestor_id ); else if ( message[TYPE] == WAIT_FOR HIT ) Queue_request_for_lat er( message, 
requestor_id ); } Example 2 Cursor tracker( Tablet administrator ) { repeat { message[TYPE] = REQUEST_FOR_CHANGED_COORDINATES; 
.Send( message, Tablet_administrator ); Position_cursor( message[X], message[Y] ); } } Example 3 painting 
algorithm, we needed a scheme for painting that would gracefully lag behind eventually catching up. Provid-ing 
an independent process to draw the brush permits the paint program to display a brush periodically while 
tracking the user, building up a queue of coordinates for the brushing process to join together as quickly 
as it can. The draw brush process accepts two forms of messages, one to establish the brush shape and 
color, and the other to indicate the position where it should draw. This process is implemented as a 
server process which receives work order messages. Example 4 shows pseudocode for the draw brush process. 
Since the brush shape definition may be larger than the Thoth eight-word message buffer, a pointer into 
team memory is passed in the message. Note that no critical race condition is introduced since messages 
synchronize processes; the message is not sent until the definition is ready, and the pointer is not 
used until the message is received. The Brushing/Line Drawing Processes The brushing and line drawing 
processes are very similar. The brushing process tracks the pointing device as long as the button is 
depressed, periodically sampling its posi- tion. The line drawing process connects "hits" with straight 
lines. Fill Algorithm Process The area fill process is a straightforward adaptation of the basic fill 
algorithm described by Smith [19]. His paper presents the algorithmic details in a much simpler form 
than he actually implemented. Presumably this is because the details necessary to properly synchronize 
all of the actions in a complete program would overwhelm the reader. The multiple process approach avoided 
any complications in han- dling concurrent activities and thus we could implement the published algorithm 
directly, vastly improving our under-standing of the program. An important concern is user control of 
the algorithm when it begins to "bleed," or fill the region outside the expected area. An abort capability 
is needed. Rather than have the fill algorithm periodically check for an abort request, the multiple-process 
program structure provides an elegant alternative using an overseer process. An overseer process is created 
along with the fill process when a fill is begun. The fill overseer is a very simple process which requests 
the next tablet hit and sends a message to the user interface process. The overseer blocks awaiting the 
next tablet hit before sending a message. The area fill process executes until completion before sending 
a message. These two processes are in a race: the first one to complete sends a message to the user interface, 
which is blocked awaiting a message from either process. When a message arrives from either process, 
both processes are destroyed. Either the area was filled completely or the operation was aborted, but 
it makes no difference to the user interface. The usual problem of race conditions do not exist. The 
User Interface Process The user interface implemented in the paint program presently uses both the frame 
buffer display and an auxiliary screen for menu information. Since several concurrent processes may wish 
to interact with the auxiliary screen, an administrator process is needed to synchronize message display. 
Otherwise, as happened during development, messages can be arbitrarily interleaved as two concurrent 
processes execute in parallel. The interleaved messages can be extremely difficult to interpret! When 
restoring pictures from files, especially during de- monstrations, it is convenient to display a menu 
list of avail- able pictures and use the pointing device to select one. At other times, a new picture 
name is to be entered from the keyboard. Programming both sources of input is difficult on most operating 
systems because each input request causes Draw_brush() { repeat { .Reply( message, .Receive( message 
) ); if ( message[TYPE] == NEW BRUSH ) { Update_brush_shape( message[BRUSH_DEFINITION] ); } else { 
 Display_brush( message[X], message[Y] ); } } }  Example 4 the program to wait for the input. In a multiple-process 
im- plementation, two processes can be created to request the input selection, and the first to succeed 
can send the selection to the user interface, which is waiting for either process to respond. Thus, a 
natural user interface feature can be pro- vided simply and elegantly using small processes with short 
life-times. The processes needed for this feature are a key- board listener and a cursor tracker for 
the auxiliary screen. Impact on the User Interface The major impacts of a multiple-process design on 
the user interface are the effects on responsiveness and user feed- back. Responsiveness of Multiple 
Processes With several processes executing in parallel on a single central processor, it is imperative 
that appropriate process scheduling guarantee real-time interaction. Two scheduling methods have been 
used to experiment with apparent interac- tion times. The first is a natural-break scheme, in which a 
process continues execution until it blocks awaiting some event. The second is a priority scheme, in 
which each process is assigned a process priority and whenever a process is unblocked, the highest-priority 
unblocked process is executed. The natural break scheme requires careful consideration of the time necessary 
to complete an algorithm. For exam- ple, the area fill algorithm would continue execution until the area 
was completely filled, thwarting all attempts to abort the fill process! Thus, breaks are created to 
provide a voluntary relinquishing of the processor. Normally, such break points occur at the end of an 
iteration or at other significant points in an algorithm. In the Thoth operating system, scheduling decisions 
are made when a process blocks on a message primitive. A voluntary relinquish can be im- plemented as 
a message send. A sponge process is used to accept these messages as efficiently as possible. The natural 
break process is simply a repeat-forever receive-reply loop. Example 5 shows pseudocode for this process. 
Natural break is sufficient for simple algorithms, such as area fill, but becomes both an annoying overhead 
and sometimes imprac- tical scheme for more complex algorithms such as tint fill. In contrast, the process 
priority scheduling scheme requires that a priority ranking be imposed on processes. Natural_break() 
The highest priority processes will execute, when ready for execution, before lower priority processes. 
Thus, processes responsible for user interaction are assigned the highest priorities. { repeat { .Reply( 
message, } } In the paint program, the highest priorities are assigned to the cursor tracking processes, 
middle priorities are assigned to the user interface processes, and lowest priorities are assigned to 
the algorithm processes. This assignment guarantees that user actions to manipulate the input devices 
are responded to as quickly as the real-time scheduler permits. Decisions made by the user interface 
are carried out as soon as all feedback is complete. When no user interactions are pending, algorithm 
processes can execute. The disadvantage with this solution is that a consistent set of priorities must 
be assigned the various processes. User Control of the Paint Program The multiple-process implementation 
of the paint program provides for an elegant and simple way of control- ling painting actions. Implementing 
the fill overseer as an independent process relieves the area fill process of-any concern for the method 
of aborting the process. The overseer is a very simple program which expresses the idea of a human overseer 
waiting to respond to a problem. The benefit to the program designer is that natural and effective user 
interface actions can now be implemented in a natural and elegant way. Multiple Input Sources for the 
User Interface The use of multiple processes to handle multiple inputs is another example of the simplifying 
benefits of such a design. The selection of picture names by pointing or entry on the keyboard is modelled 
by a straightforward process for each form of input. The synchronization of message communication resolves 
which action occurred first, and the dynamic creation and destruction of processes permits input sources 
to be enabled and disabled as appropriate.  Migration of Processes to Firmware A significant goal of 
the multiple-process paint program was to create a design which permitted the migration of processes 
from the host computer to a microprocessor embedded within the frame buffer. .Receive( message ) ); 
 Example 5 The first process to be moved into a microprocessor was the tablet secretary. The graphics 
tablet is connected to the host computer via a Honeywell MLCP interface, which uses a special purpose 
microprocessor to implement various termi- nal protocols. The tablet communicates by means of a simple 
asynchronous ASCII protocol. The filtering algorithm performed by the tablet secretary was implement- 
ed in the MLCP so that only "good" coordinates would be sent to the host computer and the tablet administrator. 
This reduces the number of messa.ges by allowing only noticeable motions of the tablet stylus or puck 
to be transmitted. The cursor and draw brush processes have migrated into the frame buffer microprocessor, 
as microprograms written in Tiny C. Messages which contain pointers to brush defini- tions and positioning 
coordinates are implemented as data transfers to the scratchpad area accessible to both the host and 
microprocessor. Following our anthropomorphic design methodology, a forwarding agent process remains 
on the host to accept Thoth messages and coordinate the synchronization of the data transfers to a receiving 
agent within the microprocessor. Real parallelism now exists between the processes within the host and 
those executing independently as the tablet secretary, cursor and draw brush processes within the two 
microprocessors. The area fill process is the next candidate for migration into microcode. The same overseer 
concept used with multi- ple processes will be used to oversee operation of the microprocessor. Thus, 
we are confident that the anthropo- morphic design methodology provides a reliable control and synchronization 
scheme for multiple processes executing in multiple processors. Extrapolated Designs Based on Multiple 
Processes The use of independent processes makes it natural to think of performing actions in parallel. 
The artist procedure paradigm helps to design processes which perform a role until the process is told 
otherwise. Airbrushing is a natural application of this idea. Airbrushing is a painting technique that 
causes pixels to be tinted in a seemingly random pattern near the brush posi- tion. A process to perform 
airbrushing might be created when the tablet stylus is depressed. The current coordinates of the airbrush 
can be moved by the tracking process. While executing, the airbrush process will continue to increase 
the tint of pixels. When the tablet stylus is lifted, the airbrush process would be destroyed. Color 
table animation is a technique to provide dynamic images on a fairly static display such as a frame buffer. 
The animation process involves modifying over time the contents of the color lookup table. By arranging 
that areas of the im- age have unique color table entries, and then changing those entries from background 
to a highlight color, or by sequenc- ing through several color values, apparent motion can be created. 
Controlling the timing of such changes is another application for an independent process. After the color 
table has been defined and the modification specified, a color table animator process can be created 
to perform the modifications [4]. When the animation is to cease the process can be destroyed. In the 
interim, various other actions can be performed without added complexity which would arise if this checking 
were accomplished within a conventional sequential program. This technique follows from the process structures 
in Dyment [8] and Gentleman [10]. A common visual workspace can be created using multiple-process techniques. 
This may be possible by supporting several input tablets or by joining several paint programs together. 
An example of the former might be a two-person game in which several tablets are controlled by copies 
of the tablet administrator. An example of the latter would be a teleconferencing application where messages 
within the paint program could be transmitted over a communication network. Conclusions The process 
structuring concepts described in this paper have made the paint program easier to write, modify, and 
debug than expected with conventional program structures. Anthropomorphism and process structure diagrams 
reveal the parallelism in interactive graphics programs. We are us- ing these concepts to implement other 
graphics programs, especially where multiple processors can be exploited. Documentation graphics programs 
[1] will involve several processes for composing text, mathematical notation, tables, and various styles 
of charts and diagrams. A multiprocessor implementation of the plane sweep visible surface algorithm 
[2] will also rely on these multiple process concepts. Figure 4 illustrates some results of the paint 
program, showing the Acadia Axeman, a butterfly, a schematic of the Ikonas 3000, the Waterloo crest, 
the Canadian space shuttle paying a visit to the Arts Library (subtitled "Overdue Notice"), and a tree. 
The final two pictures show the menu and demonstrate the zoom feature. Acknowledgement Much of the research 
reported in this paper relies heavi- ly on the Thoth operating system which was designed and implemented 
by the Software Portability Group at the University of Waterloo. The help of Burt Bonkowski, Tom Cargill, 
Dave Cheriton, Morven Gentleman, Mike Malcolm, Gary Sager and Gary Stafford is particularly acknowledged. 
Paul Breslin wrote the MLCP microcode for the tablet driver and most of the initial frame buffer support 
programs. Pres-ton Gurd's compiler has been an invaluable tool for develop- ing high-level microcode. 
All of the people who acted as guinea pigs for early versions of the Paint program made it possible to 
test our ideas against actual users; their patience is appreciated. Jim Diamond, Steve Hayman, Steve 
MacKay, Bev Marshman and Julie Pakalns created the illus- trations. Our work was supported in part by 
the Natural Sciences and Engineering Council of Canada by Postgraduate Scholarships and Grants No. A3022 
and No. A4037. Preparation of this paper was assisted by the use of [21] Tilbrook, D.M, "A Newspaper 
Pagination System," USENET, ARPANET and facilities at the Xerox Palo Alto M.Sc. thesis, Department of 
Computer Science, Research Center. References [1] Beach, R.J., Beatty, J.C., Booth, K.S., White, A.R., 
"Documentation Graphics at the University of Waterloo," International Conference on Research and Trends 
in Document Preparation Systems, Swiss Institute of Technology, Lausanne, 1981. [2] Beatty, J.C., Booth, 
K.S., Matthies, L.H., "Watkins Algorithm Revisited," CMCCS, Waterloo, 1981. [3] Booth, K.S., and Gentleman, 
W.M., "Anthropo-morphic Programming," Conference on Issues for Large Scale Computing, Salishan Lodge, 
Oregon, March 1982. [4] Booth, K.S., and MacKay, S.A., "Techniques for Frame Buffer Animation," Graphics 
Interface '82 Conference Proceedings, Toronto, 1982. [5] Cargill, T.A., "A View of Source Text for Diversely 
Configurable Software," PhD thesis, University of Waterloo, 1979. [6] Cheriton, D.R., "Multi-process 
Structuring and the Thoth Operating System," PhD thesis, University of Waterloo, 1979. [7] Cheriton, 
D.R., Malcolm, M.A., Melen, L.S., and Sager, G.R., "Thoth, a Portable Real-time Operating System," CACM, 
Vol 22, No 2, 1979. [8] Dyment, Doug, "A Corkscrew for the Software Bottleneck," Micros 1:2 (October 
1980) pp 21-24. [9] Evans, K.B., Tanner, P.P., and Wein, M., "Tablet Based Valuators that Provide One, 
Two, or Three Degrees of Freedom," Computer Graphics, Vol 15, No 3, Aug. 1981. [10] Gentleman, W.M., 
"Message Passing Between Sequential Processes: the Reply Primitive and Administrator Concept," Software-Practice 
and Experience, Vol 11, pp. 435-466, 1981. [11] Goldberg, A., and Ingalls, D.H.H., "The Smalltalk- 80 
System," BYTE, Vol 6, No 8, Aug. 1981. [12] Gurd, R.P., "A Tiny C Compiler for a Bit-Sliced Microprocessor," 
Master's thesis, University of Waterloo (in preparation), 1982. [13] Kahn, K, and Hewitt, C., "Dynamic 
Graphics Using Quasi Parallelism," Computer Graphics, Vol 12, No 3, Aug. 1978. [14] Kay, A., Goldberg, 
A., "Personal Dynamic Media," Computer, IEEE, Vol 10, No 3, Mar. 1977. [15] Levoy, M., "Computer Animation 
Tutorial Notes," SIGGRAPH '81, 1981. [16] Malcolm, M.A., et al., Zed Reference Manual, Thoth Computer 
Research Foundation, University of Waterloo, 1980. [17] Reynolds, C.W., "Computer Animation with Scripts 
and Actors," Computer Graphics, this issue. [18] Smith, A.R,, "Paint," Tech. Memo. No. 7, Computer Graphics 
Lab, NYIT, Old Westbury, NY, July 1978. [19] Smith, A.R., "Tint Fill," Computer Graphics, Vol 13, No 
2, Aug. 1979. [20] Shoup, R.G., "Color Table Animation," Computer Graphics, Vol 13, No 2, Aug. 1979. 
University of Toronto, 1976.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801293</article_id>
		<sort_key>289</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>42</seq_no>
		<title><![CDATA[Computer animation with scripts and actors]]></title>
		<page_from>289</page_from>
		<page_to>296</page_to>
		<doi_number>10.1145/800064.801293</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801293</url>
		<abstract>
			<par><![CDATA[<p>A technique and philosophy for controlling computer animation is discussed. Using the Actor/Scriptor Animation System (ASAS) a sequence is described by the animator as a formal written SCRIPT, which is in fact a program in an animation/graphic language. Getting the desired animation is then equivalent to &#8220;debugging&#8221; the script. Typical images manipulated with ASAS are synthetic, 3D perspective, color, shaded images. However, the animation control techniques are independent of the underlying software and hardware of the display system, so apply to other types (still, B&amp;W, 2D, line drawing ...). Dynamic (and static) graphics are based on a set of geometric object data types and a set of geometric operators on these types. Both sets are extensible. The operators are applied to the objects under the control of modular animated program structures. These structures (called <bold>actor</bold>s) allow parallelism, independence, and optionally, synchronization, so that they can render the full range of the time sequencing of events. <bold>Actor</bold>s are the embodiment of imaginary players in a simulated movie. A type of animated number can be used to drive geometric expressions (nested geometrical operators) with dynamic parameters to produce animated objects. Ideas from programming styles used in current Artificial Intelligence research inspired the design of ASAS, which is in fact an extension to the Lisp programming environment. ASAS was developed in an academic research environment and made the transition to the &#8220;real world&#8221; of commercial motion graphics production.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Lisp]]></kw>
			<kw><![CDATA[Motion picture production]]></kw>
			<kw><![CDATA[Procedural animation languages]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.7</cat_node>
				<descriptor>Animation</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.5</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003752.10010061.10010063</concept_id>
				<concept_desc>CCS->Theory of computation->Randomness, geometry and discrete structures->Computational geometry</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP31093867</person_id>
				<author_profile_id><![CDATA[81100469355]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Reynolds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Information International Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Abelson, H. and diDessa, A. Turtle Geometry, MIT Press (Series in Artificial Intelligence), Cambridge, MA, 1981.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Austin, H., "The LOGO Primer", MIT A.I. Lab. Logo Working Paper 19.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1096495</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Church, A "The Calculi of Lambda Conversions", Annals of Mathematical Studies 6, Princeton University Press 1941, Reprinted by Klaus Reprint Co., 1965.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360354</ref_obj_id>
				<ref_obj_pid>360349</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Clark, J., "Hierarchical Geometric Models for Visible Surface Algorithms", CACM, October 1976.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1098648</ref_obj_id>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Dahl, Myhrhaug, and Nygaard The SIMULA 67 Common Base Language, Norwegian Computing Centre, Oslo, 1968.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[DeFanti, T. "The Digital Component of the Circle Graphics Habitat", Proceedings NCC 1976.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Dijkstra, E.W. "Notes on Structured Programming", August 1969]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563863</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Eastman, C. and Henrion, M. "GLIDE: A Language for Design Information Systems", SIGGRAPH '77 Proceedings, July 1977, San Jose, CA.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807365</ref_obj_id>
				<ref_obj_pid>800248</ref_obj_pid>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Futrelle, R. P. and Barta, G. "Towards the Design of an Intrinsically Graphical Language", SIGGRAPH '78 Proceedings, August 1978, Atlanta, GA.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807475</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[Goates, G., Griss, M. and Herron, G. "PICTUREBALM: A Lisp-based Graphics Language System with Flexible Syntax and Hierarchical Data Structure", SIGGRAPH '80 Proceedings, July 1980, Seattle, WA.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Goldberg, A. and Kay, A. SMALLTALK-72 Instruction Manual Learning Research Group, Xerox Palo Alto Research, March 1976.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_obj_id>512984</ref_obj_id>
				<ref_obj_pid>512976</ref_obj_pid>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Greif, I. and Hewitt, C. "Actor Semantics of PLANNER-73", Proc. of ACM SIGPLAN-SIGACT Conf., Palo Alto, CA, January 1975.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Hewitt, C. and Smith, B. "Towards a Programming Apprentice", MIT AI Lab Working Paper 90, January 1975.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_obj_id>512975</ref_obj_id>
				<ref_obj_pid>512950</ref_obj_pid>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Hewitt, C. and Atkinson, R., "Parallelism and Synchronization in Actor System", ACM Symposium on Principles of Programming Languages 4, January 1977, L. A. CA.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804727</ref_obj_id>
				<ref_obj_pid>800143</ref_obj_pid>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Jones, B. "An extended ALGOL-60 for Shaded Computer Graphics", ACM SIGPLAN/SIGGRAPH Symposium on Graphical Languages, April 1976.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_obj_id>1024278</ref_obj_id>
				<ref_obj_pid>1024273</ref_obj_pid>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Kahn, K. "An Actor-Based Computer Animation Language", Proc. of the ACM-SIGGRAPH Workshop on User-Oriented Design of Computer Graphics Systems, Pittsburg, PA, October 1976.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Kahn, K., "A Computational Theory Of Animation", MIT A.I. Lab. Working Paper 145, April 1977.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>802795</ref_obj_id>
				<ref_obj_pid>800087</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Larkin, F. "Computing with Text-Graphic Forms", Conference Record of the 1980 LISP Conference, August 1980, Stanford University.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807476</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Larkin, F. "A Structure from Manipulation for Text-Graphic Objects", SIGGRAPH '80 Proceedings, July 1980, Seattle, WA.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Moon, D. MacLisp Reference Manual, Revision 0, MIT Project MAC, December 1975.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Newman, W. and Sproull, R. Principles of Interactive Computer Graphics, McGraw-Hill, 1973 and 1979.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_obj_id>804725</ref_obj_id>
				<ref_obj_pid>800143</ref_obj_pid>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Pfister, G. "A High Level Language Extension for Creating and Controlling Dynamic Pictures", ACM SIGPLAN/SIGGRAPH Symposium on Graphical Languages, April 1976.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Preissler, M. "Multi-processed Music Synthesis", BS Thesis MIT EECS Department, May 1976.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. "A Multiprocessing Approach to Computer Animation", SB thesis, MIT EECS Department, August 1975.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. "Computer Animation in the World of Actors and Scripts", SM thesis, MIT (Architecture Machine Group), May 1978]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
			<ref>
				<ref_seq_no>26</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. Actor / Scriptor Animation System User's Manual 3.0, (ASASUM 3), Information International Inc., in preparation.]]></ref_text>
				<ref_id>26</ref_id>
			</ref>
			<ref>
				<ref_seq_no>27</ref_seq_no>
				<ref_text><![CDATA[Smoliar, S. "A Parallel Processing Model of Musical Structures" MIT AI Lab Technical Report 242, September 1971.]]></ref_text>
				<ref_id>27</ref_id>
			</ref>
			<ref>
				<ref_obj_id>889230</ref_obj_id>
				<ref_seq_no>28</ref_seq_no>
				<ref_text><![CDATA[Sussman, G. and Steele, G. "SCHEME - An Interpreter for Extended Lambda Calculus", MIT AI Lab Memo 349, December 1975.]]></ref_text>
				<ref_id>28</ref_id>
			</ref>
			<ref>
				<ref_seq_no>29</ref_seq_no>
				<ref_text><![CDATA[Teitelman, W. InterLISP Reference Manual, Xerox Palo Alto Research Center, 1978.]]></ref_text>
				<ref_id>29</ref_id>
			</ref>
			<ref>
				<ref_obj_id>540414</ref_obj_id>
				<ref_seq_no>30</ref_seq_no>
				<ref_text><![CDATA[Winograd, T. Understanding Natural Language, Academic Press, 1974.]]></ref_text>
				<ref_id>30</ref_id>
			</ref>
			<ref>
				<ref_obj_id>60990</ref_obj_id>
				<ref_seq_no>31</ref_seq_no>
				<ref_text><![CDATA[Winston, P. and Horn, B. Lisp, Addison Wesley, 1981.]]></ref_text>
				<ref_id>31</ref_id>
			</ref>
			<ref>
				<ref_seq_no>32</ref_seq_no>
				<ref_text><![CDATA[Wirth, N. "MODULA: a Language for Modular Multiprogramming", Software, Practice and Experience 7,1; 1977 pp. 3-35.]]></ref_text>
				<ref_id>32</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Animation with Scripts and Actors by Craig W. Reynolds Information International Inc. Abstract 
A technique and philosophy for controlling computer anima- tion is discussed. Using the Actor/Scriptor 
Animation System (ASAS) a sequence is described by the animator as a formal written SCRIPT, which is 
in fact a program in an anima-tion/graphic language. Getting the. desired animation is then equivalent 
to "debugging" the script. Typical images manipu- lated with ASAS are synthetic, 3D perspective, color, 
shaded images. However, the animation control techniques are inde-pendent of the underlying software 
and hardware of the display system, so apply to other types (still, B&#38;W, 2D, line drawing ...). Dynamic 
(and static) graphics are based on a set of geometric object data types and a set of geometric operators 
on these types. Both sets are extensible. The operators are applied to the objects under the control 
of modular animated program structures. These structures (called actors) allow parallelism, independence, 
and optionally, synchronization, so that they can render the full range of the time sequencing of events. 
Actors are the embodiment of imaginary players in a simulated movie. A type of animated number can be 
used to drive geometric expressions (nested geometrical operators) with dynamic parameters to produce 
animated objects. Ideas from programming styles used in current Artificial Intelligence research inspired 
the design of ASAS, which is in fact an extension to the Lisp programming environment. ASAS was developed 
in an academic research environment and made the transition to the "real world" of commercial motion 
graphics production. CR Categories and Subject Descriptors: 1.3 [Computer Graphics]; !.3.5 [CG]: Computational 
Geometry and Object Modeling; 1.3.6 [CG]: Methodology and Techniques--Languages; 1.3.7 [CG]: Three-Dimensional 
Graphics and Realism--Animation' General Terms: Design, Languages Additional Key Words and Phrases: Lisp, 
Procedural Animation Languages, Motion Picture Production Author's addresses; US Mail: III, 5933 Slauson 
Ave., Culver City, CA 90230 ARPAnet mail: Reynolds@Rand-AI Uocp network mail: ucbvax!randvax!reynolds 
 Permission to copy without fee all or part of this material is granted provided that the copies are 
not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
 publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
&#38;#169; 1982 ACM 0-89791-076-1/82/007/0289 $00.75 Introduction This paper describes the Actor/Scriptor 
Animation System (ASAS), which is a way of thinking about and describing computer graphic animation. 
ASAS is basically a notation for animated graphics. The notation for an animated sequence (the script) 
can be automatically read and converted into animated images by an ASAS interpreter. As in the case of 
musical notation being interpreted by a group of musicians--or the script of a video production being 
executed by a host of actors, camera, audio, lighting and video technicians--ASAS allows the crea-tion 
and use of any number of simulated particpants, "actors" each of which can control one or more aspects 
of the animation. The ability of ASAS actors to operate independently or (by communicating with each 
other) to act in synchronization allows a simple and unambiguous description of the function of each 
actor. ASAS differs from "performance" based real time computer graphics systems as well as from command 
or "menu" based systems. Writing the ASAS notation for an animated sequence will probably take longer 
than the final running time of the sequence. On the other hand, an ASAS script is typically more compact 
than a simple listing of the value of all relevant parameters for each frame, as might be required in 
a command- menu system. This results from the fact that ASAS is a procedural notation, a programming 
language for animation and graphics. In fact ASAS is a "full" programming language and includes all of 
the typical modern structured programming features (procedures (recursive), local variables, "if then 
else"s loops, typed data structures and generic operators). Addition- ally ASAS supports independent, 
parallel, "animated" program structures (actors), and includes a rich set of geometric and photometric 
objects and generic operators on these objects. The existence of a formal notation for a field of endeavor 
leads to a workable procedure for the development of an idea. Like an algorithm being debugged by a computer 
programmer, or a musical score being revised, an ASAS script being developed is both unambiguous and 
precisely modifiable. It is possible to change just one small aspect while keeping everything else exactly 
the same. This property of notation allows the process of progressive refinement ("tweaking") to be used 
to converge on the desired algorithm, music or animation. History ASAS was developed at the Architecture 
Machine Group at MIT as two thesis projects between 1975 and 1978 [24,24]. "ASAS 0" was not a full implementation, 
but "ASAS 1" did actually work, an equivalent definition in MacLisp would be: (define houses (group 
red-house yellow-house brown-house)) (defun thrice (x) (times x 3)) The "point of view" object (pov) 
is used to define the point of The first example could then be rewritten: view of an observer (for example 
the ASAS camera) or of an (define wheels object. That is, a pov describes the three coordinate axis basis 
(thrice tricycles)) vectors and the position of the origin of an arbitrary coordinate space. We refer 
to such spaces by names like "eye space" and Special Symbols "an object's local coordinate space". Note: 
a pov plays a role very similar to a "4X4 homogeneous transform matrix" in other 3D Within a script certain 
aspects of the production are controlled graphics systems (there is a simple transformation from a pov 
by the values given to some special symbols. None of these to a 4X4 matrix) but a pov is a geometrical 
object composed of symbols are actually "reserved words", but it is best to use the vectors and can be 
manipulated just like any other object. script symbols background and camera only for the purpose of 
defining the current color of the graphical background of A subworld is an object associated with a pov. 
This allows ASAS the image and the current camera description (as a pov, see the to manipulate a complex 
object by modifying only the pov, section on geometric objects). The initial ASAS environment has hence 
various "instances" of an object may share the same other symbols defined to various frequently used 
objects (axes, underlying data. Subworlds also allow ASAS to work with colors, basic solids), it is good 
practice to know these and avoid "levels of abstraction" in a graphic database, when a subworld redefining 
them. is formed it notes the "overall size" and "typical color" of its contents. At display time this 
allows efficient tree structured Geometric Objects clipping (when an entire subworld is offscreen) and 
handling In addition to the data types found in most programming of detail too small to see (when an 
entire subworld lies within languages, ASAS provides a set of geometric (and photometric) a single pixel). 
[4] Hence the user can build levels of abstraction objects: vector, color, polygon, solid, group, pov, 
subworld, and light. Figure 2: How to make an Arch Fractal (~iven an arch element). The vector represents 
a position in three dimensional Cartesian space. It allows three (defop arch-fractalizer parameters, 
the X, Y and Z coordinates. Trail- ing zero coordinates may be omitted. (param: arch-element top-color 
bot-color levels fractal-ratio height width leg-width) A color object may be specified either by its 
Red, Green and Blue components, or by Inten- (local: (total-levels levels) sity, Hue, Saturation. The 
two operators are (offset-dist (half (dif width leg-width))) called rgb and ihs, each of which accept 
three numbers between 0 and 1. (sub-tower-offset-1 (sub-tower-offset-2 (vector (mirror offset-dist 0 
0)) x-axis sub-tower-offset-I))) A simple polygon contains a color and a list (arch-tower levels)) of 
vectors, the "boundary". The cut-hole operator allows the construction of polygons with "holes and islands" 
(that is, multiple (defop arch-tower boundaries). The color can be a group of a front color and a back 
color. The boundary (param: levels) points may be listed separately or as a group (if (zerop levels) 
of vectors. Here is a polygon expression for a certain blue triangle: (then (else nothing) (add-arch-level 
(arch-tower (dif levels 1)))))) (polygon blue (vector 1 0 O) (vector 0 1 O) (defop add-arch-level (vector 
0 0 1)) (param: sub-tower) A solid represents a bounded region of space, a closed polyhedron. It is composed 
of vertices and faces (as vectors and polygons) in addi- tion to topological connection information. 
(grasp sub-tower (scale fractal-ratio) (move (vector 0 height 0)) (rotate 0.25 y-axis)) Several geometric 
objects can be "glued to-gether" into a group object, which is then manipulated as a whole by the geometric 
(grasp arch-element (recolor (interp (quo levels total-levels) bot-color operators. A group expression 
allows any number of parameters -geometrical objects to top-color))) be grouped together, including other 
groups. (subworld (group arch-element (move subtower-offset-1 sub-tower) (move subtower-offset-2 sub-tower)))) 
 into a geometric database by the nesting of subworld objects. Objects to be seen in shaded images are 
illuminated by light objects. Lights are composed of a position vector and a color. Geometric Operators 
ASAS's geometric operators are the tools the animator uses to shape, move and orient objects. An object's 
shape may come directly from the action of operators, or parts encoded by hand with a digitizer can be 
assembled with the operators. The same operators are used both for static arrangements, or to create 
animated motion, by operating frame by frame under the control of an actor. In many command/menu based 
graphics systems it is difficult to precisely specify the correct ordering of geometric transfor- mation. 
For example, there will be a "rotate" command which accepts three numbers, the angles of rotation for 
each axis. Often there is no mention of in what order the rotations are applied, let alone a way to specify 
the desired order. In ASAS, the animator explictly determines the ordering of operations by the structure 
of the nesting of the expressions written in the script. The basic operators are ~generic", they can 
be given any type of geometric object and operate on it as is appropriate for that objecrs type. ASAS 
operators NEVER modify the object they are operating on. The value returned by an operator is a geometri- 
cally modified copy of the original object with otherwise the same type and structure. A notational shorthand 
is provided for the common occurence of a series of operations to be performed on a single object. The 
object to be operated upon can be made the "current" object (using the grasp operator). The "grasped" 
object will then be redefined by calls to operators which do not explicitly specify an object to operate 
on. Two basic types of geometric operators are provided by ASAS, "global" and "local" (sometimes called 
"self relative").The ASAS global geometric operators are called: scale, move, rotate, stretch and mirror. 
Generally these operators apply the named geometrical trans- form to any given geometrical object The 
transforms are relative to the origin and major axes of the global coordinate space. The parameter types 
to each are numbers and vectors as appropri- ate. ("Stretch" is a differential scaling for each axis, 
specified by a vector of scale factors). As an example of the usage of the ASAS global operators see 
Figures 1 and 2, these show how last year's SIGGRAPH cover was constructed. The "local" operators are 
similar in effect to the global operators, except that they are based on an object's OWN coordinate system 
rather than the global coordinate system. A subworid carries along its own little coordinate system, 
its pov. Not only does this allow efficient modification of the subworld but it also provides a reference 
for operations in the object's local coordinate space. The local operators were inspired by the "turtle" 
of the LOGO graphics language [2], and are intended to be a three dimensional analog of the turtle operations 
(walk forwards or backwards, turn right or left). This notion of a 3D turtle (more of a deep sea swimming 
turtle than a land crawling tortoise) was first used by Jim Stansfield and then refined by Henry Lieberman 
in a 3D line drawing extension to LOGO. A good treatment of this subject can be found in [1]. Usually 
objects will be defined so that the origin of their local coordinate space is at the center of the object. 
For this reason we will informally refer to the "origin of the local coordinate system" as the "center". 
Local operators are provided for moving, rotating, scaling and "zooming" relative to the local coordinate 
system. All of these operators accept one or two parameters, the second optional parameter is the object 
to operate on, if none is specified, the currently grasped object is redefined. Note: the relationship 
between global and local operators is similar to the that of pre- and post-multiplication of transform 
matrices. Also note: when objects other than flubworlds or povs are passed to self relative operators 
they are first put into an identity ("home") subworld, then operated on. Local operators: g row scale 
up about local center shrink scale down about local center forward move along local +Z axis backward 
move along local -Z axis left rotate to left about local Y axis right rotate to right about local Y axis 
up rotate upward about local X axis down rotate downward about local X axis cw rotate clockwise about 
local Z axis ccw rotate counter-clockwise about local Z axis zoom-in scale up local Z axis zoom-out scale 
down local Z axis local-move move along arbitrary local vector local-stretch scale each local axis independently 
home resets back to original definition space Examples: an operator sequence which (if evaluated each 
frame) will cause the AIRPLANE (that is, the sequence of objects which form the animated value of the 
variable AIRPLANE) to perform "barrel rolls", and one to cause the CAMERA to pan around while zooming 
out: (grasp airplane) (forward 0.1) (cw 0.02) (up 0.02)   (grasp camera) (right pan-speed) (zoom-out 
1.01)  Various other ASAS operators are available but will not be discussed here. There are recolor 
and cut-hole, and interp the general purpose interpolater, row and ring which make regular groups of 
objects, and prism which makes solids by projecting a polygon. Here are some examples of some of them, 
an operator to make a n-sided regular polygon (inscribed within a unit radius circle), and an operator 
to make a prism with regular "ends": (defop regular-polygon (param: color sides) (polygon color (ring 
sides (vector 0 1 O) z-axis))) (defop regular-prism (param: color sides thickness)  (prism color (vector 
0 0 thickness) (regular-polygon color sides))) This summary of ASAS operators suffers because of the 
language's extensibility; the full list is endless since the user invents new ones as needed. Beyond 
simple combinations of basic linear operations, there is a large class of nonlinear "bending" operators. 
For example consider "curl-up" which takes a long thin object and curls it into a spiral (Escher fans 
will know the application for that). Scripts and Animate Blocks The main program an ASAS user writes 
is called a script, which is a special type of defop. A script handles the setting up and setting down 
needed to produce an animated sequence (or write a file for later production by another system). The 
script expression includes a name and any number of subexpressions. The effect is to define an operator 
with that name which opens production, evaluates each expression, in the body, and closes production. 
There is no restriction, but the things in the body are usually either animate expressions ("animate 
blocks") or production utilities (such as "make N blank frames", "put this slate text", or "make an N 
second countdown"). An animate block is a special type of loop. Each time around the loop, after it evaluates 
its body, a flame of animation is produced automatically. Usually the body contains cue expres- sions 
("cue at frame N .... "L These cause objects to be made visible (with the see operator) or start, stop 
or direct actors. Animate blocks are exited when a cut operator is evaluated. This example script contains 
one animate block, which starts two similar actors at different times. Both actors then run until the 
end of the block. (script spinning-cubes  (local: (runtime 96) (midpoint (half runtime))) (animate 
(cue (at O) (start (spin-cube-actor green))) (cue (at midpoint) (start (spin-cube-actor blue)))  (cue 
(at runtime) (cut)))) structure of the action. While an ASAS "cue" is in fact simply a number (a frame 
number relative to the current animate block) it should not be thought of as a constant. Because of the 
computational nature of a script it can be quite easy to move cues around, since all cue points can be 
handled symbolically (by name rather than by a literal number). For example it is a simple rrlatter to 
change the overall runtime of a script (for a "quick run through" test) if all cue points are defined 
relative to one variable (e.g. "runtime"). Library macros exist to facilitate just such a scheme. The 
animator may find it necessary for artistic reasons to move a cue point within a script, again this will 
be quite painless if everything which is supposed to begin or end at that cue point refers to it only 
symbolically. Actors The control structure of an animation system would be very simple if we could assume 
that all sequences to be produced had at most one independently animated feature at any one time. On 
the other hand, if we assume that there may be any number of fully independent animated features (starting 
and stopping at random times, happening at different rates, running in syne or not) then conventional 
control structures are no longer the most appropriate. An ASAS actor can be thought of several ways. 
Most basically an actor is a "chunk" of code which will be executed once each frame. Usually an actor 
(or a team of them) is responsible for one visible element in an animation sequence, hence it contains 
all values and computations which relate to that object. In this sense an actor serves to modularize 
and localize the code related to one aspect, isolating it from unrelated code. From a formal point of 
view, an actor is an independent computing process in a non-hierarchical system with synchronized activa- 
tion and able to communicate with other actors by message passing. When an actor is "on" (between being 
started and stOped), it will be awakened once each flame, its local variables restored, its body evaluated, 
its variables saved, then put back to sleep. (Hence an actor has properties between a "closure" and a 
"process" in recent Lisp implementations.) Actors are put into action with the start operator, which 
takes an actor and returns the "actor instance id", a unique number for each active actor. An actor can 
deactivate itself, or can be gunned down by the script or another actor, this is done with the StOp operator 
which accepts an "actor instance id". Run is a combination of start and stop, it starts an actor with 
a predetermined stop cue. This is the definition of the operator "spin-cube-actor" used in the script 
"spinning-cubes": (defop spin-cube-actor (param: color)   (actor (local: (angle O) (d-angle (quo 3 
runtime)) (my-cube (recolor color cube))) (Note: cut accepts an optional frame number, and will cut 
only (see (rotate angle y-axis my-cube)) if that is the current frame, so that third cue could have been 
 (define angle written as "(cut runtime)") When an animate block is exited, all (plus angle d-angle)))) 
 of the actors associated with it are stopped. Hence animate blocks are somewhat like the "scenes" of 
a movie, the coarse It expects one parameter, a color, and returns an actor object. Computer Graphics 
Volume 16, Number 3 July 1982 The actor itself has three local variables, each of which is assigned an 
initial value in this case: "angle" is the current angle of rotation for this actors cube, "d-angle" 
is the incremental velocity of the angle, "my-cube" is a recolored version of ASAS's predefined "cube" 
solid. Each frame the actor constructs the rotated version of "my-cube" and passes it to the displayer, 
the current "angle" is updated for the next frame. Animated Numbers In the last example the symbol "angle" 
took on a series of numeric values, frame by frame, forming an arithmetic series. But for more complex 
time behavior (quadratic or cubic curves) the inline code to handle and update all those linear difference 
terms becomes a burden. To avoid this, ASAS supports an animated numeric object called a newton (as in 
Newtonian mechanics). Newtons can be used any place a number would be used, such as a coordinate in a 
vector or the angle parameter for rotate. Between frames however, newtons are automati-cally updated 
to the next value in their predefined sequence. The newton data structure holds its future as a chain 
of piecewise cubic curves with selectable degree of continuity at the joints. A newton can be specified 
in terms of position, velocity, acceleration and delta acceleration ("jerk" or "jerkiness") when those 
values are known. But more typically newtons are defined with utilities which produce curves with certain 
properties. Animators are familiar with terms like "slow in" or "slow out" meaning that an action should 
start (or end) with zero velocity (first derivative). The five most common curves (or pieces of curves) 
used in ASAS are: hold, linear, slowin, slowout and slowio. Hold accepts a value and a length of time, 
each of the others takes a starting and ending value and a time. SIowio (slow in and slow out) has zero 
derivatives at both ends. When none of the standard curves are appropriate an interpolating cubic spline 
fit is used. Actors and Behavior Simulation Some animation is made to match a preconceived image, especially 
in commercial production. Other times, animation is produced as an experiment, the answer to "what would 
happen if ...". In the second type, which might be called "behavior simulation", the animator sets up 
a little world by defining the rules of behavior and selecting the cast of characters. When the behavior 
simulation is run we obtain images of what went on in the little world. A classic example of this sort 
of thing is to try to build a computer graphic simulation of a flock of birds. We must define the behavior 
of a single bird so that when a lot of instances of the bird are simulated, they flock convincingly. 
The flock seems to be following a leader, but each time they turn, a new bird becomes the leader. The 
flock changes direction like a single unit, yet it is just an assembly of individuals. The flock is a 
dense cluster, but the birds do not often collide. ASAS actors provide a convenient way of implementing 
such behavior simulations. As mentioned before, one of the features of actors is the way they promote 
"separation of powers", independent modules of code which do not interfere with each other. This allows 
an actor to take the part of one characters in the simulation. If the same sort of character occurs many 
times in the simulation (e.g. many copies of BIRD) we can use independent "instances" of a given "class" 
of actors. The other key feature of actors which makes them suitable for behavior simulation is the ability 
to pass messages. Clearly the birds in the flock are exchanging information, through the action of light 
and sound on the bird's senses each one is aware of where the others are and where they are going (in 
an intrinsically depth sorted order!) In an actor simulation of the flock we would not go to the extent 
of modeling light and sound, but we could realistically have each bird broadcasting to everyone the message 
"I am here (x y z) and I'm heading (dx dy dz)". In that implementation, each bird would have to put the 
others in order of importance, probably using a ~hidden bird algorithm".  Message Passing ASAS messages 
are handled by two special operators: send and receive. Send composes messages and posts them at the 
recipient's mailbox. Receive reads each message in the mail- box, responding to each in a manner depending 
on the type of message. (ASAS actors act once each frame, not whenever a message comes, hence mail may 
pile up between frames so the mailbox is implemented as a FIFO queue.) Send takes an address, a "message 
type" (optional), and any other specific message data (numbers, geometric objects, sym- bols). The address 
is either an actor id or a special symbol; uall" means send to all actors and ~script" and "animate" 
can be used to send messages to the surrounding script or animate block. The "message type" is any symbol 
used to describe the type of message, it must match the message type in the recipient's receive construct. 
For example, these sends (1) tell "bouncer" to speedup by 10 percent and (2) announce to all birds where 
we are: (send bouncer speedup 1.10) (send all bird-state cur-position cur-velocity) The receive construct 
has a body much like a case construct. Each message in the mailbox is examined in turn, the "message 
type" of each is compared with the type of the various clauses. If one of the clauses in the body of 
the receive matches the incoming message type, the body of the clause is evaluated in response to the 
message. Message type "any" in a clause will match any incoming message. The contents of a message (past 
the type) may be accessed by specifying parameter bindings for the clause type. For example, this actor 
knows how to receive only "speedup" and "slowdown" message: (receive ((speedup f) (define speed (times 
speed f))) ((slowdown f) (define speed (quo speed f))) (any (print 'What?))) The message passing mechanism 
described above is based on a more primative operator called in. The in operator allows the evaluation 
of any expression "inside" the local variable space of an actor, thus allowing examining and setting 
the local variables of the actor. This is a useful but dangerous tool which should be used only in a 
well designed protocol.  References [22] Pfister, G. "A High Level Language Extension for Creating [1] 
Abelson, H. and diDessa, A. Turtle Geometry, MIT Press (Series in Artificial Intelligence), Cambridge, 
MA, 1981. [2] Austin, H., "The LOGO Primer", MIT A.I. Lab. Logo Working Paper 19. [3]' Church, A "The 
Calculi of Lambda Conversions", Annals of Mathematical Studies 6, Princeton University Press 1941, Re- 
printed by Klaus Reprint Co., 1965. [4] Clark, J., ~Hierarchical Geometric Models for Visible Surface 
Algorithms", CACM, October 1976. [5] Dahl, Myhrhaug, and Nygaard The SIMULA 67 Common Base Language, 
Norwegian Computing Centre, Oslo, 1968. [6] DeFanti, T. "The Digital Component of the Circle Graphics 
Habitat", Proceedings NCC 1976. [7] Dijkstra, E.W. "Notes on Structured Programming", August 1969 [8] 
Eastman, C. and Henrion, M. "GLIDE: A Language for Design Information Systems", SIGGRAPH '77 Proceedings, 
July 1977, San Jose, CA. [9] Futrelle, R. P. and Barta, G. "Towards the Design of an Intrinsically Graphical 
Language", SIGGRAPH '78 Pl~gceedings, August 1978, Atlanta, GA. [10] Goates, G., Griss, M. and Herron, 
G. "PICTUREBALM: A Lisp-based Graphics Language System with Flexible Syntax and Hierarchical Data Structure", 
SIGGRAPH '80 Proceedings, July 1980, Seattle, WA. [11] Goldberg, A. and Kay, A. SMALLTALK-72 Instruction 
Manual Learning Research Group, Xerox Palo Alto Research, March 1976. [12] Gre!f, I. and Hewitt, C. "Actor 
Semantics of PLANNER-73", Proc. of ACM SIGPLAN-SIGACT Conf., Palo Alto, CA, January 1975. [13] Hewitt, 
C. and Smith, B. "Towards a Programming Appren- tice", MIT AI Lab Working Paper 90, January 1975. [14] 
Hewitt, C. and Atkinson, R., "Parallelism and Synchroniza- tion in Actor System", ACM Symposium on Principles 
of Programming Languages 4, January 1977, L. A. CA. [15] Jones, B. "An extended ALGOL-60 for Shaded Computer 
Graphics", ACM SIGPLAN/SIGGRAPH Symposium on Graphical Languages, April 1976. [16] Kahn, K. "An Actor-Based 
Computer Animation Language", Prec. of the ACM-SIGGRAPH Workshop on User-Oriented Design of Computer 
Graphics Systems, Pittsburg, PA, October 1976. [17] Kahn, K., "A Computational Theory Of Animation", 
MIT A.I. Lab. Working Paper 145, April 1977. [18] Larkin, F. "Computing with Text-Graphic Forms", Confer-ence 
Record of the 1980 LISP Conference, August 1980, Stanford University. [19] Larkin, F. "A Structure from 
Manipulation for Text-Graphic Objects", SIGGRAPH '80 Proceedings, July 1980, Seattle, WA. [20] Moon, 
D. MacLisp Reference Manual, Revision O, MIT Project MAC, December 1975. [21] Newman, W. and Sproull, 
R. Principles of Interactive Computer Graphics, McGraw-Hill, 1973 and 1979. and Controlling Dynamic 
Pictures", ACM SIGPLAN/SIGGRAPH Symposium on Graphical Languages, April 1976. [23] Preissler, M. "Multi-processed 
Music Synthesis", BS Thesis MIT EECS Department, May 1976. [24] Reynolds, C. "A Multiproeessing Approach 
to Computer Animation", SB thesis, MIT EECS Department, August 1975. [25] Reynolds, C. "Computer Animation 
in the World of Actors and Scripts", SM thesis, MIT (Architecture Machine Group), May 1976 [26] Reynolds, 
C. Actor / Scriptor Animation System User's Manual 3.0, (ASASUM 3), Information International Inc., in 
preparation. [27] Smoliar, S. "A Parallel Processing Model of Musical Struc- tures" MIT AI Lab Technical 
Report 242, September 1971. [28] Sussman, G. and Steele, G. "SCHEME - An Interpreter for Extended Lambda 
Calculus", MIT AI Lab Memo 349, December 1975. b [29] Teitelman, W. InterLISP Reference Manual, Xerox 
Palo Alto Research Center, 1978. [30] Winograd, T. Understanding Natural Language, Academic Press, 1974. 
[31] Winston, P. and Horn, B. Lisp, Addison Wesley, 1981. [32] Wirth, N. "MODULA: a Language for Modular 
Multi-programming", Software, Practice and Experience 7,1; 1977 pp. 3-35. Note This entire paper is 
an example of computer graphics. All pictures were produced with Digital Scene Simulation, and directly 
(digitally) converted into four color halftones in the Infocolor format.The text was edited and composed 
on TECS, and assembled with a Page Makeup System. Camera ready, full page art (including typesetting 
and halftone generation) was produced with a COMp80/2-Pagesetter. All of these systems are products of 
Information International Inc.   
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801294</article_id>
		<sort_key>297</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>43</seq_no>
		<title><![CDATA[Color image quantization for frame buffer display]]></title>
		<page_from>297</page_from>
		<page_to>307</page_to>
		<doi_number>10.1145/800064.801294</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801294</url>
		<abstract>
			<par><![CDATA[<p>Algorithms for adaptive, tapered quantization of color images are described. The research is motivated by the desire to display high-quality reproductions of color images with small frame buffers. It is demonstrated that many color images which would normally require a frame buffer having 15 bits per pixel can be quantized to 8 or fewer bits per pixel with little subjective degradation. In most cases, the resulting images look significantly better than those made with uniform quantization.</p> <p>The color image quantization task is broken into four phases:</p> <p>1) Sampling the original image for color statistics</p> <p>2) Choosing a colormap based on the color statistics</p> <p>3) Mapping original colors to their nearest neighbors in the colormap</p> <p>4) Quantizing and redrawing the original image (with optional dither).</p> <p>Several algorithms for each of phases 2-4 are described, and images created by each given.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Dither]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.4.1</cat_node>
				<descriptor>Quantization</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor>Clustering</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>H.3.3</cat_node>
				<descriptor>Search process</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Digitizing and scanning</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010507</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Graphics recognition and interpretation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003347.10003356</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Retrieval tasks and goals->Clustering and classification</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010405.10010497.10010504.10010506</concept_id>
				<concept_desc>CCS->Applied computing->Document management and text processing->Document capture->Document scanning</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317.10003325</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval->Information retrieval query processing</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003317</concept_id>
				<concept_desc>CCS->Information systems->Information retrieval</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003227.10003351.10003444</concept_id>
				<concept_desc>CCS->Information systems->Information systems applications->Data mining->Clustering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Algorithms</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P221483</person_id>
				<author_profile_id><![CDATA[81100383628]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Heckbert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, New York Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>862270</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bellman, R. Dynamic Programming. Princeton University Press, Princeton, 1957.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_obj_id>356797</ref_obj_id>
				<ref_obj_pid>356789</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bentley, J. L., Friedman, J. H. Data structures for range searching. Computing Surveys 11, 4 (Dec. 1979), 397-409.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Bruce, J. D. Optimum Quantization. MIT R.L.E. Technical Report #429, (1965).]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Elias, P. Bounds on performance of optimum quantizers. IEEE Trans. on Information Theory IT-16, 2 (Mar. 1970) 172-184.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Floyd, R. W., Steinberg, L. An adaptive algorithm for spatial gray scale. SID 75, Int. Symp. Dig. Tech. Papers (1975), 36.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_obj_id>355745</ref_obj_id>
				<ref_obj_pid>355744</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Friedman, J. J., Bentley, J. L., and Finkel, R. A. An algorithm for finding best matches in logarithmic expected time. ACM Trans. Math. Software 3, (Sept. 1977), 209-226.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Gray, R. M., Kieffer, J. C., and Linde, Y. Locally optimal block quantizer design. Information and Control 45 (1980) 178-198.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Heckbert, P. Color Image Quantization for Frame Buffer Display. B.S. thesis Architecture Machine Group, MIT, Cambridge, Mass., 1980.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA[Huang, T. S., Tretiak, O. J., Prasada, B. T., and Yamaguchi, Y. Design considerations in PCM transmission of low-resolution monochrome still pictures. Proc. IEEE 55, 3 (Mar. 1967), 331.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA[In der Smitten, F. J. Data-reducing source encoding of color picture signals based on chromaticity classes. Nachrichtentech. Z. 27, (1974), 176.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA[Jain, A. K., and Pratt, W. K. Color image quantization. National Tele-communications Conference 1972 Record, IEEE Pub. No. 72, CHO 601-5-NTC, (Dec. 1972).]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Jarvis, J. F., Judice, N., and Ninke, W.H. A survey of techniques for the display of continuous tone pictures on bilevel displays. Computer Graphics and Image Processing 5, 1 (Mar. 1976), 13-40.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_obj_id>280635</ref_obj_id>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA[Knuth, D. E. The Art of Computer Programming, vol. 3, Sorting and Searching. Addison-Wesley, Reading, Mass., 1973.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Koontz, W. L. G., Narendra, P. M., and Fukunaga, K. A branch and bound clustering algorithm. IEEE Trans. Comput. C-24, 9 (Sept. 1975), 908-915.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Limb, J. O., Rubinstein, C. B., and Thompson, J. E. Digital coding of color video signals - a review. IEEE Trans. Commun. COM-25, 11 (Nov. 1977), 1349-1385.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[Lloyd, S. P. Least squares quantization in PCM's. Bell Telephone Labs Memo, Murray Hill, N.J., 1957.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA[Max, J. Quantizing for minimum distortion. IRE Trans. Information Theory IT-6, (Mar. 1960), 7.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M., and Sproull, R. F. Principles of Interactive Computer Graphics. MacGraw-Hill, New York, 1979.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Panter, P. F., and Dite, W. Quantization distortion in pulse-count modulation with nonuniform spacing of levels. Proc. IRE 39, 1 (Jan. 1951), 44.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>108781</ref_obj_id>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K. Digital Image Processing. John Wiley and Sons, New York, 1978.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Roberts, L. G. Picture coding using pseudo-random noise. IRE Trans. Information Theory IT-8, (Feb. 1962), 145.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA[Stenger, L. Quantization of TV chrominance signals considering the visibility of small color differences. IEEE Trans. Communications COM-25, 11 (Nov. 1977), 1393.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 COLOR IMAGE QUANTIZATION FOR FRAME BUFFER DISPLAY Paul Heckbert Computer Graphics Lab New York Institute 
of Technology ABSTRACT Algorithms for adaptive, tapered quant- ization of color images are described. 
The research is motivated by the desire to display high-quality reproductions of color images with small 
frame buffers. It is demonstrated that many color images which would normally require a frame buffer 
having 15 bits per pixel can be qua ntized to 8 or fewer bits per pixel with little subjective degradation. 
In most cases, the resulting images look significantly better than those made with uniform quant- ization. 
 The color image quantization task is broken into four phases: i) Sampling the original image for color 
statistics 2) Choosing a colormap based on the color statistics 3) Mapping original colors to their nearest 
neighbors in the colormap 4) Quantizing and redrawing the original image (with optional dither). Several 
algorithms for each of phases 2-4 are described, and images created by each given. CR CATEGORIES: II.3.3 
(Information Stor- age and Retrieval): Information Search and Retrieval -clustering; search process; 
 1.3.3 (Computer Graphics): Picture/Image Generation -digitization and scanning; display algorithms; 
1.4.1 (Image Process- ing) : Digitization -quantization. General Terms: Algorithms. Additional Key 
Words and Phrases: dither. Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice 
and the title of the publication and its date appear, and notice is given that copying is by permission 
of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or 
specific permission. 1982 ACM 0-89791-076-1/82/007/0297 $00.75 INTRODUCTION The power and versatility 
of frame buffers has created an increasing demand for them in industry, education, and the home. Most 
of these frame buffers are capable of displaying a static color image, yet many of them do not contain 
the amount of memory necessary to match the spatial and color resolution of the human eye. The eye is 
capable of distinguishing at least fifty thousand colors [15]. There- fore, it would take a frame buffer 
with at least ]5 bits per pixel to reproduce and display a color image with no noticeable contouring. 
On smaller frame buffers, contouring effects can become objectionable. One way to eliminate some of this 
quant- ization error is to employ the method of tapered quantization. The purpose of this paper is to 
explore techniques for color image quantization with the goal of high-quality image display on frame 
buffers. The Original Image Our input data are the red, green, and blue separations of a digitized color 
 image. A typical form for the input image is a rectangular array of pixels each hav- ing 24 bits (8 
bits per component). The color components are usually represented by numbers in the range [0,255]. If 
the original image is in this form, then strictly speaking it has already been quantized (when it was 
digitized from a video signal, for instance). We will assume that this initial quantization does not 
cause perceptible quantization errors. This will be the case if (a) the full gamut of RGB space is used, 
that is, if the digitization equipment is set up so that black is quantized to (r,g,b)=(0,0,0), white 
to (255,255,255), red to (255,0,0), etc. and (b) the 256 levels are approx- imately equally spaced perceptually. 
Given these conditions, we can regard the 24-bit original image as the "true" image. We will try to 
approximate it as closely as possible when we quantize. Computer Graphics Frame Buffers and Colormaps 
 It is useful to distinguish between two types of frame buffer architectures: let's call them "segregated" 
and "inte- grated". In segregated frame buffers, there are three independent memories for the red, green, 
and blue components of an image. Typically 8 bits are used per pixel. An integrated frame buffer, on 
the other hand, stores a single color number for each pixel rather than three separate components. These 
color numbers (pixel values) are used as addresses into a single color lookup table (colormap). The color- 
 map provides a level of indirection between the data in the picture memory and the actual displayed 
image. For a more thorough discussion of frame buffer hard- ware, see Newman and Sproull [18]. The algorithms 
we will discuss are intended for integrated frame buffers having a colormap. Introduction to Quantization 
 Definitions: Quantization is the process of assign- ing representation values to ranges of input values. 
In image processing, the value being quantized can be an analog or digital signal. Color image quantization 
is the process of selecting a set of colors to represent the color gamut of an image, and computing the 
mapping from color space to representative colors. There are two general classes of quantization methods: 
uniform and tapered. In uniform quantization, the range of the input variable is divided into intervals 
of equal length. The choice of intervals in tapered quantization is usually based on the statistical 
distribution of the input variable. To compare the quality of different quantizations, a distortion measure, 
or error metric, is often intro- duced. With this formalism, one can search for the "optimal" tapered 
quant- ization of a variable (or image). Notation: In the following, let x be an M-dim- ensional input 
point (a 3-D color for our purposes). A quantizer consists of: (a) a set of K representative or output 
points: Y = lYi, i=I,2 ..... KI, (b) a partition of the input space into  regions (quantization cells) 
: R = {r i, i=1,2 .....KI,  (c) a mapping from input points to rep-  representative indices: p(x) = 
i if x6r i, and  (d) the quantization function which maps input points into output points:  q(x) = 
y[p(x)]. In color image quantization, Y is the colormap into which we will quantize, K is the number 
of colors in the colormap  Volume 16, Number 3 July 1982 (usually 1024 or less), and p is a mapping 
from colors in the original image to pixel values in the quantized image. The images are notated as 
follows: Let ci, j be the color of the pixel in the original image at row i, column j, where 0~i<NI 
and 0~j<NJ. Denote the pixel value at row i, column j of the final (quantized) image by fi,j" Note that 
c is a vector matrix and f a scalar matrix. We assume the original ann final images have the same resolution. 
 COLOR I~GE QUANTIZATION Uniform quantization, though compu- tationally much faster than adaptive, tapered 
quantization, leaves much room for improvement. Compare the 24-bit original image in fig. 2 with the 
uniform 8-bit quantization in fig. 3. The contouring here is quite serious. It results because many of 
the colors in the colormap are not used in the final picture; they are wasted. By adapting a colormap 
to the color gamut of the original image, we are assured of using every color in the colormap, and thereby 
reproducing the original image more closely. That is the intuitive concept behind tapered color image 
quantization. We will now develop these ideas formally. When an image is quantized, each of the 3-dimensional 
colors in the original image must be encoded into a single pixel value. To do this we compute the mapping: 
 fi,j = P(Ci,j) for 0~i<NI, 0 ~j<NJ. The display processor in the frame buffer displaying our final 
picture passes the pixel values through the colormap Y: YP(ci,j) = q(ci,j). This will display a picture 
closely resem- bling the original if we have quantized well. Measuring Quantization Error To measure 
the difference between the original and quantized images (the total quantization error), we use the following 
formula: --d(ci,4, i,j D = L J q(ci,j)) where d(x,y) is a distortion function or color metric which 
measures the "diff- erence" between corresponding colors in the original and final images [7]. We will 
use a very simple color metric, distance squared in RGB space: d(x,y) = (xr-Yr) 2+(xg-yg)2+(xb-Yb) 2 
 where x = (Xr,Xg,X b) and Y=(Yr,Yg,Yb)"  Computer Graphics Volume 16, Number 3 July 1982 This formula 
is chosen for its computa- tional speed and simplicity. Ideally, the color metric should be perceptually-based, 
since the human eye is final judge of quantization quality. The use of YIQ or Lab color space for the 
color metric would probably improve our quantizers somewhat [15]. We define the "optimal" quantizer 
(for a given image and number of colors K) as the one which minimizes D. Quantization Literature One-dimensional 
quantization has an extensive literature [4], [9] , [17] , [19] . It is possible to find optimal 1-dimen- 
sional quantizers efficiently. Algorithms which make use of dynamic programming [I] to find an optimal 
quantizer for an N-level input in O(N2K) time are given in [3] and [8]. These can be used to quantize 
a mono- chrome picture in a matter of seconds at today's computer speeds. Color image quantization has 
received little attention in the literature until recently. It is usually done by treating the three 
color components independently. Independent quantization in spaces such as YIQ and Lab (see [15] and 
[20]) is ineffic- ient because much of their space lies outside the RGB color cube [ii]. Subjec- tive 
experiments were used by In der Smitten to subdivide RGB space into 125 volumes [i0]. Some contouring 
is visible with his quantizer. Stenger has also done some tests of tapered color quantization [22]. 
Koontz, Narendra, and Fukunaga [14] have published an algorithm for finding the optimal quantization 
(they call them "clusterings") for small input and output sets. Their program required 28 seconds to 
find the optimal classification of 120 points into 8 classes. They do not ana- lyze the speed of their 
algorithm, so it is difficult to predict the computation time for larger quantization jobs such as ours. 
Assuming a linear-time algorithm (a conservative guess), quantizing several hundred thousand colors 
would take half a day. Clearly this is not practical. Multidimensional quantization is much more difficult 
than 1-dimensional quant- ization. The reason for this is the increased interdependency of quantization 
cells. While in the one-dimensional case all intervals are determined by the two thresholds at either 
end, in the multi- dimensional case the quantization cells can be polytopes with any number of sides. 
The complex topology of multidimensional tapered quantization cells is suggested by the shapes in fig. 
18. Optimal multidimensional quantization has no known fast solution [7]. The methods we will describe 
use heuristic approaches to approximate the optimal. ALGORITHMS FOR COLOR I~GE QUANTIZATION The algorithms 
for color quantization described below use the following four phases: i) sample image to determine color 
dis- tribution 2) select colormap based on the distri- bution 3) compute qua ntization mapping from 
24- bit colors to representative colors (ie. colors in the colormap) 4) redraw the image, quantizing 
each pixel. Choosing the colormap is the most challenging task. Once this is done, computing the mapping 
table from colors to pixel values is straightforward. PHASE i: SAMPLING THE ORIGINAL IMAGE The information 
needed by the colormap selection algorithms of phase 2 is a hist- ogram of the colors in the original 
image. This is collected in one pass over the input image. To conserve memory, a pre- quantization from 
24 bits to 15 bits (5 bits red, 5 bits green, 5 bits blue) is suggested. In this case the color fre- 
quency histogram will be a table of length 32768. This clumping of the colors has the effect of reducing 
the number of different colors and increasing the frequency of each color. These properties are important 
to the algorithms described below. PHASE 2: CHOOSING A COLOR~P We discuss two algorithms for choosing 
a set of representatives (colormap) based on the input distribution, and a process which can be used 
to perturb the choice of representatives to improve a quantizer. The Popularity Algorithm The popularity 
algorithm was developed independently by two groups in 1978: Tom Boyle and Andy Lippman at MIT's Architech- 
ture Machine Group and Ephraim Cohen at the New York Institute of Technology. Boyle &#38; Lippman's ideas 
were implemented by the author at MIT [8]; the latter is unpub- lished. The assumption of this algorithm 
is that the colormap can be made by finding the densest regions in the color distri- bution of the original 
image. The popu- larity algorithm simply chooses the K colors from the histogram with the highest frequencies, 
and uses these for the color- map. This can be done with a simple selection sort [13]. This will take 
time O(NK), where N is the number of colors in the histogram. The popularity algorithm functions well 
for many images (fig. 4), but performs poorly on ones with a wide range of colors  Computer Graphics 
Volume 16, Number 3 July 1982 (fig. 15), or when asked to quantize to a small number of colors (say<50). 
neglects colors in sparse regions color space. It often of the The Median Cut Alqorithm The median cut 
algorithm was proposed by the author in [8], and is reprinted here with minor changes. Kenneth Sloan 
has pointed out that the database used in this algorithm is nearly identical to Bentley's k-d trees [2]. 
 The concept behind the median cut algo- rithm is to use each of the colors in the synthesized colormap 
to represent an equal number of pixels in the original image. This algorithm repeatedly subdivides color 
space into smaller and smaller rectangular boxes. We start with one box which tightly encloses the colors 
of all NIxNJ pixels from the original image. The number of different colors in this first box is dependent 
on the color resolution used. Experimental results show that 15 bits per color (the resolution of the 
histogram) is sufficient in most cases. Iteration step: split a box. The box is "shrunk" to fit tightly 
around the points (colors) it encloses, by finding the minimum and maximum values of each of the color 
coordinates. Next we use "adaptive partitioning" (Bentley's termin- ology) to decide which way to split 
the box. The enclosed points are sorted along the longest dimension of the box, and segregated into two 
halves at the median point. Approximately equal numbers of points will fall on each side of the cutting 
plane. The above step is recursively applied until K boxes are generated. If, at some point in the subdivision, 
we attempt to split a box containing only one point (repeated many times, perhaps), the spare box (which 
would have gone unused) can be reassigned to split the largest box we can find. After K boxes are 
generated, the repre- sentative for each box is computed by aver- aging the colors contained in each. 
The list of representatives is the colormap Y. The sorting'used in the iteration step can be done efficiently 
with a radix list sort [13], since the color coordinates are small integers, generally within the range 
[0,255]. Splitting each box will therefore take time proportional to the number of different colors 
enclosed. Generating the colormap will take O(NlogK) time, where N is the number of different colors 
in the first box. Images quantized by the median cut tech- nique are shown in figures 5 and ll. Subjective 
tests have shown that the median cut algorithm produces better quantizers than the popularity algorithm. 
In some cases the difference is striking (compare figures 15 and 16). Other criteria could be used to 
decide which coordinate to bisect. Instead of choosing the coordinate with the largest range, one might 
use the one with the largest variance. Likewise, one could choose the split plane so that the sum of 
variances for the two new boxes is mini- mized. This would tend to minimize the mean squared error better 
than the median criterion. A Fixed Point Algorithm for Improving A Quantizer Gray, Kieffer, and Linde 
[7], have described an algorithm for finding a locally optimal multidimensional quantizer. It is an extension 
of a method first pro- posed by Lloyd [16]. A quantizer is called locally optimal if small perturbations 
in Y, the set of representative points, cannot decrease the total distortion D. Given a set of representatives 
Y, the optimal partition R'(Y) is: r~ = ix : d(x,y k) ~d(x,yj), j ~k]  which is the locus of points 
whose nearest neighbor is y~. Given a partition R, the optimal set o~ representatives Y'(R) is the set 
of y~ such that y~ is the centroid of all input points ci,~ which lie inside r k. These can be comblne~ 
to define a mapping T which perturbs Y so that D never increases: TY = Y' (R' (Y)) .  To paraphrase 
the equations, for each representative point Yk, one finds the centroid of all input points whose nearest 
 neighbor in Y is Yk" Lloyd's algorithm applies this mapping repeatedly in order to improve a quantizer, 
and hopefully converge on a fixed point of the mapping T (a point where TY=Y). Linde et al. have proven 
that the algorithm converges in a finite number of iterations if the input distribution is finite (as 
ours is). The fixed point will be a local minimum of D, but not necessarily a global one [7]. This fixed 
point algorithm can be used to improve quantizers generated by the popularity or median cut algorithms. 
Experimental results show that the improv- ement is slight for the latter. The iter- ation will help 
more when the first guess is crude, such as a uniform lattice of points, as seen in fig. 13. To make 
this algorithm practical, one must be able to find nearest neighbors quickly. That is our next topic. 
 300  Computer Graphics Volume 16, Number 3 July 1982 PHASE 3: ~PPING COLORS TO NEAREST NEIGHBORS IN 
COLORMAP Given an input distribution c and a set of representatives Y, D is minimized when q maps a 
point to its nearest represen- tative: p(x) = i ~ if d(x,Yi) ~d(x,yj) , j ~i  q(x) Yi This operation 
is sometimes called a "near- est neighbor query" [2]. In our applica- tion it could also be thought of 
as an "inverse colormap", since it maps colors into pixel values. By evaluating this function for each 
color in the original image, and saving this information in a table, one can speed up phase 4 significantly. 
The alternative is to evaluate p once per pixel. The former will be faster if the number of different 
colors in the original image is smaller than the number of pixels in the image. If one uses a prequantization 
to 15 bits, as suggested for phase i, the number of colors will be under 32768. For all but low-resolution 
frame buffers, this is smaller than the number of pixels. Note that the quantization mapping table will 
fit conveniently in the same array that was used for the histogram. There are several methods for computing 
the function p: Exhaustive Search The straightforward way to compute p(x) is to test all K representatives 
and choose the one which minimizes d(x,y.) . Unfor- . 1  tunately, this method is slow. Much time is 
wasted c'onsidering distant points which couldn't possibly be the nearest neighbor. It would be shrewder 
to do some pre- processing on Y to set up a database which enables faster queries. Locally Sorted Search 
 We create a database consisting of an NxNxN lattice of cubical cells each containing a sorted list of 
representatives. Each cell's list should include all representatives which are the nearest neighbors 
of some point in that cell. Each list entry con- tains two variables: a representative's number (rep_no) 
and its distance (dist) from the nearest point in the cell. Dist is defined to be zero for representatives 
inside the cell. To create the list, we compute each representative's distance from the cell, put these 
in a list, and then sort the list by the distance key. Note that a given representative'can occur in 
several lists. A simple way to limit the length of" the lists is to eliminate representatives which 
could not possibly be the nearest neighbors of any point inside the cell. As shown in fig. I, one finds 
the representative point nearest the center of the cell and computes its distance from the farthest corner 
of the cell. This gives us an upper bound on the distance from any point in the cell to its nearest representative. 
All representatives whose distance to the cell is greater than this can be left out of the list, thereby 
speeding the list sorting operation and conserving memory. TO compute the function p(x) with this database, 
we first find the cell which encloses x, and then execute the following procedure: min = infinity; 
i = 0; while (min>entry[i].dist) begin distance = d(x,y[entry[i].rep_no]) ; if (distance<min) then 
begin nearest = i; min = distance; end i = i+l; end return(nearest); ! , ! C ~not in list in 
list l r cell tB I I ! Y ! T ! ! ! I I I fig. i: Point A is representative closest to cell center. 
The distance from A to the corner of the cell most distant from it (B) is r. Since all points in the 
cell are less than r units away from A, any repre- sentative more than r units away from the cell can 
be eliminated from the cell list. Thus C will be excluded, but D included.  How much memory and computation 
is involved in the creation of these lists? This is dependent on the size and number of cells. If we 
use a lattice of N 3 cells, and the average list length is L entries, the memory required by the database 
is O(N3L). Computing distances from each representative to a cubical cell takes O(K) time, and the list 
sort takes O(LIo~L), so the preparation time is therefore O(N~K+ N3LIogL). In practice, one should avoid 
computing the representative lists for unused cells. Only the most colorful images will contain colors 
in all N 3 cells. One way to compute only the needed cells is to create them dynamically, the first time 
they are used. When the function p is asked for the nearest neighbor of a point in an "unchart- ed" cell, 
it creates the representative list for that cell, processes the query, and marks the cell as "charted". 
 When choosing N one must compromise between fast search times and fast pre- processing times. A fine 
lattice (large N) will lead to short cell lists and hence low search times, but high preprocessing cost. 
An extremely coarse lattice, such as N=I, will function much like exhaustive search: the long lists will 
result in high search times, but the preprocessing time will be negligible. The best compromise will 
depend on the number and distribution of queries. Experimental Results for Locall~ Sorted Search The 
average list length L is dependent on the number and distribution of repre- sentatives and the number 
of cells. Empirical tests on the colormaps generated by the median cut algorithm for a sample set of 
17 images had an average list length of 35 when K=256 and N=8. With exhaustive search, each call to the 
func- tion p requires inspection of K represen- tatives. Using locally sorted search, the average number 
of representatives tested was only ii (also for the case K=256, N=8). This is 23 times smaller than the 
number of tests the exhaustive method would make. Locally sorted search shows the great- est advantage 
over exhaustive search when K is large and when the colors in the input image have a wide distribution. 
In these cases the preprocessing time to create the database is overshadowed by the savings in search 
time. In the tests men- tioned above, locally sorted search was never slower than the exhaustive method. 
For the image in fig. ii, it was three times faster than the exhaustive method. K-D Tree Search author 
only recently, has been proposed by Friedman, Bentley, and Finkel [6]. Using a k-d tree database to structure 
the K representative points, they achieved a search time of O(logK). Their algorithm has not yet been 
tested for the application of color image quantization. PHASE 4: QUANTIZING AND REDRAWING THE IMAGE 
 To quantize the image, we simply pass each pixel of the original image through the quantization mapping 
table created during phase 3, and write the pixel values into a frame buffer. This will redraw the image 
(quantized of course) using only K colors. Depending on the image, the quantiza- tion errors may be 
obvious or invisible. Images with high spatial frequencies (such as hair or grass) will show quantization 
errors much less than pictures with large, smoothly shaded areas (such as faces). This is because the 
high-frequency contour edges introduced by the quantization are masked by the high frequencies in the 
original image. When quantizing to very few colors, or to a poorly-chosen colormap, the contouring can 
be visually distracting (see fig. 6). Images which suffer from severe contouring when quantized can 
be improved with the technique of dithering. Dithering The basic strategy of dithering is to trade intensity 
resolution for spatial resolution. By averaging the intensities of several neighboring pixels one can 
get colors not represented by the colormap. If the resolution of the frame buffer is high enough, the 
eye will do the spatial blending for us. Taking advantage of this, it is possible to reproduce many color 
images using only four colors, as is done in color halftoning. One simple way to dither is to modulate 
the original image with a high frequency signal, such as random noise, before quantization [21]. A survey 
of various dithering techniques can be found in [12]. The dithering technique we reco~aend is due to 
Floyd and Steinberg [5]. Their algorithm compensates for the quantization error introduced at each pixel 
by propa- gating it to its neighbors. If the propa- gation is directed only to pixels below or to the 
right of the "current pixel", we can do both quantization and propagation in one top-to-bottom pass over 
the image. Another algorithm for nearest neighbor queries, which came to the attention of the A program 
to quantize and dither a color image using their algorithm would look something like: for i=0 to NI-I 
do for j=0 to NJ-I do begin x = c. .; (read a color) 1,3 k = p(x); (find nearest rep.) fi,j = k; 
(draw quantized image) e = x-Yk; (quantization error) (distrib. in 3 directions) c. = c. + e'3/8; 
 1,j+l l,j+l Ci+l, j = Ci+l, j + e'3/8; Ci+l,j+ 1 = Ci+l,j+ 1 + e/4; \  end In the above, x and e 
are vectors; i, j, and k are scalars. The improvement that dithering makes for an image quantized by 
the median cut method is shown in figures 12 and 17. If the colors are carefully chosen, the Floyd- Steinberg 
scheme can do surprisingly well with only 4 colors (fig. 7). Using dither in the last phase of our quantizers 
raises several unanswered questions. Should our algorithms for colormap selection be altered because 
we are dithering? If so, how? One would like to guarantee that all colors in the original image can be 
generated by blending (taking a linear combination of) colors in the colormap. This will be true only 
if the input colors lie inside the convex hull of the representative colors. Methods to guarantee 
this deserve further research. CONCLUSIONS  We found that the architecture of inte- grated frame buffers 
forces certain restrictions on any attempt to display color images. One is naturally led to the non-separable 
multidimensional quantization problem. Although the optimal solution of this problem seems computationally 
intract- able, there are approximate techniques which allow high-quality color quantization to be done 
efficiently. Using one of the algorithms described, it is possible to display a full-color image using 
only 256 colors, thus tripling memory efficiency. To put together a color image quantizer with the algorithms 
described here, the author would recommend the following. .The median cut algorithm is suggested for 
phase 2 because its sensitivity to the color dis- tribution of the original image is much better than 
that of the popularity algorithm. To map colors to their nearest neighbors in the colormap, locally 
sorted search has proven fastest. Dithering is a nice option which is often worth the extra computation 
required. The author's implementation of the above ensemble on a VAX 11/780 can quantize a 512x486x24-bit 
image to 256 colors in under one minute. The quantization techniques presented here could be improved 
in several ways. By changing the color metric to be more perceptually-based, better-looking quanti- zation 
would result. Also, it would be nice to find a single database which functions for all phases of the 
quantiza- tion process, to replace the hodgepodge used here. Perhaps the k-d tree created by the median 
cut algorithm could be used for nearest neighbor search as well. ACKNOWLEDGEMENTS  Much of the research 
reported here was done while I was an undergraduate at MIT, working part-time at the Architecture 9~chine 
Group. I would like to thank Professors Nicholas Negroponte and Andrew Lippman for their support. Thanks 
to Tom Boyle for introducing me to this fascin- ating topic, and to Paul Trevithick and Professor Gilbert 
Strang of the Math Department for assisting with the theor- etical formulation of the problem. Dan Franzblau, 
Walter Bender, and Professor Ron MacNeil were my principal image critics. Kenneth Sloan, now at MIT, 
was partially responsible for re-sparking my interest in color image quantization. At NYIT, Lance Williams 
lent a critical eye, and Becky Allen assisted with prep- aration of the paper. REFERENCES  [1] Bellman, 
R. Dynamic Programminq. Princeton University Press, Princeton, 1957. [2] Bentley, J. L., Friedman, J. 
H. Data structures for range searching. Computing Survey s ii, 4 (Dec. 1979), 397-409. [3] Bruce, J. 
D. Optimum Quantization. MIT R.L.E. Technical Report #429, (1965). [4] Elias, P. Bounds on performance 
of optimum quantizers. IEEE Trans. on Information Theory IT-16, 2 (-Mar.--~970) 172-184. [5] Floyd, 
R. W., Steinberg, L. An adapt- ive algorithm for spatial gray scale. SID 75, Int. Symp. Dig. Tech. Papers 
(1975), 36. [6] Friedman, J. J., Bentley, J. L., and Finkel, R. A. An algorithm for find- ing best matches 
in logarithmic expected time. ACM Trans. Math. Software 3, (Sept. 1977), 209-226. [7] Gray, R. M., Kieffer, 
J. C., and Linde, Y. Locally optimal block quantizer design. Information and Control 45 (1980) 178-198. 
 [8] Heckbert, P. Color Image Quantization for Frame Buffer Display. B.S. thesis Architecture Machine 
Group, MIT, Cambridge, Mass., 1980. [9] Huang, T. S., Tretiak, O. J., Prasada, B. T., and Yamaguchi, 
Y. Design considerations in PCM transmission of low-resolution monochrome still pic- tures. Proc. IEEE 
55, 3 (Mar. 1967), 331. [i0] In der Smitten, F. J. Data-reducing source encoding of color picture signals 
based on chromaticity classes. Nachrichtentech. Z. 27, (1974), 176. [ii] Jain, A. K., and Pratt, W. 
K. Color image quantization. National Tele- communications Conference 1972 Record, IEEE Pub. No. 72, 
CHO 601-5-NTC, (Dec. 1972). [12] Jarvis, J. F., Judice, N., and Ninke, W.H. A survey of techniques 
for the display of continuous tone pictures on bilevel displays. Computer Graphics and Image Processing 
5, 1 (Mar. 1976), 13-40. [13] Knuth, D. E. The Art of Computer Programming, vol. 3, Sorting and Searching. 
Addison-Wesley, Reading, Mass., 1973. [14] Koontz, W. L. G., Narendra, P. M., and Fukunaga, K. A branch 
and bound clus- tering algorithm. IEEE Trans. Comput. C-24, 9 (Sept. 1975), 908-915. [15] Limb, J. 0., 
Rubinstein, C. B., and Thompson, J. E. Digital coding of color video signals -a review. IEEE Trans. Commun. 
COM-25, ii (Nov. 1977), 1349-1385. [16] Lloyd, S. P. Least squares quantiza- tion in PCM's. Bell Telephone 
Labs Memo, Murray Hill, N.J., 1957. [17] Max, J. Quantizing for minimum distortion. IRE Trans. Information 
Theory IT-6, (Mar. 1960), 7. [18] Newman, W. M., and Sproull, R. F. principles of Interactive Computer 
Graphics. MacGraw-Hill, New York, 1979.  [19] Panter, P. F., and Dite, W. Quanti- zation distortion 
in pulse-count modulation with nonuniform spacing of levels. Proc. IRE 39, 1 (Jan. 1951), 44.  [20] 
Pratt, W. K. Digital Image Processing. John Wiley and Sons, New York, 1978. [21] Roberts, L. G. Picture 
coding using pseudo-random noise. IRE Trans. Information Theory IT-8, (Feb. 1962), 145.  [2~ Stenger, 
L. Quantization of TV chrom- inance signals considering the visi- bility of small color differences. 
IEEE Trans. Communications COM-25, ii (Nov. 1977), 1393.    
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801295</article_id>
		<sort_key>309</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>44</seq_no>
		<title><![CDATA[The role of videotex (Panel Session)]]></title>
		<page_from>309</page_from>
		<page_to>310</page_to>
		<doi_number>10.1145/800064.801295</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801295</url>
		<abstract>
			<par><![CDATA[<p>Frank W. Tompa</p> <p>Interactive videotex systems may soon emerge as a principal information, entertainment and communications medium relying heavily on computer graphics. Until now, much has been written about the image presentation and data facilities of the several current systems, but videotex literature and research is unexpectedly far removed from mainstream graphics audiences. The purpose of this panel session is therefore to narrow the gap.</p> <p>In outline, the panel session includes an introduction to videotex, followed by several short presentations covering videotex graphics, experience, alternatives, and prospects.</p> <p>Brian Botten</p> <p>David Godfrey</p> <p>John Norton</p> <p>Lois Schneider</p> <p>Andries van Dam</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>H.4.3</cat_node>
				<descriptor>Videotex</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Interaction techniques</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10010391</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Graphics input devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002951.10003260.10003282.10003286</concept_id>
				<concept_desc>CCS->Information systems->World Wide Web->Web applications->Internet communications tools</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP43124254</person_id>
				<author_profile_id><![CDATA[81100157184]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frank]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Tompa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Department of Computer Science, University of Waterloo]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329238</person_id>
				<author_profile_id><![CDATA[81100435785]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Botten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[President, International Vedeotex Information  Providers Association and Director, Extel Computing Ltd., London, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329765</person_id>
				<author_profile_id><![CDATA[81100408169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Godfrey]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chairman, Department of Computer Science, Univ. of Victoria, Victoria, BC]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P331538</person_id>
				<author_profile_id><![CDATA[81544267656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Norton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VP Engineering, Norpak Ltd., Kanata, Ontario]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P332089</person_id>
				<author_profile_id><![CDATA[81100155297]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lois]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schneider]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Director, Videotex Design and Development, CBS Venture One, Fairlawn NJ]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P18590</person_id>
				<author_profile_id><![CDATA[81452592989]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andries]]></first_name>
				<middle_name><![CDATA[van]]></middle_name>
				<last_name><![CDATA[Dam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chairman, Department of Computer Science, Brown University, Providence, RI]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL: The Role of Videotex CHAIR: Frank W. Tompa Department of Computer Science University of Waterloo 
 PANELISTS: Brian Botten President, International Videotex Information Providers Association and Director, 
Extel Computing Ltd., London, UK David Godfrey Chairman, Department of Computer Science Univ. of Victoria, 
Victoria, BC John Norton VP Engineering Norpak Ltd., Kanata, Ontario Lois Schneider Director, Videotex 
Design and Development CBS Venture One, Fairlawn NJ Andries van Dam Chairman, Department of Computer 
Science Brown University, Providence, RI SUMMARY Interactive videotex systems may soon emerge as a 
principal information, entertainment and communications medium relying heavily on computer graphics. 
Until now, much has been written about the image presentation and data facilities of the several current 
systems, but videotex literature and research is unexpectedly far removed from mainstream graphics audiences. 
The purpose of this panel session is therefore to narrow the gap. In outline, the panel session includes 
an introduction to videotex, followed by several short presentations covering videotex graphics, experience, 
alternatives, and prospects. Introduction The first development towards videotex began around 1970, 
when the British Broadcasting Corporation began Ceefax, a teletext system that provides color graphics 
and text frames to a conventional televi- sion set via a decoder that interprets data received during 
the verti- cal blanking interval. The British Post Office subsequently developed viewdata, later named 
Prestel, an interactive videotex system that uses compatible decoders and conventional television receivers 
to pro- vide graphics and text frames on request via the telephone network. Several systems, including 
Antiope, Bildschirmtext, Captain, Teletel, and Telidon, have been developed thoughout the world since 
that time. 309 PANEL (continued): The Role of Videotex Graphics Videotex systems are based on pages 
of graphics and text, each page corresponding to a display image. The display techniques used to date 
are so-called alpha-mosaics, dynamically redefined characters sets, alpha-geometrics, and alpha-photographics. 
The first of these is based on a grid-like division of the screen (typically 40 columns by 20-25 rows), 
each of whose cells can contain any one of a fixed number of "characters" which include pre-defined graphic 
characters. Dynamically redefined character sets allow each page to use a tailored "character" set by 
first transmitting the cell display corresponding to each code and subsequently one or more grids of 
codes that define complete display images. Alpha-geometrics correspond to vector (or rather area) generation 
displays: Images are composed of points, lines, arcs, represented transmitted s polygons, by alpha-phequentially. 
and otog text. Finally, raphics, where raster codes for graphics pixels are are Experience There have 
been two major audiences targeted for videotex appli- cations: business and home users. The business 
user perceives videotex primarily as a communications medium serving information needs that are measured 
in a small number of hours; that is, it fills the void between a real-time information system (e.g., 
a wire service) and printed media (e.g. newspapers, newsletters, memos, reports). Business needs include 
electronic messaging, on-line (but not neces- sarily immediate) order entry, fast analyses of business 
events, etc. Home videotex, instead, has been based on entertainment, transaction invocation (e.g., shopping 
or making reservations), computer-based instruction, integrated services to include "telemetering, telemoni- 
toring, television, and telephone", etc. Although videotex systems have been available since the mid-70's, 
 pilot trials and field trials have dominated the scene, with a few commercial operations appearing only 
in the last few years. It is important to examine what has been learned so far, especially as it relates 
to application opportunities and end user acceptance. Alternatives The attraction for videotex is its 
use of existing components particularly delivery networks (e.g., the telephone system) and receivers 
(television sets). What then makes videotex different from other development? Can the goals of videotex 
be achieved with stan- dard computer/communications services together with data systems based on (or 
incorporating) existing on-line services for information dis- semination? Prospects Because videotex 
is a recent development, future directions can- not be predicted reliably by past trends. However, the 
incorporation of hardware, software, and applications improvements and limitations must be based on a 
vision of expected videotex activity. One goal of the panel is to give a realistic appraisal of the prospects 
for videotex in the near future, emphasizing the range of applications and the role of graphics. 310 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801296</article_id>
		<sort_key>311</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>45</seq_no>
		<title><![CDATA[A language for bitmap manipulation]]></title>
		<page_from>311</page_from>
		<doi_number>10.1145/800064.801296</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801296</url>
		<abstract>
			<par><![CDATA[<p>In this paper we propose that bitmaps, or raster images, should be given full citizen status in the world of computer science. We introduce a calculus of bitmap operations and MUMBLE, a programming language appropriate for describing bitmap computations. We illustrate the use of MUMBLE by several interesting graphical applications. We also discuss the structure of BOP, an efficient implementation of the bitmap calculus that is the underpinning of our system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Bitmap and framebuffer operations</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.6</cat_node>
				<descriptor>Languages</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003128</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction techniques</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Languages</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P170488</person_id>
				<author_profile_id><![CDATA[81452606669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Leo]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Guibas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox PARC and Stanford University, Stanford, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39030834</person_id>
				<author_profile_id><![CDATA[81100171458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jorge]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stolfi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox PARC and Stanford University, Stanford, California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801297</article_id>
		<sort_key>313</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>46</seq_no>
		<title><![CDATA[A device independent graphics imaging model for use with raster devices]]></title>
		<page_from>313</page_from>
		<page_to>319</page_to>
		<doi_number>10.1145/800064.801297</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801297</url>
		<abstract>
			<par><![CDATA[<p>In building graphic systems for use with raster devices, it is difficult to develop an intuitive, device independent model of the imaging process, and to preserve that model over a variety of device implementations. This paper describes an imaging model and an associated implementation strategy that:</p> <p>1. Integrates scanned images, text, and synthetically generated graphics into a uniform device independent metaphor;</p> <p>2. Isolates the device dependent portions of the implementation to a small set of primitives, thereby minimizing the implementation cost for additional devices;</p> <p>3. Has been implemented for binary, grey-scale, and full color raster display systems, and for high resolution black and white printers and color raster printers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>I.4.10</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.2</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010520.10010521.10010542.10011714</concept_id>
				<concept_desc>CCS->Computer systems organization->Architectures->Other architectures->Special purpose systems</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010240.10010241</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Computer vision representations->Image representations</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14143441</person_id>
				<author_profile_id><![CDATA[81100406185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Warnock]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Palo Alto Research Centers, 3333 Coyote Hill Road, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329971</person_id>
				<author_profile_id><![CDATA[81100283995]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Douglas]]></first_name>
				<middle_name><![CDATA[K.]]></middle_name>
				<last_name><![CDATA[Wyatt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xerox Palo Alto Research Centers, 3333 Coyote Hill Road, Palo Alto, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Garey, M. R., D. S. Johnson, F. P. Preparata, and R. E. Tarjan. "Triangulating a Simple Polygon." Information Processing Letters 7, 4 (1978): 175-179.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, D. T., and F. P. Preparata. "Location of a Point in a Planar Subdivision and its Applications." SIAM J. Comput. 6, 3 (1977): 594-606.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Newell, M. E., and C. H. Sequin. "The Inside Story on Self-intersecting Polygons." Lambda 1, 2 (1980): 20-24.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M., and R. F. Sproull. Principles of Interactive Computer Graphics. Second Edition. New York: McGraw-Hill, 1979.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_obj_id>360802</ref_obj_id>
				<ref_obj_pid>360767</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Sutherland, I. E., and G. W. Hodgman. "Reentrant Polygon Clipping." CACM, 17(1):32, January 1974.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Device Independent Graphics hnaging Model for Use with Raster Devices John Warnock and Douglas K. 
Wyatt Xerox Palo Alto Research Centers 3333 Coyote Hill Road Palo Alto, CA 94304 Abstract In building 
graphic systems for use with raster devices, it is difficult to develop an intuitive, device independent 
nmdel of the imaging process, and to preserve that model over a variety of device implementatio.ls. This 
paper describes an imaging model and an associated implementation strategy that: 1. Integrates scanned 
images, text, and synthetically generated graphics into a uniform device independent metaphor; 2. Isolates 
the device dependent portions of the implementation to a small set of primitives, thereby minimizing 
the implementation cost for additional devices; 3. Has been implemented for binary, grey-scale, and full 
color raster display systems, and for high resolution black and white printers and color raster printers. 
 Introduction The work described in this paper is designed for a multi-application programming environment 
where programmers use raster display devices to provide the visual communication links between users 
and systems. The displays are used for simple typescript-style text applications as well as for more 
involved applications requiring drawings, scanned images, and other complex combinations of graphics 
and text: a music composition system, a general window management package for a programming environment, 
a high quality document design system, a VLSI design system, and a graphics arts design package. In an 
environment that supports such diverse graphic user interfaces on a variety of display devices, it is 
desirable to maintain a flexible unified graphics imaging model and an associated programming interface, 
independent of display devices, with which all the application programs can work. This paper describes 
an imaging model and its programming interfaces, discusses the advantages in using such a model, and 
outlines a basic implementation strategy. Permission to copy without fee all or part of this material 
is granted provided that the copies are not made or distributed for direct commercial advantage, the 
ACM copyright notice and the title of the publication and its date appear, and notice is given that copying 
is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires 
a fee and/or specific permission. &#38;#169; 1982 ACM 0-89791-076-1/82/007/0313 $00.75 Raster Devices 
The class of raster devices encompasses a wide range of displays, plotters, and printers. These include 
full color (24 bit per pixel) displays, grey level displays, simple low resolution binary (1 bit per 
pixel) displays, electrostatic plotters, high resolution film recorders, and laser printers. Raster devices, 
because of their potential ability to display a rich set of images, serve as a useful class over which 
to define a device independent programming abstraction. Broadening the class of raster devices to include 
other kinds of graphic devices ~vector drawing displays, pen plotters, or storage tube displays) can 
lead to problems in defining the imaging abstractions. Either the image types become restricted or the 
imaging metaphors become strained and unnatural. For example, the implementation of solid areas on some 
vector drawing devices is impractical. Restricting the set of devices to raster devices allows the imaging 
metaphor to remain simple, consistent, and efficient. Device Independence Device independence, in the 
context of using raster devices, can be defined in several ways depending on the level of abstraction 
desired. One definition dictates that the imaging model provides an abstraction of how an image ideally 
looks on a perfect medium; this model abstracts the appearance of the image. The implementation for each 
specific display must mimic the appearance of an ideal image to the best of its ability. This kind of 
device independence attempts to maintain global image properties in spite of wide variations in display 
type. For example, a device that can show grey values might render the appearance of color values by 
substituting appropriate grey values. A binary device might render colors with stipple patterns that 
give a visual impression of grey values. Other kinds of device independent abstractions can be less strict. 
Image representations may not model image appearance but instead describe some form of information content. 
In this case, the implementation of a given device is only required to convey certain information content 
of the application, and may not be constrained in any way to the appearance of the image. For example, 
a black and white device implementation might choose to display colors with iconic labels rather than 
intensity levels. With this kind of abstraction, few constraints are put onto particular device implementations. 
One consequence of this latter kind of abstraction is that the programmer cannot have precise expectations 
of how a device implementation will attempt to represent images. Device independence benefits the implementors 
of a graphics system as well as its clients, since the bulk of the system can be shared across all devices. 
Only a small portion of the code need be concerned with a specific device type. If the interface to this 
device- specific code is well designed, implementing a new device type requires minimal programming, 
effort. Computer Graphics Volume 16, Number 3 July 1982 f Figure 1: The imaging model. The Imaging 
Modei The imaging model described here is designed for applications related to the typesetting and graphics 
arts industry, where image appearance is vital. For this reason the model abstracts the geometric and 
color properties of an image. In taking this approach, the imaging model makes two value judgements: 
first, that global image fidelity is important: second, that it is valuable for the application to be 
able to rely on a device implementation to render images as accurately as possible. It should be noted 
at the outset that for a number of applications this choice may be inappropriate. The imaging model specifies 
how geometric shapes and colors are combined. It follows a metaphor that loosely corresponds to the procedure 
used by a silk-screen printer: pushing colored ink through a stencil onto paper. The left side of Figure 
1 illustrates this operation. The ink, above, is solid gray; the screen, in the middle, has an a-shaped 
opening; on the paper, below, the result is an a-shaped patch of gray ink. The artist can build a complex 
image by repeating this basic operation with different combinations of screens and inks. Ink laid down 
later may obscure ink laid down earlier. The programming interface presents a similar model. The programmer 
calls a series of procedures to define a stencil, and other procedures to define a source. Each primitive 
display procedure produces on the display the effect of pushing a given source through a given stencil. 
The programmer can build a complex image by calling a sequence of display primitives with different combinations 
of stencils and sources. Stencils may be represented in two forms: shapes and masks. A shape consists 
of a collection of closed piecewise analytic curves (straight lines and parametric cubics); these curves 
represent the outlines of holes in the stencil. A mask consists of a binary two dimensional array; "ones" 
in the array represent holes in the stencil, There is no special representation for text. Characters 
are just letter- shaped stencils, which can be represented either as closed analytic outlines or as masks. 
Sources (inks) may be represented either as single colors or as multi- colored two dimensional sampled 
images. When the source is multi- colored, the imaging model is much more powerful than any analogous 
silk-screening operation. Picture a slide projector shining a general colored image through a stencil 
onto the paper. The fight side of Figure 1 shows the result of pushing a multi-colored source through 
a stencil. Other properties of the imaging model lack good analogies in the silk-screening metaphor. 
The first of these is an additional level of stencil called a clipping region. The purpose of the clipping 
region is to restrict the area where ink is displayed regardless of what other shapes or masks are used. 
When a clipping region is specified, then only ink falling inside that region is displayed. Figure 2 
illustrates the effect of a clipping region. Also unlike anything in the silkscreening process are the 
model's general mapping facilities. Under control of the application, stencils and sources may be mapped 
independently through any linear transformation prior to display. Imagine rotating the stencil~ or stretching 
a rubber stencil to expand or skew it; at the same time, imagine rotating the slide projector, or pulling 
it back to enlarge the image. The mapping facility gives the application program a great deal of flexibility 
in the composition of images. Figure 2: A clipping region. Additional operators generate shapes that 
correspond to drawn lines and curves. These operators take trajectories (piecewise analytic segments, 
open or closed) and brush information (just another shape) and generate closed shapes that correspond 
to lines and curves drawn with the given brush. The resulting lines and curves then act like any other 
stencils, and may have inks pushed throu~a them. This model does not address issues concerning the display 
of projected three dimensional objects, or issues dealing with complex conformal mappings. It is assumed 
that these kinds of objects, if desired, are transformed into appropriate two dimensional imaging constructs 
prior to display. Treating source images in this way gives a pleasing symmetry to the implementation 
as well. The interface to a raster input source provides the source's boundary in the source coordinate 
system, and provides a mapping from the source coordinate system to the virtual coordinate system. The 
interface to a raster output device provides a mapping from the virtual coordinate system to the device 
coordinate system, and provides the device's boundary in the device coordinate system. Given these interfaces, 
the implementation has all the information it needs to map coordinates directly from source to destination, 
and to compute the intersection of their boundaries. The following description of an implementation of 
the imaging model will illustrate how all the above concepts hang together. Interfaces and Implementation 
The following description centers on the interfaces which define the boundaries between program components. 
A carefully designed interface effectively decouples its clients from the internal details of its implementors 
by defining a set of available operations. Each client retains a pointer to the state information needed 
by the implementer, but uses only the interface-defined operations to manipulate that state. The implementation 
outlined here relies heavily on this notion of an interface to achieve device independence. The programmer 
who wants to display or print pictures is a client of the application interface. The operations defined 
in this interface allow the application programmer to construct images by combining various sources and 
stencils. This is the interface that presents the imaging model described above. Equally important, however, 
are the internal interfaces which separate the device-independent components of the implementation, from 
the device-dependent components. These interfaces isolate most of the implementation from the peculiar 
characteristics of different display devices and image sources. Coordinate Systems One of the key ideas 
in making applications independent of devices is defining coordinate systems and isolating them from 
each other. Because pixel addressing conventions vary across different raster devices, it is particularly 
important to isolate the device coordinate system (IX:S) from the application program view of the system. 
To achieve this isolation, the imaging model defines an intermediate coordinate system called the virtual 
coordinate system (vcs). This common coordinate system serves as the meeting ground for device implementations 
and user applications. The system implementor writing the device dependent code for a particular display 
is concerned only with the mapping between the vcs and the Des. The application programmer using the 
imaging operators is concerned only with building images relative to the virtual coordinate system. Another 
aspect of coordinate systems often overlooked in building raster display sytems is the isolation of the 
source coordinate system (SCS) in which sampled image sources are defined. Sampled images are, in some 
sense, dual to display devices. They provide a raster input form for images in the same way that devices 
provide a raster output form for images. The application program should be no more concerned with scanning 
properties or coordinate system particulars of source images than with scanning properties or coordinate 
sytems of display devices; it should deal with images in the virtual coordinate system. To accomplish 
this, the imaging model adopts conventions for source independence analogous to its conventions for device 
independence. Knowing these conventions, the application can predict how images or masks will be mapped 
into the virtual coordinate system. Therefore, the application can manipulate and transform the image 
geometrically without regard to the scanning or resolution properties of the image or mask. Application 
Interface Each application using the imaging model may invoke a collection of imaging operators through 
the application interface. These operators define mappings, clipping regions, shapes, colors, masks and 
image sources, and cause shapes to be displayed. Because of the many differences in languages and operating 
systems, only an indication of the framework of operators is given here. Additions are needed to fill 
in the details for a specific programming environment. The state information associated with an application 
is called the display context. As an application uses the display, the imaging operators use and modify 
the information in the display context. This information includes: 1. An interface to a device. 2. The 
current position (cpx, cpy) in the device coordinate system. 3. The transformation matrix T that maps 
application defined shapes into the device coordinate system. 4. The clipping region (ce).  The notation 
used to indicate the procedure interface is of the form: P roced u reName: PROCEDURE [p 1 : PTypel, p2: 
PType2, ...] RETURNS[r1: RTypel, r2: RType2, ...]; Here it is assumed that each procedure takes input 
parameters (named p7, p2 .... ) of various types, and returns results (named rl, r2 .... ) of various 
types. Some of the type names should be obvious (e.g., Real for floating point numbers); othel~ (e.g., 
Trajectory) are left undefined. For these undefined types, it is assumed that the implementation will 
define an appropriate data structure. The particulars of the data structures chosen are not important 
to this discussion, and need not be known by the application using the interface. The Device and Image 
types hold state information for particular display devices and image sources. The interfaces to devices 
and images will be described below. Note, however, that the application can treat them entirely as "black 
boxes"; it need not know even their interface definitions. New X X X Device: PROCEDURE [<optional parameters>] 
RETURNS[Device]; A procedure of this form is provided by each device implementation. NewXXXlmage: PROCEDURE 
[<optional parameters>] RETURNS [Image]; A procedure of this form is provided by each scanned image 
type. NewDisplayContext: PROCEDURE[device: Device] RETURNS[de: DisplayContext]; Initializes a display 
context. The transformation Tis initialized to the VCS-to- DCS mapping provided by the device. The clipping 
region CR is initialized to be the boundary of the device. The current position (cpx.cpy)isset to 0,0. 
 GetCu rrentPosition: PROCEDURE[dc: DisplayContext] RETURNS [x,y: Real]; Returns x,ysuch that (x~y)T 
= (cpx.cpy). SetCu rrentPosition: PROCEDURE[de: DisplayContext, x,y: Real]; Sets(cpx, cpy) to (x,y)T. 
NewTrajectory: PROCEDURE [x,y: Real] RETURNS [t: Trajectory]; Returns a new trajectory. Every trajectory 
has a first position (FP) and a last position (LP): for a new trajectory, both FP and LP are set to (x.y). 
LineTo: PROCEDURE [t: Trajectory, x,y: Real] RETURNS [U: Trajectory]; Returns a trajectory u that includes, 
in addition to t, the line segment from t's LP to (x,y). The LP ofo is (x,y). Cu rveTo: PROCEDURE [t: 
Trajectory, x 1,y1,x2,y2,x3,y3: Real] RETURNS [U: Trajectory]; Returns a trajectory u that includes, 
in addition to t. a cubic curve segment from t's LP to (x3,g3). The curve segment is defined by its four 
Bezier control points:t's LP. (x 1,yl ), (x2,y2), and(x3,y3). The LP ofu is (x3,y3). Close: PROCEDURE 
[t: Trajectory] RETURNS [U: Trajectory]; Returns a trajectory u that includes, in addition to t, the 
line segment from t's LP to t's PP. The FP and LP ofu are equal. Rectangle: PROCEDURE[xl,yl,xu,yu: Real] 
RETURNS [t: Trajectory]; A convenience function, equivalent to: t *- NowTrajectory[xl,yl]; t *- LineTo[t,xu,yl]; 
t ~- LineTo[t,xu,yu]; t ~ LineTo[t,x/,yu]; t *" Close[t]; NewShape: PROCEDURE RETURNS[S; Shape]; Returns 
an empty shape list. AddToShape: PROCEDURE [S: Shape, t: Trajectory] RETURNS [r: Shape]; Returns a Shape 
r that contains, in addition to the trajectories of s, the trajectory t. MakeLineShape: PROCEDURE[brush: 
Shape, t: Trajectory] RETURNS Is: Shape]; The locus ofeach point interior to the brush is computed as 
the origin of the brush shape is moved along the trajectory. The union of all these loci form a set of 
solid areas. The boundaries of these areas make up a shape. It is this shape that is returned by MakeLineShaoe. 
Note: the above definition is just that, and does not describe how line shapes might really be computed. 
MakeColorSou tee: PROCEDURE [hue,sat,brightness: Real] RETURNS [S.' Source]; Supplies a Source data structure 
representing solid ink of the specified color. MakelmageSou rce: PROCEDURE[image: Image] RETURNS [S: 
Source]; Supplies a Source data structure representing the specified sampled image. DrawShape: PROCEDURE[dc: 
DisplayContext, shape: Shape, source: Source]; Maps the shape and source through the transformation 
T, clips the shape against the CR, and displays the shape with the given source as the ink~ DrawMask: 
PROCEDURE[de: DisplayContext, mask: Image, source: Source]; Maps the mask, its boundary and the source 
through the transformation T. clips the boundary against the CR. and displays the resulting clipped mask 
with the given source as the ink. SetClipShape: PROCEDURE [de: DisplayContext, shape: Shape]; Maps the 
shape throughthe transfoimation T, clips the shape against the CR, and installs the shape as the new 
clipping region. Translate: PROCEDURE[dc: OisplayContext, x,y: Real]; Builds a transformation matrix 
M that will translate (0,0) onto (x,y), and sets T, in the display context, toMT. Rotate: PROCEDURE[de: 
DisplayContext, angle: Real]; Builds a rotation matrix M that will rotate (l,0) onto (cos(ang/e),sin(angle)), 
and sets T, in the display context, to MT Scale: PROCEDURE[de: DisplayContext, sx,sy: Real]; Builds a 
transformation matrix M that will scale (1,1) onto (sx,sy). and sets T, in the display context, to MT 
Concatenate: PROCEDURE[dc: DisplayContext, m: Matrix]; Sets T, in the display context, to m T GetMatrix: 
PROCEDURE[dc: DisplayContext] RETURNS [t: Matrix]; Returns the matrix t such that ifM is the matrix 
that transforms VCS to DCS, then t = TM "1. Because characters of fonts are treated like any other shapes 
in this imaging model, routines to display text do not properly belong in the above set of primitive 
operators. However, since applications often use text and characters extensively, convenience routines 
can be provided to make the display of text simple. A typical set would include: MakeFont: PROCEDURE 
[<font name>] RETURNS [f: Fontld]; DisplayChar: PROCEDURE [c: Character, f: Fontld]; DisplayText: PROCEDURE 
[s: String, f: Fontld]; GetCharMetrics: PROCEDURE [C: Character, f: Fontld] RETURNS [<whatever metrics 
a font provides>]; Device Independent Procedures The routines that implement the imaging model depend 
on a large number of ideas and algorithms. It is not practical to describe these in detail. ]nstead the 
few critical observations and algorithms necessary to get the central ideas of the implementation are 
discussed. To best understand how these ideas work together, one needs to understand several fundamental 
operations found in the DrawShape, DrawMask and SetClipShape procedures. Shape Mapping Shape Clipping 
Given a shape (set of closed trajectories) and a transformation matrix An approximated reduced shape 
will be called clipped if each of its M, we will say that the shape is mapped using M when all points 
and convex polygons has been clipped against the set of the convex cubics that comprise each of the shape's 
trajectories are transformed polygons that make up a clipping region. using hi. Figure 3: Shape mapping. 
Figure 5: Shape clipping. Shape Approxhnation Given a shape (set of closed trajectories) in the DCS we 
will say that the shape is approximated when all cubics, in each of the shape's trajectories, are replaced 
by piecewise linear approximations. Since the shape is in the DCS, it is possible to make piecewise linear 
approximations as a function of device resolution. An approximated shape exists as a collection of polygons, 
which may be mutually intersecting, self intersecting and concave. Shape Reduction An approximated shape 
is reduced when the polygons that make up the shape are converted into a collection of disjoint, convex 
polygons which tile the interior of the shape. The locus of the shape's interior is determined by applying 
a wrap number convention. There are a number of known filing algorithms [1,2,3]. Figure 4: Shape reduction. 
 The above concepts are used freely in the process of displaying a shape. Of equal importance is the 
description of the basic operation that is carried out by the device dependent part of the system. Internal 
Interfaces Just as the application program is a client of the application interface, the device-independent 
portion of the system is a client of the device and source interfaces. Information available at the device 
interface includes: 1. The transformation matrix that maps the virtual coordinate system to the device 
coordinate system. 2. The shape (in the device coordinate system) that bounds the display area. 3. 
A vector of procedures that implement the ~can conversion primitiveS. These are device-specific procedures; 
their calling sequences do not vary from device type to device type, but the way they perform their function 
may vary dramatically across device types.  The source interface is used both for scanned image sources 
and for masks. Information available at the source interface includes: 1. The transformation matrix that 
maps the image or mask coordinate system to the virtual coordinate system. 2. The shape (in the source 
coordinate system) that bounds the image or mask area. 3. A vector of procedures that implement the 
source accessing primitives. These are source-specific procedures.  The operations defined by these 
interfaces are called as required by the device independent portions of the system. Some examples of 
their use are illustrated below.  Device Dependent Procedures Scan Conversion Strictly speaking, a given 
device implementation need supply only one procedure, DisplayConvexPolygon. This procedure implements 
the most general case of scan conversion that the device must handle: pushing a general mapped scanned 
image, as a source, through a mapped mask, The arguments taken by the procedure are; 1. A convex polygon. 
This polygon represents either part of a shape (if no mask is given), or the boundary of a mask (if a 
mask is given). 2. A source, which is either a constant value, or: a. A mapping S from the source's 
scs to the DCS, and b. A pointer to the source sample array.  3. An optional mask which includes: 
 a. A mapping M from the mask's SCS to the rxc's, and b. A pointer to the mask sample array.   The 
operation carded out by this procedure is: For each pixel position (x,y) in the interior of the convex 
polygon in DCS, compute (Xs.Ys) = (x,y)S "l, and (Xm,Ym) = (x,y)M "1. If the value in the mask array 
at (Xm,Ym) =1, then interpret the source value at (xs, ys) for the device type and display an appropriate 
value at (x,y). Note: in practice, instead of mapping each point in DES through two inverse mappings 
(computationally expensive), incremental mapping techniques are used; in this way each mapping is replaced 
with two add operations. This is discussed later in the section on optimizations. In most implementations 
of a given device, special cases that are expected to occur frequently can be provided as subcases of 
the above. Two common special cases are listed below. 1. DisplayRectangularMask. This procedure is a 
special case ofthe above where: the polygon is rectangular in the DCS, the source is a constant color, 
and the mapping from the SCS to DCS contains only a translation component. This procedure is typically 
used for the display of most characters. 2. DisplaySimplePolygon. This procedure is a special case of 
DisplayConvexPolygon where the source is constant, and  there is no mask. Most line drawings and simple 
filled shapes use this procedure. It can be seen how the aboye device independent and device dependent 
notions are brought together by examining the processing steps associated with the DrawShape, DrawMask 
and SetClipShape procedures. DrawShape (source is a constant color) 1. Map the input shape into the Des 
using T (the transformation from the display context). 2. Approximate the resulting shape, reduce it, 
and clip it to CR (the clipping region from the display context). 3. Pass each resulting convex polygon 
to the DisplayConvexPolygon procedure (found in the device interface in the display contex0, along with 
the constant color value.  DrawShape (source is a scanned image) 1. Concatenate the SCS-to-vcs transfomaation 
Q (from the scanned image interface) with the VCS-to-DCS transformation T (from the. display contex0 
to form the SC'S-to-DCS transformation s (s = gT). 2. Map the shape that bounds the scanned image (from 
the scanned image interface) into the DCS, using S. 3. Approximate the resulting shape, reduce it, and 
clip it to CR. 4. Use the resulting set of convex polygons to define a new clipping region, cg: 5. 
Map the input shape (the argument to DrawShape) into the DCS, using T. 6. Approximate the resulting 
shape, reduce it, and clip it to CR: At this point, the resulting convex polygons represent the intersection 
of the mapped boundary of the image and the mapped input shape. 7. Pass each convex polygon to the DisplayConvexPolygon 
routine, along with transformation S and a pointer to the image samples.  DrawMask (source is a constant 
color) 1. Concatenate the sos-to-yes transformation R (from the mask interface) with the VCS,to-DCS transformation 
T to form the SC-'S- to-DeS transformation M (M = RT). 2. Map the bounding shape of the mask into the 
DCS, using M. 3. Approximate the resulting shape, reduce it, and clip it to CR. 4. Pass each resulting 
convex polygon to the device's DisplayConvexPolygon procedure, along with the constant color value, transformation 
M, and a pointer to the mask samples.  DrawMask (source is a scanned image) 1. Concatenate the scs-to-vCs 
transformation Q (from the scanned image interface) with the vcs-to-I)CS transformation T to form the 
SCS-to-DCS transformation S (S = QT). This mapping transforms coordinates from the scanned image to the 
device. 2. Map the bounding shape of the image into the DCS, using 5'. 3. Approximate the resulting 
shape, reduce it, and clip it to CR. 4. Use the resulting set of convex polygons to define a new clipping 
region, CR'. 5. Concatenate the SCS-to-vcs transformation R (from the mask interface) with the VCS-to-DCS 
transformation T to form the SCS- tO-DCS transformation M (M = RT) This mapping transforms coordinates 
from the mask to the device. 6. Map the bounding shape of the mask into the I~:S, using M. 7. Approximate 
the resulting shape, reduce it, and clip it to CR: 8. Pass each resulting convex polygon to the device's 
DisplayConvexPolygon procedure, along with transformations S and M and pointers to the image and mask 
samples.  SetClipShape 1. Map the input shape into the DCS, using T. 2. Approximate the resulting 
shape, reduce it, and clip it to cg. 3. Install the resulting set of convex, non-intersecting polygons 
as the new CR in the display context.  Note that the DisplayConvexPolygon routine never needs to do 
any boundary checking. When a scanned image or mask is present, the shape to be scan convened always 
represents an interior portion of the image or mask. Also, the clipping region in the display context 
always lies inside the device boundary. Optimizations Although the above algorithms may involve extensive 
computation, certain simple expected cases can be made to short circuit most of the above machinery without 
losing any of the generality. Three of these short cuts will now be described. The display of characters 
In situations where high performance is required, character fonts are designed to work with a given display 
type, i.e., a character is defined to be a mask whose resolution and scanning characteristics are the 
same as the device's. Also the bounding shape of a character mask is a rectangle. Under these circumstances, 
the mapping taking the mask in scs to vcs, when concatenated with the device's mapping from vcs to DeS, 
will be the identity mapping (with, perhaps, an application-introduced translation component). Since 
this is the case, the bounding rectangle of the mask will map into a rectangle in the DCS. If in addition 
the source color is solid black and the bounding rectangle is not clipped, the scan conversion process 
reduces to the one-to-one transfer of pixels in one rectangle to pixels in another. To avoid checking 
for the identity transformation for each character, the combined transformation from sos to vcs to Des 
can be noted as an identity in the display context. If no operations that change transformations (other 
than translation) take place between the display of succesive characters, then no transformation checking 
need be done. If any of the above conditions is not true, then the optimization fails and the more general 
machinery will display the character. Transforming sources and masks The rotation, scaling and sampling 
of sources and masks can be computationally expensive. To reduce this computation the following steps 
are taken (the discussion centers on the handling of sources, but the same discussion applies to masks). 
Scan conversion, in the device coordinate system (DCS), is carried out pixel-by-pixel along successive 
scan lines. If the unit vector (1,0) representing the delta vector between successive pixels in a scan 
line is mapped through S "1 (the mapping from the device coordinate system to the source coordinate system), 
then the delta vector ds between required sample positions in the source is obtained. Successive sample 
positions in the source that correspond to the successive pixels along the scan line can be incrementally 
computed by mapping the beginning of the scan line in the DCS through S "1, and incrementally adding 
ds to obtain the position of the next sample to be used in the scan line. This optimization replaces 
a general point transformation with two additions. Bounding boxes An additional artifact, a bounding 
rectangle associated with a shape, can be introduced after mapping into the DCS. Bounding rectangles 
can be used to short cut the amount of computing done by the shape reduction and clipping machinery: 
i.e., if a bounding rectangle is exterior to the set of clipping regions, then the associated shape need 
not be processed further. On the other hand, if a bounding rectangle is totally within the clipping region, 
then the shape need not be clipped since it is totally within the region. Conclusion A consistent, simple, 
device independent imaging model, and an implementation of the model, provide powerful tools to the application 
programmer. The model presented in this paper, presents a general interface that has been useful over 
a wide range of applications and devices. Users have commented positively on the simplicity of the model 
and its ease of use. The aspects of this model that have been most successful are the following. Treating 
text characters like other graphic objects, and not as low- level, device dependent primitives has been 
a big win. High level font dependent abstractions are kept in the application programs and not in the 
low-level device driving programs. Furthermore, because of optimization of special cases, no significant 
performance loss results from this generality. The generalized clipping facility is valuable. There are 
simple interactive metaphors and complex images that are difficult to build without the clipping facilities. 
In addition to the application advantages, the clipping facilities are used extensively in the implementation 
to avoid bounds testing in the low-level routines. With this model an application does not know the particulars 
of the device, or even whether the device is shared. High level display management facilities can allocate 
a portion of the screen, set the clipping region to that portion, and pass the display context, representing 
the entire "device," to a subapplication. This aspect of device independence simplifies systems integration 
activities. Acknowledgements Many people have contributed ideas relating to the imaging model and its 
implementation. In particular, we would like to thank Martin Newell, Warren Teitelman, and Bob Sproull 
for their valuable contributions and suggestions. Bibliography [1] Garey, M. R., D. S. Johnson, F. P. 
Preparata, and R. E. Tarjan. "Triangulating a Simple Polygon." Information Processing Letters 7. 4 (1978): 
175-179. [2] Lee, D. T., and F. P. Preparata. "Location of a Point in a Planar Subdivision and its Applications." 
SIAM J. Comput. 6, 3 (1977): 594- 606. [3] Newell, M. E., and C. H. Sequin. "The Inside Story on Self-intersecting 
Polygons." Lambda 1, 2 (1980): 20-24. [4] Newman, W. M., and R. F. Sproull. Principles of Interactive 
Computer Graphics. Second Edition. New York: McGraw-Hill, 1979. [5] Sutherland, 1. E., and G. W. Hodgman. 
"Reentrant Polygon Clipping." CACM, 17(1):32, January 1974.  
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801298</article_id>
		<sort_key>321</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>47</seq_no>
		<title><![CDATA[A conceptual model of raster graphics systems]]></title>
		<page_from>321</page_from>
		<page_to>328</page_to>
		<doi_number>10.1145/800064.801298</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801298</url>
		<abstract>
			<par><![CDATA[<p>In this paper we present a conceptual model of raster graphics systems which integrates, at a suitable level of abstraction, the major features found in both contemporary and anticipated graphics systems. These features are the refresh buffer; the image creation (scan-conversion) system; the single address-space architecture which integrates the address space of the refresh buffer with those of the image creation system and the associated general-purpose computer; the RasterOp or BitBlt instructions found in some single address-space architectures; the video look-up table, and refresh buffer to screen transformations. Also included are the major components from the conceptual model of vector graphics systems which are embodied in the ACM/SIGGGRAPH/GSPC Core System. Using the conceptual model as a base, we proceed to sketch out the capabilities we have defined in a substantial addition to the Core System. The capabilities are currently being implemented as part of the George Washington University Core System.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[Composition]]></kw>
			<kw><![CDATA[Look-up table]]></kw>
			<kw><![CDATA[Pixel-matrix]]></kw>
			<kw><![CDATA[Raster graphics]]></kw>
			<kw><![CDATA[RasterOp]]></kw>
			<kw><![CDATA[Refresh buffer]]></kw>
			<kw><![CDATA[View]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.1</cat_node>
				<descriptor>Raster display devices</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.4</cat_node>
				<descriptor>Graphics packages</descriptor>
				<type>S</type>
			</other_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372.10010373</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering->Rasterization</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010583.10010588.10010591</concept_id>
				<concept_desc>CCS->Hardware->Communication hardware, interfaces and storage->Displays and imagers</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010387</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Graphics systems and interfaces</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Design</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>PP14118547</person_id>
				<author_profile_id><![CDATA[81100327050]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Acquah]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Information Science and Technology, Department of Electrical Engineering and Computer Science, School of Engineering and Applied Science, The George Washington University, Washington, D.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39036758</person_id>
				<author_profile_id><![CDATA[81100302796]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Information Science and Technology, Department of Electrical Engineering and Computer Science, School of Engineering and Applied Science, The George Washington University, Washington, D.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP39044874</person_id>
				<author_profile_id><![CDATA[81100481379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sibert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Information Science and Technology, Department of Electrical Engineering and Computer Science, School of Engineering and Applied Science, The George Washington University, Washington, D.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>PP31100370</person_id>
				<author_profile_id><![CDATA[81332535158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Patricia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wenner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Information Science and Technology, Department of Electrical Engineering and Computer Science, School of Engineering and Applied Science, The George Washington University, Washington, D.C.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Acquah, J. B., J. D. Foley, and P. Wenner, "Reference Manual for Advanced Raster Graphics Extensions to ACM/SIGGRAPH Core System." IIST Report 82-15, Department of Electrical Engineering and Computer Science, George Washington University, Washington, D.C., March 1982.]]></ref_text>
				<ref_id>1</ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA["Apollo Domain Architecture," Apollo Computer, North Billerica, MA, February 1981.]]></ref_text>
				<ref_id>2</ref_id>
			</ref>
			<ref>
				<ref_obj_id>909934</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Carlbom, Ingrid B., System Architecture for High-Performance Vector Graphics, Ph.D. thesis, Dept. of Computer Science, Brown University, Providence, R.I., 1980.]]></ref_text>
				<ref_id>3</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807505</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Catmull, E. and A. R. Smith. "3-D Transformations of Images in Scanline Order," SIGGRAPH '80 Proceedings, published as Computer Graphics, 14(3), July 1980, pp. 279-285.]]></ref_text>
				<ref_id>4</ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Clard, D. W., B. W. Lampson, K. A., Pier, "The Memory System of a High-Performance Personal Computer." IEEE Transactions on Computers, October 1981, p. 715.]]></ref_text>
				<ref_id>5</ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Driscoll, T., and C. Walker, "The Evolution of Image Processing Technology." Computer Graphics World, June 1981, pp 29.]]></ref_text>
				<ref_id>6</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807420</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D., J. Templeman, and D. Dastyar, "Some Raster Graphics Extensions to the Core System," SIGGRAPH '79 Proceedings, published as Computer Graphics, 13(2), August 1979, pp. 15-24.]]></ref_text>
				<ref_id>7</ref_id>
			</ref>
			<ref>
				<ref_obj_id>6684</ref_obj_id>
				<ref_seq_no>8</ref_seq_no>
				<ref_text><![CDATA[Foley, J. D., and A. van Dam, "Interactive Computer Graphics," Addison-Wesley, Reading, MA, 1982.]]></ref_text>
				<ref_id>8</ref_id>
			</ref>
			<ref>
				<ref_seq_no>9</ref_seq_no>
				<ref_text><![CDATA["Gino-F User Manual," Issue 2, Computer-Aided Design Centre, Cambridge, England, December 1976.]]></ref_text>
				<ref_id>9</ref_id>
			</ref>
			<ref>
				<ref_seq_no>10</ref_seq_no>
				<ref_text><![CDATA["Status Report of the Graphics Standards Planning Committee," Computer Graphics, 11, 1977.]]></ref_text>
				<ref_id>10</ref_id>
			</ref>
			<ref>
				<ref_obj_id>988498</ref_obj_id>
				<ref_obj_pid>988497</ref_obj_pid>
				<ref_seq_no>11</ref_seq_no>
				<ref_text><![CDATA["Status Report of the Graphics Standards Committee," Computer Graphics 13(3), August 1979.]]></ref_text>
				<ref_id>11</ref_id>
			</ref>
			<ref>
				<ref_seq_no>12</ref_seq_no>
				<ref_text><![CDATA[Ingalls, D., "The Smalltalk Graphics Kernel," special issue on Smalltalk, BYTE, 8(8), August 1981.]]></ref_text>
				<ref_id>12</ref_id>
			</ref>
			<ref>
				<ref_seq_no>13</ref_seq_no>
				<ref_text><![CDATA["3400 Reference Manual," Lexidata Corporation, 755 Middlesex Turnpike, Billerica, MA, 1981.]]></ref_text>
				<ref_id>13</ref_id>
			</ref>
			<ref>
				<ref_seq_no>14</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. and R. F. Sproull, "An Approach to Graphics System Design," Proc. IEEE, 62(4), April 1974, pp. 571-483.]]></ref_text>
				<ref_id>14</ref_id>
			</ref>
			<ref>
				<ref_obj_id>5532</ref_obj_id>
				<ref_seq_no>15</ref_seq_no>
				<ref_text><![CDATA[Newman, W. M. and R. F. Sproull, "Principles of Interactive Computer Graphics," 2nd ed., McGraw-Hill, New York, 1979.]]></ref_text>
				<ref_id>15</ref_id>
			</ref>
			<ref>
				<ref_seq_no>16</ref_seq_no>
				<ref_text><![CDATA[PERQ, Three Rivers Computer Corporation, Pittsburgh, PA, 1980.]]></ref_text>
				<ref_id>16</ref_id>
			</ref>
			<ref>
				<ref_seq_no>17</ref_seq_no>
				<ref_text><![CDATA["RM-9400 Series Graphic Display System Software Reference Manual," Ramtek Corporation, 2211 Lawson Lane, Santa Clara, CA, 1979.]]></ref_text>
				<ref_id>17</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807418</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>18</ref_seq_no>
				<ref_text><![CDATA[Shoup, R., "Color Table Animation," SIGGRAPH '79 Proceedings, published as Computer Graphics, 13(2), August 1979, pp. 8-13.]]></ref_text>
				<ref_id>18</ref_id>
			</ref>
			<ref>
				<ref_seq_no>19</ref_seq_no>
				<ref_text><![CDATA[Sloan, K. and C. Brown, "Color Map Techniques," Computer Graphics and Image Processing, 10(4), August 1979, pp. 297-317.]]></ref_text>
				<ref_id>19</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807428</ref_obj_id>
				<ref_obj_pid>800249</ref_obj_pid>
				<ref_seq_no>20</ref_seq_no>
				<ref_text><![CDATA[Sproull, R. F., "Raster Graphics for Interactive Programming Environments," SIGGRAPH '79 Proceedings, published as Computer Graphics, 13(2), August 1979, pp. 83-93.]]></ref_text>
				<ref_id>20</ref_id>
			</ref>
			<ref>
				<ref_seq_no>21</ref_seq_no>
				<ref_text><![CDATA[Teitelman, W., "A Display-Oriented Programmer's Assistant," Proceedings 5th International Joint Conference on Artificial Intelligence, 1977, pp. 905-915.]]></ref_text>
				<ref_id>21</ref_id>
			</ref>
			<ref>
				<ref_seq_no>22</ref_seq_no>
				<ref_text><![CDATA["4112 Computer Display Terminal," Tektronix Inc., P.O. Box 500, Beaverton, Oregon, 1981.]]></ref_text>
				<ref_id>22</ref_id>
			</ref>
			<ref>
				<ref_seq_no>23</ref_seq_no>
				<ref_text><![CDATA[Thacker, C. P., E. M. McCreight, B. W. Mapson, R. F. Spruoll, and D. R. Boggs, "Alto: A Personal Computer," in D. Siewiorek, G. Bell, and A. M. Newell, Computer Structures: Readings and Examples, 2nd ed., McGraw-Hill, 1981.]]></ref_text>
				<ref_id>23</ref_id>
			</ref>
			<ref>
				<ref_obj_id>563878</ref_obj_id>
				<ref_obj_pid>563858</ref_obj_pid>
				<ref_seq_no>24</ref_seq_no>
				<ref_text><![CDATA[van den Box, J., L. C. Caruthers and A. van Dam, "GPGS: A Device-independent General Purpose Graphic System," SIGGRAPH '77 Proceedings, published as Computer Graphics, 11(2), Summer 1977, pp. 112-119.]]></ref_text>
				<ref_id>24</ref_id>
			</ref>
			<ref>
				<ref_obj_id>807506</ref_obj_id>
				<ref_obj_pid>800250</ref_obj_pid>
				<ref_seq_no>25</ref_seq_no>
				<ref_text><![CDATA[Weiman, C., "Continuous Anti-Aliased Rotation and Zoom of Raster Images," SIGGRAPH '80 Proceedings, published as Computer Graphics, 14(3), July 1980, pp. 286-293.]]></ref_text>
				<ref_id>25</ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computer Graphics Volume 16, Number 3 July 1982 A Conceptual Model of Raster Graphics Systems James 
Acquah James Foley John Sibert Patricia Wenner Institute for Information Science and Technology Department 
of Electrical Engineering and Computer Science School of Engineering and Applied Science The George Washington 
University Washington, D.C. 20052 Abstract In this paper we present a conceptual model of raster graphics 
systems which integrates, at a suitable level of abstrac- tion, the major features found in both contemporary 
and antici- pated graphics systems. These features are the refresh buffer; the image creation (scan-conversion) 
system; the single address-space architecture which integrates the address space of the refresh buffer 
with those of the image creation system and the associated general-purpose computer; the RasterOp or 
BitBlt in- structions found in some single address-space architectures; the video look-up table, and 
refresh buffer to screen transformations. Also included are the major components from the conceptual 
model of vector graphics systems which are embodied in the ACM/SIGGGRAPH/GSPC Core System. Using the 
conceptual model as a base, we proceed to sketch out the capabilities we have defined in a substantial 
addition to the Core System. The capabilities are currently being implemented as part of the George Washington 
University Core System. Keywords: raster graphics, refresh buffer, look-up table, pixel- matrix, view, 
composition, RasterOp. CR Categories: 1.3.1, 1.3.3, 1.3.4 This work was partially sponsored by NASA Langley 
Research Center Grant NAG-l-185, Department of Energy Office of Basic Energy Science Grant DE- AS05-ER10521, 
and United States Geological Survey Contract 14-08-0001-20448. Permission to copy without fee all or 
part of this material is granted provided that the copies are not made or distributed for direct commercial 
advantage, the ACM copyright notice and the title of the publication and its date appear, and notice 
is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, 
or to republish, requires a fee and/or specific permission. (~ 1982 ACM0-89791-076-1/82/007/0321 $00.75 
 1. Introduction Raster graphics has rapidly emerged in the past several years as the dominant graphics 
system technology. Many raster systems have been developed, each with a slightly different set of features 
and each supported by manufacturer-provided, architec- ture-specific software. A few architecture-independent 
abstrac- tions have been developed, but none are sufficiently robust to span the entire gamut of system 
features and thus to form the basis for an overall conceptual model of raster graphics systems. Yet such 
abstractions and models are needed to place existing and new systems into context, to aid in pedagogy, 
and to form the base for developing higher-level, architecture-independent graphics subroutine packages. 
The development of abstractions for raster graphics very much parallels developments in other computer 
science domains. For example, in the evolution of programming languages, FOR- TRAN represented the first 
abstraction of the von Neuman archi- tecture, while Ada and Pascal represent more refined contem- porary 
abstractions. Existing graphics abstractions include the graphics system models in [NEWM74], device-independent 
graphics subroutine packages such as GPGS [VAND77], GINO [GINO76], and the Core System [GSPC77, GSPC79], 
and Carlbom's abstraction of high performance vector graphics systems [CARLS0]. The original set of vector 
graphics abstrac- tions found in the Core System has already been extended to deal with a few raster 
system capabilities [FOLE79, GSPC79]. The extensions were carefully designed to be functionally compatible 
with the original Core, so that essentially the same image, except for color and the obscuring of filled 
areas, could be displayed on either a vector or raster display. This meant that several important features, 
unique to raster graphics, were not included in the extensions. In this paper we develop additional abstrac- 
tions to fill this void. Why do we emphasize merging the abstractions of a vector graphics-based package 
such as the Core System with new raster abstractions? Because most applications of raster graphics sys- 
tems are in fact driven by a vector-based application data struc- ture, in the traditional textbook sense 
[FOLE82, NEWM79], and hence need the general types of capabilities found in the Core System. Certain 
raster applications (interactive painting, sometimes page make-up) have only a raster-based application 
data structure. The proposed abstractions have been developed so that the implementers of such raster 
applications need be aware only of the raster abstractions. What capabilities found in raster graphics 
systems require integration into an extended set of abstractions? There are sev- eral, discussed in detail 
in Section 2 of this paper. The first is a 2D array of pixel values called a 'pixel matrix,' {also called 
forms [INGA81], and rasters [NEWM79]. The pixel matrix is most com- monly embodied as the refresh buffer 
of the traditional raster display. The second feature is the integration of the three individ- ual address 
spaces of the refresh buffer, scan-conversion process, and application process into a single address 
space. The third feature, found primarily in such single address-space architec- tures, is the BitBlt 
[INGA81] {also [NEWM79] called RasterOp) command for combining pixel matrixes in powerful ways. Another 
feature is the video look-up table, useful not only for color changes but also for animation. The final 
feature is the video-rate transformation of the refresh buffer to the display, useful for animation as 
well as for panning, zooming, and poten- tially also for rotation. This present work clearly draws on 
previous vector graphics abstractions, and also on the key development of the BitBlt oper- ation at Xerox 
PARC. Hence we view our work as a blending together of both existing and new elements into a cohesive 
whole. 2. Overview of Raster Architecture In raster graphics, an image to be displayed is defined in 
a raster {scan-line) format and is mapped in some manner to a view- surface. With raster film recorders, 
the mapping is often a scan- line at a time. With some simulation systems, the mapping is done a pixel 
at a time as real-time hidden surface removal and scan conversion is performed. In the vast majority 
of interactive raster systems, the image is stored in a raster refresh buffer from which it is displayed 
at video rates. A typical refresh buffer con- sists of n planes of memory which can be used to hold a 
single image with 2**n color values. Alternatively, the buffer may be partitioned into groups of planes 
so that each group holds an image with a reduced color resolution. In the limiting case, each plane holds 
an independent image of only two colors. Figure 1 illustrates three of the many ways to partition the 
planes of an 8-plane {8 bits/pixel) refresh buffer. j1 ONE CROUP: 3 CROUPS: GROUPS: 8 PLANES DEEP 2 PLANES 
(4 COLORS) PLANE EACH (256 COLORS) 3 PLANES (8 COLORS) (2 COLORS EACH) 3 PLANES (8 COLORS) Figure 1. 
Frame Buffer Partitioning by Planes Raster display devices have traditionally been designed as peripherals, 
loosely coupled to a general purpose host computer, as shown in Figure 2. In these systems the application 
program executes on the host computer, and the resulting image data is transferred to the graphics device, 
either via high-speed DMA link or lower-speed serial link, for scan conversion by the image creation 
system and subsequent display. The graphics processor cannot access the host computer memory, while the 
host pro- cessor can access refresh memory on the raster device only by I/O commands. HOST HOST IMAGE 
IMAGE IMAGE VlEWSURFACE CPU MEMORY CREATION MEMORY DISPLAY SYSTEM SYSTEM Figure 2. Another important 
feature of many raster displays is the look-up table, or LUT (also called a color map or video look-up 
table). The LUT, which is part of the image display system in Figure 1, allows deferred binding of pixel 
values to displayed colors: each pixel value indexes an entry in the LUT, which in turn contains an encoding 
of the color to be displayed for that value, as in Figure 3. In addition to the customary advantages 
of deferred binding, the LUT allows multiple images stored on separate planes of the refresh buffer to 
be displayed individually or as brief animation sequences, displayed in fade-in, fade-out se- quences, 
displayed with different priorities for visibility, and combined in arbitrary ways [FOLE82, SLOA79]. 
In addition, look-up table animation can be used to animate cyclic {repeating) sequences if the elements 
of the animation displayed at each step are disjoint one from another in the refresh buffer [SHOU79]. 
Systems without the LUT, called direct-color systems, have a fixed association between pixel values and 
the resulting color. Direct color is found in some very low-cost systems, and in sys- tems designed for 
simultaneous display of a large number (at least several thousand) of different colors. REFRESH BUFFER 
VIEWSURFACE Figure 3. Many raster systems allow a video transformation to be per- formed on the image 
defined in the refresh buffer en route to its display on the screen. This transformation does not modify 
the stored image and generally includes a scaling (zoom) of a section of the image, as well as the ability 
to translate (pan)over the full refresh buffer. Amongst other uses, this feature can be used for short 
animation sequences using images stored in different areas of the buffer--see Figure 4. Partitioning 
by planes and areas can be combined to obtain even more images, but at the cost of both spatial and color 
resolution--see Figure 5. In some cases the re- fresh buffer contains more pixels than can be displayed 
at once, so that panning must be used simply to examime the entire image [LEXI81, APOL81]. Some systems, 
such as the Ramtek 9400, allow different transformations to be applied to different memory planes [RAMT81]. 
Prototype video rotation systems have also been demonstrated [CATM80, WEIM80], but are not yet com- mercially 
available. Figure 4. Frame Suffer Partitioning by Areas Figure 5. Frame Buffer Partitioning by Planes 
and Areas An important emerging raster architecture places the refresh buffer within the memory space 
of the host computer, as shown in Figure 6, thus eliminating the link in Figure 1. Such a shared- address 
space can simplify programming and enhance perfor- mance by allowing the refresh buffer to be manipulated 
directly rather than indirectly as in the traditional system of Figure 2. This is particularly true if 
addressing modes are provided that enable sequential 1-D addresses in memory to be treated as 2-D address 
spaces. Furthermore, if refreshing can start anywhere in the address space, then an image can be built 
in one part of mem- ory while a completed image is displayed from another part. Images can also be built 
in several arbitrarily sized pieces, and combined in another area to form the final image. Such images 
can be modified by selectively modifying the component pieces and recombining them. We believe that this 
single address-space architecture will ultimately come to dominate raster graphics. Examples of systems 
with this architecture are the APPLE, TRS-80, APOLLO [APOL81], PERQ [PERQ81], STAR [SEYB81], DORADO [CLAR81], 
and ALTO [THAC81]. (In some of these systems the buffer is a separate physical address space but is memory-mapped 
onto the main address space.} The more powerful single address-space systems normally have some form 
of RasterOp (also called BitBlt) instruction. De- veloped originally at Xerox PARC and also described 
in [NEWM79], RasterOp operates on two or three two-dimensional arrays of pixel values in the single address 
space. Typical opera- tions are transferring pixel values from a source array to a desti- nation array, 
or logically combining (using any of the 16 boolean functions of two variables} the source and destination 
in the des- tination array (one of the 16 functions actually reduces to the copy operation}. Some versions 
of RasterOp also allow arithmetic operations between the source and destination. RasterOp is a very versatile 
way to combine multiple images, display menus, and take advantage of the general capabilities of raster 
systems. The "Programmer's Assistant" of Tietelman [TEIT77] is an early example of the power of RasterOp, 
while the Xerox STAR [SEYB81] is a more recent example. 3. The Conceptual Model of Raster Graphics Systems 
The model which we will now describe captures the wide range of capabilities described in the previous 
section. We begin with a brief high level overview of the structure of the model, (Figure 7) then proceed 
to a more detailed definition of its compo- nents (summarized in table 1). In Figure 7 we see that output 
primitives are passed through a window to viewport transforma- tion into one of several normalized device 
coordinate (NDC) spaces. Each primitive in NDC space is then scan converted into a pixel matrix. The 
application program can also read/write arbi- trary parts of pixel matrixes. RasterOps may be performed 
to transfer parts of one pixel matrix to another, or to a different area in the same pixel matrix. A 
view maps selected pixels from a pixel matrix window to a screen viewport on the viewsurface, where they 
are combined according to a composition rule and displayed. The concepts of a 'window' on a world coordinate 
space and a 'viewport' in a normalized device coordinate space are exactly the same as in the conceptual 
model of vector graphics [GSPC77]. The raster model also includes all the functions of the initial raster 
extensions [GSPC79] with one minor modification: whereas in the current Core system, output primitives 
that have passed through the viewing pipeline are scan-converted and dis- played directly; they are now 
scan-converted into an intermediate 'pixel matrix'. A pixel matrix is a two-dimensional array of pixel 
values. Height, width and bits/pixel are static attributes of a pixel matrix, defined when it is created 
and remaining unchanged until it is deleted. A distinct pseudo-display file as in the Core system is 
associated with each pixel matrix and contains the out- put primitives, arranged by segments that are 
scan converted into the pixel matrix. HOST SHARED IMAGE VIEWSURFACE CPU MEMORY DISPLAY SYSTEM  (World) 
windows, (NDC) viewports  One or more pixel matrixes  An index table for each pixel matrix  RasterOp 
 Pixel-matrix-windows, screen viewports, views  Logical device coordinates  View-composition, composition 
frames  Viewsurfaee  Figure 6.  Table 1. Major Components of the Conceptual Model. 323 Associated 
with each pixel matrix is an 'index table' which specifies the color or intensity corresponding to each 
possible pixel value in that pixel matrix. Thus the same pixel value in dif- ferent pixel matrixes can 
represent a different color in each of them. In addition, each pixel matrix has dynamic attributes which 
specify individual transparency values, and erasability. The transparency of a pixel value indicates 
that pixels with that value are not to be displayed, and thus do not obscure other pixels they overlap. 
Any number of pixel values may be trans- parent. The erasability attribute'is used to allow a pixel matrix 
to be unaffected by a newframe. 'RasterOps' are defined as operations on pixel matrixes. A RasterOp moves 
an artibrary rectangular array of pixels from a specified location in a source pixel matrix to a location 
in a desti- nation pixel matrix. The source and destination may be the same pixel matrix. Rotation and 
scaling transformations may be applied as part of RasterOp and scaling transformations may be applied 
as part of RasterOp as between pixel matrixes b and a in Figure 7. An application program may transfer 
pixels directly between a pixel matrix and an appropriately formatted 2-D array of pixel values, thus 
entirely bypassing the viewing process and segmented pseudo-display file of the vector graphics systems. 
Pixels are rendered in temporal priority and may overwrite images already in the pixel matrix. A window 
defined on the world and an associated viewport defined on NDC space specify a viewing transformation 
through which real objects are mapped into NDC space. In the same way, a 'pixel matrix window' and 'screen 
viewport' define another transformation called a 'view' which maps pixel matrix images onto the viewsurface, 
as in Figure 7. All parts of a pixel matrix appearing in the pixel matrix window are mapped into the 
screen viewport. Any number of views may be defined on a pixel matrix. Because a view is defined as a 
transformation from one raster co- ordinate space (the pixel matrix) to another (the viewsurface), the 
pixel matrix window and screen viewport are defined in logical raster coordinates as are the pixel matrixes 
and viewsurface. The resolution of the viewsurface in logical raster coordinates is set by the application 
program at initialization time, and the map- ping to physical device coordinates is performed, if needed; 
a uniform integer replication to use the maximum screen area. The use of logical device cqordinates for 
both pixel matrixes and view- surfaces allow a programmer to work entirely with raster graphics concepts, 
if the Core System capabilities themselves are unneeded. INDEX TABLE A PIXEL_MATRIX A m ~,. "r "'~" 
I VIEW 1 I NDC-SPACE A SCREEN_VIEWPORT 3 PIXEL_MATRIX B i i II 1 I I NDC-SPACE B FROM APPLICATION PROGRAM~ 
FROM VIEWING I "~,~" ~ PROCESS L-I I "-x I L.../J NDC-SPACE C ~, SCAN r-. -,I- - ~ I i L.4-.J ~ VIEW 
2 .... J j PIXEL_MATRIX C VIEW 3 .... -i--~ i r.-..-I L.t--.J L ...... .I,~, PIXEL_MATRIX_WINDOW 3 
RASTEROP VIEWS COMPOSITION CONVERSION Figure 7. Conceptual model Computer Grapl~ics The final visible 
picture on the viewsurface is a combination of some or all of the views and is created by a process called 
'view composition.' A composition is specified by a 'composition frame' and a 'composition expression.' 
The composition frame is a rec- tangular region defined on the view-surface. Only those views or parts 
of views that fall within a composition frame are composed using its composition expression. The composition 
expression de- fines how several overlapping images from different views are to be rendered on the viewsurface. 
The composition expression includes conditional, boolean and arithmetic operators on any number of pixel 
matrixes and is applied to all non-transparent pixel values. The full set of 16 boolean operations is 
supported. Thus, for example, the final picture may by the sum of all images in a composition frame, 
or it may be an arrangement with higher- priority images obscuring lower-priority ones. Any number of 
composition frames may be in effect at a time. Where composi- tion frames overlap, the visible images 
are in strict temporal priority of composition frame creation--the most recently cre-ated takes precedence. 
Similarly, although the aspect ratios of composition frames or their composition-rules may be changed 
arbitrarily, the more recently modified compositions will have priority over earlier ones. I I L COMPOSITION--FRAME 
A .J I I I I L COMPOSITION-FRAME B ..I I I i ! / L. ! ! ! ~..I SCREEN--VlEWPORT 1 SCRE£N--VlEWPORT 2 
VIEWSURFACE I I I I! ..1 Figure 8. Volume 16. Number 3 July 1982 As an example, consider the two composition 
frames and the two screen viewports shown in Figure 8. The clear areas in both views represent transparent 
pixels. The image on the viewsurface is the result of composing the images in screen viewports 1 and 
2 under the following simple composition rules: 1. In composition frame A, the image in screen viewport 
1 is logically OR'ed with the image in screen viewport 2. 2. In composition frame B, the image in screen 
viewport 2 re- places the image in screen viewport 1.  The second rule, being more recently defined 
has temporal priority. In summary, the conceptual model of raster graphics intro- duces multiple pixel 
matrixes with associated index tables, RasterOp operations on them and new, raster oriented, viewing 
transformations called views which can be combined to form a final image on the viewsurface. Detailed 
functional specifications for the entire conceptual model can be found in [ACQU82]. 4. Compatibility 
of Conceptual Model with the Core System We have attempted to maintain a high level of compatibility 
between our model and the current Core System specification. The output process is essentially the same 
with the only change being explicit recognition of an intermediate step {i.e. the pixel matrix) between 
NDC space and the viewsurfaee. All additional raster-oriented output functions operate in this post-NDC 
environment and do not conflict with the vector graphics part of the Core System. For example, although 
a RasterOp modifies data in a pixel matrix, the result is not considered to be a new output primitive. 
Images created by RasterOps are analogous to temporary segments in that a new frame erases them on pixel 
matrixes that are currently erasable. Input processes are more affected by the new model. On raster devices, 
both the physical pick and loeator functions usu- ally work by specifying a pixel location. It is then 
necessary to correlate that location with either a location in NDC space (loca- tor) or a segment name 
and pick id (pick). When a composition has occurred or when multiple screen viewports overlap, it is 
quite possible for a pixel on the screen to represent different loca- tions in several pixel matrixes 
and hence in several NDC spaces. To guarantee correct results for the current core input func- tions, 
there is an input-priority list for views. When an input action is incident on more than one view, only 
information corres- ponding to the one with the highest priority is returned. We have also provided two 
new raster input functions to take advantage of the additional facilities provided by the new model. 
The first, RASTER LOCATOR, returns logical-raster-coordi-nates as well as a view-name and pixel-matrix-names 
for all views that the locator is incident upon. The second, RASTER STROKE is exactly analogous to the 
current stroke except that it returns logical-raster-coordinates rather than NDC coordinates. A third 
function CORRELATE PICK is not an input func- tion, but it can be used in conjunction with RASTER LOCATOR 
to simulate a more extensive pick function. The application pro- gram passes a location in logical raster 
coordinates for a specific pixel matrix to the correlate pick function, and a segment name and pick id 
are returned if the location corresponds to a segment in the NDC space associated with the pixel matrix. 
 5. Implementation Considerations The idea of an abstraction implemented by underlying soft- ware and 
hardware is a powerful tool because it enables the pro- grammer to think in terms of high-level device-independent 
con- structs and operations. However, for efficient implementation of the abstraction, the objects and 
capabilities defined in the model must reasonably approximate the available hardware features so that 
the software mapping from abstraction to hardware is minimal. In this section we first outline how the 
conceptual model maps onto the various raster features discussed in Section 2, and then go on to discuss 
various implementation strategies. Note that multiple implementation strategies exist for many of the 
abstractions. If need be, software implementations can always be used. The operation of LUT composition 
is illustrated in Figure 9. Suppose a pixel at [a] in pixel matrix [1] with value [ia] is to be composed 
with a pixel at [b] in pixel matrix [2] with value [ib], into the refresh buffer at location [c] where 
there is a pixel of value [ic] corresponding to the composition of [ia] and [ib]. Then the value [va] 
in Index Table 1 at [ia], is combined with the value [vb] in Index table 2 at [ib] (as specified by the 
composi- tion expression), to produce a result [vc] which is written into the LUT at [ic]. Thus whenever 
the refresh system refreshes any pixel with value [ic] through the LUT, it automatically produces the 
correct color. Because LUT composition does not modify the pixel matrix or refresh buffer, but only the 
(much smaller) LUT, it is fast (video rates), and reversible. Note that LUT composition is performed 
by the graphics package and relieves the application programmer from the drudg- ery of manipulating video 
look up table indexes directly. The explicit RasterOp capability in the model can of course be implemented 
using a hardware RasterOp. A limited RasterOp (i.e., a move) is found on some traditional systems, but 
a general software implementation without an actual RasterOp instruction will always be relatively slow 
compared to the hardware-based implementation. A pixel matrix readily maps either onto main memory of 
a single address-space system or onto the refresh buffer of a tradi- tional-architecture system. In the 
latter case, the refresh buffer may be subdivided as in Figures 1, 4, and 5, if the requisite addi- tional 
capabilities are also available: the LUT for partitioning by planes, and the buffer-to-screen video transformation 
for parti- tioning by area. The pixel matrix window to screen viewport mapping can be implemented in 
the traditional architecture by using a separate memory plane to store the pixel matrix window as a filled 
rec- tangle, loading the LUT to mask the pixel matrix (which is just one or more additional planes) outside 
the window rectangle, and using the buffer-to-screen video transformation to place the con- tents of 
the screen viewport. In the single address-space system, RasterOp instructions can be used to achieve 
the same effect. Similarly, the composition capability embodied in the compo- sition frame and composition 
rule is implemented either using the LUT or software simulation. In the LUT case, the composition frames 
are themselves represented as different pixel values in m planes of the refresh buffer, allowing up to 
2**m-1 frames plus the background area covered by no frames (m is implementation- specific). VIDEO LOOK-UP 
TABLE PIXEL MATRIX 1 PIXEt MAIRIX 2 REFRESH BUFFER Figure 9. LUT-composition We see, then, that the 
individual elements of the conceptual model can be mapped onto the various capabilities found in con- 
temporary raster systems. One might, however, desire to define a partial implementation of the conceptual 
model to closely match a particular hardware environment or to meet the needs of a specific application 
without incurring the costs of unneeded capa- bilities. It is reasonable to prune the conceptual model 
in the fol- lowing areas: 1. Number of pixel matrixes: --one --more than one  2. Number of views per 
pixel matrix:  one --more than one 3. Types of transformations in a view: --the "easy" integer transformations 
of translate and in- teger scale, often implemented in hardware and also im- plementable with RasterOp 
--the "easy" transformations plus rotation in multiples of 90 degrees --both of the above, plus the "hard" 
transformations (not yet found in hardware) of continuous translation, rota- tion, and scale. 4. Independence 
of transformations among views: --all transformations the same (this facilitates implemen- tation using 
the buffer-to-screen video transformation) --independent transformations 5. Types of operations provided 
in a composition rule: --no composition rule allowed (i.e., no overlapping screen viewports) --replace 
only --replace plus boolean operations --replacement, boolean, and arithmetic operations 6. Types of 
operations provided by RasterOp --none (i.e., no RasterOp) --replace only --replace plus boolean operations 
--replace, boolean, and arithmetic operations   Not all of the possible combinations of these various 
options are useful. The useful combinations can be grouped into several sets of capabilities that can 
readily be mapped onto classes of raster devices. The capability sets are: Set 1: -- 1 pixel matrix --1 
view, identify transformation -- no composition --no RasterOp Set 2: --multiple pixel matrixes --1 view 
per pixel matrix, with integer translation and in- teger scale --no RasterOp -- full composition Set 
3: --multiple pixel matrixes --1 view per pixel matrix, with differen¢ integer transla- tions and integer 
scales -- no RasterOp --full composition Set 4: --multiple pixel matrixes -- multiple views per pixel 
matrix, with independent inte- ger translate and integer scale and 90 degree rotation -- full composition 
-- boolean RasterOp Set 5: -- multiple pixel matrixes --multiple views/pixel matrix (independent, continuous 
rotation, scale, and translation} -- full composition --RasterOp The current raster extensions to the 
Core and raster film record- ers correspond to capability set 1, while many traditional raster systems 
correspond to capability set 2. For these sytems, each pixel matrix is implemented as a slice of the 
refresh buffer, as in Figure 1. The uniform scale is implemented using the hardware video zoom. The uniform 
translate is implemented by panning over the refresh buffer. If the raster device has independently- 
scrollable memory planes, the system has capability set 3. Nearly all announced or available single address-space 
machines have capability set 4 [APOL81, PERQ80, INGA81]. Future systems with these features as well as 
color 4 video look- up tables continuous scale and rotate will have capability set 5. 7. Conclusions 
The primary assumption underlying this work has been that the existing raster extensions of the Core 
System are inadequate to capture the full capabilities of modern raster graphics sys- tems. In order 
to correct this inadequacy, we have proposed a new conceptual model of raster graphics devices to serve 
as a basis for a revised standard specification. Our approach has been to define a single model which 
includes all raster devices, in con- trast to the earlier approach [GSPC79] of defining a separate model 
for each of several categories of raster devices. The disad- vantage of the earlier approach is that 
it leads to a standards specification based on only the common elements of the models. This leads to 
the exclusion of important raster capabilities such as the refresh buffer. The disadvantage of our single 
model ap- proach is the risk that the model may be too general, and hence unimplementable. Our section 
on implementation strategy shows how the model can be subsetted to match limited hardware capabilities. 
Our work with this model has demonstrated that it is necessary for a raster standard to be somewhat more 
indepen- dent from the vector standard than is currently the case. Our rea- sorting is that certain key 
raster concepts have no real analogs in vector graphics and therefore cannot be captured in the same 
standard. These concepts are: 1. A raster buffer (pixel matrix or refresh buffer} is distinctly different 
from a display file. A raster buffer is inherently two- dimensional and can be conceptually equated to 
a portion or all of the viewsurface. 2. RasterOps are mass memory operations which allow shift- ing 
of pieces of images from one location to another in one or more refresh buffers. 3. The LUT enables 
complex image manipulation operations to be performed at video rates on images stored in the raster buf- 
fers, with the result being a "blend" of the original images.  The model is also useful as a pedagogical 
device. By captur- ing in an integrated whole all of the elements of contemporary raster graphics hardware, 
it can serve as a tool for comparing capabilities, for understanding different architectures and for 
learning how to develop transportable software. ACQU82 APOL81 CARL80 CATM80 CLAR81 DRIS81 FOLE79 
 FOLEY82 References Acquah, J. B., J. D. Foley, and P. Wenner, "Refer- ence Manual for Advanced Raster 
Graphics Exten- sions to ACM/SIGGRAPH Core System." IIST Report 82-15, Department of Electrical Engineering 
and Computer Science, George Washington Univer- sity, Washington, D.C., March 1982. "Apollo Domain Architecture," 
Apollo Computer, North Billerica, MA, February 1981. Carlbom, Ingrid B., System Architecture for High- 
Performance Vector Graphics, Ph.D. thesis, Dept. of Computer Science, Brown University, Providence, R.I., 
1980. Catmull, E. and A. R. Smith. "3-D Transformations of Images in Scanline Order," SIGGRAPH '80 Pro- 
ceedings, published as Computer Graphics, 14(3), July 1980, pp. 279-285. Clard, D. W., B. W. Lampson, 
K. A., Pier, "The Mem- ory System of a High-Performance Personal Com- puter." IEEE Transactions on Computers, 
October 1981, p. 715. Driscoll, T., and C. Walker, "The Evolution of Image Processing Technology." Computer 
Graphics World, June 1981, pp 29. Foley, J. D., J. Templeman, and D. Dastyar, "Some Raster Graphics Extensions 
to the Core System," SIGGRAPH '79 Proceedings, published as Com-puter Graphics, 13{2), August 1979, pp. 
15-24. Foley, J. D., and A. van Dam, "Interactive Computer Graphics," Addison-Wesley, Reading, MA, 1982. 
 GINO76 GSPC77 GSPC79 INGA81 LEXI81 NEWM74 NEWM79 PERQ80 RAMT81 SHOU79 SLOA79 SPRO79 TEIT77 TE KT81 THAC81 
VAND77 WEIM80 "Gino-F User Manual," Issue 2, Computer-Aided Design Centre, Cambridge, England, December 
1976. "Status Report of the Graphics Standards Planning Committee," Computer Graphics, 11, 1977. "Status 
Report of the Graphics Standards Commit- tee," Computer Graphics 13(3), August 1979. Ingalls, D., "The 
Smalltalk Graphics Kernel," special issue on Smalltalk, BYTE, 8(8}, August 1981. "3400 Reference Manual," 
Lexidata Corporation, 755 Middlesex Turnpike, Billerica, MA, 1981. Newman, W. M. and R. F. Sproull, "An 
Approach to Graphics System Design," Proc. IEEE, 62(4), April 1974, pp. 571-483. Newman, W. M. and R. 
F. Sproull, "Principles of In- teractive Computer Graphics," 2nd ed., McGraw- Hill, New York, 1979. PERQ, 
Three Rivers Computer Corporation, Pitts- burgh, PA, 1980. "RM-9400 Series Graphic Display System Software 
Reference Manual," Ramtek Corporation, 2211 Law- son Lane, Santa Clara, CA, 1979. Shoup, R., "Color Table 
Animation," SIGGRAPH '79 Proceedings, published as Computer Graphics, 13(2), August 1979, pp. 8-13. Sloan, 
K. and C. Brown, "Color Map Techniques," Computer Graphics and Image Processing, 10(4), August 1979, 
pp. 297-317. Sproull, R. F., "Raster Graphics for Interactive Pro- gramming Environments," SIGGRAPH '79 
Pro-ceedings, published as Computer Graphics, 13(2), August 1979, pp. 83-93. Teitelman, W., "A Display-Oriented 
Programmer's Assistant," Proceedings 5th International Joint Con- ference on Artificial Intelligence, 
1977, pp. 905-915. "4112 Computer Display Terminal," Tektronix Inc., P.O. Box 500, Beaverton, Oregon, 
1981. Thacker, C. P., E. M. McCreight, B. W. Mapson, R. F. Spruoll, and D. R. Boggs, "Alto: A Personal 
Computer," in D. Siewiorek, G. Bell, and A. M. Newell, Computer Structures: Readings and Examples, 2rid 
ed., McGraw-Hill, 1981. van den Box, J., L. C. Caruthers and A. van Dam, "GPGS: A Device-independent 
General Purpose Graphic System," SIGGRAPH '77 Proceedings, published as Computer Graphics, 11(2), Summer 
1977, pp. 112-119. Weiman, C., "Continuous Anti-Aliased Rotation and Zoom of Raster Images," SIGGRAPH 
'80 Proceed- ings, published as Computer Graphics, 14(3), July 1980, pp. 286-293. 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801299</article_id>
		<sort_key>329</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>48</seq_no>
		<title><![CDATA[Health &amp; safety issues in computer graphics (Panel Session)]]></title>
		<page_from>329</page_from>
		<page_to>331</page_to>
		<doi_number>10.1145/800064.801299</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801299</url>
		<abstract>
			<par><![CDATA[<p>Dr. Wordie Parr</p> <p>Dr. Parr is Chief of the Physical Agents Effects Branch of NIOSH (National Institute for Occupational Safety and Health. He is internationally known as an expert in radiation biology. Currently, Dr. Parr's duties involve directing field and research programs in noise and electromagnetic radiation (EMR). As a result of this involvement, one of Dr. Parr's recent activities has been centered around the issue of whether video display terminals (VDTs) pose a radiation health issue to workers in various fields. Dr. Parr and his staff at NIOSH have evaluated over 1000 different VDTs of all types, makes, and models. The results of these evaluations have all been published in the literature and will be reviewed in this session.</p> <p>David Sliney</p> <p>Mr. Sliney has been involved in standards development and in investigating potential hazards associated with lasers and conventional lightsources (including devices using cathode ray tubes and other optical display techniques). He recently co-authored a book on Optical Radiation Safety.</p> <p>R. C. Petersen</p> <p>R. C. Petersen works in the Environmental Health and Safety Department of Bell Laboratories in Murray Hill, NJ, where his responsibilities include non-ionizing radiation protection for the Bell System. This assignment includes characterizing and documenting the accessible levels of electromagnetic energy associated with existing and prototype RF and microwave transmission equipment used by the Bell System.</p> <p>W. M. Zuk</p> <p>Dr. W. M. Zuk is Head of the X-Rays Section, Consumer and Clinical Radiation Hazards Division, Health Protection Branch of Health and Welfare Canada.</p> <p>William H. Cushman</p> <p>Dr. William H. Cushman is a research psychologist and ergonomics consultant for Eastman Kodak Company in Rochester, NY. He has particular interests in visual perception, workplace design, lighting and industrial ergonomics.</p> <p>Optical Radiation Emissions from VDTs</p> <p>David Sliney</p> <p>Several groups of scientists in this country and overseas have measured the ultraviolet, visible and infrared radiations emitted by visual display devices to determine if potential optical radiation hazards exist from such displays. These measurements will be explained and conclusions drawn will be summarized and discussed.</p> <p>Radiofrequency and Microwave Emissions from VDTs</p> <p>R. C. Peterson</p> <p>Questions are continually being raised concerning potential health effects associated with exposure to radio frequency and microwave emissions from optical display devices. Because of these questions, several studies were undertaken by private and governmental organizations to characterize such emissions as a function of frequency and intensity. In particular, the band of frequencies extending from 10kHz to 18GHz was examined by Bell Laboratories for a variety of VDTs under normal conditions of operation. Similar emission measurements were made at a repair facility where a number of malfunctioning devices with their housings removed were operated simultaneously. In all cases, the sweep frequency and numerous harmonics were detected along with emissions at higher frequencies that could be associated with the digital clock or other switching equipment located within the terminal or within peripheral equipment. In no case did individual levels or the sum of all measured emissions even remotely approach exposure guidelines existing or proposed in either the United States or any other nation.</p> <p>The conclusion of this study, which is supported by the results of other similar studies, is that there is absolutely no evidence to indicate, nor is it even a subject of speculation, that the emissions associated with VDTs will or could have deleterious effects on the health of those persons using such devices.</p> <p>Measurements of X-ray Emission from VDTs</p> <p>W. M. Zuk</p> <p>The proliferation of video display terminals in the work environment has given rise to questions about possible adverse health effects associated with their use. In particular, considerable concern has been voiced by the media and labour organizations about the possibility of VDTs posing a radiation hazard to the operators of these devices. In response to this concern, a large number of reputable scientific surveys of radiation emissions from VDTs have been carried out all over the world. Collectively these surveys have encompassed virtually every make and model of VDT and have included emission measurements for x-ray, microwave, radiofrequency, ultraviolet infrared and visible radiations. The Radiation Protection Bureau of Health and Welfare Canada, in common with other health agencies, has made measurements for all of these radiations, however, this presentation will focus on the x-ray measurements only.</p> <p>In the past eleven years the Radiation Protection Bureau has carried out x-ray measurements on over 300 VDTs, comprising over 150 different models. In all cases no x-ray emission above instrumental background levels was detected. To assess the validity that VDTs might be emitting x-rays below levels detectable by the most commonly used field survey instruments (Victoreen 44DRF/C), 38 different models were measured in the Radiation Protection Bureau's low-level counting facility. This facility can detect low energy x-rays at emission levels some 500,000 times lower than the regulatory limit. No x-ray emission was detected.</p> <p>Human Factors Issues in VDT Workplace Design</p> <p>William H. Cushman</p> <p>The design of VDT workplaces has an important effect on the performance and attitude of people working with VDTs. Lighting factors are particularly important in determining the overall ergonomics of VDT workplaces. Some lighting factors that must be considered include illumination and luminance; task lighting; minimizing glare; selection and arrangement of luminaires; indirect lighting; and screens and filters for contrast enhancement. Other important factors include terminal design, furniture design, noise levels and color use.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>K.4.1</cat_node>
				<descriptor>Computer-related health issues</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003456.10003462.10003602</concept_id>
				<concept_desc>CCS->Social and professional topics->Computing / technology policy->Medical information policy</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Human Factors</gt>
		</general_terms>
		<authors>
			<au>
				<person_id>P334497</person_id>
				<author_profile_id><![CDATA[81100331679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wordie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NIOSH]]></affiliation>
				<role><![CDATA[Chairman]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P329812</person_id>
				<author_profile_id><![CDATA[81100366889]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sliney]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chief, Laser Branch, Laser-Microwave Division, U.S. Army Environmental Hygiene Agency]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P333094</person_id>
				<author_profile_id><![CDATA[81332520669]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[R.]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Petersen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bell Laboratories]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P334313</person_id>
				<author_profile_id><![CDATA[81100150156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[W.]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Zuk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Health and Welfare Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P298853</person_id>
				<author_profile_id><![CDATA[81100577770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[William]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Cushman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eastman Kodak]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PANEL: Health &#38; Safety Issues in Computer Graphics CHAIR: Dr. Wordie Parr NIOSH Dr. Parr is Chief 
of the Physical Agents Effects Branch of NIOSH (National Institute for Occupational Safety and Health. 
He is inter- nationally known as an expert in radiation biology. Currently, Dr. Parr's duties involve 
directing field and research programs in noise and electromagnetic radiation (EMR). As a result of this 
involvement, one of Dr. Parr's recent activities has been centered around the issue of whether video 
display terminals (VDTs) pose a radiation health issue to workers in various fields. Dr. Parr and his 
staff at NIOSH have evaluated over i000 different VDTs of all types, makes, and models. The results of 
these evaluations have all been published in the literature and will be reviewed in this session. Panelist: 
David Sliney Chief, Laser Branch, Laser-Microwave Division U.S. Army Environmental Hygiene Agency Mr. 
Sliney has been involved in standards development and in investi- gating potential hazards associated 
with lasers and conventional light sources (including devices using cathode ray tubes and other optical 
display techniques). He recently co-authored a book on Optical Radia- tion Safety. Panelist: R. C. Petersen 
Bell Laboratories R. C. Petersen works in the Environmental Health and Safety Department of Bell Laboratories 
in Murray Hill, NJ, where his responsibilities include non-ionizing radiation protection for the Bell 
System. This assignment includes characterizing and documenting the accessible lev- els of electromagnetic 
energy associated with existing and prototype RF and microwave transmission equipment used by the Bell 
System. Panelist: W. M. Zuk Health and Welfare Canada Dr. W. M. Zuk is Head of the X-Rays Section, 
Consumer and Clinical Radiation Hazards Division, Health Protection Branch of Health and Welfare Canada. 
 Panelist: William H. Cushman Eastman Kodak Dr. William H. Cushman is a research psychologist and ergonomics 
con- sultant for Eastman Kodak Company in Rochester, NY. He has particular interests in visual perception, 
workplace design, lighting and indus- trial ergonomics. Abstract: Optical Radiation Emissions from VDTs 
 David Sliney U.S. Army Environmental Hygiene Agency Several groups of scientists in this country and 
overseas have measured the ultraviolet, visible and infrared radiations emitted by visual display devices 
to determine if potential optical radiation 329 PANEL (continued): Health &#38; Safety Issues in Computer 
Graphics hazards exist from such displays. These measurements will be explained and conclusions drawn 
will be summarized and discussed. Abstract: Radiofrequency and Microwave Emissions from VDTs R. C. 
Petersen Bell Laboratories Questions are continually being raised concerning potential health effects 
associated with exposure to radio frequency and microwave emissions from optical display devices. Because 
of these questions, several studies were undertaken by private and governmental organizations to characterize 
such emissions as a function of fre- quency and intensity. In particular, the band of frequencies extending 
from 10kHz to 18GHz was examined by Bell Laboratories for a variety of VDTs under normal conditions of 
operation. Similar emission measure- ments were made at a repair facility where a number of malfunctioning 
devices with their housings removed were operated simultaneously. In all cases, the sweep frequency and 
numerous harmonics were detected along with emissions at higher frequencies that could be associated 
with the digital clock or other switching equipment located within the terminal or within peripheral 
equipment. In no case did individual levels or the sum of all measured emissions even remotely approach 
exposure guidelines existing or proposed in either the United States or any other nation. The conclusion 
of this study, which is supported by the results of other similar studies, is that there is absolutely 
no evidence to indicate, nor is it even a subject of speculation, that the emissions associated with 
VDTs will or could have deleterious effects on the health of those persons using such devices. Abstract: 
Measurements of X-ray Emission from VDTs W. M. Zuk Health and Welfare Canada The proliferation of video 
display terminals in the work environ- ment has given rise to questions about possible adverse health 
effects associated with their use. In particular, considerable concern has been voiced by the media and 
labour organizations about the possibil- ity of VDTs posing a radiation hazard to the operators of these 
dev- ices. In response to this concern, a large number of reputable scien- tific surveys of radiation 
emissions from VDTs have been carried out all over the world. Collectively these surveys have encompassed 
vir- tually every make and model of VDT and have included emission measure- ments for x-ray, microwave, 
radiofrequency, ultraviolet infrared and visible radiations. The Radiation Protection Bureau of Health 
and Welfare Canada, in common with other health agencies, has made meas- urements for all of these radiations, 
however, this presentation will focus on the x-ray measurements only. In the past eleven years the Radiation 
Protection Bureau has car- ried out x-ray measurements on over 300 VDTs, comprising over 150 dif- ferent 
models. In all cases no x-ray emission above instrumental background levels was detected. To assess the 
validity that VDTs might be emitting x-rays below levels detectable by the most commonly used field survey 
instruments (Victoreen 44DRF/C), 38 different models were measured in the Radiation Protection Bureau's 
low-level counting facility. This facility can detect low energy x-rays at emission lev- els some 500,000 
times lower than the regulatory limit. No x-ray emis- sion was detected. 330 PANEL (continued): Health 
&#38; Safety Issues in Computer Graphics Abstract: Human Factors Issues in VDT Workplace Design William 
H. Cushman Eastman Kodak The design of VDT workplaces has an important effect on the per- formance and 
attitude of people working with VDTs. Lighting factors are particularly important in determining the 
overall ergonomics of VDT workplaces. Some lighting factors that must be considered include illumination 
and luminance; task lighting; minimizing glare; selection and arrangement of luminaires; indirect lighting; 
and screens and filters for contrast enhancement. Other important factors include terminal design, furniture 
design, noise levels and color use. 331 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
	<article_rec>
		<article_id>801300</article_id>
		<sort_key>333</sort_key>
		<display_label></display_label>
		<article_publication_date>07-01-1982</article_publication_date>
		<seq_no>49</seq_no>
		<title><![CDATA[Author index]]></title>
		<page_from>333</page_from>
		<doi_number>10.1145/800064.801300</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=801300</url>
		<categories>
			<primary_category>
				<cat_node>A.2</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002944.10011122.10002946</concept_id>
				<concept_desc>CCS->General and reference->Document types->Reference works</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944.10011122.10002946</concept_id>
				<concept_desc>CCS->General and reference->Document types->Reference works</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<general_terms>
			<gt>Theory</gt>
		</general_terms>
		<authors>
			<au>
				<seq_no />
				<first_name />
				<middle_name />
				<last_name />
				<suffix />
				<role />
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ,q mmmmm Acquah, James Anson, Ed Barnett, Jane Beach, Richard J. Beatty, John C. Blinn, James F. 
Booth, Kellogg S. Carberry, J. S. Carlson, Wayne E. Clark, James H. Cobb, Elizabeth S. Crow, F. C. 
Denlinger, Jerry L. Dennehy, T. G. Evans, Steven R. Fiume, Eugene L. Foley, James Frank, Andre Friedell, 
Mark Ganapathy, S. Goad, Chris Greenberg, Donald P. Guibas, Leo J. Hanrahan, Pat M. Heckbert, Paul 
S. Kajiya, James T. Kasik, David J. Kawaguchi, Yoichiro Kessner, Rens Kramlich, David 321 107 181 
277 277 21, 273 277 208 255 127 265 9 189 69 115 277 321 199 181 69 167 157, 233 311 77 297 245 
99 223 33 181 SIGGRAPH '82 Author Index Lipkie, Daniel E. 115 MacKay, Steven A. 213 Mantyla, Martti 
51 McKeown, Jr., David M. 189 Michener, James C. 33 Newlin, John K. 115 Norton, Alan i, 61 Pfaff, Gunther 
33 Pittman, Jon 233 Plebon, Darlene A. 277 Potel, Michael J. 213 Reid, Eric R. 87 Reed, Theodore N. 
39 Reynolds, Craig W. 289 Rockwood, Alyn P. 1 Rosenthal, David S.H. 33 Sabin, Malcolm 33, 47 Sayre, 
Richard E. 213 Schweitzer, Dino 265 Shelley, Kim L. 157 Sibert, John 321 Skolmoski, Philip 1 ten Hagen, 
Paul 45 Turkowski, Kenneth 19 Warnock, John 313 Weissman, Robert L. 115 Wenner, Patricia 321 Whelan, 
Daniel S. 147 Wong, Peter C. S. 87 Wyatt, Douglas K. 313 Zyda, Michael J. 135 333 
			]]></ft_body>
		</fulltext>
		<ccc>
			<copyright_holder>
				<copyright_holder_name>ACM</copyright_holder_name>
				<copyright_holder_year>1982</copyright_holder_year>
			</copyright_holder>
		</ccc>
	</article_rec>
</content>
</proceeding>
