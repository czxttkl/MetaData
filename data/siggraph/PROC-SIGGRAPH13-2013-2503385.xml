<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>07/21/2013</start_date>
		<end_date>07/25/2013</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Anaheim]]></city>
		<state>California</state>
		<country></country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2503385</proc_id>
	<acronym>SIGGRAPH '13</acronym>
	<proc_desc>ACM SIGGRAPH 2013 Posters</proc_desc>
	<conference_number>2013</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-2342-0</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2013</copyright_year>
	<publication_date>07-21-2013</publication_date>
	<pages>115</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Posters are a convenient method for presenting in-progress research, student projects, and late-breaking work. Poster topics range from applications of computer graphics to in-depth research in specific areas. They are on display for attendees to browse at their leisure. Poster authors meet and discuss their work with attendees during Poster Presentations.</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
</proceeding_rec>
<content>
	<section>
		<section_id>2503386</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>2503387</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Digital Ira]]></title>
		<subtitle><![CDATA[creating a real-time photoreal digital actor]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503387</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503387</url>
		<abstract>
			<par><![CDATA[<p><b>Overview</b> In 2008, the "Digital Emily" project [Alexander et al. 2009] showed how a set of high-resolution facial expressions scanned in a light stage could be rigged into a real-time photoreal digital character and driven with video-based facial animation techniques. However, Digital Emily was rendered offline, involved just the front of the face, and was never seen in a tight closeup. In this collaboration between Activision and USC ICT shown at SIGGRAPH 2013's <i>Real-Time Live</i> venue, we endeavoured to create a real-time, photoreal digital human character which could be seen from any viewpoint, in any lighting, and could perform realistically from video performance capture even in a tight closeup. In addition, we wanted this to run in a real-time game-ready production pipeline, ultimately achieving 180 frames per second for a full-screen character on a two-year old graphics card.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190559</person_id>
				<author_profile_id><![CDATA[81458649961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oleg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190564</person_id>
				<author_profile_id><![CDATA[81421597317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Graham]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fyffe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190565</person_id>
				<author_profile_id><![CDATA[81440600975]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jay]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Busch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190566</person_id>
				<author_profile_id><![CDATA[81440613130]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xueming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190567</person_id>
				<author_profile_id><![CDATA[81414619646]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ryosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ichikari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190568</person_id>
				<author_profile_id><![CDATA[81100556708]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190569</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[debevec@ict.usc.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190570</person_id>
				<author_profile_id><![CDATA[82458669357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Jorge]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jimenez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190571</person_id>
				<author_profile_id><![CDATA[82458893057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Etienne]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Danvoye]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190560</person_id>
				<author_profile_id><![CDATA[82459173457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Bernardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Antionazzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190561</person_id>
				<author_profile_id><![CDATA[82459255857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eheler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190562</person_id>
				<author_profile_id><![CDATA[82459280957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Zybnek]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kysela]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190563</person_id>
				<author_profile_id><![CDATA[82459206757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Javier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[von der Pahlen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Activision, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Javier.Pahlen@activision.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1667251</ref_obj_id>
				<ref_obj_pid>1667239</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Alexander, O., Rogers, M., Lambeth, W., Chiang, M., and Debevec, P. 2009. Thedigital emily project: photoreal facial modeling and animation. In <i>ACM SIGGRAPH 2009 Courses</i>, ACM, New York, NY, USA, SIGGRAPH '09, 12:1--12:15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024163</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Fyffe, G., Tunwattanapong, B., Busch, J., Yu, X., and Debevec, P. 2011. Multiview face capture using polarized spherical gradient illumination. <i>ACM Trans. Graph. 30</i>, 6 (Dec.), 129:1--129:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2341906</ref_obj_id>
				<ref_obj_pid>2341836</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Jimenez, J., Jarabo, A., Gutierrez, D., Danvoye, E., and von der Pahlen, J. 2012. Separable subsurface scattering and photorealistic eyes rendering. In <i>ACM SIGGRAPH 2012 Courses</i>, ACM, New York, NY, USA, SIGGRAPH 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Digital Ira: Creating a Real-Time Photoreal Digital Actor Oleg Alexander Graham Fyffe Jay Busch Xueming 
Yu Jorge Jimenez Etienne Danvoye Bernardo Antionazzi Ryosuke Ichikari Andrew Jones Paul Debevec* Mike 
Eheler Zybnek Kysela Javier von der Pahlen USC Institute for Creative Technologies Activision, Inc. 
Figure 1: (Left) Three of eight high-res (0.1mm) light stage scans of the actor in static expressions. 
(Middle) Seven-camera HD performance recording. (Right) 180Hz video-driven blendshape model with screen-space 
subsurface scattering and advanced eye shading effects. Overview In 2008, the Digital Emily project [Alexander 
et al. 2009] showed how a set of high-resolution facial expressions scanned in a light stage could be 
rigged into a real-time photo­real digital character and driven with video-based facial animation techniques. 
However, Digital Emily was rendered of.ine, involved just the front of the face, and was never seen in 
a tight closeup. In this collaboration between Activision and USC ICT shown at SIGGRAPH 2013 s Real-Time 
Live venue, we endeavoured to cre­ate a real-time, photoreal digital human character which could be seen 
from any viewpoint, in any lighting, and could perform real­istically from video performance capture 
even in a tight closeup. In addition, we wanted this to run in a real-time game-ready pro­duction pipeline, 
ultimately achieving 180 frames per second for a full-screen character on a two-year old graphics card. 
3D Scanning We began by scanning accomodating researcher Ari Shapiro in thirty high-resolution expressions 
using the USC ICT s Light Stage X system [Ghosh et al. 2011], producing 0.1mm resoution geometry and 
4K diffuse and specular re.ectance maps per expression. We chose eight expressions for the real-time 
perfor­mance rendering, maximizing the variety of .ne-scale skin defor­mation observed in the scans. 
The expressions were merged onto an artistically built back-of-the head model. To record performances 
for the character, we shot seven views of 30fps video of the actor improvising lines using the same seven 
Canon 1Dx cameras used for the scans. We used a new tool called Vuvuzela to interactively and precisely 
correspond all expression texture (u,v) coordinates to the neutral expression, which was retopologized 
to a low-polygon clean artist mesh. Performance Animation Our of.ine animation solver creates a performance 
graph from dense GPU optical .ow between the video frames and the eight expressions. This graph gets 
pruned by an­alyzing the correlation between the video frames and the expres­sion scans over twelve facial 
regions. The algorithm then computes dense optical .ow and 3D triangulation yielding per-frame spatially 
varying blendshape weights approximating the performance. The Game Rig To create the game-ready facial 
rig, we trans­ferred the mesh animation to standard bone animation on a 4K poly­gon mesh using a bone 
weight and transform solver. The solver op­timizes the smooth skinning weights and the bone animated 
trans­ *debevec@ict.usc.edu Javier.Pahlen@activision.com forms to maximize the correspondence between 
the game mesh and the reference animated mesh. Real-Time Rendering The rendering technique uses surface 
stress values to blend diffuse texture, specular, normal, and dis­placement maps from the different high-resolution 
expression scans per-vertex at run time. As a result, realistic wrinkles appear around the actor s eyes 
when he squints and on his foreheard when he raises his eyebrows; the color of the skin also changes 
with expres­sion due to shifting blood content. The DirectX11 rendering takes into account light transport 
phenomena happening in the skin and eyes, from large scale events like the re.ection of light of the 
own face into the eyes, to the shadowing and occlusion happening in the skin pores. In particular, it 
includes separable subsurface scattering [Jimenez et al. 2012] in screen-space, translucency, eye refraction 
and caustics, advanced shadow mapping and ambient occlusion, a physically-based two-lobe specular re.ection 
with microstructure, depth of .eld, post effects, temporal antialiasing (SMAA T2x), and .lm grain. Acknowledgements 
We thank Borom Tunwattanapong, Koki Nagano, Domi Piturro, Alejo von der Pahlen, Joe Alter, Curtis Bee­son, 
Mark Daly, Mark Swain, Jen-Hsun Huang, Ari Shapiro, Va­lerie Dauphin, and Kathleen Haase for their important 
assistance and contributions to this work. This work was supported by USA RDECOM, USC, and Activision, 
Inc; no endorsement is implied. References ALE XAN D E R , O., ROGER S , M., LA MB ETH, W., CHI A N G, 
M., AND DEB EV EC , P. 2009. Thedigital emily project: photo­real facial modeling and animation. In ACM 
SIGGRAPH 2009 Courses, ACM, New York, NY, USA, SIGGRAPH 09, 12:1 12:15. GH O S H , A., FY FF E , G., 
TU N WATTA NAPON G , B., BU SC H , J., YU, X., AN D DEB E V EC , P. 2011. Multiview face capture using 
polarized spherical gradient illumination. ACM Trans. Graph. 30, 6 (Dec.), 129:1 129:10. JI M E N E Z, 
J., JA RAB O , A., GU TI E R RE Z , D., DANVOYE , E., A N D VO N D ER PAHLE N, J. 2012. Separable subsurface 
scattering and photorealistic eyes rendering. In ACM SIGGRAPH 2012 Courses, ACM, New York, NY, USA, SIGGRAPH 
2012. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503388</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Efficient speech animation synthesis with vocalic lip shapes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503388</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503388</url>
		<abstract>
			<par><![CDATA[<p>Computer-generated speech animations are commonly seen in video games and movies. Although high-quality facial motions can be created by the hand crafted work of skilled artists, this approach is not always suitable because of time and cost constraints. A data-driven approach [Taylor et al. 2012], such as machine learning to concatenate video portions of speech training data, has been utilized to generate natural speech animation, while a large number of target shapes are often required for synthesis. We can obtain smooth mouth motions from prepared lip shapes for typical vowels by using an interpolation of lip shapes with Gaussian mixture models (GMMs) [Yano et al. 2007]. However, the resulting animation is not directly generated from the measured lip motions of someone's actual speech.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190572</person_id>
				<author_profile_id><![CDATA[81492655713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ai-zumi@ruri.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190573</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190574</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2421770</ref_obj_id>
				<ref_obj_pid>2421731</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Taylor, S. L., et al. 2012. Dynamic Units of Visual Speech. In Proc. ACM SCA 2012, 275--284.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1280752</ref_obj_id>
				<ref_obj_pid>1280720</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yano, A., et al. 2007. Variable Rate Speech Animation Synthesis. In Proc. ACM SIGGRAPH 2007, Poster, no. 18.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lee, A., et al. 2009. Recent Development of Open-source Speech Recognition Engine Julius. In Proc. APSIPA ASC 2009, 131--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Efficient Speech Animation Synthesis with Vocalic Lip Shapes Figure 1: Animation synthesis using our 
method.  Daisuke Mima* Akinobu Maejima Shigeo Morishima  Waseda University       1 Introduction 
 Computer-generated speech animations are commonly seen in video games and movies. Although high-quality 
facial motions can be created by the hand crafted work of skilled artists, this approach is not always 
suitable because of time and cost constraints. A data-driven approach [Taylor et al. 2012], such as machine 
learning to concatenate video portions of speech training data, has been utilized to generate natural 
speech animation, while a large number of target shapes are often required for synthesis. We can obtain 
smooth mouth motions from prepared lip shapes for typical vowels by using an interpolation of lip shapes 
with Gaussian mixture models (GMMs) [Yano et al. 2007]. However, the resulting animation is not directly 
generated from the measured lip motions of someone's actual speech. In this study, we propose a novel 
technique for synthesizing speech animation from an input utterance (voice and text) by interpolating 
lip shapes of the five cardinal vowels. In addition, we determine the blend weights of the target lip 
shapes according to calculated lip motions from training data of real speech. Our method has the following 
two advantages: it can be used for never -heard-before dialogs without complicated computation or advance 
preparation, and it can generate smooth speech animation with minimum number of target shapes. 2 Lip 
Motion Estimation The first step of our work is to classify units of vocalic lip motions from real speech 
motions. By using a motion capture system, we first obtain the motion of each marker placed on the lips 
of an actor as the actor uttered various sentences (199 types in this study). Second, the captured lip 
motions are divided into individual phoneme segments [Lee et al. 2009], and each part of segmented motions 
is categorized into 11 groups, five types of vowels and six types of consonants. Third, the lip motion 
that belongs to successive phoneme segments (/consonant/-/vowel/-/consonant/) is deemed a vocalic lip 
motion , and motions that have same phonemes are grouped as a unit. The second step is to define a cost 
function (1) to presume lip motions from an input speech. ..        ^ where functions and respectively, 
represent distance and gradient differences of successive lip motions (  and ) in the region . We eventually 
acquire an optimum that provides the lowest by the gradient descent method (the details are provided 
in supplementary supporting document). In this study, because of a small repertoire of target lip shapes 
for synthesis, we define lip motions as transitions of length between neutral and opening lip shapes 
in the horizontal and vertical directions; provided, however, that we may add a new category of target 
shapes. 3 Animation Synthesis Result We determine the blend weights for the target shapes (five types 
in this work) by fitting GMMs, whose variables are obtained from equation (2) to an estimated lip motion. 
.| ..  (    )   | where   are variables of Gaussian functions, is an estimated lip motion, 
 is an ending time of an input speech, is the number of target lip shapes and is the number of Gaussian 
functions. The actor's lip shapes equivalent to the targets are s, which vanish in the event that  
is found in a segment that has a different vowel from that of . With our method, natural speech animations 
are generated even if there are a few target shapes, because the blend weights for the shapes are calculated 
according to the well-approximated lip motions of real people. In this study, we demonstrate speech animation 
generated by our method in a supplementary video, in which complex lip motions were observed, such as 
closing the mouth immediately before bilabial consonants. The advantage of our method is to synthesize 
a speech animation by interpolating a small number of target lip shapes with GMMs. Each parameter of 
the Gaussian functions is computed on the basis of estimated lip motions, and this means more realistic 
lip motions can possibly be obtained than those obtained when GMMs are simply applied to an input sentence 
[Yano et al. 2007]. It remains for future work to automatically control an estimation of lip motions 
according to utterance speed or facial expressions changes. References TAYLOR, S.L., et al. 2012. Dynamic 
Units of Visual Speech. In Proc. ACM SCA 2012, 275-284. YANO, A., et al. 2007. Variable Rate Speech Animation 
Synthesis. In Proc. ACM SIGGRAPH 2007, Poster, no.18. LEE, A., et al. 2009. Recent Development of Open-source 
Speech Recognition Engine Julius. In Proc. APSIPA ASC 2009, 131-137. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503389</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Deformation transfer based on stretchiness ratio]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503389</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503389</url>
		<abstract>
			<par><![CDATA[<p>Recent work for deformation transfer is almost based on the deformation gradient framework [Sumner and Popovi&#263; 2004], which deforms a target mesh by transferring local affine transformation in an optimization framework. We aim to explore a coordinate-invariant quantity for deformation transfer. What we propose here is to achieve deformation by transferring stretchiness ratio between the source meshes to the target.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190575</person_id>
				<author_profile_id><![CDATA[82459110457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yung-Hsiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190576</person_id>
				<author_profile_id><![CDATA[82459051757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wan-Chun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190577</person_id>
				<author_profile_id><![CDATA[81100319756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ouhyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2309650</ref_obj_id>
				<ref_obj_pid>2309616</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ma, W.-C., Wang, Y.-H., Fyffe, G., Chen, B.-Y., and Debevec, P. 2012. A blendshape model that incorporates physical interaction. <i>Computer Animation and Virtual Worlds 23</i>, 3-4, pp. 235--243.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1142001</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shi, L., Yu, Y., Bell, N., and Feng, W.-W. 2006. A fast multigrid algorithm for mesh deformation. In <i>ACM SIGGRAPH 2006 Papers</i>, ACM, SIGGRAPH '06, pp.1108--1117.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015736</ref_obj_id>
				<ref_obj_pid>1186562</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sumner, R. W., and Popovi&#263;, J. 2004. Deformation transfer for triangle meshes. In <i>ACM SIGGRAPH 2004 Papers</i>, ACM, SIGGRAPH '04, pp. 399--405.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Deformation Transfer based on Stretchiness Ratio Yung-Hsiang Yang Wan-Chun Ma Ming Ouhyoung National 
Taiwan University  Figure 1: The result from the proposed deformation transfer technique. From left 
to right, source reference mesh S, source deformed mesh S , target reference mesh T , and target deformed 
mesh T . 1 Introduction Recent work for deformation transfer is almost based on the de­formation gradient 
framework [Sumner and Popovi´ c 2004], which deforms a target mesh by transferring local af.ne transformation 
in an optimization framework. We aim to explore a coordinate­invariant quantity for deformation transfer. 
What we propose here is to achieve deformation by transferring stretchiness ratio between the source 
meshes to the target. 2 Proposed Technique Given a source reference mesh S, a source deformed mesh S 
, and a target reference mesh T , the objective is to transfer deformation between the source reference 
mesh and the source deformed mesh onto the target reference mesh and produce a target deformed mesh T 
. We assume both S and T are vertex-wise corresponded. A set of mass-spring systems is built for S, S 
, and T , both in their equilibrium state (in other words, the length of each edge in the mesh is equivalent 
to the rest length of the spring). Let ei,j . E denote a spring that connects vi and vj , and ri,j denote 
the rest length of the spring. The desired rest lengths for the target deformed mesh T are computed by 
transferring the stretchiness ratio between the rest lengths of source meshes to the target reference 
mesh: S T T i,j r = r r, (1) i,j i,j × S ei,j . E. r i,j TTS S ri,j , ri,j , ri,j and ri,j are rest 
lengths of edge ei,j in mesh T , T , S, and S , respectively. Once the desired rest lengths of T are 
obtained, T the equilibrium state is computed based on ri,j to obtain the vertex positions of T . According 
to Hooke s law, T vi - vj f(vi) =ki,j (|vi - vj | - ri,j ) = 0. (2) |vi - vj | .i,j We assume the spring 
constant k to be inversely proportional to the rest length. This strategy ensures every spring contributes 
similar amount of force during the optimization process. The mass of each vertex is set as constant and 
thus can be ignored. In order to preserve surface properties, the mass-spring systems are augmented with 
additional bending and internal springs [Ma et al. 2012] to prevent surface from collapsing. The equilibrium 
state of mass-spring system can be solved with Newton-Raphson method. However, the optimization is dif.cult 
to solve correctly if a large condition number of the system matrix presents. We adopt a multi­grid approach 
to accelerate state convergence. A mesh coarsening approach [Shi et al. 2006] is used to construct different 
levels. Our system estimates the error of current state on a coarse-level grid of the objective function. 
The error is diffused on the coarse-level grid, and then it is interpolated to approximate the error 
on a .ne-level grid. The error propagation from coarse-level gives good initial guess of the equilibrium 
state of the mass-spring system. Figure 1 shows the deformation transfer result. The proposed method 
produces analogous deformation on the target mesh. Note that it is possible that the stretchiness ratio 
can be controlled to al­low shape interpolation. One major limitation of the system is that mass-spring 
system faces element inversion problem such that the nonlinear optimization converges to a local minimum. 
We would like to investigate other representations that are more stable numer­ically. References MA, 
W.-C., WA N G , Y.-H., FY FFE, G., CH EN , B.-Y., A N D DE-B EV EC , P. 2012. A blendshape model that 
incorporates physical interaction. Computer Animation and Virtual Worlds 23, 3-4, pp. 235 243. SH I, 
L., YU, Y., BELL , N., AN D FE N G, W.-W. 2006. A fast multigrid algorithm for mesh deformation. In ACM 
SIGGRAPH 2006 Papers, ACM, SIGGRAPH 06, pp.1108 1117. SU MNE R , R. W., A N D PO P OV I C´ , J. 2004. 
Deformation transfer for triangle meshes. In ACM SIGGRAPH 2004 Papers, ACM, SIGGRAPH 04, pp. 399 405. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503390</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Expressive dance motion generation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503390</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503390</url>
		<abstract>
			<par><![CDATA[<p>The power of expression such as accent in motion and movement of arms is an indispensable factor in dance performance because there is a large difference in appearance between natural dance and expressive motions. Needless to say, expressive dance motion makes a great impression on viewers. However, creating such a dance motion is challenging because most of the creators have little knowledge about dance performance. Therefore, there is a demand for a system that generates expressive dance motion with ease. Tsuruta et al. [2010] generated expressive dance motion by changing only the speed of input motion or altering joint angles. However, the power of expression was not evaluated with certainty, and the generated motion did not synchronize with music. Therefore, the generated motion did not always satisfy the viewers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190578</person_id>
				<author_profile_id><![CDATA[82459327657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Narumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi-pink@fuji.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190579</person_id>
				<author_profile_id><![CDATA[82459234757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190580</person_id>
				<author_profile_id><![CDATA[81504688282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tsukasa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukusato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190581</person_id>
				<author_profile_id><![CDATA[81488654875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190582</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tsuruta, S. et al, 2010. Generation of Emotional Dance Motion for Virtual Dance Collaboration System. Digital Humanities, pp.368--371.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shiratori, T. et al, 2006.Dancing-to-Music Character Animation. Eurographics (Computer Graphics Forum), Vol. 25, No.3, pp. 449--458.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Expressive Dance Motion Generation Narumi Okada i Kazuki Okami Tsukasa Fukusato Naoya Iwamoto Shigeo 
Morishima ii (a)   (b)        Figure 2: Result  Mocap Data http://www.google.co.jp/url?source=imglanding&#38;ct=img&#38;q=http://www.ninsin-news.com/files/420/099277c5ec0580cdb85a1c1faf111d17.jpg&#38;sa=X&#38;ei=dpOgUOWiGK7MmAWsu4DgDA&#38;ved=0CAsQ8wc4Vw&#38;usg=AFQjCNE1BtIuygs2LpFOhYh_vXa6OasUqg 
 http://www.google.co.jp/url?source=imglanding&#38;ct=img&#38;q=http://www.ninsin-news.com/files/420/099277c5ec0580cdb85a1c1faf111d17.jpg&#38;sa=X&#38;ei=dpOgUOWiGK7MmAWsu4DgDA&#38;ved=0CAsQ8wc4Vw&#38;usg=AFQjCNE1BtIuygs2LpFOhYh_vXa6OasUqg 
 (a) Subjective Assessment http://www.google.co.jp/url?source=imglanding&#38;ct=img&#38;q=http://upload.wikimedia.org/wikipedia/commons/thumb/6/69/Euclidean_vs_DTW.jpg/220px-Euclidean_vs_DTW.jpg&#38;sa=X&#38;ei=0pKgUILhKs2imQWMrIH4DA&#38;ved=0CAsQ8wc4WA&#38;usg=AFQjCNEPoCxPNGRj9ZVMcvxc1jyZPHUa6Q 
         Figure 1: Overview of Our Method  Waseda University Changing the accent     Changing 
the amplitude   1. Introduction To keep the tempo The power of expression such as accent in motion 
and movement of arms is an indispensable factor in dance performance because there is a large difference 
in appearance between natural dance and expressive motions. Needless to say, expressive dance motion 
makes a great impression on viewers. However, creating such a dance motion is challenging because most 
of the creators have little knowledge about dance performance. Therefore, there is a demand for a system 
that generates expressive dance motion with ease. Tsuruta et al. [2010] generated expressive dance motion 
by changing only the speed of input motion or altering joint angles. However, the power of expression 
was not evaluated with certainty, and the generated motion did not synchronize with music. Therefore, 
the generated motion did not always satisfy the viewers. (c) Accent Filter (b) Segmentation Expression 
Converting Filters (d) Emphasis Filter i e-mail narumi-pink@fuji.waseda.jp ii e-mail shigeo@waseda.jp 
 Therefore, we propose a method that transforms arbitrary dance motion into more expressive motion by 
filtering in accent and power. Original dance motion is divided into segments and converted to expressive 
dance to keep the original tempo. The expression conversion rule is extracted by analyzing motion capture 
data from training dance motions that include neutral and expressive motions labeled by subjective assessment. 
2. Expression Converting Filters Our proposed method is composed of 4 steps as shown in Figure1. (a) 
Subjective Assessment: We examined the criteria that viewers use to judge expression. Then, we use the 
dance motion that was captured by a motion capture system. The contents of the data are standard motion 
and motions that express pleasure depending on the dancer s individual performance, which we call Natural 
and Happy , respectively. As a result, we proved the benefit of the data and found that viewers focused 
on accent and dynamic motion. We also analyze the performer s knee movements because they are usually 
synchronized with the tempo of music. (b) Segmentation: We divide a series of dance motion into several 
segments to generate Accent Filter depending on Weight Effort [Shiratori et al. 2006] which is defined 
as the minimum points of the following equation. W(f)= .........=1.|......(..)-......(..-1)|..={..,..,..} 
 (1)  where stands for each joint, is the number of joints, .. is the frame number, .. is degree 
of each joint, and . is the weight. (c) Accent Filter : In order to find the timing pattern when the 
accents appear, we synchronize Natural with Happy by DTW (Dynamic Time Warping). We up-sample the motion 
data from 120 fps to 1200 fps by B-spline curve in order to avoid the discontinuous correspondence. Simultaneously 
we define upper limit of the joint angular velocity in DTW process to avoid the collapse of motion. By 
observing the correspondence between Natural and Happy , the filters are categorized into two types, 
accelerating in the first part then slowing down at the last part of segment and slowing down at first 
then accelerating at the last. (d) Emphasis Filter : We define a converting filter of the amplitude of 
motion. According to the local minimum points of the degree angle of the knees in Natural and Happy , 
we calculate the average ratio among the dynamic range of joint angles. Then, we altered the angle on 
the basis of the ratio that is calculated by linear interpolation between the minimum and maximum points. 
3. Results and Conclusions As the results of open test by applying expression converting filters to arbitrary 
dance, we can get motion as shown in Figure2. With the Accent Filter , the generated motion has sudden 
slow and rapid changes in movements (see Figre2 (a)). The center of gravity showed greater movement after 
applying the Emphasis Filter (Figure2 (b)). The whole of dance seems vibrant and lively, since we divide 
a series of motion into segments, to keep the tempo uniform. Consequently, our method enables to easily 
generate expressive dance motion by applying filter to non-expressive captured dance data. Our future 
work approaches a practical application in contents production. So, we are developing a system that generates 
dance motion to fit the music mood, and allows users to choreograph the character s expressive dance 
motion. References TSURUTA, S. et al, 2010. Generation of Emotional Dance Motion for Virtual Dance Collaboration 
System. Digital Humanities, pp.368-371. SHIRATORI, T. et al, 2006.Dancing-to-Music Character Animation. 
Eurographics (Computer Graphics Forum), Vol. 25, No.3 ,pp. 449-458. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503391</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Fine water with coarse grids]]></title>
		<subtitle><![CDATA[combining surface meshes and adaptive discontinuous Galerkin]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503391</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503391</url>
		<abstract>
			<par><![CDATA[<p>Simulating water for visual effects demands a high resolution surface with precise dynamics, but a fine discretization of the entire fluid volume is generally inefficient. Prior adaptive methods using octrees or unstructured meshes carry large overheads and implementation complexity. We instead show the potential of sticking with coarse regular Cartesian grids, using detailed cut cells at boundaries, and introducing a <i>p</i>-adaptive Discontinuous Galerkin (DG) method to discretize the pressure projection step in our fluid solver. This retains much of the data structure simplicity of regular grids, more efficiently captures smooth parts of the flow, and offers the flexibility to increase resolving power where needed.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190583</person_id>
				<author_profile_id><![CDATA[81508699875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Essex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Edwards]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British, Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[essex@cs.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190584</person_id>
				<author_profile_id><![CDATA[81100248660]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bridson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British, Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rbridson@cs.ubc.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1654951</ref_obj_id>
				<ref_obj_pid>1654948</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brochu, T., and Bridson, R. 2009. Robust topological operations for dynamic explicit surfaces. <i>SIAM J. Sci. Comput. 31</i>, 4, 2472--2493.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cockburn, B., Kanschat, G., and Sch&#246;tzau, D. 2005. The local Discontinuous Galerkin method for linearized incompressible fluid flow: a review. <i>Computers & Fluids 34</i>, 4, 491--506.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1551026</ref_obj_id>
				<ref_obj_pid>1550957</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kaufmann, P., Martin, S., Botsch, M., and Gross, M. 2009. Flexible simulation of deformable models using Discontinuous Galerkin FEM. <i>Graphical Models 71</i>, 4, 153--167. Special Issue of ACM SIGGRAPH/Eurographics Symp. Comp. Anim. 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fine Water with Coarse Grids: Combining Surface Meshes and Adaptive Discontinuous Galerkin Essex Edwards* 
Robert Bridson* Simulating water for visual effects demands a high resolution sur­face with precise dynamics, 
but a .ne discretization of the entire .uid volume is generally inef.cient. Prior adaptive methods us­ing 
octrees or unstructured meshes carry large overheads and im­plementation complexity. We instead show 
the potential of stick­ing with coarse regular Cartesian grids, using detailed cut cells at boundaries, 
and introducing a p-adaptive Discontinuous Galerkin (DG) method to discretize the pressure projection 
step in our .uid solver. This retains much of the data structure simplicity of regular grids, more ef.ciently 
captures smooth parts of the .ow, and offers the .exibility to increase resolving power where needed. 
We use the El Topo library [Brochu and Bridson 2009] for tracking the water surface with an explicit 
triangle mesh. Explicit surface tracking has excited much interest, offering unmatched ef.ciency in following 
thin and detailed features but without a matching discretization of the .ow physics or appropriate regularization, 
.ne­scale features may behave badly or even cause instability. We also use FLIP particles for advection, 
including the surface mesh ver­tices. This novel combination greatly reduces numerical dissipation found 
in previous .uid solvers using explicit surface tracking. Adaptive Discontinuous Galerkin The Discontinuous 
Galerkin approach to discretizing partial dif­ferential equations is closely related to the famous .nite 
element method. For DG, the domain is partitioned into cells, and some (typically polynomial) approximation 
space is assumed within each cell. DG allows each cell to have different approximation functions regardless 
of discontinuities across cell boundaries, and the cells need not be simple shapes. Of the many DG methods 
in the lit­erature, we use the Local Discontinuous Galerkin (LDG) method [Cockburn et al. 2005], which 
is well-studied, .exible, and has no mesh-dependent parameters. DG has already been used in anima­tion 
for elasticity [Kaufmann et al. 2009]. For the elements of the DG simulation, we use the cut cells pro­duced 
by intersecting the Cartesian grid cells with the water vol­ume de.ned by the surface mesh. This discretizes 
the PDE in the detailed shape of the high resolution surface -directly capturing thin sheets and other 
small features. The principle technique of a p-adaptive approach is to use differ­ent approximation spaces 
in different cells. Within cells near the boundary, we use high-degree polynomials for high-.delity solu­tions. 
Away from the surface, where less detail is needed, we use low-degree polynomials for ef.ciency. This 
p-adaptive approach allows us to use a coarse regular grid for the whole domain, while still achieving 
.ne-scale details where desired. In contrast to h-adaptive techniques, such as octrees, p-adaptation 
is an unexplored avenue within computer graphics. For smooth func­tions, p-re.nement can produce more 
accurate results with fewer degrees of freedom than h-re.nement. For non-smooth functions, h-re.nement 
is more appropriate, but water simulations typically have smooth solutions. The major exception is when 
topology changes occur and introduce a nearly-discontinuous velocity. Even in this case, we .nd our p-adaptive 
approach works well. *(essex|rbridson)@cs.ubc.ca, University of British Columbia  Figure 1: In this 
water simulation we used a 4x5 grid with quartic polynomials for velocity in each cell. Advection is 
handled by FLIP particles -shown with black ticks. Notice the very thin sheets, well below the grid resolution. 
We apply several techniques to ensure good conditioning of the ma­trix produced by LDG. First, we merge 
cut cells that have very small volume with a larger adjacent cell. Because El Topo does not produce arbitrarily 
thin sheets, such a merger is always possi­ble. Second, we choose a basis that is adapted to the cut 
cell shape by .tting a simplex to each cut cell and using the Lagrange basis with nodal points distributed 
on this simplex. Our 2D implementation shown in Figure 1 captures thin sheets, splashes, and overturning 
waves all well below the grid resolution. References BRO C HU, T., A N D BR ID S O N , R. 2009. Robust 
topological opera­tions for dynamic explicit surfaces. SIAM J. Sci. Comput. 31, 4, 2472 2493. CO CKBU 
R N , B., KAN S C HAT, G., A N D SC H ¨ OTZ AU , D. 2005. The local Discontinuous Galerkin method for 
linearized incompress­ible .uid .ow: a review. Computers &#38; Fluids 34, 4, 491 506. KAU FMAN N , P., 
MA RT IN, S., BOT S C H , M., A N D GRO S S , M. 2009. Flexible simulation of deformable models using 
Discon­tinuous Galerkin FEM. Graphical Models 71, 4, 153 167. Spe­cial Issue of ACM SIGGRAPH / Eurographics 
Symp. Comp. Anim. 2008. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503392</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Generating eye movement during conversations using Markov process]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503392</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503392</url>
		<abstract>
			<par><![CDATA[<p>Generating realistic eye movements is a significant topic in Computer Graphics(CG) contents production field. Appropriate modeling and synthesis for eye movements are greatly difficult because they have a lot of important features.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190585</person_id>
				<author_profile_id><![CDATA[81504685722]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoyori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190586</person_id>
				<author_profile_id><![CDATA[81492655713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190587</person_id>
				<author_profile_id><![CDATA[81331497177]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kubo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190588</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190589</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[E. Gu, S. P. Lee, J. B. Badler, and N. I. Badler, "Eye Movementss, Saccad es, and Multiparty conversations", Data-Driven 3D Facial animation, pp79--97, December 11, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2342999</ref_obj_id>
				<ref_obj_pid>2342896</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Iwao, D. Mima, H. Kubo, A. Maejima, S. Morishima," Analysis and Synthesis of Realistic Eye Movement in Face-to-face Communication", Siggraph 2012, August 5-9, 2012]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generating Eye Movement during Conversations C:\Users\Tomoyori\Desktop\.1.bmp Using Markov Process Tomoyori 
Iwao*  Daisuke Mima  Hiroyuki Kubo  Akinobu Maejima  Shigeo Morishima Waseda University    Figure 
1. Proposed Markov Process 1. Introduction Generating realistic eye movements is a significant topic 
in Computer Graphics(CG) contents production field. Appropriate modeling and synthesis for eye movements 
are greatly difficult because they have a lot of important features. Gu et al[2007] proposed a method 
for automatically synthesizing realistic eye movements during conversations according to probability 
models. Despite eye movements during conversations include both saccades and fixational eye movements 
(FEMs), they synthesized only saccades which are relatively large eye movements. We proposed a method 
for automatically synthesizing both saccades and FEMs[Iwao et al 2012]. While we classified eye movements 
accurately, we did not consider the time dependency of eye movements at all. For example, an eyeball 
often moves in the same direction to the previous direction, however, we could not express such eye movements 
in our previous work. In addition, we often synthesized unnatural eye movements like consecutive large 
movements. It is necessary to consider the time dependency of eye movements because a prospective eye 
movement has a strong relationship to the present one. We propose a method for synthesizing realistic 
eye movements and blinks during face-to-face conversations using the first order Markov process model. 
Our contributions can be summarized as follows. 1-We appropriately analyze eye movements using the first 
order Markov process derived from actual measurements. 2-We easily synthesize realistic eye movements 
by simply inputting the initial eye movement state. 2. Measurement of Eye Movements and Blinks We measured 
eye movements of 21 male and female subjects during actual conversations using EMR-9, an eye tracker 
manufactured by NAC Image Technology. This device can measure eye movements according to the corneal 
reflection. During the measurements, we set the distance between two subjects to 1.5 meters that they 
are able to have a conversation comfortably. Meanwhile, we also recorded videos of blinks by a camcorder 
to analyze them after measurements. To capture eye movements accurately, we requested the subjects not 
to move their head during measurements. 3. Analysis and Synthesis We analyze eye movements and blinks 
using the first order Markov process to consider the time dependency of these movements. We separate 
the whole frames of acquired movies into every six frames. Six frames correspond to one node of a Markov 
process. First, we define states in a Markov process. Eye movements during conversations include two 
different movements, saccades and FEMs. In our previous work, we can classify eye movements into saccades 
and FEMs by setting the threshold of angular rotation of the eye to two degrees. Therefore, states in 
a Markov process consist of three parts, such as saccades, FEMs and blinks. Second, we determine the 
state of the nodes according to angular rotation of eyes from actual eye movements. We also acquire the 
initial state probabilities. Third, we unite the consecutive same state nodes into a state cluster of 
nodes. For example, we unite three saccade nodes into a saccade cluster of three nodes. We also calculate 
state transition probabilities between the two same state clusters. Next, we define eye movement elements 
to accurately represent eye movements. We classify saccade into four elements, such as angular rotation, 
angular direction and duration time. We also classify blinks into two elements, blink time and blink 
time interval. We do not classify FEMs into elements because they are very small movements and difficult 
to analyze. We use uniform random number to synthesize FEMs. We calculate each element transition probability 
between two same state clusters by using element values in clusters. We also acquire the initial element 
probabilities. Finally, we synthesize eye movements and blinks by using both state transition probabilities 
and element transition probabilities with the Markov process trained from actual measurements. We present 
the synthesis result with the first order Markov process in Figure1. 4. Conclusions and Discussions 
In this paper, we propose a method for synthesizing realistic eye movements and blinks during face-to-face 
conversations using the first order Markov process model. Using our model which considers time-dependent 
transition, we are able to synthesize reliable human eye animations in arbitrary situations without any 
additional eye motion capturing. To figure out the relationships between eye movements and other human 
body actions, such as head motions and hand gestures, are our future works. References E.Gu,S.P.Lee,J.B.Badler,andN.I.Badler, 
EyeMovementss,Saccades,and Multiparty conversations , Data-Driven 3D Facial animation,pp79-97,December 
11, 2007 T.Iwao,D.Mima, H.Kubo, A.Maejima, S.Morishima , Analysis and Synthesis of Realistic Eye Movement 
in Face-to-face Communication ,Siggraph 2012, August 5-9, 2012 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503393</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Humanlike behavior model with probabilistic intention]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503393</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503393</url>
		<abstract>
			<par><![CDATA[<p>Animating the motion of a realistic crowd of pedestrians has been an important goal in the field of computer graphics, video games and movies. Pedestrians in the real world seem to consider not only collision avoidance but also interaction with other pedestrians. Thus, it is required to develop pedestrian models that produce humanlike behavior which includes social interactions such as giving-way in order to simulate the realistic motion of pedestrians. Human behaviors and activities have been studied with various approaches. Social Force Model (SFM) describes each pedestrian's motion by virtual forces from its target point, other pedestrians and surrounding environment [Helbing and Moln&#225;r 1995]. Each individual motion is described by differential equations and calculated in parallel. SFM is one of the most suitable models to simulate the motion of pedestrians and there are several extensions to simulate pedestrian behaviors in large spaces by applying some rules to SFM (e.g., [Pelechano et al. 2007]). However, these studies treated pedestrian intention as the internal state which consists of several discrete states though the human intention in the real world is continuous and ambiguous. As a result, humanlike behaviors could not be produced when the intention of other pedestrian is unclear.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190590</person_id>
				<author_profile_id><![CDATA[82459339057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ibe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ibe@robot.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190591</person_id>
				<author_profile_id><![CDATA[82458898157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gakuto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masuyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[masuyama@robot.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190592</person_id>
				<author_profile_id><![CDATA[82458660757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamashita@robot.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190593</person_id>
				<author_profile_id><![CDATA[82458614957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hajime]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[asama@robot.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Helbing, D., and Moln&#225;r, P. 1995. Social force model for pedestrian dynamics. <i>Physical Review E 51</i>, 4282--4286.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1272705</ref_obj_id>
				<ref_obj_pid>1272690</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pelechano, N., Allbeck, J. M., and Badler, N. I. 2007. Controlling individual agents in high-density crowd simulation. In <i>Proceedings of the 2007 ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA2007)</i>, 99--108.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Humanlike Behavior Model with Probabilistic Intention Naoki Ibe, Gakuto Masuyama, Atsushi Yamashita, 
Hajime Asama The University of Tokyo. Figure 1: Give-way behavior at a narrow passage. (a), (b): Pedestrian 
A (blue) cannot decide to whether give-way or not because the intention of B(red) is unclear. Hence hesitation 
of Ais produced, which means Aalternates between (a) directing to the passage and (b) giving-way. (c): 
Aenters the passage because it is estimated that Bgives way. (d), (e): Benters the passage after Ahas 
passed. 1 Introduction Animating the motion of a realistic crowd of pedestrians has been an important 
goal in the .eld of computer graphics, video games and movies. Pedestrians in the real world seem to 
consider not only collision avoidance but also interaction with other pedestri­ans. Thus, it is required 
to develop pedestrian models that pro­duce humanlike behavior which includes social interactions such 
as giving-way in order to simulate the realistic motion of pedestri­ans. Human behaviors and activities 
have been studied with various approaches. Social Force Model (SFM) describes each pedestrian s motion 
by virtual forces from its target point, other pedestrians and surrounding environment [Helbing and Moln 
´ ar 1995]. Each indi­vidual motion is described by differential equations and calculated in parallel. 
SFM is one of the most suitable models to simulate the motion of pedestrians and there are several extensions 
to simu­late pedestrian behaviors in large spaces by applying some rules to SFM (e.g., [Pelechano et 
al. 2007]). However, these studies treated pedestrian intention as the internal state which consists 
of several discrete states though the human intention in the real world is con­tinuous and ambiguous. 
As a result, humanlike behaviors could not be produced when the intention of other pedestrian is unclear. 
In this paper, the pedestrian model that produces humanlike behav­iors by considering probabilistic intention 
of pedestrians is demon­strated. The pedestrian intention is represented as a superposition of orthogonal 
states that correspond to behaviors such as giving­way and directing to the goal. We also propose a new 
method to predict the pedestrian motion by the inverse calculation of SFM. 2 Approach It must be necessary 
to take into account probabilistic intention of pedestrians and estimate other s intention by long-term 
prediction of other s motion, in order to realize humanlike behaviors. Introduction of probabilistic 
intention In our model, SFM is adapted to calculate actual motion and our model of pedestrians is designed 
to switch their behavior by changing subgoal that corre­sponds to the target point in SFM. We introduce 
a mechanism to select a suitable subgoal according to the probabilistic intention of the pedestrian at 
the moment. To introduce the probabilistic in­tention, this research focused on the idea of quantum information 
theory. Pedestrian intention is represented by quantum state |.. .e-mail:{ibe, masuyama, yamashita, asama}@robot.t.u-tokyo.ac.jp 
called intention vector. |.. is a superposition state as follows: X X |.. = ci |si. , |ci|2 = 1 (1) i 
i where ci are complex coef.cients, and |si. are orthogonal states called intention state and they correspond 
to the behaviors that are mutually exclusive. |ci|2 represents occurrence probability of |si. at each 
step. Hence each behaviors is produced from each intention state according to the corresponding probability. 
Estimation of other pedestrian intention Each pedestrian pre­dicts the future route of other pedestrian 
by the inverse calculation of the SFM. Firstly, a pedestrian estimates the direction of other s subgoal 
from other s motion and surrounding environment by in­verse calculation of SFM. Then, using the estimated 
direction of other s subgoal, future positions of other pedestrian is predicted se­quentially by SFM. 
The point set of the future positions is con­sidered as the future route. Finally, other s intention 
is estimated according to the evaluation value calculated along the future route from the function whose 
value depends on the distance between the future position at each moment and subgoals. Each pedestrian 
changes its intention according to the estimated intention of other pedestrian and produce the suitable 
behavior. 3 Implementation and Future Work We applied the proposed model in the situation where there 
is a nar­row passage between two obstacles. In the experiment, two pedes­trians approached each side 
of the passage and attempted to pass to the other side. It was shown that one pedestrian generates give-way 
behavior and the other enters the passage (Figure 1). In addition, our model can describe hesitation 
of pedestrians that cannot be realized by conventional models. Currently, we extend the model to generate 
other humanlike behaviors and to be valid for various environment and intention. References HELBING, 
D., AND MOLN ´Social force model for AR, P. 1995. pedestrian dynamics. Physical Review E 51, 4282 4286. 
 PELECHANO, N., ALLBECK, J. M., AND BADLER, N. I. 2007. Controlling individual agents in high-density 
crowd simula­tion. In Proceedings of the 2007 ACM SIGGRAPH/Eurographics Symposium on Computer Animation 
(SCA2007), 99 108. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503394</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Nutty tracks]]></title>
		<subtitle><![CDATA[symbolic animation pipeline for expressive robotics]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503394</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503394</url>
		<abstract>
			<par><![CDATA[<p>NuttyTracks is a symbolic real-time animation system for animating any robotic character using animation tools commonly used by professional animators. Our system brings artists and programmers closer to each other in the quest for creating the illusion of life in robotic characters.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[animation]]></kw>
			<kw><![CDATA[human-robot interaction]]></kw>
			<kw><![CDATA[robotics]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190594</person_id>
				<author_profile_id><![CDATA[81488665021]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tiago]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ribeiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Lisbon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tiago.ribeiro@inesc-id.pt]]></email_address>
			</au>
			<au>
				<person_id>P4190595</person_id>
				<author_profile_id><![CDATA[81436595004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paiva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Technical University of Lisbon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ana.paiva@inesc-id.pt]]></email_address>
			</au>
			<au>
				<person_id>P4190596</person_id>
				<author_profile_id><![CDATA[81482656804]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Doug]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dooley]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Willow Garage]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ddooley@willowgarage.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2157813</ref_obj_id>
				<ref_obj_pid>2157689</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gielniak, M. J., and Thomaz, A. L. 2012. Enhancing interaction through exaggerated motion synthesis. In <i>Proc. of ACM/IEEE on Human-Robot Interaction</i>, HRI'12, 375--382.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2157814</ref_obj_id>
				<ref_obj_pid>2157689</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ribeiro, T., and Paiva, A. 2012. The illusion of robotic life: Principles and practices of animation for robots. In <i>Proc. of ACM/IEEE on Human-Robot Interaction</i>, HRI'12, 383--390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Nutty Tracks -Symbolic Animation Pipeline for Expressive Robotics Tiago Ribeiro, Ana Paiva* Doug Dooley 
 INESC-ID &#38; Instituto Superior T´Willow Garage ecnico, Technical University of Lisbon Figure 1: 
Interaction between NuttyTracks, Autodesk R. and real robots. . 3ds Max R Abstract NuttyTracks is a symbolic 
real-time animation system for animat­ing any robotic character using animation tools commonly used by 
professional animators. Our system brings artists and programmers closer to each other in the quest for 
creating the illusion of life in robotic characters. Keywords: animation, robotics, human-robot interaction 
1 Introduction and Motivation Robotic characters are becoming widespread as useful tools for both assistive 
and entertainment applications in the domain of human-robot interaction. Research on robot animation 
has there­fore been struggling to overcome the physically limited expressivity that is inherent to a 
robotic embodiment. We have previously anal­ysed how traditional principles of animation can be used 
in robot animation [Ribeiro and Paiva 2012]. However, current generic robot animation and interaction 
systems require deep mathematical and programming skills, which makes it dif.cult for artists to col­laborate 
with programmers in order to achieve the illusion of life in robotic characters. This work provides an 
artist-oriented animation system that can work with any robotic embodiment. 2 Our Approach Professional 
animators often use commercial software packages such as Autodesk R. software. We have developed an . 
3ds Max Ranimation system called Nutty Tracks that can integrate with these software packages in interaction 
scenarios. Artists can therefore use their usual tools and software to design robot animation inde­pendently 
of its embodiment. The challenge of animating a robotic character with any kind of embodiment was overcome 
by devel­oping a symbolic animation pipeline (Figure 2). The pipeline is composed by Layers, which are 
in turn composed of a sequence of Animation Controllers (AC). Each AC produces and outputs an An­imationBuffer. 
Each Layer s ACs are traversed in a speci.c order. The output of one AC can be used as the input of the 
next one. The .nal AnimationBuffer computed in a Layer is then blended with those of the other Layers, 
thus producing a .nal Animation Frame. The AnimationBuffer is composed of two .elds. The Meta-Data contains 
frame information, and a reference to the BodyModel that the animation data corresponds to; The Animation 
Data is a list of pairs (Cn, V) where Cn is the name of a channel from the Body-Model, and V a value 
representing an angle or intensity. A Body­ *e-mail:{tiago.ribeiro, ana.paiva}@inesc-id.pt e-mail:ddooley@willowgarage.com 
 Figure 2: Nutty Tracks integrated with Autodesk R. soft­ . 3ds Max Rware through the NuttyMax plug-in. 
AL represents a generic Ani­mation Layer, while AC represents a generic Animation Controller. Model describes 
a robot body by listing its available channels, and their hierarchy. Each channel is associated to a 
unique name and a degree-of-freedom. After each iteration of the animation cycle, the computed AnimationBuffer 
is sent to the body, which can then interpret each channel, and act upon its physical motor controllers. 
The BodyModel is also used by the plug-in on the external anima­tion software to generate a skeleton 
that artists can animate. In order to allow incremental animation without over.owing the robot with repeated 
data (causing unresponsiveness and jitter), each frame is .ltered before being sent to the Body Interface, 
in order to remove channels containing data that does not signi.cantly differ from the previous frame. 
By bringing animation to a symbolic level, we will be able to con­nect it with Arti.cial Intelligence 
systems. It will also allow us to procedurally generate and apply operators or .lters on the anima­tion 
curves in real-time, independently of the robot used. We are currently developing animation .lters that 
can provide any robotic character with some of the principles of animation, such as Slow In/Slow Out 
or Exaggeration [Gielniak and Thomaz 2012]. We are also trying to understand to what extent and situations 
can anima­tions be generically generated for any given robot. Finally, we will study how this system 
actually makes it easier for professional ani­mators to work with robots on interactive animation scenarios, 
and how that re.ects on the overall perception of lifelikeness.  Acknowledgements This work was supported 
by EU FP7 project EMOTE under grant agreement no. 215554 and 317923, and also by national funds through 
FCT, under project PEst-OE/EEI/LA0021/2011 and through the PIDDAC Program funds. References GI E LN 
I AK, M. J., A N D TH O M A Z , A. L. 2012. Enhancing in­teraction through exaggerated motion synthesis. 
In Proc. of ACM/IEEE on Human-Robot Interaction, HRI 12, 375 382. RI BEI RO , T., AN D PAI VA , A. 2012. 
The illusion of robotic life: Principles and practices of animation for robots. In Proc. of ACM/IEEE 
on Human-Robot Interaction, HRI 12, 383 390. Permission to make digital or hard copies of part or all 
of this work for personal or classroom use isgranted without fee provided that copies are not made or 
distributed for commercial advantage and that copies bear this notice and the full citation on the first 
page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>FCT</funding_agency>
			<grant_numbers>
				<grant_number>PEst-OE/EEI/LA0021/2011</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>EU FP7 project EMOTE</funding_agency>
			<grant_numbers>
				<grant_number>215554, 317923</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503395</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Photorealistic inner mouth expression in speech animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503395</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503395</url>
		<abstract>
			<par><![CDATA[<p>We often see close-ups of CG characters' faces in movies or video games. In such situations, the quality of a character's face (mainly in dialogue scenes) primarily determines that of the entire movie. Creating highly realistic speech animation is essential because viewers watch these scenes carefully. In general, such speech animations are created manually by skilled artists. However, creating them requires a considerable effort and time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190597</person_id>
				<author_profile_id><![CDATA[82459070457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masahide]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[doara-waseda@toki.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190598</person_id>
				<author_profile_id><![CDATA[81504685722]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomoyori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190599</person_id>
				<author_profile_id><![CDATA[81492655713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190600</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190601</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073388</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chang, Y.-J., and Ezzat, T. 2005. Transferable Videorealistic Speech Animation, <i>SCA'05</i>, pp. 143--151.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Irie, A., Takagiwa, M., Moriyama, K. and Yamashita T. 2011. Improvements to Facial Contour Detection by Hierarchical Fitting and Regression, <i>1</i>&#60;sup&#62;<i>st</i>&#60;/sup&#62; <i>ACPR</i>, pp. 273--277.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531363</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mohammed, U., Prince, J. D. -S., and Kautz, J., 2009. Visio-lization: Generating Novel Facial Images, <i>SIGGRAPH'09</i>, pp. 57(1)--57(8).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Photorealistic Inner Mouth Expression in Speech Animation C:\Users\masa\Dropbox\kawai\08_SIGGRAPH\Abstract_sig\....\.3.png 
 Masahide KAWAI* Tomoyori IWAO Daisuke MIMA Akinobu MAEJIMA Shigeo MORISHIMA Waseda University Figure 
1: Overview of our method and result. 1. Introduction We often see close-ups of CG characters faces 
in movies or video games. In such situations, the quality of a character s face (mainly in dialogue scenes) 
primarily determines that of the entire movie. Creating highly realistic speech animation is essential 
because viewers watch these scenes carefully. In general, such speech animations are created manually 
by skilled artists. However, creating them requires a considerable effort and time. To solve this problem, 
we propose a method to automatically synthesize a speech animation by embedding a realistic inner mouth 
into an existing low-quality or inner mouth-less speech animation. To the best of our knowledge, there 
is little comparable work. Chang et al. [2005] proposed the system that is somewhat similar to ours. 
However, the appearance of inner mouth in the resulting animation is collapsed because the inner mouth 
morphs along with lip movement. To resolve this morphing artifact, we estimate teeth positions from human 
anatomy and estimate tongue movements from phonetics. Then, we synthesize more natural looking inner 
mouth images. Our contribution is to provide a post effect filter that improves the quality of speech 
animation created by any previous technique, as shown in the supplementary video. 2. Database Construction 
We construct database for each of the regions using the following procedures. First, we capture a video 
of an arbitrary subject gradually opening his/her mouth. Then, the teeth are cut from the images acquired 
from the video. Second, sets of consecutive tongue images of an arbitrary subject pronouncing phoneme 
combinations are acquired to preserve original continuous tongue movements as much as possible. Here 
the phoneme combinations are defined according to the visibility of the tongue; the start is invisible, 
the middle is visible, and the end is invisible. These combinations contain all variations of tongue 
movements that appear in spoken English. Finally, we capture videos of seven subjects (except for the 
target subject in the input animation) pronouncing consonants and representative symbolic sounds produced 
by different articulation regions. If a target subject s image which the teeth can be seen is available, 
the teeth-database can be automatically updated with the target subject s teeth images. This allows a 
representation of individuality. * e-mail: doara-waseda@toki.waseda.jp e-mail: shigeo@waseda.jp 3. Embedding 
the Inner Mouth The overview of our method is shown in Figure 1 (a). From a human skull bone structure, 
we assume that the distance from the position of the Anterior Nasal Spine (ANS) to the central position 
of the upper teeth is always constant. Similarly, the distance from the position of the chin to the central 
position of the lower teeth is also constant. To embed teeth into an input animation with these assumptions, 
we must first detect ANS and chin feature points using a feature point detector developed by Irie et 
al. [2011]. Then the distance between the central position of the upper and lower teeth is calculated 
using the detected feature points. Next, teeth images that have the closest teeth distance to that of 
the input animation are selected from the teeth-database. In addition, consecutive tongue image sets 
are also selected from the database according to sets of phoneme combinations on the basis of sentence 
information. Image sets are connected at the invisible point of the tongue. Finally, we reconstruct the 
images around the mouth using patch-based texture synthesis proposed by Mohammed et al. [2009] with the 
mouth-database. 4. Results and Discussions The result is shown in Figure 1 (b) and the supplementary 
video. In the supplementary video, we demonstrate how our method is effective for significantly improving 
the quality of a low-quality input animation. Note that our method can represent inner mouth appearances, 
such as teeth nipping tongue s tip or tongue s back. Such representations have been difficult to produce 
with previous methods. One possible limitation of our method is robustness in different lighting conditions 
between the input animation and images in the databases. However, thanks to a seamless cloning technique, 
we can synthesize natural inner mouth images even if different lighting conditions exist. Therefore, 
we conclude that our method is useful as a post effect filter to improve the quality of an input speech 
animation created by any previous method. References CHANG, Y.-J., AND EZZAT, T. 2005. Transferable 
Videorealistic Speech Animation, SCA 05, pp. 143-151. IRIE, A., TAKAGIWA, M., MORIYAMA, K. AND YAMASHITA 
T. 2011. Improvements to Facial Contour Detection by Hierarchical Fitting and Regression , 1st ACPR, 
pp. 273-277. MOHAMMED, U., Prince, J. D. -S., AND KAUTZ, J., 2009. Visio-lization: Generating Novel Facial 
Images, SIGGRAPH 09, pp. 57(1)-57(8). 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503396</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[PhysPix]]></title>
		<subtitle><![CDATA[instantaneous rigid body simulation of rasters]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503396</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503396</url>
		<abstract>
			<par><![CDATA[<p>Modern physics engines process collisions by leveraging vector representations (e.g. <i>Box2D</i> or <i>Open Dynamics Engine (ODE)</i>), which means that artists who work with pixel-based 2D content must map their pixel drawings onto representations such as Delaunay triangulations [Shewchuk 1996]. Effects such as destruction then require remeshing, which can be onerous to perform at runtime. The alternative is pixel-perfect collision handling, but past games such as <i>Worms!</i> and <i>Scorched Earth</i> that use this approach have not attempted true rigid body simulations. We present <i>PhysPix</i>, a 2D rigid body simulation framework based on pixels. PhysPix allows 1) artist control over the exact boundaries used for objects in the simulation, 2) natural bitmap-based support for destruction, and 3) an intuitive painting interface for properties such as non-uniform weight distributions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190602</person_id>
				<author_profile_id><![CDATA[81556377456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Domagoj]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bari&#269;evi&#263;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[domagoj@cs.ucsb.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190603</person_id>
				<author_profile_id><![CDATA[82458765057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schaffer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[james_schaffer@cs.ucsb.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190604</person_id>
				<author_profile_id><![CDATA[81452596386]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Theodore]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kim@mat.ucsb.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>673287</ref_obj_id>
				<ref_obj_pid>645908</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Shewchuk, J. R. 1996. Triangle: Engineering a 2d quality mesh generator and delaunay triangulator. In <i>Applied computational geometry towards geometric engineering</i>. Springer, 203--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PhysPix: Instantaneous Rigid Body Simulation of Rasters Domagoj Bari .cevi´c, James Schaffer, Theodore 
Kim University of California, Santa Barbara*  Figure 1: From left to right: a) Terrain destroyed by 
erasing the ground bitmap, the rigid-body simulation is instantly updated. b) Some example pixel normals 
-red pixels belong to the reference object, blue pixels belong to the colliding object, the purple center 
pixel represents the actual intersection, white arrows show partial kernel normals, and the local pixel 
normal is shown in green. c) Two intersecting objects (red and blue) and their intersection -critical 
pixels are highlighted in purple while gray pixels are not used to determine normal; local normals for 
each pixel are shown in green, these are averaged to generate the surface normal for the collision. 1 
Introduction Modern physics engines process collisions by leveraging vector representations (e.g. Box2D 
or Open Dynamics Engine (ODE)), which means that artists who work with pixel-based 2D content must map 
their pixel drawings onto representations such as De­launay triangulations [Shewchuk 1996]. Effects such 
as destruc­tion then require remeshing, which can be onerous to perform at runtime. The alternative is 
pixel-perfect collision handling, but past games such as Worms! and Scorched Earth that use this ap­proach 
have not attempted true rigid body simulations. We present PhysPix, a 2D rigid body simulation framework 
based on pixels. PhysPix allows 1) artist control over the exact boundaries used for objects in the simulation, 
2) natural bitmap-based support for de­struction, and 3) an intuitive painting interface for properties 
such as non-uniform weight distributions. 2 Approach PhysPix asks artists for a single bitmap for each 
object, and uses this bitmap as-is in the rigid body simulation. The challenge lies in extracting the 
components needed for rigid body response (sur­face normal, penetration depth) from arbitrary bitmaps. 
Sequential computation can be expensive, even for simple cases. Weight distribution and collision geometry 
-information that de­termines the behavior of a rigid body -are naturally encoded into a bitmap for 2D 
objects. The minimum amount of information re­quired from the artist is the sprite that represents the 
object graph­ically, but the artist can also additionally specify two optional bitmaps: the collision 
mask, and the weight distribution mask. By default PhysPix will use the alpha channel of the sprite to 
deter­mine the collision shape of the object, the collision mask can be used to override this shape with 
a custom one. PhysPix computes the center of gravity and inertia tensor from the supplied bitmaps. By 
default this information is extracted from the alpha channel of the sprite (or the collision mask), but 
can be be optionally extracted from the weight distribution mask. PhysPix s default method treats *e-mail: 
{domagoj, james schaffer}@cs.ucsb.edu, kim@mat.ucsb.edu each opaque pixel in the sprite as unit mass. 
If the weight distribu­tion mask is speci.ed, each pixel is assigned a relative mass based on the intensity 
of its color. Using the explicit method allows a con­tent creator to intuitively specify the precise 
weight distribution of any object in any arbitrary way -a feature which is more dif.cult to expose in 
a vector-based engine. In image processing, the task of summarizing bitmaps has been well studied and 
is known to easily map onto parallel processors. By processing bitmaps in parallel using compute shaders, 
PhysPix makes the dynamic extraction of normal and depth from intersect­ing bitmaps practical for real-time 
simulation. At runtime, PhysPix processes collisions between bitmaps on the GPU and uses a 2D­con.gured 
ODE for its rigid body simulation. At the time of col­lision, the intersecting bitmap (collision raster) 
is thrown onto the GPU to determine the collision s normal, depth, and approximate location. The location 
of the collision is simply the collision in­tersection s center of mass. For each pixel in the collision 
raster, a local normal is generated by inspecting a 3x3 kernel centered at the pixel (Fig 1b). Only the 
pixels on the surface of the collision (critical pixels) generate local normals that are useful. For 
each critical pixel, the local normal is generated by inspecting the pixel s 8 possible neighbors. This 
method gives a normal for each pixel on the surface of the collision (Fig 1c). Local normals are averaged 
into the global normal for the collision raster. The global normal is then leveraged to determine the 
depth of the collision: the collision raster is rotated so that the normal is aligned with the rows in 
the bitmap, then the GPU is used to .nd the maximum of these rows which corresponds to the depth of the 
collision. PhysPix enables fully destructible environments on the individual pixel level. Since all collisions 
are represented as raster intersec­tions, updating the collision geometry is a simple matter of updating 
the bitmap representing the object or environment (Fig 1a). References SH E W C HUK , J. R. 1996. Triangle: 
Engineering a 2d quality mesh generator and delaunay triangulator. In Applied computational geometry 
towards geometric engineering. Springer, 203 222. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use isgranted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503397</article_id>
		<sort_key>120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Raycast based auto-rigging method for humanoid meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503397</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503397</url>
		<abstract>
			<par><![CDATA[<p>In character animation, skeletons are used to animate meshes. Those skeletons are inserted inside the mesh after being drawn. This insertion, called rigging, is time consuming since it must be done for each mesh.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190605</person_id>
				<author_profile_id><![CDATA[82459214957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Romain]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lopez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESGI Paris]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190606</person_id>
				<author_profile_id><![CDATA[81549423056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christophe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Poirel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[ESGI Paris]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276467</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baran, I. and Popovic, J., 2007. Automatic rigging and animation of 3d character. Proceedings of ACM SIGGRAPH 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730810</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Miller, C., Arikan, O., and Fussell, D., 2010. Frankenrigs: building character rigs from multiple sources. Proceedings of ACM SIGGRAPH 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Raycast based auto-rigging method for humanoid meshes Romain Lopez, Christophe Poirel of ESGI Paris 
 (a) (b) (c) (d) Figure 1: (a) Mesh silhouette , (b) Silhouette outline, (c) Raycasting inside a limb 
slice, (d) Arm slices centers. 1 Introduction In character animation, skeletons are used to animate meshes. 
Those skeletons are inserted inside the mesh after being drawn. This insertion, called rigging, is time 
consuming since it must be done for each mesh. Autorigging is the automation of this process. In this 
work, we propose an automation which requires no human intervention and will identify and place each 
bone of the skeleton inside a mesh. Other works were done on the same subject : Pinocchio which uses 
mesh discretization [Baran 2007] and Frankenrigs which is based on a body parts database [Miller 2010] 
. 2 Body map In the .rst part, we will try to build a body map : we will scan the mesh and try to identify 
all the limbs. The limbs we are looking for are based on the bones we have in the skeleton : arms, legs, 
body and head. The .rst step is to reduce the complexity of our 3D point cloud (the mesh). In order to 
do so, we are going to make a serie of frontal ray casts. Each raycast will be sent with a regular spacing. 
We will keep all intersection points with the mesh and set a common depth for them. By the end of this 
.rst step, we will then have what we called the silhouette of the mesh (Figure 1a), which looks like 
a 2D point cloud projection, but with a regular spacing between each point. Since we know the spacing 
between each point, we can easily keep only the outline points of the cloud (Figure 1b). The result still 
is recognizable as a human shape, and less complex than the 3D point cloud. What we want to do next is 
to set these points in a way that will allow the program to read the silhouette. Just like if you started 
to draw this .gure with a pen, you would for example start by drawing one point in the head, then the 
next point close to it... until you go back to the .rst point. You will then have a navigation path, 
which starts at the .rst point you drew and goes all around the silhouette. We will be building three 
paths, one arch that goes inside the legs, two others that go on the side of the mesh. This can be done, 
be­cause we used a regular spacing, and we know the distance between two neighbor points. Using these 
tracks and knowing the standard rigging positions, we will be able to determine each limb. We will be 
doing a speci.c algorithm for the legs, arms, body and head. At the end of the .rst part, we will have 
our body map ready. 3 Bone placement The bone placement consists in .nding the best location for each 
joint of the skeleton. We we will use the body map to .nd entry points in the mesh for each limb. These 
entry points will be used as start locations and stop locations for the following algorithm. We start 
with the arm entry point. The armpit height will be the height limit, i.e. the stop condition of the 
algorithm. From the entry point inside the arm, we do a 360 degree raycast in the horizontal plane (Figure 
1c). These rays will hit the mesh from inside and we keep the closest intersection points. A polygon 
is made from these intersection points. It represents a simpli.ed slice of the arm. Now the goal is to 
.nd the center of this slice. To do so, we will .nd the barycenter of the polygon by applying a weight 
on each point depending on the distance to its two neighbors. This way the barycenter s position will 
not be affected by the local points den­sity. From this barycenter, we will go down the arm. Again, at 
each slice, we do a raycast to .nd the next barycenter. The algorithm stops when the next point is outside 
the mesh. Then we start again at the arm entry point, but this time we go upward. The algorithm stops 
when it reaches the armpit height (Figure 1d). The barycen­ters found at each step are candidates for 
the joint positions. We use other entry points from the body map to .nd the legs and the spine. The last 
step is to position the armature bones. From the barycen­ters, we must .nd the best candidates for the 
different joints loca­tions.To do so, we use the armature skeleton information. For each bone, we take 
the ratio between its length and the total length of the limb bones. Then, inside the mesh, we sum the 
distance between each barycenter to get the total length of the correponding limb. Now we can use the 
bones ratios to .nd which barycenters will be the locations of the joints. References BA R A N , I. 
AN D PO POV I C, J., 2007. Automatic rigging and ani­mation of 3d character. Proceedings of ACM SIGGRAPH 
2007. MILL E R, C., AR I KA N , O., AN D FUS S ELL , D., 2010. Frankenrigs: building character rigs from 
multiple sources. Proceedings of ACM SIGGRAPH 2010. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503398</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Reproduction of the behavior of the wet cloths taking the atmospheric pressure into account]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503398</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503398</url>
		<abstract>
			<par><![CDATA[<p>The property of cloth changes when it gets wet. Especially, wet cloth sticks to objects being touched. When expressing a wet cloth in computer graphics, pseudo representation is often used. As existing research, [Gascon et al. 2010] added a constrain to the part of cloth where it touches the base object. This constraint is a force which makes the cloth continue sticking to the position while the cloth is touching. In real-world situations, however, such sticking forces act even on the part which stays off the base.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190607</person_id>
				<author_profile_id><![CDATA[82458990057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[g311103518@st.teu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190608</person_id>
				<author_profile_id><![CDATA[82458962357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Taichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190609</person_id>
				<author_profile_id><![CDATA[82458697057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Masanori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190610</person_id>
				<author_profile_id><![CDATA[81328489444]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Koji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mikami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190611</person_id>
				<author_profile_id><![CDATA[81456623756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ryota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Silicon Studio Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566624</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kwang-Jin Choi, and Hyeong-Seok Ko. 2002. Stable but responsive cloth. <i>ACM Transactions on Graphics (SIGGRAPH 2002 Proceedings)</i> 21, 3, 604--611.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1921434</ref_obj_id>
				<ref_obj_pid>1921427</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jorge Gasc'on, Javier S. Zurdo, and Miguel A. Otaduy. 2010. Constraint-based simulation of adhesive contact. In <i>Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, SCA '10, 39--44.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Reproduction of the behavior of the wet cloths taking the atmospheric pressure into account Wataru 
Yamada , Taichi Watanabe , Masanori Kakimoto , Koji Mikami , Ryota Takeuchi Tokyo University of Technology, 
Silicon Studio Corporation. g311103518@st.teu.ac.jp   1. Introduction  The property of cloth changes 
when it gets wet. Especially, wet cloth sticks to objects being touched. When expressing a wet cloth 
in computer graphics, pseudo representation is often used. As existing research, [Gascon et al. 2010] 
added a constrain to the part of cloth where it touches the base object. This constraint is a force which 
makes the cloth continue sticking to the position while the cloth is touching. In real-world situations, 
however, such sticking forces act even on the part which stays off the base. We assume that the cause 
of this sticking force is a difference of air pressures. We observe that the air often does not pass 
through wet cloths and that wet cloths stick to the base object. As a result, the air between the wet 
cloth and the base object is often shut off from the open air. When an external pulling force acts on 
a point on the cloth, it lowers the internal pressure of the isolated air. The higher pressure from the 
open air presses the cloth toward the base object as is the case with a suction cup sticking to a wall. 
This pressing force causes a unique deformed shape of the cloth (Figure 1). By taking the air pressure 
into account, we realized a physically-based cloth simulation focusing on the case that the whole cloth 
is wet. 2. Our Method We used a mass spring model [Choi and Ko 2002] to simulate the standard behavior 
of cloth. To reproduce the sticking of wet cloth, we applied forces of the pressures of both trapped 
air and the open air to each mass point, as well as other forces such as gravity, spring tensions, and 
external pulling forces. The internal pressure is determined by Boyle-Charle's law taking into account 
the volume change of the trapped air. We assume that the amount of trapped air does not change. The volume 
of the trapped air is computed by integrating the volume of each triangular prism (vt) whose base is 
a vertically-projected triangle from each element triangle (At) of the cloth (Figure 2). The height of 
the triangular prism is the height of the center of gravity of the polygon (ht). 3. Results Figure 1 
is a pair of comparisons between real wet cloth shapes and our simulation results. The overall frame 
rate is 64 fps for a cloth consisting of 5000 triangles, using a single core of Core i7 (2.93GHz) with 
12GB main memory. 4. Conclusions We have presented a physically-based simulation method for the behavior 
of wet cloths, taking air pressures into account. Currently, the base object shape is limited to a plane. 
Future work includes handling of more general situations such as the base shape variations. References 
 Kwang-Jin Choi, AND Hyeong-Seok Ko. 2002. Stable but responsive cloth. ACM Transactions on Graphics 
(SIGGRAPH 2002 Proceedings) 21, 3, 604-611. Jorge Gasc´on, Javier S. Zurdo, and Miguel A. Otaduy. 2010. 
Constraint-based simulation of adhesive contact. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics 
Symposium on Computer Animation, SCA 10, 39 44. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503399</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<display_no>13</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Speech-driven realtime lip-synch animation with viseme-dependent filters]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503399</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503399</url>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190612</person_id>
				<author_profile_id><![CDATA[82458970257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shin'ichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503400</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Visual simulation of glazed frost]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503400</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503400</url>
		<abstract>
			<par><![CDATA[<p>Glazed frost is a crystal clear ice and formed from supercooled raindrops that freeze when they hit object surfaces such as the ground and branches of trees. Simulation methods for formation of ice crystal, such as frost, on the surface of objects have been proposed by Kim et al. [Kim et al. 2004]. However, a supercooling state has to be considered for simulating freezing rain, and fluid simulation is required for reproducing the effect of raindrops running down on the ice surfaces. To our best knowledge, there has been no research presenting glazed frost by using a fluid simulation. We use the fluid simulation based on FLIP method [Zhu and Bridson 2005]. Hence, raindrops and obstacles are represented by particles which are used to calculate the advection term, and the update of velocity field are calculated by using grids except for advection term. We propose a method to create an animation of glazed frost formation by taking into account the heat transfer between particles and the outside grids.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190613</person_id>
				<author_profile_id><![CDATA[81555820056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomokazu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ishikawatm@stf.teu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190614</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University/JST CREST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190615</person_id>
				<author_profile_id><![CDATA[81555874356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yonghao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University/JSPS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190616</person_id>
				<author_profile_id><![CDATA[82458697057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Masanori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190617</person_id>
				<author_profile_id><![CDATA[82458962357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Taichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Watanabe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190618</person_id>
				<author_profile_id><![CDATA[82458886157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kunio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kondo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190619</person_id>
				<author_profile_id><![CDATA[81331494952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wakayama University/UEI Research]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190620</person_id>
				<author_profile_id><![CDATA[82458820457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UEI Research/Hiroshima Shudo University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jones, K. F. 1996. Ice accretion in freezing rain. Tech. rep., DTIC Document.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028564</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kim, T., Henson, M., and Lin, M. C. 2004. A hybrid algorithm for modeling ice formation. In <i>Proceedings of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, SCA '04, 305--314.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073298</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zhu, Y., and Bridson, R. 2005. Animating sand as a fluid. <i>ACM Trans. Graph. 24</i>, 3 (July), 965--972.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visual Simulation of Glazed Frost Tomokazu Ishikawa . Yonghao Yue Taichi Watanabe Kei Iwasaki Tokyo 
University of Technology Columbia University / JSPS Tokyo University of Technology Wakayama University 
/ UEI Research Yoshinori Dobashi Masanori Kakimoto Kunio Kondo Tomoyuki Nishita Hokkaido University /JST 
CREST Tokyo University of Technology Tokyo University of Technology UEI Research / Hiroshima Shudo University 
(a) 1,000 timesteps (b) 4,000 timesteps (c) 7,000 timesteps (d) 10,000 timesteps Figure 1: Simulation 
and rendering results of generation of glazed frost. 1 Introduction Glazed frost is a crystal clear ice 
and formed from supercooled rain­drops that freeze when they hit object surfaces such as the ground and 
branches of trees. Simulation methods for formation of ice crystal, such as frost, on the surface of 
objects have been proposed by Kim et al. [Kim et al. 2004]. However, a supercooling state has to be considered 
for simulating freezing rain, and .uid simulation is required for reproducing the effect of raindrops 
running down on the ice surfaces. To our best knowledge, there has been no research presenting glazed 
frost by using a .uid simulation. We use the .uid simulation based on FLIP method [Zhu and Bridson 2005]. 
Hence, raindrops and obstacles are represented by particles which are used to calculate the advection 
term, and the update of velocity .eld are calculated by using grids except for advection term. We propose 
a method to create an animation of glazed frost formation by tak­ing into account the heat transfer between 
particles and the outside grids. 2 Our Simulation Method The heat transfer from a particle to the grid 
is calculated by using the FLIP method similar to the velocity .eld. The heat transfer be­tween particles 
and grids is calculated according to the Fourier s law. Heat transfer is not calculated between particles, 
but calcu­lated between grids. Heat transfer between grids is calculated by solving heat transfer equations. 
After calculating heat transfer, the heat is distributed to the particles. To update the velocity .eld, 
a time step is set according to the CFL conditions. We employ the Crank-Nicolson method to solve the 
heat conduction equation (1). .2 .T = a T 2 , (1) .t .xwhere T is a temperature, t is time, a is a thermal 
diffusivity and x is position vector. We decide the time to freeze by considering the heat .ux on the 
surface of the raindrop. Amount of heat .ux is calculated until freezing from the adhesion on the obstacles. 
The speed for freezing is decided by the following sum of heat .ux. Q = Qs + Ql + Qf (2) It is known 
that the smaller the sum of .ux Q in negative value is, the faster the freezing speed is [Jones 1996]. 
So we use as the condition for freezing or not whether the time integral of Qs and Ql .e-mail: ishikawatm@stf.teu.ac.jp 
(a) Simulation result of glazed frost (b) Simulation result of glazed frost taking into account the wind 
of icicle shape Figure 2: Simulation results of generating glazed frost. which have negative value exceeds 
the required amount of heat Qf . The integration calculation is limited to the time during the water 
drop adheres on the obstacles, and we assume that, in case the water drop does not adhere on the glazed 
frost or obstacles because of the in.uence of gravity or other raindrops, it does not freeze. 3 Results 
and Conclusion Figure 2 shows two simulation results of generating glazed frost. We can represent that 
supercooled water in some cases freezes in­stantly after collision, and in some cases gradually condensates 
in the process of running down on objects. This is the result by con­sidering heat .ux and can be reproduced 
like this because of cal­culation of the time to freezing. By this effect, ice covering tree branches, 
as typically seen in glazed frost, can be reproduced. We have proposed a technique which is highly compatible 
for FLIP method to solve the heat conduction equation and we have repro­duced formation of glazed frost. 
  References JO N E S , K. F. 1996. Ice accretion in freezing rain. Tech. rep., DTIC Document. KI M 
, T., HE NSO N , M., A N D LI N , M. C. 2004. A hybrid algorithm for modeling ice formation. In Proceedings 
of the 2004 ACM SIGGRAPH/Eurographics symposium on Computer animation, SCA 04, 305 314. ZH U , Y., A 
N D BR ID S O N , R. 2005. Animating sand as a .uid. ACM Trans. Graph. 24, 3 (July), 965 972. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503401</article_id>
		<sort_key>160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Wrinkle flow for compact representation of predefined clothing animation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503401</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503401</url>
		<abstract>
			<par><![CDATA[<p>Animations of a clothed character play a key role in enriching visual realism in various CG applications. However, the challenge remains in finding a compact representation of predefined clothing animations while preserving fine-scale details such as wrinkles and folds. In this work, we present a method to decouple clothing animation into a skinning and a non-skinning component. The skinning component controls the general look of a clothing mesh, which is derived from the underlying kinematic character animation; the non-skinning component mainly contributes to the formation of wrinkles, which is further reduced into a lower dimensional subspace to support a compact representation as depicted in Kry <i>et al</i>.'s [2002] work. From a practical viewpoint, our method offers a simple, fast, and effective way to compactly encode predefined clothing animations with fine details.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190621</person_id>
				<author_profile_id><![CDATA[82458766057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Byung-Uck]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bukim@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P4190622</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[farbizf@i2r.a-star.edu.sg]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>545286</ref_obj_id>
				<ref_obj_pid>545261</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kry, P. G., James, D. L., and Pai, D. K. 2002. Eigenskin: real time large deformation character skinning in hardware. In <i>Proceedings of the 2002 ACM SIGGRAPH/Eurographics symposium on Computer animation</i>, ACM, New York, NY, USA, SCA '02, 153--159.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wrinkle Flow for Compact Representation of Prede.ned Clothing Animation Byung-Uck Kim Farzam Farbiz 
Institute for Infocomm Research, A*STAR, Singapore *  Figure 1: From left to right, (a) Original animation 
(Ground truth), (b) Skinned clothing animation, (c) Wrinkle Flow, (d) Our method. 1 Introduction Animations 
of a clothed character play a key role in enriching vi­sual realism in various CG applications. However, 
the challenge remains in .nding a compact representation of prede.ned cloth­ing animations while preserving 
.ne-scale details such as wrinkles and folds. In this work, we present a method to decouple cloth­ing 
animation into a skinning and a non-skinning component. The skinning component controls the general look 
of a clothing mesh, which is derived from the underlying kinematic character anima­tion; the non-skinning 
component mainly contributes to the forma­tion of wrinkles, which is further reduced into a lower dimensional 
subspace to support a compact representation as depicted in Kry et al. s [2002] work. From a practical 
viewpoint, our method offers a simple, fast, and effective way to compactly encode prede.ned clothing 
animations with .ne details. 2 Our Approach Input data Clothing animations on a virtual character can 
be pre­recorded through cloth simulation and/or artist s direct manipula­tion. In our experiment, we 
employ a commercial cloth simulation package 1 to animate a clothing mesh on top of a skinned character 
animation from the CMU motion capture database 2 . Such prede­.ned clothing animation with m frames can 
be represented with 1 2 m i the matrix X = [x , x , . . . , x ], where xj . R3 indicates j-th i . R3n 
 vertex position at i-th frame and x for n vertices (thus, size(X) O(3n × m)). Skinning component with 
Deformation Transfer In general, a clothing mesh conforms to the underlying character pose and shape, 
and this is particularly pertinent for tight-.tting garments such as shirts and pants. We exploit this 
observation to extract the skin­ning component of clothing animation. To this end, we employ a simple 
vertex-to-vertex deformation transfer technique; assuming that xj (a clothing mesh s vertex) corresponds 
to vk (a character mesh s vertex) in terms of proximity (l2 distance through a kd-tree) in their rest 
pose, we simply transfer vk s bone transformations (Tb) with bone-vertex in.uence weights (wkb ) to xj 
and rewrite the typ­ i x ical linear blend skinning (LBS) as x.= ( wkb Tb i)xj . This j b.B approach 
enables us not only to easily obtain the compact represen­tation of clothing animation by reusing the 
character skinning for­mula, but also to quickly approximate the general look of a clothing mesh in a 
kinematic fashion. However, the resulting deformation *e-mail:{bukim, farbizf}@i2r.a-star.edu.sg 1http://www.marvelousdesigner.com 
 2http://mocap.cs.cmu.edu  may suffer from a lack of .ne-scale wrinkles; the .ne details are diminished 
and smoothed out (Fig. 1(b)). Non-skinning component with Wrinkle Flow To enrich a cloth­ing mesh with 
high quality wrinkles, we handle wrinkles at the individual vertex level instead of per bone. Thus, we 
compute i ii per-vertex residual vector aj = xj - x j and evaluate the matrix 1 2 m i of wrinkle .ow 
A = [a , a , . . . , a ], where a . R3n (thus, A . R3n×m). However, the wrinkle .ow itself is not the 
ideal representation in terms of compactness. Two observations lead us to a subspace method for reducing 
the data size of wrinkle .ow: 1) the overall appearance of the clothing mesh has already been estab­lished 
at the skinning stage, and 2) the residual vector .eld mostly handles the remaining subtle component. 
Thus, we do not need to retain the full data set to accurately approximate the original wrin­kle .ow. 
In this case, it is good to use principal component analy­sis (PCA) to reduce the data while preserving 
important features. In A = USV with SVD, by taking k principal components with the largest variance values 
in S, we can obtain the projection matrix 1 2 k Ur = [u , u , . . . , u ], where u i . R3n (thus, Ur 
. R3n×k), and transform the original wrinkle .ow into the reduced subspace with i = UT a projection 
of z r a i, where z i . Rk. As a result, we obtain 1 2 mthe matrix Z = [z , z , . . . , z ], where Z 
. Rk×m. This method enables us to signi.cantly reduce the data size of wrinkle .ow up to O(3n × k + k 
× m) from O(3n × m) because k is far smaller than m in most cases. In contrast to the previous work [Kry 
et al. 2002], our wrinkle .ow involves the effects of wrinkle formation due to not only pose, but also 
dynamics variation. Reconstruction Our animation pipeline is straightforward. We start with a clothing 
mesh in a rest pose. Next, we perform LBS. Then, we reconstruct wrinkle .ow. Finally, we combine them 
to­ ii i gether to complete the clothing animation: x j = x.j + (Urz )j = ( x b.B wkb Tb i)xj + (Urz 
i)j . Conclusion We believe that our system is an advance for cloth­ing animation, especially in CG applications 
such as videogames or VR simulations, where virtual characters with various costumes are commonplace 
and their combined animations usually prede.ned. References KRY, P. G., JA M E S , D. L., A N D PAI 
, D. K. 2002. Eigenskin: real time large deformation character skinning in hardware. In Proceedings of 
the 2002 ACM SIGGRAPH/Eurographics sympo­sium on Computer animation, ACM, New York, NY, USA, SCA 02, 
153 159. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503402</section_id>
		<sort_key>170</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art and design]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>2503403</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Activity theory and interface design for autism treatment]]></title>
		<subtitle><![CDATA[tracking, collaborating, and enriching the classroom experience]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503403</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503403</url>
		<abstract>
			<par><![CDATA[<p>Autism is a growing health issue in today's youth that necessitates careful monitoring and commitment to education. iPads have quickly proven to be a great tool in assisting autistic children through prompting, interactive educational games, and language development. However, the instructors and therapists of autistic children can also significantly benefit from iPads as an interactive tool to more quickly and accurately track student data, and ultimately be able to teach the child in a more individualized and precise way. Activity theory considers an entire working activity system (including teams and organizations) beyond just one user, focusing on consciousness, the relationship between people and things, and the role of artifacts (objects, such as an iPad) in everyday life. By analyzing data collected from instructors of autistic students, we can identify areas of sensorial, emotion, and interactions for maximizing user experience for three distinct collaborative audience segments: teachers, therapists, and students. With that understanding, we can then evaluate, ideate, and describe the functionality of an iPad interface that allows careful data tracking and collaboration to best position an autistic student for academic success.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190623</person_id>
				<author_profile_id><![CDATA[82458977957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Courtney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Marchese]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Quinnipiac University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>223826</ref_obj_id>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nardi, Bonnie A. 1995. Context and Consciousness: Activity Theory and Human-Computer Interaction.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Garrett, Jesse James. 2011. The Elements of User Experience.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Activity Theory and Interface Design for Autism Treatment: Tracking, Collaborating, and Enriching the 
Classroom Experience Courtney Marchese,Quinnipiac University   (a) (b) (c) Figure 1: (a) paper data 
tracking of autistic children, (b) one portion of the extensive flowchart for Nedu, (c) initial ideation 
of data tracking 1  Introduction Autism is a growing health issue in today s youth that necessitates 
careful monitoring and commitment to education. iPads have quick­ly proven to be a great tool in assisting 
autistic children through prompting, interactive educational games, and language develop­ment. However, 
the instructors and therapists of autistic children can also significantly benefit from iPads as an interactive 
tool to more quickly and accurately track student data, and ultimately be able to teach the child in 
a more individualized and precise way. Activity theory considers an entire working activity system (in­cluding 
teams and organizations) beyond just one user, focusing on consciousness, the relationship between people 
and things, and the role of artifacts (objects, such as an iPad) in everyday life. By analyzing data 
collected from instructors of autistic students, we can identify areas of sensorial, emotion, and interactions 
for max­imizing user experience for three distinct collaborative audience segments: teachers, therapists, 
and students. With that understand­ing, we can then evaluate, ideate, and describe the functionality 
of an iPad interface that allows careful data tracking and collaboration to best position an autistic 
student for academic success. 2  Motivation and Approach The idea for this app came from a discussion 
with a special needs teacher in Bronx, New York, who indicated that tracking autistic students with stacks 
of papers on clipboards is cumbersome and inefficient. Items currently tracked in the classroom by hand 
in­clude skill mastery, play scenarios, cooperative play, and verbal skills (fig 1a). Between timing 
and recording, there is little room to collect extra beneficial data that could potentially be revealed 
if using a well-designed app. Ideally, data notes on behavior frequen­cy, proportion, episodes, duration, 
latency, inter-response time, in­tensity, and behavior quality could all be recorded at various times 
with a customizable iPad app. The data collected on the iPad would then be sent and stored on a secure 
website for parents, teachers, and doctors to view and analyze. From there, more comprehensive treatment 
programs could be developed, graphed out data from the short and long term could be printed, and progress 
could be closely monitored. To successfully execute this complex app requires un­derstanding of theory 
and thorough planning. Activity theory suggests that when people interact with an environ­ment, they 
produce outward mental processes that can later become communicable and useful for social interaction. 
Any task, or activi­ty, can be broken down into actions that are further subdivided into operations. 
For design purposes, these categories can provide un­derstanding of the steps necessary for a user to 
carry out a task. We may assume that an individual can and usually does participate in several simultaneous 
activities, and that those activities are contin­uously changing and developing. By analyzing and understanding 
the everyday activity of autistic children, teachers, and therapists, we can successfully design applications 
that aid in effective com­munication and documentation in the classroom. To most seamless­ly accomplish 
this, we can turn to gestural interface technology by combining three key elements - sensory, emotion, 
and interactions - that must be utilized to their fullest to maximize user experience. Ultimately, Nedu 
is an iPad app design concept created carefully from the bottom planes to the top. The word Nedu is based 
on the Spanish word nido , meaning nest , which is known as a safe and nurturing, carefully-crafted structure 
to develop in. The app is designed as a series of data collecting tools to make autism treat­ment easier 
and less time-consuming for teachers, therapists, and students. All data is used to generate charts and 
graphs to better monitor progress and share amongst a team of people seeking to treat autism in an educational 
environment. With functional goals in mind, we can plan categories and information flow for getting users 
to successfully complete desired tasks. The immediate feedback, functionality, and opportunity for collab­oration 
make Nedu an effective classroom tool for tracking autism, and offers promising opportunity for tracking 
other illnesses and disorders through gestures and interactivity. References Nardi, Bonnie A. 1995. Context 
and Consciousness: Activity Theo­ry and Human-Computer Interaction. Garrett, Jesse James. 2011. The Elements 
of User Experience.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503404</article_id>
		<sort_key>190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Amazing sketchbook]]></title>
		<subtitle><![CDATA[extended drawing on a sketchbook using 3DCG]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503404</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503404</url>
		<abstract>
			<par><![CDATA[<p>Drawing on a sketchbook is one of the most familiar arts for us, and people of all ages can enjoy it. Thus a lot of CG applications on which a user can create 2D and 3DCG images with drawing operations have been developed [Igarashi et al. 1999]. These applications are useful for many people to create CG without special knowledge or technique. However it is necessary to use special equipment such as an LCD tablet for true drawing operations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190624</person_id>
				<author_profile_id><![CDATA[82458721057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nanako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kondo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aichi Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[b13718bb@aitech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190625</person_id>
				<author_profile_id><![CDATA[82459169757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Saori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aichi Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190626</person_id>
				<author_profile_id><![CDATA[81547432256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizuno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aichi Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s_mizuno@aitech.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>311602</ref_obj_id>
				<ref_obj_pid>311535</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Igarashi, T., Matsuoka, S., Tanaka, H., 1999, Teddy: A Sketching Interface for 3D Freeform Design, <i>SIGGRAPH '99</i>, pp. 409--416.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2503488</ref_obj_id>
				<ref_obj_pid>2503385</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goto, S., Kondo, N., Mizuno, S., 2013, RAKUGACKY: making sounds with drawing, <i>SIGGRAPH 2013 Posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Amazing Sketchbook: extended drawing on a sketchbook using 3DCG Nanako Kondo Saori Goto Shinji Mizuno 
Aichi Institute of Technology {b13718bb, s_mizuno}@aitech.ac.jp  Figure 1: In our tool, a user just 
draws a picture on an ordinary sketchbook with ordinary color pens (a). A video camera monitors a picture 
on the sketchbook. The tool analyzes the picture and deforms a virtual plane on which the picture is 
mapped (b). A 3DCG image is synthesized from the picture in real time (c). It is possible to deform the 
3DCG with fingers interactively (d). 1. Introduction Drawing on a sketchbook is one of the most familiar 
arts for us, and people of all ages can enjoy it. Thus a lot of CG applications on which a user can create 
2D and 3DCG images with drawing operations have been developed [Igarashi et al. 1999]. These applications 
are useful for many people to create CG without special knowledge or technique. However it is necessary 
to use special equipment such as an LCD tablet for true drawing operations. In this paper, we propose 
a novel media tool "Amazing Sketchbook" that can extend drawing on a sketchbook using 3DCG. A user just 
draws a picture with ordinary color pens on an ordinary sketchbook. Our tool monitors a picture on a 
sketchbook, and synthesizes a 3DCG image in real time. Using our tool, a user can enjoy creating a 3DCG 
image during drawing a picture on a sketchbook and can see changes of the original picture in the 3DCG. 
 2. Synthesizing a 3DCG image from a 2D image Figure 1 shows the process of creating a 3DCG image with 
our tool. Our tool is composed of a PC and a video camera in addition to a physical sketchbook and color 
pens. The information of the colors of pens is given to the PC. A user draws a picture on the sketchbook 
with the pens, the video camera of the tool monitors the sketchbook, and the PC synthesizes 3DCG images 
in real time. Viewed in extending drawing on a sketchbook, we use a virtual plane constructed of triangular 
patches and deform the plane to synthesize a 3DCG from a 2D image. The whole virtual plane corresponds 
to the whole sketchbook. Each frame image of the video camera is mapped on the virtual plane in succession, 
and the virtual plane is deformed based on the frame images during drawing. As a result, the user could 
get experience of using special pens that can not only paint, but also deform a sketchbook. The ways 
of deformation is decided by analyzing frame images of the video camera. A frame image is separated into 
regions based on colors. In each region, the distance to the closest background pixel (Figure 2(a)) and 
the distance to the lowest pixel of the region are calculated for all pixels in the region (Figure 2(b)), 
and two types of distance images are generated. The circularity of each region is also calculated. The 
parts of the virtual plane corresponding to each region are deformed based on the distance images (Figure 
2(a)(b)). The way of deformation is changed by the color and the circularity of each region.  (a) (b) 
Figure 2: Deformation of virtual plane based on two types of distance images. 3. Experiments In Figure 
3(a), a 3DCG image is generated gradually during drawing a picture on a sketchbook. Yellow and red regions 
are piled up, flowers are added in green regions, and the parts of the 3DCG image are vibrating. In Figure 
3(b), green regions are turned up like a pop-up book. When the user touches the picture on the sketchbook, 
the distance images are changed and the 3DCG image is deformed interactively (Figure 1(d)). We could 
observe that many children enjoy extended drawing and gained motivation to draw pictures through the 
use of our media tool. Using analyzing results of the picture, it is possible to put various sound sources 
for each region in the 3DCG space and synthesize stereo sound for the picture interactively [Goto et 
al. 2013].  (a) (b) Figure 3: Examples of 3D CG images created by children. Reference IGARASHI, T., 
MATSUOKA, S., TANAKA, H., 1999, Teddy: A Sketching Interface for 3D Freeform Design, SIGGRAPH '99, pp. 
409-416. GOTO, S., KONDO, N., MIZUNO, S., 2013, RAKUGACKY: making sounds with drawing, SIGGRAPH 2013 
Posters. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503405</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Augmented participatory design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503405</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503405</url>
		<abstract>
			<par><![CDATA[<p>Although various tools have been developed to transform 2D sketches to 3D geometry on digital platforms, they are still far from offering the same rich experience that manipulation of physical models can provide. It is particularly important for designers to have access to their familiar tools, such as physical prototyping models and traditional pens and paper, in order to support design activities.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190627</person_id>
				<author_profile_id><![CDATA[81498640947]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sheng-Ying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[aithpao@media.mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190628</person_id>
				<author_profile_id><![CDATA[81100470656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Larson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab, Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kll@mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2407716</ref_obj_id>
				<ref_obj_pid>2407707</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Pao, S-Y., Reben, A., Larson K. 2012. FlickInk: Bridging the Physical and Digital for Creative Work. In <i>SIGGRAPH Asia 2012 Emerging Technologies (SA '12)</i>, Article 9, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Van Krevelen, D. W. F., And Poelman, R. 2010. A Survey of Augmented Reality Technologies, Applications and Limitations. <i>International Journal of Virtual Reality</i>, 9(2), 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Augmented Participatory Design Sheng-Ying PaoKent Larson MIT Media Lab MIT Media Lab Cambridge, MA, 
USACambridge, MA, USAaithpao@media.mit.edu kll@mit.edu Figure1.  Augmented remote reality, leveraging 
familiar design tools such as pen, paper and prototyping building blocks. (A) The physicalprototype is 
manipulated by a remote collaborator, with whom a designer communicates through a videoconference. (B) 
The pen gesture­based Augmented Participatory Design system captures sketches. (C) Flicking the pen towards 
the model on the screen instantly transfersthe paper sketch to the corresponding spot on the remote model, 
with which the remote collaborator creates new physical prototypes (D). 1. Introduction Although various 
tools have been developed to transform 2Dsketches to 3D geometry on digital platforms, they are still 
farfrom offering the same rich experience that manipulation of physical models can provide. It is particularly 
important fordesigners to have access to their familiar tools, such as physicalprototyping models and 
traditional pens and paper, in order to support design activities. Augmented reality attempts to address 
such problems by re­integrating electronic information back into the real world [VanKrevelen and Poelman 
2010]. However, it also introduces newchallenges: First, the augmented physical surfaces require digitalinput 
and the digital content to be overlaid is usually pre-defined,making it difficult to utilize pen and 
paper in real time. This alsoneglects the designer s need to incorporate their intuitive sketchesas they 
design and modify prototypes. As we further move into adigital-oriented augmented reality, the well-known 
modality ofpen and paper has become increasingly segregated in its ownanalog world. In addition, the 
majority of the augmented reality solutions focuson the combination of virtual elements and local objects. 
As knowledge work becomes more distributed and mobile, the challenges come from not only the gaps between 
the physical and digital worlds, but also from those between the remote workspaces, especially when the 
physical prototypes are not accessible to the distant collaborators. Registering remote real­world objects 
and precisely matching the corresponding real-timecontent generated from familiar input devices is increasinglyimportant 
to innovations for augmented participatory design. 2.1 Augmented Remote Reality An innovative application 
for augmented reality is to connect people through technology, leveraging familiar tools in the realtime. 
Rather than immersing people in an artificially createddigital world, we developed a pen-mediated system 
for augmentedremote design (Figure 1). Wireless sensing technologies and computer graphics algorithms 
were developed, with the goal ofaugmenting objects in remote locations to provide a wealth ofdigital 
information and communication capabilities for remotecollaborators. This is accomplished through the 
FlickInk system[Pao et al. 2012], with its recent development in wireless sensingcapability, augmented 
reality, MIT CityScope s parametric 3Dcomputer models and urban simulations.  3. Editing Physical Objects 
with Familiar Tools Instead of replacing physical objects with computers, we createdsystems that allow 
people to interact with the real world withfamiliar tools (Figure 2). The augmented pen interaction offersopportunities 
to manipulate physical models remotely as well asto dynamically annotate physical prototypes in co-located 
designactivities. Gesturing at surrounding environments with the penenables augmented expression and 
superimposes design drafts onto the intended physical objects. This allows designers and collaborators 
to take advantage of using familiar analog design tools such as pen and paper to interact with surrounding 
objectsand benefit from the power of augmented reality in real time. Figure 2. Tapping the pen on co-located 
physical objects allowsfor augmented annotation of the physical model with real-timewritten content. 
 References PAO, S-Y., REBEN, A., LARSON K. 2012. FlickInk: Bridging the Physical and Digital for Creative 
Work. In SIGGRAPH Asia 2012 Emerging Technologies (SA '12), Article 9, ACM. VAN KREVELEN, D. W. F., AND 
POELMAN, R. 2010. A Survey of Augmented Reality Technologies, Applications and Limitations. International 
Journal of Virtual Reality, 9(2), 1. Permission to make digital or hard copies of part or all of this 
work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503406</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Color assignment via region area and color harmony]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503406</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503406</url>
		<abstract>
			<par><![CDATA[<p>Unlike an expert, an ordinary user may find it difficult to assign colors to regions when considering harmony and purpose. Various conditions should be considered, such as the size of the arranged region, the relationship between the arranged colors, and the purpose of the color uses. In this study, with these conditions in mind, we suggest a novel method for assigning a color each region. Recently, a large number of psychologists have proposed a new framework for color harmony as an alternative to the traditional color harmony theory. In these studies, experiments were carried out to examine the degree of preference or harmony between colors by a large number of subjects [Ou and Luo 2006] [Szab et al. 2010]. According to Munsell [Munsell 1921], color combinations are balanced or harmonious when stronger colors occupy less space than weaker colors. Balance is achieved when area X value (brightness) X chroma (saturation) is equivalent in two regions. We suggest a new color assignment method based on the theory of Ou and Munsell.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190629</person_id>
				<author_profile_id><![CDATA[81492649545]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hye-Rin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hrkim@cs.yonsei.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190630</person_id>
				<author_profile_id><![CDATA[81416606232]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Min-Joon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[debussy@cs.yonsei.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190631</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iklee@yonsei.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Munsell, A. 1921. <i>A Grammar of Color</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ou, L.-C., and Luo, M. R. 2006. A colour harmony model for two-colour combinations. <i>Color Research and Application 31</i>, 3, 191--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Szab, F., Bodrogi, P., and Schanda, J. 2010. Experimental modeling of colour harmony. <i>Color Research and Application 35</i>, 1 (Feb.), 34--39.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Color Assignment via Region Area and Color Harmony Hye-Rin Kim* Min-Joon Yoo In-Kwon Lee Yonsei University 
Yonsei University Yonsei University  Figure 1: An examaple of different four con.gurations applied with 
our method. (a) Con.guration with connectivity of regions. (b) Con.gu­ration without background region. 
(c) Con.guration with background region (d) Con.guration with text 1 Introduction Unlike an expert, an 
ordinary user may .nd it dif.cult to assign colors to regions when considering harmony and purpose. Various 
conditions should be considered, such as the size of the arranged re­gion, the relationship between the 
arranged colors, and the purpose of the color uses. In this study, with these conditions in mind, we 
suggest a novel method for assigning a color each region. Recently, a large number of psychologists have 
proposed a new framework for color harmony as an alternative to the traditional color harmony theory. 
In these studies, experiments were carried out to examine the degree of preference or harmony between 
colors by a large num­ber of subjects [Ou and Luo 2006] [Szab et al. 2010]. According to Munsell [Munsell 
1921], color combinations are balanced or har­ monious when stronger colors occupy less space than weaker 
col­ors. Balance is achieved when area × value (brightness) × chroma (saturation) is equivalent in two 
regions. We suggest a new color assignment method based on the theory of Ou and Munsell. 2 Our Approach 
and User Test Given regions to be assigned color and some colors, we assign a color to a region respectively 
and maximize the sum of our energy function when all colors are assigned to all regions. Our energy function 
has two conditions. The .rst condition is for harmony between colors. The harmony of all color pairs 
is computed using [Ou and Luo 2006] s theory, and we set two normalized weights as the distance and adjacent 
length between two areas. In other words, a shorter distance between regions and a greater adjacent length 
between areas has a greater effect on maximizing the total energy value. The second condition is for 
area between regions. We assign colors in each region based on balance theory by Munsell [Munsell 1921]. 
To test our proposed method, we conducted a survey by apply­ing the actual variety of color samples and 
using the con.gura­tions of several different regions. We then used the colors pro­vided by Adobe Kuler 
as the sample data. We used eight color themes and three region con.gurations, and then conducted a sur­vey 
of twenty-six participants. Each participant chose the combina­tion they deemed most harmonious and preferable 
among the four possible combinations with different rankings. We set the scores from 4 to 1 in descending 
order of rank. *e-mail:hrkim@cs.yonsei.ac.kr e-mail:debussy@cs.yonsei.ac.kr e-mail:iklee@yonsei.ac.kr 
As a result, the .rst con.guration recorded an average of 2.75; the second, an average of 2.64; and the 
third, an average of 2.86. An average score of around 4 indicates that the participants chose the best 
combination among the candidates made using our proposed method. The results of all three con.gurations 
came close to an average score of 2.75, which shows the effectiveness of our method. We added a con.guration 
with text additionally. To test the consistency of the users color assignment and obtain more information, 
we performed a second experiment and inter­view with four of the participants from the .rst experiment, 
.ve days later. The second experiment was carried out using the color samples and con.guration of the 
regions used in the .rst experi­ment, and the consistency between the two experiments was ap­proximately 
65%. However, the second experiment result of four participants recorded an average of 3.0, whereas the 
.rst recorded an average of 2.80. By interviewing four participants, we organized their comments as follows: 
1) Both background and wide area have the greatest impact. 2) The distinction between the areas should 
be sure. 3) Dark or strong colors can be better to assign to the narrow regions than the wide part of. 
4) If the connectivity exists between the regions, the colors also have connectivity. Based on the results 
of the interview, Comment 1), 2), and 3) can be applied using the proposed method, whereas 4) cannot. 
Because wide area has great effect, as comment 1), we assigned high weight if two regions have many adjacent 
lengths. Comments 2) and 3) can be seen as similar to Munsells theory. About comment 4), we would like 
to proceed with further studies. Acknowledgements This work was supported by the National Research Foundation 
of Korea(NRF) grant funded by the Korea government(MEST) (No. 2012-0008768). References MU NSEL L , 
A. 1921. A Grammar of Color. OU, L.-C., A ND LU O , M. R. 2006. A colour harmony model for two-colour 
combinations. Color Research and Application 31, 3, 191 204. SZ A B, F., BO DRO GI , P., AND SC H A N 
DA , J. 2010. Experimental modeling of colour harmony. Color Research and Application 35, 1 (Feb.), 34 
39. Permission to make digital or hard copies of part or all of this work for personal or classroom use 
isgranted without fee provided that copies are not made or distributed for commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Research Foundation of Korea(NRF) grant funded by the Korea government (MEST)</funding_agency>
			<grant_numbers>
				<grant_number>2012-0008768</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503407</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Generating folding sequences from crease patterns of flat-foldable origami]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503407</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503407</url>
		<abstract>
			<par><![CDATA[<p>The most common form to convey origami is through <i>origami diagrams</i>, which are step-by-step sequences, composed of figures that represent the state of the folded paper, in combination with lines and arrows indicating the position of the folds and the movement of the paper (Figure 1b). With the development of modern techniques of origami design, the range of achievable shapes have increased drastically and the <i>crease pattern</i> (the pattern of creases left on the paper after folding an origami model) has gained importance as an efficient method of documenting origami [Lang 2012]. However, the disadvantage of crease patterns is that it is difficult to use them to re-create the design, since crease patterns show only where each crease must be made and not folding instructions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190632</person_id>
				<author_profile_id><![CDATA[82459291957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hugo]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Akitaya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hugoakitaya@npal.cs.tsukuba.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190633</person_id>
				<author_profile_id><![CDATA[81100209891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba/JST ERATO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mitani@cs.tsukuba.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190634</person_id>
				<author_profile_id><![CDATA[81421600624]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yoshihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanamori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kanamori@cs.tsukuba.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190635</person_id>
				<author_profile_id><![CDATA[81100388191]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yukio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fukui@cs.tsukuba.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>313918</ref_obj_id>
				<ref_obj_pid>313852</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bern, M., and Hayes, B. 1996. The complexity of flat origami. In <i>Proceedings of the seventh annual ACM-SIAM symposium on Discrete algorithms</i>, Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, SODA '96, 175--183.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lang, R. J. 2012. <i>Origami Design Secrets: Mathematical Methods for an Ancient Art. Second Edition</i>. CRC Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mitani, J. 2007. Development of origami pattern editor (ORIPA) and a method for estimating a folded configuration of origami from the crease pattern. <i>IPSJ Journal 48</i>, 9 (sep), 3309--3317.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Generating Folding Sequences from Crease Patterns of Flat-Foldable Origami Hugo A. Akitaya* Jun Mitani 
Yoshihiro Kanamori Yukio Fukui University of Tsukuba University of Tsukuba / JST ERATO University of 
Tsukuba University of Tsukuba  (b) (a) Figure 1: (a) Example of a step sequence graph containing several 
possible ways of simplifying the input crease pattern. (b) Results obtained using the path highlighted 
in orange from Figure 1(a). 1 Introduction The most common form to convey origami is through origami 
dia­grams, which are step-by-step sequences, composed of .gures that represent the state of the folded 
paper, in combination with lines and arrows indicating the position of the folds and the movement of 
the paper (Figure 1b). With the development of modern techniques of origami design, the range of achievable 
shapes have increased drastically and the crease pattern (the pattern of creases left on the paper after 
folding an origami model) has gained importance as an ef.cient method of documenting origami [Lang 2012]. 
However, the disadvantage of crease patterns is that it is dif.cult to use them to re-create the design, 
since crease patterns show only where each crease must be made and not folding instructions. We introduce 
a system to create step-by-step diagrams from crease patterns of .at origami by modeling the origami 
steps as graph rewriting steps. Our system provides automatically traditional origami diagrams notation 
in order to help people inexperienced in folding crease patterns as well as automatizing almost completely 
the time-consuming task of drawing diagrams. 2 Proposed Method Our method tries to obtain a folding 
sequence by reverse engineer­ing the input crease pattern. We simplify the input by rewriting a portion 
of the pattern in order to obtain the previous step. Repeat­ing this rewriting process should eventually 
result in the unfolded square of paper. By reversing the order of the obtained sequence, we have one 
folding sequence that can be used to produce origami diagrams. The rewriting must maintain the resultant 
crease pattern .at foldable. We group creases into paths called re.ection paths by using the theorems 
of local foldability [Bern and Hayes 1996] so that its removal from the crease pattern would not affect 
local .at foldability of any node apart from the two ending nodes of the path. When a re.ection path 
makes a loop or crosses the origami from border to border, we call it a complete re.ection path, and 
its simple removal can produce a valid result (Figure 2a). If such situation does not occur, more complex 
rewriting is re­quired. We cataloged four recurrent types of origami steps that oc­cur on existing origami 
diagrams and called them maneuvers. Each maneuver modi.es the pattern of re.ection paths in a systematic 
*e-mail:hugoakitaya@npal.cs.tsukuba.ac.jp e-mail:{mitani, kanamori, fukui}@cs.tsukuba.ac.jp (a) (b) 
Figure 2: (a) Example of simpli.cation by simple removal of re­.ection path (highlighted in orange). 
(b) Example of simpli.cation by rewriting step (above) using the rewriting rule shown below. way, allowing 
them to be modeled as rewriting steps (Figure 2b). For each maneuver, a rewriting rule is assigned and, 
in order to un­fold the model, a matching must be found inside the crease pattern. For a general state 
of the origami model, there may be more than one matched pattern, originating different results and, 
therefore, different ways to unfold an origami. All the results are organized into the step sequence 
graph, containing different paths between the unfolded paper and the .nal form (Figure 1a). By choosing 
any path, the user can obtain a folding sequence of his/her choice. We implemented a GUI that allows 
navigation through the step se­quence graph from the square towards the desired origami in an intuitive 
way. The chosen sequence can be exported in a vector graphics format, in addition to simple diagrams 
symbols, such as lines and arrows, as shown in Figure 1b. The diagram notation is obtained automatically, 
by comparing the positions of vertices and folds between two consecutive steps. We use the method described 
by [Mitani 2007] to calculate the folded form and also, in order to produce clearer diagrams, introduce 
some distortions in the draw­ings by changing vertex positions based on the layer ordering. References 
BER N , M., AND HAY E S , B. 1996. The complexity of .at origami. In Proceedings of the seventh annual 
ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathe­matics, Philadelphia, 
PA, USA, SODA 96, 175 183. LA N G, R. J. 2012. Origami Design Secrets: Mathematical Meth­ods for an Ancient 
Art. Second Edition. CRC Press. MITA N I, J. 2007. Development of origami pattern editor (ORIPA) and 
a method for estimating a folded con.guration of origami from the crease pattern. IPSJ Journal 48, 9 
(sep), 3309 3317. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503408</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[HandON]]></title>
		<subtitle><![CDATA[a tabletop interface for dynamic erasable handwriting]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503408</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503408</url>
		<abstract>
			<par><![CDATA[<p>Today there are various studies and works using handwritten information. For example, "Drawn"[1] makes us not only draw but also move handwritten illustration with our hand. Furthermore, the author[2] developed handwritten input interface. These works provide us a performance that we have never been. The greatest feature of handwritten is that it has "handwriting" and that is also different from a character of computer. Handwriting is information reflecting personality as same as stature and voice quality. Thus, handwriting may be able to portray yourself. Personal features are limitless and others may be able to make result which you can't.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190636</person_id>
				<author_profile_id><![CDATA[82458831157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Megumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kmagumero@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190637</person_id>
				<author_profile_id><![CDATA[81488673349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190638</person_id>
				<author_profile_id><![CDATA[82459187657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190639</person_id>
				<author_profile_id><![CDATA[81504682817]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Zachary Lieberman, "Drawn", http://thesystemis.com/projects/drawn/ (2013.02)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2343465</ref_obj_id>
				<ref_obj_pid>2343456</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tetsuaki Baba, Yuya Kikukawa, Toshiki Yoshiike, Tatsuhiko Suzuki, Rika Shoji, Kumiko Kushiyama, and Makoto Aoki, "Gocen: a handwritten notational interface for musical performance and learning music", In ACM SIGGRAPH 2012 Emerging Technologies(SIGGRAPH '12). ACM, New York, NY, USA, Article 9, 1 pages.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[POLOT, Friction ballpoint pen, http://www.pilot.co.jp/products/pen/ballpen/gel_ink/frixionball/ (2013.02)]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Kouhei Nishimura, Junghyun Kim, Takeshi Naemura, "AReLaser: Erasable Handwriting Interface with Paper and Pen", IPSJ Interaction 2012, pp. 575--580(2012.3)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HandON : A Tabletop Interface for Dynamic Erasable Handwriting Megumi Kato, Yuya Kikukawa, Tetsuaki 
Baba, Kumiko KushiyamaGraduate School of System Design , Tokyo Metropolitan Universitykmagumero@gmail.com 
1. Introduction Today there are various studies and works using handwritten information. For example, 
Drawn [1] makes us not only drawbut also move handwritten illustration with our hand. Furthermore, the 
author[2] developed handwritten input interface. These works provide us a performance that we have never 
been. The greatest feature of handwritten is that it has handwriting and that is also different from 
a character of computer. Handwriting is information re.ecting personality assame as stature and voice 
quality. Thus, handwriting may beable to portray yourself. Personal features are limitless and others 
may be able to make result which you can t.We have developed loop sequencer using handwritten character 
which can disappear and move. Disappearing and moving arenew elements in works using handwriting and 
the elementsenable handwriting to behave interactively. We think our newkind interface enable audience 
to enjoy performance visually unlike conventional instrument and sequencer as well as proposes new play 
style. 2. Prototype We obtained images of handwritten characters and recognized what kind of characters 
are written. After this, we erased the handwritten characters by Peltier device. Simultaneously, we projected 
the image of the handwritten characters on the paper and user could move handwritten characters projected 
with his finger and play sound when the characters hit others or edge of paper.  3. Implementation In 
this chapter, we explain our implementations to enable disappear, moving and sound handwritten character.First, 
we explain erasing handwritten character. We used thermochromic ink pen[3]. The ink becomes transparent 
when it is heated up to about 60 degrees. In previous study, handwriting was erased by heat of laser[4], 
however, thismethod could not erase large surface. Accordingly, we heated up surface of paper with Peltier 
device (see Figure 2). Then, wecooled Peltier device of surface in contact with paper in order to ensure 
safety after erasing handwritten characters. Second, weexplain moving characters with hand. We implemented 
.nger tracking to move character with hand using image processing. We made binary image by skin detection 
and contour data fromthe image (see Figure 3). Furthermore, we obtained a convex hull with contour data 
and the vertex of convex hull by corner detection. This vertex is the tip of the .nger we need. Handwritten 
characters were moved when this vertex hit it. Finally, we explain character recognition. In our work, 
weassigned characters optional interval according to German-stylesound name. Figure 4 shows a result 
of this processing. Green rectangles are the handwritten characters cut out from an inputimage and put 
on the input image again. Red characters in theupper left corner of the green rectangle are a result 
of character recognition. Figure 2. A state of erasing handwritten character by Peltier device. Figure 
3. left: A result of skin detection, center: Convex hull and corner detect from convex hull, right: A 
result of .ngertracking by our algorithm Figure 4. A result of character recognition. 4. Future work 
We should take advantage of the feature such as a shape, a lineweight and size because we think it leads 
to the realization of amore natural and diverse interaction. Reference [1] Zachary Lieberman, Drawn , 
http://thesystemis.com/ projects/drawn/ (2013.02) [2] Tetsuaki Baba, Yuya Kikukawa, Toshiki Yoshiike, 
Tatsuhiko Suzuki, Rika Shoji, Kumiko Kushiyama, and Makoto Aoki, Gocen: a handwritten notational interface 
for musical performance and learning music , In ACM SIGGRAPH 2012 Emerging Technologies(SIGGRAPH 12). 
ACM, New York, NY, USA, Article 9, 1 pages. [3] POLOT, Friction ballpoint pen, http://www.pilot.co.jp/ 
products/pen/ballpen/gel_ink/frixionball/ (2013.02) [4] Kouhei Nishimura, Junghyun Kim, Takeshi Naemura, 
AR- eLaser: Erasable Handwriting Interface with Paper and Pen , IPSJ Interaction 2012, pp. 575 - 580(2012.3) 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503409</article_id>
		<sort_key>240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[&#924;m<sup>3</sup> &#215; &#8734;]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503409</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503409</url>
		<abstract>
			<par><![CDATA[<p>Ant walks on a wall as well as on the ground, whereas horse cannot make it on a wall. However, if a horse becomes as small as an ant, the horse should be able to walk on the wall. This is because another situation may happen in such a small-scaled world where electrostatic and frictional forces are superior to gravity.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190640</person_id>
				<author_profile_id><![CDATA[81504682630]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Akiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[akisato@iis.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190641</person_id>
				<author_profile_id><![CDATA[82459027857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503410</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Inkantatory paper]]></title>
		<subtitle><![CDATA[interactive paper interface with multiple functional inks]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503410</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503410</url>
		<abstract>
			<par><![CDATA[<p>There has been an increase in the number of solutions dealing with integrating paper and digital information. For the input method, the Anoto pen enables us to capture handwritten content as digital information. For the display method on paper, there are two approaches: (1) projecting visible light and (2) controlling colors of chromic inks. The authors focus on the latter approach since it can dynamically display information on paper in print-like color. Hand-rewriting [Hashida et al. 2012] realized automatic rewrite processing on paper corresponding to handwriting using regular pen and paper. It utilizes a laser-based heating system to erase handwritten characters with bistable thermochromic ink. However, this means that it works just on a special desk equipped with the laser system.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190642</person_id>
				<author_profile_id><![CDATA[82458770857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsujii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tsujii@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P4190643</person_id>
				<author_profile_id><![CDATA[81504685158]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nishimura@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P4190644</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hashida@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P4190645</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[naemura@nae-lab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2396660</ref_obj_id>
				<ref_obj_pid>2396636</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hashida, T., Nishimura, K. and Naemura, T. 2013. Hand-rewriting: Automatic Rewriting like Natural Handwriting, In <i>Proceedings of Interactive Tabletops & Surfaces</i> pp.153--162.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Simon, O and Juergen, S. 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Towards Understanding Erasing-based Interactions: Adding Erasing Capabilities to Anoto Pens, <i>PaperComp</i>. p. 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Inkantatory Paper: Interactive Paper Interface with Multiple Functional Inks E:\Users\NAELAB\tsujii\siggraph 
pic\maaaaap.png E:\Users\NAELAB\tsujii\siggraph pic\key2.jpg E:\Users\NAELAB\tsujii\siggraph pic\meeeemo.png 
 Takahiro Tsujii The University of Tokyo tsujii@nae-lab.org Kohei Nishimura The University of Tokyo nishimura@nae-lab.org 
Tomoko Hashida The University of Tokyo hashida@nae-lab.org Takeshi Naemura The University of Tokyo naemura@nae-lab.org 
Figure 1. Erasing handwritten content   Figure 2. Highlighting printed content    Figure 3. Dynamic 
questionnaires paper 1. Introduction  There has been an increase in the number of solutions dealing 
with integrating paper and digital information. For the input method, the Anoto pen enables us to capture 
handwritten content as digital information. For the display method on paper, there are two approaches: 
(1) projecting visible light and (2) controlling colors of chromic inks. The authors focus on the latter 
approach since it can dynamically display information on paper in print-like color. Hand-rewriting [Hashida 
et al. 2012] realized automatic rewrite processing on paper corresponding to handwriting using regular 
pen and paper. It utilizes a laser-based heating system to erase handwritten characters with bistable 
thermochromic ink. However, this means that it works just on a special desk equipped with the laser system. 
 The aim of this paper is to realize an interactive paper interface that can automatically erase printed 
and handwritten contents and highlight printed contents on paper anywhere depending on a user s handwriting. 
For this purpose, the authors have developed a printing method for multiple functional inks including 
conductive silver ink, reversible thermochromic ink, and bistable thermo-chromic ink. We call this incantatory 
system ``Inkantatory Paper. The merit of the proposed system is that we can keep the flexibility and 
mobility of paper since it does not require any special desk. 2. Inkantatory Paper  The Inkantatory 
Paper system has three technical innovations: (1) it retains the flexibility of paper, so we can use 
it freely without special desks; (2) this is realized by printing multiple functional inks, which enables 
localized heating for changing color of printed and handwritten thermochromic contents and (3) the printing 
system consists of commercially available products: an ink-jet printer and a screen printing kit for 
home use. We utilize three types of functional inks: (a) printable conductive silver ink (MU01, MITSUBISHI 
PAPER MILLS Ltd.), (b) bistable thermochromic ink (FRIXION, PILOT Corp.), and (c) reversible thermochromic 
ink (THERMOCHROMIC Pigments 29°C, QCR Solutions Corp). We confirmed that the conductive ink can be printed 
on the back of a paper by PIXUS iP100 (CANON Inc.) and is useful for heating paper surface by electrifying 
the printed pattern. We filled an Anoto pen with the bistable thermochromic ink that becomes colorless 
at 65°C for handwriting input and automatic erasing [Simon et al. 2010]. The reversible thermochromic 
ink changes its color around 29°C, for instance, red/transparent at 30°C and black at 28°C. We can print 
it on the front side of paper by a screen printing kit (T-shirts Kun, Taiyoseiki Co. Ltd.) for automatic 
highlighting and hiding of printed content. Inkantatory Paper has the following four functions. 1. Erasing 
handwritten content: By electrifying the conductive ink pattern, the temperature of paper surface reaches 
65°C, at which point handwritten FRIXION ink disappears. 2. Highlighting printed content: By electrifying 
the conductive ink pattern, the temperature of paper surface reaches 29°C, at which point printed reversible 
thermochromic ink changes its color. 3. Hiding printed content: the same as in highlighting, but the 
printed reversible thermochromic ink becomes transparent. 4. Capturing: We use Anoto pen in this system, 
so Inkantatory Paper can capture and save handwritten information as input. Applying the above-described 
functions, the authors implemented three types of interactive applications. Figure 1 shows the combination 
of Erasing and Capturing functions. For example, when the user writes something and checks the send box 
on the paper, the handwritten content is captured and sent to the indicated receiver. Then all the handwritten 
letters are auto-matically erased, so the paper is reusable. Figure 2 shows the combination of Highlighting 
and Capturing functions. When the user checks the city's name, the color of the city on a map is turned 
to red. Figure 3 shows an application of dynamic questionnaire paper that uses all the functions. Depending 
on the user's handwriting, the system hides useless content and highlights important content on paper. 
When the user checks the submit box, all the handwritten answers are captured on computer and erased 
on paper. References HASHIDA, T., NISHIMURA, K. AND NAEMURA, T. 2013. Hand-rewriting: Automatic Rewriting 
like Natural Handwriting, In Proceedings of Interactive Tabletops &#38; Surfaces pp.153-162. SIMON, O 
AND JUERGEN, S. 2010 Towards Understanding Erasing-based Interactions: Adding Erasing Capabilities to 
Anoto Pens, PaperComp . p. 4. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503411</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Literacy LABELS]]></title>
		<subtitle><![CDATA[emergent literacy application design for children with autism]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503411</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503411</url>
		<abstract>
			<par><![CDATA[<p>We introduce the design and development of the iPad application, Literacy LABELS, which aims to help children with Autism Spectrum Disorders (ASD) develop the early literacy skills necessary to bridge toward reading and writing. There is a common thread among children with ASD to use decoding skills to read. This ability can even occur before they develop functional verbal communication. Yet, these children struggle with reading for meaning and lack the ability to comprehend text. (Randi, Newman, and Grigorenko 2010) This application is our effort on improving this situation. Labeling is used in many typical development preschool and kindergarten classrooms to help non-readers connect the orthography and the meaning of the word by placing a label on the actual object. Literacy LABELS was designed for the iPad to help children with ASD develop the skills necessary to read for meaning by using this emergent literacy skill.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[application design]]></kw>
			<kw><![CDATA[autism spectrum disorders]]></kw>
			<kw><![CDATA[emergent literacy]]></kw>
			<kw><![CDATA[iPad]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190646</person_id>
				<author_profile_id><![CDATA[81557335656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nancy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rasche]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nrasche@purdue.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190647</person_id>
				<author_profile_id><![CDATA[82459065557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pourcho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jpourcho@purdue.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190648</person_id>
				<author_profile_id><![CDATA[82458846657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shuang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wei93@purdue.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190649</person_id>
				<author_profile_id><![CDATA[81557552956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Cheryl]]></first_name>
				<middle_name><![CDATA[Zhenyu]]></middle_name>
				<last_name><![CDATA[Qian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[qianz@purdue.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190650</person_id>
				<author_profile_id><![CDATA[82458676857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Victor]]></first_name>
				<middle_name><![CDATA[Yingjie]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Purdue University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[victorchen@purdue.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1414475</ref_obj_id>
				<ref_obj_pid>1414471</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Putnam, C., and L. Chong. 2008. "Software and Technologies Designed for People with Autism: What Do Users Want?" In <i>Proceedings of the 10th International ACM SIGACCESS Conference on Computers and Accessibility</i>, 3--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Randi, J., T. Newman, and E. L Grigorenko. 2010. "Teaching Children with Autism to Read for Meaning: Challenges and Possibilities." <i>Journal of Autism and Developmental Disorders</i> 40 (7): 890--902.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2470178</ref_obj_id>
				<ref_obj_pid>2470026</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rasche, N., and C. Z. Qian. 2012. "Work in Progress: Application Design on Touch Screen Mobile Computers (TSMC) to Improve Autism Instruction." In <i>Frontiers in Education Conference (FIE), 2012</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Literacy LABELS: Emergent Literacy Application Design for Children with Autism Nancy Rasche, John Pourcho, 
Shuang Wei, Cheryl Zhenyu Qian, and Victor Yingjie Chen Purdue University*  (a) (b) (c) (d) Figure 1: 
(a) The user persona explains the autistic learner. (b) The two learning methods are scanning QR codes 
on labels and reinforcing skills by playing the game. (c) Sample of the mock-up and formative evaluation 
feedback. (d) Illustration of the final design and graphics. 1 Introduction We introduce the design and 
development of the iPad application, Literacy LABELS, which aims to help children with Autism Spectrum 
Disorders (ASD) develop the early literacy skills necessary to bridge toward reading and writing. There 
is a common thread among children with ASD to use decoding skills to read. This ability can even occur 
before they develop functional verbal communication. Yet, these children struggle with reading for meaning 
and lack the ability to comprehend text. (Randi, Newman, and Grigorenko 2010) This application is our 
effort on improving this situation. Labeling is used in many typical development preschool and kindergarten 
classrooms to help non-readers connect the orthography and the meaning of the word by placing a label 
on the actual object. Literacy LABELS was designed for the iPad to help children with ASD develop the 
skills necessary to read for meaning by using this emergent literacy skill. Keywords: Autism Spectrum 
Disorders, emergent literacy, iPad, application design . 2 Research and Design We started from reviewing 
literature to understand the cognitive deficiencies associated with ASD and determining instructional 
needs and target zones for instruction. The cognitive deficits revealed an interruption in the ability 
to process information. Autism makes attention and perception difficult to achieve and directly affects 
learning. (Rasche and Qian 2012) We chose to use the iPad for instruction because it helps to focus attention 
and increase time on task to enhance learning. (Putnam and Chong 2008) Our main design goal is to help 
children with ASD to read for meaning. The focus of labeling actual objects in context using an environment 
where many of the objects were readily understood was important. With this in mind, we decided to design 
the application for the home environment. Figure 1(a) demonstrates a persona of the initial targeted 
audience of children with ASD. After reviewing the design mockup with professionals, we realized that 
it could actually be built as an inclusive design that fits the widest possible audience ­children with 
varying types of learning disabilities and typical development preschoolers. Figure 1(b) shows the two 
instructional methods in Literacy LABELS. The first method is the novel approach of using the iPad camera 
as a scanner to allow for a visual and audio response for all the printed labels. By interpreting the 
context of the actual objects, it is easier for the children to build up the understanding of the text 
meaning. The second method is playing the game to reinforce the skill being taught with the direct instruction 
of the physical labels. We propose that the combination of using these two learning methods will help 
to strengthen the association between the orthography and the object. Usability considerations for the 
interface had to meet the nature of our user group. For example, we placed teacher controls in a secure 
locked location to prevent the child from selecting these buttons. Another adjustment was to lock the 
screen while the prompt is being read to discourage the child from randomly making selections before 
hearing the instructions. As displayed in Figure 1(c), we created a mockup of the system and conducted 
a series of formative evaluations with facilitators at Purdue University s Speech Clinic. Based on the 
results, we made changes and refined the application s structural design and graphics as illustrated 
in Figure 1(d). Our prototype of the QR code decoder and one room of the application are ready for a 
summative evaluation with the user groups. As part of the iterative process, we will revise the system 
after the evaluation and then release it for final coding. References Putnam, C., and L. Chong. 2008. 
Software and Technologies Designed for People with Autism: What Do Users Want? In Proceedings of the 
10th International ACM SIGACCESS Conference on Computers and Accessibility, 3 10. Randi, J., T. Newman, 
and E. L Grigorenko. 2010. Teaching Children with Autism to Read for Meaning: Challenges and Possibilities. 
Journal of Autism and Developmental Disorders 40 (7): 890 902. Rasche, N., and C.Z. Qian. 2012. Work 
in Progress: Application Design on Touch Screen Mobile Computers (TSMC) to Improve Autism Instruction. 
In Frontiers in Education Conference (FIE), 2012. *e-mail: {nrasche, wei93, jpourcho, qianz and victorchen}@purdue.edu 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503412</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Morphological computation on two dimensional self-assembly system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503412</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503412</url>
		<abstract>
			<par><![CDATA[<p>Self-assembly is a process in which components autonomously organized into structure without external direction. There are many researches of self-assembly system at molecular scale to macro scale. Such a self-assembly system may serve as the new production method with the characteristics, such as decomposability, self-repairing, self-replicating and adaptability. For developing such a new production method, an understanding of the interaction of shape and pattern, which can be called as morphological computation, is important.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190651</person_id>
				<author_profile_id><![CDATA[82459323257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masumori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University in Kanagawa Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ats.msmr@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190652</person_id>
				<author_profile_id><![CDATA[82458988257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University in Kanagawa Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[htanaka@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Miyashita, S., Nagy, Z., Nelson, B. J., and Pfeifer, R. 2009. The Influence of Shape on Parallel Self-Assembly. <i>Entropy 11</i>, 4, 643--666.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nakajima, K., Ngouabeu, A. M. T., Miyashita, S., G&#246;ldi, M., F&#252;chslin, R. M., and Pfeifer, R. 2012. Morphology-induced collective behaviors: dynamic pattern formation in water-floating elements. <i>PloS one 7</i>, 6, e37805.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Morphological Computation onTwo Dimensional Self-Assembly System Atsushi Masumori Keio University in 
Kanagawa Japan ats.msmr@gmail.com 1. Introduction Self-assembly is a process in which components autonomously 
organized into structure without external direction. There are ma­ny researches of self-assembly system 
at molecular scale to macroscale. Such a self-assembly system may serve as the new produc­tion method 
with the characteristics, such as decomposability,self-repairing, self-replicating and adaptability. 
For developingsuch a new production method, an understanding of the interac­tion of shape and pattern, 
which can be called as morphologicalcomputation, is important. There is a few research about morphological 
computation on self-assembly system without a target structure [Miyashita et al.2009] [Nakajima et al. 
2012], however, there is few researchabout that with target structure. Therefore, this research focuseson 
the morphological computation on self-assembly system withtarget structure. In this research, we first 
designed and implemented a two­dimensional self-assembly system and second we conducted some experiments 
using the system for understanding of morphologicalcomputation on self-assembly system. 2. Design and 
Development We designed and developed a two dimensional self-assemblysystem where components which have 
a concave-convex patternand one magnet on their each side (figure 1a) are put into a con­tainer and by 
shaking the container, the components move aroundand are bonded if they have a complementary pattern 
on their sideand finally the target structure is assembled (figure 1b). We developed the optimization 
tool, which can optimize a concave-convex pattern using genetic algorithm in order to elimi­nate a bonding 
error and reduce a metastable-state where compo­nents that have partially complementary pattern, are 
bonded with relatively weaker magnetic force. As the result of testing the tool, it has been demonstrated 
that an assembling-time required to as­semble a target structure in case of using optimized patterns 
is a (a) (b)    Figure 1. (a) Components. (b) Assembling a target structure. few times shorter than 
that of not optimized. We also developed a physics simulation tool for simulating abehavior of the self-assembly 
system on a computer as an experi­ment environment in computer simulation (figure 2a) and theshaker machine 
which can shake a container by arbitrary kineticpatterns as an experiment environment in real world (figure 
2b). Hiroya Tanaka Keio University in Kanagawa Japan htanaka@sfc.keio.ac.jp  (a)         (b) 
  Figure 2. (a) Physics simulation tool. (b) Shaker machine.  3. Experiment We first conducted some 
experiment in computer simulationand we observed a behavior of the self-assembly system wherethe set 
of three kinds shapes of components and three kinds ofshapes, sizes and kinetic patterns of container 
were applied. Asthe result, it has been shown that there is a relatively strong corre­lation between 
assembling-time and interaction-frequency be­tween each components and angular speed of component. It 
hasalso been shown that there is a tendency that as the shape or pat­tern more resembles to a circle, 
assembling-time becomes shorter(figure 3a). In addition, by comparing motion trajectories, speedand angular 
speed of components of each set of shapes and pat­terns, it has been shown that there are different tendencies 
of thepattern for each set of shapes and patterns (figure 3b). We also conducted an experiment in real 
world in same condi­tion to that of computer simulation. As the results, it has beenshown the same tendency 
to that of in physics simulation. (a) (b)   Figure 3: (a) Comparison of interaction level. (b) Comparison 
of motion trajectory. References MIYASHITA, S., NAGY, Z., NELSON, B.J., AND PFEIFER, R. 2009. TheInfluence 
of Shape on Parallel Self-Assembly. Entropy 11,4, 643 666. NAKAJIMA, K., NGOUABEU, A.M.T., MIYASHITA, 
S., GÖLDI, M., FÜCHSLIN, R.M., AND PFEIFER, R. 2012. Morphology­induced collective behaviors: dynamic 
pattern formation inwater-floating elements. PloS one 7, 6, e37805. Permission to make digital or hard 
copies of part or all of this work for personal or classroom use isgranted without fee provided that 
copies are not made or distributed for commercial advantage and that copies bear this notice and the 
full citation on the first page. Copyrights for third-party components of thiswork must be honored. For 
all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503413</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[The garden of virtual delights]]></title>
		<subtitle><![CDATA[virtual fauna for a botanical garden]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503413</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503413</url>
		<abstract>
			<par><![CDATA[<p>We describe <i>The garden of virtual delights</i>, an interactive installation developed for the botanical garden of Coimbra, with the goal of attracting visitors and promoting the visibility of the garden. The installation builds and simulates an artificial ecosystem, where visitors become part of this ecosystem, interacting effortlessly and seamlessly with the artificial organisms. The name of the installation is a reference to Hieronymus Bosch's masterwork "The Garden of Earthly Delights", where a swarm of birds can be seen in the upper left corner.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190653</person_id>
				<author_profile_id><![CDATA[81371593407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tiago]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Martins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tiagofm@student.dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4190654</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4190655</person_id>
				<author_profile_id><![CDATA[81504683049]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Artur]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rebelo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[arturr@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C., 1987. Flocks, herds and schools: A distributed behavioral model. In <i>Computer graphics and interactive techniques (SIGGRAPH '87 Proceedings of the 14th annual conference)</i>, ACM, 25--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The garden of virtual delights: Virtual fauna for a botanical garden Tiago Martins1, Penousal Machado2 
and Artur Rebelo3 CISUC, Department of Informatics Engineering, University of Coimbra  Figure 1 Working 
prototype of the installation. The cardboard blocks represent people. 1 Introduction We describe The 
garden of virtual delights, an interactive installation developed for the botanical garden of Coimbra, 
with the goal of attracting visitors and promoting the visibility of the garden. The installation builds 
and simulates an artificial ecosystem, where visitors become part of this ecosystem, interacting effortlessly 
and seamlessly with the artificial organisms. The name of the installa­tion is a reference to Hieronymus 
Bosch s masterwork The Gar­den of Earthly Delights , where a swarm of birds can be seen in the upper 
left corner. The ecosystem is composed of several species, which constitute a food chain. Each species 
is characterized by its physiological and behavioral features appearance, dimension, energy, life span, 
speed, stamina, reproduction and predatory behavior. The organ­isms are influenced by the presence and 
movement of the visitors, which become part of the ecosystem and top of the food chain. 2 Implementation 
and behavior The ecosystem is implemented through a particle system where each species is represented 
by a swarm [Reynolds, 1987] and where each particle is an organism. The behavior of each organism is 
determined by its local view of the environment. Like in the work of Reynolds [1987] intra-species behavior 
follows the rules of: separation, steering away from close organisms; cohesion, steer towards the center 
of the organisms in their vicinity; alignment, steer towards the average heading of the agents in the 
vicinity (fig­ure 2). Inter-species behavior is attained as follows: predators steer towards preys in 
their vicinity, i.e. the cohesion and alignment rules are applied; preys, steer away from predators, 
i.e. separation rules are applied. By adjusting the weights and vicinity s radius of intra and inter 
species rules we are able to convincingly simulate the desired predator-prey behavior. The physiological 
and behavioral characteristics of each organism depend on its species but also change through time. The 
energy level determines the dimension of the organism, the probability of reproduction, and its stamina, 
which influences its ability to hunt prey and flee from predators. The age of an organism determines 
its maximum velocity, probability of reproduction and of death by aging. When an organism dies its body 
remains in the ecosystem for a given period of time, providing food for scavengers. If it died at the 
hands of a predator, the predator gains half of its energy, and the other half becomes available for 
scavengers. If it died due to age, its entire energy becomes available. Figure 2 Swarming behavior of 
the organisms. The detection of people is performed in real time using Microsoft Kinect, employing a 
tracking mechanism that allows the analysis of the movement of each volume individually. Speed determines 
the area of repulsion of each volume (see figure 3). As such, sudden movements are perceived as threats, 
making the organisms flee away. Gentle and subtle movements promote trust and curiosity, triggering the 
interest of the organisms by such visitors, and mak­ing them steer towards them. This encourages a contemplative 
attitude that fits the nature of the space. Figure 3 Snapshot of the application that controls the installation. 
Two circles are drawn for each detected volume. The inner circle represents the area of the volume, the 
outer circle represents the repulsion area. Acknowledgments This research is partially funded by FEDER 
through POFC COM-PETE, project VisualyzARt with reference QREN 23201. References REYNOLDS, C., 1987. 
Flocks, herds and schools: A distributed behav­ioral model. In Computer graphics and interactive techniques 
(SIGGRAPH '87 Proceedings of the 14th annual conference), ACM, 25-34. 1 tiagofm@student.dei.uc.pt 2 machado@dei.uc.pt 
3 arturr@dei.uc.pt Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>FEDER through POFC - COMPETE, project VisualyzARt</funding_agency>
			<grant_numbers>
				<grant_number>QREN 23201</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
	<section>
		<section_id>2503414</section_id>
		<sort_key>290</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Computer vision]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>2503415</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A novel representation for digital scenes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503415</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503415</url>
		<abstract>
			<par><![CDATA[<p><i>SCENE</i> is an ongoing research project dedicated to create and deliver richer media experiences [Hilton and Fuenmayor 2013]. A con-sortium of international research and industry partners aims to enhance the whole chain of multidimensional media production: new capturing devices, advanced processing tools and a dedicated renderer. Central is a novel scene representation which enhances and facilitates production processes of video content.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190656</person_id>
				<author_profile_id><![CDATA[81100067319]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[T.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herfet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Visual Computing Institute, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190657</person_id>
				<author_profile_id><![CDATA[82459052657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[C.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haccius]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Visual Computing Institute, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[haccius@intel-vci.uni-saarland.de]]></email_address>
			</au>
			<au>
				<person_id>P4190658</person_id>
				<author_profile_id><![CDATA[82458909957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[V.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matvienko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel Visual Computing Institute, Saarbr&#252;cken, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4285366</person_id>
				<author_profile_id><![CDATA[81319491699]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[S.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fort]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Barcelona Media, Barcelona, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hilton, A., V. And Fuenmayor, E., 2013, Novel scene representations for richer networked media, http://3d-scene.eu, retrieved Feb. 2013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Novel Representation for Digital Scenes T. Herfet, C. Haccius*, V. Matvienko, Intel Visual Computing 
Institute, Saarbrücken, Germany S. Fort, Barcelona Media, Barcelona, Spain Figure 1: Frame showing the 
different scene elements 1 Introduction SCENE is an ongoing research project dedicated to create and 
deliver richer media experiences [Hilton and Fuenmayor 2013]. A con-sortium of international research 
and industry partners aims to enhance the whole chain of multidimensional media production: new capturing 
devices, advanced processing tools and a dedicated renderer. Central is a novel scene representation 
which enhances and facilitates production processes of video content. Current video production processes 
are still based on video acquisition methods that were shaped by physical bottlenecks of image capturing 
devices. The necessary exposure time of film or image sensor dictated shutter time or lens aperture. 
As a consequence loss of image quality due to blurs was in inevitable. However, image sensors and acquisition 
hardware have advancing and removed this physical bottleneck. As blurs have defined motion pictures they 
remain to be stylistic devices. Current video representations fail to make use of both: the advanced 
abilities of image capturing devices and the creative input of the cameraman. We envision a scene representation 
that enables computational videography on captured video data and maintains the camera-man s creativity. 
Additionally, our representation meets several desires from movie processing and production. In detail 
these are: 1) A single format for all information necessary for video processing, such as camera calibration 
data, lighting information, spatial knowledge or coherency information. 2) Storage of captured and generated 
data in its best quality, so scaling or sampling of data is not required. Quality decreasing effects 
do not actually affect the captured data. 3) While traditional video content is frame based, we envision 
an object based representation. This facilitates object modification enables content interaction. 4) 
Captured video and computer generated data are currently living in very separate worlds. An early integration 
of both facilitates postproduction and makes computer generated content more realistic.  Figure 2: Replacing 
segmented object and blur effect  2 Our Approach We have designed a layer-based scene representation 
architecture based on the idea of recreating virtual scenes for virtual movie production. We call the 
layers base-, scene- and director s layer. The base layer corresponds to a collection of stage settings, 
stage props, actors and other objects. These elements are the atomic elements a scene is composited of; 
therefore named acels (atomic scene elements). Those acels are coherent in themselves, but independent 
of other scene elements. They can have any number of dimensions and are not limited in the information 
content. The scene layer creates relations among the acels, thus placing the independent acels in a global 
scene context. The scene dimensions are a superset of all the acel dimensions occurring in a scene, and 
each acel is registered with respect to each dimension in the global context. Acel coherencies stored 
in this layer additionally assign relations to dimensions of different acels in a scene. The third layer 
is the director s layer which contains the director s decisions. Most basic are camera parameters contained 
in calibration matrices. Furthermore, effects (like blurs, color offsets, etc.) are stored in this layer, 
modifying the underlying data representation but not the data itself. Last, interaction rules may allow 
user interaction with the scene elements. A prove-of-concept implementation shows the effect of the architecture 
presented above. Figure 1 shows a billiard scene captured with color plus depth sensor. A computer generated 
mesh was relighted according to the scene lighting conditions and inserted above the pool table to demonstrate 
the merge of captured video and computer generated data. A small effect shows the use of the director 
s layer: The white ball is replaced by a black ball and a motion blur is added on top of the replaced 
object, as shown in Figure 2. This small implementation already shows that existing production steps 
are facilitated and new possibilities are created by the use of computational videography on this data. 
References HILTON, A., V. AND FUENMAYOR, E., 2013, Novel scene representations for richer networked media, 
http://3d-scene.eu, retrieved Feb. 2013.  *e-mail: haccius@intel-vci.uni-saarland.de 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503416</article_id>
		<sort_key>310</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Consistent depth maps recovery of video via object segmentation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503416</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503416</url>
		<abstract>
			<par><![CDATA[<p>Video depth reconstruction is a long-term problem in computer vision. A fair amount of previous work adopts Markov random field model in depth estimation formulation. Various local prior terms are included into global smoothness of those optimization frameworks. However, the estimated depth discontinuities are usually not consistent with object boundaries and intra-object smoothing is not well achieved. Such problems are addressed in color segment-based methods but cannot be solved well when color segments exist across object boundaries or are not temporarily consistent.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190659</person_id>
				<author_profile_id><![CDATA[81550884556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chia]]></first_name>
				<middle_name><![CDATA[Ju]]></middle_name>
				<last_name><![CDATA[Ho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190660</person_id>
				<author_profile_id><![CDATA[81550903556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hong]]></first_name>
				<middle_name><![CDATA[Shiang]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190661</person_id>
				<author_profile_id><![CDATA[81100319756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ouhyoung]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2192087</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bleyer, M., Rother, C., Kohli, P., Scharstein, D., and Sinha, S. 2011. Object stereo:joint stereo matching and object segmentation. 3081--3088.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073234</ref_obj_id>
				<ref_obj_pid>1073204</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Li, Y., Sun, J., and Shum, H.-Y. 2005. Video object cut and paste. <i>ACM Transactions on Graphics (TOG) 24</i>, 3, 595--600.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zhang, G., Jia, J., Wong, T.-T., and Bao, H. 2008. Recovering consistent video depth maps via bundle optimization. 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Consistent Depth Maps Recovery of Video via Object Segmentation Chia Ju Ho Hong Shiang Lin Ming Ouhyoung 
National Taiwan University  Figure 1: (a) One frame of the input video. (b) The result of [Zhang et 
al. 2008]. (c) Our result. 1 Introduction Video depth reconstruction is a long-term problem in computer 
vi­sion. A fair amount of previous work adopts Markov random .eld model in depth estimation formulation. 
Various local prior terms are included into global smoothness of those optimization frame­works. However, 
the estimated depth discontinuities are usually not consistent with object boundaries and intra-object 
smoothing is not well achieved. Such problems are addressed in color segment­based methods but cannot 
be solved well when color segments exist across object boundaries or are not temporarily consistent. 
Motivated by object stereo [Bleyer et al. 2011], we propose an ef­ fective object-aware video depth estimation 
framework. Consistent object labels and depth values in time sequence gives better oc­clusion reasoning, 
thus reducing errors than color segment-based methods. We compare our result with [Zhang et al. 2008] 
and it shows better depth reconstruction on object boundaries (see Fig­ure 1). 2 Our system Our framework 
consists of two stages. In .rst stage, our system se­lects key frames of the input video, and then jointly 
estimates depths and object labels for each key frame. In second stage, those depth maps and object maps 
are warped to intermediate frames. And then the depth maps of intermediate frames are re.ned according 
to the optimization framework in [Zhang et al. 2008]. object map of the reference view to the other 
view, the object la­beling of the other view heavily relies on the initial disparity map of the reference 
view. Our system projects the disparity planes of key frames to 3D space and merges transformed planes 
to unify ob­ject labels. The object labeling on each key frame still equals the disparity plane assignment 
on itself in this updating strategy. There­fore, the occlusion reasoning is more stable since it is supported 
by disparity initialization of multiple images. Intermediate-frame Depths The object-aware disparity 
maps of key frames provide a good guidance of depth estimation on inter­mediate frames. Depth initialization 
of each frame is achieved by fusing warped disparity map of all key frames. The bi-directional depth 
fusion improves the depth estimation and reduces holes in occluded regions than that of mono-directional 
propagation. We adopt the energy minimization framework in [Zhang et al. 2008] to generate complete disparity 
maps and enhance their temporal con­sistency. 3 Result and Conclusion Figure 1 shows one of our experimental 
cases. Note that the result of [Zhang et al. 2008] is directly from author s presentation. We im­ plement 
their system for disparity map re.nement on intermediate frames, and our result is presented in Figure 
1 (c). Since the object segmentation better separates foreground and background than pure color segmentation 
around object boundaries ( see Figure 2 ), the re­ sult shows that our method has better performance 
on the indicated regions. In conclusion, we have developed an object-aware video depth estimation system 
and the future work is to incorporate video object cut techniques [Li et al. 2005] to effectively computing 
layer assignment for each frame in a video for .lm post-production. References BLEYER , M., ROT H E 
R, C., KOH LI , P., SC H AR S T EI N , D., A N D SINH A , S. 2011. Object stereo:joint stereo matching 
and object segmentation. 3081 3088. LI, Y., SU N , J., A N D SH UM, H.-Y. 2005. Video object cut and 
paste. ACM Transactions on Graphics (TOG) 24, 3, 595 600. ZH A NG, G., JIA, J., WONG, T.-T., A N D BAO 
, H. 2008. Recover­ing consistent video depth maps via bundle optimization. 1 8. Permission to make digital 
or hard copies of part or all of this work for personal or classroom use isgranted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503417</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Customizing time of flight modulation codes to resolve mixed pixels]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503417</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503417</url>
		<abstract>
			<par><![CDATA[<p>We couple custom binary sequences with sparsity based approaches to address the multi-path problem in time of flight imaging. In particular, we utilize maximum length m-sequences that allow us to produce non band-limited correlation functions. Coupled with a tailored sparse deconvolution approach, we are able to resolve the constituent phases and amplitudes of a mixed pixel measurement. Finally, we demonstrate application scenarios, including depth imaging of near-transparent objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190662</person_id>
				<author_profile_id><![CDATA[82459043457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Achuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kadambi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[achoo@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190663</person_id>
				<author_profile_id><![CDATA[82458847657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Refael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whyte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waikato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190664</person_id>
				<author_profile_id><![CDATA[81464665244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ayush]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhandari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190665</person_id>
				<author_profile_id><![CDATA[82459127257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Streeter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waikato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190666</person_id>
				<author_profile_id><![CDATA[81504688659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barsi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190667</person_id>
				<author_profile_id><![CDATA[81309482324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorrington]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waikato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190668</person_id>
				<author_profile_id><![CDATA[81548005482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1905539</ref_obj_id>
				<ref_obj_pid>1904935</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fuchs, S. 2010. Multipath interference compensation in time-of-flight camera images. In <i>Pattern Recognition (ICPR), 2010 20th International Conference on</i>, IEEE, 3583--3586.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Godbaz, J. P., Cree, M. J., and Dorrington, A. A. 2012. Closed-form inverses for the mixed pixel/multipath interference problem in amcw lidar. In <i>IS&T/SPIE Electronic Imaging</i>, International Society for Optics and Photonics, 829618--829618.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Customizing Time of Flight Modulation Codes to Resolve Mixed Pixels Achuta Kadambi* Refael Whyte Ayush 
Bhandari Lee Streeter Christopher Barsi MIT Media Lab University of Waikato MIT Media Lab University 
of Waikato MIT Media Lab Adrian Dorrington Ramesh Raskar University of Waikato MIT Media Lab  (a) Amplitude 
Image of the Scene (b) Phase Image (c) Depth Recovery of Transparency Figure 1: Depth imaging of transparent 
objects. One modulation frequency, ZERO post-image processing. (a) The amplitude image of a scene as 
measured by our customized time of .ight camera. In this scene a near-transparent acrylic unicorn is 
placed at approximately the same depth as an opaque coffee mug. Specularities can be seen on the edges 
of the unicorn. (b) The measured phase image. The body of the unicorn is a mix between the background 
and foreground. The edges are resolved due to the specularities, but the rest of the unicorn is at an 
incorrect depth. (c) By resolving the mixed pixel problem we are able to obtain the correct depth of 
the near-transparent unicorn. Abstract We couple custom binary sequences with sparsity based approaches 
to address the multi-path problem in time of .ight imaging. In par­ticular, we utilize maximum length 
m-sequences that allow us to produce non band-limited correlation functions. Coupled with a tai­lored 
sparse deconvolution approach, we are able to resolve the con­stituent phases and amplitudes of a mixed 
pixel measurement. Fi­nally, we demonstrate application scenarios, including depth imag­ing of near-transparent 
objects. 1 Introduction Current time of .ight cameras cannot resolve the mixed pixel prob­lem. The mixed 
pixel problem occurs when light from two or more different depths combine at a single pixel. Even a perfect, 
planar, opaque object is susceptible to mixing the edge pixels are often a mixture of foreground and 
background. More troublesome, how­ever, are transparent and re.ecting objects. In a transparent object, 
light from the background and foreground mix to yield a depth that is neither the foreground or background 
(see Figure 1b). Most re­ alistic scenes captured with a time of .ight camera contain mixed pixels, such 
as from a corner, which acts as a re.ector. In recent years, many algorithms have been proposed to resolve 
the mixed pixel problem. Success with many current approaches is limited due to scene-dependent assumptions. 
For instance, [Fuchs 2010] proposed an interesting forward model that serves as the ba­ sis for unmixing. 
However, their forward model is scene-dependent and has not been experimentally validated on other scenes. 
In a more general framework, [Godbaz et al. 2012] have proposed using multiple frequencies of modulation 
to unmix pixels, though this ap­proach is ill-conditioned, which limits the practical implications. *e-mail:achoo@mit.edu 
 2 Our Approach In this paper, we provide two key insights: By sending custom codes, coupled with sparse 
deconvolution, we are able to resolve the multi-path problem on real, experi­mental data.  We consider 
each component of multipath to be important. In particular, for the case of a transparent object, we 
can obtain both the depth of the foreground object, as well as the back­ground wall.  In order to send 
custom codes, we have built our own time of .ight camera, with operation details provided in supplementary 
material. In brief, the camera is able to send custom binary sequences to both the reference and illumination. 
We formulate the problem such that the codes form a convolution with the environment. In matrix form 
this can be expressed simply as a linear system: y = Hx. The smearing matrix H is a Toeplitz matrix determined 
by the particu­lar codes that are selected for use with the camera, y is the mea­sured cross-correlation 
at a mixed pixel and x is the recovered en­vironment variable to create the output. By optimizing the 
codes, and accounting for sparsity, we can reconstruct accurate component depths: see Figure 1c for one 
particular case.  References FU CH S , S. 2010. Multipath interference compensation in time-of­.ight 
camera images. In Pattern Recognition (ICPR), 2010 20th International Conference on, IEEE, 3583 3586. 
GO D BAZ, J. P., CRE E , M. J., A ND DO R RI N G TO N, A. A. 2012. Closed-form inverses for the mixed 
pixel/multipath interference problem in amcw lidar. In IS&#38;T/SPIE Electronic Imaging, Inter­national 
Society for Optics and Photonics, 829618 829618. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use isgranted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503418</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Driver drowsiness estimation using facial wrinkle feature]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503418</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503418</url>
		<abstract>
			<par><![CDATA[<p>In recent years, the rate of fatal motor vehicle accidents caused by distracted driving resulting from factors such as sleeping at the wheel has been increasing. Therefore, an alert system that detects driver drowsiness and prevents accidents as a result by warning drivers before they fall asleep is urgently required. Non-contact measuring systems using computer vision techniques have been studied, and in vision approach, it is important to decide what kind of feature we should use for estimating drowsiness.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190669</person_id>
				<author_profile_id><![CDATA[82459296457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Taro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[taronoaddress@akane.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190670</person_id>
				<author_profile_id><![CDATA[82458682457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tatsuhide]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsuda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190671</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190672</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hachisuka, S., Kimura, T., Ozaki, N., Ishida, K., and Nakatani, H. 2010. Drowsiness Detection Using Facial Expression Features. <i>SAE International paper. 2010-01-0466</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Irie, A., Takagiwa, M., Moriyama, K., and Yamashita, T. 2011. Improvements to Facial Contour Detection by Hierarchical Fitting and Regression. <i>The First Asian Conference on Pattern Recognition Oral. pp. 273--277</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Driver Drowsiness Estimation Using Facial Wrinkle Feature   Taro NAKAMURA*   Tatsuhide MATSUDA 
  Akinobu MAEJIMA   Shigeo MORISHIMA Waseda University 1 Introduction In recent years, the rate 
of fatal motor vehicle accidents caused by distracted driving resulting from factors such as sleeping 
at the wheel has been increasing. Therefore, an alert system that detects driver drowsiness and prevents 
accidents as a result by warning drivers before they fall asleep is urgently required. Non-contact measuring 
systems using computer vision techniques have been studied, and in vision approach, it is important to 
decide what kind of feature we should use for estimating drowsiness. In a related study, by analyzing 
a video of captured facial expressions, [Hachisuka et al. 2010] focused on the movement of facial feature 
points on the eyes, eyebrows, and mouth. In this study, the drowsiness degree is evaluated on the basis 
of changes in these points with reference to an awakened state. However, it not clear whether this feature 
is enough or not to estimate drowsiness. To realize more precise drowsiness estimation, we need to select 
other effective feature. We detected a new drowsiness feature by comparing video image and CG model which 
are applied the existing feature point information. Moreover, we estimated drowsiness degree using new 
facial wrinkle feature, which has not been studied to date, and we confirmed that it improves the estimation 
accuracy. 2 Our Approach 2.1 Definition of drowsiness level: We recorded a video (60 fps frame rate 
and 320 x 240 pixels resolution) of 10 drivers faces while they were operating a driving simulator. The 
drowsiness degree was divided into five levels on the basis of the captured video: level 1: not sleepy, 
 level 2: slightly sleepy, level 3: sleepy, level 4: rather sleepy, level 5: very sleepy. Two assessors 
who had studied the psychological measurement technique estimated drowsiness every 5 s. Traffic accidents 
occur when the level is 3 or higher. Therefore, our objective is to accurately estimate the transition 
from level 2 to level 3. 2.2 Existing drowsiness estimation feature: Owing to drowsiness the eyes of 
the subject are partially closed. To capture changes of such facial expressions, features based on feature 
point have been used. We also used the variation of the distance between two feature point positions 
from one awakened state to another as feature parameter. Eighteen feature points were detected by Irie 
s technique [Irie et al. 2011] in each video frame. We calculate 38 sets of distances between two feature 
points related to eyes, eyebrows, mouth, etc. The feature parameters are the lengths of arrows in Fig. 
1. 2.3 k-NN based learning and estimation: Drowsiness estimation, which classifies the current state 
into five drowsiness levels, is performed using the k-NN method with the variation of the distance between 
two feature points. Feature vectors corresponding to each frame of the videos are calculated and averaged 
every 300 frames (5 s) in both learning and testing stages. The videos were divided into levels on the 
basis of the assessed ground truth level, and each video was further divided into alternate halves. One 
half is used for learning data, while the other half is used for testing data. We used the following 
two criteria for the evaluation index. - Correct answer rate: The correct answer rate is defined as the 
percentage of the number of correctly estimated data compared to the ground truth level, which is the 
integer level from 1 to 5. - RMSE: The RMSE is the root mean square error between the estimated score 
and ground truth score. e-mail: {*taronoaddress@akane., shigeo@}waseda.jp     (a)       
  (b)  Figure 1: (a) Distance parameters, (b) Filtering area    (a)      (b)       
(c)   Figure 2: Detecting new feature (a) Awake, (b) Drowsy, (c) CG face model 2.4 Detecting new 
feature: To realize more precise drowsiness estimation, we need to select other effective feature. To 
detect new feature, we made drowsy CG face model which was applied feature point information. We deformed 
the shape of 661 points 3D mesh according to feature points of drowsy face, and mapped an awakened texture 
to this deformed model frame by frame. We compared original images and CG models, and we could clearly 
consider a feature which we should capture. Wrinkles appeared on brows, mouth, and nasolabial fold on 
original image s face when a subject resists drowsiness. Therefore, we introduced edge-intensity features 
which are calculated by applying Laplacian filtering at 5 areas on the face, as shown in Fig. 1: between 
the eyebrows, nasolabial folds, and corners of the mouth. Drowsiness estimation is performed again using 
the k-NN method with the feature vector connecting the variation of the distance between two feature 
points and the variation of the edge intensity. The evaluation results with open tests for the same subject 
are summarized in Table I. Table I. Evaluation Only Distance Only Edge Distance + Edge Correct answer 
rate [%] 63.4 60.9 71.0 RMSE 0.68 0.74 0.58  Moreover, in an actual driving situation, it is necessary 
to generate learning data beforehand from subjects other than the driver. Therefore, we attempt to use 
the leave-one-out approach. Because our purpose is to provide a warning when the drowsiness level reaches 
3, it is necessary to verify if level 3 or higher levels have been correctly distinguished from level 
2 or lower levels. We selected one person for this test and the other nine for learning data. For evaluation, 
the correct answer rate of two class separations was calculated. We realized two classes of recognition 
with an accuracy of 82.2%. References HACHISUKA, S., KIMURA, T., OZAKI, N., ISHIDA, K., AND NAKATANI, 
H. 2010. Drowsiness Detection Using Facial Expression Features. SAE International paper. 2010-01-0466. 
IRIE, A., TAKAGIWA, M., MORIYAMA, K., AND YAMASHITA, T. 2011. Improvements to Facial Contour Detection 
by Hierarchical Fitting and Regression. The First Asian Conference on Pattern Recognition Oral. pp. 273 
277. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503419</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Refocusing images captured from a stereoscopic camera]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503419</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503419</url>
		<abstract>
			<par><![CDATA[<p>Traditional photography projects a 3D scene to a 2D image without recording the depth of each local region, which prevents users from changing the focus plane of a photograph once it has been taken. To tackle this problem, Ng et al. [2005] presented light-field cameras that record all focus planes of a scene and synthesized the refocused image using ray tracing. Nevertheless, the captured photographs are of low resolution because the image sensor is divided into sub-cells. Levin et al. [2007] embedded a coded aperture on the camera lens and recover depth information from blur patterns in a single image. However, the coded aperture blocks around 50% of light. Their system requires longer exposition time when taking pictures. Liang et al. [2008] also embedded a coded aperture on the camera lens to capture the scene but with multiple exposures. It produces high quality depth maps yet is not suitable to hand-held devices. Recently, Microsoft Kinect directly estimates the depth information using infrared light, which works only in a indoor environment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190673</person_id>
				<author_profile_id><![CDATA[82459087557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chia-Lun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ku]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chiao Tung University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190674</person_id>
				<author_profile_id><![CDATA[82258888357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yu-Shuen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chiao Tung University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190675</person_id>
				<author_profile_id><![CDATA[82459315957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Chia-Sheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190676</person_id>
				<author_profile_id><![CDATA[82459223857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hung-Kuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190677</person_id>
				<author_profile_id><![CDATA[82458645557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chih-Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1340245</ref_obj_id>
				<ref_obj_pid>1340087</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hirschmuller, H. 2008. Stereo processing by semiglobal matching and mutual information. <i>IEEE Trans. Pattern Anal. and Mach. Intell. 30</i>, 2, 328--341.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276464</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Levin, A., Fergus, R., Durand, F., and Freeman, W. T. 2007. Image and depth from a conventional camera with a coded aperture. <i>ACM Trans. Graph. 26</i>, 3, 328--341.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360654</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Liang, C.-K., Lin, T.-H., Wong, B.-Y., Liu, C., and Chen, H. 2008. Programmable aperture photography: Multiplexed light field acquisition. <i>ACM Transactions on Graphics 27</i>, 3, 55:1--55:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Bredif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light field photography with a handheld plenoptic camera. <i>Stanford University Computer Science Tech Report CSTR 2005-02</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Refocusing Images Captured from a Stereoscopic Camera Chia-Lun Ku1 Yu-Shuen Wang1 Chia-Sheng Chang2 
Hung-Kuo Chu2 Chih-Yuan Yao3 1National Chiao Tung University 2National Tsing Hua University 3National 
Taiwan University of Science and Technology Figure 1: Left: The left image captured from a stereoscopic 
camera. Middle: The estimated depth map. The dark color indicates the pixel is close to the camera. Right: 
The synthesized image, in which the focus is at the chair. 1 Introduction Traditional photography projects 
a 3D scene to a 2D image without recording the depth of each local region, which prevents users from 
changing the focus plane of a photograph once it has been taken. To tackle this problem, Ng et al. [2005] 
presented light-.eld cameras that record all focus planes of a scene and synthesized the refocused image 
using ray tracing. Nevertheless, the captured photographs are of low resolution because the image sensor 
is divided into sub­cells. Levin et al. [2007] embedded a coded aperture on the camera lens and recover 
depth information from blur patterns in a single image. However, the coded aperture blocks around 50% 
of light. Their system requires longer exposition time when taking pictures. Liang et al. [2008] also 
embedded a coded aperture on the camera lens to capture the scene but with multiple exposures. It produces 
high quality depth maps yet is not suitable to hand-held devices. Recently, Microsoft Kinect directly 
estimates the depth information using infrared light, which works only in a indoor environment. We introduce 
a refocus technique that recovers the depth from a pair of images captured from consumer stereoscopic 
cameras. A dispar­ity map is estimated using semi-global matching (SGM) algorithm [Hirschmuller 2008] 
to represent the parallax caused by perspec­ tive projection. Our key ideas are 1) the sign of disparity 
indicates whether the pixel is in front of or in rear of the focus plane; and 2) the magnitude of disparity 
shows the distance to the focus plane. As a result, the depth value of each local region can be obtained 
easily. Another key observation is that consumer cameras usually carry small lenses to enhance mobility, 
where their depths of .eld are deep and the captured photographs are nearly all focused. There­fore, 
we can simply blur image pixels whenever the focus plane is changed and avoid the challenging deblurring 
problem. Compared to previous works, our approach does not modify existing cameras, demands short exposition 
time, and produces high resolution refo­cus images. 2 Our Approach The inputs to our system are a pair 
of stereo images captured from a pre-calibrated stereoscopic camera. The two images are .rst rec­ti.ed 
and the disparity map is then computed using SGM algorithm [Hirschmuller 2008]. As there are some mismatched 
or occluded pixels within the images, which lead to holes in the disparity map, we .ll the holes according 
to the neighboring disparity values and the gradients of pixel colors. As different objects are usually 
seg­mented by sharp boundaries, the regions with similar pixel colors can be assumed to have similar 
depth values while the others are not. We formulate this idea into the objective function arg minwij 
|pi - pj |2 +|pi - ci|2 , (1) P{i,j}.E i.C and compute for the complete disparity map using a linear 
solver, where P = {p0, p1, ..., pn} is the disparity map, E denotes the 4­adjacent neighbors, C is the 
set of pixels that have disparity values, and wij is the Guassian of color difference. Once the complete 
disparity map is obtained, the refocused image can be synthesized whenever the focus plane is given. 
As only blur­ring is demanded in our system, this process could be accomplished by applying spatial .lters 
with different sizes to pixels. The farther distance to the focus plane, the larger .ler size we use. 
We also blend neighboring pixels that have similar depth values to prevent artifacts induced by fusing 
foregrounds and backgrounds together. 3 Results and Conclusions We have presented a system to refocus 
stereo images. This frame­work was built on consumer stereo cameras and requires no mod­i.cation to hardwares. 
It also enjoys the advantages such as short exposition time and high resolution results that are absent 
in pre­vious works. The experimental results shown in Figure 1 and our supplemental materials demonstrate 
the feasibility of our technique. References HI R S C H MU L LE R , H. 2008. Stereo processing by semiglobal 
matching and mutual information. IEEE Trans. Pattern Anal. and Mach. Intell. 30, 2, 328 341. LE V I N 
, A., FE R G U S , R., DU R A ND , F., A N D FR EE M A N , W. T. 2007. Image and depth from a conventional 
camera with a coded aperture. ACM Trans. Graph. 26, 3, 328 341. LI A N G , C.-K., LI N , T.-H., WO N 
G , B.-Y., LI U , C., AN D CH E N , H. 2008. Pro­ grammable aperture photography: Multiplexed light .eld 
acquisition. ACM Trans­ actions on Graphics 27, 3, 55:1 55:10. NG, R., LE VOY, M., BR ED I F, M., DU 
VA L, G., HO ROW I T Z , M., A N D HA NR A H A N , P. 2005. Light .eld photography with a handheld plenoptic 
camera. Stanford University Computer Science Tech Report CSTR 2005-02. Permission to make digital or 
hard copies of part or all of this work for personal or classroom use isgranted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503420</section_id>
		<sort_key>350</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Haptics]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>2503421</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A real-time sensing and rendering of haptic perception based on bilateral control]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503421</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503421</url>
		<abstract>
			<par><![CDATA[<p>In recent years, various haptic devices and sensors have been developed. However, development of the haptic sensor is expensive, and there is a difference in performance between these sensors and haptic rendering devices (For example, measuring method, resolution, maximum measuring force of haptic sensor and maximum rendering force of haptic rendering device, and minimum moving distance, etc.). Therefore, it is difficult to render the difference of haptic impression by the obtained data with haptic sensor. Moreover, traditional researches on the haptic sensing can not measure the various haptic informations such as the shape, hardness, and friction at same time[Okamoto et al. 2012]. Therefore, we develop a real-time sensing method of haptic perception parameter such as the shape, hardness, and friction force, based on bilateral control while feeling the haptic impression.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[bilateral control]]></kw>
			<kw><![CDATA[haptic perception]]></kw>
			<kw><![CDATA[virtual reality]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190678</person_id>
				<author_profile_id><![CDATA[81464673753]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wakita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wakita@cv.ci.ritsumei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190679</person_id>
				<author_profile_id><![CDATA[82458989157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiromi]]></first_name>
				<middle_name><![CDATA[T.]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ritsumeikan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2346885</ref_obj_id>
				<ref_obj_pid>2346881</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Okamoto, S., Konyo, M., and Tadokoro, S. 2012. Discriminability-based evaluation of transmission capability of tactile transmission systems. <i>Virtual Reality 16</i>, 2, 141--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Real-time Sensing and Rendering of Haptic Perception based on Bilateral Control Wataru Wakita* Ritsumeikan 
University Keywords: Virtual Reality, Haptic Perception, Bilateral Control 1 Introduction In recent years, 
various haptic devices and sensors have been de­veloped. However, development of the haptic sensor is 
expensive, and there is a difference in performance between these sensors and haptic rendering devices 
(For example, measuring method, reso­lution, maximum measuring force of haptic sensor and maximum rendering 
force of haptic rendering device, and minimum moving distance, etc.). Therefore, it is dif.cult to render 
the difference of haptic impression by the obtained data with haptic sensor. More­ 2 = where, Figure 
1: System Overview *e-mail: wakita@cv.ci.ritsumei.ac.jp Hiromi T. Tanaka Ritsumeikan University If collision 
between slave s grip and a real object is detected, the distance between a position of master s grip 
and a position of slave s grip is increased. At this time, we estimate s.j is a .rst col­lision point 
s.0. Then we calculate a surface normal of a real object from these collision points. If collision is 
detected, we calculate normal component N.j and tangent component T.j from m.j and s.j . Where, N.j and 
T.j are the unit vector. We estimate hardness E as follow. |(m.j - s.j )N.j | E = Ch (2) |(s.j - s.0)N.j 
| where, Ch is a constant parameter, |(m.j -s.j )N.j | is a stress, |(s.j - s.0)N.j | is a strain. Then 
we estimate friction force F as follow. |m.j - s.0||(m.j - s.j )N.j | F = Cf (3) |(m.j - s.j )T.j | where, 
Cf is a constant parameter, and s.0 is updated when position of slave s grip was moved while collision 
is detected. 3 Sensing Result As an example, Figure 2 shows sensing results of a hardness of a rubber, 
silicon gel, and sponge. We obtained clear results of a difference of the stiffness. Figure 2: Sensing 
Results of Hardness 4 Conclusion and Future Work We developed a real-time sensing method of haptic perception 
pa­rameter such as the shape, hardness, and friction force, based on bi­lateral control while feeling 
the haptic impression. In future work, we plan to develop a real-time sensing and rendering system with 
multi .nger haptic device.  References OK A M OTO , S., KO N YO , M., A ND TA D O KORO, S. 2012. Discriminability-based 
evaluation of transmission capability of tactile transmission systems. Virtual Reality 16, 2, 141 150. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503422</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[enchanted scissors]]></title>
		<subtitle><![CDATA[a scissor interface for support in cutting and interactive fabrication]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503422</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503422</url>
		<abstract>
			<par><![CDATA[<p>We present an approach to support basic and complex cutting processes through an interactive fabrication experience [Willis et al. 2011]. Our system, enchanted scissors, is a digitally controlled pair of scissors (Figure 1). It restricts areas that can be cut while requiring the user's exertion of force and decision to execute each cut. Therefore, unlike a completely digitalized cutting device, the user can freely apply improvisations within the permitted areas in real-time. A pair of scissors is a common tool seen and used in everyday life; the user can instantly recognize its operation method. It has varieties of usage from opening a letter to creating a complicated paper craft. While using scissors, it is common to cut unintended parts or difficult to control the blades for cutting intricate details. enchanted scissors prevents these errors in advance by using two switchable programs to restrict the areas that can be cut. Both programs provide real-time feedback to the user during the cutting process as regular scissors would. This allows a comfortable connection of the user's physical input and the output implemented by the device.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190680</person_id>
				<author_profile_id><![CDATA[82458808157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mayu]]></first_name>
				<middle_name><![CDATA[M.]]></middle_name>
				<last_name><![CDATA[Yamashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[t10554my@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190681</person_id>
				<author_profile_id><![CDATA[81442615273]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Junichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamaoka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio Universtiy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamajun@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190682</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio Universtiy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1935716</ref_obj_id>
				<ref_obj_pid>1935701</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Willis, K. D., Xu, C., Wu, K.-J., Levin, G., and Gross, M. D. 2011. Interactive fabrication: New interfaces for digital fabrication. In <i>Proceedings of TEI 11</i>, ACM, 69--72.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 enchanted scissors: A Scissor Interface for Support in Cutting and Interactive Fabrication  Figure 
1: enchanted scissors Figure 2: System Design Figure 3: Cutouts from the same template 1 Introduction 
We present an approach to support basic and complex cutting pro­cesses through an interactive fabrication 
experience [Willis et al. 2011]. Our system, enchanted scissors, is a digitally controlled pair of scissors 
(Figure 1). It restricts areas that can be cut while re­quiring the user s exertion of force and decision 
to execute each cut. Therefore, unlike a completely digitalized cutting device, the user can freely apply 
improvisations within the permitted areas in real-time. A pair of scissors is a common tool seen and 
used in ev­eryday life; the user can instantly recognize its operation method. It has varieties of usage 
from opening a letter to creating a com­plicated paper craft. While using scissors, it is common to cut 
un­intended parts or dif.cult to control the blades for cutting intricate details. enchanted scissors 
prevents these errors in advance by using two switchable programs to restrict the areas that can be cut. 
Both programs provide real-time feedback to the user during the cutting process as regular scissors would. 
This allows a comfortable con­nection of the user s physical input and the output implemented by the 
device. 2 enchanted scissors enchanted scissors was created using mostly the same parts as those of 
a regular pair of scissors. We focused on the conductivity of the scissors metal blades and use conductive 
ink to mark the areas the user can cut or avoid to cut. Since the device reacts only when the blades 
come in contact with the line drawn in conductive ink, the user is able to predominantly control the 
line s design and the execution of each cut. A capacitive sensor can be created when a conductive line 
or a shape is attached to the paperclip connected to a micro-controller (ATmega328P). This allows low 
electric current to .ow into the line, thus forming a circuit. enchanted scissors has a wire connect­ing 
one of the blades with the inside of its handle where conductive tape is attached. This way, when the 
blades touch the circuit, elec­tric current extends from the surface of the paper to the interior of 
the handle where the user would hold. In other words, the user can indirectly touch the conductive line 
through the device (Figure 2). When the blades touch the circuit, the micro-controller reads a *e-mail: 
{t10554my, yamajun, ykakehi}@sfc.keio.ac.jp certain degree of capacitance and the servo motor contained 
in the device reacts according to that value. The device operates on two modes. The .rst program forbids 
the drawn line to be cut, thus the user can cut adjacent and/or nonadja­cent to the line. When the user 
attempts to place the blades on the line, the angular position of the servo motor s arm changes, forcing 
the two handles apart from each other. The second program allows the user to cut only on the line drawn 
in conductive ink. The servo motor s arm recedes when the blades touch the line, enabling the user to 
cut freely with the device. Our objective is to have different types of users incorporate en­chanted 
scissors in multiple situations to improve their cutting per­formances. Children can practice coordinating 
scissors as the de­vice dynamically provides tactile restrictions if the blades go off track. By accomplishing 
to cut accurately with the device, they are able to learn the skills of using scissors and possibly continue 
to improve. Changing the thickness of the conductive line allows the user more or less space to experiment 
with improvisation. The thinner the line, the more likely the chance of fabricating detailed crafts. 
On the other hand, with thicker lines, the restriction would be lower and the user can enhance creativity. 
As seen in Figure 3, the three cutouts originate from identical template (3cm-thick cir­cles drawn in 
black conductive ink), despite resulting in the shape of different animals. It was based on the user 
s decision to add orig­inal arrangements within the painted area. With further practice, enchanted scissors 
can be used with eyes closed. Even if the user cuts carelessly without visual information, the device 
will protect the designated areas. This prevents undesirable accidents to happen such as cutting the 
contents inside envelopes. Moreover, enchanted scissors can be a supportive cutting tool for the visually 
impaired. In the future, we intend to modify the device to output gradual hap­tic feedbacks depending 
on the distance between the blades and the conductive line. Acknowledgement. This research is supported 
by CREST, JST. References WIL L IS, K. D., XU, C., WU, K.-J., LE V IN , G., AN D GRO S S, M. D. 2011. 
Interactive fabrication: New interfaces for digital fabrication. In Proceedings of TEI 11, ACM, 69 72. 
 Permission to make digital or hard copies of part or all of this work for personal or classroom use 
isgranted without fee provided that copies are not made or distributed for commercial advantage and that 
copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>CREST, JST</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503423</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Haptic-emoticon]]></title>
		<subtitle><![CDATA[a framework for creating and sharing haptic contents]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503423</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503423</url>
		<abstract>
			<par><![CDATA[<p>Haptics is a complex sensation with pressure, temperature, vibration, and so on. Therefore developing an interface for creating or designing haptic contents is an attractive challenge. One solution is reduction and simplification of the haptic information. For example, Minamizawa et al. have simplified it as a temporal signal (sound) in <i>TECHTILE toolkit</i> [1]. We have inspired from handwriting motion on a skin. Our solution is a reduction to spatiotemporal information and we employ a spatiotemporal stroking trajectory on a 2D surface for a haptic content.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190683</person_id>
				<author_profile_id><![CDATA[81421601844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kumamoto University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190684</person_id>
				<author_profile_id><![CDATA[81365594770]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nagoya Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190685</person_id>
				<author_profile_id><![CDATA[82458836057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ippei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Torigoe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kumamoto University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2343478</ref_obj_id>
				<ref_obj_pid>2343456</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, K., Kakehi, Y., Nakatani, M., Mihara, S., and Tachi, S., "TECHTILE toolkit: a prototyping tool for designing haptic media," in <i>SIGGRAPH 2012 Emerging Technologies</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Israr, A., Poupyrev, I., "Control space of apparent haptic motion," in Proc. of <i>IEEE World Haptics Conference (WHC) 2011</i>, pp.457--462, 2011.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 https://mail.google.com/mail/e/softbank_ne_jp/338 https://mail.google.com/mail/e/softbank_ne_jp/340 
 E:\KeiNakatsuma\Publication\SI2012\Paper\Fig\android01.png C:\Users\Kei\Dropbox\HapticsMan\ROBOMEC2012\figs\example.png 
E:\KeiNakatsuma\Publication\SI2012\Paper\Fig\IMG_4405.JPG C:\Users\Kei\Dropbox\HapticsMan\ROBOMEC2012\figs\device_hold.jpg 
Haptic-Emoticon: A Framework for Creating and Sharing Haptic Contents Figure 1. (from left to right) 
Haptic-Emoticon creating web application, Haptic-Emoticon trajectory with color graduation (timestamps), 
a 3x3 vibrotactile physical rendering display, Haptic-Emoticon haptic and visual rendering. Kei Nakatsuma 
Kumamoto University Takayuki Hoshi Nagoya Institute of Technology Ippei Torigoe Kumamoto University 1. 
Introduction Haptics is a complex sensation with pressure, temperature, vibration, and so on. Therefore 
developing an interface for creating or designing haptic contents is an attractive challenge. One solution 
is reduction and simplification of the haptic information. For example, Minamizawa et al. have simplified 
it as a temporal signal (sound) in TECHTILE toolkit [1]. We have inspired from handwriting motion on 
a skin. Our solution is a reduction to spatiotemporal information and we employ a spatiotemporal stroking 
trajectory on a 2D surface for a haptic content. According to our fundamental concept, in this demonstration, 
we present Haptic-Emoticon system which is a platform for creating, sharing, and rendering haptic contents 
for emotionally enhancing and decorating text messages. Emoticons (e.g. :-) , :-( , , etc.)) are 
used in text messages to represent writers feelings and moods over various text-based communication tools 
(e-mails, short message services, comments on social networking services, etc.). Since our reduction 
simplifies haptic information into 2D spatiotemporal trajectories, we can create the haptic contents 
as we draw some line pictures. By utilizing RGB color data on pixels as timestamps, we can store and 
share the haptic contents as an image file. This enables the haptic contents (spatiotemporal trajectories) 
being embedded into text messages like emoticons. We call them Haptic-Emoticon and exhibit its creating, 
sharing, and rendering system. 2. Haptic-Emoticon Technical Approach Our Haptic-Emoticon system includes 
a creating interface, an encoding algorithm and its sharing strategy, and a physical rendering device. 
The whole system is developed under a concept that it should be available as many people as possible. 
Therefore, our system utilizes some conventional devices and networking services. Users can create (design) 
the Haptic-Emoticon with common GUI devices with pointing input interfaces (e.g. touchscreens, mouse, 
etc.) because it is just a spatiotemporal stroking trajectory. The Haptic-Emoticon creating system is 
developed as a platform-independent web application. A user creates a Haptic-Emoticon by drawing a stroking 
trajectory on a touchscreen. The Haptic-Emoticon is graphically encoded in a conventional image format 
(Portable Network Graphics, PNG). The spatial information of the Haptic-Emoticon is recorded as its trajectory 
on an image, while the timestamps along the trajectory is stored as RGB color data of each pixel on it. 
In other words, the trajectory s color changes gradationally. Since Haptic-Emoticon is stored with the 
PNG format, it is easily distributed and shared with text messages over various internet protocol (IP) 
communication services such as e-mails, short message services (SMS), comments on social networking services 
(SNS), etc. Our first prototype supports Twitter (http://www.twitter.com) so that the Haptic-Emoticon 
images are posted with short text messages. The Haptic-Emoticon is physically rendered on a skin surface 
by using a custom 3x3 vibrotactile device. Haptic displays with 2D vibrator arrays have a long history 
(e.g. [2]). Similar to prior works, we applied a rendering algorithm based on the haptic apparent motion 
and the phantom sensation. We fabricated our prototype by using some affordable components such as nine 
vibration motors (FM23, Tokyo Parts), a microcontroller (Arduino Mega) with a basic motor driver circuit. 
The Haptic-Emoticon is rendered haptically on a palm and visually on a screen simultaneously. 3. Features 
and Future Works We proposed a novel haptic contents creating and sharing platform based on a 2D spatiotemporal 
reduction of haptic information. We exhibit the Haptic-Emoticon system based on our concept. While the 
system is not suitable to render any real haptic sensation, users can create and share their original 
Haptic-Emoticon and express their personal emotions as if drawing pictures. According to our basic idea 
that the system should be available for as many people as possible, we should employ a built-in vibrator 
in a smartphone, for example, and this is one of our future works. Since the creation and encoding of 
the Haptic-Emoticon are quite simple, we are planning to combine it with various artworks and entertainment 
contents (music, movies, paintings, poems, etc.) for exploring possibilities of emotional expression 
of haptic contents. Acknowledgement This work is partly supported by JSPS Grant-in-Aid for Scientific 
Research (24800009). References [1] Minamizawa, K., Kakehi, Y., Nakatani, M., Mihara, S., and Tachi, 
S., TECHTILE toolkit: a prototyping tool for designing haptic media, in SIGGRAPH 2012 Emerging Technologies. 
[2] Israr, A., Poupyrev, I., "Control space of apparent haptic motion," in Proc. of IEEE World Haptics 
Conference (WHC) 2011, pp.457-462, 2011. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Grant-in-Aid for Scientific Research</funding_agency>
			<grant_numbers>
				<grant_number>24800009</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503424</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[i-me TOUCH]]></title>
		<subtitle><![CDATA[detecting human touch interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503424</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503424</url>
		<abstract>
			<par><![CDATA[<p>i-me TOUCH is able to detect when one human touches another. By observing the different patterns of conductivity between two humans, the system is also able to record touch gestures such as tapping and rubbing. It detects tap gestures with 95% accuracy and rub gestures with 92% accuracy. And, a new idea is developed in this paper. That is the ability to detect when one person touches his or her own body. As a principle of operation, we extend the "Body as an Antenna" concept (developed by [Cohn et al. 2011]), but focus on the case of human-to-human touch. The approach used by [Sato et al. 2012] is accurate and can determine the type of substance. Our prototype and some preliminary experiments are discussed in this poster.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[touching behavior]]></kw>
			<kw><![CDATA[wearable sensors]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>H.5.2</cat_node>
				<descriptor>Input devices and strategies (e.g., mouse, touchscreen)</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10003120.10003121.10003125.10011666</concept_id>
				<concept_desc>CCS->Human-centered computing->Human computer interaction (HCI)->Interaction devices->Touch screens</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P4190686</person_id>
				<author_profile_id><![CDATA[82458807657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Yuko_Zou@ipci.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190687</person_id>
				<author_profile_id><![CDATA[82458666857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Leo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miyashita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Leo_Miyashita@ipci.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190688</person_id>
				<author_profile_id><![CDATA[81508690135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hayakawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Tomohiko_Hayakawa@ipci.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190689</person_id>
				<author_profile_id><![CDATA[82458617857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Siu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Independent Artist]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ericsiucm@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190690</person_id>
				<author_profile_id><![CDATA[81100469350]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Carson]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reynolds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[carson@k2.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190691</person_id>
				<author_profile_id><![CDATA[81100353752]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Masatoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Masatoshi_Ishikawa@ipci.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1979058</ref_obj_id>
				<ref_obj_pid>1978942</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cohn, G., Morris, D., Patel, S. N., and Tan, D. S. 2011. Your noise is my command. In <i>Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11</i>, ACM Press, 791.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Du Bois, D., and Du Bois, E. 1916. Clinical calorimetry: tenth paper a formula to estimate the approximate surface area if height and weight be known. <i>Archives of internal medicine 17</i>, 863--871.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2207743</ref_obj_id>
				<ref_obj_pid>2207676</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sato, M., Poupyrev, I., and Harrison, C. 2012. Touch&#233;. In <i>Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems - CHI '12</i>, ACM Press, New York, New York, USA, 483.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 i-me TOUCH: Detecting Human Touch Interaction Yuko Zou* Leo Miyashita Tomohiko Hayakawa University 
of Tokyo University of Tokyo University of Tokyo Eric Siu§ Carson Reynolds¶ Masatoshi Ishikawal Independent 
Artist University of Tokyo University of Tokyo Abstract i-me TOUCH is able to detect when one human 
touches another. By observing the different patterns of conductivity between two humans, the system is 
also able to record touch gestures such as tapping and rubbing. It detects tap gestures with 95% accuracy 
and rub gestures with 92% accuracy. And, a new idea is developed in this paper. That is the ability to 
detect when one person touches his or her own body. As a principle of operation, we extend the Body as 
an Antenna concept (developed by [Cohn et al. 2011]), but focus on the case of human-to-human touch. 
The approach used by [Sato et al. 2012] is accurate and can determine the type of substance. Our prototype 
and some preliminary experiments are discussed in this poster. CR Categories: H.5.2 [Information interfaces 
and presentation]: User Interfaces Input devices &#38; strategies Keywords: Wearable sensors; touching 
behavior We are interested in designing a wearable touch sensor that only responds to humans. The i-me 
TOUCH is minimal in that outside of the sensor, no other special devices are used. Because the sensor 
is light, there are many possible places on the body it can be worn (such as wrist, ankle, ear, etc.). 
According to a formula by [Du Bois and Du Bois 1916], a human being s skin surface area can be ap­ proximated 
by height and weight. For instance, a person who is 170cm and 65kg has approximately 1.75m 2 of skin. 
That s quite a bit, so why not use this surface area? Figure 1: Two iterations of i-me TOUCH, showing 
the shrinking form-factor. When a human approaches the antenna of an analog radio or tele­vision they 
can alter the signal reception. This ability to interact with the near .eld of a radio-frequency antenna 
is exploited by de­vices such as the Theremin. We became interested in this and re­lated bioelectric 
phenomena. We began experiments to duplicate the human-antenna effect making use of our own radio-frequency 
beacon. Figure 1 above shows the .rst prototype. A Faraday cage *e-mail: Yuko Zou@ipc.i.u-tokyo.ac.jp 
e-mail: Leo Miyashita@ipc.i.u-tokyo.ac.jp e-mail: Tomohiko Hayakawa@ipc.i.u-tokyo.ac.jp §e-mail: ericsiucm@gmail.com 
¶e-mail: carson@k2.t.u-tokyo.ac.jp Ie-mail: Masatoshi Ishikawa@ipc.i.u-tokyo.ac.jp shields the transmitter 
and is used as a reference signal for the re­ceiver. An electrode from the transmitter is attached to 
the wearer s skin, effectively making his or her entire body an antenna. The received signal varies when 
conductive substances come in con­tact with the wearer including other humans. This early version was 
tested with N = 11 participants to detect tap gestures with 95% accuracy and rub gestures with 92% accuracy. 
The poster ac­companying this paper shows that hand gestures are detected by variations of an intensity. 
And now, our new i-me TOUCH fo­cuses on the phenomena that the received signal varies its power. Theremin 
and our old i-me TOUCH use an oscillator as a special device to emit an electromagnetic wave and that 
makes the sys­tem bigger and heavier. If the environmental noise from electrical goods, which is 50Hz 
(in Japan), can be used and no other device is needed, the system can take advantage of this phenomena 
and also be made more lightweight and useful to wear. We imagine i-me TOUCH being used in crowded places, 
which usually have environmental noise. The principle of operation for our new pro­totype is to allow 
the human wearer to manipulate the reception of environmental noise by his or her own body movement. 
To observe how the noise works when human s approach, we made some pre­liminary experiments. Figure 4 
on the accompanying poster shows that when i-me TOUCH is given some mechanical vibration, the noise becomes 
bigger. The i-me TOUCH uses electrodes whose contacts are physically changed when mechanical force (such 
as a bump) is applied by the wearer or another person. Figure 5 on the poster shows that the received 
signal s power is varied when the i-me TOUCH is brought near wearer s other hand. By using this, like 
the Theremin, the i-me TOUCH can detect the wearer s ges­tures. Of course, more experiments are needed 
to understand how the motion artifacts can be used for interaction. The i-me TOUCH is still a work-in-progress 
and many open re­search questions remain. One matter that we have not settled is the precise unit of 
measure for the sensor s readings. Our experiments looked at touching, rubbing, and clapping as input 
gestures but new experiments might consider a wider variety of gesture types using a real-time classi.er. 
We feel further research rooted in bioelec­tric phenomena may lead us to a better understanding of our 
own mysterious skin. References CO H N , G., MO R R IS, D., PAT E L , S. N., A N D TA N , D. S. 2011. 
Your noise is my command. In Proceedings of the 2011 annual conference on Human factors in computing 
systems -CHI 11, ACM Press, 791. DU BO I S , D., A N D DU BO I S , E. 1916. Clinical calorimetry: tenth 
paper a formula to estimate the approximate surface area if height and weight be known. Archives of internal 
medicine 17, 863 871. SATO , M., POUP Y RE V, I., AND HAR RI S O N , C. 2012. Touche.´In Proceedings 
of the 2012 ACM annual conference on Human Factors in Computing Systems -CHI 12, ACM Press, New York, 
New York, USA, 483. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503425</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Perch on my arm!]]></title>
		<subtitle><![CDATA[a haptic device that presents weight and a sense of being grabbed]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503425</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503425</url>
		<abstract>
			<par><![CDATA[<p>Touching and felling physical objects has been an important method of understanding and communicating with others. Although most of the existing haptic devices are limited to the hand, arms are used in a daily basis to interact with other people and animals by linking, hugging or being grabbed. In attempt to realize interactions with a virtual character we propose a novel haptic device that can grab and provide pressure to the human arm. We have also created a system that utilizes two of these devices to virtually represent a bird perching on a person's arm. Unlike the approach of wearable devices [Sato et al. 2008], our system does not require attachment in advance, so as to make the experience intuitive. In addition, the users can feel different feedback according to their actions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190692</person_id>
				<author_profile_id><![CDATA[82459135157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishihara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190693</person_id>
				<author_profile_id><![CDATA[82459231657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190694</person_id>
				<author_profile_id><![CDATA[82458639257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kotaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Abe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190695</person_id>
				<author_profile_id><![CDATA[82458774157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Fuka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nojiri]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190696</person_id>
				<author_profile_id><![CDATA[82459289157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Eri]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sekiguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190697</person_id>
				<author_profile_id><![CDATA[82458988557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hitomi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190698</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401618</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sato, K., Sato, Y., Sato, M., Fukushima, S., Okano, Y., Matsuo, K., Ooshima, S., Kojima, Y., Matsue, R., Nakata, S., Hashimoto, Y. and Kajimoto, H. 2008. Ants in the Pants. ACM <i>SIGGRAPH 2008, New Tech Demos</i>. Article No. 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Perch on My Arm!: A Haptic Device that Presents Weight and a Sense of Being Grabbed Yumi Nishihara* 
Marina Mitani* Kotaro Abe* Fuka Nojiri* Eri Sekiguchi* Hitomi Tanaka* Yasuaki Kakehi* Keio University 
  Figure 1: Basic Mechanism Figure 2: Perch on My Arm! 1 Introduction Touching and felling physical 
objects has been an important method of understanding and communicating with others. Although most of 
the existing haptic devices are limited to the hand, arms are used in a daily basis to interact with 
other people and animals by linking, hugging or being grabbed. In attempt to realize interactions with 
a virtual character we propose a novel haptic device that can grab and provide pressure to the human 
arm. We have also created a system that utilizes two of these devices to virtually represent a bird perching 
on a person s arm. Unlike the approach of wearable devices [Sato et al. 2008], our system does not require 
attachment in advance, so as to make the experience intuitive. In addition, the users can feel different 
feedback according to their actions. 2 Mechanism of a Haptic Device for the Arm We propose a novel device 
that presents weight and grabbing sen­sations to the human arm. It is made of a simple yet durable frame­work 
and does not require any attachment to the user. We have adopted an underactuated structure to this device 
which can be con­trolled to naturally grab objects with few motors. At the top of the device, two servomotors 
are installed to move the structure verti­cally by using a crank mechanism. This device passively grabs 
when it is pressed against an object (Figure 1). The section that grabs consists of 6 plastic joints 
and is adjustable to the size of the object it is pressed against. In the current implementation, it 
is designed for grasping an object with a diameter of 35 to 60 millime­ters. This device is .exible enough 
to withstand user s movements such as twisting, shaking or pulling their arm. A DC motor is also attached 
at the top center of the device. While it is placed on the ladder it can move the device horizontally, 
enabling different ap­proaches of presentation. In addition, by changing the movement of the device the 
structure can .exibly and gradually grab objects, providing a wider variety of haptic expressions that 
have yet to be achieved by the currently existing haptic devices. 3 Application: Perch on My Arm! The 
haptic devices are implemented in the application called Perch on My Arm! in which users can virtually 
feel a bird perching on their arm (Figure 2). The tactile and audio feedback is produced from a box that 
has two holes on the facing sides. At *ykakehi@sfc.keio.ac.jp Figure 3: System Design .rst, the user 
puts his or her arm through the holes of the box. Then the bird comes .ying from above and perches on 
the arm. In an attempt to keep its balance, it moves its feet and walks sideways. The user can also interact 
with the virtual bird. When the user shakes the inserted arm, the bird wildly .aps its wings in alarm 
and takes a .rmer grip of the arm. When the user taps on the side of the box with the opposite hand, 
the bird jolts in surprise and chirps correspondingly. After interacting with the user for a while, the 
bird is satis.ed and .ies away. Figure 3 shows the system overview. This system is managed by a program 
on the computer which controls the microcontrollers. The program runs according to the values obtained 
by the sensors that detect the user. Just below the hole of the box, a photosensor and photore.ector 
is set to detect the user s arm when it is completely inserted and when it is shaken. A pressure sensor 
is attached beneath the haptic device to detect whether it was able to .rmly grab the user s arm and 
to monitor the amount of pressure it is applying to the user. A contact microphone is attached to the 
side of the box and is used to detect when the user taped the box. To represent the sense of .apping 
wind, air is generated by an air compressor and ejected by opening and closing the solenoid valve which 
is attached to hoses. The audio of the bird chirping and .apping is played from the computer through 
speakers that are placed inside the box. This application was demonstrated in several exhibitions. Despite 
the absence of visual feedback, many users enjoyed the weight and movements of the virtual bird. As a 
result of the human arm being less sensitive than the hand and the haptic devices .exible movements, 
many users who observed the haptic devices after the experience remarked that the bird s feet had felt 
softer than the plastic joints appeared and several commented that it felt similar to the texture of 
rubber or felt. In the future, we intend to present an extended variety of sensations by changing and 
improving the size, movements and grasping strength of the device in order to develop additional applications. 
 References SATO, K., SATO, Y., SATO, M., FUKUS H I MA, S., OKA N O , Y., MATSUO , K., OO S H I M A , 
S., KO J I M A, Y., MATSUE, R., NA K ATA , S., HA S H I M OTO, Y. AN D KAJI M OTO , H. 2008. Ants in 
the Pants. ACM SIGGRAPH 2008, New Tech Demos. Article No. 3. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use isgranted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503426</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Pot the magic pot]]></title>
		<subtitle><![CDATA[interactive modification of the perceived angular shape]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503426</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503426</url>
		<abstract>
			<par><![CDATA[<p>"Pot the Magic Pot" is an interactive system which enables us to experience a haptic illusion as if we were handling various shapes of object of which form and stiffness are modified by users interactively.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190699</person_id>
				<author_profile_id><![CDATA[81488658104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ban]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ban@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190700</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190701</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190702</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2343032</ref_obj_id>
				<ref_obj_pid>2342896</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. Ban et al.: Magic Pot: interactive metamorphosis of the perceived shape: ACM SIGGRAPH2012 Emerging Technologies p. 14, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2345620</ref_obj_id>
				<ref_obj_pid>2345616</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Y. Ban et al.: Modifying an identified curved surface shape using pseudo-haptic effect: HAPTICS2012, pp. 211--216, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2345620</ref_obj_id>
				<ref_obj_pid>2345616</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Y. Ban et al.: Modifying an identified angle of edged shapes using pseudo-haptic effects: Haptics: Perception, Devices, Mobility, and Communication, pp. 25--36, 2012.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1502413</ref_obj_id>
				<ref_obj_pid>1502409</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications of Pseudo-Haptic Feedback, Presence: Teleoperators and Virtual Environments, MIT Press, Volume 8, Issuel, pp. 39--53, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pot the Magic Pot: Interactive Modification of the Perceived Angular Shape Yuki Ban , Takuji Narumi 
, Tomohiro Tanikawa , Michitaka Hirose Graduate School of Information Science and Technology, the University 
of Tokyo  Fig1. Presentable shape   Fig.2 Video See-through System Fig.3 Distortion map based on 
an edge's posture  1. Introduction1  email: {ban, narumi, tani, hirose}@cyber.t.u-tokyo.ac.jp "Pot 
the Magic Pot" is an interactive system which enables us to experience a haptic illusion as if we were 
handling various shapes of object of which form and stiffness are modified by users interactively. Haptics 
is one of the most important sensations in our life, and many researches have been conducted to realize 
a device to present virtual haptic sensations. However, because these devices are mainly focus on active 
haptics which aim to reproduce physical force feedback, they become physically too complicated when we 
try to reproduce complex haptic sensations to use them easily. We have aimed to construct the perception-based 
shape display system which can display various shapes with a brief mechanism, by modifying a perception 
of shape using visuo- haptic interaction. We already constructed video-see through system which can modify 
an identified curved surface shape by displacing and deforming user's hand image on the monitor [1]. 
With this system, we can compose visual feedback as if users were touching various curved surface shape 
although they are touching only a static cylinder in actual. To present more various and complex shape 
with this system, we also should adapt to modify the perception of angular shape, stiffness, texture 
and so on. However, it is very difficult to make users to perceive the angular shape with only touching 
a static cylinder. Though, we revealed that we can display angular or scoop with a small particular shape 
such as small bump through the user experiments [2]. Meanwhile, we also revealed that modifying the movement 
of hand image to fit to the virtual object can control an identified angle and position of edges on an 
object [3]. In addition, Lecuyer et al. revealed the pseudo-haptic effect can alter the perception of 
stiffness with space ball [4]. From these knowledge, this time we constructed interactive visuo-haptic 
system which can alter more complicated shape include the angular shape and shapes of various stiffness 
(Fig.1). 2. Rendering algorithm for Visual Feedback  To evoke effective pseudo-haptic illusion, we composed 
a video see-through system (Fig.2) which can make up an inconsistent situation between our vision and 
haptic sensation. As the physical object users actually touch behind the monitor, we set a cylindrical 
object with small bump (Fig.2). The elastic rubber with pressure sensors cover a surface and a bump of 
this object. The system measures the surface pressure users push and determine the amount of deformation 
of a surface to modify the perceptual stiffness. The elastic rubber is the source of various perceptual 
stiffness the system composes. The hand image is replaced and deformed to fit to the virtual object. 
This rendering algorithm is composed of three processes. First, from captured image, we detect the contact 
point and the area of hand and cylindrical object based on color. Then the system generate a distortion 
map based on difference between the shape of the physical object and the one of virtual object. In this 
phase, we compose the distortion map to synchronize the timing that a finger passes through an edge physically 
and virtually to modify the perception of position and angle of edge (Fig.3). Finally, we translate and 
deform the shape of the user's hands and fit it to the virtual shape. To deform user's hands, we use 
the algorithm based on moving least squares, which can generate the natural deformation considering the 
rigidity of the object, based on the displacement of control points. With this method, we already revealed 
that it can be possible to modify an identified size of object handled with two or more fingers . We 
displace the user's hand according to the distortion map computed in previous step. Also this system 
is able to modify its stiffness variously. The system measures the surface pressure and controls the 
deformation amount to modify an identified stiffness of the virtual object. 3. "Forming" Shape Interactively 
 With the rendering algorithm previously described, we implemented an interactive system which displays 
a variety of shapes of virtual objects. In this experience, a user reform the virtual shape freely like 
pugging a clay by pinching and pushing the edges and surface of shape. The shape of virtual object is 
deformed based on these actions. Then we compare the shape of a physical cylinder placed behind the monitor 
and the one of virtual object in the display, and render the visual feedback to evoke pseudo-haptic sensations, 
which make users feel as if the curvature and the edge of the cylindrical object changed according to 
reforming. References [1] Y.Ban et al.: Magic Pot: interactive metamorphosis of the perceived shape: 
ACM SIGGRAPH2012 Emerging Technologies p. 14, 2012. [2] Y.Ban et al.: Modifying an identified curved 
surface shape using pseudo-haptic effect: HAPTICS2012, pp. 211-216, 2012. [3] Y.Ban et al.: Modifying 
an identified angle of edged shapes using pseudo-haptic effects: Haptics: Perception, Devices, Mobility, 
and Communication, pp. 25-36, 2012. [4] A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey 
of Research and Applications of Pseudo-Haptic Feedback, Presence: Teleoperators and Virtual Environments, 
MIT Press , Volume 8, Issuel, pp. 39-53, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503427</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Visuo-haptic interaction with mobile rear touch interface]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503427</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503427</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose the visuo-haptic system that is able to evoke the haptic sensation on the mobile device, without any haptic devices, only using visuo-haptic interaction.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190703</person_id>
				<author_profile_id><![CDATA[82459331757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Arata]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kokubun]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kkbnart@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190704</person_id>
				<author_profile_id><![CDATA[81488658104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ban]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ban@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190705</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190706</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190707</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1294259</ref_obj_id>
				<ref_obj_pid>1294211</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Wigdor, S et al.: Lucid touch: a see-through mobile device, the 20th ACM UIST, pp.269--278, 2007]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1502413</ref_obj_id>
				<ref_obj_pid>1502409</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications of Pseudo-Haptic Feedback, Presence: Teleoperators and Virtual Environments, MIT Press, Volume 8, Issue 1, pp. 39--53, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visuo-haptic Interaction with Mobile Rear Touch Interface Arata Kokubun , Yuki Ban , Takuji Narumi , 
Tomohiro Tanikawa , Michitaka Hirose Graduate School of Information Science and Technology, the University 
of Tokyo    Fig.1 Polygon deformation on pushing   Fig.2 Sine wave deformation   Fig.3 The ratio 
that subjects answered the large deformed model was softer against deformation-ratio 1. Introduction1 
 email: {kkbnart, ban, narumi, tani, hirose}@cyber.t.u-tokyo.ac.jp In this paper, we propose the visuo-haptic 
system that is able to evoke the haptic sensation on the mobile device, without any haptic devices, only 
using visuo-haptic interaction. Touch panel is a compelling input modality for interactive devices. However, 
touch input is inaccurate and hard to see objects under touch due to occlusion by a user's finger. To 
solve this problem, the rear touch interface is actively researched and commercialized [Wigdor et al. 
2007]. Using this interface, users can input with a touch panel arranged behind a device, and a cursor 
shows the touching point on the screen. On the other hand, there are increasing numbers of works that 
focus on passive haptics, like the visuo-haptic interaction. The visuo-haptic interaction is a kind of 
cross modal effect between our visual and haptic sense. Pseduo Hpaitcs is one of the well known phenomenon 
of the visuo-haptic interaction, which indicates an illusional perception in our haptic sensation evoked 
by vision [Lecuyer 2009]. This illusion is evoked under an inconsistent situation between the physical 
behavior of our body and the one observed in our vision. For example, it is revealed that the mismatch 
between a speed of physical computer mouse and the one of corresponding cursor in display evokes illusional 
force feedback on our hands. To provoke the visuo-haptic interaction which includes pseudo haptic effect, 
it is need to hide a user s hand or put it other place like the positional relation between a cursor 
on the screen and a hand on a mouse. Thus it is difficult to provoke the visuo-haptic interaction, for 
the normal type of touch panel which we contact directly. Meanwhile, with the rear touch interface, we 
are able to evoke the visuo-haptic interaction because users hands are hidden behind the screen. In our 
system, we compose a rendering algorithm of visual feedback to evoke the effect of the visuo-haptic interaction 
with mobile rear touch interface, which affects our perception of softness. We compose a system with 
a rear touch interface that can deform the shape of the polygon on the monitor as a function of pressure 
of pushing (Fig.1). 2. Affect Perception of Softness with Rear Touch Interface  To make up an inconsistent 
situation between our vision and haptic sensation to affect our perception of softness, we composed a 
rear touch interface which can control the deformation amount of the polygon on the monitor which users 
push from the back of devices. To investigate the effect of the visuo-haptic interaction with rear touch 
interface, we constructed prototype system. For this system, we used an android mobile device, and attached 
a pressure sensor on the back of it. The shape of polygon on the monitor is deformed depending on pressure 
intensity with which users push the back of the device. The visual feedback of the polygon s deformation 
modifies the user s perception of softness. To deform the polygon, we assumed the situation that a user 
locally pushes the center of the device's backside, and the polygon is deformed as a shape of sine wave 
(Fig.2). The softness of the polygon was determined by the value of deformation module, k (d = k*p, d 
and p means amount of deformation and inserted pressure respectively). We constructed 5 polygon models 
whose deformation modules were set to the ratio of 0:1:2:3:4. 3. Experiment and Results  To investigate 
the effect of visual feedback to the perception of softness, we conducted user experiment. In each trial 
of this experiment, subjects were asked to push in the center of the device's backside with two different 
models and answer which model is softer or same. The study had 12 subjects, and we conducted this trial 
60 times to each subject (20 combinations to choose 2 from 5 models, and 3 times of each combination). 
Fig.3 shows the ratio that subjects answered the large deformed model was softer against deformation-ratio. 
Deformation-ratio is defined as the value of larger k divided by smaller k of two models to compare in 
each trial. In this graph, the larger deformation-ratio is, the more subjects likely to feel the gap 
of softness between two models. Moreover, the graph tend to go flat where deformation-ratio is under 
1.5 and over 3.0. This fact indicates that it is possible to change the perception of softness gradually 
by controlling deformation-ratio between 1.5 and 3.0. Our result shows that it is possible to affect 
the perception of softness using visuo-haptic interaction with rear touch interface. For future works, 
we will do accurate and quantitative evaluation of the effect of visuo-haptic interaction of softness. 
In addition, we are currently working on evoking other force than softness and applying these forces 
to applications on mobile devices. Especially, shear force can be evoked using pseudo-haptic effects. 
Moreover, we are trying to evoke stronger haptic sensation by showing deformed image of user s hand on 
the monitor. References Wigdor, S et al.: Lucid touch: a see-through mobile device, the 20th ACM UIST, 
pp.269-278, 2007 A. Lecuyer: Simulating Haptic Feedback Using Vision: a Survey of Research and Applications 
of Pseudo-Haptic Feedback, Presence: Teleoperators and Virtual Environments, MIT Press , Volume 8, Issue 
l, pp. 39-53, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503428</section_id>
		<sort_key>430</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>2503429</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A nonluminous display that controls the evaporation velocity]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503429</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503429</url>
		<abstract>
			<par><![CDATA[<p>In this paper, we propose a novel ambient display technology that uses variations in moisture across a nonluminous material surface to render target images.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190708</person_id>
				<author_profile_id><![CDATA[82459207757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomomi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sonoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190709</person_id>
				<author_profile_id><![CDATA[81504685909]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190710</person_id>
				<author_profile_id><![CDATA[84460708057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mitsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsushita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kansai University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dorm Light Show: http://www.collegehumor.com/video/4034511/dorm-light-show]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2076410</ref_obj_id>
				<ref_obj_pid>2076354</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Okude, E. and Kakehi, Y.: Rainterior: An Interactive Water Display with Illuminating Raindrops, In <i>Proc. ITS2011</i>, pp. 270--271 (2011).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Nonluminous Display that Controls the Evaporation Velocity. Tomomi Sonoda Takuma Tanaka Mitsunori 
Matsushita Kansai University Figure 1: Proposed system con.guration 1 Introduction In this paper, we 
propose a novel ambient display tech­nology that uses variations in moisture across a nonlumi­nous material 
surface to render target images. In recent years, various unconventional display systems have been proposed. 
These systems often make use of objects commonly available in our surroundings. For ex­ample, the dorm 
light show [1] uses windows of a dor­mitory as a pixel matrix, with the ON/OFF state of each room s light 
.xture expressing the 1-bit luminosity of a tar­get image. Another unconventional system, Rainterior 
[2], projects images onto water surfaces using the ripples made by coordinated raindrops. It is well 
known that the color of the surface of a road darkens after a rain shower, and reverts to its original 
color as the road surface dries. We propose an imaging system that makes use of this visible transformation 
between a wet and dry surface as a motif for display. The proposed system controls imaging by controlling 
the dryness of a nonlumi­nous display material. 2 Proposed System Figure 1 shows the structure of our 
proposed display, consisting of a surface material, a rubber .xture, temper­ature control devices, a 
heat sink, and an acrylic plate. For prototype purposes, we selected as our surface mate­rial an allochroic 
paper typically used in Japanese calligra­phy. Since this paper is extremely thin, it must be backed 
by a rubber .xture for stability and durability. Peltier devices, which are square-shaped discs with 
varying surface tem­peratures, are used to control the temperature of the display surface. In this way, 
surface temperatures can be varied from 20 to 60.C. In our prototype system, Peltier devices are mapped 
onto a 4 × 4 matrix. Each Peltier device corresponds to a sin­gle pixel of the display, and a heat sink 
made of copper is attached to each device so that excess heat can be radiated away, as shown in Figure 
1. Figure 2: Changes in the display surface The surface of our display can transition from wet to dry 
on a per pixel basis. After water is sprinkled across the dis­play surface, the system can control the 
evaporation rate at each pixel location by changing the temperature of the cor­responding Peltier device. 
Wet regions of the surface corre­spond to the color black, and dry regions to white. Hence, immediately 
after water is sprinkled across the surface, the entire display is black. To change a pixel to white, 
the system sets the corre­sponding Peltier device to hot, and to change it to black, the corresponding 
Peltier device is set to cold. These con­trol settings enable the absorbed water to evaporate at dif­ferent 
rates, with the intended pattern appearing after 1 or 2 min, and then disappearing a couple of minutes 
later, as shown in the upper column of Figure 2. Although the heat­ing function is what performs the 
transformation of black pixels to white, both cooling and heating functions are nec­essary, especially 
at the edges of the image where the tem­perature of one pixel can affect the temperature of neigh­boring 
pixels, causing them to blur (see lower column of Figure 2). 3 concluding remarks We proposed a nonluminous 
display that makes use of the phenomenon of evaporation. To support expression of more detailed forms, 
like letters and symbols, we intend to increase the number of Peltier devices in future prototypes. Furthermore, 
given the limitation of the current system to black and white patterns, we would like to test materials 
that change color when they absorb water. References [1] Dorm Light Show: http://www.collegehumor. com/video/4034511/dorm-light-show 
[2] Okude, E. and Kakehi, Y.: Rainterior: An Interactive Water Display with Illuminating Raindrops, In 
Proc. ITS2011, pp. 270 271 (2011). Permission to make digital or hard copies of part or all of this work 
for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and thatcopies bear this notice and the full citation on the first page. Copyrights 
for third-party components of this work must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California.2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503430</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Computational light field display for correcting visual aberrations]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503430</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503430</url>
		<abstract>
			<par><![CDATA[<p>We create a computational light field display that corrects for visual aberrations. This new method enables better image resolution and higher image contrast. The prototype is built using readily available off-the-shelf components and has a thin form factor for mobile devices.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190711</person_id>
				<author_profile_id><![CDATA[81548306256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fu-Chung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190712</person_id>
				<author_profile_id><![CDATA[81508692799]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gordon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wetzstein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190713</person_id>
				<author_profile_id><![CDATA[81100387067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Barsky]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Berkeley]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190714</person_id>
				<author_profile_id><![CDATA[81548005482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2366204</ref_obj_id>
				<ref_obj_pid>2366145</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Huang, F.-C., Lanman, D., Barsky, B. A., and Raskar, R. 2012. Correcting for optical aberrations using multilayer displays. <i>ACM Trans. Graph. (SIGGRAPH Asia) 31</i>, 6, 185:1--185:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2185577</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pamplona, V., Oliveira, M., Aliaga, D., and Raskar, R. 2012. Tailored displays to compensate for visual aberrations. <i>ACM Trans. Graph. (SIGGRAPH)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Computational Light Field Display for Correcting Visual Aberrations Fu-Chung Huang Gordon Wetzstein 
Brian A. Barsky Ramesh Raskar UC Berkeley MIT Media Lab UC Berkeley MIT Media Lab Figure 1: Correcting 
myopic vision using light .eld displays. People with optical aberrations in their eyes see blurred images 
with a conventional display. Huang et al.[2012] use multilayer pre.ltering to enhance sharpness, but 
the contrast is reduced. Pamplona et al.[2012] employ a ray tracing solution requiring very high angular 
resolution light .eld based display; unfortunately, due to hardware tradeoff, the spatial resolution 
is very low. Our method combines advantges from both methods, enabling both high spatial resolution and 
high contrast. Abstract We create a computational light .eld display that corrects for visual aberrations. 
This new method enables better image resolution and higher image contrast. The prototype is built using 
readily avail­able off-the-shelf components and has a thin form factor for mobile devices. 1 Introduction 
Correcting for visual aberrations with a computational display would enable millions of people to see 
displays without wearing eyeglasses. Recent research on vision-correcting displays propose technologies 
that are either low resolution[Pamplona et al. 2012] or low contrast[Huang et al. 2012]. We present a 
new computational display approach that allows a screen to show a two-dimensional image at a distance 
from the physical device but within the focal range of the observer. Through designing optics hardware 
in concert with pre.ltering algorithms, the proposed computational display architecture achieves signi.­cantly 
higher resolution and contrast than exhibited by prior ap­proaches to vision-correcting image display. 
We evaluate our dis­play architecture using both simulation and a prototype vision­correcting device. 
 2 Method To have a desired target image received by the eye, we determine the display side object light 
.eld. he rays of the light .eld undergo a series of transformations such as propagations and refraction 
in the eye, and the retinal irradiance I(x e) can be obtained from the object surface light .eld using 
the inverted combined transforms M-1 . By uniformly sampling N discrete rays over the aperture, we obtain 
the following expression for every target image pixel received on the retina: e 1 -1 e T -1 I(x ) = 
w·Lo(M·[x , .]) . Lo = WI .Lo = 0 N where w is the number of ray samples that fall on a speci.c object 
light .eld coordinate, and W is the projection matrix from a 4D light .eld to a 2D image. For the object 
light .elds to be phys­ically representable, we further constrain the objective to be non­negative. This 
forms a basic framework for light .eld pre.ltering; speci.cally, for a given target image to be received 
by the eye, the object side light .eld is determined by inverting the projection ma­trix gives. 3 Implementation 
and Results We solve the light .eld and construct the experiment with hyperopic vision. On the left of 
Fig. 2 are the photographed results using a prototype constructed with a pinhole array mask and an iPod 
touch, as shown on the right of Fig. 2. Light .eld predistortion [Pamplona et al. 2012] requires high 
angular resolution, otherwise it is only slightly better than that without correction. Our approach using 
the same hardware produces much sharper images, matching the simulated results. Figure 2: Photographs 
of -6D hyperopic correction and prototype. The camera simulates a human pupil with a diameter of 6 mm 
at a distance of 25 cm to the screen, while the eye is focused at 45 cm.  References HUA NG , F.-C., 
LA N M A N, D., BA R SKY, B. A., AND RASK A R, R. 2012. Correcting for optical aberrations using multilayer 
displays. ACM Trans. Graph. (SIGGRAPH Asia) 31, 6, 185:1 185:12. PA MPLONA , V., OLI V E I RA , M., AL 
IAG A, D., AND RA S K A R, R. 2012. Tailored displays to compensate for visual aberrations. ACM Trans. 
Graph. (SIGGRAPH). Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503431</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Cost-based workload balancing for ray tracing on multi-GPU systems]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503431</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503431</url>
		<abstract>
			<par><![CDATA[<p>Ray tracing is at the core of most techniques for creating realistic imagery. Parallel implementations of ray tracing handle the irregular workload through task systems. The strengths of static and dynamic scheduling strategies are complementary to each other. Static strategies do not incur in synchronization overhead while dynamic strategies generally provide computational times closer to the optimal scheduling. Hybrid strategies combining good static initialization and dynamic task assignation have been shown to be a better alternative than pure static and dynamic strategies [Heirich and Arvo 1998]. We experiment with a novel strategy for load balancing on multi-GPU systems. We obtain a quick estimate of the cost of traversing batches of rays over bounding volume hierarchies. The estimated costs are used to achieve a tighter assignation of tasks to processing units. Results suggest that cost-based initialization can enhance common balancing strategies and reduce rendering times.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[multi-GPU computing]]></kw>
			<kw><![CDATA[ray tracing]]></kw>
			<kw><![CDATA[workload balancing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190715</person_id>
				<author_profile_id><![CDATA[82459268157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mario]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rinc&#243;n-Nigro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Houston]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mario.rincon.nigro@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190716</person_id>
				<author_profile_id><![CDATA[81100180219]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Zhigang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Deng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Houston]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zdeng@cs.uh.edu.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1572792</ref_obj_id>
				<ref_obj_pid>1572769</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Aila, T., and Laine, S. 2009. Understanding the efficiency of ray traversal on GPUs. In <i>Proc. High-Performance Graphics 2009</i>, 145--149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>285112</ref_obj_id>
				<ref_obj_pid>285109</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Heirich, A., and Arvo, J. 1998. A competitive analysis of load balancing strategies for parallel ray tracing. <i>The Journal of Supercomputing 12</i>, 1-2, 57--68.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cost-based Workload Balancing for Ray Tracing on Multi-GPU Systems Mario Rinc ´on-Nigro* Zhigang Deng 
University of Houston Keywords: Ray Tracing, Workload Balancing, Multi-GPU Com­puting 1 Introduction 
Ray tracing is at the core of most techniques for creating realistic imagery. Parallel implementations 
of ray tracing handle the irregu­lar workload through task systems. The strengths of static and dy­namic 
scheduling strategies are complementary to each other. Static strategies do not incur in synchronization 
overhead while dynamic strategies generally provide computational times closer to the op­timal scheduling. 
Hybrid strategies combining good static initial­ization and dynamic task assignation have been shown 
to be a bet­ter alternative than pure static and dynamic strategies [Heirich and Arvo 1998]. We experiment 
with a novel strategy for load balancing on multi-GPU systems. We obtain a quick estimate of the cost 
of traversing batches of rays over bounding volume hierarchies. The estimated costs are used to achieve 
a tighter assignation of tasks to processing units. Results suggest that cost-based initialization can 
enhance common balancing strategies and reduce rendering times. 2 Our Approach We estimate the cost 
of processing each task by performing a reduced traversal of the rays over bounding volume hierarchies 
(BVHs). The reduced traversal of a ray does not return ray hits, but the number of primitive intersection 
tests (i.e. boxes and tri­angles) that need to be performed in order to compute the hit, and can be performed 
faster than a full trace operation. This works as current high performance implementations of ray tracing 
on GPUs (e.g., [Aila and Laine 2009]) rely on texture memory for caching BVH nodes during ray traversals. 
Triangle primitives on the other hand are not cached, for these are not requested nearly as often as 
the subset of nodes that were recently traversed. Performing the re­duced traversal for every ray within 
a task results in excessive over­head, however. We reduce the estimation overhead by sampling the tasks. 
Coherent rays are sampled over a Z-curve, and diffuse rays are randomly sampled. Enhanced initialization 
of the tasks system can then be achieved by enforcing a scheduling in which the re­maining task with 
the largest cost is assigned to the GPU with the least amount of work. 3 Results The time taken by the 
reduced traversal was measured to be 42.5% and 43.5% of the full trace operation on a NVidia Tesla C1060 
GPU, for coherent and diffuse rays, respectively. We empirically found that sampling 12.5% of rays results 
in overheads of 3.2% and 4.1% of the full trace operation, and estimation errors of 3.2% and 4.1%, for 
coherent and diffuse rays, respectively. Figure 1 shows the effect on the overall tracing times of using 
the estimated costs for initializing a static distributed queue, and a centralized queue. Experiments 
were performed on an AMAX machine with a Xeon E5520 processor, 8GB of RAM memory, and 3 NVIDIA Tesla 
C1060 GPUs. Our implementation of the centralized multi­ *e-mail:mario.rincon.nigro@gmail.com e-mail:zdeng@cs.uh.edu.com 
GPU queue involves one kernel launch for each task in order to synchronize the GPUs. The best tracing 
times were measured for the static distribution with cost initialization. Intuitively one would expect 
the centralized queue to outperform the static strategy, but current general purpose GPU programing frameworks 
(e.g. CUDA and OpenCL) do not provide direct ways to avoid the kernel inter­ruption required for synchronization 
between units. Figure 1: Tracing times for cost-initializated (colored bars), and regular versions (background 
bars) of a static distributed queues task system and a centralized queue task system for various task 
sizes and two scenes (Backyard, 213802 tris; Mini, 234443 tris). 4 Future Work Global memory accesses 
are the bottleneck in current high per­formance GPU tracers. The reduced traversal for cost estimation 
works by taking advantage of this shortcoming. A more general approach for cost estimation needs to be 
investigated for use within parallel platforms other than current multi-GPU systems. Addi­tionally, ef.cient 
synchronization mechanisms for the centralized queue in multi-GPU systems should be investigated for 
a complete evaluation. References AI L A , T., AN D LAI N E , S. 2009. Understanding the ef.ciency of 
ray traversal on GPUs. In Proc. High-Performance Graphics 2009, 145 149. HEI R I C H , A., A N D ARVO, 
J. 1998. A competitive analysis of load balancing strategies for parallel ray tracing. The Journal of 
Supercomputing 12, 1-2, 57 68. Permission to make digital or hard copies of part or all of this work 
for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503432</article_id>
		<sort_key>470</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[GPU accelerated interactive integral photography system using extended fractional view method]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503432</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503432</url>
		<abstract>
			<par><![CDATA[<p>We developed a novel system of interactively displaying 3D images, in which integral photography images with the method of extended fractional views both with horizontal and vertical parallax could be rendered much faster by using a graphics processing unit (GPU). Therefore, displayed auto-stereoscopic 3D computer graphics (CG) objects could be moved very smoothly with a gamepad.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190717</person_id>
				<author_profile_id><![CDATA[81365598536]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yanaka@ic.kanagawa-it.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190718</person_id>
				<author_profile_id><![CDATA[81421601221]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kazuhisa Yanaka: Integral photography using hexagonal fly's eye lens and fractional view, Proc. SPIE 6803, Stereoscopic Displays and Applications XIX, 68031K, pp. 1--8, 2008.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1666808</ref_obj_id>
				<ref_obj_pid>1666778</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Masahiko Yoda, Akifumi Momose, and Kazuhisa Yanaka: Moving integral photography using a common digital photo frame and fly's eye lens, SIGGRAPH ASIA Posters 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kazuhisa Yanaka: Real-time rendering for integral photography that uses extended fractional view, Proc. SPIE 7237, Stereoscopic Displays and Applications XX, 723723, pp. 1--8, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPU Accelerated Interactive Integral Photography System Using Extended Fractional View Method Kazuhisa 
Yanaka and Sho Kimura Kanagawa Institute of Technology 8  C:\Users\admin\Desktop\....jpg C:\Users\admin\Desktop\....\image\IP_image.png 
  Fly s eye lens   8    Gamepad GPU  Figure 3 GPU Accelerated interactive integral photography 
system Figure 2 Multi-viewpoint rendering Figure 1 Integer view vs. fractional view  Abstract We developed 
a novel system of interactively displaying 3D images, in which integral photography images with the method 
of extended fractional views both with horizontal and vertical parallax could be rendered much faster 
by using a graphics processing unit (GPU). Therefore, displayed auto-stereoscopic 3D computer graphics 
(CG) objects could be moved very smoothly with a gamepad. 1 Motivation Integral photography (IP) is 
an ideal system to display 3D images because not only horizontal but also vertical parallax can be obtained 
without having to wear special glasses. The simplest IP system can be made by placing a fly s eye lens 
on a liquid crystal display (LCD). The pitch of a fly s eye lens in the past strictly had to be an integer 
multiple of the pixel pitch of the LCD, as shown in Figure 1 (a). However, this restriction was removed 
in the extended fractional view (EFV) method [1] shown in Figure 1 (b). Therefore, almost unrestricted 
combinations of fly s eye lenses and LCDs became possible, both of which are inexpensive ready-made products. 
Another advantage of EFV is that the quality of auto-stereoscopic images is considerably higher even 
if relatively low-resolution LCDs are used. Therefore, EFV can be used, for example, for auto-stereoscopic 
CG animation using digital photo-frames [2] where real-time rendering is not necessary. It has been technologically 
difficult to apply EFV to real-time interactions, on the other hand, because only very low frame rates 
could be achieved because numerous computations were required [3]. 2 Method We solved this problem by 
introducing GPU processing. Although GPUs have the potential of tremendously accelerating processing 
speed, we must be careful because they have basically been designed for embarrassingly parallel problems. 
The multi-viewpoint rendering in Figure 2 can suitably be applied to GPUs, but existing algorithms being 
used to synthesize IP images cannot be applied because the processing of pixels is dependent on other 
pixels. Therefore, we developed a new algorithm that was more suitable for GPUs, in which any pixel position 
was mapped into an equivalent point within a rectangle near the origin, by making use of the periodicity 
of a fly s eye lens. In addition, the 8 × 8 images captured from cameras in Figure 2 were connected together 
to form a single image to avoid restrictions where a maximum of 16 images could be read by the shader. 
 e-mail: yanaka@ic.kanagawa-it.ac.jp   C:\Users\yanaka\Pictures\Documents\SIGGRAPH2013\....png  
    Figure 4 Frame rate vs. number of characters  3 Experimental Results Figure 3 has a photograph 
of the experimental system that consisted of a high-resolution LCD (IBM T221 and 3840×2400 pixels), a 
fly s eye lens (Fresnel Technologies No. 360), and a PC with an upper midrange GPU (GeForce GT660Ti which 
had 1344 CUDA cores). More than 60 fps were secured even with large IP images. The relation between the 
number of characters and the frame rate obtained by one of three kinds of GPUs is plotted in Figure 4. 
Here, 512 × 512 LCD pixels were assigned to an IP image, and a character called CatBatDeamon had 956 
polygons. The processing speed was sufficiently fast to animate characters quite smoothly, provided that 
there were not too many characters even when the GPU (6 EUs) embedded in the CPU was used. 4 Conclusion 
Real-time processing became possible by introducing a GPU, and the rendering speed of the IP images improved 
sharply. Generally speaking, the higher the performance of a GPU is, the smoother the motion of an object 
becomes, and this becomes more suitable for applications such as games and virtual reality systems that 
require excellent image quality and rigorous real-time processing. References [1] Kazuhisa Yanaka: Integral 
photography using hexagonal fly's eye lens and fractional view, Proc. SPIE 6803, Stereoscopic Displays 
and Applications XIX, 68031K, pp. 1 8, 2008. [2] Masahiko Yoda, Akifumi Momose, and Kazuhisa Yanaka: 
Moving integral photography using a common digital photo frame and fly's eye lens, SIGGRAPH ASIA Posters 
2009. [3] Kazuhisa Yanaka: Real-time rendering for integral photography that uses extended fractional 
view, Proc. SPIE 7237, Stereoscopic Displays and Applications XX, 723723, pp. 1 8, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503433</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[High-speed parallel processing bio-microscope based on integral imaging]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503433</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503433</url>
		<abstract>
			<par><![CDATA[<p>Recently, fluorescence and confocal type bio-microscopes have been widely used in variety of fields, including hospitals and research institutes in the observation and inspection for the cells, blood and skin tissues. The most challenging issues of these bio-microscopes are resolution improvement and real-time observation. Another issue is how to acquire and display three-dimensional (3D) information of the observation samples. In the real-time observation, 3D image generating time is needed for the constant optical slices. Lim et al.[Lim et. al, 2009] proposed bio-microscope which generating the 3D information of object; however could not provide in real-time. In this study, we proposed an improved 3D bio-microscope which is applied realtime generating 3D information of the observation sample using Open-computer-language (OpenCL) parallel processing and optical reconstruction using integral imaging technique with simultaneously improved resolution.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190719</person_id>
				<author_profile_id><![CDATA[81550850556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ji-Seong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jeong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chungbuk National University in Cheongju, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[farland83@chungbuk.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190720</person_id>
				<author_profile_id><![CDATA[82458760457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Munkh-Uchral]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Erdenebat]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chungbuk National University in Cheongju, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[uchka@osp.chungbuk.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190721</person_id>
				<author_profile_id><![CDATA[82459259457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Young-Tae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chungbuk National University in Cheongju, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ytlim@osp.chungbuk.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190722</person_id>
				<author_profile_id><![CDATA[82458764857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ki-Chul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kwon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chungbuk National University in Cheongju, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kwon@osp.chungbuk.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190723</person_id>
				<author_profile_id><![CDATA[81488657959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Nam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chungbuk National University in Cheongju, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[namkim@chungbuk.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190724</person_id>
				<author_profile_id><![CDATA[81384614789]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kwan-Hee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chungbuk National University in Cheongju, South Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[khyoo@chungbuk.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lim, Y.-T., Park, J.-H., Kwon, K.-C., and Kim, N. 2009. Three-Dimensional Display of Microscopic Specimen using Integral Imaging Microscope and Display. <i>The Journal of Korea Information and Communication Society</i> 34, 11, 1319--1327]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology</funding_agency>
			<grant_numbers>
				<grant_number>2011-0025849</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503434</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Learning based compression for real-time rendering of surface light fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503434</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503434</url>
		<abstract>
			<par><![CDATA[<p>Photo-realistic rendering in real-time is a key challenge in computer graphics. A number of techniques where the light transport in a scene is pre-computed, compressed and used for real-time image synthesis have been proposed, e.g. [Ramamoorthi 2009]. We extend on this idea and present a technique where the radiance distribution in a scene, including arbitrarily complex materials and light sources, is pre-computed and stored as surface light fields (SLF) at each surface. An SLF describes the full appearance of each surface in a scene as a 4D function over the spatial and angular domains. An SLF is a complex data set with a large memory footprint often in the order of several GB per object in the scene. The key contribution in this work is a novel approach for compression of SLFs enabling real-time rendering of complex scenes. Our learning-based compression technique is based on exemplar orthogonal bases (EOB) [Gurumoorthy et al. 2010], and trains a compact dictionary of full-rank orthogonal basis pairs with sparse coefficients. Our results outperform the widely used CPCA method [Miandji et al. 2011] in terms of storage cost, visual quality and rendering speed. Compared to PRT techniques for real-time global illumination, our approach is limited to static scenes but can represent high frequency materials and any type of light source in a unified framework.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190725</person_id>
				<author_profile_id><![CDATA[81421603169]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ehsan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miandji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190726</person_id>
				<author_profile_id><![CDATA[81488647582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kronander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190727</person_id>
				<author_profile_id><![CDATA[81100120690]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jonas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Unger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Link&#246;ping University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1737467</ref_obj_id>
				<ref_obj_pid>1737463</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gurumoorthy, K., Rajwade, A., Banerjee, A., and Rangarajan, A. 2010. A method for compact image representation using sparse matrix and tensor projections onto exemplar orthonormal bases. <i>Image Processing, IEEE Transactions on 19</i>, 2 (feb.) 322--334.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Miandji, E., Kronander, J., and Unger, J. 2011. Geometry independent surface light fields for real time rendering of precomputed global illumination. In <i>SIGRAD 2011</i>, Linkping University Electronic Press, 27--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1576264</ref_obj_id>
				<ref_obj_pid>1576263</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi R. 2009. Precomputation-based rendering. <i>Found. Trends. Comput. Graph. Vis. 3</i>, 4 (Apr.), 281--369.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Learning Based Compression for Real-Time Rendering of Surface Light Fields Ehsan Miandji Joel Kronander 
Jonas Unger C-Research, Link ¨ oping University Figure 1: An illustration of surface light .elds along 
with a visualization of clusters is shown in (a). A scene rendererd using our approach is shown in (b). 
Close ups of the teapot rendered using CPCA and our method, both with the same storage cost, are shown 
in (c) and (d), respectively. The scene was rendered at 52 fps using our method and 35 fps using CPCA. 
1 Introduction Photo-realistic rendering in real-time is a key challenge in com­puter graphics. A number 
of techniques where the light transport in a scene is pre-computed, compressed and used for real-time 
image synthesis have been proposed, e.g. [Ramamoorthi 2009]. We ex­ tend on this idea and present a technique 
where the radiance distri­bution in a scene, including arbitrarily complex materials and light sources, 
is pre-computed and stored as surface light .elds (SLF) at each surface. An SLF describes the full appearance 
of each surface in a scene as a 4D function over the spatial and angular domains. An SLF is a complex 
data set with a large memory footprint often in the order of several GB per object in the scene. The 
key contribution in this work is a novel approach for compression of SLFs enabling real-time rendering 
of complex scenes. Our learning-based com­pression technique is based on exemplar orthogonal bases (EOB) 
[Gurumoorthy et al. 2010], and trains a compact dictionary of full­ rank orthogonal basis pairs with 
sparse coef.cients. Our results outperform the widely used CPCA method [Miandji et al. 2011] in terms 
of storage cost, visual quality and rendering speed. Compared to PRT techniques for real-time global 
illumination, our approach is limited to static scenes but can represent high frequency materials and 
any type of light source in a uni.ed framework. 2 Learning based SLF compression As described in Figure 
1a, an SLF can be thought of as a set hemi­ spherical radiance distribution functions (HRDF) regularly 
dis­tributed over the surface. This can be described as a 3rd-order tensor F . RN×m1×m2 , where N is 
the number of spatial sam­ples in (u, v)-space, and m1 × m2 is the (angular) resolution of the HRDFs. 
EOB is based on training a set of K « N full-rank orthogonal basis pairs (exemplars) such that projecting 
each data point onto one basis pair would lead to the most sparse coef.cient matrix while minimising 
the L2-error. This can be expressed by minimizing the following energy function: NK K K 2 E({Ua, Va, 
Sia, Mia}) = MiaiHi - UaSiaVa T ii=1 a=1 subject to K Ua T Ua = Va T Va = I , .a, iSiai0 = T and Mia 
= 1, .a, a . Rm1×m2 where matrices Hi correspond to HRDFs; T repre­. Rm1×m2 sents the sparsity of the 
coef.cient matrix Sia and M . RN×K is a binary matrix associating each HRDF to its cor­. Rm1×m1 responding 
exemplar pair (Ua and Va . Rm2×m2 ). Using an iterative algorithm this can be ef.ciently minimized [Gu­ 
rumoorthy et al. 2010], resulting in a set of exemplars (basis pairs) Ua and Va for a = 1 . . . K . In 
order to best exploit the coher­ence in data, HRDFs are represented as matrices instead of vectors. To 
assist the convergence of EOB, we do a pre-clustering using K-Means to group similar HRDFs. Hence a different 
set of exemplars is trained for each cluster. Although this approach adds to memory footprint, it will 
improve accuracy and convergence of EOB. For training the exemplars, we randomly select 20 - 70% of the 
HRDFs with a probability distribution proportional to iHii2. The value of T is .xed during training. 
Given trained exemplars, nul­lifying m1m2 - T coef.cients from the optimal projection matrix Sia = Ua 
T HiVa results in a sparse coef.cient matrix. This is done in a greedy manner by selecting an exemplar 
pair and incrementally adding the most signi.cant coef.cients until the reconstruction er­ror falls below 
a threshold. Therefore the sparsity is different for each HRDF during the testing phase. The non-zero 
elements of Si are stored as a vector of triplets with elements Sit, t = 1 . . . Ti con­taining indices 
and corresponding values. Hence our method pro­duces a very compact dictionary with sparse coef.cients 
adapted to the given SLF function. 3 Rendering and results To reconstruct outgoing radiance at a point 
and along a view ray, we .rst .nd the corresponding cluster c and exemplar pair a for that point (denoted 
Uca and Vca). Due to uniform spatial sampling this can be done in O(1). Our reconstruction method can 
be formulated as follows: Ti K Hi(.1, .2) = Uca(Sit(1), .1) × Sit(3) × Vca(Sit(2), .2) t=1 where Hi(.1, 
.2) is an element in the HRDF to be reconstructed. Figure 1b shows a scene rendered in real-time (52 
fps using a Geforce 460) using our method including global illumination ef­fects not possible to render 
with other real-time methods. This project was funded by the Swedish Foundation for Strategic Research 
through grant IIS11-0081, and Link ¨ oping University Cen­ter for Industrial Information Technology. 
 References GURU M OO RT H Y, K., RA J WA D E, A., BA N E R J E E, A., AN D RA N G AR A JAN , A. 2010. 
A method for compact image representation using sparse matrix and tensor projec­tions onto exemplar orthonormal 
bases. Image Processing, IEEE Transactions on 19, 2 (feb.), 322 334. MI AN D J I , E., KRONA N D E R 
, J., A N D UN G E R , J. 2011. Geometry independent surface light .elds for real time rendering of precomputed 
global illumination. In SIGRAD 2011, Linkping University Electronic Press, 27 34. RA MA M O O RT H I 
, R. 2009. Precomputation-based rendering. Found. Trends. Comput. Graph. Vis. 3, 4 (Apr.), 281 369. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Linkoping University Center for Industrial Information Technology</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Swedish Foundation for Strategic Research</funding_agency>
			<grant_numbers>
				<grant_number>IIS11-0081</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503435</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Manual NC plotter]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503435</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503435</url>
		<abstract>
			<par><![CDATA[<p>Even in todays world where we can touch information very intuitively with a gadget such as an iPhone, it is still interesting to know, actually, how bits are translated to physical matters and vice versa. Printers, for instance, print digital images on actual paper by moving its motors according to electrical signals. As to digital fabrication technologies, only 3D printers and laser cutters are often in limelight at the moment but this fields objective is how things can be digital, or how to translate bits into atoms and atoms into bits.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190728</person_id>
				<author_profile_id><![CDATA[82459327457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190729</person_id>
				<author_profile_id><![CDATA[82458989357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1663349</ref_obj_id>
				<ref_obj_pid>1663347</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bresenham, J. E. 1965. Algorithm for computer control of a digital plotter. <i>IBM Syst. J. 4</i>, 1 (Mar.), 25--30.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Manual NC Plotter Yuichi Hirose* Keio University 1 Introduction Even in todays world where we can touch 
information very intu­itively with a gadget such as an iPhone, it is still interesting to know, actually, 
how bits are translated to physical matters and vice versa. Printers, for instance, print digital images 
on actual paper by mov­ing its motors according to electrical signals. As to digital fabri­cation technologies, 
only 3D printers and laser cutters are often in limelight at the moment but this .elds objective is how 
things can be digital, or how to translate bits into atoms and atoms into bits. In this research, we 
developed a plotter (Figure 1) which is com­puter numerically controlled but you need to make it work 
manually and which works without electricity but with gears. We also devel­oped an application software 
(Figure 2) which generates vector data of racks (straight gears) according to your arbitrary drawings. 
You laser-cut the racks and insert them into the plotter and rotate a han­dle of a gear, then the plotter 
draws your drawing on a paper. This machine helps us to understand easily how information is translated 
into bits (data of 0/1) and how bits are translated into real things, and what digital actually means. 
For instance, how an oblique line can be described by a set of 0 and 1 was a problem when we de­veloped 
the application, and it probably also was a problem in the early period of computer graphics. This manual 
NC plotter may be able to make bits more tangible than an iPhone. Figure 1: (a) The manual NC plotter 
(b) A plotted example 2 System We developed an application which generates vector data of racks according 
to your drawings. (Figure 2) Racks, which are straight *e-mail: {yhirose, htanaka}@sfc.keio.ac.jp Hiroya 
Tanaka* Keio University gears, moves in a horizontal direction by the rotation of a pinion (a circular 
gear) and vice versa. The rack which our application generates has parts with gear teeth (teeth parts) 
and parts without teeth (null parts), which represent 1 and 0, respectively. When a teeth part is inserted, 
the pinion rotates then the pen moves, while the pinion and the pen do not move when a null part is inserted. 
Our application generates 4 types of racks, which are X-axis plus direction, X-axis minus direction, 
Y-axis plus direction and Y-axis minus direction. After laser-cutting them, you need to stick them together. 
(Figure 3a) The insertion slot of the plotter has 4 pinions, each of which is engaged with each of 4 
types of racks, then conveys a power to the pen to move in each direction.(Figure 3b)  Figure 3: . (a) 
A laser-cut and stuck set of 4 types of racks (b) The insertion slot has 4 pinions to engage with the 
racks 3 Algorithm We used the Bresenham line algorithm [Bresenham 1965] to ap­proximate oblique lines. 
The head of the plotter can move only vertically and horizontally so oblique lines should be approximated 
with sets of short lines in the directions of X-axis and Y-axis. The Bresenham line algorithm was developed 
for digital plotting and now is commonly used for describing lines with pixels on a com­puter screen. 
 References BR E S E N H AM, J. E. 1965. Algorithm for computer control of a digital plotter. IBM Syst. 
J. 4, 1 (Mar.), 25 30. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503436</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Multifrequency time of flight in the context of transient renderings]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503436</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503436</url>
		<abstract>
			<par><![CDATA[<p>The emergence of commercial time of flight (ToF) cameras for realtime depth images has motivated extensive study of exploitation of ToF information. In principle, a ToF camera is an active sensor that emits an amplitude modulated near-infrared (NIR) signal, which illuminates a given scene. The per-pixel phase difference of the modulation between reflected light and a reference signal determines the path length, and hence a depth map, of the scene.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190730</person_id>
				<author_profile_id><![CDATA[81464665244]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ayush]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bhandari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ayush@mit.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190731</person_id>
				<author_profile_id><![CDATA[82459043457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Achuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kadambi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190732</person_id>
				<author_profile_id><![CDATA[82458847657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Refael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Whyte]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waikato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190733</person_id>
				<author_profile_id><![CDATA[82459127257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Streeter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waikato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190734</person_id>
				<author_profile_id><![CDATA[81504688659]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Christopher]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Barsi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190735</person_id>
				<author_profile_id><![CDATA[81309482324]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorrington]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waikato]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190736</person_id>
				<author_profile_id><![CDATA[81548005482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Godbaz, J. P., Cree, M. J., and Dorrington, A. A. 2012. Closed-form inverses for the mixed pixel/multipath interference problem in amcw lidar. In <i>IS&T/SPIE Electronic Imaging</i>, International Society for Optics and Photonics, 829618--829618.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kolb, A., Barth, E., Koch, R., and Larsen, R. 2009. Time-of-flight sensors in computer graphics. In <i>Proc. Eurographics (State-of-the-Art Report)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multifrequency Time of Flight in the Context of Transient Renderings Ayush Bhandari* Achuta Kadambi 
Refael Whyte Lee Streeter Christopher Barsi MIT Media Lab MIT Media Lab University of Waikato University 
of Waikato MIT Media Lab Adrian Dorrington Ramesh Raskar University of Waikato MIT Media Lab  Figure 
1: Dragon behind Transparency. In this scene, a dragon model (leftmost) exhibiting specularities and 
occlusions is placed behind a screen. Using our time resolved renderings, we can capture different time 
slots of light washing over the model. The reconstructions (bottom right) are exact in the noiseless 
case. Figure 2: We compare our approach to existing SVD-based methods on the K = 3 bounce cases. The 
norm is taken with respect to the original component from the renderings. Note that the Matrix Pencil 
at 25 dB SNR is comparable to the existing approach at 40 dB SNR. 1 Introduction The emergence of commercial 
time of .ight (ToF) cameras for re­altime depth images has motivated extensive study of exploitation 
of ToF information. In principle, a ToF camera is an active sen­sor that emits an amplitude modulated 
near-infrared (NIR) signal, which illuminates a given scene. The per-pixel phase difference of the modulation 
between re.ected light and a reference signal deter­mines the path length, and hence a depth map, of 
the scene. Although ToF ranging has several bene.ts over other methods, such as stereo, structured light, 
and laser scanning, it still suffers from severe systematic errors [Kolb et al. 2009]. In particular, 
the ToF principle assumes that the light travels directly between the camera and the object. However, 
because the sensor illuminates the entire scene, multiple paths can contribute to a measurement. This 
multi­path interference (MPI) corrupts the depth measurement, which comprises a nonlinear combination 
of the components. 2 Contributions We establish a new method for unmixing component phases and returning 
the corresponding amplitudes. Our approach draws upon several principles from signal processing to per­form 
per-pixel unmixing of a measured range image. We in­ *e-mail:ayush@mit.edu crease the dimensionality 
of our measurement by acquiring depth maps at different modulation frequencies. By disentangling phases 
and amplitudes we are able to in­crease the accuracy of time of .ight range maps.  Our approach is able 
to decompose a scene by characterizing light paths in terms of bounces and pathlengths (Figure 1, 2). 
 We analyze existing hardware and demonstrate the relation­ship between amplitude, phase and frequency 
for mixed pix­els.  We demonstrate comparisons to the state of the art, e.g., [Godbaz et al. 2012] (Figure 
2) and provide a rendering dataset for experi­ ments. We envision application scenarios in range imaging 
of par­tially occluded objects (Figure 1), corners, and imaging outside the .eld of view. References 
GO D BAZ, J. P., CRE E , M. J., A ND DO R RI N G TO N, A. A. 2012. Closed-form inverses for the mixed 
pixel/multipath interference problem in amcw lidar. In IS&#38;T/SPIE Electronic Imaging, Inter­national 
Society for Optics and Photonics, 829618 829618. KO LB , A., BA RTH, E., KO CH , R., A N D LA RSE N, 
R. 2009. Time­of-.ight sensors in computer graphics. In Proc. Eurographics (State-of-the-Art Report). 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503437</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Polka dot]]></title>
		<subtitle><![CDATA[the garden of water spirits]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503437</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503437</url>
		<abstract>
			<par><![CDATA[<p>Physical and tangible representations of information have provided users with intuitive interactions in which users can control information through tangible controls using their hands. In these techniques, flexible materials have often been utilized. For example, clay has been used for an intuitive modeling tool which senses the shape of the clay and updates its 3D model data [Piper et al. 2002]. This example enables users to create 3D models without knowledge of computational methods for constructing 3D models. Though these tangible representations of information accept user input, they cannot provide bi-directional physical interactions since their physical properties are not controlled by computers. Therefore, it is difficult to represent dynamic changes of information using physical properties such as motion, size, and color. These tangible user interfaces employ static materials.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190737</person_id>
				<author_profile_id><![CDATA[82458957557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tamasui@hi-mail.ise.eng.osaka-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190738</person_id>
				<author_profile_id><![CDATA[82459301957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kazuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190739</person_id>
				<author_profile_id><![CDATA[81100329599]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yuichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Itoh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190740</person_id>
				<author_profile_id><![CDATA[81442608795]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kazuyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190741</person_id>
				<author_profile_id><![CDATA[82458909157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Taku]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190742</person_id>
				<author_profile_id><![CDATA[82458689457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Kosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Osaka University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>503439</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Piper B., Ratti C., and Ishii H. 2002. Illuminating Clay: A 3-D Tangible Interface for Landscape Analysis. <i>in Proc. of CHI '02</i>. pp. 355--362.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Polka Dot The Garden of Water Spirits Yohei KOJIMA* Yuichi ITOH* Taku FUJIMOTO* Kazuma AOYAMA* Kazuyuki 
FUJITA* Kosuke NAKAJIMA* Graduate School of Information Science and Technology, Osaka University   
Fig. 1 Polka Dot  1. Introduction Physical and tangible representations of information have pro­ vided 
users with intuitive interactions in which users can control information through tangible controls using 
their hands. In these techniques, flexible materials have often been utilized. For example, clay has 
been used for an intuitive modeling tool which senses the shape of the clay and updates its 3D model 
data [Piper et al. 2002]. This example enables users to create 3D models without knowledge of computational 
methods for constructing 3D models. Though these tangible representations of information accept user 
input, they cannot provide bi-directional physical interactions since their physical properties are not 
controlled by computers. Therefore, it is difficult to represent dynamic changes of information using 
physical properties such as motion, size, and color. These tangible user interfaces employ static materials. 
Thus, we focus on water as a more flexible material that can easily change position, motion, shape, size, 
and color. We propose a fabric tabletop display called Polka Dot, which can control various properties 
of water drops on its surface. 2. Polka Dot Polka Dot controls the properties of water drops including 
their position, motion, shape, size, and color. The system can represent dynamic changes of information 
and expand interaction. Figure 2 shows the system overview. The system consists of a large piece of water-repellent 
fabric, an array of actuators, syringes, and a projector, camera, and computer. Fabric composes the surface 
of this system. An array of actuators and corresponding power supply lines are set under the fabric. 
The projector is connected to the computer and put behind the actuators and the camera is fixed in a 
location that looks down on the surface of the fabric. To display the changing of position and motion 
of the water drops on the fabric, the system adjusts the unevenness of the fabric using actuators. Figure 
3 shows the mechanism that moves the water drops. Since the water drops on the water-repellent fabric 
easily slide to the lower area by gravity, the system can move the water drops by pushing and pulling 
the fabric and changing the height using an underlying array of actuators. To control many actuators 
without delay and to keep a simple configuration even   Fig. 3 The Mechanism for controlling the position 
of the water drops when enlarging the system, we employ simple optical communication between the computer 
and each actuator using the projector under the actuators. Each actuator is equipped with an optical 
sensor and a microcontroller. When the projector lights the actuator, the actuator pulls the fabric down. 
Then the area around the lighted actuator becomes concave. On the other hand, the actuator that is not 
lighted pushes the fabric up. Thus, all actuators communicate with the computer individually and simultaneously. 
This mechanism makes the system scalable. The size of the water drops can also be changed by injecting 
(or extracting) the water on the fabric using a syringe with a thin needle stuck to the fabric from the 
back side. Small holes caused by the needles do not interfere with the water-repellent property of the 
fabric. Additionally, the system adds color to the water drops by injecting colored water using the syringes 
or putting water-soluble paint on the fabric. These novel characteristics expand methods for expression 
in the physical world and open up the rich channel of communication between the user and computer. 3. 
Prototype We investigated the minimum size of water drops able to be controlled by this configuration. 
We measured the minimum angles of water-repellent fabric to make water drops slide by gravity. We put 
various sizes of water drops (3.0 mm, 5.0 mm, 10 mm, and 15 mm in diameter) on a rigid plate covered 
with water­ repellent fabric. While we tilted the plate slowly, we measured the minimum angle where the 
drops began to slide. As a result, the minimum angle for each water drop was 17, 6.1, 4.1, or 2.7 degree 
in order. The prototype actuators that push 10 mm up and are arranged 50 mm apart can make a slope at 
Tan-1 (0.2) = 11.3 degree. Since 11.3-degree angle is bigger than the minimum angle for 5.0-mm diameter 
drops and smaller than that for 3.0-mm diameter drops, the prototype can control 5-mm diameter water 
drops. Furthermore, the same configuration has a potential to move smaller size of water drops when using 
smaller actuators and denser arrangement of them. This improvement enables richer physical expressions 
of information. References PIPER B., RATTI C., AND ISHII H. 2002. Illuminating Clay: A 3-D Tangible 
Interface for Landscape Analysis. in Proc. of CHI 02. pp. 355-362. Permission to make digital or hard 
copies of part or all of this work for personal or classroom use isgranted without fee provided that 
copies are not made or distributed for commercial advantage and that copies bear this notice and the 
full citation on the first page. Copyrights for third-party components of thiswork must be honored. For 
all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503438</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Practical 3D+2D displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503438</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503438</url>
		<abstract>
			<par><![CDATA[<p>High-end televisions, monitors and gaming laptop screens are shifting from 2D to 3D. Most commercially available 3D displays show stereoscopic images to viewers wearing special glasses, while showing incomprehensible ghosted images to viewers without glasses. It is not always desirable to require that all viewers wear stereo glasses. They may cause flickering, interfere with other activities, or be prohibitively expensive.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190743</person_id>
				<author_profile_id><![CDATA[82458869857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jingliu@soe.ucsc.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190744</person_id>
				<author_profile_id><![CDATA[81100603625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[davis@soe.ucsc.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2487229</ref_obj_id>
				<ref_obj_pid>2487228</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Scher, S., Liu, J., Vaish, R., Gunawardane, P., and Davis, J. 2013. 3D+2D TV: 3D Displays With No Ghosting for Viewers Without Glasses. <i>ACM Trans. on Graphics (@ SIGGRAPH13)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Practical 3D+2D Displays Jing Liu,* James Davis University of California Santa Cruz  (a) Left Eye Image 
(b) Right Eye Image (c) Standard 3D Display (d) Our 3D+2D Display Figure 1: A typical glasses-based 3D 
display shows a different image to each eye of viewers wearing stereo glasses, visible through the glasses: 
(a) Left eye image is labeled with the letter L. (b) Right eye image is labeled with the letter R. (c) 
On a standard 3D display, viewers without glasses see both images superimposed, visible directly on the 
screen at the top of the .gure. (d) Our 3D+2D display likewise shows a different image to each eye of 
viewers wearing stereo glasses, but shows only one of these images to those without glasses, removing 
the ghosted double-image. 1 3D+2D Displays without Ghosting High-end televisions, monitors and gaming 
laptop screens are shifting from 2D to 3D. Most commercially available 3D dis­plays show stereoscopic 
images to viewers wearing special glasses, while showing incomprehensible ghosted images to viewers with­out 
glasses. It is not always desirable to require that all viewers wear stereo glasses. They may cause .ickering, 
interfere with other activities, or be prohibitively expensive. Our method enables stereoscopic 3D displays 
to be watched by 3D and 2D viewers simultaneously. Ghosted images that are observed on traditional 3D 
displays can be eliminated for viewers without stereoscopic glasses while 3D perception is preserved 
for viewers with glasses. We accomplish simultaneous viewing of 3D and 2D images by replacing the pair 
of images [Left, Right] with a triplet [Left, Right, Neither]. Those wearing glasses see the Neither 
image with neither eye; only those without stereo glasses can see it. The Neither image is the negative 
of the Right image so that they sum to a grey image when superimposed, leaving only the Left image visible 
to 2D viewers. Unfortunately, this raises the minimum black level of the display for viewers without 
stereo glasses, decreasing the contrast ratio. This can be mitigated by reducing the brightness level 
of the Right im­age, aR, while maintaining the Left image at full brightness. Reducing aR improves the 
contrast ratio for 2D viewers. Howev­er if aR is decreased too much, the 3D experience of viewers with 
glasses will deteriorate. We conducted experiments, on both view­ers with and without glasses, identifying 
the acceptable range of aR to be 20% = aR = 60%. Further details validating the design for both 3D and 
2D viewers can be found in [Scher et al. 2013]. 2 Practical Enhancements Ghosting-control weight Although 
viewers prefer contrast loss to a fully ghosted image, the contrast loss is also undesirable. Thus we 
investigated the optimal trade-off between contrast loss and ghost­ing for 2D viewers. *jingliu@soe.ucsc.edu 
davis@soe.ucsc.edu The image that 2D viewers see can be represented as: [Lef t] + aR * [Right] + w * 
[N either] When w equals aR, no ghosting is visible. Decreasing w improves the contrast ratio, at the 
cost of some visible ghosting. Varying w will not affect 3D viewers, but provides the bene.ts of searching 
in a larger space for a satisfying 2D image. Initial studies were conducted by letting users modify w 
interac­tively. When aR = 40% our test subjects preferred w = 60%, an intermediate level of both ghosting 
and contrast. (a) (b) Figure 2: (a) In the three frame method, a third channel is inserted to display 
the [Neither] image. (b) A two frame approximation can be achieved by adding the [Neither] image to the 
[Left] image. Two frame approximation The three frame method is ideal, but can not be implemented without 
hardware modi.cation. Figure 2 shows an approximation for standard 2-frame displays. The [Nei­ther] image 
is simply added to the [Left] image slot. This introduces negative crosstalk into the [Left] image. However, 
our display, like many existing stereo displays, has noticeable pos­itive crosstalk due to hardware limitations. 
Since the negative and positive crosstalk cancel, this results in total crosstalk to the left eye only 
a little greater than the standard level. Test users have found the 3D image quality acceptable. References 
SCHER , S., LI U, J., VA I S H , R., GU NAWA R DA N E , P., A N D DAV I S, J. 2013. 3D+2D TV: 3D Displays 
With No Ghosting for Viewers Without Glasses. ACM Trans. on Graphics (@ SIGGRAPH13). Permission to make 
digital or hard copies of part or all of this work for personal or classroom use isgranted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503439</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Reflective, deformable, colloidal display]]></title>
		<subtitle><![CDATA[a waterfall-based colloidal membrane using focused ultrasonic waves]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503439</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503439</url>
		<abstract>
			<par><![CDATA[<p>Our previous research[1] indicates that we can control the reflectance, transparency, and the view angle of the colloidal membrane using ultrasonic waves. The problem was the membrane was rather easy to break. However, in order to widen the view angle and extend the research, the screen is required to withstand powerful ultrasonic waves.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190745</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ochyai@me.com]]></email_address>
			</au>
			<au>
				<person_id>P4190746</person_id>
				<author_profile_id><![CDATA[82459164857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[Oyama Takayuki]]></middle_name>
				<last_name><![CDATA[Hoshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nagoya Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190747</person_id>
				<author_profile_id><![CDATA[81100008564]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rekimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SONY CSL]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2342907</ref_obj_id>
				<ref_obj_pid>2342896</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Y. Ochiai et al., A colloidal display: membrane screen that combines transparency, BRDF and 3D volume. In ACM SIGGRAPH 2012 Emerging Technologies, Article 2, 2012]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1908041</ref_obj_id>
				<ref_obj_pid>1907654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Hoshi et al., Noncontact Tactile Display Based on Radiation Pressure of Airborne Ultrasound, IEEE Transactions on Haptics, vol. 3, no. 3, pp. 155--165, 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Re.ective, Deformable, Colloidal Display: a waterfall-based colloidal membrane using focused ultrasonic 
waves. Yoichi Ochiai* Alexis Oyama Takayuki Hoshi** Jun Rekimoto* ****the University of Tokyo **Nagoya 
Institute of Technology ***SONY CSL Figure 1: (top-left).exible screen. (top-center)Omnidirectional 
display. (top-right)waterfall frame. (bottom-left) system overview. (bottom-center) re.ective material 
display. (bottom-right) deformable display 1. Introduction Our previous research[1] indicates that we 
can controlthe re.ectance, transparency, and the view angle of thecolloidal membrane using ultrasonic 
waves. The prob­lem was the membrane was rather easy to break. How­ever, in order to widen the view angle 
and extend theresearch, the screen is required to withstand powerfulultrasonic waves. In this research, 
we developed a new colloidal screen using the waterfall concept. This system can now supply the colloidal 
solution continuously to the colloidal mem­brane resulting in the extension of the membrane's life with 
high stability against the powerful ultrasonic waves. In addition to this system, we are using a different 
ultra­sonic speaker which can be focused[2]. Using thesecomponents, we are able to display different 
kinds of materials, create a deformable screen, and widen theview angle. This research opens up a new 
.eld in .exi­ble display.  2. Design Our system has 4 components, the .rst is the water tank with the 
colloidal solution, the second is the mem­brane frame that contains the water supply and the re­placing 
mechanism, the third is the focused ultrasonicspeaker, and the fourth is the projector. The setup of 
these components is displayed in Figure 1(bottom-left) and the system overview is displayed in Figure 
2. Thewater supply system(in Figure1 top-right) supplies thecolloidal solution from the water tank to 
the frame con­tinuously using a pump. If the .lm breaks, it will be re­placed by dipping into the water 
tank using a servo.  *email: ochyai@me.com 3. Application 3.1 Re.ective Material Display By switching 
the membrane's re.ectance in high fre­quency, it creates an optical illusion that displays realis­tic 
materials such as glitter, diffuse, and so on. Figure1(bottom-center) shows some of the materials we 
wereable to demonstrate. 3.2 Deformable Display When the ultrasonic waves are focused in one spot, 
aforce .eld is created. We developed a deformable screen by controlling the spatial position and the 
power of theforce .eld. The deformation can be controlled from 0 to 20mm with 1khz animation. Focused 
ultrasonic systemuses 40kHz. The result can be seen in Figure 1(bottom­right). 3.3 Omnidirectional Display 
 Previously[1], we were only able to control the viewangle for up to 40 degrees. We increased the range 
con­trol to up to 160 degrees by applying stronger ultrasonicwaves. This can be used as a multi-view 
angle screen by changing its re.ectance (Figure 1 top-center).  4. Conclusion &#38; Future Work In this 
research, we conquered the problem of soap .lm's stability using a waterfall based design, and up­graded 
the speakers to allow the focusing of the ultra­sonic waves. We have demonstrated some applicationsthat 
can be used with these new components. Becausethis cannot be easily scaled to a bigger membrane, our 
next potential goal is to conquer this problem and createa bigger colloidal screen with unique interactions. 
REFERENCES [1] Y. Ochiai et al., A colloidal display: membrane screen that combines transparency, BRDF 
and 3D volume. In ACM SIGGRAPH 2012 Emerging Technologies, Article 2, 2012 [2] T. Hoshi et al., Noncontact 
Tactile Display Based on Radia­tion Pressure of Airborne Ultrasound,IEEE Transactions on Haptics, vol. 
3, no. 3, pp. 155-165, 2010. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and thatcopies bear this notice and the full citation on the first page. Copyrights 
for third-party components of this work must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California.2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503440</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[SplineGrip]]></title>
		<subtitle><![CDATA[an eight degrees-of-freedom flexible haptic sculpting tool]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503440</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503440</url>
		<abstract>
			<par><![CDATA[<p>Haptic sculpting allows a designer to create virtual models with the aid of haptic feedback. Just as in real life sculpting, different tools are used to work the model into a desired shape. Several haptic sculpting tools appear in the literature. For example, [Gao 2006] proposes an ellipsoidal tool whose pose (position and orientation) is controlled by a six degrees-of-freedom (DOF) haptic device, and [McDonnell 2001] describes a set of modeling tools for virtual clay controlled by three DOF haptics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190748</person_id>
				<author_profile_id><![CDATA[81314484578]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pontus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olsson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Uppsala University, Uppsala, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pontus.olsson@it.uu.se]]></email_address>
			</au>
			<au>
				<person_id>P4190749</person_id>
				<author_profile_id><![CDATA[81508706086]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fredrik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nysj&#246;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Uppsala University, Uppsala, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190750</person_id>
				<author_profile_id><![CDATA[82459114557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Bj&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aneer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Independent Artist, Stockholm, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190751</person_id>
				<author_profile_id><![CDATA[81100006231]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Seipel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Uppsala University, Uppsala, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190752</person_id>
				<author_profile_id><![CDATA[81508687938]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ingrid]]></first_name>
				<middle_name><![CDATA[B.]]></middle_name>
				<last_name><![CDATA[Carlbom]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Uppsala University, Uppsala, Sweden]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1649914</ref_obj_id>
				<ref_obj_pid>1649591</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gao, Z, and Gibson I, 2006. Haptic Sculpting of MultiResolution B-spline Surfaces with Shaped Tools, <i>Computer Aided Design 38</i>, 661--676.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>364395</ref_obj_id>
				<ref_obj_pid>364338</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[McDonnell, KT, Qin H, and Wlodarczyk RA, 2001, Virtual Clay: A Real-time Haptics-based Sculpting System with Haptic Toolkits, <i>Proc. ACM Symp. Interact. 3D Graphics</i>, 179--190.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SplineGrip - An Eight Degrees-of-Freedom Flexible Haptic Sculpting Tool A3.JPG C2.JPG B2.JPG D2.JPG 
 Full System.JPG Pontus Olsson1,*, Fredrik Nysjö1, Björn Aneer2, Stefan Seipel1, Ingrid B. Carlbom1 
1Uppsala University, Uppsala, Sweden; 2Independent Artist, Stockholm, Sweden  Figure 1. SplineGrip. 
Examples of hand articulations and their corresponding virtual sculpting tools.   1 Introduction Haptic 
sculpting allows a designer to create virtual models with the aid of haptic feedback. Just as in real 
life sculpting, different tools are used to work the model into a desired shape. Several haptic sculpting 
tools appear in the literature. For example, [GAO 2006] proposes an ellipsoidal tool whose pose (position 
and orientation) is controlled by a six degrees-of-freedom (DOF) haptic device, and [MCDONNELL 2001] 
describes a set of modeling tools for virtual clay controlled by three DOF haptics. We propose a flexible 
haptic sculpting tool, SplineGrip, which senses the articulation and pose of the sculpting hand in eight 
DOF. The tool captures the hand articulation in two DOF, and uses a commercial haptic device that tracks 
the hand pose in six DOF, while simultaneously providing three DOF haptic feedback. The eight DOF input 
is mapped to the pose and shape of a virtual representation of a sculpting tool, offering versatile interaction 
with a virtual model. 2 Technical Approach We capture the two DOF hand articulation with two Flex resistive 
bend sensors (Figure 1, A, B) from Images SI Inc. mounted in two directions on a flexible plastic sheet 
(C) cut to fit the hand. The bend sensors have curvature dependent resistance, which we sample with an 
Arduino Leonardo board connected to a PC running a sculpting system. The sensors measure the plastic 
sheet curvature controlled by the thumb and by the middle and ring fingers, respectively. The user may 
stabilize the tool with the index and little fingers as shown in Figure 1. The plastic sheet is mounted 
on a Phantom Omni device (D) from Geomagic. SplineGrip controls the shape and pose of the virtual sculpting 
tool from the articulation and pose of the sculpting hand. When all fingers are straight, the virtual 
sculpting tool takes the shape of a line segment (I). By bending one sensor with the middle and ring 
fingers, the user changes the virtual tool curvature. By bending the other sensor with the thumb, the 
user changes the width of the virtual tool. A curvature increase at zero width turns the line into a 
spline (II), and a width increase at zero curvature creates a plane (III). By bending both sensors, the 
user may simultaneously control the curvature and width of a NURBS surface (IV). The user may toggle 
between convex and concave tools (IV a, IV b). 3 Sculpting System To evaluate SplineGrip, we implemented 
a simple sculpting system where the user starts with a block of material and uses the virtual sculpting 
tool to gradually remove material. The virtual model is represented by a voxel grid, with increasing 
voxel values towards the interior of the model (an approximate distance field), allowing extraction of 
an isosurface mesh for model surface visualization, and fast calculation of haptic contact forces. We 
calculate the force feedback by uniformly sampling the virtual tool surface and determining force directions 
and magnitudes from the corresponding positions in the model distance field. The contact forces are given 
the direction of the negative distance field gradients and are scaled by the distance field values, which 
yields repelling forces with magnitudes proportional to the tool penetration depth into the model. Around 
each sample point on the tool surface, we remove material by decreasing the voxel values using a pre-computed 
radial-basis-function kernel to retain the gradient at the model boundaries when material is removed. 
We control the material removal rate by the magnitude of the contact force; a higher user force yields 
a higher removal rate. 4 Discussion The tool described herein maps the pose and articulation of the 
hand to the pose, width and curvature of a virtual sculpting tool, but this mapping is only one of several 
possible mappings. Furthermore this tool is not restricted to volumetric models, but can equally well 
be used with surface models, such as those based on B-spline surfaces. We have illustrated the modeling 
tool with subtractive modeling, but this approach applies also to free-form additive modeling and physics-based 
modeling. References GAO, Z, AND GIBSON I, 2006. Haptic Sculpting of Multi-Resolution B-spline Surfaces 
with Shaped Tools, Computer Aided Design 38, 661-676. MCDONNELL, KT, QIN H, AND WLODARCZYK RA, 2001, 
Virtual Clay: A Real-time Haptics-based Sculpting System with Haptic Toolkits, Proc. ACM Symp. Interact. 
3D Graphics, 179-190. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503441</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Tracking magnetics above portable displays]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503441</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503441</url>
		<abstract>
			<par><![CDATA[<p>We present a system of the passive magnetic tangible designs that enables 3D tangible interactions above the portable displays. When a thin magnetic sensor grid is attached to the back of the display, the 3D position and partial 3D orientation of the magnetic tangibles, <i>GaussBits</i>, can be resolved by the proposed bi-polar magnetic field tracking technique. The occlusion-free properties of this tracking method also enrich the design of tangible interactions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190753</person_id>
				<author_profile_id><![CDATA[81549145756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Rong-Hao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University and Academia Sinica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190754</person_id>
				<author_profile_id><![CDATA[81416600159]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kai-Yin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190755</person_id>
				<author_profile_id><![CDATA[81556577256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Liwei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Academia Sinica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190756</person_id>
				<author_profile_id><![CDATA[81557660056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chuan-Xhyuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Peng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190757</person_id>
				<author_profile_id><![CDATA[82459154657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Mike]]></first_name>
				<middle_name><![CDATA[Y.]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190758</person_id>
				<author_profile_id><![CDATA[81100043933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Rung-Huei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Academia Sinica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190759</person_id>
				<author_profile_id><![CDATA[81100353194]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[De-Nian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Academia Sinica]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190760</person_id>
				<author_profile_id><![CDATA[81100108039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Bing-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2466185</ref_obj_id>
				<ref_obj_pid>2470654</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Liang, R.-H., Cheng, K.-Y., Chan, L., Peng, C.-X., Chen, M. Y., Liang, R.-H., Yang, D.-N., and Chen, B.-Y. 2013. GaussBits: Magnetic tangible bits for portable and occlusion-free near-surface tangible interactions. In <i>Proc. ACM CHI 2013</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tracking Magnetics above Portable Displays  Rong-Hao Liang*Kai-Yin Cheng* Liwei Chan Chuan-Xhyuan Peng 
Mike Y. Chen* Rung-Huei Liang De-Nian Yang Bing-Yu Chen* *National Taiwan University Academia Sinica 
National Taiwan University of Science and Technology Abstract We present a system of the passive magnetic 
tangible designs that enables 3D tangible interactions above the portable displays. When a thin magnetic 
sensor grid is attached to the back of the display, the 3D position and partial 3D orientation of the 
magnetic tangibles, GaussBits, can be resolved by the proposed bi-polar magnetic .eld tracking technique. 
The occlusion-free properties of this tracking method also enrich the design of tangible interactions. 
1 INTRODUCTION Tangible user interface has been developed on the prevalent mobile displays to improve 
the user experience1 . However, the interac­tions are con.ned to the 2D space because of the limitations 
of the capacitive-based sensing technology. Although traditional vision-2 and magnetic-based3 approaches 
can track the 3D interactions, the former usually suffer occlusion problems on detection and the latter 
usually involve excessively heavy components for portable usage. In this paper, we present a system of 
the passive magnetic tangi­ble designs that enables 3D tangible interactions in the near-surface space 
of portable displays. By attaching a thin-form Hall-sensor grid to the back of an unmodi.ed portable 
display, the speci.cally designed magnetic unit, GaussBits [Liang et al. 2013] allow the 3D position 
and partial 3D orientation (tilt or roll) of the magnetic unit to be stably resolved on or above the 
display, by using the proposed bipolar magnetic .eld tracking technology. Additionally, since the magnetic 
.eld can easily penetrate through any non-ferrous mate­rial, such as the user s hand, interaction designers 
can incorporate the magnetic unit into an appropriately shaped non-ferrous object to exploit metaphors 
from simulating the real-world tasks, and users can freely manipulate them by hands or using other non-ferrous 
tools without interfering with the tracking. 2 DESIGN We implemented a grid of 32×24 = 768 Winson WSH138 
Hall sen­sors, with an 160 (W)×120 (H) sensing area (Figure 1(a)). Each sensor element detects both N-and 
S-polar magnetic .eld inten­sities, in a range from 0 to 200 Gauss on a 256-point scale. The captured 
bi-polar magnetic .eld data are 16x up-sampled using the bi-cubic interpolation method, and processed 
by a bipolar sensing algorithm. The 3D position of a magnetic unit can be resolved by analyzing the distribution 
and the maximum strength of the sensed magnetic .eld. The tilt of a magnetic unit can be resolved in 
any direction without ambiguity if an axially magnetized cylindrical or ring magnet is used (Figure 1(c)). 
The roll of a magnetic unit can be resolved if a laid axially magnetized cylindrical magnet or a bi­polar 
magnet pair is used (Figure 1(d)). Four applications (Figure 2) are presented to demonstrate how the 
enabling technology can enrich the mobile interaction experience. 1http://www.appmatestoys.com/ 2http://www.vicon.com/ 
3http://www.polhemus.com/  Figure 1: Our system tracks magnetic tangibles above a portable display by 
attaching a (a) Hall-sensor grid to (b) the back. The (c) tilt or (d) roll information of the magnetic 
units can be resolved as well as the 3D position. In all applications, magnetic tangibles are tracked 
by a Hall-sensor grid attached to the back of a portable display (Figure 1(b)). Going 3D. As a clock 
widget (Figure 2(a)), a user can lift it to a different height to select a hand to adjust, then rotate 
it (in the air) to adjust the selected hand. As a map navigator (Figure 2(b)), a user can tilt it to 
navigate around the map, lift it to zoom-out at the where it hovers, and .ip it to switch the function 
to zoom-in. Providing favorable form factors. In the .ight simulation (Fig­ure 2(c)), a user can pinch 
an magnetic toy aircraft to control it above the display. Therefore, the user can avoid the obstacles 
by moving and/or tilting it, climb or dive by raising or lowering its nose, and pick up the bonus by 
landing it. Playing with non-ferrous instruments. In the .sh-frying simula­tion (Figure 2(d)), a user 
can place the magnetic .sh on the plastic pan, .ip the .sh using the wooden chopsticks, hold and shake 
the pan to fry the .sh, and then pour the fried .sh onto the display. Figure 2: Sample applications. 
(a) Clock widget. (b) Map naviga­tor. (c) Flight simulation. (d) Fish-frying simulation.  References 
LIANG , R.-H., CHE NG, K.-Y., CH A N , L., PEN G , C.-X., CHE N, M. Y., LIA N G , R.-H., YANG , D.-N., 
A N D CHEN, B.-Y. 2013. GaussBits: Magnetic tangible bits for portable and occlusion­free near-surface 
tangible interactions. In Proc. ACM CHI 2013. Permission to make digital or hard copies of part or all 
of this work for personal or classroom use isgranted without fee provided that copies are not made or 
distributed for commercial advantage and that copies bear this notice and the full citation on the first 
page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503442</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Wearable line-of-sight detection system using transparent optical sensors on eyeglasses and their applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503442</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503442</url>
		<abstract>
			<par><![CDATA[<p>The line-of-sight (LOS) detection system has various applications, such as in the field of communication technology, human-computer interaction, safety and security [1]. In early studies, using an external camera and infrared camera to detect the location and motion of the pupil is the mainstream method of detecting LOS. However, these methods involve several problems in that they restrict users' activities and expose them to physical and mental stress. With this in mind, a solution to enhance the practicality of the promising LOS detection applications, with systems that do not impose any restriction on users' activities nor expose them to any stress, is strongly demanded.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190761</person_id>
				<author_profile_id><![CDATA[82458698657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ozawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ozawamasataka@a2.keio.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190762</person_id>
				<author_profile_id><![CDATA[82458960057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[A.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190763</person_id>
				<author_profile_id><![CDATA[82458637057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kota]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sampei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190764</person_id>
				<author_profile_id><![CDATA[82458797757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[N.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University and JST PRESTO]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1170260</ref_obj_id>
				<ref_obj_pid>1170131</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[N. Murray and D. Roberts, Proceedings of 10th IEEE International Symposium on Distributed Simulation and Real-Time Applications 2006 (DS-RT'06), pp. 70--76 (2006)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 K:\SIGGRAPH\SIGRRAPHabstract.png  Wearable Line-of-Sight Detection System Using Figure 1: (a) Line-of-Sight 
(LOS) detection system which has micro-fabricated transparent optical sensors on eyeglasses. (b) The 
optical sensors detect the light reflected from the eyeball. (c) The deduced LOS on a whiteboard. (d) 
The selection of the drink from the vending machine. Transparent Optical Sensors on Eyeglasses and Their 
Applications M. Ozawa1, A. Oikawa1, Kota Sampei1 and N. Miki1,2 1Keio University, 2JST PRESTO (a) 
     (b)      (c)     (d) 1. Introduction The line-of-sight (LOS) detection system has 
various applications, such as in the field of communication technology, human-computer interaction, safety 
and security [1]. In early studies, using an external camera and infrared camera to detect the location 
and motion of the pupil is the mainstream method of detecting LOS. However, these methods involve several 
problems in that they restrict users activities and expose them to physical and mental stress. With this 
in mind, a solution to enhance the practicality of the promising LOS detection applications, with systems 
that do not impose any restriction on users activities nor expose them to any stress, is strongly demanded. 
 2. Our work We demonstrate a line-of-sight detection system which has micro-fabricated transparent optical 
sensors on eyeglasses as photographed in Figure 1(a). Dye-sensitized photovoltaic cells are micro-patterned 
to be used as optical sensors. As shown in Figure 1(b), optical sensors on eyeglasses detect the reflection 
light from the eye, which is stronger from the white than the black, and can deduce the pupil position. 
The system has only 4 cells. Since this system is wearable and transparent, it neither limits users activities 
nor blocks their sight. No external camera and no infrared camera are necessary to detect the pupil position, 
which contributes the simplicity and light weight of the system (60 g). 3. Experiments First, we developed 
the algorithm to detect the view angle using the output voltages of the cells in an analogue manner. 
The view angle was correlated to the output of the cells. Figure 1(c) shows the deduced trajectory of 
the LOS on a whiteboard. The averaged detection accuracy was 1.5 ° in the view angle. The results verified 
that the proposed cell design achieved highly accurate LOS detection in both the horizontal and vertical 
directions Second, we deduced characteristic output signals of the cells when the user blinked as shown 
in Figure 1(d). Eye-blinking can be used as a trigger to initiate the LOS measurement, select objects, 
etc. Finally, we successfully demonstrated applications of the user s LOS that was detected by the proposed 
system, which included selection of objects to express the user s intent and tracking the point of regards 
to deduce the user s interest. Figure 1(d) shows the attempt to select a product in the vending machine 
on a street. The system could be successfully used outside of the laboratory. References [1] N. Murray 
and D. Roberts, Proceedings of 10th IEEE International Symposium on Distributed Simulation and Real-Time 
Applications 2006 (DS-RT 06), pp. 70 76 (2006) 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503443</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>53</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Wide area projection method for active-shuttered real image autostereoscopy]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503443</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503443</url>
		<abstract>
			<par><![CDATA[<p>In the field of glass-free autostereoscopic display, the working area of the display is an important topic for research. It is desirable to have a display that provides a projected image with a large working area. The working area is defined on the basis of two parameters: the distance and the direction from the display to the target user. We have previously proposed the glass-free autostereoscopic display using Active-shuttered Real Image Autostereoscopy (ARIA) technology[Nii et al. 2012]. To increase the work distance of this system, we propose the double-layer active-shutter control method. ARIA is a simple technology for developing a glass-free auto-stereoscopic display without any mechanical moving components. The display made with this technology consists of two LCD devices and a large projection lens with an eye-tracking system. One of the LCD devices acts as the image display while the other serves as an active-shutter panel made of a set of pixels. This panel controls the light source by changing the level of transparency of its pixels in order to control the direction of the projection area on the basis of the eye location. As shown in Fig. 1(a), the image projected at the left eye while a shutter blocks the light travelling to the right eye, so that the next image is projected at the right eye in the next frame. This mechanism enables time division for the stereo images. The shutter was installed at a fixed distance from the lens. When the user moves from the designated position, thus changing the distance from the display, the image for the right eye may be received at the left eye. Please observe the line at the "Near point" in Fig. 1(a). When the user moves to the near point of the real image, the right eye views the image intended for the left eye. To avoid this problem, the image size is decreased to be smaller than the designated distance. The method for estimation was described for a specific distance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190765</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Research Laboratory, IIJ Innovation Institute, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[h-nii@iij.ad.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2407720</ref_obj_id>
				<ref_obj_pid>2407707</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nil, H., Zhu, K., Yoshikawa, H., Htat, N. L., Aigner, R., and NAKATSU, R. 2012. Fuwa-vision: an auto-stereoscopic floating-image display. In <i>SIGGRAPH Asia 2012 Emerging Technologies</i>, ACM, New York, NY, USA, SA '12, 13:1--13:4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 WideAreaProjetionMethodforAtive-shutteredRealIma eAutostereosopy HideakiNii. ResearhLaboratory,IIJInnovationInstitutein. 
 Figur (a)ThemethodofanAtive-shutteredRealImage 1Introdution Inthefeldofglass-freeautostereosopidisplay,theworkingarea 
ofthedisplayisanimportanttopiforresearh.Itisdesirableto haveadisplaythatprovidesaprojetedimagewithalargework­ingarea.Theworkingareaisdefnedonthebasisoftwoparam­eters:thedistaneandthediretionfromthedisplaytothetar­getuser.Wehavepreviouslyproposedtheglass-freeautostereo­sopidisplayusingAtive-shutteredRealImageAutostereosopy 
(ARIA)tehnology[Niietal.2012].Toinreasetheworkdistane ofthissystem,weproposethedouble-layerative-shutterontrol 
method.ARIAisasimpletehnologyfordevelopingaglass-free auto-stereosopidisplaywithoutanymehanialmovingompo­nents.ThedisplaymadewiththistehnologyonsistsoftwoLCD 
deviesandalargeprojetionlenswithaneye-trakingsystem. OneoftheLCDdeviesatsastheimagedisplaywhiletheother 
servesasanative-shutterpanelmadeofasetofpixels.Thispanel ontrolsthelightsourebyhangingtheleveloftransparenyof 
itspixelsinordertoontrolthediretionoftheprojetionareaon thebasisoftheeyeloation.AsshowninFig.1(a),theimagepro­jetedatthelefteyewhileashutterbloksthelighttravellingtothe 
righteye,sothatthenextimageisprojetedattherighteyeinthe nextframe.Thismehanismenablestimedivisionforthestereo 
images.Theshutterwasinstalledatafxeddistanefromthelens. Whentheusermovesfromthedesignatedposition,thushanging 
thedistanefromthedisplay,theimagefortherighteyemaybe reeivedatthelefteye.Pleaseobservethelineatthe"Nearpoint" 
inFig.1(a).Whentheusermovestothenearpointofthereal image,therighteyeviewstheimageintendedforthelefteye.To 
avoidthisproblem,theimagesizeisdereasedtobesmallerthan thedesignateddistane.Themethodforestimationwasdesribed 
foraspeifdistane. 2Disussion Inthedesignoftheshutter,itisrequiredtofndamehanismfor projetinglightraysfromtheimagedisplaytotherighteyewhile 
ompletelyblokingthelighttothelefteye.Allthelightraysthat reahtheeyethroughthelenspassthroughtheonjugatefoal 
point.Aordingly,theoriginalproblemonvertstoageometri problem.Theproblemisdesribedasonewherelightraysreah 
thefouspointbutallthelightraysdonotpassthroughtheinhib­itedfouspoint,thispointisonjugatefoalpointofthelefteye. 
Theidealapproahtosolvingthisproblemistohangingtheme­hanialpositionoftheative-shutterpanelalongthelinebetween 
theobjetandthelens,dependingontheuser'sdistane.Hene,we .e-mail:h-nii iij.ad.jp Autostereosopy,(b)Themethodofdoublelayerssystem 
 Figur2 Theomparisonbetweensinglelayeranddoublelayer shuttermethod exludethissolutiontokeepthesystemwithoutmehanialmov­ingomponents. 
Theproposedapproahisthedouble-layershuttermethod (Fig.1(b)).Inthismethod,thelightraysanbepreiselyontrolled 
bysuitablyontrollingthedouble-layershutterstoshutattheon­jugatefoalpoint,asshowninFig.1(b).Weestimatetheimage 
widthforbothsingleanddoublelayersunderthesameonditions. 1)Thefrstshutterisinstalled0.31mbehindofthelens,andthe 
seondshutter(proposedinthisreport)isinstalled0.28mbehindof thelens.2)Thelensdiameteris0.5mandthefoallengthis0.25 
m.3)Theimagedisplayisinstalled0.5mbehindthelens.The resultsareshowninFig.2.Forthesameimagewidthof0.3m,the 
useranviewtheorretimagefrom1to2.9monthedouble-layer systemandfrom1.3to1.5monthesingle-layersystem.Thelimi­tationofthissystemiswithregardtothebrightnessoftheprojeted 
image.ThetransparenyofoneoftheLCDpanelsusedisapproxi­mately0.3underpolarizedlight.Thedouble-layersystemprovides 
reduedbrightnessasomparedtothesingle-layersystem. 3Conlusion Wereportontheapproahforinreasingtheworkdistaneofthe 
ARIAdisplaysystem.Theuseofthedouble-layershutteranin­reasetheimagesizeasomparedtothatobtainedwiththeuseof 
thesingle-layershutter.Further,thedouble-layershuttersystem hasawideappliationrange.Thelimitationofthissystemisin 
termsofbrightness,butthisanberesolvedbyusingabrightLCD panelinthenearfuture. Referenes N..,H.,Z..,.,Y........,H.,H...,N.L.,A.....,R., 
...N......,R.2012.Fuwa-vision:anauto-stereosopi foating-imagedisplay.InSIGGRAPHAsia2012Emerging Tehnologies,ACM,NewYork,NY,USA,SA'12,13:1-13:4. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503444</section_id>
		<sort_key>590</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Image and video processing]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>2503445</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A patch analysis approach for seam-carved image detection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503445</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503445</url>
		<abstract>
			<par><![CDATA[<p><i>Seam carving</i> is a content-aware image resizing method [Shamir and Avidan 2007], which assigns Sobel-operator-based energy to each pixel and describes seams as the eight-connected paths of pixels. Successive removal of the optimal seams, i.e., those seams with the lowest sum of energy, allows reduction in image size. Pixels with lower energy are generally removed earlier; implying that (1) the modifications to the image are difficult to identify and (2) low energy can be deliberately assigned to particular objects so that they can be removed from the image. These two observations reveal that, although difficult, it is important to design a seam carving detection method.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190766</person_id>
				<author_profile_id><![CDATA[82459266957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jyh-Da]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chang Gung University, Taoyuan, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jdwei@mail.cgu.edu.tw]]></email_address>
			</au>
			<au>
				<person_id>P4190767</person_id>
				<author_profile_id><![CDATA[82458641257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yu-Ju]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chang Gung University, Taoyuan, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190768</person_id>
				<author_profile_id><![CDATA[82458981357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yi-Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chang Gung University, Taoyuan, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190769</person_id>
				<author_profile_id><![CDATA[82459004457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Li-Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Chang Gung University, Taoyuan, Taiwan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fillion, C., and Sharma, G. 2010. Detecting content adaptive scaling of images for forensic applications. In <i>Proc. SPIE: Media Forensics and Security</i>, vol. 7541, 36--47.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1597837</ref_obj_id>
				<ref_obj_pid>1597817</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sarkar, A., Nataraj, L., and Manjunath, B. S. 2009. Detection of seam carving and localization of seam insertions in digital images. In <i>Proc. 11th ACM workshop on Multimedia and security</i>, 107--116.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276390</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shamir, A., and Avidan, S. 2007. Seam carving for content aware image resizing. <i>ACM Transactions on Graphics 26</i>, 3, 107--216.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Patch Analysis Approach for Seam-Carved Image Detection Jyh-Da Wei* , Yu-Ju Lin, Yi-Jing Wu and Li-Wei 
Kang Chang Gung University, Taoyuan, Taiwan  Figure 1: Selection of the Optimal Patch Type. There are 
nine manners patching the mini-square to the referee pattern. In this example, the eighth type is selected 
as the optimal. 1 Introduction Seam carving is a content-aware image resizing method [Shamir and Avidan 
2007], which assigns Sobel-operator-based energy to each pixel and describes seams as the eight-connected 
paths of pixels. Successive removal of the optimal seams, i.e., those seams with the lowest sum of energy, 
allows reduction in image size. Pixels with lower energy are generally removed earlier; im­plying that 
(1) the modi.cations to the image are dif.cult to identify and (2) low energy can be deliberately assigned 
to particular objects so that they can be removed from the image. These two observa­tions reveal that, 
although dif.cult, it is important to design a seam carving detection method. Existing methods to detect 
seam-carved images can be categorized into two classes: those derived from steganography attacks and 
those based on other statistical features. Sarkar et al. introduced a steganography attacking algorithm 
for seam carving detection in 2009 [Sarkar et al. 2009]. This algorithm uses a 324-dimensional Markov 
feature consisting of 2D difference histograms in the 8 ×8 block-based discrete cosine transform domain 
for input into an SVM classi.er system. This way, although proven to be well suited to steganography 
attacks (> 96% accuracy), yields accuracies of 70.4% and 77.3% for detecting 20% and 30% seam-carved 
images respectively. Fillion and Sharma proposed some new detection methods for seam-carved images in 
2010 [Fillion and Sharma 2010]. The basic idea behind these methods includes the bias of energy distribution, 
the dispersal of seam behavior, and the affection of wavelet absolute moments. These statistical features 
achieved higher detection accu­racies of 84.0% and 91.3% for 20% and 30% seam-carved images respectively. 
*e-mail:jdwei@mail.cgu.edu.tw This work was supported in part by the Republic of China National Science 
Council under the Grants NSC 101-2221-E-182-073. 2 Our Approach We believe that the detection accuracy 
can be further improved by .nding an adequate detection method for seam-carving behavior. Speci.cally, 
the desired feature must re.ect the fact that several insigni.cant pixels have been removed during the 
seam carving process. Here, we propose a novel method, referred to as patch analysis to detect seam-carved 
images. After converting the test image into its intensity component I, our method divides I into 2 ×2 
blocks called mini-squares . A mini­square, Si,j , is de.ned as follows: I(2i, 2j) I(2i, 2j +1) Si,j 
=. (1) I(2i +1, 2j) I(2i +1, 2j +1) For each mini-square, we have nine types of 2 ×3 patches that roll 
it back from possible seam carving effects. These patches, shown in Fig. 1, can be constructed by interpolation 
between adjacent pixels. We also have a referee pattern for deciding the optimal patch type. The referee 
pattern, Ri,j ,isa 2 ×3 image pattern generated from the local area of the corresponding mini-square: 
1 1 1 Ri,j = 4a=0b=0 T I(2i -1+ a, 2j -1+ b) I(2i +1 + a, 2j -1+ b) I(2i -1+ a, 2j + b) I(2i +1 + a, 
2j + b) . I(2i -1+ a, 2j +1 + b) I(2i +1 + a, 2j +1 + b) (2) Using cosine similarity as the criterion, 
we assign each mini-square one type number in {0, 1, ···, 8}to indicate the optimal patch type. We can 
consequently calculate three patch transition probability matrices that connect the mini-squares in three 
directions, namely, subdiagonal, vertical, and diagonal. The entries in these three 9 ×9 matrices, together 
with the probabilities of the nine types, form a 252-dimensional detection feature. This feature is sent 
to an SVM classi.er system that detects whether the test image has been seam carved. While our description 
only focuses on detecting those im­ages with vertical seam carving, the proposed method can be used to 
detect horizontal seam removal in a similar manner. Our method results in the currently best detection 
accuracies, namely, 92.2%, 92.6% and 95.8% for 20%, 30% and 50% seam­carved images respectively. Besides 
the detection accuracy, this method has potential applications that deserves further research, for example, 
identi.cation of the hot regions frequently crossed by carved seams. References FILLION, C., AND SHARMA, 
G. 2010. Detecting content adaptive scaling of images for forensic applications. In Proc. SPIE: Media 
Forensics and Security, vol. 7541, 36 47. SARKAR, A., NATARAJ , L., AND MANJUNATH , B. S. 2009. De­tection 
of seam carving and localization of seam insertions in digital images. In Proc. 11th ACM workshop on 
Multimedia and security, 107 116. SHAMIR, A., AND AVIDAN , S. 2007. Seam carving for content aware image 
resizing. ACM Transactions on Graphics 26,3, 107 216. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503446</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A spatial-temporal method to refine a depth image]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503446</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503446</url>
		<abstract>
			<par><![CDATA[<p>In this poster, a spatial-temporal depth image refinement is proposed. First, we perform a spatial denoising by applying Joint Bilateral Filter(JBF) to an image pyramid iteratively. Second, we employ a signed distance function(SDF) [Curless 1996] to obtain an optimal depth image from the depth video in which the temporal noise occurs between video frames.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[depth image]]></kw>
			<kw><![CDATA[multi-scale]]></kw>
			<kw><![CDATA[signed distance function]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190770</person_id>
				<author_profile_id><![CDATA[81421600485]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[In]]></first_name>
				<middle_name><![CDATA[Yeop]]></middle_name>
				<last_name><![CDATA[Jang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[inyeop@gist.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190771</person_id>
				<author_profile_id><![CDATA[82458978157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Min]]></first_name>
				<middle_name><![CDATA[Ki]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190772</person_id>
				<author_profile_id><![CDATA[82459324557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Seung]]></first_name>
				<middle_name><![CDATA[Joo]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190773</person_id>
				<author_profile_id><![CDATA[82459092457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sun]]></first_name>
				<middle_name><![CDATA[Jong]]></middle_name>
				<last_name><![CDATA[Jeong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190774</person_id>
				<author_profile_id><![CDATA[82459140657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kyu]]></first_name>
				<middle_name><![CDATA[Je]]></middle_name>
				<last_name><![CDATA[Lim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190775</person_id>
				<author_profile_id><![CDATA[82458997557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yong]]></first_name>
				<middle_name><![CDATA[Hwi]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190776</person_id>
				<author_profile_id><![CDATA[82458753657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Kwan]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>237269</ref_obj_id>
				<ref_obj_pid>237170</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Curless, V., Levoy M. 1996. A volumetric method for building complex models from range images. In <i>Proceedings of of the 23rd annual conference on Computer graphics and interactive techniques</i>, 303--312.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A spatial-temporal method to re.ne a depth image In Yeop Jang* Min Ki Park, Seung Joo Lee, Sun Jong 
Jeong, Kyu Je Lim, Yong Hwi Kim, Kwan H. Lee GIST GIST GIST  (a) (b) (c) (d) Figure 1: (a) The input 
data from Kinect sensor. (b) The spatial .ltering based on the pyramidal joint bilateral .lter. (c) The 
temporal depth re.nement based on the signed distance functions. (d) The optimal surface and the re.ned 
depth image. Keywords: Depth Image, Multi-Scale, Signed Distance Function 1 Introduction In this poster, 
a spatial-temporal depth image re.nement is pro­posed. First, we perform a spatial denoising by applying 
Joint Bi­lateral Filter(JBF) to an image pyramid iteratively. Second, we em­ploy a signed distance function(SDF) 
[Curless 1996] to obtain an optimal depth image from the depth video in which the temporal noise occurs 
between video frames. 2 The proposed method An image which contains objects of many sizes contains also 
the noises of many sizes. We perform a multi-scale image .ltering to reduce the noises by constructing 
an image pyramid(Figure(b)). We apply JBF to the depth image pyramid iteratively from the smallest depth 
image with scaling up to the original size. In our joint bilat­eral .ltering, the corresponding color 
image is used as additional information at each level. Our spatial .lter can induce a pyrami­dal .ltering 
classifying local noises and spread noises. Also, it improves the ambiguous depths of boundary regions 
of adjacent objects in the depth image by imposing a constraint based on the corresponding color information 
during the JBF .ltering. The pyramidal JBF is applied over k-frames. Now, we perform a temporal re.nement 
to obtain an optimal depth image using the k .ltered images. Even though the depth image is spatially 
.ltered at each frame, the depth pixels suffer from .ickered which occurs between frames. To resolve 
this, we employ a signed distance func­tion which represents the signed distance of each point x to the 
nearest range surface along the line of sight to the depth camera. The re.ned k frames are converted 
the signed distance functions f1(x), ..., fk(x) and they are integrated in a discrete voxel grid. First, 
a depth value is assigned to each voxel according to distance from the depth camera. Then, the pixels 
of depth image are pro­jected to the voxel grid along their projection lines (see Figure 2). Next, the 
differences are computed between the depth of each voxel and the one projected from each depth pixel. 
The difference val­ues mean the signed distance values for the depth image. Then, a SDF is de.ned by 
extracting the voxels having the minimum signed *e-mail:inyeop@gist.ac.kr Figure 2: The .gure depicts 
how SDFs and their range surfaces are computed. distance value along the projection lines. Accordingly, 
k depth im­ages converted as k SDFs and the SDFs construct k range surfaces (Figure 1(c), the black curves 
in the voxel grid of Figure 2) in the voxel grid. The range surfaces may be still interoperated since 
they are based on the .ickered depth values. We .nally compute a cu­mulative SDF F (x) from all the SDFs, 
which extract an optimal range surface. The optimal surface can be obtained by satisfying F (x) = f1(x) 
+ .. + fk(x) = 0 in the least squares sense. Figure 1(d) shows the optimal depth image and the corresponding 
optimal range surface. It takes about 30 seconds to process a 640x480 depth image. Conclusion In this 
poster, a depth image is spatially and temporally re.ned by applying a pyramidal JBF and the integration 
of SDFs. Acknowledgements This work was supported by the National Re­search Foundation of Korea(NRF) 
grant funded by the Korea gov­ernment(MEST) (No.2013031191). References CU RL E S S , V., LEVOY M. 1996. 
A volumetric method for building complex models from range images. In Proceedings of of the 23rd annual 
conference on Computer graphics and interactive techniques, 303 312. Permission to make digital or hard 
copies of part or all of this work for personal or classroom use isgranted without fee provided that 
copies are not made or distributed for commercial advantage and that copies bear this notice and the 
full citation on the first page. Copyrights for third-party components of thiswork must be honored. For 
all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Research Foundation of Korea (NRF) grant funded by the Korea government (MEST)</funding_agency>
			<grant_numbers>
				<grant_number>2013031191</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503447</article_id>
		<sort_key>620</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[A stereo six-band motion picture capturing using 4K digital cinema camera]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503447</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503447</url>
		<abstract>
			<par><![CDATA[<p>In digital archiving for cultural heritage preservation, in the medical field, and in some industrial fields, the high-fidelity reproduction of color, gloss, texture, three-dimensional (3-D) shape, and movement is very important. Multi-spectrum imaging can provide accurate color reproduction. Although several types of multi-spectral camera systems have been developed, all of them, except for the six-band HDTV camera system developed by Ohsawa et al [Ohsawa et al. 2004], are multi-shot and none can take still images of moving objects and moving pictures. However, Ohsawa et al.'s system requires very complex and expensive customized optics whose optical elements must be arranged precisely, which makes it far from practical. In order to make multi-spectrum video systems pervasive, the equipment costs must be reduced by ensuring they have as much compatibility with existing video camera systems as possible. To meet this requirement, several stereo one-shot six-band still image capturing systems that also combine multi-spectrum and stereo imaging techniques have been proposed [Tsuchida et al., 2010; Shrestha et al., 2011]. In this paper, we propose a system that applies their concept to existing 4K digital cinema cameras and show the possibility using the proposed system for cinematography.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190777</person_id>
				<author_profile_id><![CDATA[81466642662]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Masaru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsuchida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tsuchida.masaru@lab.ntt.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190778</person_id>
				<author_profile_id><![CDATA[81556462856]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190779</person_id>
				<author_profile_id><![CDATA[81557141556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190780</person_id>
				<author_profile_id><![CDATA[82458710957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mukai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190781</person_id>
				<author_profile_id><![CDATA[81502671774]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kunio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kashino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190782</person_id>
				<author_profile_id><![CDATA[81100230754]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Junji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Communication Science Laboratories, NTT Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190783</person_id>
				<author_profile_id><![CDATA[81100590294]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aoki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tohoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ohsawa, K., et al., 2004. Six-band hdtv camera system for spectrum-based color reproduction. <i>Journal of Imaging Science and Technology, 48, 2, 85--92</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836917</ref_obj_id>
				<ref_obj_pid>1836845</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tsuchida, M., et al., 2010. a stereo one-shot multi-band camera system for accurate color reproduction. <i>In proceedings of Siggraph, Poster, ACM</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Shrestha, R., et al., 2011. One-shot multispectral color imaging with a stereo camera. <i>In proceedings of SPIE-IS&T Electronic Imaging</i>, 7876, 797609-1--797609-11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Takita, K., et al., 2003. High-accuracy image registration based on phase-only correlation. <i>IEICE Transaction of Fundamentals, Vol. E86-A, no.8, 1925--1934</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>66134</ref_obj_id>
				<ref_obj_pid>66131</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Bookstein, F. L., 1989. Principal Warps: Thin-Plate Splines and the Decomposition of Deformations. <i>IEEE Transaction on Pattern Analysis and Machine Intelligence, Vol. 11, No. 16, 567--585</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Pratt, W. K., et al., 1976. Spectral estimation techniques for the spectral calibration of a color image scanner. <i>Applied Optics, Vol.15. 73--75</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>7</ref_seq_no>
				<ref_text><![CDATA[Hashimoto, M., 2008. Two-Shot type 6-band still image capturing system using Commercial Digital Camera and Custom Color Filter. <i>In proceedings of Fourth European Conference on Colour in Graphics, Imaging, and Vision (CGIV2008), 538--541</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A stereo six-band motion picture capturing using 4K digital cinema camera Masaru Tsuchida*, Shuji Sakai**, 
Koichi Ito**, Ryo Mukai*, Kunio Kashino*, Junji Yamato*, and Takafumi Aoki** * Communication Science 
Laboratories, NTT Corporation, ** Graduate School of Information Sciences, Tohoku University  Fig. 1. 
4K stereo six-band camera system and its spectral sensitivities. 1. Introduction In digital archiving 
for cultural heritage preservation, in the medical field, and in some industrial fields, the high-fidelity 
reproduction of color, gloss, texture, three-dimensional (3-D) shape, and movement is very important. 
Multi-spectrum imaging can provide accurate color reproduction. Although several types of multi-spectral 
camera systems have been developed, all of them, except for the six-band HDTV camera system developed 
by Ohsawa et al [Ohsawa et al. 2004], are multi-shot and none can take still images of moving objects 
and moving pictures. However, Ohsawa et al. s system requires very complex and expensive customized optics 
whose optical elements must be arranged precisely, which makes it far from practical. In order to make 
multi-spectrum video systems pervasive, the equipment costs must be reduced by ensuring they have as 
much compatibility with existing video camera systems as possible. To meet this requirement, several 
stereo one-shot six-band still image capturing systems that also combine multi-spectrum and stereo imaging 
techniques have been proposed [Tsuchida et al., 2010; Shrestha et al., 2011]. In this paper, we propose 
a system that applies their concept to existing 4K digital cinema cameras and show the possibility using 
the proposed system for cinematography. 2. 4K stereo six-band camera system Our camera system consists 
of two digital motion picture cameras and a custom interference filter. The filter is mounted in front 
of the lens of one camera. It cuts off short wavelengths, the peaks of both the blue and red in the original 
spectral sensitivity of the camera, and also cuts off the long wavelength of green. The measured spectral 
sensitivities of the six-band camera used in experiments are shown in Fig. 1. The camera with the filter 
captures a specialized three-band image; the other camera captures an ordinary RGB color image as shown 
in Fig. 2. 3. Generating six-band image from stereo image The two captured images have parallax. Therefore, 
to generate a six-band image from the pair of images, one image should be transformed to adjust it to 
the other. The generated six-band image is then converted into RGB image using the illumination spectrum 
and the characteristics of the display monitor. There are two main steps for generating a six-band image: 
(i) sub-pixel correspondence matching and (ii) geometric correction. To find corresponding points between 
a stereo image pair, we use a sub-pixelcorrespondence matching approach that combines local block matching 
by the phase-only correlation method (POC) [Takita et al., 2003] and the coarse-to-fine strategy with 
image pyramids. POC is a high-accuracy image matching technique that uses phase information in the Fourier 
domain, which can estimate translation between two images in sub-pixel accuracy. POC is also robust against 
illumination changes and noise and color shifts caused by differences in the spectral sensitivity of 
a camera. The image captured with the interference filter is transformed using the detected corresponding 
points. The thin-plate spline (TPS) model [Bookstein, 1989] is used for image transformation in this 
system. The resultant two three-band images are combined into a six-band image. Note that the process 
of six-band creation does not use depth information. 4. Color reproduction using six-band image Color 
reproduction based on the Wiener estimation method [Pratt et al., 2003] is conducted. Using the estimated 
spectral reflectance, spectral power distribution of illumination for observation, and the tone-curves 
and chromaticity values of primary-colors of the display monitor, we calculate the output RGB signals. 
Even when the illumination light used in at the observation site is different from that for image capturing 
(e.g., daylight is used for image capturing and a fluorescent lamp is used at the observationsite), the 
color observed under the observation light can be reproduced as if the object is in front of observers. 
* e-mail: tsuchida.masaru@lab.ntt.co.jp Fig. 2. Captured image Left: image without filter, Right: image 
with filter.  5. Experimental results In the experiments, we took images of dancer, who wore Japanese 
kimono, while she performed a classical Japanese dance in front of a golden folding screen. Two digital 
cinematography cameras (CineAltaTM F65RS, Sony) were used. The camera system can write out raw image 
data without any color correction and can take 4K (4096 x 2160 pixels) images, each of which has bit-depth 
of 16 bits at 120 fps. The baseline length of the two cameras was 15 cm and the distance between the 
cameras and the dancer was 10 m, which makes it possible to reduce the influence of image parallax between 
the two cameras in six-band image generation. Six-band time-sequential images were generated by combining 
the image captured without the filter and the resultant image transformed from the image captured with 
the filter (Fig. 2). Figure 3 shows a result of color reproduction. Few artifacts (e.g., double edges 
or pseudo color) caused by image transformation error are observed. Comparing the resultant image to 
the real object, one can see that the color of the object is well reproduced. This system and two-shot 
six-band camera system [Hashimoto, 2008] that uses the same digital camera and filter reproduce almost 
the same image quality, especially with respect to color. 6. Summary A 4K six-band motion picture acquisition 
and visualization system using stereo imaging has been proposed. The system consists of two commercially 
available digital cinematography cameras, which makes it easy to introduce a current cinematography system. 
The proposed system can be applied for stereoscopic 3D displays. The two captured images are transformed 
respectively to adjust the shape of one to that of the other and a stereo-pair of six-band image is generated. 
A stereoscopic six-band stereoscopic video system whose image size is XGA (1024 x 768 pixels) has also 
been developed. In this system, all image processing steps after image capture are implemented on GPUs 
and the frame rate of the system is 30 fps.  References OHSAWA, K., et al., 2004. Six-band HDTV camera 
system for spectrum-based color reproduction. Journal of Imaging Science and Technology, 48, 2, 85-92. 
TSUCHIDA, M., et al., 2010. A stereo one-shot multi-band camera systemfor accurate color reproduction. 
In proceedings of Siggraph, Poster, ACM. SHRESTHA, R., et al., 2011. One-shot multispectral color imaging 
with a stereo camera. In proceedings of SPIE-IS&#38;T Electronic Imaging, 7876, 797609-1 - 797609-11. 
TAKITA, K., et al., 2003. High-accuracy image registration based onphase-only correlation. IEICE Transaction 
of Fundamentals, Vol. E86-A, no.8, 1925-1934. BOOKSTEIN, F. L., 1989. Principal Warps: Thin-Plate Splines 
and the Decomposition of Deformations. IEEE Transaction on Pattern Analysis and Machine Intelligence, 
Vol. 11, No. 16, 567-585. PRATT, W. K., et al., 1976. Spectral estimation techniques for the spectral 
calibration of a color image scanner. Applied Optics, Vol.15. 73-75. HASHIMOTO, M., 2008. Two-Shot type 
6-band still image capturing system using Commercial Digital Camera and Custom Color Filter. In proceedings 
of Fourth European Conference on Colour in Graphics, Imaging, and Vision (CGIV2008), 538-541. granted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of this 
work must be honored. For all other uses, contact the Owner/Author. SIGGRAPH 2013, July 21 25, 2013, 
Anaheim, California. 2013 Copyright held by the Owner/Author. ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503448</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Adaptive manga re-layout on mobile device]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503448</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503448</url>
		<abstract>
			<par><![CDATA[<p>In the present day, smart phones and tablets are popular electronic devices for business, entertainment or study due to their convenience, portability and intuitive user interfaces. However, these advantages also induce one of their limitations: the limited available screen size. It is not comfortable to read articles, mangas, magazines,...etc, under such a small screen. Figure 1 shows the overview of our system. Before our work re-layouts a manga, the system must first extract all panels in the manga through our designed <b>corner matching algorithm</b>, sort the extracted panels into a queue, i.e a play list, based on the generally accepted manga reading rules and transform panels in the list to display under arbitrary accessing conditions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190784</person_id>
				<author_profile_id><![CDATA[82458607957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chia-Jung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tsai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190785</person_id>
				<author_profile_id><![CDATA[82458645557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chih-Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cyuan.yao@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190786</person_id>
				<author_profile_id><![CDATA[82459140857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pei-ying]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chiang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taipei University of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190787</person_id>
				<author_profile_id><![CDATA[82458619157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yu-Chi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190788</person_id>
				<author_profile_id><![CDATA[81309487399]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ming-Te]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National ChengChi University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190789</person_id>
				<author_profile_id><![CDATA[82459223857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hung-Kuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190790</person_id>
				<author_profile_id><![CDATA[82459226957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Yu-Shiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wong]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190791</person_id>
				<author_profile_id><![CDATA[82258888357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yu-Shuen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chiao Tung University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1779569</ref_obj_id>
				<ref_obj_pid>1779459</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chan, C. H., Leung, H., and Komura, T. 2007. Automatic Panel Extraction of Color Comic Images. <i>LNCS, Advances in Multimedia Information Processing</i>, 775--784.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Li, L., Wang, Y., Tang, Z., and Liu, D. 2013. Comic image understanding based on polygon detection. <i>Proc. SPIE 8658, Document Recognition and Retrieval XX 8658</i> (Feb.), 86580B--86580B--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yamada, M., Budiarto, R., and Endo, M. 2004. Comic image decomposition for reading comics on cellular phones. <i>IEICE Transaction on Information and Systems</i>, xx, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive Manga Re-Layout On Mobile Device Chia-Jung Tsai1 Chih-Yuan Yao1* Pei-ying Chiang5 Yu-Chi Lai1 
Ming-Te Chi2 Hung-Kuo Chu3 Yu-Shiang Wong3 Yu-Shuen Wang4 1 National Taiwan University of Science and 
Technology 2 National ChengChi University 3 National Tsing Hua University 4 National Chiao Tung University 
5 National Taipei University of Technology  &#38;#169; Eiichiro Oda / Shueisha (a) (b) Figure 1: (a) 
Using corner matching algorithm to extract the all panels in the manga, even it is the breakout panel. 
(b) Re-layout the panels of the manga on the arbitrary devices, and users could use simple gestures to 
read manga easily and clearly. 1 Introduction and Motivation In the present day, smart phones and tablets 
are popular electronic devices for business, entertainment or study due to their conve­nience, portability 
and intuitive user interfaces. However, these ad­vantages also induce one of their limitations: the limited 
available screen size. It is not comfortable to read articles, mangas, maga­zines,...etc, under such 
a small screen. Figure 1 shows the overview of our system. Before our work re-layouts a manga, the system 
must .rst extract all panels in the manga through our designed cor­ner matching algorithm, sort the extracted 
panels into a queue, i.e a play list, based on the generally accepted manga reading rules and transform 
panels in the list to display under arbitrary accessing conditions. 2 Our Approach In the past, several 
algorithms[Yamada et al. 2004; Chan et al. 2007; Li et al. 2013] have been proposed to extract all panels 
of one page in a general case but they cannot properly segment all panels out when there exists the case 
of breakout panels which are used for emphasis of some speci.c situations or special effects, such as 
the showing up of an important character or a critical play for the lead­ing role, by the artists. Our 
work can overcome this limitation with the following steps. We use a connected-component labeling al­gorithm 
to get initial partitions of the manga and the rough partiti­ions can help to precisely locate the corners 
of panel boundaries using Harris Corner Detection. Then, for each detected cor­ner, we would traverse 
its bi-direction along the boundary edges in short distance as its matching vectors, such as the Figure 
1(a) shown. Through careful observation the shape of a manga panel is always a convex polygon, and thus, 
each corner of a panel can .nd its neighboring two corners based on the inner product of its match­ing 
vectors with other corners matching vectors. When the value of the inner product between two matching 
vectors is equal to -1, the two corners is labelled as a pair and connected to form the real boundary 
of a panel. The process is repeated until we .nd all real * Corresponding author:cyuan.yao@gmail.com 
 boundaries of each panel and extract all panel precisely, even when there exist some breakout panels 
as shown in the Figure 1(a). In the last, all panels must be displayed onto a screen with an ar­bitrary 
size under an arbitrary accessing direction with the require­ment of clearly presenting all content and 
text in a panel to viewers. Our algorithm resize the panel based on the screen size and the selection 
of the vertical or horizontal accessing direction of the mo­bile device. When selecting a direction and 
a screen size, we could .nd a scale ratio, a, to uniformly deform each panel area, Pa, ac­cordingly to 
.t screen of mobile device. In order to .ll the screen of mobile device with area, Ma, Eqn. 1 is used 
to estimate how many panels could be displayed in the selected screen size and reading direction and 
the currently chosen panel. n E = arg min Ma - ai Pi a , E > 0 (1) n i=1 Although some empty space may 
be left when following Eqn. 1, this action could guarantee that the original meaning of the manga layout 
and the presented order be maintained. The left space would be .lled in the following panels in the play 
list with the transpar­ent state. To summarize the entire work, we propose an automatic method to extract 
all panels of a manga, and re-layout the panels of a manga to make accessing easier and more clear than 
a traditional manga reader on a mobile device. References CH A N , C. H., LE U N G , H., A N D KO M 
U R A , T. 2007. Automatic Panel Extraction of Color Comic Images. LNCS, Advances in Multimedia Information 
Processing, 775 784. LI, L., WA N G , Y., TA N G , Z., A N D LI U , D. 2013. Comic image understanding 
based on polygon detection. Proc. SPIE 8658, Document Recognition and Retrieval XX 8658 (Feb.), 86580B 
86580B 11. YA M A DA , M., BU D I A RTO , R., A N D EN D O , M. 2004. Comic im­age decomposition for 
reading comics on cellular phones. IEICE Transaction on Information and Systems, xx, 1 8. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503449</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>58</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Adjusting the disparity of stereoscopic 3D media in post production]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503449</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503449</url>
		<abstract>
			<par><![CDATA[<p>Selecting the convergence point and interaxial distance before the production of stereoscopic 3D media presents several challenges. There are few tools for previsualizing or validating model calculations, so estimating these parameters to match an intended shot relies on costly experience, intuition and iteration. Poor selection of these parameters can also cause or amplify viewer discomfort [Mendiburu 2009]. Figure 1 illustrates the relationship between discomfort and disparity (input images by Scharstein and Pal [2007]).</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[post production]]></kw>
			<kw><![CDATA[stereo 3D]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.2.10</cat_node>
				<descriptor>3D/stereo scene analysis</descriptor>
				<type>S</type>
			</primary_category>
			<other_category>
				<cat_node>I.3.3</cat_node>
				<descriptor>Display algorithms</descriptor>
				<type>S</type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371.10010382</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Image manipulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010372</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Rendering</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010178.10010224.10010226.10010239</concept_id>
				<concept_desc>CCS->Computing methodologies->Artificial intelligence->Computer vision->Image and video acquisition->3D imaging</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P4190792</person_id>
				<author_profile_id><![CDATA[81466647327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lesley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Northam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lanortha@uwaterloo.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190793</person_id>
				<author_profile_id><![CDATA[81100185028]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Asente]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Adobe Systems]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[asente@adobe.com]]></email_address>
			</au>
			<au>
				<person_id>P4190794</person_id>
				<author_profile_id><![CDATA[81466647282]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Istead]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jwistead@uwaterloo.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190795</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[csk@uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mendiburu, B. 2009. <i>3D Movie Making: Stereoscopic Digital Cinema from Script to Screen</i>. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2493797</ref_obj_id>
				<ref_obj_pid>2493621</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Northam, L., Asente, P., and Kaplan, C. S. 2013. Stereoscopic 3d image stylization. <i>Computers and Graphics 37</i>, 5, 389--402.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Scharstein, D., and Pal, C. 2007. Learning conditional random fields for stereo. In <i>Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wang, L., Jin, H., Yang, R., and Gong, M. 2008. Stereoscopic inpainting: Joint color and depth completion from stereo images. In <i>Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adjusting the Disparity of Stereoscopic 3D Media in Post Production Lesley Northam * Paul Asente Joe 
Istead Craig S. Kaplan§ University of Waterloo Adobe Systems University of Waterloo University of Waterloo 
 Figure 1: Red-cyan anaglyph 3D images. Left: dif.cult-to-fuse original with a large parallax/disparity 
and range of depth. Middle: disparities reduced by half. Right: non-uniform reduction of disparities 
adjusts uncomfortable parallax more than others. CR Categories: I.2.10 [Arti.cial Intelligence]: Vision 
and Scene Understanding 3D/stereo scene analysis; I.3.3 [Computer Graph­ics]: Picture/Image Generation 
Display algorithms; Keywords: stereo 3D, post productionLinks: DL PDF 1 Introduction Selecting the convergence 
point and interaxial distance before the production of stereoscopic 3D media presents several challenges. 
There are few tools for previsualizing or validating model calcu­lations, so estimating these parameters 
to match an intended shot relies on costly experience, intuition and iteration. Poor selection of these 
parameters can also cause or amplify viewer discomfort [Mendiburu 2009]. Figure 1 illustrates the relationship 
between dis­ comfort and disparity (input images by Scharstein and Pal [2007]). Some of this discomfort 
can be avoided by correcting the conver­gence point in post-production (post) by horizontally shifting 
the right-side view, but there are no methods to adjust the disparity in isolation for poorly selected 
interaxial distances. Consequently, shots that exhibit an uncomfortable disparity present a dif.cult 
and costly choice: retake the shot, scrap it altogether, or ignore viewer discomfort and green-light 
it for post. The shot may also be recon­structed in post with better parameters by upconverting one of 
the stereo pair s views to 3D, but many productions shun this approach because of the extra cost and 
potential compromise in artistic qual­ity. We present a fast, simple method for adjusting disparity in 
post using only the stereoscopic pair and disparity map. 2 Method Horizontal view shifting reduces the 
uncomfortable disparity but also moves the convergence point without regard to artistic intent. Shifting 
also leaves the range of depth .xed, so disparity reduction * lanortha@uwaterloo.ca asente@adobe.com 
jwistead@uwaterloo.ca §csk@uwaterloo.ca at one end of the range increases it at the other. Our approach, 
based on the consistent stylization method by Northam et al. [2013], allows .xed convergence points and 
adjustable range of depth. First, decompose the left and right views (L, R) into merged layers Md of 
all pixels with disparity d. Note that the merged view Md is wider than the left/right views since it 
has dimension (width + d, height). Then adjust the disparity of layer Md : 1. Let d ' be the desired 
disparity (e.g., a function of the initial disparity, d ' = d/2). 2. The adjusted left layer view L 
' d is formed by the pixels of Md in the region (d ' , width + d ' ) × (0, height). 3. The adjusted 
right layer view R ' d is formed by the pixels of Md in the region (2d ' , width + 2d ' ) × (0, height). 
 Once the disparities have been adjusted for each of the Md , com­bine all layer views L ' d , R d ' 
into the respective left and right views L ' , R ' . Occluded surfaces in L or R may produce empty regions 
(holes) in L ' , R ' that can be in-painted [Wang et al. 2008]. References MENDIBURU, B. 2009. 3D Movie 
Making: Stereoscopic Digital Cinema from Script to Screen. Focal Press. NORTHAM, L., ASENTE, P., AND 
KAPLAN, C. S. 2013. Stereo­scopic 3d image stylization. Computers and Graphics 37, 5, 389 402. SCHARSTEIN, 
D. , AND PAL, C. 2007. Learning conditional ran­dom .elds for stereo. In Computer Vision and Pattern 
Recogni­tion, 2007. CVPR 07. IEEE Conference on, 1 8. WANG, L. , JIN, H. , YANG, R. , AND GONG, M. 2008. 
Stereo­scopic inpainting: Joint color and depth completion from stereo images. In Computer Vision and 
Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 1 8. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use isgranted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503450</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>59</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[An examination of a gradation number for high-gradation displays based on luminance-differences]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503450</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503450</url>
		<abstract>
			<par><![CDATA[<p>With recent advances in high dynamic range displays, high-gradation displays have been actively studied. High-gradation displays have more than 256 gradations. When a luminance range of a display is very wide, a luminance-difference between each pixel value becomes small by increasing a gradation number. If this luminance-difference is larger than just noticeable difference (JND), the viewer may see contours on changes in gradation. Therefore, the number of gradations must be set up so that the luminance-difference between each pixel value is smaller than JND [Toshiyuki et al. 2008]. On the other hand, in usual high-gradation studies [Seetzen et al. 2004], the number of recognizable gradations is treated as one of the performance metrics because medical use high-gradation displays are based on DICOM GSDF and have recognizable gradations. Therefore, we must examine what kind of luminance-difference is appropriate for the outside of the medical field because high-gradation displays will be used there as well.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190796</person_id>
				<author_profile_id><![CDATA[82459294257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michimi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Inoue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Utsunomiya University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ino.mcm@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190797</person_id>
				<author_profile_id><![CDATA[82458713557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Utsunomiya University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190798</person_id>
				<author_profile_id><![CDATA[81488671331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Miyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ayama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Utsunomiya University, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190799</person_id>
				<author_profile_id><![CDATA[81100135451]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Naoki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashimoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Electro-Communications, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fujine, T., Kanda, T., Sugino, M., Yamamoto, Y., and Ohta, N. 2008. Evaluation for display color reproduction ability using number of distinguishable colors. <i>The Imaging Society of Japan 47</i>, 6, 508--519.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Inoue, M., Tanaka, T., Sato, M., Kasuga, M., and Hashimoto, N. 2012. An analysis of response characteristics for high dynamic range display. <i>The 2012 International Workshop on Advanced Image Technology</i>, 512--516.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., and Vorozcovs, A. 2004. High dynamic range display systems. <i>Proc. of SIGGRAPH '04 (Special issue of ACM Transactions on Graphics) 23</i>, 760--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Examination of a Gradation Number for High-Gradation Displays Based on Luminance-Differences Michimi 
Inoue* , Mie Sato, Miyoshi Ayama Naoki Hashimoto Utsunomiya University, Japan, *JSPS Research Fellow 
The University of Electro-Communications, Japan 1 Introduction With recent advances in high dynamic 
range displays, high­gradation displays have been actively studied. High-gradation dis­plays have more 
than 256 gradations. When a luminance range of a display is very wide, a luminance-difference between 
each pixel value becomes small by increasing a gradation number. If this luminance-difference is larger 
than just noticeable difference (JND), the viewer may see contours on changes in gradation. Therefore, 
the number of gradations must be set up so that the luminance-difference between each pixel value is 
smaller than JND [Toshiyuki et al. 2008]. On the other hand, in usual high-gradation studies [Seetzen 
et al. 2004], the number of recognizable grada­tions is treated as one of the performance metrics because 
medical use high-gradation displays are based on DICOM GSDF and have recognizable gradations. Therefore, 
we must examine what kind of luminance-difference is appropriate for the outside of the medical .eld 
because high-gradation displays will be used there as well. This study compares impressions of images 
displayed with the recognizable luminance-difference and with the unrecognizable luminance-difference, 
in order to examine the luminance-difference that is suitable for high-gradation displays. In addition, 
we exam­ine the necessary gradation number based on results of impression assessment. 2 Our Approach 
 First, we examined the JND in our experimental environment. We measured the numbers of recognizable 
gradations along the re­sponse characteristics of the gamma values 3.33 and 2.20 with our high-gradation 
display. Following the previous studies [Michimi et al. 2012], we constructed an experimental environment 
for our high-gradation display that had 1024 gradations. Our measurement was performed in a darkroom 
using our high-gradations display with a luminance range of 1.85 1841.40 cd/m2 . We examined the ratio 
of subjects that could distinguish the difference between each pixel value. Then, by multiplying the 
luminance-difference of each gradation-difference by each ratio value, we calculated the JND in our experimental 
environment. Figure 1 shows our JND, which is larger than the existing JND. Second, we performed an impression 
assessment experiment with differences of gradation number. We .rst carried out the experi­ment that 
was limited to a dark luminance range because the JND is different by luminance and human eyes are sensitive 
to luminance­difference in a dark environment. Our measurement was performed in a darkroom and the luminance 
range of our experiment was 6.82 62.27 cd/m2 . We prepared seven gradation numbers (8, 12, 16, 32, 64, 
128, 256 gradations). The 64, 128 and 256 gradations had luminance-differences that could not be discriminated. 
For the as­sessed images, we used RAW images that could be obtained by a single-lens re.ex camera. In 
addition, we converted RAW images to gray scale images in order to focus on the luminance-difference. 
We used six words as assessment terms and .ve-step Likert scales. Using correspondence analysis, we analyzed 
our experiment s re­sults. We found that the larger the number of gradations, the more *e-mail:ino.mcm@gmail.com 
 Figure 1: Our JND. we felt a good impression. However, the impression of the image became constant 
above 64 gradations. Hence, the impression of the image might not improve, even if we increased the number 
of gradations. In conclusion, we consider that the necessary gradation number is 64 in our experimental 
luminance range. In addition, luminance­difference of 64 gradations is less than our JND. Therefore, 
we think that a good impression is felt when the luminance-difference is less than our JND. It is said 
that the necessary gradation num­ber in a dark luminance range is less than a usual gradation number 
(256 gradations). In the future, we will perform the experiment on a wide luminance range including bright 
luminance. In addition, if we assess a display by the existing JND, the impression of the image may not 
improve because the existing JND is less than the luminance-difference of 64 gradations where the impression 
of the image becomes constant. Consequently, by continuing our study, we can propose a new indicator 
to assess the high-gradation dis­plays. This work was supported by JSPS KAKENHI Grant Number 24500251. 
 References FUJINE,T., KANDA ,T., SUGINO, M., YAMAMOTO ,Y., AND OHTA , N. 2008. Evaluation for display 
color reproduction abil­ity using number of distinguishable colors. The Imaging Society of Japan 47, 
6, 508 519. INOUE, M., TANAKA ,T., SAT O , M., KASUGA, M., AND HASHIMOTO , N. 2012. An analysis of response 
characteristics for high dynamic range display. The 2012 International Work­shop on Advanced Image Technology, 
512 516. SEETZEN, H., HEIDRICH,W., STUERZLINGER,W., WARD, G., WHITEHEAD, L., TRENTACOSTE , M., GHOSH, 
A., AND VOROZCOVS , A. 2004. High dynamic range display systems. Proc. of SIGGRAPH 04 (Special issue 
of ACM Transactions on Graphics) 23, 760 768. Permission to make digital or hard copies of part or all 
of this work for personal or classroom use isgranted without fee provided that copies are not made or 
distributed for commercial advantage and that copies bear this notice and the full citation on the first 
page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>JSPS KAKENHI</funding_agency>
			<grant_numbers>
				<grant_number>24500251</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503451</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>60</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[An improved rendering technique for active-appearance-model-based automated age progression]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503451</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503451</url>
		<abstract>
			<par><![CDATA[<p>Age progression is the process of creating images that suggest how a person may appear in a certain amount of time based on the effects of the aging process. Traditionally these images have been created manually by forensic artists who use both art and science to guide how representations appear, whether drawn or photo-manipulated. Automated age-progression seeks to use algorithmic methods to create accurate images of how the individual in a photo could appear after aging effects. It is still a fairly young area of research, but one promising technique suggested so far has been to use parametrically driven face models such as Active Appearance Models to modify the face appearance in an image based on a data-driven model of face aging. These can be successful but tend to suffer from reconstructed texture artifacts.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190800</person_id>
				<author_profile_id><![CDATA[81436594468]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Patterson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina Wilmington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pattersone@uncw.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190801</person_id>
				<author_profile_id><![CDATA[81436602687]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Amrutha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sethuram]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina Wilmington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sethurama@uncw.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190802</person_id>
				<author_profile_id><![CDATA[81436601181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Karl]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ricanek]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of North Carolina Wilmington]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ricanekk@uncw.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1736423</ref_obj_id>
				<ref_obj_pid>1736406</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Patterson, E., Sethuram, A., Ricanek K., and Bingham, F. Improvements in Active-Appearance-Model-Based Synthetic Age Progression for Adult Aging. <i>Proceedings of the IEEE Conference on Biometrics: Theory, Applications, and Systems</i>, Washington, D.C., 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>628788</ref_obj_id>
				<ref_obj_pid>628328</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lanitis, A., Taylor, C. J., and Cootes, T. F. Toward Automatic Simulation of Aging Effects on Face Images. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 24(4), 2002.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>996344</ref_obj_id>
				<ref_obj_pid>993451</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Matthews, I. and Baker, S. Active Appearance Models Revisited. <i>International Journal of Computer Vision</i>, Vol. 60, 2004, pp. 135--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Improved Rendering Technique for Active-Appearance-Model-Based Automated Age Progression Eric Patterson, 
Amrutha Sethuram, and Karl Ricanek University of North Carolina Wilmington* 1 Introduction Age progression 
is the process of creating images that suggesthow a person may appear in a certain amount of time based 
on theeffects of the aging process. Traditionally these images have been created manually by forensic 
artists who use both art and scienceto guide how representations appear, whether drawn or photo­manipulated. 
Automated age-progression seeks to use algorithmicmethods to create accurate images of how the individual 
in aphoto could appear after aging effects. It is still a fairly young area of research, but one promising 
technique suggested so far hasbeen to use parametrically driven face models such as ActiveAppearance 
Models to modify the face appearance in an imagebased on a data-driven model of face aging. These can 
be successful but tend to suffer from reconstructed texture artifacts. 2 Background The AAM is a well 
known technique for both .tting facelandmarks and for representing a face parametrically by a singlevector 
of coef.cients [Matthews and Baker 2004]. In age­progression, we are most interested in using the AAM 
as a meansto represent both the shape and texture of any new face image so that the image can be modi.ed 
in a believable and data-driven manner. An AAM trained on many representative faces, which in our case 
may be chosen to represent face aging speci.cally, isable to represent or reconstruct a face image using 
its parameter vector. This vector can be updated and reconstructed to create an age-progressed image 
[Patterson et al. 2009; Lanitis et al. 2002]. 3 Previous Technique An age-model is built by creating 
an AAM using a multitude of face images across the span of adult ages and then using SupportVector Regression 
(SVR) of the face-parameter vectors versustheir related ages to learn the aging function. Monte Carlo 
simulation follows to create an age-table of prototypical face-agevectors. A difference of target and 
source age vectors may beadded to an original face-image vector to generate a target vector, which in 
turn is reconstructed to build the age-progressed faceimage. Because this is only treating the face region 
in an image, itis often reconstructed as a separate face oval or directly drawn atop the original photo, 
often resulting in an arti.cial look.  4 New Technique The new technique is driven by the concept that 
the AAM may beused with its two parts of shape and texture to determine the age­related changes in a 
face image but not to draw them directly, thusmitigating the texture weaknesses of a reconstructed image. 
The original-face AAM vector and age-progressed target vector aredetermined as before, however the age-progressed 
imagereconstruction of the target vector is not calculated to draw back *email: {pattersone, sethurama, 
and ricanekk}@uncw.edu  Figure 1. An original photo followed by the old age-progression technique in 
the middle, then the new on the far right. on top of the original photo as in earlier approaches. Instead, 
thereconstructed age-progressed image is instead warped back to theoriginal face image s shape (temporarily 
un-doing age-related shape changes in the face). The difference of the age-progressed image now in original-face-shape 
space and the AAM­reconstructed version of the original face image, also in original­face-shape, is then 
calculated. This texture difference is added to the original photo s pixels, generally causing the appearance 
of nasolabial lines, forehead lines, etc.; however the shape is not yetcorrect for the calculated age-progression. 
To complete the process, the face-shape area of the original image is then warped to the shape of the 
reconstructed age-progressed face shaperepresentation, reintroducing age-related shape changes for the.nal 
rendered age-progression image. 5 Conclusions This novel method can result in more realistic age-progression 
images that do not require manual compositing for pleasing images. There are still some weaknesses with 
texture artifacts and warping as well as the lack of age-progressed update of hair, neck, etc., but future 
work may solve these issues. Acknowledgment: This work was partially funded by research contract from 
the Army Research Laboratory and Federal Bureau of Investigation s CJIS division. References PATTERSON, 
E., SETHURAM, A., RICANEK K., AND BINGHAM, F. Improvements in Active-Appearance-Model-Based SyntheticAge 
Progression for Adult Aging. Proceedings of the IEEEConference on Biometrics: Theory, Applications, and 
Systems,Washington, D.C., 2009. LANITIS, A., TAYLOR, C.J., and COOTES, T.F. Toward Automatic Simulation 
of Aging Effects on Face Images. IEEE Transactions on Pattern Analysis and Machine Intelligence,24(4), 
2002. MATTHEWS, I. and BAKER, S. Active Appearance Models Revisited. International Journal of Computer 
Vision, Vol. 60, 2004, pp. 135-164. Permission to make digital or hard copies of part or all of this 
work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Army Research Laboratory</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Federal Bureau of Investigation's CJIS division</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503452</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>61</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Anaglyph decomposition using disparity map]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503452</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503452</url>
		<abstract>
			<par><![CDATA[<p>Anaglyphs are the most primitive way of representing stereoscopic images. As one of the first attempts to produce 3D images, its viewing experience is not as satisfying as later methods but it is still widely used because of its simplicity and compactness. An anaglyph image is created by simply superimposing the red channel of the left image with blue and green channels of the right image. If an anaglyph image can be decomposed into its original left and right images, it can be used for compression as well as for conversion to other, more recent 3D viewing methods.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anaglyph]]></kw>
			<kw><![CDATA[stereoscopic 3D]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190803</person_id>
				<author_profile_id><![CDATA[82459080757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yong-Hun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cyhterry@naver.com]]></email_address>
			</au>
			<au>
				<person_id>P4190804</person_id>
				<author_profile_id><![CDATA[81409592301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[In-Kwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iklee@yonsei.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190805</person_id>
				<author_profile_id><![CDATA[82458625257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Young-Bae]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Joo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yonsei University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[youngbae.joo@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1409087</ref_obj_id>
				<ref_obj_pid>1457515</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bando, Y., Chen, B.-Y., and Nishita, T. 2008. Extracting depth and matte using a color-filtered aperture. In <i>ACM SIGGRAPH Asia 2008 papers</i>, ACM, New York, NY, USA, SIGGRAPH Asia '08, 134:1--134:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2407755</ref_obj_id>
				<ref_obj_pid>2407746</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lin, H.-S., Zheng, C.-L., Lin, Y.-H., and Ouhyoung, M. 2012. Optimized anaglyph colorization. In <i>SIGGRAPH Asia 2012 Technical Briefs</i>, ACM, New York, NY, USA, SA '12, 9:1--9:4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>200258</ref_obj_id>
				<ref_obj_pid>200241</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zabih, R., and Woodfill, J. 1994. Non-parametric local transforms for computing visual correspondence. In <i>Proceedings of the third European conference on Computer Vision (Vol. II)</i>, Springer-Verlag New York, Inc., Secaucus, NJ, USA, ECCV '94, 151--158.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Anaglyph Decomposition using Disparity map Yong-Hun Cho* In-Kwon Lee Young-Bae Joo Yonsei University 
Yonsei University Yonsei University  Figure 1: (a) Anaglyph Image (b) Obtained Right Disparity Map (c) 
Restored Right Image Keywords: Stereoscopic 3D, Anaglyph 1 Introduction Anaglyphs are the most primitive 
way of representing stereoscopic images. As one of the .rst attempts to produce 3D images, its viewing 
experience is not as satisfying as later methods but it is still widely used because of its simplicity 
and compactness. An anaglyph image is created by simply superimposing the red channel of the left image 
with blue and green channels of the right image. If an anaglyph image can be decomposed into its original 
left and right images, it can be used for compression as well as for conver­sion to other, more recent 
3D viewing methods. An anaglyph image can be restored into its original images with an accurate disparity 
map, but it is not easy to obtain an accurate dis­parity map. Since most of the existing stereo-matching 
algorithms use RGB color information as a basis, they are not applicable to an anaglyph image, which 
has some missing color channels. In this work, we propose a method to acquire the disparity map between 
the incomplete left and right images extracted from an anaglyph image. We use the color alignment measure 
combined with the census transform and gradient of the image. The original images are restored using 
the resulting disparity map. 2 Our Approach Given an anaglyph image(Figure 1(a)), we can extract the 
left im­age containing red channel and the right image containing the blue and green channel. For each 
pixel, we calculate the matching costs using the color alignment measure, census transform and gradient. 
The color alignment measure [Bando et al. 2008] assures pixel col­ ors within a local window to form 
a elongated cluster. The census transform [Zabih and Wood.ll 1994] is a measure that encodes the local 
structure according to its pixel intensities, de.ned over a 9x7 window in our case. The gradient of an 
image is simply computed over the left and right grayscale images. Given a pixel p = (x,y) in the right 
image and a disparity d, the corresponding matching cost for the left image at pixel (x,y+d) is computed 
as : *e-mail:cyhterry@naver.com e-mail:iklee@yonsei.ac.kr e-mail:youngbae.joo@gmail.com C(p, d) = aC 
AM (p, d) + ßC en(p, d) + .Gr(p, d) Where CAM is the color alignment measure, Cen is the census transform 
and Gr is the gradient of the image. The coef.cient a , ß and . decides the ratio of each measures. The 
resulting costs for the disparity map is optimized using standard graph cuts technique to impose smoothness. 
 3 Conclusion We proposed a method to obtain the disparity map(Figure 1(b)) from extracted left and right 
images using the color alignment mea­sure, census transform, and gradient. With this disparity map, we 
can restore the original image(Figure 1(c)). In future work, we will aim for a more accurate disparity 
map. Additionally, since it is practically impossible to obtain a perfect disparity map from an anaglyph 
image, we will work on the post­processing of restored images to identify erroneous regions and re­distribute 
the disparity depending on the color or disparity of the neighboring pixels. A comparison with existing 
work [Lin et al. 2012] is also planned. Acknowledgement This work was supported by the National Research 
Foundation of Korea (NRF) grant funded by the Korea government (MEST) (No. 2012-0008768). References 
BA NDO , Y., CH E N , B.-Y., A N D NI SHI TA, T. 2008. Extracting depth and matte using a color-.ltered 
aperture. In ACM SIG-GRAPH Asia 2008 papers, ACM, New York, NY, USA, SIG-GRAPH Asia 08, 134:1 134:9. 
LIN, H.-S., ZHE N G, C.-L., LIN , Y.-H., A N D OUH YO U N G , M. 2012. Optimized anaglyph colorization. 
In SIGGRAPH Asia 2012 Technical Briefs, ACM, New York, NY, USA, SA 12, 9:1 9:4. ZA BI H , R., A N D WOOD 
FI L L, J. 1994. Non-parametric local transforms for computing visual correspondence. In Proceed­ings 
of the third European conference on Computer Vision (Vol. II), Springer-Verlag New York, Inc., Secaucus, 
NJ, USA, ECCV 94, 151 158. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Research Foundation of Korea (NRF) grant funded by the Korea government (MEST)</funding_agency>
			<grant_numbers>
				<grant_number>2012-0008768</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503453</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>62</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Coded exposure HDR light-field video recording]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503453</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503453</url>
		<abstract>
			<par><![CDATA[<p>Capturing exposure sequences for computing high-dynamic range (HDR) images causes motion blur in case of camera movements. This also applies for light-field cameras, such as camera arrays. Images composed from multiple blurred HDR light-field perspectives are also blurred.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190806</person_id>
				<author_profile_id><![CDATA[81486655936]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Schedl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[David.Schedl@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P4190807</person_id>
				<author_profile_id><![CDATA[81503672096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Clemens.Birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P4190808</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Oliver.Bimber@jku.at]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Coded Exposure HDR Light-Field Video Recording David C. Schedl, Clemens Birklbauer, and Oliver Bimber* 
Johannes Kepler University Linz  Figure 1: HDR light-.eld recording during camera movement: Instead 
of recording complete exposure sequences per perspective of a camera array (e), we encode exposure times 
in each repeating camera quad-tuple (a,f). By computing a composite depth map from all exposures, segmenting 
it into depth layers, and tracking the optical .ow of scene features in each depth layer (c), we derive 
depth-individual PSFs that are used for local motion deblurring (b,d). Finally, we interpolate deblurred 
exposure images that have not been recorded from particular perspectives. Combining them results in HDR 
images for each camera perspective. These images can then be used for tone-mapped light-.eld rendering 
(g-l). In contrast to recording regular exposure sequences (g,i,k), our approach (h,j,l) reduces motion 
blur. Figures g-l compare tone-mapped wide-aperture (i.e., shallow depth-of-.eld) light-.eld renderings 
for different focus settings (indicated with the green arrows). Note, that remaining artifacts in out-of-focus 
regions are due to directional under-sampling (i.e., too few cameras in the array). Our Approach Capturing 
exposure sequences for computing high-dynamic range (HDR) images causes motion blur in case of camera 
movements. This also applies for light-.eld cameras, such as camera arrays. Im­ages composed from multiple 
blurred HDR light-.eld perspectives are also blurred. To reduce motion blur, we encode four exposure 
times in each re­peating camera quad-tuple of a camera array (assuming relatively small camera-baselines): 
Instead of recording all exposures sequen­tially for all perspective cameras (cf. .g. 1e), we apply the 
spatio­ temporal exposure pattern shown in .gures 1a,f. This reduces the overall recording interval, 
but also prevents from recording all ex­posures at all perspectives, and therefore constrains the smallest 
synthetic aperture to be at least 2 × 2. Within each camera quad­tuple, the shortest and longest exposures 
are captured only from one perspective each (1 and 4, respectively), while the two medium ex­posures 
are recorded interleaved from two perspectives (2 and 3). For each of the four different exposures (cf. 
.g. 1b) that are recorded at varying perspectives, we compute a depth map. Since these depth maps vary 
locally in quality (due to motion blur for the higher exposures, and due to low SNR for the lower exposures), 
we compile them to a single composite depth map based on a depth con.dence criteria (cf. .g. 1c). This 
composite depth map is then segmented into different depth layers. For each depth layer we track features 
in the low exposure images. The optical .ow of these fea­tures allows us to determine the local point-spread 
function (PSF) that causes motion blur in higher exposures. These PSFs are used for motion deblurring 
(i.e., deconvolution) of each recorded expo­sure image at all camera perspectives (cf. .g. 1d). By shifting 
the PSFs before deconvolution (as illustrated by the ar­rows in .gure 1f), we can receive three subframes 
for each record­ ing interval at all perspectives of each camera quad-tuple. Thus, for subsequent recording 
intervals of an HDR light-.eld video (coded exposures and deconvolution shifts as illustrated in .gure 
1f), we obtain the following exposure images at times 0, 4, 8: 1/N for per­spective 1, 1/(2 × N) and 
1/(4 × N) for perspectives 2 and 3, and 1/(8 × N) for perspective 4. Compared to classical exposure se­quencing 
(cf. .g. 1e), this leads to a ×3.75 higher frame rate. With the recorded and deblurred exposure images 
we .nally compute an enhanced composite depth map to interpolate deblurred expo­sure images for all camera 
perspectives that have not been directly recorded. Figures 1g-l illustrate tone-mapped, wide-synthetic-aperture 
(i.e., shallow depth-of-.eld) images rendered from a light .eld that was recorded during camera motion. 
Figures 1g,i,k show the results at different focus settings (green arrows indicate selected in-focus 
re­gion) with regular exposure sequences for each camera perspective (cf. .g. 1e). Figures 1h,j,l show 
the same images recorded with our coded exposures (cf. .g. 1f) and computed with the technique outlined 
above. *.rstname.lastname@jku.at Permission to make digital or hard copies of part or all of this work 
for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503454</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>63</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[GOTHIC]]></title>
		<subtitle><![CDATA[glare optimizer tool for high-dynamic-range images and content with implementation in video]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503454</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503454</url>
		<abstract>
			<par><![CDATA[<p><b>Introduction:</b> The luminance range of the sun to the night sky is approximately 14 orders of magnitude. Current display technology can present approximately three orders of magnitude, however this number is increasing as High-Dynamic-Range (HDR) technology develops to further emulate reality [Seetzen et al. 2004]. Another benefit to HDR technology is the increased bit-depth enabling the display of more information. However, a major limitation in the perception of added bit-depth is veiling glare. The increased luminance range in HDR displays have the ability to produce glare sources that can reduce the visible contrast in neighboring dark areas. This effect is especially undesirable in the visualization of scientific data and in medical images. The HDR presentation must be optimized so that the benefits of a wide luminance range are not diminished by glare in the human visual system. One important question is, what is the largest luminance range that avoids these veiling glare effects while presenting the most bit-depth? We have found that the answer is highly dependent on the spatial and luminance distribution in the image. Many models have been proposed to estimate the veiling glare in a given image. A well known model is High-Dynamic-Range Visual Difference Predictor 2 (HDR-VDP-2) [Mantiuk et al. 2011], a calibrated method able to determine the visibility of differences in HDR images. Building on a number of previous metrics of visible difference, this model operates in a broad range of viewing conditions, from scotopic to photopic vision. More importantly, HDR-VDP-2 can be used to represent the effects of visual glare in signal detection. The inputs of the HDR-VDP-2 are a luminance map of an image, a reference image, and an image with the target. The software outputs the probability of target detection accounting for various visual effects including veiling glare.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190809</person_id>
				<author_profile_id><![CDATA[82458685157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CDRH, FDA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190810</person_id>
				<author_profile_id><![CDATA[81456606270]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Fahad]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zafar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CDRH, FDA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190811</person_id>
				<author_profile_id><![CDATA[81548042577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Joel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CDRH, FDA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190812</person_id>
				<author_profile_id><![CDATA[81100317295]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Gianni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramponi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Univ. of Trieste]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190813</person_id>
				<author_profile_id><![CDATA[82459151757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Wei-Chung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CDRH, FDA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190814</person_id>
				<author_profile_id><![CDATA[81442610432]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Luigi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Albani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Barco, FIMI]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190815</person_id>
				<author_profile_id><![CDATA[81548042580]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Aldo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Badano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CDRH, FDA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[aldo.badano@fda.hhs.gov]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Choi, M., Zafar, F., Sharma, D., Cheng, W., Albani, L., and Badano, A. 2012. Does veiling glare in the human eye hinder detection in high-dynamic-range displays? <i>Journal of Display Technology</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1964935</ref_obj_id>
				<ref_obj_pid>2010324</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mantiuk, R., Kim, K., Rempel, A., and Heidrich, W. 2011. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. <i>ACM Transactions on Graphics (TOG) 30.4</i>, 40.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015797</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Seetzen, H., Heidrich, W., Stuerzlinger, W., Ward, G., Whitehead, L., Trentacoste, M., Ghosh, A., and Vorozcovs, A. 2004. High dynamic range display systems. <i>ACM Transactions on Graphics (TOG) 23.3</i>, 760--768.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 G OT H I C: Glare Optimizer Tool for High-Dynamic-Range Images and Content with Implementation in Video 
 Mina Choi, Fahad Zafar, Joel Wang Gianni Ramponi Wei-Chung Cheng Luigi Albani Aldo Badano* CDRH, FDA 
Univ. of Trieste CDRH, FDA Barco, FIMI CDRH, FDA Figure 1: GOT H IC in action for CG image and medical 
images: An HDR image is analyzed to determine each pixel contribution to the augmented contrast detection 
thresholds due to veiling glare experienced by a human subject at a chosen location in the image (see 
middle image with region-of-interest and marked target location). The pixel map of the veiling glare 
effect is shown as a red-channel overlay on the right-most images. This analysis is then iterated for 
different visualizations until a tolerance level for the degradation due to veiling glare is attained. 
The images show the application of G OTHI C to computer-generated and medical images. Credit: Image from 
CG-HDR Modi.ed HDRLighting C++ Sample. Microsoft DirectX SDK February 2012 Release, Lung Image Database 
Consortium and TG18. Introduction: The luminance range of the sun to the night sky is approximately 14 
orders of magnitude. Current display technology can present approximately three orders of magnitude, 
however this number is increasing as High-Dynamic-Range (HDR) technology develops to further emulate 
reality [Seetzen et al. 2004]. Another bene.t to HDR technology is the increased bit-depth enabling the 
display of more information. However, a major limitation in the perception of added bit-depth is veiling 
glare. The increased lu­minance range in HDR displays have the ability to produce glare sources that 
can reduce the visible contrast in neighboring dark ar­eas. This effect is especially undesirable in 
the visualization of sci­enti.c data and in medical images. The HDR presentation must be optimized so 
that the bene.ts of a wide luminance range are not di­minished by glare in the human visual system. One 
important ques­tion is, what is the largest luminance range that avoids these veiling glare effects while 
presenting the most bit-depth? We have found that the answer is highly dependent on the spatial and luminance 
distribution in the image. Many models have been proposed to es­timate the veiling glare in a given image. 
A well known model is High-Dynamic-Range Visual Difference Predictor 2 (HDR-VDP­2) [Mantiuk et al. 2011], 
a calibrated method able to determine the visibility of differences in HDR images. Building on a num­ber 
of previous metrics of visible difference, this model operates in a broad range of viewing conditions, 
from scotopic to photopic vi­sion. More importantly, HDR-VDP-2 can be used to represent the effects of 
visual glare in signal detection. The inputs of the HDR-VDP-2 are a luminance map of an image, a reference 
image, and an image with the target. The software outputs the probability of tar­get detection accounting 
for various visual effects including veiling glare. Approach: We describe a novel tool using our own 
glare model for optimizing the detection of subtle features in dark regions of high-dynamic-range (HDR) 
images by minimizing the deleterious effect of veiling glare in the human eye. Our method relies on pre­dictions 
of contrast detection thresholds based on psychophysics measurements and modeling of the HDR display 
device charac­teristics [Choi et al. 2012]. We .rst calculate the appropriate lu­ minance mapping on 
a HDR display that minimizes veiling glare effects within the display to isolate perceived glare. We 
measured *e-mail: aldo.badano@fda.hhs.gov contrast detection thresholds for a Gaussian target on a noisy 
back­ground with the presence of a veiling glare source. The veiling glare source was a concentric ring 
with varying radii and illuminances. The experimental results were used to construct the model which 
.nds the contrast detection threshold dependent on angular distance and illuminance of glare source similarly 
to the CIE disability glare function. We use the empirical glare model to develop a more ver­satile, 
image-dependent model for the assessment of complex HDR images. The model predicts human contrast detection 
threshold for any speci.ed location in the image. The image-dependent model is validated with observer 
studies for synthetic, medical, and natural images. Once HDR images (and speci.c locations within those 
im­ages) with signi.cant veiling glare effects are identi.ed, G OT H IC selects a luminance map that 
reduces the glare in the image presen­tation while attempting to maintain the widest possible luminance 
range and depending on a tolerance factor for contrast reduction. GOT H IC does not account for color 
effects or transient glare effects. Results: The .gure above shows GOTHIC in action for one com­puter 
generated image, two chest computed tomography images, and one mammogram. The intensity of the red overlay 
in the third image for each set represents relative contributions of the pixel to the glare effect. G 
OTHI C can be of practical use in applications such as gaming, video, and dynamic medical image viewers. 
We demonstrated the optimization in a computer generated HDR video sequence. It is currently being improved 
to be implemented with sub-ms execution times (a 1000x increase with respect to its cur­rent MATLAB implementation). 
Also, we are investigating a GPU­based implementation in conjunction with approaches to minimize the 
actual computation load. CH O I , M., ZA FA R , F., SH AR MA , D., CH E NG , W., AL BA NI , L., A N D 
BA DA N O , A. 2012. Does veiling glare in the human eye hinder detection in high-dynamic-range displays? 
Journal of Display Technology. MA N T I U K , R., KI M , K., RE M P E L , A., A ND HE I D RI C H , W. 
2011. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. 
ACM Transactions on Graphics (TOG) 30.4, 40. SE E T Z E N , H., HE I DR I C H , W., ST U E R ZL I NG 
E R , W., WA R D, G., WH IT E HE A D , L., TRE N TAC O S T E , M., GH O SH , A., AN D VO RO Z C OVS , 
A. 2004. High dynamic range display systems. ACM Transactions on Graphics (TOG) 23.3, 760 768. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503455</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>64</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Photorealistic aged face image synthesis by wrinkles manipulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503455</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503455</url>
		<abstract>
			<par><![CDATA[<p>Many studies on an aged face image synthesis have been reported with the purpose of security application such as investigation for criminal or kidnapped child and entertainment applications such as movie or video game.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190816</person_id>
				<author_profile_id><![CDATA[82459000757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizokawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ai.mizokawa@suou.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190817</person_id>
				<author_profile_id><![CDATA[82459246157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190818</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190819</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2343002</ref_obj_id>
				<ref_obj_pid>2342896</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tazoe, Y., Gohara, H., Maejima, A., and Morishima, S. 2012. Facial Aging Simulator Considering Geometry and Patch-tiled Texture. <i>ACM SIGGRAPH2012 Posters Article No.90</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531363</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mohammed, U., Prince, S., and Kautz, J. 2009. Visiolization: Generating Novel Facial Images. In <i>Transactions on Graphics (Proceedings of SIGGRAPH 2009)</i>, vol.28, ACM, No.57.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882269</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Perez, P., Gangnet, M., and Blake, A. 2003. Poisson Image Editing. In <i>Transactions on Graphics (Proceedings of SIGGRAPH 2003)</i>, vol. 22, ACM, 313--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Photorealistic Aged Face Image Synthesis by Wrinkles Manipulation (a)     (b)     (c) C:\Users\Ai_Mizokawa_VST\Dropbox\ai\lab\[20130213]SIGGRAPH\my_abstruct\.2.bmp 
C:\Users\Ai_Mizokawa_VST\Dropbox\ai\lab\[20130213]SIGGRAPH\.3.bmp C:\Users\Ai_Mizokawa_VST\Dropbox\ai\lab\[20130213]SIGGRAPH\.1.bmp 
C:\Users\Ai_Mizokawa_VST\Dropbox\ai\lab\[20130213]SIGGRAPH\.2.bmp C:\Users\Ai_Mizokawa_VST\Dropbox\ai\lab\[20130213]SIGGRAPH\.4.bmp 
C:\Users\Ai_Mizokawa_VST\Dropbox\ai\lab\[20130213]SIGGRAPH\fujishiro2.bmp Figure 2: Results of Tazoe 
s Method (a) Original image (24 year old), (b) Result by Visio-lization (45 year old), (c) Image normalized 
to the original geometry Ai MIZOKAWA*  Hiroki NAKAI  Akinobu MAEJIMA  Shigeo MORISHIMA Waseda University 
           1. Introduction (a)     (b)     (c)     (d)     (e) Many studies 
on an aged face image synthesis have been reported with the purpose of security application such as investigation 
for criminal or kidnapped child and entertainment applications such as movie or video game. Tazoe et 
al. [2012] proposed a facial aging technique based on Visio-lization [Mohammed et al. 2009]. Their results 
described skin texture such as rough or dull skin which helped in determining a person s age. However, 
in their method, representing wrinkles one of the most important elements in reflecting age characteristics 
is difficult because the selection of patches is influenced by lighting conditions and individual skin 
color, rather than a wrinkle s shape or edge. Additionally, it is difficult to infer the location and 
shape of future wrinkles because they depend on individual factors such as living environment, eating 
habits, and DNA. Therefore, we have to consider several possibilities of wrinkles locations. In this 
paper, we propose an aged face image synthesis method that can represent clear wrinkles by reducing the 
influence of the lighting environment and individual skin color owing to an image binarization process. 
Then, we synthesized realistic aged-related wrinkles by adding artificial freehand wrinkles onto the 
face image. Our contribution is to provide interactive tool that can create plausible aged face images 
for criminal investigation or entertainment, and to be able to represent wrinkles at any optional location 
user want to add them with photorealistic quality. (i)      (ii)    (iii)    (iv) Figure 
1: Process and Results (a) Original image with freehand wrinkles (24 year old), (b) Image normalized 
to the average face model, (c) Best matched wrinkle image selected, (d) Cut&#38;paste wrinkles of (c) 
into Figure 2(b), (e) Image normalized to the original geometry Figure 3: Extracting wrinkles from the 
database face image *e-mail: ai.mizokawa@suou.waseda.jp e-mail: shigeo@waseda.jp (i)     (ii)  
    (iii)    (iv) Figure 3: How to extract wrinkles from the database face image 2. Aged Face 
Synthesis 2.1 Reconstruction of original image: Here, assuming that, input age is 24, and objective age 
is 45. First, utilizing Tazoe s method, we divide a 2D face image into small square patches and replace 
the input patches with patches from 45 year old database. Based on the criteria of Euclidean distance 
about pixel color information, the most similar patches are selected from the database. 2.2 Wrinkle extraction: 
First, we draw freehand wrinkle lines on an original image, as shown in Figure 1(a). Then, we normalize 
it by fitting to the average geometry, and determine the rectangular area of each wrinkle, as illustrated 
by the dotted line in Figure 1(b). Next, wrinkle extraction from a face image in the database is performed 
by the following steps(Figure 3): (i) Select face images which are age at about 45 in the database (ii) 
Normalize the images to the average geometry (iii) Apply adaptive binarization and remove granular noise 
(iv) Extract wrinkles with the same rectangle size as input wrinkles 2.3 Selection of the best matched 
wrinkles: Wrinkles that are the most similar in shape to the freehand wrinkles are automatically selected 
from the database. The similarity is calculated by pixel to pixel comparison of the database s binarized 
wrinkle images and the input wrinkle image. Then, the number of pixels with equal binarized values between 
the database and input images is counted. 2.4 Pasting of wrinkles: The selected wrinkles are pasted into 
the Tazoe s result using the seamless cloning method proposed by Perez et al. [2003]. This method allows 
us to preserve the gradients of the best matched wrinkles by applying the selected wrinkles to the appropriate 
area of a face image (45 year old), as shown in Figure 1(d). 3. Result and Conclusion As illustrated 
in Figure 1, the results show that our method can add realistic wrinkles that are similar to freehand 
wrinkles. The results of Tazoe s method are shown in Figure 2. By comparing our results (Figure 1(e)) 
with Tazoe s results (Figure 2(c)), it is evident that we succeeded in generating a face that looks 45 
years old precisely. We selected the best matched wrinkles by reducing the influence of lighting conditions 
and individual skin color by adaptive binarization and consequently could generate elderly faces easily 
and interactively by adding freehand artificial lines. In the future, it will be necessary to conduct 
subjective evaluations to verify the performance of our method. References TAZOE, Y., GOHARA, H., MAEJIMA, 
A., AND MORISHIMA, S. 2012. Facial Aging Simulator Considering Geometry and Patch-tiled Texture. ACM 
SIGGRAPH2012 Posters Article No.90. MOHAMMED, U., PRINCE, S., AND KAUTZ, J. 2009. Visio-lization: Generating 
Novel Facial Images. In Transactions on Graphics (Proceedings of SIGGRAPH 2009), vol.28, ACM, No.57. 
PEREZ, P., GANGNET, M., AND BLAKE, A. 2003. Poisson Image Editing. In Transactions on Graphics (Proceedings 
of SIGGRAPH 2003), vol. 22, ACM, 313-318. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503456</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>65</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[High dynamic range imaging using coded electronic shutter]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503456</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503456</url>
		<abstract>
			<par><![CDATA[<p>High dynamic range (HDR) imaging aims to increase the dynamic range of imaging devices, capturing better representations of target scenes. Since the seminal work of Debevec and Malik [1997], tremendous progress has been achieved utilizing multiple images of different exposures that provide complementary brightness information of a scene. However, their application is limited to static scenes with no motions during the sequential capture of images, because changes between images can cause undesirable artifacts such as ghosts. Special imaging devices such as exposure-filtering masks [Nayar and Mitsunaga 2000] could reduce motion artifacts, but manufacturing costs have limited their practicality.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190820</person_id>
				<author_profile_id><![CDATA[81488663916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hojin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nucl23@postech.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190821</person_id>
				<author_profile_id><![CDATA[81409594327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Seungyong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[POSTECH]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[leesy@postech.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., and Malik, J. 1997. Recovering high dynamic range radiance maps from photographs. In <i>Proc. the 24th annual conference on Computer graphics and interactive techniques</i>, SIGGRAPH '97, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1961129</ref_obj_id>
				<ref_obj_pid>1961108</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lenzen, F., and Scherzer, O. 2011. Partial differential equations for zooming, deinterlacing anddejittering. <i>International Journal of Computer Vision (IJCV) 92</i>, 2, 162--176.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nayar, S., and Mitsunaga, T. 2000. High dynamic range imaging: spatially varying pixel exposures. In <i>Proc. CVPR 2000</i>, vol. 1, 472--479.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High Dynamic Range Imaging Using Coded Electronic Shutter* Hojin Cho and Seungyong Lee POSTECH  Figure 
1: Multiple image based HDR imaging vs. our single-shot HDR imaging for moving objects. The input image 
of our method was captured using a prototype camera equipped with a coded electronic shutter. Both results 
are equally tone-compressed for visualization. Our method is robust to motions (e.g., object motions 
and camera shakes) and does not suffer from rolling shutter artifacts such as skew or wobble. 1 Introduction 
High dynamic range (HDR) imaging aims to increase the dynamic range of imaging devices, capturing better 
representations of tar­get scenes. Since the seminal work of Debevec and Malik [1997], tremendous progress 
has been achieved utilizing multiple images of different exposures that provide complementary brightness 
in­formation of a scene. However, their application is limited to static scenes with no motions during 
the sequential capture of images, because changes between images can cause undesirable artifacts such 
as ghosts. Special imaging devices such as exposure-.ltering masks [Nayar and Mitsunaga 2000] could reduce 
motion artifacts, but manufacturing costs have limited their practicality. In this paper, we propose 
a novel HDR imaging method using a coded electronic shutter (CES) that allows simultaneous capture of 
spatially varying exposures in a single image. Recently, electronic shutter has been widely adopted in 
digital cameras, where the me­chanical front curtain shutter (Fig. 2a) is replaced by the electronic 
reset signal. On top of the electronic shutter, CES can be easily implemented by triggering row-wise 
different reset signals (Fig. 2b). As the speed of row-reset is comparable to that of the physical shutter, 
electronic shutters do not suffer from rolling shutter arti­facts such as skew or wobble. The major bene.t 
of using CES in HDR imaging is signi.cant reduction of ghosts due to the concur­rent capture of multiple 
exposures. Using a single image of multiple exposures acquired by CES, we extend its dynamic range without 
extracting sub-images of each exposure while preventing jaggy ar­tifacts and preserving .ne details. 
We adapt several image .ltering algorithms based on variational formulations to suppress possible artifacts. 
As shown in Fig. 1, our method can robustly produce a high quality HDR image without artifacts. 2 Algorithms 
The key idea of our approach is exploiting the multiple exposure pixels obtained using CES to extend 
the dynamic range. Our timing function for HDR imaging consists of two exposure values (Fig. 2b), and 
accordingly we obtain I = {IL, IS } from CES where IL and IS represent long-and short-exposure pixels, 
respectively. *This work was supported in part by Basic Science Research Program of NRF (2013R1A1A2011692). 
e-mail:{nucl23, leesy}@postech.ac.kr Legend (a) (b) Figure 2: Conceptual timing charts for camera shutter 
functions. (a) Two-curtain focal-plane shutter and (b) Coded electronic shutter. Extracting two subimages 
IS and IL can lose .ne-details, so we directly manipulate I itself without extracting the subimages. 
Since IS and IL have different exposure values, we .rst need to compensate their different exposures. 
To do this, we perform pho­tometric calibration using the response curve of an image sensor, where the 
curve can be precomputed. As a result of calibrating the short-exposure to become long-exposure, we obtain 
{IL, IS.L}. Since IS.L may contain noise due to the inaccuracy of pho­tometric calibration, we further 
remove the calibration error us­ing anisotropic diffusion, based on statistical analysis of the error. 
Then, as IL and IS.L are expected to have the same long exposure value, we restore over-/under-exposed 
areas using the complemen­tary information of IL and IS.L. For example, the over-exposed pixels of IL 
are recovered using nearby pixels of IS.L. We pro­pose a novel bilateral .lter for this step, which considers 
edge di­rection and the state of pixels (i.e., under-/well-/over-exposed). Fi­nally, since IL may contain 
longer motions than IS.L when a dy­namic scene is caputred, we further surpress the potential motion 
artifacts using mean curvature .ow like diffusion algorithm derived from a variational formulation [Lenzen 
and Scherzer 2011]. As shown in Fig. 1, our method is robust to motions and produces high quality HDR 
images. References DE B E V E C , P. E., A N D MA LI K , J. 1997. Recovering high dynamic range radiance 
maps from photographs. In Proc. the 24th annual conference on Computer graphics and interactive techniques, 
SIGGRAPH 97, 369 378. LE N ZE N , F., A ND SC H E R Z E R , O. 2011. Partial differential equations for 
zooming, deinterlacing anddejittering. International Journal of Computer Vision (IJCV) 92, 2, 162 176. 
NAYA R , S., A N D MI TS U NAGA , T. 2000. High dynamic range imaging: spatially varying pixel exposures. 
In Proc. CVPR 2000, vol. 1, 472 479. Permission to make digital or hard copies of part or all of this 
work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503457</section_id>
		<sort_key>720</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Mixed reality and games]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>2503458</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>66</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A photorealistic compositing tool for mobile augmented reality]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503458</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503458</url>
		<abstract>
			<par><![CDATA[<p>We present an interactive tool that enables photorealistic compositing for mobile augmented reality (AR), using integrated sensors to drive a real-time rendering rig. The system links existing technologies to new devices, resulting in a significantly enhanced experience compared to conventional compositing and AR applications.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[application poster]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[context-awareness]]></kw>
			<kw><![CDATA[mobile]]></kw>
			<kw><![CDATA[photorealistic compositing]]></kw>
			<kw><![CDATA[real-time]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190822</person_id>
				<author_profile_id><![CDATA[82458810157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ricardo]]></first_name>
				<middle_name><![CDATA[Rendon]]></middle_name>
				<last_name><![CDATA[Cepeda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Bristol]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rr1461@my.bristol.ac.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566636</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Perlin, K. 2002. Improving Noise. In <i>Proceedings of the 29th Annual Conference on Computer Graphics and Interactive Techniques</i>, SIGGRAPH 2002, 681--682]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reda, I. and Andreas, A. 2003. Solar Position Algorithm for Solar Radiation Applications. <i>NREL Report No. TP-560-34302</i>, Revised January 2008]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Reinhard, E., Oguz Akyuz, A., Colbert, M., Hughes, C., and O'Connor, M. 2004. Real-Time Color Blending of Rendered and Captured Video. In <i>I/ITSEC</i>, 2004: 1502:1--1502:9]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Taylor, P. 2001. Per-Pixel Lighting. <i>Microsoft MSDN Driving DirectX Column</i>, November 13 2001]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503459</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>67</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA["Tworlds"]]></title>
		<subtitle><![CDATA[twirled worlds for multimodal 'padiddle' spinning &#38; tethered 'poi' whirling]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503459</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503459</url>
		<abstract>
			<par><![CDATA[<p>Modern smartphones and tablets have magnetometers that can be used to detect yaw, which data can be distributed to adjust ambient media. Either static (pointing) or dynamic (twirling) modes can be used to modulate multimodal displays, including 360&#176; imagery and virtual environments. Azimuthal tracking especially allows control of horizontal planar displays, including panoramic and turnoramic imaged-based rendering, spatial sound, and the position of avatars, virtual cameras, and other objects in virtual environments such as Alice, as well as rhythmic renderings such as musical sequencing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190823</person_id>
				<author_profile_id><![CDATA[81406592137]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cohen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mcohen@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190832</person_id>
				<author_profile_id><![CDATA[81474684002]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rasika]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ranaweera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[d8121104@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190833</person_id>
				<author_profile_id><![CDATA[81500655264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kensuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m5161107@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190834</person_id>
				<author_profile_id><![CDATA[81485655460]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Yuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m5151107@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190835</person_id>
				<author_profile_id><![CDATA[81500649837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Shun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Endo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s1160037@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190836</person_id>
				<author_profile_id><![CDATA[82458662157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s1170010@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190837</person_id>
				<author_profile_id><![CDATA[82459115457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Tetsunobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s1170052@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190838</person_id>
				<author_profile_id><![CDATA[82458645257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Yukihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s1170119@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190839</person_id>
				<author_profile_id><![CDATA[82459257657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s1170144@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190824</person_id>
				<author_profile_id><![CDATA[82459294057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Anzu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s1180027@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190825</person_id>
				<author_profile_id><![CDATA[81340493524]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Juli&#225;n]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Villegas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Spatial Media Group, University of Aizu; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[julian@u-aizu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190826</person_id>
				<author_profile_id><![CDATA[82459140057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Yong]]></first_name>
				<middle_name><![CDATA[Ping]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eyes, JAPAN; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yongping-c@nowhere.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190827</person_id>
				<author_profile_id><![CDATA[81553559556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Sascha]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holesch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eyes, JAPAN; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sascha@nowhere.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190828</person_id>
				<author_profile_id><![CDATA[82458625357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>14</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamadera]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Eyes, JAPAN; Aizu-Wakamatsu, Fukushima; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamadera@nowhere.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190829</person_id>
				<author_profile_id><![CDATA[81553935356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>15</seq_no>
				<first_name><![CDATA[Hayato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GClue; Aizu-Wakamatsu, Fukushima & Tokyo; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hayato@gclue.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190830</person_id>
				<author_profile_id><![CDATA[82458656157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>16</seq_no>
				<first_name><![CDATA[Yasuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GClue; Aizu-Wakamatsu, Fukushima & Tokyo; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[saito@gclue.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190831</person_id>
				<author_profile_id><![CDATA[82458953157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>17</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GClue; Aizu-Wakamatsu, Fukushima & Tokyo; Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[akira@gclue.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cohen, M., Ranaweera, R., Nishimura, K., Sasamoto, Y., Oyama, T., Nishikawa, Y., Ohashi, T., Kanno, R., Nakada, A., Yamadera, J., Holesch, S., Chen, Y. P., Sasaki, A., and Ito, H. 2012. "Whirled Worlds": twirling interface for "mobile ambient," "practically panoramic" whole-body entertainment. In <i>DCE: &#60;b&#62;D&#60;/b&#62;igital &#60;b&#62;C&#60;/b&#62;ontents &#60;b&#62;E&#60;/b&#62;xpo</i>, Digital Content Association of Japan. www.dcexpo.jp/2012/en/program/exhibition/detail.php#IT201222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477911</ref_obj_id>
				<ref_obj_pid>1477862</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cohen, M. 2008. Integration of laptop sudden motion sensor as accelerometric control for virtual environments. In <i>VRCAI: Proc. ACM Int. Conf. on &#60;b&#62;V&#60;/b&#62;irtual-&#60;b&#62;R&#60;/b&#62;eality &#60;b&#62;C&#60;/b&#62;ontinuum and Its &#60;b&#62;A&#60;/b&#62;pplications in &#60;b&#62;I&#60;/b&#62;ndustry</i>. isbn-13 978-1-60558-335-8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tworlds : Twirled Worlds for Multimodal Padiddle Spinning &#38; Tethered Poi Whirling Michael Cohen, 
Rasika Ranaweera, Kensuke Nishimura, Yuya Sasamoto, Shun Endo, Tomohiro Oyama, Tetsunobu Ohashi, Yukihiro 
Nishikawa, Ryo Kanno, Anzu Nakada, &#38; Juli´an Villegas* Spatial Media Group, University of Aizu; Aizu-Wakamatsu, 
Fukushima; Japan Yong Ping Chen, Sascha Holesch, &#38; Jun Yamadera Eyes, JAPAN; Aizu-Wakamatsu, Fukushima; 
Japan Hayato Ito, Yasuhiko Saito, &#38; Akira Sasaki GClue; Aizu-Wakamatsu, Fukushima &#38; Tokyo; Japan 
 Figure 1: Avatar ambidexterity allows affordance attitude alignment: As the subjective camera orbits 
in an inspection gesture between mirrored and tethered perspectives around an objective character, which 
is continuously animated by the connection between real-life motion capture and mixed reality environment 
rigging, the avatar automatically switches manipulating hand to preserve intuitive natural association. 
Modern smartphones and tablets have magnetometers that can be used to detect yaw, which data can be distributed 
to adjust ambient media. Either static (pointing) or dynamic (twirling) modes can be used to modulate 
multimodal displays, including 360. imagery and virtual environments. Azimuthal tracking especially allows 
control of horizontal planar displays, including panoramic and turnoramic imaged-based rendering, spatial 
sound, and the position of avatars, virtual cameras, and other objects in virtual environments such as 
Alice,1 as well as rhythmic renderings such as musical sequencing. Embedding such devices into twirling 
affordances allows padiddle -style interfaces, spinning a .atish object, and poi ­style interfaces, whirling 
a tethered device, for novel interaction techniques. One can free spin padiddle a tablet, or embed such 
a device in a larger object such as a pillow. Poi, originally a M¯ aori performance art featuring whirled 
weights, also combines elements of dance and juggling. It has been embraced by modern festival cul­ture 
(especially rave-style electronic music events), including ex­tension to glowstringing and .re twirling, 
in which a glowstick or burning wick is whirled at the end of a tether. Objects in mixed reality environments 
can be repositioned accord­ing to spinning or whirling, and monitored in endocentric teth­ered or mirrored 
V R perspectives (switching handedness be­tween them). A novel feature of our rigging, as shown in Figure 
1, is that the avatars are ambidextrous: although a human player typ­ically uses a particular hand (usually 
the right), as the viewpoint moves between re.ected, frontal mirror and projected, dorsal tethered perspectives, 
the puppet dynamically switches hands, even while the prop is whirling to allow visual and logical align­ment 
between avatar and human actor facing the display. (To con­summate the perspective-invariant visual alignment, 
we are work­ing on an extension of this newly developed feature to also support elegant phase modulation 
of the mixed reality prop, and will have it deployed by the summer.) Table 1: Multimodal display and 
control roomware audio and M ID I-sequenced polyphonic music stereo panning spatial sound and music visual 
 QTVR I B R: 360. photo-realistic imagery panoramas ( panos, inside-looking-out) turnoramas ( turnos 
or object movies, outside-looking-in) VR scenes: realtime C G Alice (v. 3) haptic (touch &#38; proprioception) 
static pointing dynamic twirling spinning (padiddling) whirling (poi) Padiddling and poi interfaces 
reveal the potential for embodied in­teraction exploiting the happy alignment of gravity-oriented hor­izontal 
twirling gestures, the latitudinal geomagnetic .eld, and the horizontally favored visual .eld and auditory 
directionaliza­tion acuity. These active exertoys represent physical affordances for whole body interaction, 
practically panoramic interfaces that can be enjoyed as location-based entertainment systems for cross­platform, 
mobile ambient applications. References CO H E N , M., RANAW E E R A, R., NIS H I MUR A , K., SASA M 
OTO, Y., OYAM A , T., NI S HI K AWA , Y., OHA S H I, T., KANN O , R., NA K ADA , A., YA M A DE R A , 
J., HOL E S CH , S., CH E N , Y. P., SA S AKI , A., A N D ITO , H. 2012. Whirled Worlds : twirling interface 
for mobile ambient, practically panoramic whole­body entertainment. In DCE: Digital Contents Expo, Digital 
Content Association of Japan. www.dcexpo.jp/2012/en/ program/exhibition/detail.php#IT201222. * e·mail: 
{mcohen, d8121104, m5161107, m5151107, s1160037, s1170010, s1170052, s1170119, s1170144, s1180027, julian}@u-aizu.ac.jp 
 e·mail: {yongping-c, sascha, yamadera}@nowhere.co.jp e·mail: {hayato, saito, akira}@gclue.jp 1www.alice.org 
 CO H E N , M. 2008. Integration of laptop sudden motion sen­sor as accelerometric control for virtual 
environments. In VR-CAI: Proc. ACM Int. Conf. on Virtual-Reality Continuum and Its Applications in Industry. 
I S BN-13 978-1-60558-335-8. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of this work must be honored. For all other uses, contact the Owner/Author. 
SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author. ACM 978-1-4503-2261-4/13/07 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503460</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>68</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Advanced interfaces to stem the data deluge in mixed reality]]></title>
		<subtitle><![CDATA[placing human (un)consciousness in the loop]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503460</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503460</url>
		<abstract>
			<par><![CDATA[<p>We live in an era of data deluge and this requires novel tools to effectively extract, analyze and understand the massive amounts of data produced by the study of natural and artificial phenomena in many areas of research.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190840</person_id>
				<author_profile_id><![CDATA[81460657492]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alberto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Betella]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[alberto.betella@upf.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190847</person_id>
				<author_profile_id><![CDATA[82458716557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Enrique]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mart&#237;nez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190848</person_id>
				<author_profile_id><![CDATA[81555987356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Riccardo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zucca]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190849</person_id>
				<author_profile_id><![CDATA[82458807457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Xerxes]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Arsiwalla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190850</person_id>
				<author_profile_id><![CDATA[81436600877]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Omedas]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190851</person_id>
				<author_profile_id><![CDATA[81421597848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Sytse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wierenga]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190852</person_id>
				<author_profile_id><![CDATA[81421599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Anna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190853</person_id>
				<author_profile_id><![CDATA[81384607320]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>8</seq_no>
				<first_name><![CDATA[Johannes]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wagner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Augsburg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190854</person_id>
				<author_profile_id><![CDATA[81490692106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>9</seq_no>
				<first_name><![CDATA[Florian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lingenfelser]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Augsburg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190841</person_id>
				<author_profile_id><![CDATA[81100557226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>10</seq_no>
				<first_name><![CDATA[Elisabeth]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Andr&#233;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Augsburg, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190842</person_id>
				<author_profile_id><![CDATA[81550971056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>11</seq_no>
				<first_name><![CDATA[Daniele]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mazzei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pisa, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190843</person_id>
				<author_profile_id><![CDATA[81447600845]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>12</seq_no>
				<first_name><![CDATA[Alessandro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tognetti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pisa, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190844</person_id>
				<author_profile_id><![CDATA[81318498140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>13</seq_no>
				<first_name><![CDATA[Antonio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lanat&#224;]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pisa, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190845</person_id>
				<author_profile_id><![CDATA[81464649331]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>14</seq_no>
				<first_name><![CDATA[Danilo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[De Rossi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pisa, Italy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190846</person_id>
				<author_profile_id><![CDATA[81100464259]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>15</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[F. M. J.]]></middle_name>
				<last_name><![CDATA[Verschure]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SPECS, Universitat Pompeu Fabra, Spain and ICREA, Instituci&#243; Catalana de Recerca i Estudis Avan&#231;ats, Spain]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[paul.verschure@upf.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bernardet, U. et al. 2010. The eXperience Induction Machine: A New Paradigm for Mixed-Reality Interaction Design and Psychological Experimentation. In The <i>Engineering of Mixed Reality Systems</i>, Human-Computer Interaction Series. Springer, 357--379.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gerhard, S. et al. 2011. The Connectome Viewer Toolkit: An Open Source Framework to Manage, Analyze, and Visualize Connectomes. <i>Frontiers in neuroinformatics</i> 5:3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Advanced Interfaces to Stem the Data Deluge in Mixed Reality: Placing Human (un)Consciousness in the 
Loop Alberto Betella*inez*, Riccardo Zucca*, Xerxes D. Arsiwalla*, Pedro Omedas* , Enrique Mart´  Sytse 
Wierenga*, Anna Mura*, Johannes Wagner , Florian Lingenfelser , Elisabeth Andr ´eDaniele Mazzei , Alessandro 
Tognetti , Antonio Lanat a` , Danilo De Rossi , Paul F. M. J. Verschure* § *SPECS, Universitat Pompeu 
Fabra, Spain Human Centered Multimedia, University of Augsburg, Germany Research Centre E. Piaggio , 
University of Pisa, Italy §ICREA, Instituci ´¸ats, Spain o Catalana de Recerca i Estudis Avanc  Figure 
1: (a) Exploration of a large dataset in the eXperience Induction Machine (XIM) using explicit and implicit 
interaction. (b) Visual­ization of a human connectome dataset composed of ~30k connections and ~1k nodes. 
(c) Mean structural score of the participants exposed to the Connectome Viewer and to the XIM: error 
bars represent the SD. (d) The glove prototype measures the user s physiological state. 1 Introduction 
We live in an era of data deluge and this requires novel tools to ef­fectively extract, analyze and understand 
the massive amounts of data produced by the study of natural and arti.cial phenomena in many areas of 
research. We built a mixed reality system that uses multi-modal input and output and permits embodied 
interaction with large datasets. One of the applications of our system is in the exploration of the human 
brain connectome (Fig. 1b): the network of nodes and connections that de.nes the key information .ow 
in the brain. With our sys­tem the user can be fully immersed in this complex data seeking to understand 
their dynamics and to discover new patterns. 2 Methods The novelty of our approach consists in the introduction 
of sub­conscious states of the user to boost the exploration process. For this, we use the mixed reality 
space eXperience Induction Machine (XIM) [Bernardet et al. 2010] along with wearable and unobtrusive 
sensor systems that assess the user s physiological state. With our system the user can study the multi-dimensional 
organi­zation of the brain and complex networks in general (Fig. 1a) by physically moving in the data 
space and by exploring different brain regions using natural hand gestures. The function of the brain 
is closely coupled to its structure. For this reason we coupled the structural representation of the 
network with a real-time neuronal network simulator. This allows the user to stimulate speci.c areas 
and observe the resulting activation that is propagating through the network, leading to an appreciation 
of structural and functional interaction. Additionally, we can operate through intuitive gestures on 
the pa­rameters of the system and .lter the number of visible connections by strength or complexity. 
We provide an ecological form of interaction since the user can lit­erally grab data clusters and manipulate 
them. The glove (Fig. 1d) transduces the user s explicit signals and measures electrodermal activity. 
In addition, we measure heart rate and respiration with *e-mail: {alberto.betella, paul.verschure}@upf.edu 
wearable sensors. These implicit responses are analyzed in real time to detect the user s interest and 
suggest relevant associations in the dataset. Our system can thus provide a discovery map of the user 
by highlight­ing areas where interest was peaked or rather diminished and by guiding the user to new 
relevant locations in the data space. 3 Results We compared our system to the Connectome Viewer [Gerhard 
et al. 2011], a state of the art software for neuroimaging network vi­sualization and analysis. 20 participants 
(mean age 27.3 ±3.45) di­vided in two groups, were asked to explore a complex connectome structural dataset. 
The .rst group was exposed to the Connectome Viewer using a common PC, while the second group experienced 
the connectome data in the XIM. We designed a questionnaire to measure the structural understand­ing 
of the network. Participants performed signi.cantly better (in terms of structural score) using our system 
(mean = 4.30 ±0.949) as opposed to the Connectome Viewer (mean = 2.80 ±1.619) (p < 0.05, Fig. 1c). 4 
Future Improvements This active exploration process will be further guided by a synthetic Sentient Agent 
that will enhance our system with suggestions based on the collective experience of past users and a 
deep understanding of the physiological signatures of insight and discovery. Acknowledgements CEEDS 
(FP7-ICT-2009-5) grant agreement n. 258749. References BERNARDET, U. et al. 2010. The eXperience Induction 
Machine: A New Paradigm for Mixed-Reality Interaction Design and Psy­chological Experimentation. In The 
Engineering of Mixed Reality Systems, Human-Computer Interaction Series. Springer, 357 379. GERHARD, 
S. et al. 2011. The Connectome Viewer Toolkit: An Open Source Framework to Manage, Analyze, and Visualize 
Con­nectomes. Frontiers in neuroinformatics 5:3. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use isgranted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>CEEDS (FP7-ICT-2009-5)</funding_agency>
			<grant_numbers>
				<grant_number>258749</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503461</article_id>
		<sort_key>760</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>69</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[An estimation method for blurring effect in augmented reality]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503461</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503461</url>
		<abstract>
			<par><![CDATA[<p>The perceptual issues in Augmented Reality (AR) systems, is drawing more and more attention these days. Among the perceptual issues, the incorrect depth interpretations degrade user experience significantly. One way to improve the depth interpretations in AR systems is to improve the consistency of the real scene and the virtual objects by using the depth cue method based on blurring effect. In the research of [Okumura 2006], a blur rendering method based on measuring the defocusing on the AR marker from the captured real scene has been proposed. However, the blur effect could only be rendered on the marker position where the blur in the real scene is measured. This limits the practicality of the system since in many applications the virtual objects are out of the marker's range or even moving. In our work, a new method to estimate the degree of blur is proposed. This method makes it possible to calculate the degree of blur that varies spatially with the scene.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190855</person_id>
				<author_profile_id><![CDATA[81555962356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xueting]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[linxueting.sjtu@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190856</person_id>
				<author_profile_id><![CDATA[81100412426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takefumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ogawa@nc.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1514239</ref_obj_id>
				<ref_obj_pid>1514201</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Okumura, B., Kanbara, M., and Yokoya, N. 2006. <i>Augmented Reality Based On Estimation of Defocusing And Motion Blurring from Captured Images. In Proc. of the 5th IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR'06)</i>, 219--225.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Estimation Method for Blurring Effect in Augmented Reality Lin Xueting* and Takefumi Ogawa The University 
of Tokyo  (a) (b) Figure 1: (a) Virtual cubes rendered with no blurring effect, (b) Virtual cubes rendered 
with the blurring effect estimated by our method. 1 Introduction The perceptual issues in Augmented 
Reality (AR) systems, is drawing more and more attention these days. Among the perceptual issues, the 
incorrect depth interpretations degrade user experience significantly. One way to improve the depth interpretations 
in AR systems is to improve the consistency of the real scene and the virtual objects by using the depth 
cue method based on blurring effect. In the research of [Okumura 2006], a blur rendering method based 
on measuring the defocusing on the AR marker from the captured real scene has been proposed. However, 
the blur effect could only be rendered on the marker position where the blur in the real scene is measured. 
This limits the practicality of the system since in many applications the virtual objects are out of 
the marker s range or even moving. In our work, a new method to estimate the degree of blur is proposed. 
This method makes it possible to calculate the degree of blur that varies spatially with the scene. 
2 Our Approach Based on the thin lens model, we found that as long as the focus length (f), the diameter 
(D) of the lens, the PSF (point spread function) of one point in the image and its position in the real 
scene, the PSF of another point on a known position could be estimated. In order to conduct this estimation, 
we introduce a one­time checkerboard registration method. The checkerboard is first detected and the 
parameters of the camera, such as position and tilting angle, are registered. The blur parameter (the 
blur circle radius of a point in the image, written as . in Function (1)) is calculated by measuring 
the intensity of pixels perpendicular to the black-white edge on the checkerboard. As shown in Function 
In this can be calculated. . . are known,..and.. ,..(1), when way, the blur parameters in the whole image 
view can be estimated by just offering the spatial information. After the estimation process, we render 
the estimated blur parameter to the virtual objects in the AR system by applying Gaussian filter. A prototype 
of the proposed AR system was implemented in our work. . ..     ... .... . ..... ............. . 
..... ........ where . is the distance between a point in the real scene and the lens, . the distance 
between the lens and the image plane. . is an *e-mail: linxueting.sjtu@gmail.com e-mail: ogawa@nc.u-tokyo.ac.jp 
 introduced parameter which could be calculated by the CCD sensor size divided by the image resolution. 
 3 Evaluation and Future Work To evaluate our estimation method, we compared the value of the measured 
blur parameters with the value of the estimated blur parameters (estimation value is based on part of 
the measurement data) on the checkerboard and found that the value of the estimated blur parameters are 
larger than the value of the measured blur parameters. With the increasing distance to the focus plane, 
the difference of the value increases. The imperfection of the imaging system is the most plausible reason 
to explain this difference. In order to know how the users would perceive the different blur parameters 
as a depth cue, we invited 17 student observers to check the difference between the same virtual textures 
rendered under the measured blur parameters and the estimated blur parameters. The results showed that 
with the increasing distance to the focus plane, the observers are more likely to feel that the textures 
rendered with the estimated blur parameters show a better consistency with the background, and some even 
reported that they could tell which one seems farther in depth. The observers also found that the brightness 
of the virtual texture (the brightness of the virtual texture changes after filtering) affects their 
perception to some extent. In the future work, the rendering method will be modified by making up the 
loss of the brightness caused by filtering. The motion blurring and the blur effect caused by lighting 
or other environmental factors will also be studied. This research was supported in part by a Grant-in-Aid 
for Scientific Research (C) numbered 25330227 by the Japan Society for the Promotion of Science (JSPS). 
 References OKUMURA, B., KANBARA, M., AND YOKOYA, N. 2006. Augmented Reality Based On Estimation of Defocusing 
And Motion Blurring from Captured Images. In Proc. of the 5th IEEE and ACM International Symposium on 
Mixed and Augmented Reality (ISMAR 06), 219-225. Permission to make digital or hard copies of part or 
all of this work for personal or classroom use isgranted without fee provided that copies are not made 
or distributed for commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for third-party components of thiswork must be honored. For all other uses, contact 
the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 
978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503462</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>70</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Asymmetrical gameplay across heterogeneous devices]]></title>
		<subtitle><![CDATA[designing a lexicon for cross-platform development]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503462</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503462</url>
		<abstract>
			<par><![CDATA[<p>Mobile devices have become a major market consideration for developers in recent years, as smartphones and tablets become part of a new technological and social space. However, game experiences on mobile platforms still don't offer an acceptable level of interconnectivity across diverse ecosystems.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190857</person_id>
				<author_profile_id><![CDATA[82459335357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Speck]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190858</person_id>
				<author_profile_id><![CDATA[82459041457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Diefenbach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Asymmetrical Gameplay Across Heterogeneous Devices: Designing a Lexicon for Cross-Platform Development 
 Robert Speck, Paul J. Diefenbach, Ph.D.   Figure 1. Heterogeneous experience mockups 1. Introduction 
 Mobile devices have become a major market consideration for developers in recent years, as smartphones 
and tablets become part of a new technological and social space. However, game experiences on mobile 
platforms still don t offer an acceptable level of interconnectivity across diverse ecosystems. This 
project aims to demonstrate that it is possible to develop a compelling multiplayer game experience that 
works in a heterogeneous environment. It explores the potential of cross-platform development tools to 
create asymmetrical gameplay between different devices, and seeks to develop a set of techniques and 
guidelines to be used in designing heterogeneous gaming experiences. Research has been conducted into 
existing solutions for cross-platform play. 2. Approach Methods to address the limitations of current 
cross-platform ecosystems have been explored. In support of this research, a multiplayer collaborative 
game experience has been created, featuring an emphasis on seamless design and user interactions tailored 
to device features and constraints. The game joins players wirelessly and examines the role of users 
and user interaction techniques in a collaborative and/or antagonistic experience. Current trends in 
game design for popular mobile games has been noted and integrated into the prototype experience, and 
used as a basis for examining the impact of the presence of mobile devices in a shared game experience. 
  Figure 2. Representative prototype images (PC, tablet, smartphone) As a result, I have defined the 
issues in the development of heterogeneous gameplay: hardware constraints, game experience, network communication, 
and development considerations. Different approaches to solving these issues have been explored. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503463</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>71</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Attention guiding principles in 3D adventure games]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503463</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503463</url>
		<abstract>
			<par><![CDATA[<p>Computer game design lacks a language for visual narrative principles in ways similar to those in architecture, film, and theme park design. We develop visual narrative methods in which spatial composition principles enhance goal direction attention within the overarching level structure of computer adventure games. Based on our observation of goal-directed attention game design patterns in existing 3D adventure games, we attempt to define a language that game designers can utilize to prototype levels more efficiently and apply them to a testing scenario.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190859</person_id>
				<author_profile_id><![CDATA[82459174857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Glenn]]></first_name>
				<middle_name><![CDATA[Joseph]]></middle_name>
				<last_name><![CDATA[Winters]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University in Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[glenn.j.winters@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P4190860</person_id>
				<author_profile_id><![CDATA[81488646953]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jichen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Drexel University in Philadelphia, PA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jichen.zhu@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bj&#246;rk, S., and Holopainen, J. 2005. <i>Patterns In Game Design</i>. Charles River Media Game Development Series. Charles River Media.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836141</ref_obj_id>
				<ref_obj_pid>1836135</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Milam, D., and Nasr, M. S. E. 2010. Design patterns to guide player movement in 3d games. In <i>Proceedings of the 5th ACM SIGGRAPH Symposium on Video Games</i>, ACM, New York, NY, USA, Sandbox '10, 37--42.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Attention Guiding Principles in 3D Adventure Games Glenn Joseph Winters and Dr. Jichen Zhu {glenn.j.winters, 
jichen.zhu}@gmail.com Drexel University in Philadelphia, PA Figure 1: Images from Dear Esther, Uncharted 
3, and Journey. Abstract Computer game design lacks a language for visual narrative princi­ples in ways 
similar to those in architecture, .lm, and theme park design. We develop visual narrative methods in 
which spatial com­position principles enhance goal direction attention within the over­arching level 
structure of computer adventure games. Based on our observation of goal-directed attention game design 
patterns in ex­isting 3D adventure games, we attempt to de.ne a language that game designers can utilize 
to prototype levels more ef.ciently and apply them to a testing scenario. 1 Problem Designers are constructing 
experiences utilizing visual narrative principles that have not been de.ned like the standard cues to 
help formalize the level design process and allow for ef.cient level con­struction. Milam and El Nasr 
introduce game mechanic concepts regarding design patterns that guide a player s movement in 3D games 
[Milam and Nasr 2010]. Different from our study, their pat­ tern results focus on literal visual guidance. 
Bjork et al. discuss several hundred of game design patterns but lack focus on speci.c level design construction 
terminology [Bj ¨ ork and HOLOPAINEN 2005]. 2 Our Solution For the purpose of this project we focused 
on structural composi­tion principles. More speci.cally how does shape, space, and form relate to one 
another and create attention to certain areas of a level. By utilizing these principles the designer 
can shift the player s at­tention by taking into consideration her surroundings. Contrasting Shape Principle: 
A silhouette contrasts with the ad­jacent environments overall shape structure; often the picturesque 
versus the rigid line. Through the use of such contrast, designers can employ silhouette to emphasize/de-emphasize 
certain proces­sional goals. This principle focuses attention by the .gural qual­ity of the silhouette 
shape and also by affecting the environmental boundary around the shape. This can be seen in Figure 1.1. 
Framed Structure Principle: A frame may occlude foreground in­formation, focusing attention on a midground 
or background view­point. More simply, a framing device may simply enframe a view without occlusion suggesting 
an inside and outside to this .gure. This principle focuses attention by heightening the legibility of 
the subset view. This can be seen in Figure 1.2. Directional Line Principle: A patterning of repetitive 
lines and/or edges de.nes a visual, actual and/or metaphoric pathway(s) linking foreground, midground, 
and background through foreshortening. The principle focuses attention by de.ning an implied perspective 
view thought the diminishing size of repetitive patterning. This can be seen in Figure 1.3. Shifting 
Elevation Principle: A spatial relationship between ground plane and line of sight. The processional 
sequence is char­acterized as below, in line, or above an implied horizontal, often with diagonal or 
parallel relationships of succeeding ground planes. The principle focuses attention by manipulating the 
relationship of the ground plane in relation to foreground, mid-ground and back­ground elements. This 
can be seen in Figure 1.4. Structural Exaggeration Principle: Exaggerated structures con­trast their 
surroundings to showcase emphasis and direct the player attention towards her next goal. The pattern 
creates attention on the area by breaking the horizontal plane created when structures are similar in 
scale in Y coordinates. This can be either used to create the end goal or a subset of goals within the 
designer s path and can be seen in Figure 1.5. The study was organized further within two separate levels. 
Each contained six zones which evaluated if the principle s hypotheses had any in.uence on the user s 
decision making process. In a post interview, player s choices were observed via heat mapping tech­nology 
and asked if there was any reason why they chose one di­rection over another.  3 Future Development 
It would be bene.cial to take into consideration more external deci­sion making factors such as cultural 
in.uences. As already men­tioned, El Nasar and Milam s work proved to be bene.cial and we feel that combining 
their patterns with our results may provide stronger points in de.ning what visual narrative means to 
3D com­puter adventure games. A more procedural approach would provide a better detailed analysis. Furthermore, 
these principals can be ana­lyzed in relation to the overall story arcs similar to the effectiveness 
of Journey.  References BJ ¨ O RK, S., A N D HOLOPAINEN, J. 2005. Patterns In Game De­sign. Charles 
River Media Game Development Series. Charles River Media. MILAM , D., A N D NASR , M. S. E. 2010. Design 
patterns to guide player movement in 3d games. In Proceedings of the 5th ACM SIGGRAPH Symposium on Video 
Games, ACM, New York, NY, USA, Sandbox 10, 37 42. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503464</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>72</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Depth image based cloth deformation for virtual try-on]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503464</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503464</url>
		<abstract>
			<par><![CDATA[<p>The affordability of RGBD cameras has pushed the retail shopping to the next level with the Virtual Try-on (VTO) application. Existing approaches [Giovanni et al. 2012], [Kevelham and Magnenat-Thalmann 2012] use skinning whereas systems like FitNect augment skinning with cloth simulation. Alternatively, image-based approach [Zhou et al. 2012] utilizes video clips for virtual try-on. In all of the existing approaches, the user's depth profile is not taken into consideration. Therefore, the user does not feel a good fit. Using customized avatars [Yuan et al. 2012] solves the fitting issue but it requires an offline user-look-alike avatar creation stage which is improper for walk-through scenarios. Furthermore, accurate physics based simulation of a 3D garment on a detailed avatar model is very time consuming.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190861</person_id>
				<author_profile_id><![CDATA[82459137257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Muhammad]]></first_name>
				<middle_name><![CDATA[Mobeen]]></middle_name>
				<last_name><![CDATA[Movania]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, A* Star, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[movaniamm@i2r.a-star.edu.sg]]></email_address>
			</au>
			<au>
				<person_id>P4190862</person_id>
				<author_profile_id><![CDATA[81100397017]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Farzam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Farbiz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute for Infocomm Research, A* Star, Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[farbizf@i2r.a-star.edu.sg]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Giovanni, S., Choi, Y. C, Huang, J., Khoo, E. T., and Yin, K. 2012. Virtual try-on using kinect and hd camera. In <i>Proc. Motion in Games 2012</i>, vol. 7660, Springer-Verlag Berlin Heidelberg, 55--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2407570</ref_obj_id>
				<ref_obj_pid>2407516</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kevelham, B., and Magnenat-Thalmann, N. 2012. Fast and accurate gpu-based simulation of virtual garments. In <i>Proceedings of VRCAI 2012</i>, ACM, 223--226.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2407171</ref_obj_id>
				<ref_obj_pid>2407156</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yuan, M., Khan, I. R., and Farbiz, F. 2012. Aligning a 3d headless avatar with a user's 2d face images in real-time. In <i>SIGGRAPH Asia 2012 Posters</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2407779</ref_obj_id>
				<ref_obj_pid>2407746</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zhou, Z., Shu, B., Zhuo, S., Deng, X., Tan, P., and Lin, S. 2012. Image-based clothes animation for virtual fitting. In <i>SIGGRAPH Asia 2012 Technique Briefs</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Depth Image based Cloth Deformation for Virtual Try-on Muhammad Mobeen Movania and Farzam Farbiz * Institute 
for Infocomm Research, A*Star, Singapore.  (a) (b) (c) (d) (e) Figure 1: Output results of virtual tryon 
showing the user side view (a), point rendering of the original undeformed mesh (b), point rendering 
of depth based deformed mesh (c), underformed mesh depth values color mapped (d), and deformed mesh depth 
values color mapped (e). 1 Introduction The affordability of RGBD cameras has pushed the retail shop­ping 
to the next level with the Virtual Try-on (VTO) application. Existing approaches [Giovanni et al. 2012], 
[Kevelham and Mag­nenat-Thalmann 2012] use skinning whereas systems like FitNect augment skinning with 
cloth simulation. Alternatively, image­based approach [Zhou et al. 2012] utilizes video clips for virtual 
try-on. In all of the existing approaches, the user s depth pro.le is not taken into consideration. Therefore, 
the user does not feel a good .t. Using customized avatars [Yuan et al. 2012] solves the .tting issue 
but it requires an of.ine user-look-alike avatar creation stage which is improper for walk-through scenarios. 
Furthermore, accurate physics based simulation of a 3D garment on a detailed avatar model is very time 
consuming. To circumvent these problems, instead of cloth simulation, we pro­pose to deform the clothes 
model using the nearest depth point. This way, the user experiences a better .t with a more pleasing 
virtual try-on as shown in Figure 1. 2 Methodology The steps involved in our approach are: 1. Segment 
and .lter the user s depth map obtained from RGBD camera 2. Calculate the normal map from the depth 
map of the user 3. For each vertex in the garment mesh, .nd the closest point in the depth map 4. Displace 
the garment vertex in the normal direction of the  found point in the depth map The normal map is obtained 
by extracting the eigenvalues (.k ) and eigenvectors (Vk ) from a covariance matrix (C) in the neighbor­hood 
of a given point (xi) as 1 n T C = (xi - x¯)(xi - x¯), C.V k = .k .V k , k . 0, 1, 2 (1) n i=1 The 
normal is used for displacement of garment vertices and will be utilized for collision resolution in 
our future cloth simulation * e-mail:{movaniamm,farbizf}@i2r.a-star.edu.sg pipeline. To accelerate search 
for the closest point to a given depth, we employ a kd-tree acceleration structure which is generated 
at ini­tialization. The entire pipeline is illustrated in Figure 2. After initial Figure 2: Our proposed 
pipeline. deformation, some vertices are left out at the extremities. These are stitched to the closest 
point in the depth map. The stitched mesh vertex displacements can also be used for user size recommenda­tion 
i.e. small, medium, large etc. References GI OVA N N I , S., CH O I , Y. C., HUA N G , J., KH O O , 
E. T., A N D YI N , K. 2012. Virtual try-on using kinect and hd camera. In Proc. Motion in Games 2012, 
vol. 7660, Springer-Verlag Berlin Hei­delberg, 55 65. KE V E L H A M , B., A N D MAG N E NAT-TH A L M 
A N N , N. 2012. Fast and accurate gpu-based simulation of virtual garments. In Pro­ceedings of VRCAI 
2012, ACM, 223 226. YUA N , M., KH A N , I. R., A N D FA R B I Z , F. 2012. Aligning a 3d headless avatar 
with a user s 2d face images in real-time. In SIGGRAPH Asia 2012 Posters, ACM. ZH O U , Z., SH U , B., 
ZH U O , S., DE N G , X., TA N , P., A N D LI N , S. 2012. Image-based clothes animation for virtual 
.tting. In SIGGRAPH Asia 2012 Technique Briefs, ACM. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503465</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>73</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[HaptoMIRAGE]]></title>
		<subtitle><![CDATA[a multi-user autostereoscopic visio-haptic display]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503465</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503465</url>
		<abstract>
			<par><![CDATA[<p>HaptoMIRAGE is a visuo-haptic display that provides a wide-angle autostereoscopic 3D image on the content-adjustable haptic display so that it enables us to have an enchant interaction with the virtual world via tangible object with multi-modal sensation not only by one user but also by multi users.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190863</person_id>
				<author_profile_id><![CDATA[82459052057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190864</person_id>
				<author_profile_id><![CDATA[82459133257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nobuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hanamitsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190865</person_id>
				<author_profile_id><![CDATA[82458951857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizushina]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190866</person_id>
				<author_profile_id><![CDATA[82459223757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Mina]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shibasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190867</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190868</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Research Laboratory, IIJ Innovation Institute, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190869</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2407720</ref_obj_id>
				<ref_obj_pid>2407707</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nii, H., Zhu, K., Yoshikawa, H., Nyan, L, H., Roland, A., and Nakatsu, R. 2012. Fuwa-Vision: An auto-stereoscopic floating-image display. In Proceedings of ACM SIGGRAPH Asia 2012 Emerging Technologies Singapore.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2331745</ref_obj_id>
				<ref_obj_pid>2331714</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Minamizawa, K., Kakehi, Y., Nakatani, M., Mihara, S., and Tachi, S. 2012. TECHTILE toolkit - A prototyping tool for design and education of haptic media. In Proceedings of ACM VRIC 2012, Laval, France.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HaptoMIRAGE-A Multi-user Autostereoscopic Visio-Haptic Display - Yuta UEDA*1, Nobuhisa HANAMITSU*1, 
Yusuke MIZUSHINA*1, Mina SHIBASAKI*1 Kouta MINAMIZAWA*1, Hideaki NII*2, and Susumu TACHI*1 *1 Graduate 
School of Media Design, Keio University *2 Research Laboratory, IIJ Innovation Institute, Inc.   Figure 
1: Concept drawing of the proposed system Summery HaptoMIRAGE is a visuo-haptic display that provides 
a wide-angle autostereoscopic 3D image on the content-adjustable haptic display so that it enables us 
to have an enchant interaction with the virtual world via tangible object with multi-modal sensation 
not only by one user but also by multi users. Our aim is to implement a platform for storytelling, entertainment 
and creative collaboration by combining 3D vision and haptic sensation in shown Figure 1. Based on our 
active-shttered real image autostereoscopic technology [Nii et al, 2012], we have developed a 3D image 
projection technology for multi users that provides autostereoscopic real image in midair with a view 
of 180 degrees. We have also developed a content­adjustable haptic display based on the simple and realistic 
record &#38; playback method [Minamizawa et al, 2012], of which we can easily design the shape and the 
vibrotactile sensation according to the scenario of the content. Active-shuttered Real Image Autostereosopy 
 The 180 degrees autostereoscopic display consists of three components such as Figure 2; each component 
has 60 degrees field of view, and provides an autostereoscopic image for one user. The Fresnel lens makes 
the real image from the LCD display, and the position of the user is measured by camera­based motion 
capture system, and the active shutter using transparent LCD panel provides the time-divided rays of 
the light for left-eye and right-eye. Then the user can see the real image as a floating 3D image in 
shown Figure 3. In this way the users up to three can see the autostereoscopic image from different viewpoints 
at the same time. Content-Adjustable Tangible Haptic Display The haptic display has multiple vibrators 
to provide spatially distributed haptic sensation to user. In order that user can fabricate the form 
of the haptic display, we developed a creation method of haptic display using polymer clay and multiple 
vibrotactile actuators. The record and play-back method of haptic sensation is based on the method of 
TECHTILE toolkit [Minamizawa et. al.] and then the creator can design both the shape and applied sensation 
according to the content such as Figure 4.  Figure 5: System construction of the autostereoscopic display 
User Experience By using Active-Shuttered Real Image Autostereoscopy method and contents adjustable haptic 
display, we realized that multiple users could experience with 3D contents via using haptic display. 
The user can sense the vibro-tactile sensation from the feeling of moving or texture of 3D contents. 
Additionally to this, haptic display can be fabricated various shapes that are contents adjustable. Therefore, 
the user can experience the story with using the objects that includes the world of story. Acknowledgment 
This research is supported by JST-CREST Haptic Media project. References NII, H., ZHU, K., YOSHIKAWA, 
H., NYAN, L, H., ROLAND, A., AND NAKATSU, R. 2012. Fuwa-Vision: An auto-stereoscopic floating­image display. 
In Proceedings of ACM SIGGRAPH Asia 2012 Emerging Technologies Singapore. MINAMIZAWA, K., KAKEHI, Y., 
NAKATANI, M., MIHARA, S., AND TACHI, S. 2012. TECHTILE toolkit -A prototyping tool for design and education 
of haptic media. In Proceedings of ACM VRIC 2012, Laval, France. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use isgranted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>JST-CREST Haptic Media project</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503466</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>74</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Immersive virtual reality and affective computing for gaming, fear and anxiety management]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503466</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503466</url>
		<abstract>
			<par><![CDATA[<p>Video game developers are enthusiastic about creating novel interaction approaches that yield a better gaming experience; such interactions are usually built with physical and emotional immersion in mind. Technologies such as Wii&#174;, Kinect&#174;, and Playstation Move&#174; focus on the physical movement of play to encourage seamless and natural behaviors during gameplay. On the other hand, technologies such as biofeedback are not yet being utilized to any large degree in the commercial industry and could be used to gain further knowledge of player's behavior and emotions. Biofeedback refers to technologies that provide awareness of human physiological functions through signals in order to control a system or improve those functions. This technology was primarily developed for clinical purposes to treat diseases such as headaches, high blood pressure, and epilepsy. The patients obtain the skill to control functions associated with aforementioned diseases by being exposed to equipment that measures and displays their bodily functions such as brain waves, heart rate, and galvanic skin response (GSR). This enables them to observe those senses through visualization and exert control over their physiological response over time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190870</person_id>
				<author_profile_id><![CDATA[82458804857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mehdi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Karamnejad]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mehdi_karamnejad@sfu.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190871</person_id>
				<author_profile_id><![CDATA[82459149057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Amber]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[achoo@sfu.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190872</person_id>
				<author_profile_id><![CDATA[81100453293]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Diane]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gromala]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gromala@sfu.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190873</person_id>
				<author_profile_id><![CDATA[81100396860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shaw]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shaw@sfu.ca]]></email_address>
			</au>
			<au>
				<person_id>P4190874</person_id>
				<author_profile_id><![CDATA[82459270657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jeremy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mamisao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Simon Fraser University, Surrey, BC, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jpm11@sfu.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1753453</ref_obj_id>
				<ref_obj_pid>1753326</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[K. Kuikkaniemi, T. Laitinen, M. Turpeinen, T. Saari, I. Kosunen, and N. Ravaja, "The influence of implicit and explicit biofeedback in first-person shooter games," in <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, New York, NY, USA, 2010, pp. 859--868.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[C. Shaw, D. Gromala, and A. Fleming Seay, "The meditation chamber: Enacting autonomic senses," <i>Proc. of ENACTIVE</i>, vol. 7, pp. 405--408, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1978958</ref_obj_id>
				<ref_obj_pid>1978942</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[L. E. Nacke, M. Kalyn, C. Lough, and R. L. Mandryk, "Biofeedback game design: using direct and indirect physiological control to enhance game interaction," in <i>Proceedings of the 2011 annual conference on Human factors in computing systems</i>, 2011, pp. 103--112.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Immersive Virtual Reality and Affective Computing for Gaming, Fear and Anxiety Management  Mehdi Karamnejad 
Amber Choo Diane Gromala Chris Shaw Jeremy Mamisao Simon Fraser University Simon Fraser University Simon 
Fraser University Simon Fraser University Simon Fraser University Surrey, BC, Canada Surrey, BC, Canada 
Surrey, BC, Canada Surrey, BC, Canada Surrey, BC, Canada mehdi_karamnejad@sfu.ca achoo@sfu.ca gromala@sfu.ca 
shaw@sfu.ca jpm11@sfu.ca 1. Introduction Video game developers are enthusiastic about creating novel 
interaction approaches that yield a better gaming experience; suchinteractions are usually built with 
physical and emotional immersion in mind. Technologies such as Wii®, Kinect®, and Playstation Move® focus 
on the physical movement of play to encourage seamless and natural behaviors during gameplay. Onthe other 
hand, technologies such as biofeedback are not yet being utilized to any large degree in the commercial 
industry and could be used to gain further knowledge of player s behavior and emotions. Biofeedback refers 
to technologies that provide awareness of human physiological functions through signals inorder to control 
a system or improve those functions. This technology was primarily developed for clinical purposes to 
treat diseases such as headaches, high blood pressure, and epilepsy.The patients obtain the skill to 
control functions associated withaforementioned diseases by being exposed to equipment thatmeasures and 
displays their bodily functions such as brain waves,heart rate, and galvanic skin response (GSR). This 
enables themto observe those senses through visualization and exert controlover their physiological response 
over time.Numerous biofeedback technologies have been widely utilized to develop video games for entertainment 
and medical purposes ­this is also known as affective gaming that accounts for player semotional state 
to affect the gameplay [Kuikkaniemi, et al 2010].GSR (aka skin-conductance level and electrodermal activity 
is atype of biological signal that is used to measure psychologicalarousals such as fear, stress, and 
fatigue by applying electricalcurrent to the skin and measuring the resulting electrical conductance. 
The level of skin conductance changes with respectto sweat glands activities, i.e., different levels 
of sweating causethe electrical conductance vary on the skin and the more intensethe psychological arousal 
is, the more skin conductance would be. Shaw et al. created a biofeedback virtual reality environment 
forchronic pain patients to manage their pain by well-known mindfulness meditation. The environment uses 
GSR obtained from the participant to drive the meditation session and inform theparticipants of their 
progress [Shaw, et al. 2007]. Nacke et al. introduced a biofeedback approach to directly control games 
using GSR and eye gaze [Nacke, et al. 2011], for example. 2. Our Approach We are developing a horror 
biofeedback game that implicitlymanipulates game elements utilizing a player s emotional state(figure 
1). Fear experienced during gameplay triggers emotional arousal that is visible and measurable via GSR 
sensors and can be used to affect gaming experience accordingly. Additionally, to provide more exciting 
and pleasurable gameplay, it is played and partially controlled using an immersive head-mounted display(HMD). 
Furthermore, we eventually seek to teach players to developstrength against fear by practicing relaxation 
techniques. Usingbiometric feedback such as GSR in game development creates a direct emotional link between 
the player and the game, allowing the game to intelligently perceive what the player is experiencing 
and enable it to change events in the game space accordingly. Additionally, using HMD technology along 
with biofeedback can extend the means of immersion, potentiallyresulting a more realistic experience 
and sense of presence. Theresult of this practice could also be utilized to help patients withspecific 
phobias to face their fears and develop skills to exertcontrol over such disorders. Figure 1. A screenshot 
of the environment in early development By using an escalating sense of fear with the horror genre, wehope 
to capitalize on the dramatic theories of Catharsis to help players control their fear responses in a 
safe, fun and familiar way. References K. Kuikkaniemi, T. Laitinen, M. Turpeinen, T. Saari, I. Kosunen,and 
N. Ravaja, The influence of implicit and explicit biofeedback in first-person shooter games, in Proceedings 
of the SIGCHI Conference on Human Factors in Computing Systems, New York, NY, USA, 2010, pp. 859 868. 
C. Shaw, D. Gromala, and A. Fleming Seay, The meditation chamber: Enacting autonomic senses, Proc. of 
ENACTIVE,vol. 7, pp. 405 408, 2007. L. E. Nacke, M. Kalyn, C. Lough, and R. L. Mandryk, Biofeedback game 
design: using direct and indirect physiological control to enhance game interaction, in Proceedings of 
the 2011 annual conference on Human factorsin computing systems, 2011, pp. 103 112. Permission to make 
digital or hard copies of part or all of this work for personal or classroom use isgranted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503467</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>75</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[KinEmotion]]></title>
		<subtitle><![CDATA[context controllable emotional motion analysis method for interactive cartoon generator]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503467</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503467</url>
		<abstract>
			<par><![CDATA[<p>Recently, cartoon contents are applying to various media like interactive systems. In a near future, the desire of user may become to immerse their live experience into a cartoon content deeply. In automatic cartoon generation environment using NUI (Natural User Interface) like Kinect, we can comprehend the importance of linking emotion expression with user's posture. Its story and impression are uncontrollable, if the system could not choose a suitable effect for each user motion. The system should have story driven method to protect the interpretation of the world, even if there is its original piece of manga.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190875</person_id>
				<author_profile_id><![CDATA[82459296557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190876</person_id>
				<author_profile_id><![CDATA[81488670906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190877</person_id>
				<author_profile_id><![CDATA[81550525656]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yukua]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koide]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190878</person_id>
				<author_profile_id><![CDATA[82458708057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Genki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunitomi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190879</person_id>
				<author_profile_id><![CDATA[82458794157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Akihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[manga@shirai.la]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 KinEmotion: Context controllable emotional motion analysis method for interactive cartoon generator 
Yuto NARA Wataru FUJIMURA Yukua KOIDE Genki KUNITOMI Akihiko SHIRAI* Kanagawa Institute of Technology 
 Figure 1: (Left) Background is controlled by user s posture / (Right) Printed copies of generated cartoons 
1 Motivation Recently, cartoon contents are applying to various media like inter­active systems. In a 
near future, the desire of user may become to immerse their live experience into a cartoon content deeply. 
In au­tomatic cartoon generation environment using NUI (Natural User Interface) like Kinect, we can comprehend 
the importance of link­ing emotion expression with user s posture. Its story and impression are uncontrollable, 
if the system could not choose a suitable effect for each user motion. The system should have story driven 
method to protect the interpretation of the world, even if there is its original piece of manga.  2 
KinEmotion: simpli.ed algorithm for posture-emotion controller We have developed AccuMotion algorithm 
to recognize accumu­lated motion for NUI game play in past research. AccuMotion ob­tains similarities 
with a target motion and a key motion from current posture. The timing difference of key and target motion 
can real­ize an intuitive input and an expression of user sensory for energy accumulation. This method 
is useful for presentation and operation for home electronics as well as a game play. However, cartoon 
is story driven world interpretation. The evaluation function needs to output a grade value instead of 
binary to perform one s physical emotional expression with a suitable visual. It is ideal if we can ac­quire 
human emotion using kinematics from Kinect, However, it is unreasonable to analyse the human emotion 
that will be in.nite in­cluding context in real time. We propose KinEmotion algorithm for this issue. 
It is an extension of AccuMotion. A consecutive evaluation output is operated from each joints inner 
product with signed weight coef.cients. For example, a hero posture can be expressed as .ve dot products 
from both elbows, both sides, and tilt of the back. It is able to design emotion evaluation function 
to .t in an expected range if we give suitable signed weight coef.cients for a target emotion. The teaser 
image is example which assigned the evalu­ation function to three conditions like Blank-Flash-Breakdown. 
A background changes continuously when user bends underarm and an elbow. The background effect and words 
balloon are automat­ically layout and they follow the movement of the user cartooned *e-mail: manga@shirai.la 
 Figure 2: The evaluation function of KinEmotion avatar. The function can be described as a dynamic 
effector which can be con.gured with contribution ratio for each joints. It is easy to understand to 
control linked posture-emotion for artists instead of complex mathematics. Through the natural experience 
of the user, it can perform physical emotional display freely. This algorithm is design abstraction then 
the story can be written by scripting. The designer assigns frame cuts, image .le names and evaluation 
function with signed weight coef.cients to prepare a dynamic scenario instead of hard implementation. 
 3 Manga Generator: a demo system We have applied KinEmotion to a demonstration system Manga Generator 
. It is a game-style attraction system. Player is making a pose to accord a story which is prepared for 
beforehand freely. The player can take the generated cartoon by printed copy and it is the only unique 
cartoon in the world. It have been tested in several public events and TV programs by thousands of visitors. 
All the player moved their body to a story naturally. It realized a failless game system then it got 
positive evaluation from all attributes. In a student competition, IVRC2012 .nal stage, it was chosen 
by 19 percents of 1,122 public visitors as the best one from 11 projects. In the near future, the technique 
of KinEmotion is not only an at­traction but also will be applicable in the recognition of the feelings 
of the people who are in the space of the entertainment system. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use isgranted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503468</article_id>
		<sort_key>830</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>76</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Near-field illumination for mixed reality with delta radiance fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503468</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503468</url>
		<abstract>
			<par><![CDATA[<p>Apart from geometric registration, fusing synthetic objects with a real context requires believable interaction of real and virtual light. Where shadows provide the visual cues to locate a synthetic object in a real scene and transferred light from real light sources let it appear in harmony with its surroundings, the mutual indirect interaction of illumination is a necessary detail to convince an observer that the rendered result is not merely augmented but part of the scene. Such a <i>mixed reality</i> (MR) system has applications in movie production, advertisement of unfinished products or cultural heritage visualization. While there are a range of relighting tools available for offline renderers or static scenes (e.g. photos), interactive MR systems usually disregard proper lighting entirely or greatly simplify shading. A method often employed is to merge virtual light and shadows cast from synthetic objects with a real background with Differential Rendering, leaving out any other light interaction such as indirect light bounces. Attempts have been made to resolve this issue with Instant Radiosity [Knecht et al. 2010; Lensing and Broll 2012]. To suppress flickering, a large number of virtual point lights (VPL) is necessary, drastically taxing execution speed. We propose to model all light, virtual and real, as a unified radiance field instead to avoid performance issues from oversampling and to maintain temporal coherence.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190880</person_id>
				<author_profile_id><![CDATA[81365592112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tobias]]></first_name>
				<middle_name><![CDATA[Alexander]]></middle_name>
				<last_name><![CDATA[Franke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer IGD]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Sainz, M., Green, S., and Eisemann, E. 2011. Interactive indirect illumination using voxel cone tracing. <i>Computer Graphics Forum</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730821</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kaplanyan, A., and Dachsbacher, C. 2010. Cascaded light propagation volumes for real-time indirect illumination. In <i>Proc. of the 2010 ACM SIGGRAPH symposium on Interactive 3D Graphics and Games</i>, ACM, I3D '10, 99--107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Knecht, M., Traxler, C., Mattausch, O., Purgathofer, W., and Wimmer, M. 2010. Differential Instant Radiosity for Mixed Reality. In <i>2010 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</i>, 99--107.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2474249</ref_obj_id>
				<ref_obj_pid>2473501</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lensing, P., and Broll, W. 2012. Instant indirect illumination for dynamic mixed reality scenes. <i>2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</i>, 109--118.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Near-Field Illumination for Mixed Reality with Delta Radiance Fields Tobias Alexander Franke, Fraunhofer 
IGD (a) No illumination (b) Full DLPV rendering (c) DLPV effects only (d) Volume visualization Figure 
1: In.nite Head model inserted into a real scene with one reconstructed light source: (a) a synthetic 
object is inserted without illumination, (b) visible .rst bounce around the base as well as low resolution 
shadow (32 propagations, 5122 VPLs, 11ms per frame), (d) indirect effects without synthetic object for 
better visualization, (d) visualization of the DLPV (red dots indicate negative values). 1 Introduction 
Apart from geometric registration, fusing synthetic objects with a real context requires believable interaction 
of real and virtual light. Where shadows provide the visual cues to locate a synthetic ob­ject in a real 
scene and transferred light from real light sources let it appear in harmony with its surroundings, the 
mutual indirect in­teraction of illumination is a necessary detail to convince an ob­server that the 
rendered result is not merely augmented but part of the scene. Such a mixed reality (MR) system has applications 
in movie production, advertisement of un.nished products or cul­tural heritage visualization. While there 
are a range of relighting tools available for of.ine renderers or static scenes (e.g. photos), interactive 
MR systems usually disregard proper lighting entirely or greatly simplify shading. A method often employed 
is to merge virtual light and shadows cast from synthetic objects with a real background with Differential 
Rendering, leaving out any other light interaction such as indirect light bounces. Attempts have been 
made to resolve this issue with Instant Radiosity [Knecht et al. 2010; Lensing and Broll 2012]. To suppress 
.ickering, a large number of virtual point lights (VPL) is necessary, drastically taxing exe­cution speed. 
We propose to model all light, virtual and real, as a uni.ed radiance .eld instead to avoid performance 
issues from oversampling and to maintain temporal coherence. 2 Technical Approach A manually reconstructed 
model of the real scene is geometrically registered with a marker and real light sources are importance 
sam­pled from a hemispherical image provided by a .sh-eye lens cam­era. Consider an existing radiance 
.eld Lµ. When we insert another non-emissive object O into the scene covered by Lµ, the properties of 
the radiance .eld change: light is either blocked or scattered by the newly inserted geometry. To account 
for this change in the radiance .eld, consider another .eld L. which has the same light con.guration 
as Lµ and additionally contains O. The change in a radiance .eld Lµ by introducing O is therefore L. 
= L. -Lµ. To generate L. dubbed the Delta Radiance Field we calculate two Re.ective Shadow Maps (RSM) 
for each reconstructed real light source: one contains the reconstructed scene geometry with the in­troduced 
object O, and one without O. The difference between both is used to initialize the residual direct light 
and indirect bounces into a volume representing the radiance .eld. To model this radiance .eld, we make 
use of small volumetric textures (323 pixels) with spherical harmonic encoded directional light similar 
to Light Prop­agation Volumes [Kaplanyan and Dachsbacher 2010] called Delta-LPV (DLPV). We use a two 
step injection phase, where .rst light from the RSM containing O is injected into the volume and after­wards 
light from the second RSM without O is injected negatively. Indirect light is approximated with VPLs 
created from each RSM. For direct light, we use the .ux and direction of the reconstructed real light 
source, but determine the position inside the volume with the help of the RSMs. Because of the differential 
the volume will contain negative values (i.e. Antiradiance). When superimposed on a background image, 
the DLPV introduces indirect bounces and shadows, correcting the real radiance .eld for the introduction 
of O. An example can bee seen in Figure 1. The evaluation cost of a DLPV differs from an LPV only in 
the double injection for direct and indirect light. By modeling a radiance .eld at a low resolution the 
.nal image suffers from artifacts such as light and shadows bleeding through thin geometry, self-illumination 
and self-shadowing. Considering the low-frequency nature of indirect diffuse light however, these ar­tifacts 
are not as apparent as the heavy aliasing along shadow bor­ders from the direct injection. A solution 
is to increase the spatial resolution to model a more accurate Delta Radiance Field. Further­more, we 
want to include indirect specular bounces from synthetic objects on real surfaces. With Sparse Voxel 
Octrees [Crassin et al. 2011] all of the problems mentioned above may be resolved. References CR ASSI 
N , C., NE Y R ET, F., SA I N Z , M., GR EE N, S., A N D EI S E-MAN N , E. 2011. Interactive indirect 
illumination using voxel cone tracing. Computer Graphics Forum. KA P L A N YAN , A., A N D DACH S BAC 
HE R, C. 2010. Cascaded light propagation volumes for real-time indirect illumination. In Proc. of the 
2010 ACM SIGGRAPH symposium on Interactive 3D Graphics and Games, ACM, I3D 10, 99 107. KN EC H T, M., 
TR A XL E R, C., MATTAUSC H , O., PUR G AT H O F E R, W., A N D WI M M E R, M. 2010. Differential Instant 
Radiosity for Mixed Reality. In 2010 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 
99 107. LE N S ING , P., A ND BRO L L , W. 2012. Instant indirect illumination for dynamic mixed reality 
scenes. 2012 IEEE International Sym­posium on Mixed and Augmented Reality (ISMAR), 109 118. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503469</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>77</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Nevermind]]></title>
		<subtitle><![CDATA[creating an entertaining biofeedback-enhanced game experience to train users in stress management]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503469</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503469</url>
		<abstract>
			<par><![CDATA[<p>Nevermind is a PC-based biofeedback-enhanced adventure horror game developed in Unity that challenges the player to go outside of his psychological comfort zone. Its use of biofeedback technology isolates the problem of fear and stress, making it a concrete, measurable entity that can be identified and addressed on a very direct and personal level. The high entertainment value of the game serves to compel players to push further to find out "what happens next" - and, in turn, the demand to venture into the terrifying unknown and return unscathed (both as an in-game character and as a person) encourages players to push beyond boundaries of fear in their own lives.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190881</person_id>
				<author_profile_id><![CDATA[82458759757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Erin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reynolds]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Southern California]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[erin@nevermindgame.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Csikszentmihalyi, M. 1990. <i>Flow: The Psychology of Optimal Experience</i>. New York: Harper & Row, 1990. Kindle Ebook File. Location 1588.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shore, M. and Schore. J. 2007. Right Brain Processes in Psychotherapy: Interactive Affect Regulation as a Central Mechanism of the Change Process. <i>Right Brain Affect Regulationen Essential Mechanism Of Development, Trauma, Dissociation, And Psychotherapy</i>. Yellowbrick. Web. 01 Apr. 2012. &lt;http://www.yellowbrickprogram.com/papers_by_yellowbrick/RightBrainAffectRegulation_p8.html&gt;.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Nevermind: Creating an Entertaining Biofeedback-Enhanced GameExperience to Train Users in Stress Management 
Erin ReynoldsUniversity of Southern Californiaerin@nevermindgame.com  1. Introduction Nevermind is 
a PC-based biofeedback-enhanced adventure horror game developed in Unity that challenges the player to 
go outsideof his psychological comfort zone. Its use of biofeedback technology isolates the problem of 
fear and stress, making it aconcrete, measurable entity that can be identified and addressedon a very 
direct and personal level. The high entertainment valueof the game serves to compel players to push further 
to find out what happens next -and, in turn, the demand to venture into the terrifying unknown and return 
unscathed (both as an in-gamecharacter and as a person) encourages players to push beyondboundaries of 
fear in their own lives. 2. Exposition and Biofeedback In Nevermind, players are placed in the role 
of a Neuroprober, ascientist who travels inside the sub-consciousness of severe PTSD patients to reconstruct 
the forgotten memory of the original psychological traumatic event. The realm of the subconscious is 
a terrifying place. Players mustsubject themselves uncomfortable and horrific situations in orderto find 
the lost memories. In order to succeed, the player must stay calm. If the biofeedback sensor detects 
psychological arousalin the player, the game will become more difficult. When theplayer calms down, the 
game returns to its easier, default state. Nevermind uses an ANT+ Stick and a Garmin Heart Rate strap 
todetect the player s heart rate during gameplay. The gameinterprets that data to calculate the player 
s Heart Rate Variability.to understand if the player is feeling calm or stressed. Figure 3. An example 
of different game states based on theplayer s psychological arousal levels. Figure 2. Gameplay reactive 
biofeedback challenges players affective tolerance [Schore 2007] while encouraging them toachieve a flow 
state. [Csikszentmihalyi 1990]  3. Next Steps Beyond play testing with hundreds of users and informallyobserving 
consistent responses to in-game stimuli, we have also conducted a series of more formalized internal 
testing at USCwhich yielded encouraging results. We have plans to test in a more rigorous environment 
via clinical trials in the near future. References CSIKSZENTMIHALYI,M.1990.Flow:ThePsychologyofOptimalExperienceNewYork:HarperRow,1990.KindleEbookFile.Location1588. 
 SHORE,M.ANSCHORE,J.2007.RightBrainProcessesinPsychotherapy:InteractiveAffectRegulation  asaCentralMechanism 
 oftheChangeProcess.RightBrainAffectRegulation:AnEssentialMechanismOfDevelopment,Trauma,Dissociation,AndPsychotherapy.Yellowbrick.Web.0Apr.2012.<http://www.yellowbrickprogram.com/papers_by_yellowbrick/RightBrainAffectRegulation_p8.html>. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503470</section_id>
		<sort_key>850</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>2503471</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>78</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D reconstruction of intricate objects using planar cast shadows]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503471</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503471</url>
		<abstract>
			<par><![CDATA[<p>3D reconstruction for intricate objects such as mesh structures or translucent materials is a challenging task. One way to form the shape of an intricate object is to probe its silhouette. We propose a 3D reconstruction system based on planar cast shadows and the shape from silhouette algorithm. Our work focuses on simplifying the calibration procedure and equalizing the numbers of effective pixels of shadows in all captured images. With this design, the spatial resolution is improved and it is able to carve intricate shapes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190882</person_id>
				<author_profile_id><![CDATA[82459109257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tzung-Han]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[thl@mail.ntust.edu.tw]]></email_address>
			</au>
			<au>
				<person_id>P4190883</person_id>
				<author_profile_id><![CDATA[82458863257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hao-Teng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m10025014@mail.ntust.edu.tw]]></email_address>
			</au>
			<au>
				<person_id>P4190884</person_id>
				<author_profile_id><![CDATA[82459292757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shang-Jen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m10025010@mail.ntust.edu.tw]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Yamazaki, S., Narasimhan, S., Baker, S. and Kanade, T. 2007. Coplanar shadowgrams for acquiring visual hulls of intricate objects. In <i>Proc. of IEEE International Conference on Computer Vision</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618502</ref_obj_id>
				<ref_obj_pid>1661412</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mitra, N. J. and Pauly, M. 2009. Shadow art. In <i>ACM Siggraph Asia</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1177003</ref_obj_id>
				<ref_obj_pid>1176991</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Savarese, S. and Andreetto, M. 2005. 3D reconstruction by shadow carving: Theory and practical evaluation. <i>International Journal of Computer Vision</i>. 71, 3, 305--336.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1409084</ref_obj_id>
				<ref_obj_pid>1409060</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lanman, D., Raskar, R., Agrawal, A. and Taubin, G. 2008. Shield fields: modeling and capturing 3D occluders. <i>ACM Transactions on Graphics (TOG)</i>. 27, 5, 131.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503472</article_id>
		<sort_key>870</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>79</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A facial tracking and transfer method with a key point refinement]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503472</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503472</url>
		<abstract>
			<par><![CDATA[<p>We propose a key point detection and refinement method for a facial motion tracking and transfer. Since key-point-based approaches for representing facial expressions usually deform faces by interpolating movements of the key points, the approaches cause errors between the deformed face and the original one. To solve this problem, our method tracks non-rigid deformations of surfaces to detects additional key points for minimizing the errors.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[facial animation]]></kw>
			<kw><![CDATA[facial transfer]]></kw>
			<kw><![CDATA[key point detection]]></kw>
			<kw><![CDATA[motion tracking]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190885</person_id>
				<author_profile_id><![CDATA[82459182057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kagoshima Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[akagi@ibe.kagoshima-u.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190886</person_id>
				<author_profile_id><![CDATA[81100401744]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Furukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima City Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190887</person_id>
				<author_profile_id><![CDATA[81322504926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sagawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190888</person_id>
				<author_profile_id><![CDATA[82458640157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Koichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ogawara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wakayama Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190889</person_id>
				<author_profile_id><![CDATA[81100265106]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Hiroshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kagoshima Univ.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kawasaki@ibe.kagoshima-u.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2355120</ref_obj_id>
				<ref_obj_pid>2354409</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cao, X., Wei, Y., Wen, F., and Sun, J. 2012. Face alignment by explicit shape regression. In <i>CVPR</i>, IEEE, 2887--2894.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2006992</ref_obj_id>
				<ref_obj_pid>2006853</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Jian, B., and Vemuri, B. C. 2011. Robust point set registration using gaussian mixture models. <i>IEEE Trans. Pattern Anal. Mach. Intell. 33</i>, 8 (aug), 1633--1645.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276478</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sumner, R. W., Schmid, J., and Pauly, M. 2007. Embedded deformation for shape manipulation. In <i>ACM SIGGRAPH 2007 papers</i>, ACM, SIGGRAPH '07.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A facial tracking and transfer method with a key point re.nement Yasuhiro Akagi* Ryo Furukawa Ryusuke 
Sagawa Koichi Ogawara Hiroshi Kawasaki Kagoshima Univ. Hiroshima City Univ. AIST Wakayama Univ. Kagoshima 
Univ.  Figure 1: (a)Input facial motion (slapping). (b)Visualization of the errors between the original 
face and the tracked face. (c)Transition of the mean errors in the minimization process. (d)Initial (left) 
and transferred face (right) from the slapping motion. Abstract We propose a key point detection and 
re.nement method for a facial motion tracking and transfer. Since key-point-based approaches for representing 
facial expressions usually deform faces by interpolat­ing movements of the key points, the approaches 
cause errors be­tween the deformed face and the original one. To solve this prob­lem, our method tracks 
non-rigid deformations of surfaces to de­tects additional key points for minimizing the errors. Keywords: 
Facial animation, Motion tracking, Facial transfer, Key point detection 1 Introduction By the development 
of scanning and tracking methods of a facial motion, a face model consists of more than 100,000 points. 
On the other hand, key point based approaches such as face rigging are still useful ways to create and 
edit facial animations. Even if a facial tracking method gives us all point-to-point correspondences, 
an suitable set of key points is needed for such kind of applica­tions. Therefore, the set of key points 
which ef.ciently represents the motion of a face is needed. Moreover, to create a motion of an arti.cial 
face such as animal characters, virtual humans and etc. in CG animations, facial transfer(cloning) methods 
are needed. We think a detection method of the suitable set of key points between a face to another face 
can be a solution for the facial transfer. 2 Facial tracking The key idea of our facial tracking method 
is that it detects the deformation Dt(F t) . F 0 and the inverse deformation Dt-1 (F 0) . F t (F t is 
a shape of tth frame in a facial motion). We utilize a deformation method based on a Radial Basis Func­tion 
[Sumner et al. 2007]. If the difference between Dt(F t) and F 0 can be minimized, the deformation Dt-1 
represents the defor­mations from the F 0 to the closest shape of F t . The detection process of the 
key points is as follows: (1)Calculating a temporal Dt-1 by using temporal key points given by a landmark 
detection method[Cao et al. 2012]. (2)Sampling the candidates of key points randomly form F 0 . (3)To 
evaluate the importance of the candi­dates, our method calculates a correlation coef.cient between a 
key *e-mail:akagi@ibe.kagoshima-u.ac.jp e-mail:kawasaki@ibe.kagoshima-u.ac.jp point and a motion of a 
neighboring area in Dt-1(F 0) as a degree of representation of the motion. If the value of the correlation 
coef­.cient of a point is below the average value of all sampled points, the point is rejected from the 
candidates. (4)Even though the defor­ mation Dt-1 contains errors, the each position of the key points 
in each frame is close to its true position. Then, our method detects the true positions of the key points 
by using the Non-Rigid Reg­istration method proposed by Jian et. al.[Jian and Vemuri 2011]. (5)Returning 
to the process (2) till the ratio of error decreases to less than 1%. By using this recursive process, 
it can minimize the error between Dt-1(F 0) and F t . 3 Facial transfer The key idea of a facial transfer 
method is how to interpolate the difference between a source face and a target one. Our fa­cial tracking 
method deforms 3D space around a face based on the transitions of key points. This method can be a solution 
to represent a transfer between two faces. The process of a pro­posed facial transfer method is as follows: 
(1)Creating deforma­tions Dt ) . T 0 from a source motion of a face F t to a tar­ f it (F t get one T 
0 . (2)Using the key point sampler described in section 2 to de.ne correspondences between Dt f it (F 
t) and T 0. (3)By us­ing the distance from the source face to the deformed source face Dif t = f it (F 
t) - F t||, calculating a deformation for facial ||Dt transfer Dt (T 0) . Dt ) + Dif t . trns f it (F 
t 4 Results Figure (b) and (c) shows the error between the input motion and tracking results. In these 
results, the mean error between the input faces and tracked faces is less than 1.0mm in the facial motion. 
However, the result (b) shows that the errors are still remaining around the area where the hand touches 
the face. Figure 1 (d) and (e) show the transfered shapes of the face. These results show that the movements 
of the mouth and cheeks can be transferred.  References CAO, X., WE I, Y., WE N , F., A N D SU N , J. 
2012. Face alignment by explicit shape regression. In CVPR, IEEE, 2887 2894. JI A N , B., A N D VEM U 
R I , B. C. 2011. Robust point set registra­tion using gaussian mixture models. IEEE Trans. Pattern Anal. 
Mach. Intell. 33, 8 (aug), 1633 1645. SU MNE R , R. W., SC H MI D , J., A N D PAULY, M. 2007. Embedded 
deformation for shape manipulation. In ACM SIGGRAPH 2007 papers, ACM, SIGGRAPH 07. Permission to make 
digital or hard copies of part or all of this work for personal or classroom use isgranted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503473</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>80</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An expressive text-driven 3D talking head]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503473</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503473</url>
		<abstract>
			<par><![CDATA[<p>Creating a realistic talking head, which given an arbitrary text as input generates a realistic looking face speaking the text, has been a long standing research challenge. Talking heads which cannot express emotion have been made to look very realistic by using concatenative approaches [Wang et al. 2011], however allowing the head to express emotion creates a much more challenging problem and model based approaches have shown promise in this area. While 2D talking heads currently look more realistic than their 3D counterparts, they are limited both in the range of poses they can express and in the lighting conditions that they can be rendered under. Previous attempts to produce videorealistic 3D expressive talking heads [Cao et al. 2005] have produced encouraging results but not yet achieved the level of realism of their 2D counterparts.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190890</person_id>
				<author_profile_id><![CDATA[81508683004]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Anderson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ra312@cam.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P4190891</person_id>
				<author_profile_id><![CDATA[81344498528]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bj&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stenger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Research Europe, Cambridge, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bjorn.stenger@crl.toshiba.co.uk]]></email_address>
			</au>
			<au>
				<person_id>P4190892</person_id>
				<author_profile_id><![CDATA[81100388301]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vincent]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Toshiba Research Europe, Cambridge, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[vincent.wan@crl.toshiba.co.uk]]></email_address>
			</au>
			<au>
				<person_id>P4190893</person_id>
				<author_profile_id><![CDATA[81100053634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Roberto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cipolla]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rc10001@cam.ac.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anderson, R., Stenger, B., Wan, V., and Cipolla, R. 2013. Expressive visual text-to-speech using active appearance models. In <i>CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1095881</ref_obj_id>
				<ref_obj_pid>1095878</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Cao, Y., Tien, W., Faloutsos, P., and Pighin, F. 2005. Expressive speech-driven facial animation. <i>ACM TOG 24</i>, 4, 1283--1302.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wang, L., Han, W., Soong, F., and Huo, Q. 2011. Text driven 3D photo-realistic talking head. In <i>Interspeech</i>, 3307--3308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Expressive Text-Driven 3D Talking Head Robert Anderson1, Bj ¨ orn Stenger2, Vincent Wan2, Roberto 
Cipolla1 1Department of Engineering, University of Cambridge, UK * 2Toshiba Research Europe, Cambridge, 
UK (a) (b) (c) (d) (e) Figure 1: An Active Appearance Model (AAM) based, expressive 2D talking head 
is trained on 7000 sentences of video data (a). Given only 30 3D scans obtained by combined multiview 
and photometric stereo we obtain a set of training samples with both depth and normal maps (b). Depth 
and normals are mapped to the existing AAM, allowing the same synthesis pipeline used for the 2D talking 
head to drive renderings of either geometry (c), texture (d) or a combination of the two (e). 1 Introduction 
Creating a realistic talking head, which given an arbitrary text as input generates a realistic looking 
face speaking the text, has been a long standing research challenge. Talking heads which cannot express 
emotion have been made to look very realistic by using concatenative approaches [Wang et al. 2011], however 
allowing the head to express emotion creates a much more challenging prob­lem and model based approaches 
have shown promise in this area. While 2D talking heads currently look more realistic than their 3D counterparts, 
they are limited both in the range of poses they can ex­press and in the lighting conditions that they 
can be rendered under. Previous attempts to produce videorealistic 3D expressive talking heads [Cao et 
al. 2005] have produced encouraging results but not yet achieved the level of realism of their 2D counterparts. 
One challenge in building 3D talking heads is the collection of suf­.ciently large training datasets. 
While 2D systems can be trained from thousands of example sentences from video, capturing the same amount 
of 3D data at high quality is currently still time con­suming and challenging. We propose a method for 
creating an ex­pressive 3D talking head by leveraging a large amount of 2D train­ing data and a small 
amount of 3D data. 2 Technical Approach Given a training set of 7000 recorded sample sentences, we .rst 
construct a 2D talking head using an Active Appearance Model (AAM) as a representation of the face. Synthesis 
is performed with a Hidden Markov Model text-to-speech (HMM-TTS) system that uses cluster adaptive training 
(CAT) as described in [Anderson et al. 2013]. This allows for synthesis over a continuous space of expres­ 
sions created by setting the weights of .ve basic emotions (happi­ness, sadness, fear, anger and tenderness). 
Given input text an audio .le is generated along with a set of AAM parameters that allow for rendering 
of the face. As additional training data we recorded 5 *e-mail: {ra312, rc10001}@cam.ac.uk e-mail: {bjorn.stenger, 
vincent.wan}@crl.toshiba.co.uk sentences in each emotion using a 3D scanner, from which we used 30 frames 
covering a variety of expressions and phonemes. The 3D data is captured using a system consisting of 
two video cameras and three colored light sources which allows for a combination of mul­tiview stereo 
and color photometric stereo to produce high quality geometry of the face region at video framerates. 
In order to avoid retraining the CAT speech synthesis model we add data from the 3D scans to the existing 
AAM. This allows the 3D talking head to be driven in exactly the same way as the 2D one, from the same 
AAM parameters. To model coarse geometry we add a depth component to each vertex in the AAM, essentially 
turning it into a morphable model. To model .ne geometry we add a normal map component, which is dealt 
with in the same way as the texture component of a standard AAM, except that an additional normal­ization 
step is used. To .nd the depth and normal map modes which correspond to the original 2D AAM, it is .rst 
registered to the 30 3D training scans. Depth and normal maps are then extracted from these training 
examples and the modes are found by minimizing the difference between the observed samples and reconstructions 
syn­thesized by the AAM in a least squares sense. The 3D scanning method used does not accurately reconstruct 
the teeth or the inside of the mouth and so in future work we would like to better model these. The 2D 
position of the teeth is given by the original AAM and we aim to drive a rigid 3D teeth model from this. 
 References AN D E R S ON, R., STE NGER , B., WA N, V., AND CI P O L LA , R. 2013. Expressive visual 
text-to-speech using active appearance models. In CVPR. CAO, Y., TI E N , W., FA L O UT S OS, P., AND 
PI G H IN, F. 2005. Ex­pressive speech-driven facial animation. ACM TOG 24, 4, 1283 1302. WA NG, L., 
HA N , W., SO O N G , F., AND HU O , Q. 2011. Text driven 3D photo-realistic talking head. In Interspeech, 
3307 3308. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503474</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>81</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Click&#38;draw selection]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503474</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503474</url>
		<abstract>
			<par><![CDATA[<p>In interactive shape modeling, surface selection is one of the basic, cornerstone interaction: this task may be performed hundreds of times by the user for the modification of a single shape. Despite the numerous automatic selection methods introduced in the literature, this remains a cumbersome operation in a number of scenarios, where the user is ultimately asked to paint over everything she wants to select. Without imposing a full visual grammar, we observe that a more effective selection process can be designed around a simple classification of the user interaction: <i>point click, open</i> and <i>close strokes</i>. Our basic idea is to relate this simple classification to a specific set of selection algorithms, targeting the three main classes of surface selections: <i>connected components, parts</i> and <i>patches</i>. We also address the problem of repetitive similar selections by providing an automatic <i>expansion</i> process which captures regions which are detected as similar to the selected one.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190894</person_id>
				<author_profile_id><![CDATA[82459310657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Emilie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Guy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190895</person_id>
				<author_profile_id><![CDATA[81502774394]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jean-Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thiery]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190896</person_id>
				<author_profile_id><![CDATA[81309487978]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tamy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Boubekeur]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Telecom ParisTech - CNRS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2381114</ref_obj_id>
				<ref_obj_pid>2381112</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ben-Chen, M., and Gotsman, C. 2008. Characterizing shape using conformal factors. In <i>Eurographics workshop on 3D object retrieval</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531379</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chen, X., Golovinskiy, A., and Funkhouser, T. 2009. A benchmark for 3d mesh segmentation. <i>ACM Trans. Graph. (TOG) 28</i>, 3, 73.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015817</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cohen-Steiner, D., Alliez, P., and Desbrun, M. 2004. Variational shape approximation. In <i>ACM Trans. Graph</i>, vol. 23, ACM, 905--914.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Zheng, Y., and Tai, C.-L. 2010. Mesh Decomposition with Cross-Boundary Brushes. <i>Computer Graphics Forum 29</i>, 2 (June), 527--535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Click&#38;Draw Selection Emilie Guy, Jean-Marc Thiery, Tamy Boubekeur Telecom ParisTech -CNRS  Figure 
1: Left: click&#38;draw overview. Middle: user selection. Right: results. 1 Introduction In interactive 
shape modeling, surface selection is one of the ba­sic, cornerstone interaction: this task may be performed 
hundreds of times by the user for the modi.cation of a single shape. De­spite the numerous automatic 
selection methods introduced in the literature, this remains a cumbersome operation in a number of sce­narios, 
where the user is ultimately asked to paint over everything she wants to select. Without imposing a full 
visual grammar, we observe that a more effective selection process can be designed around a simple classi.cation 
of the user interaction: point click, open and close strokes. Our basic idea is to relate this simple 
classi­.cation to a speci.c set of selection algorithms, targeting the three main classes of surface 
selections: connected components, parts and patches. We also address the problem of repetitive similar 
se­lections by providing an automatic expansion process which cap­tures regions which are detected as 
similar to the selected one. 2 User Interaction &#38; Selections We state that the interaction should 
carry more or less information based on how evident the selection is for the user. Based on this simple 
observation, our tool classi.es user interactions from a sin­gle click to multiple strokes and matches 
each of them with a par­ticular selection algorithm. Connected components are selected by point clicks, 
parts (resp. patches) by open (resp. close) strokes. Connected Component Selection : When loading the 
mesh we perform a .ood .lling algorithm to link each face with it connected component. When the user 
clicks on a triangle, we simply select all the faces which have the same component number. Patch Selection 
: We compute a principal component analysis on the user stroke to .nd a local frame Fc. This frame is 
used to roughly classi.ed faces as selected, not selected, or unclassi.ed. Then a .ood .lling algorithm, 
started from the selected faces and driven by a normal-based metric, sets all unclassi.ed faces as se­lected 
or not. We evaluated a panel of different metrics, and even­tually found that the L2,1 distance [Cohen-Steiner 
et al. 2004] and the Lcross distance [Zheng and Tai 2010] give the best results. Part Selection : Isolines 
of a harmonic .eld, guided and progres­sively re.ned by user strokes, are used as cutting boundaries 
for the selection of a part. The .eld is obtained as the solution of the following system: ... . L 0 
. .. W0P0 . F = W0B0 W1P1 W1B1 where L is the mesh Laplacian (cotan scheme), W0 and W1 are positional 
weighting matrices, P0 and P1 are positional constraints matrices, B1 = (1..1)T and B0 = (0..0)T . We 
set 0 and 1-constraints on each side of the user stroke. The system is factorized only once when the 
mesh is loaded. Then we update it only from the few dynamic constraints during interaction. To handle 
meshes with multiple connected components, we set 0­constraints on all the components not touched by 
the stroke. 3 Expansion To .nd similar connected component selections, we compute the area of each component 
and select the closest ones to the selected reference. This simple strategy happens to be surprisingly 
effective in practice. For parts selection expansion, we use the conformal factor [Ben-Chen and Gotsman 
2008] as a similarity map to detect similar parts. This is computed once by solving: LF = KT - Korigin 
, where Korigin is the mesh gaussian curvature and KT is the average gaussian curvature. To .nd similar 
parts, we compute the average conformal factor of the user stroke and .nd isolines with a similar value: 
they act as potential selections. For each isoline we apply the part selection process and we compare 
the potential selection found with the reference one based on their conformal factor dis­tributions. 
The algorithm is ef.cient because it uses the locality of the part selection and the globality of the 
similarity map to .nd the best similar selections. 4 Results We compare our selections with human selections 
strokes from the benchmark of Chen et al. [Chen et al. 2009]. For all the models we were able to .nd 
similar selections in a small amount of time and using few strokes. In future work, we plan to de.ne 
a uni.ed framework for the expansion process. References BE N-CH E N , M., A N D GOT S M A N , C. 2008. 
Characterizing shape using conformal factors. In Eurographics workshop on 3D object retrieval, 1 8. CH 
EN , X., GOL OV I N S K I Y, A., A ND FU N K H O US ER , T. 2009. A benchmark for 3d mesh segmentation. 
ACM Trans. Graph. (TOG) 28, 3, 73. CO H E N -ST E I N ER , D., AL L I E Z , P., A N D DE S B RU N, M. 
2004. Variational shape approximation. In ACM Trans. Graph., vol. 23, ACM, 905 914. ZH E N G , Y., A 
N D TAI , C.-L. 2010. Mesh Decomposition with Cross-Boundary Brushes. Computer Graphics Forum 29, 2 (June), 
527 535. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use isgranted without fee provided that copies are not made or distributed for commercial advantage and 
that copies bear this notice and the full citation on the first page. Copyrights for third-party components 
of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 
2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503475</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>82</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Development of a three dimensional in silico model of the human respiratory system for dosimetric use]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503475</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503475</url>
		<abstract>
			<par><![CDATA[<p>The homeland security community requires a unique solution to the challenges of studying exposure to aerosol-based contaminants. The goal of this work is to create a comprehensive computational, morphologically-realistic model of the human respiratory system that can be used to study the inhalation, deposition, and clearance of contaminants, while making the model adaptable for age, race, sex, and health.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[aerosol contaminants]]></kw>
			<kw><![CDATA[biomedical and genomic]]></kw>
			<kw><![CDATA[homeland security]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190897</person_id>
				<author_profile_id><![CDATA[82458964057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ray]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Burton]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lockheed Martin]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[burton.ray@epa.gov]]></email_address>
			</au>
			<au>
				<person_id>P4190898</person_id>
				<author_profile_id><![CDATA[82459254357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jacky]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rosati-Rowe]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[EPA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rosati.jacky@epa.gov]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rosati, J., Burton, R., McCauley, R., and McGregor. G. 2010. Three Dimensional Modeling of the Human Respiratory System, 2010, <i>American Association for Aerosol Research 29</i>&#60;sup&#62;<i>th</i>&#60;/sup&#62; <i>Annual Conference</i>. Retrieved from http://aaarabstracts.com/2010/viewabstract.php?paper=859]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Development of a Three Dimensional In Silico Model of the Human Respiratory System for Dosimetric Use 
 Ray Burton (Lockheed Martin), Jacky Rosati-Rowe (EPA) burton.ray@epa.gov, rosati.jacky@epa.gov Figure 
1: A volumetric data set created from combining a series of MRI images. The image shows how a surface 
is created by extracting a particular threshold value, called an isosurface, from the volume data (a). 
Extrathoracic airways combined with the nasal and oral cavity (b). Bifurcation model combines the pharynx 
showing one complete 23 generation path in each of five lobes (c). Complete contiguous typical path model, 
including nasal and oral cavities, traces of the velocity magnitude (cm/s) are shown for an inhalation 
laminar flow study (d). Complete contiguous internal and external typical path model, including nasal 
and oral, cavities, larynx, and 24 generations with a single bifurcating path into each of the five lobes 
of the lung. (e). ABSTRACT The homeland security community requires a unique solution to the challenges 
of studying exposure to aerosol-based contaminants. The goal of this work is to create a comprehensive 
computational, morphologically-realistic model of the human respiratory system that can be used to study 
the inhalation, deposition, and clearance of contaminants, while making the model adaptable for age, 
race, sex, and health. Keywords: homeland security, biomedical and genomic, aerosol contaminants. 1 Introduction 
and Motivation The homeland security and public health community are confronted with research challenges 
when studying inhaled anthrax, ricin toxin, or other aerosol-based contaminants due to their toxicity. 
Computational studies of aerosol-based hazardous contaminants and particle-transport within the human 
respiratory system therefore have a critical role to play in this research. To aid in studying exposure 
and risk assessment, we are developing a comprehensive morphologically-realistic model of the human respiratory 
system from nares to alveoli that will respond to the dynamic changes of respiratory mechanics and abnormal 
pathologies. The project's goal is to develop a comprehensive morphologically-realistic model of the 
human respiratory system that can be used to study the inhalation, deposition, and clearance of aerosol-based 
contaminants. 2 Technical Approach We started by using magnetic resonance imaging (MRI) slices from 
the Visible Human Project s (VHP) Brigham and Women s Hospital (BWH) to create a volumetric data set. 
A threshold value was used to extract an isosurface of the head and nasal cavity, which are soft tissues. 
The upper and lower gums with teeth were combined to form a complete oral cavity. The tongue was added 
to the oral model and the upper airway models were then aligned and combined into a single morphology 
model, including sides and uvula. The branching airways were created using an algorithmic method that 
generates a single path to each of the five lobes, 23 generations down. From that model, we use subdivision 
surfaces to create a smoothed, watertight mesh, removing holes and imperfections from the original model. 
We then combine the nasal, oral, and bifurcations models into a contiguous typical path model. The internal 
structure of the respiratory system was combined to create a three-dimensional morphologically realistic 
model of the human respiratory tract from the nares to the alveoli. To confirm the model was free from 
surface imperfections we prepared a computational mesh, a process which requires a watertight surface 
and performed a trial CFD run. Traces of velocity magnitude are shown for inhalation laminar flow. The 
external surface representing the facial structure obtained from the volumetric data and the internal 
structure including the nasal and oral cavities along with the five lobe typical path bifurcation model 
was combined to form a three-dimensional morphologically realistic model of the human respiratory system. 
 3 Future Work Upcoming work includes parameterizing the models to allow for manipulation of such features 
as age, gender, race, disease states, as well as fine controls for the shape of specific features, such 
as the nasal turbinates. The model will also feature a dynamic morphology that mimics the changes in 
the airway structures during a typical breathing cycle. The model will therefore allow for any variation 
of airway geometries and disease states. The model s flexibility and adaptability could help researchers 
predict dose from exposure to hazardous contaminants, such as anthrax and ricin, and assist in estimating 
thresholds and the need for prophylactic measures. References Rosati, J., Burton, R., McCauley, R., 
and McGregor. G. 2010. Three Dimensional Modeling of the Human Respiratory System, 2010, American Association 
for Aerosol Research 29th Annual Conference. Retrieved from http://aaarabstracts.com/2010/viewabstract.php?paper=859 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503476</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>83</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Fast and accurate distance, penetration, and collision queries using point-sphere trees and distance fields]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503476</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503476</url>
		<abstract>
			<par><![CDATA[<p>Collision detection, force computation, and proximity queries are fundamental in interactive gaming, assembly simulations, or virtual prototyping. However, many available methods have to find a trade-off between the accuracy and the high computational speed required by haptics (1 kHz). [McNeely et al. 2006] presented the Voxmap-Pointshell (VPS) Algorithm, which enabled more reliable six-DoF haptic rendering between complex geometries than other approaches based on polygonal data structures. For each colliding object pair, this approach uses (<i>i</i>) a <i>voxelmap</i> or voxelized representation of one object and (<i>ii</i>) a <i>pointshell</i> or point-sampled representation of the other object (see Figure 2). In each cycle, the penetration of the points in the voxelized object is computed, which yields the collision force. [Barbi&#269; and James 2008] extended the VPS Algorithm to support deformable objects. This approach builds hierarchical data structures and distance fields that are updated during simulation as the objects deform.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190899</person_id>
				<author_profile_id><![CDATA[81474702403]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mikel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sagardia]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[German Aerospace Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mikel.sagardia@dlr.de]]></email_address>
			</au>
			<au>
				<person_id>P4190900</person_id>
				<author_profile_id><![CDATA[81474696135]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thomas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hulin]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[German Aerospace Center]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[thomas.hulin@dlr.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1449679</ref_obj_id>
				<ref_obj_pid>1449660</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Barbi&#269;, J., and James, D. 2008. Six-dof haptic rendering of contact between geometrically complex reduced deformable models. <i>Haptics, IEEE Transactions on 1</i>, 1 (jan.-june), 39--52.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[McNeely, W. A., Puterbaugh, K. D., and Troy, J. J. 2006. Voxel-based 6-dof haptic rendering improvements. <i>Haptics-e: The Electronic Journal of Haptics Research 3</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sagardia, M., Hulin, T., Preusche, C., and Hirzinger, G. 2008. Improvements of the voxmap-pointshell algorithm - fast generation of haptic data-structures. In <i>53. IWK - TU Ilmenau</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fast and Accurate Distance, Penetration, and Collision Queries Using Point-Sphere Trees and Distance 
Fields Mikel Sagardia* Thomas Hulin German Aerospace Center German Aerospace Center (a) (b) (c) (d) 
(e) Figure 1: (a) Successive point tree levels and one sphere level of of a robotic gripper. (b) Clustering 
of the point tree. (c) Different point tree levels and minimal enclosing spheres of each cluster. (d) 
Voxelized satellite module. (e) Distance interpolation in a voxel octant. 1 Introduction Collision detection, 
force computation, and proximity queries are fundamental in interactive gaming, assembly simulations, 
or vir­tual prototyping. However, many available methods have to .nd a trade-off between the accuracy 
and the high computational speed required by haptics (1 kHz). [McNeely et al. 2006] presented the Voxmap-Pointshell 
(VPS) Algorithm, which enabled more reliable six-DoF haptic rendering between complex geometries than 
other approaches based on polygonal data structures. For each colliding object pair, this approach uses 
(i) a voxelmap or voxelized represen­tation of one object and (ii) a pointshell or point-sampled represen­tation 
of the other object (see Figure 2). In each cycle, the penetra­ tion of the points in the voxelized object 
is computed, which yields the collision force. [Barbi. c and James 2008] extended the VPS Algorithm to 
support deformable objects. This approach builds hi­erarchical data structures and distance .elds that 
are updated during simulation as the objects deform. Figure 2: Left: Voxelized and point-sampled objects 
in collision; Each voxel has its voxel layer value (l) related to its penetration in the voxelmap, and 
each point its inwards pointing normal vector (ni). Right: Single point (Pi) force (Fi) can be computed 
scaling the normal vector (ni) with its penetration in the voxelmap. The cross products of forces and 
points yield torques. We present a haptic rendering algorithm for rigid bodies based on the VPS Algorithm 
which also uses hierarchies and distance .elds. Yet, our data structures are optimized for fast and accurate 
collision and proximity queries rather than for deformation simulations. *e-mail: mikel.sagardia@dlr.de 
e-mail: thomas.hulin@dlr.de 2 Our Approach First, layered voxelmap and plain point-soup representations 
of ob­jects are computed according to [Sagardia et al. 2008]. Then, real distance-.eld values (v) are 
stored in the voxels close to the surface and a point-sphere tree is built (down-top) upon the plain 
point­soup. In order to build our tree, neighbor points are organized in clusters (see Figure 1(b)). 
The point in the cluster which is closest to its center of mass belongs to the upper level in the tree, 
which is also clustered. Additionally, all points and children points within a cluster are enclosed with 
a minimal sphere (see Figure 1(c)). After the of.ine generation of our structures, the online algorithm 
computes the penetration of the likely colliding points in the vox­elmap, similarly as in Figure 2. Each 
haptic cycle, the uppermost cluster with the sphere that encloses all points is pushed to the query queue. 
The algorithm checks whether each popped cluster sphere is in collision; if so, the parent point of the 
cluster is checked for collision and children clusters are pushed to the queue. As shown in Figure 1(d), 
the .oating-point (penetration) distance (v) of a point is interpolated after computing the local distance 
.eld gradient (.v) in the voxel neighborhood (a,ß , .). Normal vectors scaled with the penetration yield 
collision forces (see Figure 2). Figures 1(a) and 1(d) show examples of our data structures. Our algorithm 
is between 1.5 and 17 times faster than the tree-less ap­proach in the classical peg-in-hole scenario 
and presents low alias­ing artifacts. Future work will address the exploitation of distance queries and 
the uniformity in point distribution and clustering. References BA R B I C., J., A N D JA M E S , D. 
2008. Six-dof haptic rendering of contact between geometrically complex reduced deformable models. Haptics, 
IEEE Transactions on 1, 1 (jan.-june), 39 52. MCNE ELY, W. A., PU TER BAU G H , K. D., A N D TROY, J. 
J. 2006. Voxel-based 6-dof haptic rendering improvements. Haptics-e: The Electronic Journal of Haptics 
Research 3. SAG A RDI A , M., HU LI N , T., PREUSC H E , C., A N D HI R ZI N G E R, G. 2008. Improvements 
of the voxmap-pointshell algorithm ­fast generation of haptic data-structures. In 53. IWK -TU Ilme­nau. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503477</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>84</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Hierarchical volumetric object representations for digital fabrication workflows]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503477</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503477</url>
		<abstract>
			<par><![CDATA[<p>With the rise of of desktop 3D printers, hackerspaces, and fab labs, more and more individuals are engaging in personal-scale digital fabrication. For historical reasons, fabrication workflows are often based on triangulated meshes. Meshes are easy to render, but cannot guarantee physical feasibility -- holes, zero-area faces, incorrect normals, and other flaws can make them nonsensical descriptions of physical objects.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190901</person_id>
				<author_profile_id><![CDATA[82459157657]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keeter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Center for Bits and Atoms]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[matt.keeter@cba.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>134027</ref_obj_id>
				<ref_obj_pid>142920</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Duff, T. 1992. Interval arithmetic recursive subdivision for implicit functions and constructive solid geometry. SIGGRAPH '92.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>344899</ref_obj_id>
				<ref_obj_pid>344779</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Frisken, S. F., Perry, R. N., Rockwood, A. P., and Jones, T. R. 2000. Adaptively sampled distance fields: a general representation of shape for computer graphics. SIGGRAPH '00.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical Volumetric Object Representations for Digital Fabrication Work.ows Matthew Keeter* MIT 
Center for Bits and Atoms  Figure 1: Summary of fabrication work.ows 1 Introduction With the rise of 
of desktop 3D printers, hackerspaces, and fab labs, more and more individuals are engaging in personal-scale 
digital fabrication. For historical reasons, fabrication work.ows are often based on triangulated meshes. 
Meshes are easy to render, but can­not guarantee physical feasibility holes, zero-area faces, incorrect 
normals, and other .aws can make them nonsensical descriptions of physical objects. This research presents 
a novel complete work.ow for design, path planning, and fabrication. The work.ow uses adaptively sampled 
distance .elds (ASDFs) on an octree as a generic intermediate rep­resentation of volumetric data [Frisken 
et al. 2000], which guaran­ tees closed, sensible objects and eliminates pain points of mesh­based work.ows. 
 2 Design Tools A family of CAD tools have been designed to generate ASDF struc­tures. They use a distance 
metric functional representation, which makes computational solid geometry and coordinate transforms 
trivial. Our geometry engine ef.ciently evaluates expressions us­ing interval arithmetic and recursive 
subdivision on an octree [Duff 1992], populating corner values of ASDF cells. Figure 2: Solid model 
of a key and a fraction of its representation Two CAD tools have been written using this geometry engine 
as a backend. One uses Python as a solid modeling language; the other represents models as a graph of 
information .ow between nodes. *e-mail:matt.keeter@cba.mit.edu  3 CT Import CT scan data provides an 
alternative entry point to our fabrication work.ow, allowing for designs informed by physical templates. 
In the example below, 122 MB of raw CT data is reduced to a 16 MB ASDF representation. Surface normals 
can be calculated from the local gradient at leaf cells, producing shaded visualizations.  Figure 3: 
Normals, shaded, and depth map rendering from a scanned piece of lace (0.1 mm voxel size, 255×255×511 
region) 4 Fabrication This work.ow allows for two, three, and .ve-axis fabrication on a variety of machines, 
using a shared core of path-planning rou­tines. Path planning is performed by discretizing the ASDF into 
a greyscale heightmap at a resolution greater than the target ma­chine s native step size. A distance 
transform compensates for tool offset and marching squares extracts toolpath contours. We support vinyl 
and laser cutters for 2D and press-.t fabrication; mini-mills for solid and mold manufacturing; large-format 
milling machines for furniture-scale fabrication; and .ve-axis milling (us­ing multiple 3D cut planes). 
The work.ow will be extended to 3D printing in the near future, completing the family of mesh-free per­sonal 
fabrication work.ows. References DU FF, T. 1992. Interval arithmetic recursive subdivision for im­plicit 
functions and constructive solid geometry. SIGGRAPH 92. FRI S K E N , S. F., PER RY, R. N., RO C K WO 
O D , A. P., A ND JO NE S, T. R. 2000. Adaptively sampled distance .elds: a general rep­resentation of 
shape for computer graphics. SIGGRAPH 00. Permission to make digital or hard copies of part or all of 
this work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503478</article_id>
		<sort_key>930</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>85</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Interactive card weaving design and construction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503478</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503478</url>
		<abstract>
			<par><![CDATA[<p>Weaving is a method of fabric production, consisting of two distinct sets of yarns (warp and weft). It is popular and similar to other fabric production methods, such as knitting, felting, and lace making. In particular, 'card weaving' is a very simple and easy weaving method. The user prepares nothing more than colored yarns and simple cardboard squares with four holes [Crockett 1991]. The user can produce exquisitely patterned woven bands, such as ribbons, straps, and hair accessories. However, the textile patterns are typically designed via a laborious manual process. The final textile design is determined by 1) the color of each warp yarn, 2) the direction of four yarns passing through each card, and 3) the direction and number of rotations of the cards.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190902</person_id>
				<author_profile_id><![CDATA[82459177157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Igarashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190903</person_id>
				<author_profile_id><![CDATA[81100209891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crockett, C. 1991. Card Weaving. Interweave Pr.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Coahranm M. and Fiume, E. 2005. Sketch-Based Design for Bargello Quilts. Eurographics Workshop on Sketch-Based Interfaces and Modeling 2005, 165--174.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Polak, G. 2002. Card Weaver. http://www.theloomybin.com]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Card Weaving Design and Construction C:\Users\yuki\conference\siggraph2013\.1.emf C:\Users\yuki\conference\siggraph2013\.2.emf 
C:\Users\yuki\conference\siggraph2013\.3.emf C:\Users\yuki\conference\siggraph2013\.5.emf Yuki Igarashi 
Jun Mitani University of Tsukuba, Japan Figure 1: System overview. (a) The user designs a textile pattern 
using a painting interface. (b) The user defines the direction of the cards. (c) The system shows the 
user how each yarn should pass through the card. (d) The user finally weaves the yarns using the cards. 
 1 Introduction Weaving is a method of fabric production, consisting of two distinct sets of yarns (warp 
and weft). It is popular and similar to other fabric production methods, such as knitting, felting, and 
lace making. In particular, card weaving is a very simple and easy weaving method. The user prepares 
nothing more than colored yarns and simple cardboard squares with four holes [Crockett 1991]. The user 
can produce exquisitely patterned woven bands, such as ribbons, straps, and hair accessories. However, 
the textile patterns are typically designed via a laborious manual process. The final textile design 
is determined by 1) the color of each warp yarn, 2) the direction of four yarns passing through each 
card, and 3) the direction and number of rotations of the cards. We therefore propose an interactive 
system to assist in the design of original weaving patterns and their construction. Coahranm and Fiume 
[2005] presented a sketch-based design system for quilting arts. In contrast, weaving textile design 
is more like pixel art (all pixels are the same size). The difference is that each pixel in a woven band 
is a diamond shape (not a square). The width (the number of columns) of a woven band is determined by 
the number of cards (n). The number of colors for each column is determined by the number of holes in 
a card (4 in our current system). The Card Weaver [Polak 2002] is a previous design system for card weaving. 
However, it only supported the design of patters with simple repetitions and did not support free painting 
described in the next section. 2 User Interface Figure 1 shows the overall process. The user first designs 
the color of warp yarns in the matrix of 4 × n cells, as shown in Figure 1(a). The user can increase/decrease 
the number of cards, and modify the direction of the four yarns passing through each card by clicking 
on the corresponding arrow (Fig. 2). The final textile design is defined by the direction and number 
of rotations of the cards (Fig. 1b). F4B4 means that the user first rotates all the cards four times 
in the forward direction and four times in the backward direction for this one set. The user repeats 
the set until he/she is satisfied with the length of the fabric. The user can check the final textile 
design in the system before real weaving. He/she then passes the yarn through the hole of each card using 
a construction guide (Fig. 1c). Finally, the user creates a real original weaving using these cards and 
yarns (Fig. 1d). The system also provides a special mode that supports free painting with binary colors 
(Fig. 3b). In this mode, the user passes the yarns with the foreground color in the adjacent two holes 
in a card and the yarns with the background color in the remaining two holes. The user then paints an 
arbitrary pattern on the band with the two colors. The system then automatically derives rotation direction 
of each card at each step from the pattern. The system also visualizes the orientation of the diamond 
shape at each cell, which is determined by the rotation direction at the cell. Figure 2: Final orientation 
of the diamond shape (each cell) depends on how the yarn passes through the hole, up or down.  Figure 
3: Snapshots of the system. (a) The standard mode. The user specifies the colors of 4 yarns of each card, 
as well as the rotation direction at each step. (b) The free painting mode. The user paints an arbitrary 
pattern and the system automatically derives the rotation direction of each card. 3 Results We used 
our system to create textile designs and actual weaving results, as shown in Figure 4. A design session 
typically took about 10 min, and production of the weaving required 1 3 hours. Figure 4 right shows 
woven bands in the standard mode (top) and in the free paining mode (bottom). Figure 4: Results using 
our system. References CROCKETT, C. 1991. Card Weaving. Interweave Pr. COAHRANM M. AND FIUME, E. 2005. 
Sketch-Based Design for Bargello Quilts. Eurographics Workshop on Sketch-Based Interfaces and Modeling 
2005, 165 174. POLAK, G. 2002. Card Weaver. http://www.theloomybin.com 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503479</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>86</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Modeling cell layers on complex surfaces using constrained Voronoi diagrams]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503479</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503479</url>
		<abstract>
			<par><![CDATA[<p>Voronoi diagrams, found in the morphology of many biological structures, produce natural-looking networks of cells in two dimensions. But creating three dimensional models of cell layers in biological tissue is difficult when the tissue has a complex shape. I present a new method that constrains a Voronoi diagram to an arbitrary surface. Unlike other approaches, my method does not require computationally expensive boundary intersection calculations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190904</person_id>
				<author_profile_id><![CDATA[82459074157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fiona]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wood]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard College]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fwood@college.harvard.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mackercher, P. A., Ivey, K. J., Baskin, W. N., and Krause, W. J. 1978. A scanning electron microscopic study of normal human oxyntic mucosa using blunt dissection and freeze fracture. <i>Am J Dig Dis 23</i>, 5, 449--59.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2177153</ref_obj_id>
				<ref_obj_pid>2177135</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yan, D.-M., Wang, W., L&#233;vy, B., and Liu, Y. 2010. Efficient computation of 3d clipped voronoi diagram. In <i>Proceedings of the 6th international conference on Advances in Geometric Modeling and Processing</i>, Springer-Verlag, Berlin, Heidelberg, GMP'10, 269--282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Modeling Cell Layers on Complex Surfaces Using Constrained Voronoi Diagrams Fiona Wood* Harvard College 
 (a) (b) (c) (d) Figure 1: Points on a sine wave illustrate which Voronoi regions share faces before 
(a) and after (b) the addition of augment points. Scanning electron microscope images (c) of gastric 
pits in human stomach lining [Mackercher et al. 1978] are closely mimicked by an extruded and smoothed 
mesh of a constrained Voronoi diagram (d) generated using my technique. Abstract Voronoi diagrams, found 
in the morphology of many biological structures, produce natural-looking networks of cells in two dimen­sions. 
But creating three dimensional models of cell layers in bi­ological tissue is dif.cult when the tissue 
has a complex shape. I present a new method that constrains a Voronoi diagram to an arbi­trary surface. 
Unlike other approaches, my method does not require computationally expensive boundary intersection calculations. 
1 Contribution A two-dimensional illustration of a layer of cells can be produced directly from the Voronoi 
diagram of a set of points or sites dis­tributed randomly inside a bounding shape. But despite the ubiq­uity 
of Voronoi diagrams in computer graphics applications, only a few efforts have been made to extend Voronoi-based 
models of cell layers to three dimensions. A signi.cant problem is that re­gions located on the boundary 
of a Voronoi diagram can have edges that extend very far from it. In two dimensions, most Voronoi re­gions 
are on the interior and are unaffected by the boundary con­ditions. In three dimensions, however, the 
regions on the bound­ary form the visible surface of the shape. One approach to this problem [Yan et 
al. 2010] is to generate a Voronoi diagram of sites placed throughout the entire volume enclosed by the 
surface and derive a network of cells from the intersections of the Voronoi dia­gram with the surface. 
My approach restricts site placement to the surface and uses additional sites to produce a Voronoi diagram 
that can be constrained using less computationally expensive methods. 2 Technical Approach First, points 
are placed on every triangular face of the surface us­ing the triangle point picking method, with the 
number of points generated for each triangle directly proportional to its area. Gener­ating each point 
in this way allows us to calculate a normal for each point using the barycentric average of the normals 
of the triangle s vertices. Next, additional Voronoi sites, called augment sites, are placed on either 
side of the surface (Figure 1b), effectively insulating the orig­ *e-mail:fwood@college.harvard.edu 
 inal Voronoi sites from all but their immediate neighbors. It is here that the barycentric normal of 
each Voronoi site is used to calcu­late the displacement of its pair of augment sites. The magnitude 
of the displacement must be chosen to ensure that for each original Voronoi site, its augment sites will 
be closer than any other sites. This method exploits the properties of Voronoi diagrams and elim­inates 
the need to search the boundary faces for intersections. The Voronoi diagram of the augmented set of 
sites is calculated us­ing the QHULL algorithm. Only faces shared by sites in the original point set 
before augmentation are recorded. All faces in the regions of augment sites and between an original site 
and an augment site are ignored. Once the 3D Voronoi diagram is generated from the augmented point set, 
several steps need to be taken to transfer the diagram into a network of cells (one polygon for each 
Voronoi region) on the original surface. The list of faces each Voronoi region shares with other regions 
is used to .nd the vertices describing a polygon on the original surface. Because these vertices are 
not in order, I employ a heuristic approximation of the clockwise order of the points around the Voronoi 
site to form a convex polygon. While a Centroidal Voronoi Tesselation (CVT) would not look or­ganic, 
minimizing the distance between each site and the centroid of its region improves the realism of the 
output. To relax the di­agram, I added a function to complete a few iterations of Lloyds algorithm. Extruding 
the faces of the relaxed diagram produces a mesh that can be smoothed (Figure 1d) to closely resemble 
refer­ ence images of real cell layers.  References MAC K E RC H E R, P. A., IVE Y, K. J., BAS K I N, 
W. N., A N D KR AU S E, W. J. 1978. A scanning electron microscopic study of normal human oxyntic mucosa 
using blunt dissection and freeze fracture. Am J Dig Dis 23, 5, 449 59. YA N , D.-M., WANG , W., L ´ 
E V Y, B., A N D LI U , Y. 2010. Ef.cient computation of 3d clipped voronoi diagram. In Proceedings of 
the 6th international conference on Advances in Geometric Modeling and Processing, Springer-Verlag, Berlin, 
Heidelberg, GMP 10, 269 282. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503480</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>87</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Street-side city modeling from ground-level imagery and a digital map]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503480</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503480</url>
		<abstract>
			<par><![CDATA[<p>With the development of massive ground-level imagery gathering methods, image based automatic reconstruction studies for 3D city models are increasing. While novel approaches have led to dramatically improved quality of the resulting 3D city model, some limitations still exist.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190905</person_id>
				<author_profile_id><![CDATA[82459250057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hyungki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology, Daejeon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[diskhkme@kaist.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190906</person_id>
				<author_profile_id><![CDATA[81319492558]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Soonhung]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Han]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Korea Advanced Institute of Science and Technology, Daejeon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shhan@kaist.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1618460</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jianxiong X., Tian F., Peng Z., Maxime L. and Long Q. 2008. Image-based Street-side City Modeling. <i>ACM Trams. Graph. 28</i>, 5.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2087789</ref_obj_id>
				<ref_obj_pid>2087756</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Zheng X., Zhang X. and Guo P. 2011. Building modeling from a single image applied in urban reconstruction. <i>VRCAI 11, 225--234</i>]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Street-side City Modeling from Figure 1: (a) The input is a digital map and ground-level imagery. From 
the footprint of the building in the digital map, we can obtain the bundle building geometry by extruding 
it. (b) We can predict where the buildings are in the picture. Recall that possible GPS inconsistencies 
between the digital map and the imagery will be corrected. (c) With the given building location and edge 
features from the bundle building as a clue, we can segment the image with a building façade and others. 
(d) From the segmented image, we can obtain the detailed shape of the façade and refine the geometry 
of the original bundle building. Further modeling process such as an image crop, warp and texturing will 
be straightforward.  Ground-Level Imagery and a Digital Map  Hyungki Kim*, Soonhung Han* Korea Advanced 
Institute of Science and Technology Daejeon, Korea {diskhkme, shhan}@kaist.ac.kr 1. Introduction With 
the development of massive ground-level imagery gathering methods, image based automatic reconstruction 
studies for 3D city models are increasing. While novel approaches have led to dramatically improved quality 
of the resulting 3D city model, some limitations still exist. In recent studies such as that by [Xiao 
2009], we can see high- quality buildings, despite the fact that the proposed approach is fully automated. 
However, the underlying risk with an image- based approach is that the resulting quality of the building 
geometry relies on the number of images obtained. Because a point cloud is constructed via the structure 
from motion (SfM) method, the number of feature points determines how many points can be obtained. In 
addition, the processing takes a long time. That is especially true for the image segmentation process. 
 In this paper, we propose a means of solving the aforementioned problems in the image-based approach. 
Our approach will create a robust result for each building in a 3D city model with a relatively short 
processing time through cooperation with digital maps. 2. Our Approach The key idea of our approach 
is its use of the footprint information of the buildings in a city. By extruding a footprint sketch, 
we initially gain the simple geometry of the building, as shown in Figure 1(a). We will call it as a 
Bundle Building. This bundle building contains a geo-registered location and the location of each edge 
resulting from the extrusion process. Then, for each building, we search for the closest picture position 
and insert vertical edges of the bundle building image into the ground-level image, as shown in Figure 
1(b). Two issues can be expected at this stage. The first one is image distortion, in which the edges 
are not straight. This can be handled by an image rectification method or by remapping image with appropriate 
projection methods. The second issue is GPS inconsistency between data. This can be handled by adjusting 
the location of the ground imagery based on the bundle building to minimize the error of detected edges 
during the early segmentation stage, as depicted in Figure 1(c). The overall segmentation process will 
be similar with [Zheng 2011], which also addressed 3D reconstruction of building from few images. In 
this way, we gain clues about which parts of the image are occupied by the building. Thus, we can shorten 
the segmentation time compared to an approach based solely on image. The last step involves refining 
the geometry, as shown in Figure 1(d). Because the footprint sketch does not give details pertaining 
to the façade geometry, a single-image-based reconstruction method can be applied at this stage to refine 
the geometry. The remaining parts, such as image cropping and texturing, are quite straightforward. 
Currently, a company named NHN and ENGIT has agreed to provide the street-level imagery of Korea for 
research use, and at present we are conducting basic research on our approach. We are expecting that 
our approach will lead to robust results in a short time, demonstrating a fully automated process. References 
JIANXIONG X., TIAN F., PENG Z., MAXIME L. AND LONG Q. 2008. Image-based Street-side City Modeling. ACM 
Trams. Graph. 28, 5. ZHENG X., ZHANG X. AND GUO P. 2011. Building modeling from a single image applied 
in urban reconstruction. VRCAI 11, 225-234 Acknowledgement This work was supported by Agency for Defense 
Development under the contract UD110039DD, Korea 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503481</article_id>
		<sort_key>960</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>88</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Unsupervised cell identification on multidimensional X-ray fluorescence datasets]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503481</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503481</url>
		<abstract>
			<par><![CDATA[<p>X-ray fluorescence microscopy is a powerful technique to map and quantify trace element distributions in biological specimens. It is perfectly placed to map nanoparticles and nanovectors within cells, at high spatial resolution. Advances in instrumentation, such as faster detectors, better optics, and improved data acquisition strategies are fundamentally changing the way experiments can be carried out, giving us the ability to more completely interrogate samples, at higher spatial resolution, higher throughput and better sensitivity. Yet one thing is still missing: the next generation of data analysis and visualization tools for multidimensional microscopy that can interpret data, identify and classify objects within datasets, visualize trends across datasets and instruments, and ultimately enable researchers to reason with abstraction of data instead of just with images.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190907</person_id>
				<author_profile_id><![CDATA[82458972957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Siwei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonne National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[siweiw@mcs.anl.gov]]></email_address>
			</au>
			<au>
				<person_id>P4190908</person_id>
				<author_profile_id><![CDATA[82458836557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jesse]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ward]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonne National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wardj@aps.anl.gov]]></email_address>
			</au>
			<au>
				<person_id>P4190909</person_id>
				<author_profile_id><![CDATA[81100552843]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sven]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Leyffer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonne National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[leyffer@mcs.anl.gov]]></email_address>
			</au>
			<au>
				<person_id>P4190910</person_id>
				<author_profile_id><![CDATA[81413596326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wild]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonne National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wild@mcs.anl.gov]]></email_address>
			</au>
			<au>
				<person_id>P4190911</person_id>
				<author_profile_id><![CDATA[82459126757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jacobsen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonne National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cjacobsen@anl.gov]]></email_address>
			</au>
			<au>
				<person_id>P4190912</person_id>
				<author_profile_id><![CDATA[81556284056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Stefan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vogt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Argonne National Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[vogt@aps.anl.gov]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2404556</ref_obj_id>
				<ref_obj_pid>2404509</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Arteta, C., Lempitsky, V., Noble, J. A., and Zisserman, A. 2012. Learning to detect cells using non-overlapping extremal regions. In <i>Proceedings of the 15th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI12)</i>, Springer-Verlag, Berlin, 348--356.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bergeest, J.-P., and Rohr, K. 2012. Efficient globally optimal segmentation of cells in fluorescence microscopy images using level sets and convex energy functionals. <i>Medical Image Analysis 16</i>, 7 (Oct.), 1436--1444.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Unsupervised Cell Identi.cation on Multidimensional X-Ray Fluorescence Datasets Siwei Wang*, Jesse Ward, 
Sven Leyffer , Chris Jacobsen¶, Stefan Wild§, Stefan Vogtl Argonne National Laboratory Figure 1: Data 
used for subsequent analysis. A mixture of three cell types was prepared, as shown in the visible light 
micrograph: red blood cells, algae, and yeast. The maps of particular elements (K, P, Mn, Fe, and Zn) 
obtained from X-ray .uorescence images hint at the characteristics of the different cell types: Mn is 
prevalent in algae and Fe in red blood cells, while Zn and P are indicative of yeast cells. The visible 
light micrograph was acquired at one focal plane and thus does not show all cells; separate slight distortions 
in relative cell positions between the X-ray .uorescence maps and the visible light micrograph were not 
adjusted for. 1 Introduction and Motivation X-ray .uorescence microscopy is a powerful technique to map 
and quantify trace element distributions in biological specimens. It is perfectly placed to map nanoparticles 
and nanovectors within cells, at high spatial resolution. Advances in instrumentation, such as faster 
detectors, better optics, and improved data acquisition strate­gies are fundamentally changing the way 
experiments can be car­ried out, giving us the ability to more completely interrogate sam­ples, at higher 
spatial resolution, higher throughput and better sen­sitivity. Yet one thing is still missing: the next 
generation of data analysis and visualization tools for multidimensional microscopy that can interpret 
data, identify and classify objects within datasets, visualize trends across datasets and instruments, 
and ultimately en­able researchers to reason with abstraction of data instead of just with images. We 
will present a novel approach to locate, identify, and re.ne po­sitions and whole areas of cell structures 
based on elemental con­tents measured by X-ray .uorescence microscopy. We show that by initializing with 
only a handful of prototypical cell regions, this ap­proach can obtain consistent cell populations, even 
when cells are partially overlapping, without training by explicit annotation. It is *e-mail:siweiw@mcs.anl.gov 
e-mail:wardj@aps.anl.gov e-mail:leyffer@mcs.anl.gov §e-mail:wild@mcs.anl.gov ¶e-mail:cjacobsen@anl.gov 
Ie-mail:vogt@aps.anl.gov elemental maps (h) shows that this area contains four red blood cells and one 
yeast cell, which overlaps one of the red blood cells. The end con.guration in (g) shows that the estimation 
procedure identi.es this con.guration correctly. Group boundaries are shown as ellipses for illustration 
only; all operations are based on taking the union of pixels. robust both to different measurements on 
the same sample and to different initializations. This effort provides a versatile framework to identify 
targeted cellular structures from datasets too complex for manual analysis, like most X-ray .uorescence 
microscopy data. 2 Our Approach We start by thresholding pixels into foreground/background com­ponents 
based on their elemental content, then obtain an initial guess of the cells based on segmentation of 
the foreground pixels. We then use a generalized likelihood ratio test to improve the cell con.gurations 
and to re.ne these putative cell areas with respect to the multiple elemental distributions simultaneously. 
One of the strengths of this algorithm is its ability to identify and distinguish even overlapping objects 
(several recent methods can handle only samples that are at most touching at the boundaries [Arteta et 
al. 2012; Bergeest and Rohr 2012]). We will demonstrate the approach on a dataset with three cell types 
shown in Fig 1 we acquired at beamline 2-ID-E of the Advanced Photon Source at Argonne Na­tional Laboratory. 
In this dataset, we identi.ed around 320 cells with many regions of strong overlap. References ART E 
TA , C., LE M P I T S K Y, V., NO B L E , J. A., A N D ZI S S E R M A N , A. 2012. Learning to detect 
cells using non-overlapping ex­tremal regions. In Proceedings of the 15th International Confer­ence on 
Medical Image Computing and Computer-Assisted In­tervention (MICCAI12), Springer-Verlag, Berlin, 348 
356. BE R G E E S T, J.-P., A N D RO H R , K. 2012. Ef.cient globally op­timal segmentation of cells 
in .uorescence microscopy images using level sets and convex energy functionals. Medical Image Analysis 
16, 7 (Oct.), 1436 1444. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503482</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>89</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Vuvuzela]]></title>
		<subtitle><![CDATA[a facial scan correspondence tool]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503482</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503482</url>
		<abstract>
			<par><![CDATA[<p>When scanning an actor's face in multiple static facial expressions, it is often desirable for the resulting scans to all have the same topology and for the textures to all be in the same UV space. Such "corresponded" scans would enable the straightforward creation of blendshape-based facial rigs. We present Vuvuzela, a semi-automated facial scan correspondence tool. Vuvuzela is currently being used in our facial rigging pipeline, and was one of the key tools in the Digital Ira project.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190913</person_id>
				<author_profile_id><![CDATA[81414619646]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ichikari]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190914</person_id>
				<author_profile_id><![CDATA[81458649961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oleg]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alexander]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oalexander@ict.usc.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190915</person_id>
				<author_profile_id><![CDATA[81100086933]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Paul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Debevec]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[USC Institute for Creative Technologies]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ekman, P., and Friesen, W. 1978. <i>Facial Action Coding System: A Technique for the Measurement of Facial Movement</i>. Consulting Psychologists Press, Palo Alto.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024163</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ghosh, A., Fyffe, G., Tunwattanapong, B., Busch, J., Yu, X., and Debevec, P. 2011. Multiview face capture using polarized spherical gradient illumination. <i>ACM Trans. Graph. 30</i>, 6 (Dec.), 129:1--129:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Vuvuzela: A Facial Scan Correspondence Tool Ryosuke Ichikari Oleg Alexander Paul Debevec USC Institute 
for Creative Technologies oalexander@ict.usc.edu  (a) (b) (c) (d) (e) Figure 1: (a) Source. (b) Target. 
(c) Vuvuzela work.ow. (d) Warped source. (e) Difference between warped source and target. 1 Introduction 
When scanning an actor s face in multiple static facial expressions, it is often desirable for the resulting 
scans to all have the same topol­ogy and for the textures to all be in the same UV space. Such corresponded 
scans would enable the straightforward creation of blendshape-based facial rigs. We present Vuvuzela, 
a semi­automated facial scan correspondence tool. Vuvuzela is currently being used in our facial rigging 
pipeline, and was one of the key tools in the Digital Ira project. 2 Our Approach Our rig building process 
begins by scanning an actor s face in our Light Stage X device [Ghosh et al. 2011]. We capture a set 
of about 30 static facial expressions, roughly corresponding to Action Units from the Facial Action Coding 
System [Ekman and Friesen 1978]. We also capture a master neutral expression, which becomes the target 
scan in our correspondence pipeline. Rather than storing our scans as geometry and textures, we choose 
instead to store our scans as images. Each one of our scans is stored as a set of 4K, 32 bit .oat EXR 
images, including diffuse, specular, specular normals, and a high resolution point cloud. The maps are 
in a cylindrically unwrapped UV space, representing our ear to ear data. However, the UV space differs 
slightly for each expression scan. Vuvuzela exploits this image-based scan represantation by doing the 
scan correspondence in 2D rather than 3D. Vuvuzela takes as input two scans: one of the expressions as 
the source and the neu­tral expression as the target. Vuvuzela provides an OpenGL UI, allowing the user 
to interact with the scans in 3D. The scans are rendered with the diffuse textures only, and all of the 
correspon­dence processing uses only the diffuse textures. The user clicks corresponding points in the 
source and target scans, such as corners of the eyes and lips, and other facial landmarks. We found that 
we don t need to put dots or markers on the face during scanning, because there is plenty of naturally 
occuring texture in the face, especially when over-sharpened. The placement of the correspondence points 
doesn t have to be exact the points are used only as an initialization by our algorithm. Once enough 
points have been placed, the user presses the Update button, which triggers our correspondence algorithm. 
The result is displayed to the user and the UI offers several modes to pre­view the quality of the correspondence, 
including a blendshape slider blending both geometry and/or texture. The user can then add, delete, or 
edit points, and repeat the process until a high qual­ity correspondence is achieved. Our algorithm has 
three steps and runs in 2D. First, we construct a Delaunay triangulation between the user supplied points 
and ap­ply af.ne triangles to roughly pre-warp the source diffuse texture to the target. Second, we use 
GPU-accelerated optical .ow to com­pute a dense warp .eld from the pre-warped source diffuse texture 
to the target. Finally, we apply the dense warp to each one of our source texture maps, including diffuse, 
specular, specular normals, and point cloud. The result is the source scan warped to the tar­get UV space. 
The submillimeter correspondence is able to align individual pores across the majority of the face. Some 
expressions are more challenging to correspond than others. Especially expressions with lots of occlusions, 
like mouth open to mouth closed. In such cases, optical .ow will fail to get a good re­sult. We assist 
optical .ow in two ways. First, we paint black masks around occlusion regions in both source and target 
diffuse textures. Second, we mark some points as pinned and those points are ras­terized into small black 
dots at runtime. Using both of these tech­niques in combination usually produces good results even in 
the toughest cases. A useful byproduct of Vuvuzela is the ability to generate blend­shapes directly from 
the corresponded scans. First, we remesh the neutral scan, creating an artist mesh with artist UVs. Then 
we load the artist mesh into Vuvuzela and export the blendshapes for all the scans by looking up vertex 
positions in the warped point clouds. All the texture maps are also warped into the artist UV space, 
which is simply an additional af.ne triangles 2D warp. The result is a set of blendshapes and texture 
maps ready to hand off to the facial rigger. References EK M AN, P., AND FRI ES EN , W. 1978. Facial 
Action Coding Sys­tem: A Technique for the Measurement of Facial Movement. Consulting Psychologists Press, 
Palo Alto. GH O S H , A., FY FF E , G., TU N WATTA NAPON G , B., BU SC H , J., YU, X., AN D DEB E V EC 
, P. 2011. Multiview face capture using polarized spherical gradient illumination. ACM Trans. Graph. 
30, 6 (Dec.), 129:1 129:10. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503483</section_id>
		<sort_key>980</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Music and sound]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>2503484</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>90</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Affective music recommendation system using input images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503484</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503484</url>
		<abstract>
			<par><![CDATA[<p>Music that matches our current mood can create a deep impression, which we usually want to enjoy when we listen to music. However, we do not know which music best matches our present mood. We have to listen to each song, searching for music that matches our mood. As it is difficult to select music manually, we need a recommendation system that can operate affectively. Most recommendation methods, such as collaborative filtering or content similarity, do not target a specific mood. In addition, there may be no word exactly specifying the mood. Therefore, textual retrieval is not effective. In this paper, we assume that there exists a relationship between our mood and images because visual information affects our mood when we listen to music. We now present an affective music recommendation system using an input image without textual information.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190916</person_id>
				<author_profile_id><![CDATA[82458775857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joudanjanai-ss@akane.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190917</person_id>
				<author_profile_id><![CDATA[82458932157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tatsunori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190918</person_id>
				<author_profile_id><![CDATA[81504683299]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hayato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190919</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Russell, J., 1980. A Circumplex Model of Affect, <i>Journal of Personality and Social Psychology</i> 1980, pp.1161--1178]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Valdez, P., and Mehrabian, A. 1994. Effects of Color on Emotions, <i>Journal of Experimental Psychology</i> 1994, pp.394--409]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Eerola, T., Lartillot, O., and Toiviainen, P. 2009. Prediction of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models, <i>Proc. International Society for Music Information Retrieval Conference</i> 2009, pp.621--626]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: Outline of Our System. Affective Music Recommendation System using Input Images Figure 2: 
Result of Evaluation Experiment.   Shoto SASAKI*   Tatsunori HIRAI Hayato OHYA   Shigeo MORISHIMA 
 Waseda University / JST CREST  1. Introduction Music that matches our current mood can create a deep 
impression, which we usually want to enjoy when we listen to music. However, we do not know which music 
best matches our present mood. We have to listen to each song, searching for music that matches our mood. 
As it is difficult to select music manually, we need a recommendation system that can operate affectively. 
Most recommendation methods, such as collaborative filtering or content similarity, do not target a specific 
mood. In addition, there may be no word exactly specifying the mood. Therefore, textual retrieval is 
not effective. In this paper, we assume that there exists a relationship between our mood and images 
because visual information affects our mood when we listen to music. We now present an affective music 
recommendation system using an input image without textual information. Our system matches an input image 
with music using an emotional plane. Russell [1980] proposed the valence/arousal model of affect. Valence 
refers to positive negative associations with affective phenomena, whereas arousal refers to energetic 
calm associations. All affective stimuli can thereby be defined as a combination of these two independent 
dimensions. Thus, it is reasonable to use this emotional plane (V-A plane) for matching music and an 
image. 2. Proposed System The system consists of three parts. Figure 1 outlines the system. The first 
part involves setting the image on the V-A plane using color and texture features. Valdez et al. [1994] 
demonstrated by means of equations the relationship of saturation S, brightness B with valence V1, arousal 
A1. These equations are given below. (1) (2) Meanwhile, we can establish associations of coarseness C, 
direction D with valence V2, arousal A2 using canonical component analysis as follow. (  )   (3) ( 
  )   (4) The learning data is the International Affective Picture System (IAPS), which is a standard 
affective image set used in psychology. We calculate V-A values using these correlations. In addition, 
the system integrates (V1, A1) with (V2, A2) on the basis of distances from the origin of the V-A plane. 
This is based on impressions being stronger toward the edge of the V-A plane than at the origin. The 
second part involves setting the user s music onto the V-A plane. Eerola et al. [2009] have shown using 
principal components analysis (PCA) that acoustic features can be related to valence and arousal. In 
this paper, we use the RWC Music database, which has equally populated sets of music categories. Coefficients 
are computed by PCA with the acoustic features of the RWC Music database. We then multiply these coefficients 
by the acoustic features of the user s music, thus obtaining V-A values for that music. The third part 
is musical recommendation. The system calculates the Euclidean distances between an input image and the 
user s music on the V-A plane. The system then arranges the distances in ascending order, thereby creating 
a recommended playlist. * e-mail: joudanjanai-ss@akane.waseda.jp e-mail: shigeo@waseda.jp      
    3. Evaluation Our proposed method was assessed using subjective evaluation by the random selection 
method. Sixteen questions were asked to a group of 17 male and female examinees, all in their twenties. 
The subjects evaluated at five levels on which music is better matched with the image on display. A score 
of 5 indicated that recommended music matched the image on display. A score of 1 indicated that randomly 
selected music matched the image on display. Figure 2 shows the scores of this experiment. The average 
score is 3.89. Therefore, our proposal technique is confirmed to be effective. 4. Discussion and Conclusion 
The results show that questions 6, 9, and 10 have not obtained good scores with the proposed method, 
particularly question 6, which has the lowest score. The system set question 6 at the origin of the V-A 
plane, indicating neutral emotion. Hence, images or music in this area tend to affect individuals differently. 
Therefore, an interactive system is necessary to deal with individuality. For instance, if a user pushes 
a skip button, the system can learn the user s preferences and make subsequent recommendations by taking 
them into account. In this paper, we have presented an affective music recommendation method based on 
an input image using an emotional plane without textual information. Further work on adding an interactive 
system is being considered. Moreover, the system can be extended to video input to correspond with change 
of scenery. References RUSSELL, J., 1980. A Circumplex Model of Affect, Journal of Personality and Social 
Psychology 1980, pp.1161-1178 VALDEZ, P., AND MEHRABIAN, A. 1994. Effects of Color on Emotions, Journal 
of Experimental Psychology 1994, pp.394-409 EEROLA, T., LARTILLOT, O., AND TOIVIAINEN, P. 2009. Prediction 
of Multidimensional Emotional Ratings in Music from Audio Using Multivariate Regression Models, Proc. 
International Society for Music Information Retrieval Conference 2009, pp.621-626 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503485</article_id>
		<sort_key>1000</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>91</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Celestia]]></title>
		<subtitle><![CDATA[a vocal interaction music game]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503485</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503485</url>
		<abstract>
			<par><![CDATA[<p>Voice is one of the most natural means of expression and vocal interaction is gaining popularity in game development field [1]. For example, a karaoke-style video game Sing Party published by Nintendo for the Wii U, Guitar Hero in which users use a guitar-shaped game controller to match notes scrolling on screen. Despite advances in the diversity of vocal interactive game, the majority on the market focuses on the task instead of visualizing complex, numerical pitch data as a form of aesthetic art.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190920</person_id>
				<author_profile_id><![CDATA[82458888457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cheng]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190921</person_id>
				<author_profile_id><![CDATA[82458774257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Carnegie Mellon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1358695</ref_obj_id>
				<ref_obj_pid>1358628</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Sri H. Kurniawan and Adam J. Sporka. 2008. Vocal interaction. In <i>CHI '08 Extended Abstracts on Human Factors in Computing Systems</i> (CHI EA '08). ACM, New York, NY, USA, 2407--2410.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Rohde and Schwarz, Dr. Florian Ramian - <i>Implementation of Real-Time Spectrum Analysis</i>, p. 6, January, 2011, accessed August 9, 2011]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Celestia: A Vocal Interaction Music Game Cheng Yang, Yang Shi, Carnegie Mellon University  1 Introduction 
Voice is one of the most natural means of expression and vocalinteraction is gaining popularity in game 
development field [1]. For example, a karaoke-style video game Sing Party published by Nintendo for the 
Wii U, Guitar Hero in which users use aguitar-shaped game controller to match notes scrolling on screen.Despite 
advances in the diversity of vocal interactive game, themajority on the market focuses on the task instead 
of visualizingcomplex, numerical pitch data as a form of aesthetic art. Reflecting on musical artwork 
by Robert Hodgin, we createdCelestia. Incorporating visualization technique, the game usesvoice input 
based on pitch detection as a primary means ofcontrol, and provides insight into innovation of vocal 
interaction.In regard to gameplay, the purpose is to guide a newborn starthrough the universe with melody. 
The user's voice can enlargethe star to absorb smaller planets and survive encounters withcomets, nebulae. 
Through the journey, the star gains more energy, displayed through series of changing colors. Everyelement 
of the experiential aesthetic is tied to the music; the background constellation is the music visualization 
with threedifferent colors reacting to high, mid and bass range of thesoundtrack in real-time. We introduced 
Celestia to a vocalist who volunteered to improvise the game for a live audience. Theshow turned out 
to be a great success and we received positive feedbacks like it s visually and aurally appealing . 
2 Our Approach Celestia is a PC Console Game coded in Processing, an open-source programming framework 
extended from Java. Contributed libraries, Ani and Minim, are included to extend thecore functionality. 
Celestia detects three specific pitches that arethe roots of major chord of the background music. The 
purposeof the setting is to help users tune by guiding their ear to thetonic chord of background music. 
Users can improvise a melodyby connecting these pitches together as game progresses. In addition to identifying 
pitch, Celestia responds to amplitude, thelouder the volume is, the more powerful the effect will be 
on thepremise that users hit the given pitch. We optimized Celestia around how users can or want to playwith 
it rather than forcing them to change their behavior for the game. We adopted two different pitch ranges 
to accommodateboth female and male voices (pitches D3, G3, D4 for female,pitches A2, C3, D#3 for male). 
Users have the option to choose the register they feel more comfortable in the calibration settings. 
Additionally, Celestia is not confined to human voice, users canplay instruments, such as guitar, piano 
or harmonica to run thegame, or more flexibly, make customized vocal tool like whistle or water bells 
by filling drinking glasses with different amountsof water. Celestia adopts Fast Fourier Transform (FFT, 
which providesfast way to transfer signal into frequency domain) [2] class inthe built-in minim library 
of Processing for spectrum analysis. After buffering the line-in audio signal, we used logarithmicaverages 
to group the spectrum into 10 octaves, and then dividedeach octave into 12 bands according to the Equal 
Temperaments.Through individual volume detection from 120 bands, we gotthe volume changes of every single 
semitone. We selected featured pitches as triggers according to the music chord featureand human voice 
range. We exploited an entertaining way to artistically visualizeacoustic input, conveying informative 
feedback to shape users'appreciation of their vocal quality and to encourage users to explore other musical 
input possibilities to control the gameplay.Additionally, pitch can be technically perceived as an intermediate 
signal between discrete and continuous, while Celestia defines one set of pitches according to the backgroundmusic 
choice, more selections can be made based on differentpreferences of background music for future developing 
purpose. 3 Reference [1] Sri H. Kurniawan and Adam J. Sporka. 2008. Vocal interaction. In CHI '08 Extended 
Abstracts on Human Factors in Computing Systems (CHI EA '08). ACM, New York, NY, USA, 2407-2410. [2] 
Rohde and Schwarz, Dr. Florian Ramian -Implementation of Real-Time Spectrum Analysis, p. 6, January, 
2011, accessed August 9, 2011 Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503486</article_id>
		<sort_key>1010</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>92</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Conducting and performing virtual orchestra]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503486</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503486</url>
		<abstract>
			<par><![CDATA[<p>There are a lot of people who have had yearning for conducting orchestra. It must be a very pleasant experience to coordinate orchestra performance with your own conduct, but it requires a vast amount of money. With such needs, there have been researches to simulate the situation of conducting orchestra by using gesture recognition [Usa][Baba][Sunagawa]. But, they do not generate performance scenes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190922</person_id>
				<author_profile_id><![CDATA[82459330757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Misato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Susaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[m-susaki@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190923</person_id>
				<author_profile_id><![CDATA[82458902557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Soichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sunagawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sunagawa@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190924</person_id>
				<author_profile_id><![CDATA[81331499700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moriya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[moriya@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190925</person_id>
				<author_profile_id><![CDATA[81319502327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tokiichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UEI Research, Adachi-ku, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[toki@vcl.im.dendai.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Usa, S., et al.1998. A Multi-modal Conducting Simulator. In <i>Intn'l Computer Music Conf. (Proc. ICMC</i>'98), pp.25--32.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baba, T., et al. 2010. VirtualPhilharmony: A Conducting System with Heuristics of Conducting an Orchestra. In <i>New Interfaces for Musical Expression (Proc. NIME</i> 2010), pp.263--270 (2010).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Sunagawa, S., et al. 2011. Musical Instrument Performance Control by IR-LED Sensor-Based Gesture Recognition, In <i>Visual Computing Sympo., Article</i> 41 (2011). (in Japanese).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1018219</ref_obj_id>
				<ref_obj_pid>1018024</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Schertenleib, S., et al. Conducting a Virtual Orchestra, <i>IEEE Multimedia, vol</i>.11, no.3 (July/September 2004) 40--49.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Conducting and Performing Virtual Orchestra C:\Users\Misato Susaki\Desktop\IWAIT\.\oc_susaki.png C:\Users\toki\Documents\2012-VCL\20130213-SIGGRAPH\....\3.png 
C:\Users\Misato Susaki\VCL\..\SIGGRAPH2013\.\..2.png Misato Susaki Soichiro Sunagawa Tomoaki Moriya 
Tokiichiro Takahashi Tokyo Denki University UEI Research 5 Adachi-ku, Tokyo, Japan { m-susaki | sunagawa 
| moriya | toki }@vcl.im.dendai.ac.jp   Figure 1. Performance Scene of Virtual Orchestra. Figure 2. 
Conducting Virtual Orchestra.  1. Introduction There are a lot of people who have had yearning for conducting 
orchestra. It must be a very pleasant experience to coordinate orchestra performance with your own conduct, 
but it requires a vast amount of money. With such needs, there have been researches to simulate the situation 
of conducting orchestra by using gesture recognition [Usa][Baba][Sunagawa]. But, they do not generate 
performance scenes. Schertenleib, et al., generated performance scenes by using 3DCG model instruments 
[Schertenleib]. However, performance information such as slur is not considered at generation of performance 
scenes. Our research aims at representing the situation that an orchestra is playing with 3DCG by considering 
performance scene generation. Our purpose is to represent the atmosphere close to the one a user actually 
conducts the orchestra. 2. System Functions We have developed a system which installs three functions 
to realize conducting and performing virtual orchestra. Details of three functions are described in the 
following sub-chapters. We have newly built up a performance scene generation function utilizing the 
performance information obtained from the music piece playback and the performance control function. 
2.1 Conductor s Baton Movement Chart Recognition Function When a user takes a gesture of conducting baton, 
the system recognizes the gesture. We experimentally built a conductor s baton type controller, in which 
IR-LED is installed as an input interface. The conductor s baton movement chart drawn with the baton 
is represented as tangential direction at the arbitrary point on the chart as the quantized 16 directional 
chain code. Our system based on Hidden Markov Model recognizes the conducting gesture precisely by representing 
such fluctuations as probability. 2.2 Piece Playback and Performance Control Function When a user changes 
a gesture of conducing baton, the system can play a piece of music by changing its tempo and volume. 
The recognition results are obtained discretely. However, as a piece of music has to be played in a series, 
future playback tempo has to be estimated from the past beat recognition result. Our system predicts 
the future tempo from the beat information of a certain amount of time in the past by using least-square 
approach. Due to this playback method based on the prediction, music continues without any break. 2.3 
Performance Scene Generation Function The system needs to equip a function to visualize the situations 
of the orchestra being performed by using 3DCG, in synchronization with the music piece playback and 
the performance control function described in the previous sub-chapter. In addition, this system analyzes 
the messages sent from MIDI in details and seeks for the information regarding performance techniques 
such as slur and bowing, pizzicato (see Figures 3, 4, and the supplemental movie). It generates the performance 
scene according to the performance information acquired automatically and in real time.  Figure 3. 
Slur. Figure 4. Pizzicato and Bowing.  3. Results Our system applies Microsoft s XNA and represents 
the orchestra performance scene with 3DCG model instruments in animation. Figures 1, 2, and the supplemental 
movie are the representation of the virtual orchestra with the application of XNA. 4. Conclusions We 
have realized the method to control musical performance by recognizing conductor s conducting gesture, 
and the method to generate 3DCG animation of instruments played. This system has made the experience 
of orchestra conduct possible. References USA, S., ET AL.1998. A Multi-modal Conducting Simulator. In 
Intn l Computer Music Conf. (Proc. ICMC'98), pp.25-32. BABA, T., ET AL. 2010. VirtualPhilharmony: A Conducting 
System with Heuristics of Conducting an Orchestra. In New Interfaces for Musical Expression (Proc. NIME 
2010), pp.263-270 (2010). SUNAGAWA, S., ET AL. 2011. Musical Instrument Performance Control by IR-LED 
Sensor-Based Gesture Recognition, In Visual Computing Sympo., Article 41 (2011). (in Japanese). SCHERTENLEIB, 
S., ET AL. Conducting a Virtual Orchestra, IEEE Multimedia, vol.11, no.3 (July/September 2004) 40-49. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503487</article_id>
		<sort_key>1020</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>93</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Musical flocks]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503487</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503487</url>
		<abstract>
			<par><![CDATA[<p><i>Musical Flocks</i> is a project in the field of music visualization. It produces animations by simulating the behavior of agents that react to the sound of music. Additionally, swarm-like behavior is attained by following the rules of separation, alignment and cohesion (Reynolds, 1987). This process produces reactive animations and static artifacts that constitute abstract representations of the pieces, with different genres resulting in artifacts with distinctive visual properties (see Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190926</person_id>
				<author_profile_id><![CDATA[82458942057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ruslan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamolov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ruslan@student.dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4190927</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4190928</person_id>
				<author_profile_id><![CDATA[81318497551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pmcruz@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>37406</ref_obj_id>
				<ref_obj_pid>37401</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C., 1987. Flocks, herds and schools: A distributed behavioral model. <i>Computer graphics and interactive techniques (SIGGRAPH '87 Proceedings of the 14th annual conference)</i>, ACM, pp. 25--34.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bown, O., 2008. <i>Beads</i> {online}. Available at: &lt;www.beadsproject.net&gt; {Accessed 16 December 2012}.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Musical Flocks Ruslan Kamolov1, Penousal Machado2 and Pedro Cruz3 CISUC, Department of Informatics Engineering, 
University of Coimbra  Figure 1 Three artifacts resulting from the visualization of music pieces of 
three different genres. 1 Introduction Musical Flocks is a project in the .eld of music visualization. 
Itproduces animations by simulating the behavior of agents thatreact to the sound of music. Additionally, 
swarm-like behavior isattained by following the rules of separation, alignment and cohe­sion (Reynolds, 
1987). This process produces reactive animationsand static artifacts that constitute abstract representations 
of thepieces, with different genres resulting in artifacts with distinctivevisual properties (see Figure 
1). 2 Approach Each agent, or boid, senses speci.c frequency intervals, reacting to the volume level 
of those frequencies and to the average vol­ume level of all frequencies in the hearing range. The application 
reads the music .le and performs a real-time analysis of the sound spectrum. The analysis of the spectrum 
s frequencies is performed using the external library beads (Bown, 2008), with the imple­mented Fast 
Fourier Transform function. The processed spectrumconsists of an array of 256 samples, each one having 
an associated volume value, being represented on screen by 256 boids. During playback, the .ock moves 
inside the application s window and reacts to the volume of the sound, leaving movement tracks. Thevariation 
of the properties of each boid represents the volumefrequencies of the music being played. In silence, 
the behavioralchanges of the .ock cease to exist and the group stops moving. by volume, perturbing its 
movement's direction, with high vol­umes resulting in large perturbations. The average volume of allfrequencies 
determines the activation of these behavioral modi.­ers, which only become active above a given threshold. 
The global behavior of the .ock depends on the following pa­rameters: the separation force that prevents 
a boid from colliding with neighbors; the alignment force that makes a boid steer to­wards the same direction 
as its neighbors; the cohesion force that makes a boid steer to the center of its neighbors, staying 
within the group; and the strength ratio applied to each of the threeforces. Different parameterizations 
of the .ock s and boids prop­erties modify the overall dynamics of the .ock and of its mem­bers, affecting 
the visual results. 3 Results The visual results depend on various aspects, such as the genreand the 
tempo of the music, the intensity of the sound, and theinstruments played. These aspects modify the temporal 
evolution of the music and its frequency spectrum at every frame, resulting in differentiated visual 
artifacts. Slow music makes the .ock react gently and move slowly, while a fast music tempo results in 
fastmovement and abrupt changes of direction. Sounds with high volume and rich frequency spectrum affect 
the majority of theboids, while low volume level and less quantity of active frequen­cies produce subtle 
visual variations and a slower graphic evolu­tion. Music with a rich and uninterrupted sound patterns 
createscontinuous black paths accompanied by large red blots. Musicwith pauses and frequent variations 
generates discontiguous and scattered black and red stains. The .nal artifacts generated by music of 
different genres vary in accordance with the changes in behavior of each agent and of the entire group, 
resulting in dis­tinctive artifacts for each genre. Acknowledgements This research is partially funded 
by FEDER through POFC COMPETE, project VisualyzARt with reference QREN 23201.  References REYNOLDS, 
C., 1987. Flocks, herds and schools: A distributed behavioral model. Computer graphics and interactive 
tech­ niques (SIGGRAPH '87 Proceedings of the 14th annual con­ ference), ACM, pp. 25-34. 1 ruslan@student.dei.uc.pt 
2 machado@dei.uc.pt BOWN, O., 2008. Beads [online]. Available at: 3 pmcruz@dei.uc.pt <www.beadsproject.net> 
[Accessed 16 December 2012]. Permission to make digital or hard copies of part or all of this work for 
personal or classroom use isgranted without fee provided that copies are not made or distributed for 
commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>FEDER through POFC - COMPETE, project VisualyzARt</funding_agency>
			<grant_numbers>
				<grant_number>QREN 23201</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503488</article_id>
		<sort_key>1030</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>94</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[RAKUGACKY]]></title>
		<subtitle><![CDATA[making sounds with drawing]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503488</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503488</url>
		<abstract>
			<par><![CDATA[<p>Images and sounds are closely related, and sounds could emphasis or change impressions of images. Thus several methods to synthesize sounds from images are developed [Hermann et al. 1999]. In this paper, we propose a novel interactive media system "RAKUGACKY" that can make sound with drawing. In this system, a user draws a picture on a screen, and the system synthesizes sounds for the picture automatically. The system analyzes the picture during drawing, and synthesizes or changes sounds interactively according to the analyzing result.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190929</person_id>
				<author_profile_id><![CDATA[82459169757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Saori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aichi Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[b13718bb@aitech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190930</person_id>
				<author_profile_id><![CDATA[82458721057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Nanako]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kondo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aichi Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190931</person_id>
				<author_profile_id><![CDATA[81547432256]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shinji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizuno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Aichi Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s_mizuno@aitech.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hermann T., Ritter H., 1999, Listen to your Data: Model-Based Sonification for Data Analysis, <i>International Institute for Advanced Studies in System research and cybernetics</i>, pp. 189--194.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 RAKUGACKY: making sounds with drawing Saori Goto Nanako Kondo Shinji Mizuno Aichi Institute of Technology 
{b13718bb, s_mizuno}@aitech.ac.jp    (a) (b) (c) (d) Figure 1: The interface of "RAKUGACKY" consists 
of a color pallet and a virtual canvas, which is similar to ordinary simple drawing software (a). The 
picture on the virtual canvas is divided into several regions based on color (b). Sound sources for each 
region are decided, and they are placed in the space based on the position of each region (c). Sound 
sources start making sounds at each position, and the system makes a mixed stereo sound for the picture 
(d). 1. Introduction Images and sounds are closely related, and sounds could emphasis or change impressions 
of images. Thus several methods to synthesize sounds from images are developed [Hermann et al. 1999]. 
In this paper, we propose a novel interactive media system "RAKUGACKY" that can make sound with drawing. 
In this system, a user draws a picture on a screen, and the system synthesizes sounds for the picture 
automatically. The system analyzes the picture during drawing, and synthesizes or changes sounds interactively 
according to the analyzing result.  2. The process of making sounds from drawing Figure 1 shows the 
process of the proposed system. The interface of the system consists of a color palette and a virtual 
canvas, which is similar to ordinary simple drawing software (Figure 1(a)). The user selects a color 
of a pen and draws a picture on the canvas with a mouse or a tablet. The picture on the virtual canvas 
is divided into several regions based on color during drawing (Figure 1(b)). Then the shape of each region 
is analyzed by calculating several feature values: the area: s, the length: l, the circularity: c, the 
inclination: i, the curvature: v, and so on. We have selected about 30 sound sources based on objects 
included in pictures drawn by children. They are prepared in the system as monaural WAV files beforehand. 
They are the mewing, the barking, the bleating, the cawing of a crow, the chirping of insects, the croaking, 
the murmuring of a stream, the whistling, the sound of dropping, and so on. The system selects one of 
sound sources for each region based on its color and feature values. For example, blue regions that relate 
to water are classified into 4 types of objects as follows. Rain drop (Figure 2(a)): s < s0 (s0: a threshold) 
 Pond (Figure 2(b)): s > s1 and c > c0 (s1, c0: thresholds)  River (Figure 2(c)): l > l0 and |i| > 
i0 (l0, i0: thresholds)  Sea (Figure 2(d)): l > l1 and |i| < i1 (l1, i1: thresholds) After sound sources 
for each region are decided, they are placed in the space based on the position of each region (Figure 
1(c)). Pitches of some sound sources are changes based on the feature values of each region. They start 
making sounds at each position, and the system makes a mixed stereo sound for the picture (Figure 1(d)). 
The process of making sound is executed during drawing in succession. Sound sources are added and the 
mixed sound is changed each time the user draws additional objects. The user can enjoy drawing with sound 
interactively. The user can draw a picture not only on the virtual canvas of the system, but also on 
a physical sketchbook to make sound in the system. The system monitors the sketchbook with a video camera, 
and analyzes the picture in the same way as drawing on the virtual canvas. It is possible to change sounds 
by moving or touching the sketchbook.  (a) Rain drop(s): splat. (b)Pond: croaking. (c) River: murmuring. 
(d) Sea: whistling.   Figure 2: The classification of blue regions based on features of the shapes. 
 Reference HERMANN T., RITTER H., 1999, Listen to your Data: Model-Based Sonification for Data Analysis, 
International Institute for Advanced Studies in System research and cybernetics, pp. 189-194. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503489</article_id>
		<sort_key>1040</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>95</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[SteganoSonic]]></title>
		<subtitle><![CDATA[a locally information overlay system using parametric speakers]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503489</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503489</url>
		<abstract>
			<par><![CDATA[<p>To construct information environment that augments the real world, there is an important theme that is realization of embedding information into a certain area and receiving it naturally. To develop these systems, it is necessary to announce the existence of additional information and intuitive connection of real world and them. Approaches that use marker-type indicators like AR marker [Kato and Billinghurst 1999] which is significant for the camera recognition but insignificant for human have exiguous intuitiveness. On the other hand, when applying wireless communication technologies like RFID or infrared light [Nishimura et al. 2004], which we cannot sense the existence of the information, we need another sign that indicates the existence of the information to embed information to certain areas. Toward these problems, we propose a novel system named SteganoSonic that embeds digital data into the sound outputted from parametric speakers. This system, with a speaker which has strong directivity, can send audible sound to certain area and embed additional information into the sound which is caught by a receiver. Users can find the existence of additional information by hearing the sound, receive additional information at the same time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190932</person_id>
				<author_profile_id><![CDATA[82458988557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hitomi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio Universtiy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[t11563ht@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190933</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio Universtiy]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>858134</ref_obj_id>
				<ref_obj_pid>857202</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kato, H., and Billinghurst, M. 1999. Marker tracking and hmd calibration for a video-based augmented reality conferencing system. In <i>Proceedings of the 2nd International Workshop on Augmented Reality</i>, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nishimura, T., Itoh, H., Nakamura, Y., Yamamoto, Y., and Nakashima, H. 2004. A compact battery-less information terminal for real world interaction. In <i>Springer LNCS 3001</i>, 124--139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 SteganoSonic: A Locally Information Overlay System Using Parametric Speakers Hitomi Tanaka* Yasuaki 
Kakehi* Keio University Keio Universtiy  Figure 1: SteganoSonic Figure 2: Prototype System. Figure 3: 
Basic System Design. 1 Introduction To construct information environment that augments the real world, 
there is an important theme that is realization of embedding infor­mation into a certain area and receiving 
it naturally. To develop these systems, it is necessary to announce the existence of addi­tional information 
and intuitive connection of real world and them. Approaches that use marker-type indicators like AR marker 
[Kato and Billinghurst 1999] which is signi.cant for the camera recog­nition but insigni.cant for human 
have exiguous intuitiveness. On the other hand, when applying wireless communication technolo­gies like 
RFID or infrared light [Nishimura et al. 2004], which we cannot sense the existence of the information, 
we need another sign that indicates the existence of the information to embed information to certain 
areas. Toward these problems, we propose a novel sys­tem named SteganoSonic that embeds digital data 
into the sound outputted from parametric speakers. This system, with a speaker which has strong directivity, 
can send audible sound to certain area and embed additional information into the sound which is caught 
by a receiver. Users can .nd the existence of additional information by hearing the sound, receive additional 
information at the same time. 2 SteganoSonic SteganoSonic embeds digital data into sound outputted from 
a para­metric speaker. Users can receive embedded information by point­ing a device. Technical contributions 
of this research are as follows. The .rst is a development of the hardware that embeds additional information 
into sound. A parametric speaker outputs ultrasonic sound modulated by audible sound, to send sound to 
narrow area. Though output sound itself is inaudible for human, while the sound propagates in the air, 
the audible sound is demodulated. Therefore parametric speaker sends audible sound to narrow range that 
ul­trasonic propagates. SteganoSonic provides two ultrasonic career wave of different frequency, and 
switches them. As this system im­plements the switching of the data by hardware, it can edit the data 
simultaneously, unlike the systems with software signal processing which need to process the sound in 
advance. The second is receiving and decoding of information by devices *e-mail: {t11563ht, ykakehi}@sfc.keio.ac.jp 
with ultrasonic microphone. In the current implementation, our system provides data in the form of serial 
communication, and the communication speed is 75 bps. The receiver converts the data into various kinds 
of information and presents them to users with audi­ble sound information. The third is area-limited 
information presentation utilizing audio directivity. In this system, as digital data and audible sound 
reaches at the same area, it can augment speci.c range in the real world by additional information without 
calibrations. By setting multi­ple speakers, the SteganoSonic transmitters can present different information 
to each location. Additionally, when the sound from a parametric speaker hits an object, the sound re.ects 
as if the ob­ject sounds. With this feature, there can be an intuitive arrangement that indicates the 
interaction between objects and additional infor­mation. We developed the prototype of the system, and 
applications using it. As an example, in a museum or other exhibitions, a parametric speaker can send 
audio explanation of the displayed item only to au­dience in front of the item. With our system, additionally, 
by hold­ing the receiver to the sound, audience can reach the additional data for example a video or 
links to web sites relative to the displayed item. Furthermore we developed a multi-channel information 
over­laying depending on the locations of audience by setting several SteganoSonic speakers from different 
directions. It can realize ap­plications, for example, the multi-lingual information presentation, or 
giving speci.c information according to audiences viewpoint. In the future, we are going to develop other 
applications of not only converting received data to visual information, but also operation of robots 
or control of devises behaviors. References KATO, H., AN D BILL IN G H U RS T, M. 1999. Marker tracking 
and hmd calibration for a video-based augmented reality conferenc­ing system. In Proceedings of the 2nd 
International Workshop on Augmented Reality, 85 94. NI S H I M U RA, T., ITO H, H., NA K A M U R A , 
Y., YAM A M OTO, Y., AND NAK A S H IM A , H. 2004. A compact battery-less informa­tion terminal for real 
world interaction. In Springer LNCS 3001, 124 139. Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503490</section_id>
		<sort_key>1050</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>2503491</article_id>
		<sort_key>1060</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>96</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A simulation of pearl optical phenomena for cosmetic preproduction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503491</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503491</url>
		<abstract>
			<par><![CDATA[<p>In the area of materials development, preproduction using visual simulation has come to be considered essential. In this paper, we propose a simulation of pearl optical phenomena for cosmetic preproduction. Pearls manifest a very specific optical phenomenon caused by their multilayered thin-film structure, and most people have a common sense that pearls have a unique beauty. Therefore, the expected merits of the simulation for cosmetic preproduction are not negligible.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190934</person_id>
				<author_profile_id><![CDATA[81387610119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kensuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tobitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tobitani@kwansei.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190935</person_id>
				<author_profile_id><![CDATA[82459285957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Akihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okada]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naris Cosmetics Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190936</person_id>
				<author_profile_id><![CDATA[82458996557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Naris Cosmetics Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190937</person_id>
				<author_profile_id><![CDATA[81328488825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190938</person_id>
				<author_profile_id><![CDATA[82459262857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Lisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190939</person_id>
				<author_profile_id><![CDATA[82459003357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>614382</ref_obj_id>
				<ref_obj_pid>614268</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Nagata, N., Dobashi, T., Manabe, Y., Usami, T., and Inokuchi, S. 1997. Modeling and visualization for a pearl-quality evaluation simulator. <i>Visualization and Computer Graphics, IEEE Transactions on 3</i>, 4 (oct-dec), 307--315.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Simulation of Pearl Optical Phenomena 1 3.2 beauty. 2 3 3.1 Figure 3: Synthesized face image. 4 Conclusion 
For cosmetic preproduction, we simulated pearl optical phenomena on a free-form surface by expansion 
of the blurring model, and con.rmed that this simulation was successful In the future, we plan to introduce 
natural .uctuations and irregularities to build a model much closer to the actual phenomena of a pearl. 
References NAG ATA , N., DO BA S H I , T. , MA NA B E , Y., US A M I , T., A N D IN O K U C H I , S . 
1997. Modeling and visualization for a pearl-quality evaluation sim- Figure 1: Simulated BRDF and BRDF 
of pearl oyster shell. ulator. Visualization and Computer Graphics, IEEE Transactions on 3, 4 (oct-dec), 
307 315. *e-mail:tobitani@kwansei.ac.jp Permission to make digital or hard copies of part or all of this 
work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503492</article_id>
		<sort_key>1070</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>97</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Emerging images synthesis from photographs]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503492</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503492</url>
		<abstract>
			<par><![CDATA[<p>Emergence refers to a phenomenon by which human perceives complete objects in a seemingly noisy image not by recognizing local parts of image but viewing the image as a whole. The Dalmatian dog image created by R. C. James is probably the best demonstration of emergence [Bach 2002]. It shows that local windows from the image reveal nothing but meaningless, complex and random black splats. Only when the image is viewed as a whole, a Dalmatian dog suddenly appears. The absence of meaningful information in local image parts largely hinders existing computer vision algorithms from recognizing emerging figures. Therefore, it makes the emergence an new type of CAPTCHA to tell human and machine apart.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190940</person_id>
				<author_profile_id><![CDATA[82458780257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mao-Fong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jian]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190941</person_id>
				<author_profile_id><![CDATA[82459223857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hung-Kuo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hkchu@cs.nthu.edu.tw]]></email_address>
			</au>
			<au>
				<person_id>P4190942</person_id>
				<author_profile_id><![CDATA[82459214557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ruen-Rone]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Tsing Hua University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190943</person_id>
				<author_profile_id><![CDATA[82459087557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Chia-Lun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ku]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chiao Tung University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190944</person_id>
				<author_profile_id><![CDATA[82258888357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yu-Shuen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Chiao Tung University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190945</person_id>
				<author_profile_id><![CDATA[82458645557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Chih-Yuan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University of Science and Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2377556</ref_obj_id>
				<ref_obj_pid>2377349</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Achanta, R., Shaji, A., Smith, K., Lucchi, A., Fua, P., and Susstrunk, S. 2012. Slic superpixels compared to state-of-the-art superpixel methods. <i>IEEE Trans. PAMI 34</i>, 11, 2274--2282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bach, M., 2002. Hidden Figures -- Dalmatian Dog.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2192179</ref_obj_id>
				<ref_obj_pid>2191740</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Cheng, Ming-Ming, Z. G.-X. M. N. J. H. X. H. S.-M. 2011. Global contrast based salient region detection. In <i>IEEE CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618509</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Mitra, N. J., Chu, H.-K., Lee, T.-Y., Wolf, L., Yeshurun, H., and Cohen-Or, D. 2009. Emerging images. <i>ACM Trans. Graph. (Proc. SIGGRAPH Asia) 28</i>, 5, 163:1--163:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Emerging Images Synthesis from Photographs Mao-Fong Jian1 Hung-Kuo Chu1* Ruen-Rone Lee1 Chia-Lun Ku2 
Yu-Shuen Wang2 Chih-Yuan Yao3 1National Tsing Hua University 2National Chiao Tung University 3National 
Taiwan University of Science and Technology  Figure 1: (a) Input photograph. (b) Over-segmentation using 
super-pixels. (c) Luminance map and edge map. (d) Our result. 1 Introduction Emergence refers to a phenomenon 
by which human perceives complete objects in a seemingly noisy image not by recognizing local parts of 
image but viewing the image as a whole. The Dalma­tian dog image created by R. C. James is probably the 
best demon­stration of emergence [Bach 2002]. It shows that local windows from the image reveal nothing 
but meaningless, complex and ran­dom black splats. Only when the image is viewed as a whole, a Dalmatian 
dog suddenly appears. The absence of meaningful in­formation in local image parts largely hinders existing 
computer vision algorithms from recognizing emerging .gures. Therefore, it makes the emergence an new 
type of CAPTCHA to tell human and machine apart. Inspired by the Dalmatian dog image, Mitra et al. [2009] 
proposed an automatic algorithm to synthesize emerging images from 3D objects. During the synthesis process, 
two guiding principles are carefully evaluated to ensure the synthesized images are easy for human to 
recognize while hard for bots to detect the embedded ob­jects. However, using 3D objects as input brings 
two drawbacks that make the system an unquali.ed CAPTCHA. Firstly, available resources of 3D model repository 
are limited, therefore it is unable to generate a huge database of emerging images. Secondly, opera­tions 
on 3D space are computational expensive means user cannot obtain or refresh a CAPTCHA puzzle in an interactive 
rate. In this work, we propose an automatic algorithm to synthesize emerging images from common photographs. 
To generate images that are easy for human, Mitra et al. [2009] rendered complex splats that capture 
silhouette and shading information of 3D objects. How­ever, we realize that comparative information could 
be retrieved from photographs as well, and we try to replace the rendering of black complex splats with 
super-pixels. It takes two further post processing steps to make segmentation harder for bots, and both 
of them could .nd counterpart operations in image domain. Sup­porting by public image databases such 
as .ickr and Picasa, we can envision a potential CAPTCHA application of our approach to mas­sively and 
ef.ciently generate emerging images from photographs. *Corresponding author:hkchu@cs.nthu.edu.tw  2 
Our Approach Given a photograph, our system .rst segments the foreground ob­ject using semi-automatic 
saliency cut [Cheng 2011]. Then it ap­ plies an image over-segmentation algorithm [Achanta et al. 2012] 
to obtain super-pixels of foreground object (see Figure 1(b)). We formulate the problem of rendering 
an emerging foreground as a binary labeling problem that assigns binary color (black or white) to super-pixels. 
To leave necessary cues for human to recognize, we introduce two maps to guide the assignment of binary 
color as shown in Fig­ure 1(c). One is a luminance map which captures the grayness of each super-pixel 
while the other is an edge map that captures the rel­ative contrast between adjacent super-pixels. Value 
of each super­pixel in luminance map and edge map is calculated by averaging the image intensity and 
gradient of its atomic pixels, respectively. Finally, we employ a graph cut algorithm to ef.ciently solve 
the la­beling problem consisting of data and smoothness terms with data term measures the distance between 
new color and original gray­ness of superpixels while the smoothness term favors the preser­vation of 
original contrast. Next, we adopt a similar copy-perturb­paste operation of [Mitra et al. 2009] to duplicate 
super-pixels from foreground to background such that the object and the rest of image look similar everywhere 
when observed through small windows. Since our approach operates on 2D super-pixels, the computational 
complexity is much cheaper than [Mitra et al. 2009] which takes 5 seconds to generate a result while 
ours take 1 second with image of moderate size using the unoptimized codes. References AC H A N TA , 
R., SH A J I , A., SM I T H , K. , LU C C H I , A., FUA , P. , A N D SU S S T RU N K , S . 2012. Slic 
superpixels compared to state-of-the-art superpixel methods. IEEE Trans. PAMI 34, 11, 2274 2282. BAC 
H , M ., 2002. Hidden Figures Dalmatian Dog. CH E N G , M I N G -M I N G , Z. G.-X. M . N . J. H. X. 
H. S . -M . 2011. Global contrast based salient region detection. In IEEE CVPR. MI T R A , N. J., CH 
U , H .-K., LE E , T. -Y. , WO L F, L., Y E S H U RU N , H . , A N D CO H E N -OR, D. 2009. Emerging 
images. ACM Trans. Graph. (Proc. SIGGRAPH Asia) 28, 5, 163:1 163:8. Permission to make digital or hard 
copies of part or all of this work for personal or classroom use isgranted without fee provided that 
copies are not made or distributed for commercial advantage and that copies bear this notice and the 
full citation on the first page. Copyrights for third-party components of thiswork must be honored. For 
all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503493</article_id>
		<sort_key>1080</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>98</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[GPU ray tracing with rayforce]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503493</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503493</url>
		<abstract>
			<par><![CDATA[<p>Rayforce is a high performance ray tracing engine designed for massively parallel computing architectures, including manycore GPUs. Rayforce leverages a novel graph-based acceleration structure that permits <i>first-hit, any-hit</i>, and <i>multi-hit</i> traversal algorithms required to solve a variety of problems in physics-based simulation domains. Rayforce exposes core ray tracing operations via a programmable interface to enable the implementation of various computer graphics and scientific computing applications.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190946</person_id>
				<author_profile_id><![CDATA[81100561107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Christiaan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gribble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Applied Technology Operation, SURVICE Engineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[christiaan.gribble@survice.com]]></email_address>
			</au>
			<au>
				<person_id>P4190947</person_id>
				<author_profile_id><![CDATA[82458875057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alexis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naveros]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Applied Technology Operation, SURVICE Engineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPU Ray Tracing with Rayforce Christiaan Gribble. Alexis Naveros Applied Technology Operation SURVICE 
Engineering  (a) .rst-hit (b) any-hit (c) multi-hit Figure 1: GPU ray tracing with Rayforce. The Rayforce 
ray tracing engine is designed for massively parallel computing architectures and leverages a novel graph-based 
spatial indexing structure that enables high performance .rst-hit, any-hit, and multi-hit traversal algorithms 
required to solve a variety of problems in physics-based simulation domains, including traditional image 
synthesis. 1 Overview Rayforce is a high performance ray tracing engine designed for massively parallel 
computing architectures, including manycore GPUs. Rayforce leverages a novel graph-based acceleration 
struc­ture that permits .rst-hit, any-hit, and multi-hit traversal algorithms required to solve a variety 
of problems in physics-based simulation domains. Rayforce exposes core ray tracing operations via a pro­grammable 
interface to enable the implementation of various com­puter graphics and scienti.c computing applications. 
 2 Graph-based Spatial Indexing A new graph-based spatial indexing structure accelerates ray/primitive 
intersection operations. The structure is: ef.cient: in-memory data layouts are carefully designed to 
minimize storage, thereby improving cache performance;  .exible: several traversal algorithms can be 
implemented with low overhead; and,  scalable: complex scenes are handled ef.ciently, as perfor­  
mance depends only on geometric complexity along a ray. The structure is comprised of sectors, which 
bound geometry, and nodes, or separation planes that disambiguate traversal steps. Con­struction proceeds 
by building a graph of sectors and nodes that minimizes a function representing traversal cost. Ray origins 
are resolved to starting sectors using displacements from other origins or a 3D bin-based lookup. Rays 
enter a sector and intersect all bounded primitives, then traverse to linked sectors if necessary. Traversal 
algorithms include: .rst-hit: returns the nearest intersection (if any); used for vis­ibility computations 
and visual effects such as re.ection, re­fraction, and other forms of indirect illumination.  any-hit: 
indicates whether any intersection occurs within a speci.c interval; used for shadow or ambient occlusion 
rays.  multi-hit: returns one or more, and possibly all, intersections in ray-order (if any); used for 
transparency or operations in non-optical simulation domains.  The images in Figures 1 and 2 illustrate 
these traversal algorithms.  3 Performance The data in Figure 3 show initial performance measurements 
for several rendering scenarios: vis: .rst-hit visibility with simple N · V shading.  x-ray: all multi-hit 
intersections with simple alpha-blending.  ao: .rst-hit visibility plus 32 any-hit ambient occlusion 
rays.  kajiya: .rst-hit visibility, any-hit shadows, and two .rst-hit diffuse interre.ection rays. 
 .e-mail: christiaan.gribble@survice.com  three scenes rendered at 1024 . 768 pixels on an NVIDIA GeForce 
GTX 690 indicate that Rayforce delivers exceptional performance. Except for image display, all per-frame 
overhead GPU kernel launch, ray generation/traversal, shading, host/device synchroniza­tion, and so forth 
is included in these measurements. 4 Future Work We are exploring additional memory optimizations for 
our acceler­ation structure and plan to implement a parallel GPU construction algorithm. We are also 
considering new methods for exploiting ray coherence to eliminate unnecessary thread divergence and reduce 
memory bandwidth on SIMT architectures. Acknowledgments This work is funded in part by research grants 
from the US Of.ce of Naval Research and the US Army Research Laboratory. Permission to make digital or 
hard copies of part or all of this work for personal or classroom use isgranted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>US Army Research Laboratory</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>US Office of Naval Research</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503494</article_id>
		<sort_key>1090</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>99</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[High-order wavelets for hierarchical refinement in inverse rendering]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503494</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503494</url>
		<abstract>
			<par><![CDATA[<p>It is common to use factored representation of visibility, lighting and BRDF in inverse rendering. Current techniques use Haar wavelets to calculate these triple product integrals efficiently [Ng et al. 2004]. Haar wavelets are an ideal basis for the piecewise constant visibility function, but suboptimal for the smoother lighting and material functions. How can we leverage compact high-order wavelet bases to improve efficiency, memory consumption and accuracy of an inverse rendering algorithm? If triple product integrals can be efficiently calculated for higher-order wavelets, the reduction in coefficients will reduce the number of calculations, therefore improving performance and memory usage. Some BRDFs can be stored five times more compactly.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190948</person_id>
				<author_profile_id><![CDATA[82458932857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Michiels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University EDM, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Nick.Michiels@uhasselt.be]]></email_address>
			</au>
			<au>
				<person_id>P4190949</person_id>
				<author_profile_id><![CDATA[82459229857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jeroen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Put]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University EDM, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Jeroen.Put@uhasselt.be]]></email_address>
			</au>
			<au>
				<person_id>P4190950</person_id>
				<author_profile_id><![CDATA[81335491115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University EDM, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Tom.Haber@uhasselt.be]]></email_address>
			</au>
			<au>
				<person_id>P4190951</person_id>
				<author_profile_id><![CDATA[81492655924]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Martin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klaudiny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Surrey, Guildford, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190952</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University EDM, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Philippe.Bekaert@uhasselt.be]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Haber, T., Fuchs, C., Bekaert, P., Seidel, H.-P., Goesele, M., and Lensch, H. P. A. 2009. Relighting objects from image collections.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015749</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Ramamoorthi, R., and Hanrahan, P. 2004. Triple product wavelet integrals for all-frequency relighting. <i>ACM Trans. Graph. 23</i>, 3 (Aug.), 477--487.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383679</ref_obj_id>
				<ref_obj_pid>2383654</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Peers, P., and Dutr&#233;, P. 2005. Inferring reflectance functions from wavelet noise. In <i>Proceedings of the Sixteenth Eurographics conference on Rendering Techniques</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, EGSR'05, 173--182.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 High-order Wavelets for Hierarchical Re.nement in Inverse Rendering Nick Michiels 1 Jeroen Put 1 Tom 
Haber 1 Martin Klaudiny 2 Philippe Bekaert 1 1 Hasselt University EDM, Belgium 2 CVSSP, University of 
Surrey, Guildford, UK  Figure 1: Iterative re.nement of illumination by adding detail coef.cients based 
on a splitting criterium. Reconstructions for both Haar (red) and the smoother Coi.et (green) wavelet 
bases are shown. Haar has a tendency to introduce disturbing high frequencies around edges. 1 Introduction 
It is common to use factored representation of visibility, light­ing and BRDF in inverse rendering. Current 
techniques use Haar wavelets to calculate these triple product integrals ef.ciently [Ng et al. 2004]. 
Haar wavelets are an ideal basis for the piecewise con­ stant visibility function, but suboptimal for 
the smoother lighting and material functions. How can we leverage compact high-order wavelet bases to 
improve ef.ciency, memory consumption and ac­curacy of an inverse rendering algorithm? If triple product 
integrals can be ef.ciently calculated for higher-order wavelets, the reduc­tion in coef.cients will 
reduce the number of calculations, therefore improving performance and memory usage. Some BRDFs can be 
stored .ve times more compactly. Current inverse rendering algorithms rely on solving large systems of 
bilinear equations [Haber et al. 2009]. We propose a hierarchical re.nement algorithm that exploits the 
tree structure of the wavelet basis. By only splitting at interesting nodes in the hierarchy, large portions 
of less important coef.cients can be skipped. The key of this algorithm is only splitting nodes of the 
wavelet tree that con­tribute to the solution of the system M (see Algorithm 1). It is crit­ ical to 
use high-order wavelets for this, as Haar wavelets can only introduce high frequencies which lead to 
blockiness. 2 Our Apprach In forward rendering, the rendering equation can be expressed as a triple 
product integral of the visibility Vi, environment map L j and BRDF .k, all three represented in the 
wavelet domain:  B(x, .0) =LiVj .k.i(.).j (.).k(.)d. (1) O ijk In contrast to the regular Haar solution 
of Ng. [Ng et al. 2004], higher-order wavelets have overlapping support, resulting in more v complex 
binding coef.cients O .i(.).j (.).k(.)d.. This cal­culation is much more complex and needs to be precalculated. 
Our method exploits the hierarchical nature and vanishing moments of wavelets, to ef.ciently solve this 
sparse tensor. 1email: {.rstname}.{lastname}@uhasselt.be Figure 2: Reconstruction of a temporal face 
dataset under dif­ferent lighting conditions and estimated with the hierarchical re.nement method. Ray 
traced occlusion maps, BRDF slices and lighting environment map are combined in the triple prod­uct integral 
calculation. Instead of solving for all coef.cients of the wavelet tree of the envi­ronment map, we can 
also use the hierarchical nature of wavelets to iteratively re.ne the estimation. The idea is to start 
with a smaller wavelet tree and adaptively add more detail coef.cients by split­ting the leaf nodes. 
Instead of previous methods [Peers and Dutr´e 2005], where they use properties of the coef.cients, we 
use the rank of the system as a splitting criterium, while also allowing a mix of high-order wavelet 
bases to improve convergence (see Al­gorithm 1). Figure 1 gives a comparison of the hierarchical method 
for both a piecewise constant Haar Basis and a smoother Coi.et basis. Reconstruction of a temporal dataset 
with this hierarchical method is shown in Figure 2. Algorithm 1 Hierarchical Re.nement Scheme M: initialize 
system to solve at root node of wavelet tree repeat K: set of possible nodes for re.nement for k . K 
do concatenate k to M: M = M|Mk if rank(M) = full then remove k from K end if end for  Solve M for 
L until no splits left References HA B E R, T., FU C H S , C., BE KAERT, P., SEI D E L, H.-P., GO E 
S E LE, M., A N D LEN S CH, H. P. A. 2009. Relighting objects from image collections. NG, R., RA M A 
M O ORT HI , R., A ND HA N R A H A N , P. 2004. Triple product wavelet integrals for all-frequency relighting. 
ACM Trans. Graph. 23, 3 (Aug.), 477 487. PE E R S , P., AN D DU TR E´ , P. 2005. Inferring re.ectance 
functions from wavelet noise. In Proceedings of the Sixteenth Eurograph­ics conference on Rendering Techniques, 
Eurographics Associ­ation, Aire-la-Ville, Switzerland, Switzerland, EGSR 05, 173 182. Permission to make 
digital or hard copies of part or all of this work for personal or classroom use isgranted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503495</article_id>
		<sort_key>1100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>100</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Lace curtain]]></title>
		<subtitle><![CDATA[rendering and animating woven cloth based on an impression-evaluation model]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503495</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503495</url>
		<abstract>
			<par><![CDATA[<p>Woven fabric is frequently used in computer graphics. To express the realistic appearance of a woven fabric, it is important to represent its luster, transparency, and motion. In our previous study, we proposed a physically based BTDF model of woven cloth [Nomura et al. 2011] and a method of estimating the motion properties of woven cloth from subjective impression [Ishida et al. 2012]. However, for the further enhancement of the texture of materials in animations, it is necessary to construct a comprehensive impression model for the evaluation of woven cloth that includes the relationship between optics and motions, impressions and emotions that are evoked by the impressions. In this study, we propose an impression-evaluation model for the rendering of woven fabrics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190953</person_id>
				<author_profile_id><![CDATA[81328488825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190954</person_id>
				<author_profile_id><![CDATA[82458703157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Sosuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kaji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190955</person_id>
				<author_profile_id><![CDATA[82459262857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190956</person_id>
				<author_profile_id><![CDATA[81387610119]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kensuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tobitani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190957</person_id>
				<author_profile_id><![CDATA[81555532956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Aya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shiraiwa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190958</person_id>
				<author_profile_id><![CDATA[81504688443]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Eriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aiba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University, JAIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190959</person_id>
				<author_profile_id><![CDATA[81100522455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nagata@kwansei.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2342903</ref_obj_id>
				<ref_obj_pid>2342896</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishida, A., Ishigo, E., Aiba, E., and Nagata, N. 2012. Lace Curtain: Rendering Animation of Woven Cloth using BRDF/BTDF- Estimating physical characteristic from subjective impression. In <i>ACM SIGGRAPH 2012 posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Montes, R., Urena, C., Garcia, R., and Lastra, M. 2009. An Importance Sampling Method for Arbitrary BRDFs. In <i>Computer Vision and Computer Graphics. Theory and Applications</i>, Springer Berlin Heidelberg, vol. 24, 41--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2037795</ref_obj_id>
				<ref_obj_pid>2037715</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Nomura, S., Ishida, A., Ishigo, E., Okamoto, T., Mizushima, Y., and Nagata, N. 2011. Lace Curtain: Modeling and Rendering Woven Cloth using Microfacet BSDF-Production of a catalog of curtain animations-. In <i>ACM SIGGRAPH 2011 posters</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Atsushi Ishida Sosuke Kaji Kwansei Gakuin University Kwansei Gakuin University Aya Shiraiwa Eriko Aiba 
Kwansei Gakuin University Kwansei Gakuin University JAIST 1 Introduction Woven fabric is frequently used 
in computer graphics. To express the realistic appearance of a woven fabric, it is important to represent 
its luster, transparency, and motion. In our previous study, we proposed a physically based BTDF model 
of woven cloth [Nomura et al. 2011] and a method of estimating the motion properties of woven cloth from 
subjective impression [Ishida et al. 2012]. However, for the further enhancement of the texture of materials 
in animations, it is necessary to construct a comprehensive impression model for the evaluation of woven 
cloth that includes the relationship between optics and motions, impressions and emotions that are evoked 
by the impressions. In this study, we propose an impression-evaluation model for the rendering of woven 
fabrics. 2 Impression-evaluation model for woven cloth In order to clarify the relationship between 
impressions and physical characteristics, we measured optical and motion properties and per­formed experiments 
to evaluate subjective impressions. Measurements of woven cloth To evaluate the six woven fab­ric samples, 
we measured BRDF/BTDF by using a BRDF instrument, the OGM-3 (Optical Gyro Measuring Machine), and we 
measured motion properties such as bend, shear, compression, and stretch by using a handle instrument, 
the KES (Kawabata Evaluation System). Then, we applied PCA (principal component analysis) to the measured 
BRDF/BTDF data to derive the major physical properties. Experiments for impression evaluation We performed 
experi­ments to evaluate impressions using actual woven fabrics whose states are stationary or moving. 
We used adjective word-pairs related to the optical and motion properties. As a result of a factor analysis 
of the data obtained, we found out that the factors softness and smooth­ness for the optics and the factors 
.exibility and weight for the motions in.uence the emotions evoked by the texture of woven fab­rics. 
Furthermore, the relationship between the measured data and the subjective impressions to weave structure 
were obtained using a mul­tiple regression analysis. 3 Estimation of optical/motion properties using 
the impression-evaluation model We estimated the physical parameters for rendering using the relation­ships 
between the optical and motion properties in the impression­evaluation model. For example, from the thickness 
of warp/weft, the optical/motion properties, such as the value of bend and the directional transmittance, 
were estimated. 4 Rendering with BTDF based photon sam­pling for global illumination We used photon 
mapping for to determine global illumination. In or­der to represent realistic transmitted light, we 
introduced a photon sampling based on a woven cloth BTDF model similar to arbitrary BRDF based importance 
sampling[Montes et al. 2009]. Thus, it was possible to precisely express the BTDF properties of woven 
cloth, such as direct transmittance and scattering by the thread. We performed the *e-mail:nagata@kwansei.ac.jp 
(a)Stiff and rough impression. The curtain motion is rigid and little in.uence of transmitted light. 
 (b)Pliable and smooth impression. The curtain motion is wavy and in.uence of the transmitted light was 
con.rmed in the direction of direct transmission.  Figure 1: Scenes of curtain animations with different 
impression (in the same .ame time). 5 Conclusion We clari.ed the structure of the impression-evaluation 
of woven fab­rics and proposed a method to render such fabrics on the basis of the impression-evaluation 
model. By estimating the physical characteris­tics from impressions, the expression of texture of woven 
fabrics was improved. We are planning to apply our approach to various materials. References IS HI DA, 
A., ISHI G O , E., AI BA, E., A ND NAGATA, N. 2012. Lace Curtain: Rendering Animation of Woven Cloth 
using BRDF/BTDF-Estimating physical characteristic from subjective impression. In ACM SIGGRAPH 2012 posters. 
MO N T ES, R., URE NA, C., GA RC I A, R., AN D LA S T RA , M. 2009. An Importance Sampling Method for 
Arbitrary BRDFs. In Computer Vision and Computer Graphics. Theory and Applications, Springer Berlin Heidelberg, 
vol. 24, 41 54. NO MUR A , S., IS H IDA , A., ISHI G O , E., OK A M OTO, T., MI Z U S H IM A , Y., AN 
D NAG ATA, N. 2011. Lace Curtain: Modeling and Rendering Woven Cloth using Microfacet BSDF-Production 
of a catalog of curtain animations-. In ACM SIGGRAPH 2011 posters. Permission to make digital or hard 
copies of part or all of this work for personal or classroom use isgranted without fee provided that 
copies are not made or distributed for commercial advantage and that copies bear this notice and the 
full citation on the first page. Copyrights for third-party components of thiswork must be honored. For 
all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503496</article_id>
		<sort_key>1110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>101</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[LightCluster]]></title>
		<subtitle><![CDATA[clustering lights to accelerate shadow computation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503496</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503496</url>
		<abstract>
			<par><![CDATA[<p>Shadows are an important part of a visualization and give the viewer supplemental details about the appearance of objects. In real-time rendering, shadow mapping [Williams 1978] is a popular approach to compute shadows. However, a shadow map must be computed for each light and thus, the memory and the computation time increases with the number of lights.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190960</person_id>
				<author_profile_id><![CDATA[82458708357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wiesenh&#252;tter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Munich University of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[daniel@wiesenhuetter.me]]></email_address>
			</au>
			<au>
				<person_id>P4190961</person_id>
				<author_profile_id><![CDATA[82459114457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klein]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Munich University of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[andreas.klein@hm.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190962</person_id>
				<author_profile_id><![CDATA[82458843957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alfred]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nischwitz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Munich University of Applied Sciences]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nischwitz@cs.hm.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Dong, Z., Grosch, T., Ritschel, T., Kautz, J., and Seidel, H.-P. 2009. Real-time indirect illumination with clustered visibility. In <i>Proc. of the VMV</i>, 187--196.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187153</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fernando, R. 2005. Percentage-closer soft shadows. In <i>ACM SIGGRAPH 2005 Sketches</i>, ACM, New York, NY, USA, SIGGRAPH '05.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>807402</ref_obj_id>
				<ref_obj_pid>965139</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Williams, L. 1978. Casting curved shadows on curved surfaces. <i>SIGGRAPH Comput. Graph. 12</i>, 3 (Aug.), 270--274.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wolfowitz, J. 1957. The minimum distance method. <i>Annals of Mathematical Statistics 28</i>, 1, 75--88.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 LightCluster -Clustering Lights to Accelerate Shadow Computation Daniel Wiesenh ¨utter*, Alfred Nischwitz 
, Andreas Klein Munich University of Applied Sciences Figure 1: Comparison between a reference solution 
and LightCluster for the Dabrovic Sponza scene at a resolution of 1920x1080. For a given distribution 
of 80 point light sources in both arcades of the atrium (a), only 26 clusters are selected with a minimum 
distance metric (b). Instead of rendering hard shadows for every point light source like in the reference 
solution (c) at 97.8 ms, soft shadows are computed for every selected cluster (d) at 43.1 ms with Percentage 
Closer Soft Shadows. In (e) the difference between the reference solution (c) and the solution of LightCluster 
(d) is displayed. Notice that dark blue color indicates a low error and a red color a high error. 1 Introduction 
Shadows are an important part of a visualization and give the viewer supplemental details about the appearance 
of objects. In real-time rendering, shadow mapping [Williams 1978] is a popular approach to compute shadows. 
However, a shadow map must be computed for each light and thus, the memory and the computation time in­creases 
with the number of lights. We present an approach, called LightCluster, to automatically se­lect representative 
light sources and accelerate the computation of direct shadows for scenes with many lights. We carefully 
select light sources as cluster centers and cluster the remaining lights us­ing a minimum distance metric 
[Wolfowitz 1957]. We represent each cluster by an area light source and use Percentage Closer Soft Shadows 
(PCSS) [Fernando 2005] to render soft shadows for each cluster. Figure 1 shows an overview. The idea 
of [Dong et al. 2009] is closely related to our work. In con­trast to Clustered Visibility we focus our 
work on high frequency shadows for direct lighting. Furthermore, our approach uses an ex­isting light 
source as a cluster center and clusters the remaining lights with a minimum distance metric. In our implementation, 
we use omnidirectional point lights. How­ever, the approach can be adapted for other light types. 2 
Our Approach Figure 2: Clustering in a 2D example scene: (a) point light distri­bution, (b) frustum culling 
of point lights, (c) selecting clusters with a minimum distance metric, (d) assigning lights to nearest 
clusters. In order to reduce the error in shadows, we perform a two pass clustering with different metrics 
in each pass. We .rst select point *e-mail:daniel@wiesenhuetter.me e-mail:andreas.klein@hm.edu e-mail:nischwitz@cs.hm.edu 
lights within the viewing frustum as cluster centers by using the light range and a minimum-distance 
metric, which is scaled by the camera distance. This allows us to generate smaller clusters and thus, 
more shadow maps, near the camera position. In the second clustering pass, the remaining point lights 
are assigned to the near­est cluster centers. Therefore, the error in shadows due to the de­creased amount 
of shadow maps can be reduced. Figure 2 shows the clustering. Then, we render a cube shadow map for each 
cluster and interpret the cluster as a disc-shaped area light source. The radius of the area light source 
is given by the minimum distance of the cluster. We use this radius to scale the .lter window of PCSS. 
The visibility factor is then stored in a texture for each cluster. This allows us to calcu­late the 
shadows iteratively and reduces the texture memory from a cube shadow map to a screen sized texture per 
cluster. In this way, we sample the cube shadow maps only once for each cluster and avoid additional 
PCSS sampling for each light source during shading. After the shadow computation, we use the set of visibil­ity 
textures for shading. We shade the scene with each point light source and modulate the resulting color 
with the visibility value stored in the texture for the point light s cluster. By adjusting the clustering 
parameters, a well de.ned ratio between quality and performance can be chosen. Due to the approximation 
of the shadows from many lights with a soft shadow from a repre­sentative light source, errors in the 
shadow can be reduced. As the clustering is fast, it can be performed in each frame and only adds a small 
overhead. If light sources can be merged to clusters, Light-Cluster can be used to increase the rendering 
performance while maintaining an acceptable shadow quality in many cases. References DO N G , Z., GRO 
S CH, T., RIT S C H E L, T., KAUT Z , J., A ND SE I-DE L , H.-P. 2009. Real-time indirect illumination 
with clustered visibility. In Proc. of the VMV, 187 196. FE R NAN D O , R. 2005. Percentage-closer soft 
shadows. In ACM SIGGRAPH 2005 Sketches, ACM, New York, NY, USA, SIG-GRAPH 05. WIL L IAM S , L. 1978. 
Casting curved shadows on curved surfaces. SIGGRAPH Comput. Graph. 12, 3 (Aug.), 270 274. WO L F OW I 
T Z , J. 1957. The minimum distance method. Annals of Mathematical Statistics 28, 1, 75 88. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503497</article_id>
		<sort_key>1120</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>102</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Lightfield media production system using sparse angular sampling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503497</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503497</url>
		<abstract>
			<par><![CDATA[<p>Traditional film and broadcast cameras capture the scene from a single view point or in the case of 3D cameras from two slightly shifted viewpoints. Important creative parameters such as the camera position and orientation, the depth of field, and the amount of 3D parallax are burned into the footage during acquisition. To realize artistic effects such as the matrix- or the vertigo-effect, complex equipment and very skilled personnel is required. Within the former effect the scene itself appears frozen, while a camera movement is simulated by placing dozens of cameras in a mainly horizontal arrangement. The latter, however, requires physical movement of the camera which is usually mounted on a dolly and translates towards and backwards from the scene while the zoom (and focus) are changed accordingly. Beside the demanding requisites towards equipment and personnel, the resulting effects can usually not be changed in post-production. In contrast lightfield acquisition techniques allow for changing the mentioned parameters in post-production. Traditionally, in absence of a geometric model of the scene, a dense sampling of the lightfield is required. This can be achieved using large cameras arrays as used by [Wilburn et al. 2005] or hand-held plenoptic cameras as proposed by [Ng et al. 2005]. While the former approach is complex in calibration and operation due to the huge number of cameras, the latter suffers from a low resolution per view, as the total resolution of the imaging sensor needs to be shared between all sub-images captured by the individual micro-lenses.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190963</person_id>
				<author_profile_id><![CDATA[81479662420]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frederik]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zilly]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Integrated Circuits IIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[frederik.zilly@iis.fraunhofer.de]]></email_address>
			</au>
			<au>
				<person_id>P4190964</person_id>
				<author_profile_id><![CDATA[81464664173]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sch&#246;berl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Integrated Circuits IIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190965</person_id>
				<author_profile_id><![CDATA[82459099057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sch&#228;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Integrated Circuits IIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190966</person_id>
				<author_profile_id><![CDATA[82458835157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ziegler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Integrated Circuits IIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190967</person_id>
				<author_profile_id><![CDATA[81337490286]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Joachim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Keinert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Integrated Circuits IIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190968</person_id>
				<author_profile_id><![CDATA[82458653757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Siegfried]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foessel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Institute for Integrated Circuits IIS]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1231663</ref_obj_id>
				<ref_obj_pid>1231529</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kauff, P., Atzpadin, N., Fehn, C., Mller, M., Schreer, O., Smolic, A., and Tanger, R. 2007. Depth map creation and image-based rendering for advanced 3dtv services providing interoperability and scalability. <i>Signal Processing: Image Communication 22</i>, 2, 217--234. Special issue on three-dimensional video and television.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ng, R., Levoy, M., Br&#233;dif, M., Duval, G., Horowitz, M., and Hanrahan, P. 2005. Light field photography with a hand-held plenoptic camera. <i>Computer Science Technical Report CSTR 2</i>, 11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1073259</ref_obj_id>
				<ref_obj_pid>1186822</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wilburn, B., Joshi, N., Vaish, V., Talvala, E.-V., Antunez, E., Barth, A., Adams, A., Horowitz, M., and Levoy, M. 2005. High performance imaging using large camera arrays. In <i>ACM SIGGRAPH 2005 Papers</i>, ACM, New York, NY, USA, SIGGRAPH '05, 765--776.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light.eld Media Production System using Sparse Angular Sampling Frederik Zilly*, Michael Sch ¨oberl, 
Peter Sch ¨afer, Matthias Ziegler, Joachim Keinert, Siegfried Foessel Fraunhofer Institute for Integrated 
Circuits IIS Department Moving Picture Technologies  Figure 1: Virtual stereo pair with user-adjustable 
inter-axial distance (left), synthetic aperture renderings (center, right) 1 Introduction Traditional 
.lm and broadcast cameras capture the scene from a single view point or in the case of 3D cameras from 
two slightly shifted viewpoints. Important creative parameters such as the cam­era position and orientation, 
the depth of .eld, and the amount of 3D parallax are burned into the footage during acquisition. To re­alize 
artistic effects such as the matrix-or the vertigo-effect, com­plex equipment and very skilled personnel 
is required. Within the former effect the scene itself appears frozen, while a camera move­ment is simulated 
by placing dozens of cameras in a mainly hori­zontal arrangement. The latter, however, requires physical 
move­ment of the camera which is usually mounted on a dolly and trans­lates towards and backwards from 
the scene while the zoom (and focus) are changed accordingly. Beside the demanding requisites towards 
equipment and personnel, the resulting effects can usually not be changed in post-production. In contrast 
light.eld acquisi­tion techniques allow for changing the mentioned parameters in post-production. Traditionally, 
in absence of a geometric model of the scene, a dense sampling of the light.eld is required. This can 
be achieved using large cameras arrays as used by [Wilburn et al. 2005] or hand-held plenoptic cameras 
as proposed by [Ng et al. 2005]. While the former approach is complex in calibration and operation due 
to the huge number of cameras, the latter suffers from a low resolution per view, as the total resolution 
of the imag­ing sensor needs to be shared between all sub-images captured by the individual micro-lenses. 
 2 Our Approach We present a system which allows capturing the light.eld of a scene using a 2D array 
of high de.nition cameras. By using disparity esti­mation techniques, we create implicit geometric information 
of the scene without requiring additional depth sensors. Subsequently, we convert the original light.eld 
with sparse angular sampling into a dense light.eld by means of Depth Image Based Rendering [Kauff et 
al. 2007]. Once a dense set of intermediate views has been gener­ ated, we can apply traditional light.eld 
rendering techniques which are suitable for real-time, or near-realtime execution. This offers the full 
creative leeway of changing camera paths, orientation and focal planes within a user-interactive environment 
as shown in Fig­ure 1. The image data is therefore ingested into a traditional post­production chain 
which is enhanced by a suitable set of plug-ins *e-mail:frederik.zilly@iis.fraunhofer.de Figure 2: Compact 
camera array used for recording a sparse light­.eld data set to control artistic effects such as the 
matrix-effect or the vertigo­effect. Our approach involves a compact camera array as shown in Figure 
2 with a small number of smart-phone camera modules with a resolution of 2048x1536 pixels which can easily 
be mounted on off-the shelf tripods. A single ethernet cable per camera is used for data acquisition, 
power supply, and triggering which reduces the complexity of the setup. First results generated using 
the pro­posed ligh.eld media production system which are shown in Figure 1 and the supplementary video, 
encourage us to pursue the chosen approach within future work.  References KAU FF , P., AT Z PAD I N 
, N., FEH N , C., ML L E R, M., SC H RE E R, O., SM O L IC , A., A N D TA N G E R, R. 2007. Depth map 
creation and image-based rendering for advanced 3dtv services providing interoperability and scalability. 
Signal Processing: Image Com­munication 22, 2, 217 234. Special issue on three-dimensional video and 
television. NG, R., LEVOY, M., BR ´ EDI F, M., DUVA L , G., HOROW I TZ , M., AND HANR A H AN , P. 2005. 
Light .eld photography with a hand-held plenoptic camera. Computer Science Technical Report CSTR 2, 11. 
WIL B UR N , B., JO S HI , N., VA ISH , V., TA LVA L A , E.-V., AN-T U NE Z , E., BA RT H , A., ADAMS, 
A., HO ROW I T Z , M., A N D LE VOY, M. 2005. High performance imaging using large cam­era arrays. In 
ACM SIGGRAPH 2005 Papers, ACM, New York, NY, USA, SIGGRAPH 05, 765 776. Permission to make digital or 
hard copies of part or all of this work for personal or classroom use isgranted without fee provided 
that copies are not made or distributed for commercial advantage and that copies bear this notice and 
the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503498</article_id>
		<sort_key>1130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>103</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Pencil tracing mirage]]></title>
		<subtitle><![CDATA[principle and its evaluation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503498</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503498</url>
		<abstract>
			<par><![CDATA[<p>To generate realistic representation of the nature scene is one of the most challenging areas in the computer graphics community. Ray tracing[1] is the most well-known technique to synthesize a realistic image. Since ray tracing is the most suitable method for simulating reflection and refraction of the light, it has been used for simulating atmospheric optical phenomena due to reflection and refraction of the light. The mirage is a kind of atmospheric optical phenomenon. Therefore, it is possible to synthesize mirages in 3DCG by simulating or modeling condition of the air. We focus on pencil tracing technique[2] that is an extention of conventional ray tracing technique based on the paraxial approximation theory. Our simple method based on pencil tracing can efficiently generate an appearance of mirage without any complex thermodynamic simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190969</person_id>
				<author_profile_id><![CDATA[81474687456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katsuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanazawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Healthcare University, Setagaya, Setagaya-ku Tokyo and Tokyo Denki University, Senju Asahi-cho, Adachi-ku Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[k-kanazawa@thcu.ac.jp; kanazawa@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190970</person_id>
				<author_profile_id><![CDATA[82458789357]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuma]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sakato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University and UEI Research, Senju Asahi-cho, Adachi-ku Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sakato@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190971</person_id>
				<author_profile_id><![CDATA[81319502327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tokiichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University and UEI Research, Senju Asahi-cho, Adachi-ku Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[toki@vcl.im.dendai.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>358882</ref_obj_id>
				<ref_obj_pid>358876</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Whitted, T, 1980. An Improved Illumination Model for Shaded Display. Comm. ACM. 23, 6, June 1980, 343--349.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>37408</ref_obj_id>
				<ref_obj_pid>37402</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Shinya, M., Takahashi, T., Naito, S, 1987. Principles and Applications of Pencil Tracing. <i>SIGGRAPH Comput. Graph</i>. 21, 4 (August 1987), 45--54. DOI=10.1145/37402.37408]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Pencil Tracing Mirage: Principle and its Evaluation Katsuhisa Kanazawa Yuma Sakato Tokiichiro Takahashi 
* Tokyo Healthcare University Tokyo Denki University/*UEI Research Setagaya 3 11 3 Setagaya-ku Tokyo 
Senju Asahi-cho 5 Adachi-ku Tokyo k-kanazawa@thcu.ac.jp {kanazawa, sakato, toki}@vcl.im.dendai.ac.jp 
 1. Introduction To generate realistic representation of the nature scene is one of the most challenging 
areas in the computer graphics community. Ray tracing[1] is the most well-known technique to synthesize 
a realistic image. Since ray tracing is the most suitable method for simulating reflection and refraction 
of the light, it has been used for simulating atmospheric optical phenomena due to reflection and refraction 
of the light. The mirage is a kind of atmospheric optical phenomenon. Therefore, it is possible to synthesize 
mirages in 3DCG by simulating or modeling condition of the air. We focus on pencil tracing technique[2] 
that is an extention of conventional ray tracing technique based on the paraxial approximation theory. 
Our simple method based on pencil tracing can efficiently generate an appearance of mirage without any 
complex thermodynamic simulation. 2. Proposed Method We extend the pencil tracing method to synthesize 
mirages. Our idea is to introduce a special kind of object in scene, and integrate a perturbation factor 
into image generation process of the pencil tracing. Our method is useful because it can treat perturbation 
of the air as simply matrix product. 2.1 Pencil Tracing The pencil tracing is a method for accelerating 
image generation of the ray tracing method by tracing a pencil (or bundle) of rays instead of an individual 
ray. Thus, a ray is not traced per a pixel or a sub-pixel, it is traced per domain of pixels. An axial 
ray (center ray of domain) is traced in the same manner as ray tracing, but paraxial rays (surrounding 
rays of the axial ray) are omitted to be traced by using result of tracing their axial ray. Therefore, 
pencil tracing is faster than the conventional ray tracing. In this method, an optical system affecting 
a path of the axial ray is represented as 4x4 matrix called system matrix, and a ray is represented as 
4D vector called ray-vector. Paraxial rays changed by the optical system are approximated by the transformation 
of their ray-vector representation with the system matrix. This is formulated as follows: l'=Tl Here, 
l,l'are ray-vectors of an original ray and of a transformed ray, respectively. Tis a system matrix. 
 2.2 Perturbation Object and Matrix Most of mirages have fluctuated appearance due to perturbation of 
the air. It is one of the most important factors of the mirage appearance. To synthesize this, we introduce 
a perturbation object. This is a special kind of scene object that adds positional and directional noise 
to a ray. To integrate this operation of addition into the form of matrix product, we extend a system 
matrix to 5x5. Extended system matrix and one for the perturbation TPis as follows:  Figure 2. Image 
Generation Time. T'=(T o), TP=(e tE) t t o1o1Here Tis a 4x4 system matrix and T'is its equivalent extended 
matrix, eis a 4x4 identity matrix, tEis noise ray-vector, and ois 4D column zero vector. 3. Results As 
shown in Fig.1, images of square sun mirage are generated by our method. These images are a part of the 
short video (450 frames). Compared to the actual video (see the supplemental video), realistic images 
are successfully generated. All the images of Fig.1 are generated by the following computer environments: 
CPU -Intel Core i7-920 (2.66GHz), system memory -16GB. We measured the image generation time of a variety 
of sizes of images and sizes of domains. Figure 2 shows the generation time of 450 images of square sun 
mirage. Measurement results verified that our method can generate images about 1.5-5 times as fast as 
the conventional method. 4. Conclusions We proposed a pencil tracing technique for synthesizing mirages. 
Our method generated images of square sun mirage fast and efficiently. References [1] WHITTED, T, 1980. 
An Improved Illumination Model for Shaded Display. Comm. ACM. 23, 6, June 1980, 343-349. [2] SHINYA, 
M., TAKAHASHI, T., NAITO, S, 1987. Principles and Applications of Pencil Tracing. SIGGRAPH Comput. Graph. 
21, 4 (August 1987), 45-54. DOI=10.1145/37402.37408 Permission to make digital or hard copies of part 
or all of this work for personal or classroom use isgranted without fee provided that copies are not 
made or distributed for commercial advantage and that copies bear this notice and the full citation on 
the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503499</article_id>
		<sort_key>1140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>104</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Real-time dust rendering by parametric shell texture synthesis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503499</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503499</url>
		<abstract>
			<par><![CDATA[<p>When we synthesize a realistic appearance of dust-covered object by CG, it is necessary to express a large number of fabric components of dust accurately with many short fibers, and as a result, this process is a time-consuming task. The dust amount prediction function suggested by Hsu [1995] proposed modeling and rendering techniques for dusty surfaces. These techniques only describe dust accumulation as a shading function, however, they cannot express the volume of dust on the surfaces. In this study, we present a novel method to model and render the appearance and volume of dust in real-time by using shell texturing. Each shell texture, which can express several components, is automatically generated in our procedural approach. Therefore, we can draw any arbitrary appearance of dust rapidly and interactively by solely controlling simple parameters.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190972</person_id>
				<author_profile_id><![CDATA[82459250557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Adachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190973</person_id>
				<author_profile_id><![CDATA[81504686871]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ukaji]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190974</person_id>
				<author_profile_id><![CDATA[81504687323]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190975</person_id>
				<author_profile_id><![CDATA[82459042957]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>617946</ref_obj_id>
				<ref_obj_pid>616034</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hsu, S. C., and Wong, T. T. 1995. Simulating Dust Accumulation. IEEE Computer Graphics and Applications, vol. 15, 18--22.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-Time Dust Rendering by Parametric Shell Texture Synthesis C:\Users\syouh\Desktop\.3.png Shohei 
ADACHI* Hiroaki UKAJI Takahiro KOSAKA Shigeo MORISHIMA Waseda University (a)Synthesized Fabric Dust 
      (b)Synthesized Fugitive Dust Figure 1 Synthesized Dust Variations by Parameter Control 
1. Introduction When we synthesize a realistic appearance of dust-covered object by CG, it is necessary 
to express a large number of fabric components of dust accurately with many short fibers, and as a result, 
this process is a time-consuming task. The dust amount prediction function suggested by Hsu [1995] proposed 
modeling and rendering techniques for dusty surfaces. These techniques only describe dust accumulation 
as a shading function, however, they cannot express the volume of dust on the surfaces. In this study, 
we present a novel method to model and render the appearance and volume of dust in real-time by using 
shell texturing. Each shell texture, which can express several components, is automatically generated 
in our procedural approach. Therefore, we can draw any arbitrary appearance of dust rapidly and interactively 
by solely controlling simple parameters. 2. Accumulated Dust Range Our method uses shell texturing for 
rapid rendering of volumes. We have to define the distance between original polygon and top-lapped texture 
as the volume of accumulated dust. We determine this distance by using Hsu s function, the value of which 
is calculated by both normal and dust source directions. However, using only this function cannot express 
an uneven surface of actual accumulated dust caused by its components. To represent unevenness, we use 
Perlin noise which can generate in procedural approach and is utilized for rendering natural phenomenon. 
When we lap a rendering object with shell textures, we control rendering range of each texture by the 
brightness of Perlin noise texture. As a result, the variation of rendering range of each texture can 
represent an uneven feature of actual accumulated dust on the object surface. 3. Dust Texture Synthesis 
For rendering components, we propose a new parametric synthesis of shell texture in procedural approach. 
We focus on a fibrous component whose appearance contributes to dust reality. To express such feature 
on a shell texture, we model each fiber with a point trajectory on 2D plane which direction is controlled 
by probability of movement map (PMmap) and 3 parameters. PMmap controls a direction of each point moving 
pixel by pixel by the probability defined for 8 pixels surrounding the current pixel. Probability for 
direction angle is given by the following functions. {   (  )      (   )   (    )( 
  )      (1) is quantized into 8 directions to generate PMmap. is arbitrary bending degree which 
is one of the additional 3 parameters. The other two parameters are movement time which controls the 
length of fiber and bending times which controls how many times fiber bends in unit fiber. By focusing 
on fibrous structure of real dust, we divide each fiber into 2 components such as slight movement part 
which has small angle not to affect a general direction of fiber and bending part which has great angle 
to affect general direction of fiber. Slight movement part is represented by movement depending on PMmap. 
Bending part is generated when probability of PMmap is recalculated by arbitrary bending degree and arbitrary 
bending times. Huge number of fibers are generated based on this procedure, then we can get a shell texture 
which can represent realistic appearance of accumulated dust fibers. 4. Results Our synthesis results 
are shown in Figure 1. These results indicate that we are able to synthesize real fabric or fugitive 
dust appearances of arbitrary dust-covered objects only by modulating simple parameters. These results 
also show that our method succeeded to express detailed dust contents, which cannot be performed by previous 
works, especially even when we watch it in enlarged illustration. The execution time of rendering Dragon 
model (550,968vtx) is 13.69[ms], and this is obtained from 2.9GHz Intel(R) Xeon(R) CPUX5647 with NVIDIA 
Quadro 4000. Using our method, it is possible to render dusty objects rapidly in real-time as well as 
original shell texturing. 5. Conclusions In this paper, we propose a method for real-time rendering 
of realistic dust-covered object by focusing on the dust components. In our method, the process for acquiring 
each texture is automatic and procedural, therefore, we achieved several appearances of dust can be synthesized 
only by controlling simple parameters. This work suggests some tasks for future researches, such as detecting 
and adjusting each parameter for real captured dust appearance. Reference HSU,S.C.,AND WONG,T. T. 1995. 
Simulating Dust Accumulation. IEEE Computer Graphics and Applications,vol. 15, 18-22. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503500</article_id>
		<sort_key>1150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>105</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Reflectance estimation of human face from a single shot image]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503500</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503500</url>
		<abstract>
			<par><![CDATA[<p>Simulation of the reflectance of translucent materials is one of the most important factors in the creation of realistic CG objects. Estimating the reflectance characteristics of translucent materials from a single image is a very efficient way of re-rendering objects that exist in real environments. However, this task is considerably challenging because this approach leads to problems such as the existence of many unknown parameters. Munoz et al. [2011] proposed a method for the estimation of the bidirectional surface scattering reflectance distribution function (BSSRDF) from a given single image. However, it is difficult or impossible to estimate the BSSRDF of materials with complex shapes because this method's target was the convexity of objects therefore, it used a rough depth recovery technique for global convex objects. In this paper, we propose a method for accurately estimating the BSSRDF of human faces, which have complex shapes. We use a 3D face reconstruction technique to satisfy the above assumption. We are able to acquire more accurate geometries of human faces, and it enables us to estimate the reflectance characteristics of faces.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190976</person_id>
				<author_profile_id><![CDATA[82459234757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kazuki-o-paper@ruri.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190977</person_id>
				<author_profile_id><![CDATA[81488654875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190978</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190979</person_id>
				<author_profile_id><![CDATA[81504685727]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383319</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jensen, H. W., Marschner, S. R., Levoy, M., and Hanrahan, P. 2001. A practical model for subsurface light transport. In <i>Proc. SIGGRAPH 2001</i>, 511--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Maejima, A., and Morishima, S. 2008. Fast Plausible 3D Face Generation from a Single Photograph. <i>ACM SIGGRAPH ASIA 2008, Poster, maejima.pdf</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Munoz, A., Echevarria, J. I., Seron, F. J., Lopez-Moreno, J., Glencross, M., and Gutierrez, D. 2011. BSSRDF Estimation from Single Images. In <i>Proc. Computer Graphics Forum</i> 30, 2, 455--464.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Reflectance Estimation of Human Face from a Single Shot Image G:\..\2012..\..\SIGGRAPH2013\abstaract\image\workflow.bmp 
Kazuki Okami* Naoya Iwamoto Akinobu Maejima Shigeo Morishima Waseda University  Figure 1. Our work 
flow 1. Introduction Simulation of the reflectance of translucent materials is one of the most important 
factors in the creation of realistic CG objects. Estimating the reflectance characteristics of translucent 
materials from a single image is a very efficient way of re-rendering objects that exist in real environments. 
However, this task is considerably challenging because this approach leads to problems such as the existence 
of many unknown parameters. Munoz et al. [2011] proposed a method for the estimation of the bidirectional 
surface scattering reflectance distribution function (BSSRDF) from a given single image. However, it 
is difficult or impossible to estimate the BSSRDF of materials with complex shapes because this method 
s target was the convexity of objects therefore, it used a rough depth recovery technique for global 
convex objects. In this paper, we propose a method for accurately estimating the BSSRDF of human faces, 
which have complex shapes. We use a 3D face reconstruction technique to satisfy the above assumption. 
We are able to acquire more accurate geometries of human faces, and it enables us to estimate the reflectance 
characteristics of faces. 2. Our Approach Our proposed method is composed of two estimation processes. 
Normal Estimation: We apply the 3D face reconstruction method proposed by Maejima et al. [2008] to acquire 
accurate information of human face geometries. This method, which is based on the deformable face model, 
creates a 3D human face model from a single frontal face image. BSSRDF Estimation: On the basis of the 
dipole diffusion approximation [Jensen et al. 2001], multiple subsurface scattering is expressed as follows: 
.     (1) We estimate the BSSRDF model from the given single frontal face image using Munoz s method, 
which is based on the previously recovered object shape, and represented color and light information. 
 is called the diffuse reflectance function, which is * e-mail: kazuki-o-paper@ruri.waseda.jp e-mail: 
shigeo@waseda.jp the only unknown factor in equation (1), and it is approximated by a linear combination 
of a set of piecewise constant functions. This approximation is derived using the Quasi-Minimal Residual 
(QMR) method. Then, this approximated function is optimized to continuous, differentiable, monotonically 
decreasing function. 3. Work Flow First, as the input in Figure 1-(a), we use a single image of the frontal 
face and a mask image to remove parts that can lead to errors, such as eyes, lips, beards. (a) Input 
Image  Next, a surface normal, irradiance, and texture color at each vertex are calculated from the 
known light information, camera position, and the normal estimation. Finally, we estimate the reflectance 
characteristics using the BSSRDF estimation. 4. Result and Discussion The reflectance characteristics 
of the input face by using Munoz s method and our method are shown in Figure 1-(b). The rendered objects 
using each estimated reflectance profile are shown in Figure 1-(c). Munoz s results show an overly reddish 
hue as the face of the input. In contrast, we can render more similar hue of input human face using our 
estimation result. As future work, it is necessary to handle unknown environments to extinguish environmental 
constraints such as light sources. At present, we use a known light source for comparison purposes that 
require accurate light information. Furthermore, we will add a new feature to express local characteristics 
of the human face. References JENSEN, H. W., MARSCHNER, S. R., LEVOY, M., AND HANRAHAN, P. 2001. A practical 
model for subsurface light transport. In Proc. SIGGRAPH 2001, 511-518. MAEJIMA, A., AND MORISHIMA, S. 
2008. Fast Plausible 3D Face Generation from a Single Photograph. ACM SIGGRAPH ASIA 2008, Poster, maejima.pdf. 
MUNOZ, A., ECHEVARRIA, J. I., SERON, F. J., LOPEZ-MORENO, J., GLENCROSS, M., AND GUTIERREZ, D. 2011. 
BSSRDF Estimation from Single Images. In Proc. Computer Graphics Forum 30, 2, 455-464. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503501</article_id>
		<sort_key>1160</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>106</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Replica exchange light transport on relaxed distributions]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503501</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503501</url>
		<abstract>
			<par><![CDATA[<p>Developing a robust method for computing global illumination is a challenging problem. A Markov chain Monte Carlo (MCMC) method, like [Jakob and Marschner 2012], samples the light path space with a probability proportional to the per-path contribution, by successively mutating path samples (e.g., perturbing a reflection direction). In practice, a path sample could get stuck in a high energy peak for multiple mutations, resulting in a bright spot artifact. To resolve this problem, we present a new unbiased rendering framework based on a replica exchange technique [Kitaoka et al. 2009], a variant of MCMC technique. A replica exchange technique incorporates a set of different distributions. We propose to introduce a set of relaxed distributions, which are beneficial for reducing the chance of getting stuck.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190980</person_id>
				<author_profile_id><![CDATA[82459024257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hisanari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Otsu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190981</person_id>
				<author_profile_id><![CDATA[82458900757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yonghao]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yue]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190982</person_id>
				<author_profile_id><![CDATA[82458648257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Qiming]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hou]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Zhejiang University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190983</person_id>
				<author_profile_id><![CDATA[81331494952]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Wakayama University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190984</person_id>
				<author_profile_id><![CDATA[81100329647]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yoshinori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dobashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190985</person_id>
				<author_profile_id><![CDATA[82458820457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Tomoyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishita]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hiroshima Shudo University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2185554</ref_obj_id>
				<ref_obj_pid>2185520</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jakob, W., and Marschner, S. 2012. Manifold exploration: a markov chain monte carlo technique for rendering scenes with difficult specular transport. <i>ACM Trans. Graph. (SIGGRAPH 2012) 31</i>, 4, 58:1--58:13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kelemen, C., Szirmay-Kalos, L., Antal, G., and Csonka, F. 2002. A simple and robust mutation strategy for the metropolis light transport algorithm. <i>Computer Graphics Forum (EUROGRAPHICS 2002) 21</i>, 3, 531--540.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kitaoka, S., Kitamura, Y., and Kishino, F. 2009. Replica exchange light transport. <i>Computer Graphics Forum 28</i>, 8, 2330--2342.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: (a) Reference image. (d): An enlarged view of the region marked red in (a). (b) and (c) show 
the enlarged rendered results for the same region, using [Kelemen et al. 2002] and our method, respectively. 
(b) and (c) are rendered with the same computation time. 1 Introduction Developing a robust method for 
computing global illumination is a challenging problem. A Markov chain Monte Carlo (MCMC) method, like 
[Jakob and Marschner 2012], samples the light path space with a probability proportional to the per-path 
contribution, by successively mutating path samples (e.g., perturbing a re.ec­tion direction). In practice, 
a path sample could get stuck in a high energy peak for multiple mutations, resulting in a bright spot 
arti­fact. To resolve this problem, we present a new unbiased rendering framework based on a replica 
exchange technique [Kitaoka et al. 2009], a variant of MCMC technique. A replica exchange tech­nique 
incorporates a set of different distributions. We propose to introduce a set of relaxed distributions, 
which are bene.cial for re­ducing the chance of getting stuck. 2 Our Framework We use multiple distributions 
fk (Figure 2), where k is a level, and assign a path sample Xk to each of these distributions. Each path 
sample Xk (a random variable in the sample space corresponding to a light path x¯) is successively mutated 
to obtain a sequence of the (i) (i) sample {X }, so that the distribution of {X } is proportional to 
k k fk. The replica exchange technique can be viewed as a Metropolis technique working on a product sample 
space X = X0 × X1 × · · · with a distribution F = f0f1 · · · . For the distributions, we choose f0 to 
be proportional to the contri­bution I(x¯) of light path x¯. For higher levels, we propose to choose 
fk to be proportional to I(x¯) + .k, where .k is a relaxation constant and is larger for higher level 
k. As the sample space for the light paths, we consider primary sample space (a high dimensional unit 
cube de.ning the parameters of a path) introduced by [Kelemen et al. 2002]. Distributions in higher levels 
are more relaxed, in the sense that when we consider the normalized versions of I(x¯) + .k, a higher 
level distribution is closer to the uniform distribution. We utilize two kinds of mutations: (1) mutating 
a path sample in each level independently, and (2) exchanging path samples be­tween two successive levels. 
For (1), the mutation and the accep­tance probability are de.ned similarly to [Kelemen et al. 2002]. 
For (2), the acceptance probability for exchanging two samples (i) (i) X and X in successive levels k 
and k + 1 is de.ned as k k+1 (i) (i) (i) (i) fk(Xk+1)fk+1(Xk a(X . X ) = min(1, ) ). Since the k k+1 
(i) (i) fk(X ) k )fk+1(Xk+1 peaks in higher levels are more smoothed, the acceptance proba-  Figure 
2: Illustration of our framework. bility for (1) becomes higher for a higher level. Hence, the samples 
can be more easily mutated across the whole space, through the relaxed distributions. As X0 is proportionally 
distributed according to I(x¯), it can be di­rectly used for rendering the image similar to a Metropolis 
frame­work. A sample in any higher level k, can also be used to provide an unbiased estimate of the .nal 
image, by multiplying a factor of . ckI(x¯) I(¯x)+.k , where ck = P (I(x¯) + .k)dx¯is a normalizing factor 
computed from a uniform sampling of the space (i.e., via large step mutations), and P is the light path 
space. (We can regard that es­timate as a result of performing importance sampling according to I(x¯) 
+ .k.) Additionally, we combine the estimates from different levels using multiple importance sampling 
for variance reduction.  3 Results and Future Work We show the rendered results in Figure 1, together 
with a compari­son against previous method [Kelemen et al. 2002]. With the same computation time, our 
method can signi.cantly reduce the bright spots. An interesting remark is that although the relaxed distribu­tion 
is different from the distribution of contributions hence seems to be less ef.cient in terms of importance 
sampling, they indeed help improve the overall computation ef.ciency. Moreover, since the values of a 
relaxed distribution are always larger than 0, ergod­icity is guaranteed even if we only use local perturbations, 
which is not the case in previous MCMC based rendering techniques. As future work, we would like to theoretically 
analyze the ef.ciency of our framework, and to seek for a spatial subdivision structure, where each region 
has an individual relaxation constant, for further improvement of the ef.ciency. We also plan to combine 
our method with [Jakob and Marschner 2012]. References JA KO B , W., A N D MA RSC H N E R, S. 2012. 
Manifold exploration: a markov chain monte carlo technique for rendering scenes with dif.cult specular 
transport. ACM Trans. Graph. (SIGGRAPH 2012) 31, 4, 58:1 58:13. KEL E M E N , C., SZ I RM AY-KALOS, L., 
AN TA L, G., A N D CS O N KA, F. 2002. A simple and robust mutation strategy for the metropolis light 
transport algorithm. Computer Graphics Forum (EUROGRAPHICS 2002) 21, 3, 531 540. KI TAO K A , S., KI 
TA M UR A , Y., A N D KI S H IN O , F. 2009. Replica exchange light transport. Computer Graphics Forum 
28, 8, 2330 2342. Permission to make digital or hard copies of part or all of this work for personal 
or classroom use isgranted without fee provided that copies are not made or distributed for commercial 
advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503502</article_id>
		<sort_key>1170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>107</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Specular lobe aware upsampling based on spherical Gaussians]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503502</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503502</url>
		<abstract>
			<par><![CDATA[<p>This poster introduces a novel weighting function of bilateral upsampling for specular surfaces. High-resolution rendering with expensive shaders (e.g., global illumination) is a considerable problem for real-time applications such as video games. Therefore, some bilateral upsampling based methods were proposed to alleviate the computational burden. Spatio-temporal upsampling [Herzog et al. 2010] is also proposed to generate more sample pixels than spatial bilateral upsampling by reusing past frames. The main challenge of these upsampling techniques is to optimize a weighting function in order to estimate a pixel value by appropriately prioritizing samples. The weighting function commonly evaluates a similarity of pixel values. For example, a surface normal continuity is generally used for evaluating a geometric similarity, which is well suited for diffuse surfaces because the shading results depend on the normal vectors. However, for specular surfaces, we have to take into account not only the normal vectors but also the eye directions and the specular sharpness to generate accurate results, since specular lobes are determined by them.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190986</person_id>
				<author_profile_id><![CDATA[81448595033]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tokuyoshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Square Enix Co., Ltd.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tokuyosh@square-enix.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Crassin, C., Neyret, F., Sainz, M., Green, S., and Eisemann, E. 2011. Interactive indirect illumination using voxel cone tracing. <i>Comput. Graph. Forum 30</i>, 7, 1921--1930.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1730819</ref_obj_id>
				<ref_obj_pid>1730804</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Herzog, R., Eisemann, E., Myszkowski, K., and Seidel, H.-P. 2010. Spatio-temporal upsampling on the gpu. In <i>Proc. of I3D 2010</i>, 91--98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1618479</ref_obj_id>
				<ref_obj_pid>1618452</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wang, J., Ren, P., Gong, M., Snyder, J., and Guo, B. 2009. All-frequency rendering of dynamic, spatially-varying reflectance. <i>ACM Trans. Graph. 28</i>, 5, 133:1--133:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Specular Lobe Aware Upsampling Based on Spherical Gaussians Yusuke Tokuyoshi* Square Enix Co., Ltd. 
 Introduction This poster introduces a novel weighting function of bilateral upsampling for specular 
surfaces. High-resolution ren­dering with expensive shaders (e.g., global illumination) is a con­siderable 
problem for real-time applications such as video games. Therefore, some bilateral upsampling based methods 
were pro­posed to alleviate the computational burden. Spatio-temporal up­sampling [Herzog et al. 2010] 
is also proposed to generate more sample pixels than spatial bilateral upsampling by reusing past frames. 
The main challenge of these upsampling techniques is to optimize a weighting function in order to estimate 
a pixel value by appropriately prioritizing samples. The weighting function com­monly evaluates a similarity 
of pixel values. For example, a surface normal continuity is generally used for evaluating a geometric 
sim­ilarity, which is well suited for diffuse surfaces because the shading results depend on the normal 
vectors. However, for specular sur­faces, we have to take into account not only the normal vectors but 
also the eye directions and the specular sharpness to generate accu­rate results, since specular lobes 
are determined by them. We propose an ef.cient weighting function based on a specular lobe similarity. 
This function is simple and has no additional storage cost. Moreover, it has no user-speci.ed parameters. 
Thus it can be easily integrated with general upsampling techniques. Our Weighting Function A weighting 
function should evaluate a similarity of the re.ected radiance between the current pixel i and the sampled 
pixel j, which is the inner product of the inci­dent radiance and the re.ection lobe. Assuming that the 
incident radiance distributions of two pixels are identical, the difference of the re.ected radiance 
is determined by only the re.ection lobes. Therefore, we introduce a weighting function wi,j representing 
the specular lobe similarity as given by computing the inner product of the two specular lobes. However, 
the shape of the specular lobe depends on the BRDF model. Furthermore, there is not always an analytical 
solution of the inner product. In this poster, we approximate the lobes by using spherical Gaus­sians 
(SGs). As described in [Wang et al. 2009], a specular lobe can be approximated with an SG G(.) where 
. is the incident direc­tion of light. The inner product of two SGs is analytically obtained. Hence, 
we de.ne the weighting function as the inner product of the normalized SGs: wi,j = Gi(.) Gi(.) · Gj (.) 
Gj (.) . (1) The range of this weighting function is [0, 1]. When the two lobes are same, wi,j = 1. 
Applications and Conclusion We here employ voxel cone trac­ing [Crassin et al. 2011] with spatio-temporal 
upsampling as an ex­ perimental example since voxel cone tracing is known to be com­putationally expensive 
to render specular surfaces. Figure 1 shows the comparison of a general normal based weight­ ing function 
and the proposed weighting function. In the left images using the normal based weighting function, there 
are some blurring and .ickering artifacts due to estimation errors. On the other hand, *e-mail:tokuyosh@square-enix.com 
 normal based proposed na¨ive rendering 2.3 ms 2.5 ms 11 ms Figure 1: Upper: the specular indirect illumination 
results via voxel cone tracing. Lower: close-ups of the upper images (brightness×4). The left and middle 
images are upsampled from 480 × 270 to 1920 × 1080 pixels. The right images are rendered with na¨ive 
per-pixel cone tracing. The high-frequency noise is due to stochastic sampling for ray marching. The 
BRDF is the Blinn-Phong model (phong exponent: 1023). The normal based weight­ing function has a Gaussian 
distribution (the variance parameter: 0.005) for this experiment. The computation time of cone tracing 
and upsampling is only 2.3 ms (left) and 2.5 ms (middle), while na¨ive per-pixel cone tracing is 11 ms 
(right) (GPU: AMD Radeon HD 6990). In addition, our method (middle) produces closer im­ages to the truth. 
our method enables higher-quality upsampling without parameter tuning. Our approach adaptively reduces 
blurring and .ickering ar­tifacts depending on specular sharpness and eye directions, in con­trast to 
the normal based weighting function does not. Our approach is limited to isotropic BRDFs which can be 
approx­imated by a single SG. However, this is not a problem for voxel cone tracing which also approximates 
BRDFs by a Gaussian lobe. In addition, we can introduce anisotropic BRDFs using several SGs with sacri.cing 
time. Our weighting function is also applicable for geometry aware blur­ring (i.e., specular lobe aware 
blurring). In future work, we would like to investigate the effectiveness of such bilateral .ltering. 
 References CR ASSI N , C., NE Y R ET, F., SA I N Z, M., GR EE N, S., A N D EI S E-MAN N , E. 2011. Interactive 
indirect illumination using voxel cone tracing. Comput. Graph. Forum 30, 7, 1921 1930. HER Z O G , R., 
EI S E M A N N, E., MYSZKOWSK I , K., A N D SE I D EL, H.-P. 2010. Spatio-temporal upsampling on the 
gpu. In Proc. of I3D 2010, 91 98. WA NG, J., REN , P., GON G , M., SN Y DE R, J., AND GU O , B. 2009. 
All-frequency rendering of dynamic, spatially-varying re­.ectance. ACM Trans. Graph. 28, 5, 133:1 133:10. 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503503</article_id>
		<sort_key>1180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>108</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[The hand as a shading probe]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503503</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503503</url>
		<abstract>
			<par><![CDATA[<p>To render computer graphics (CG) objects realistically in the real world, it is important to match their shadings to the scene. Existing methods to achieve such realism are not applicable to realtime consumer augmented reality (AR). Debevec's method requires a mirrored sphere, which ordinary consumers do not have [Debevec 1998]. Karsh's method requires user annotation, which makes it difficult to use in a realtime application [Karsch et al. 2011].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190987</person_id>
				<author_profile_id><![CDATA[81504684161]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yasuhiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nippon Telegraph and Telephone Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yao.yasuhiro@lab.ntt.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190988</person_id>
				<author_profile_id><![CDATA[81504682747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Harumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kawamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nippon Telegraph and Telephone Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kawamura.harumi@lab.ntt.co.jp]]></email_address>
			</au>
			<au>
				<person_id>P4190989</person_id>
				<author_profile_id><![CDATA[81100576829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akira]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kojima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nippon Telegraph and Telephone Corporation]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kojima.akira@lab.ntt.co.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>280864</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. 1998. Rendering synthetic objects into real scenes: bridging traditional and image-based graphics with global illumination and high dynamic range photography. In <i>SIGGRAPH '98</i>, ACM, New York, NY, USA, 189--198.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2024191</ref_obj_id>
				<ref_obj_pid>2070781</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Karsch, K., Hedau, V., Forsyth, D., and Hoiem, D. 2011. Rendering synthetic objects into legacy photographs. <i>ACM Trans. Graph. 30</i>, 6 (Dec), 157:1--157:12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>383317</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ramamoorthi, R., and Hanrahan, P. 2001. An efficient representation for irradiance environment maps. In <i>SIGGRAPH '01</i>, ACM, New York, NY, USA, 497--500.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yao, Y., Kawamura, H., and Kojima, A. 2012. Shading derivation from an unspecified object for augmented reality. In <i>ICPR '12</i>, IAPR, 57--60.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: The system calculates shading matched to the scene from a hand automatically in real time 
(left). The ringing artifact seen at the bottom left boundary, which typically appears when there are 
not enough sample points, disappeared by imposing regularization (right). 1 Introduction According to 
[Ramamoorthi and Hanrahan 2001], the ratio of SH weight magnitudes should be around the ratio of when 
they A l model irradiance or diffuse shading. The regularization is imposed To render computer graphics 
(CG) objects realistically in the real world, it is important to match their shadings to the scene. Exist­so 
that the weights will satisfy this condition. The regularization ing methods to achieve such realism 
are not applicable to realtime weakens ringing artifacts that typically appear when there are not consumer 
augmented reality (AR). Debevec s method requires a enough sample points (see Fig.1 (right)). mirrored 
sphere, which ordinary consumers do not have [Debevec 1998]. Karsh s method requires user annotation, 
which makes it Since the minimization operation shown as (1) above can calculate SH weights in a single 
step, our method is suitable for realtime dif.cult to use in a realtime application [Karsch et al. 2011]. 
applications. This is different from the method reported in [Yao Here, we propose a shading derivation 
method utilizing the human et al. 2012] which requires iterative calculation. hand as a light probe. 
We chose to use the hand since hands are readily available if people are present. The system we implemented 
the method into automatically detects the hand, calculates shading in realtime, and is easy to use. The 
method is suitable for consumer applications since it can be implemented into a system merely by 3 Implementation 
 We developed a system that combined our method with hand track­ ing by using an RGB-D sensor, ASUS Xtion 
Pro Live. The system using a commercial RGB-D sensor. 2 Our Method tracks a hand and derives its area 
by using hue and depth differ­ences. Then, from the luminance values and surface normals of the points 
in the area, it uses our method to calculate shading in re­altime and then uses the shading to render 
CG objects (see Fig.1 The method is based on regularized spherical harmonic (SH) re­(left)). Since this 
system also utilizes the hand as a geometrical gression that is modi.ed from [Yao et al. 2012] to enable 
it to work probe, it can place CG objects at the position where the hand was in realtime. For every point 
of a hand, we take the luminance value with shading matched to the position. To use this system, users 
(iis an index) and surface normal (Bi, i), and calculate SH L only need to place a hand in front of an 
RGB-D sensor. This ease i weights w m l , (0 : l: 2) as follows. of use creates possibilities for using 
our method in consumer AR applications involving persons such as virtual .tting. m lm l )}) + .r {e arg 
min l (w (w m l , 0.l.2 w  References 2 2 m l e(w m lm l (1) i - ) =L Y(Bi, i)w DEBEVEC, P. 1998. 
Rendering synthetic objects into real scenes: i l=0 m=-l bridging traditional and image-based graphics 
with global illu­ 12 mination and high dynamic range photography. In SIGGRAPH 9 0 2 0) 2 2 (w m l mm 
) = (w (w )+ 16 (w )+ r 98, ACM, New York, NY, USA, 189 198. 1 2 4 m=-1 m=-2 KARSCH, K., HEDAU, V., FORSYTH, 
D., AND HOIEM, D. 2011. ,where e (w ) is an error term and .r) is a regularization term. m l m l (w Rendering 
synthetic objects into legacy photographs. ACM 0 The term r ) is designed so that w , and are equally 
(w m lmm w w 0, Trans. Graph. 30, 6 (Dec), 157:1 157:12. 1 2 penalized when their magnitudes take the 
ratio of as rel. (2), A l where A l is described in [Ramamoorthi and Hanrahan 2001].2 = 1 : RAMAMOORTHI, 
R., AND HANRAHAN, P. 2001. An ef.cient representation for irradiance environment maps. In SIGGRAPH w 
2 1 01, ACM, New York, NY, USA, 497 500. : |w 01 0 | : |w m 2 | = A0 : A1 : A m (2) : 3 4 YAO, Y., KAWAMURA, 
H., AND KOJIMA, A. 2012. Shading *e-mail: yao.yasuhiro@lab.ntt.co.jp derivation from an unspeci.ed object 
for augmented reality. In e-mail: kawamura.harumi@lab.ntt.co.jp ICPR 12, IAPR, 57 60. e-mail: kojima.akira@lab.ntt.co.jp 
Permission to make digital or hard copies of part or all of this work for personal or classroom use isgranted 
without fee provided that copies are not made or distributed for commercial advantage and that copies 
bear this notice and the full citation on the first page. Copyrights for third-party components of thiswork 
must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, 
California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503504</article_id>
		<sort_key>1190</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>109</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Toward efficient and accurate order-independent transparency]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503504</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503504</url>
		<abstract>
			<par><![CDATA[<p>Correctly rendering multi-layered transparent geometry requires accumulating contributions from multiple fragments per pixel. Dynamic A-buffers (e.g., Yang et al's [2010] per-pixel linked lists) achieve this by storing and sorting fragments on-the-fly. We introduce two improvements to recent GPU-based interactive A-buffer techniques. First, our redesigned algorithm uses fewer costly global atomic operations to construct linked lists. Second, we decouple visibility and shading to reduce memory demands of multi-fragment rendering.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[order independent transparency]]></kw>
			<kw><![CDATA[real-time rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190990</person_id>
				<author_profile_id><![CDATA[82458965257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ethan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kerzner]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ethan-kerzner@uiowa.edu]]></email_address>
			</au>
			<au>
				<person_id>P4190991</person_id>
				<author_profile_id><![CDATA[81100265704]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wyman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Iowa]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190992</person_id>
				<author_profile_id><![CDATA[81100547853]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Lee]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Butler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[US Army Research Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P4190993</person_id>
				<author_profile_id><![CDATA[81100561107]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Christiaan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gribble]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[SURVICE Engineering]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2159640</ref_obj_id>
				<ref_obj_pid>2159616</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Liktor, G., and Dachsbacher, C. 2012. Decoupled deferred shading for hardware rasterization. 143--150.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Vasilakis, A., and Fudos, I. 2012. S-buffer: Sparsity-aware multi-fragment rendering. <i>Eurographics Symposium on Rendering</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383624</ref_obj_id>
				<ref_obj_pid>2383616</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Yang, J., Hensley, J., Grun, H., and Thibieroz, N. 2010. Real-time concurrent linked list construction on the gpu. <i>Computer Graphics Forum 29</i>, 4, 1297--1304.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Toward Ef.cient and Accurate Order-Independent Transparency Ethan Kerzner* Chris Wyman Lee Butler Christiaan 
Gribble University of Iowa University of Iowa US Army Research Laboratory SURVICE Engineering Abstract 
 Correctly rendering multi-layered transparent geometry requires accumulating contributions from multiple 
fragments per pixel. Dy­namic A-buffers (e.g., Yang et al s [2010] per-pixel linked lists) achieve this 
by storing and sorting fragments on-the-.y. We intro­duce two improvements to recent GPU-based interactive 
A-buffer techniques. First, our redesigned algorithm uses fewer costly global atomic operations to construct 
linked lists. Second, we decou­ple visibility and shading to reduce memory demands of multi­fragment 
rendering. Keywords: order independent transparency, real-time rendering 1 Introduction and Previous 
Work Multi-fragment rendering has three per-pixel steps: identifying primitive visibility, shading, 
and accumulating .nal pixel color. Yang et al s [2010] per-pixel linked lists (PPLL) determine visi­ 
bility in a single rendering pass, storing all fragments in a global buffer with a global atomic counter 
controlling write access. In contrast, the S-buffer [Vasilakis and Fudos 2012] uses two passes. The .rst 
obtains per-pixel fragment counts and allocates contigu­ous memory for each pixel s fragments. Memory 
requirements for both algorithms scale linearly with fragment count, but the S-buffer performs faster 
than PPLLs as it reduces global atomic contention during list creation and improves spatial coherence 
of fragment data when accumulating pixel colors. 2 Reduced Contention Per-Pixel Linked Lists PPLLs use 
a global atomic counter to dynamically allocate a linked list node for each fragment. We propose reducing 
global contention by allocating memory per-primitive rather than per-fragment. Our primitive-allocated 
linked lists (PALLs) use a conservative bound of a primitive s fragment count to allocate memory, reducing 
frag­ment shader atomic contention. Fragment shaders then store visi­bility data inside this region using 
a linked list structure. We implemented PALL in OpenGL. The reduced contention in PALL increases performance: 
when rendering 1.4 * 106 fragments, PALL uses 4.95 ms per frame while PPLL uses 5.36 ms. Our sup­plementary 
material includes performance metrics that show PALL scales more ef.ciently than PPLL as total fragment 
count increases. However, PALL s conservative primitive bound increases memory usage. 3 The Compact 
A-Buffer Existing interactive A-buffers store shading and visibility inside fragment lists. This saves 
per-primitive shading data repeatedly in multiple pixels. Decoupling storage of primitive and fragment 
data in our new compact A-buffer signi.cantly reduces memory over­head. This approach resembles the decoupling 
proposed by Liktor and Dachsbacher s [2012] compact G-buffer. *e-mail:ethan-kerzner@uiowa.edu Our compact 
A-buffer applies to either linked lists or the S-buffer. In both cases, rather than replicate per-primitive 
data in every frag­ment, we store a primitive ID with each fragment and create a single buffer of primitive 
data. When accumulating pixel color, we access this primitive data to shade each fragment. Although this 
adds a layer of indirection to shading computations, decoupling provides signi.cant memory savings with 
only minor performance impact. Memory consumption still scales linearly with fragment count, but with 
a strictly lower constant factor as shown in Figure 1.  Figure 1: Memory usage of regular and compact 
A-buffers, com­puted at 10242 resolution with various fragment counts. When primitive count exceeds fragment 
count, our compact A-buffer has a larger memory footprint. However, our compact A-buffer scales more 
ef.ciently as average primitive size increases. 4 Conclusion We introduced two GPU-based A-buffer optimizations. 
The primitive-allocated linked lists reduces global fragment shader con­tention while computing primitive 
visibility, resulting in faster per­formance but increased memory usage. The compact A-buffer de­couples 
storage of visibility and shading data, reducing memory demands at a small performance cost. Metrics 
and applications of these techniques are included in our supplementary material.  References LIKTOR 
, G., A ND DAC H S BACH ER , C. 2012. Decoupled deferred shading for hardware rasterization. 143 150. 
VA S IL A KI S , A., A ND FUDO S , I. 2012. S-buffer: Sparsity-aware multi-fragment rendering. Eurographics 
Symposium on Render­ing. YA N G , J., HE NSLE Y, J., GRU N, H., A N D THI B IEROZ , N. 2010. Real-time 
concurrent linked list construction on the gpu. Com­puter Graphics Forum 29, 4, 1297 1304. Permission 
to make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2503505</section_id>
		<sort_key>1200</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>2503506</article_id>
		<sort_key>1210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>110</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Flow circle]]></title>
		<subtitle><![CDATA[circular visualization of wiki revision history]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503506</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503506</url>
		<abstract>
			<par><![CDATA[<p>We visualize the Wiki revision history based on a History Flow, which is a visualization tool for a time-sequence of snapshots of a document in various stages of its creation. The previous History Flow was judged to be inadequate in that it did not display the relationships between the authors, so prevents users from analyzing the meaning of the revisions of the data. First, this study introduces the Flow Circle, which is a new exploratory data analysis tool devised to solve such problems of History Flow. Second, this tool is used to actually visualize the Wiki revision history regarding gun politics in order to understand and analyze the flow of the revision history and the relationship and conflict structures between the authors based on the results of the MDS analysis.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190994</person_id>
				<author_profile_id><![CDATA[82458864757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jaeho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rookie88@ajou.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190995</person_id>
				<author_profile_id><![CDATA[82458680257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Dongjin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kim]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[darkmoon@ajou.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190996</person_id>
				<author_profile_id><![CDATA[82458713257]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jaejune]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[qoojj@ajou.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P4190997</person_id>
				<author_profile_id><![CDATA[81458652821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kyungwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kwlee@ajou.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>985765</ref_obj_id>
				<ref_obj_pid>985692</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[F. B. Viegas, M. Wattenberg, and K. Dave. 2004. Studying cooperation and conflict between authors with history flow visualizations. In <i>Proceedings of the SIGCHI conference on Human factors in computing systems(CHI)</i>, ACM Press. New York. 575--582]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Krzywinski, M., Schein, J., Birol, I., Connors, J., Gascoyne, R., Horsman, D., Jones, S. J., and Marra, M. A. 2009. Circos: An Information Aesthetic for Comparative Genomics. Genome Res 19(9):1639--1645.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Flow circle: Circular visualization of wiki revision history Jaeho Lee*, Dongjin Kim , Jaejune Park 
and Kyungwon Lee§ Department of Digital Media, Ajou University *rookie88@ajou.ac.kr, darkmoon@ajou.ac.kr, 
qoojj@ajou.ac.kr, §kwlee@ajou.ac.kr   1. Introduction We visualize the Wiki revision history based 
on a History Flow, which is a visualization tool for a time-sequence of snapshots of a document in various 
stages of its creation. The previous History Flow was judged to be inadequate in that it did not display 
the relationships between the authors, so prevents users from analyzing the meaning of the revisions 
of the data. First, this study introduces the Flow Circle, which is a new exploratory data analysis tool 
devised to solve such problems of History Flow. Second, this tool is used to actually visualize the Wiki 
revision history regarding gun politics in order to understand and analyze the flow of the revision history 
and the relationship and conflict structures between the authors based on the results of the MDS analysis. 
 2. Approach We attempted to integrate the previous History Flow with an MDS graph. This graph allowed 
us to analyze the meaning index of the writ­ings and gave us a Circos visualization, which displayed 
the relation­ships between the authors. In this regard, we believed that changing the linear layout of 
the Flow View into a circular layout facilitated integra­tion. As a result, we perfectly integrated 3 
types of visualizations and created a visually beautiful and functional Flow Circle(Figure 2). The Revision 
Line in the Flow Circle is arranged according to the passage of time in a clockwise direction, rather 
than flowing accord- ing to the X axis. Flow view connects corresponding segments on the lines representing 
versions. This arrangement method allows users to view the first version and the last version of the 
revisions that were made, making it easier to analyze the text changes at a glance. We se­lected the 
MDS graph in order to present the meanings of each of the text. The MDS graph is composed of keywords 
individually extracted from the Revision Data of various versions. Also, the conflicting terms of the 
keywords arranged in the MDS graph are presented with con­flicting colors. For this reason, the colors 
become whiter towards the center in order to display neutrality. The established colors of the key­words 
are engaged one-on-one with the colors of each Flow View, which makes it easier for users to comprehend 
the meanings and char­acters of the text simply by looking at the Flow View based on its col­ors. The 
node of each Revision Line presents the author of each ver­sion, and the edges that connect the two nodes 
together display the editing relationships regarding the deletion of the previous version text from the 
later version. The Flow Circle can express a larger amount of data in the same space when compared to 
the History Flow. And distor­tions are not presented in the circular layout, since the part that actually 
displays the amount of data in the Flow Circle is the spacing within each revision line. 3. Results 
and Conclusions The previous History Flow did not analyze the meaning of the texts and the indicators 
of the relationship between the authors. For this reason, it was unsuitable in analyzing the conflict 
relationships be-tween authors and in analyzing the reasons for such conflicts. Another form of visuali­zation, 
other than the existing Flow View, must be combined in order to solve this problem. The straight-line 
layout of the History Flow makes it difficult to integrate with other visualizations. The Flow Circle 
changes this layout into a circular form to successfully integrate the Circos and MDS in the existing 
view, while also maintaining the advan­tages of the previous flow. These 3 Views become organically inte­grated 
with one another. For this reason, the space can be efficiently used to present a large amount of data 
at a single time. 4. References F. B. Viegas, M. Wattenberg, and K. Dave. 2004. Studying cooperation 
and conflict between authors with history .ow visualizations. In Pro­ceedings of the SIGCHI conference 
on Human factors in computing systems(CHI), ACM Press. New York. 575 582 Krzywinski, M., Schein, J., 
Birol, I., Connors, J., Gascoyne, R., Hors­man, D., Jones, S. J., and Marra, M. A. 2009. Circos: An Information 
Aesthetic for Comparative Genomics. Genome Res 19(9):1639 1645. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use isgranted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503507</article_id>
		<sort_key>1220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>111</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[GPU based graph bundling using geographic reference information]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503507</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503507</url>
		<abstract>
			<par><![CDATA[<p>A common source of challenges in geographic visualisation systems is the growing amount of available data. Datasets containing different information need to be merged and analysed under a common context. For some purposes these datasets can interact intelligently and append additional information to a scientific visualisation. In this research proposal, we show how we are planning to optimize the process of bundling graphs to traffic networks and present a general method to solve this challenge.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4190998</person_id>
				<author_profile_id><![CDATA[82459168857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Matthias]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Th&#246;ny]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mthoeny@ifi.uzh.ch]]></email_address>
			</au>
			<au>
				<person_id>P4190999</person_id>
				<author_profile_id><![CDATA[81100087869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Renato]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pajarola]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pajarola@ifi.uzh.ch]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2421942</ref_obj_id>
				<ref_obj_pid>2421899</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Holten, D., and van Wijk, J. J. 2009. Force-directed edge bundling for graph visualization. <i>Computer Graphics Forum 28</i>, 3, 983--990.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1187772</ref_obj_id>
				<ref_obj_pid>1187627</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Holten, D. 2006. Hierarchical edge bundles: Visualization of adjacency relations in hierarchical data. <i>IEEE Transactions on Visualization and Computer Graphics 12</i>, 5 (sept.-oct.), 741--748.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2421848</ref_obj_id>
				<ref_obj_pid>2421836</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lambert, A., Bourqui, R., and Auber, D. 2010. Winding roads: Routing edges into bundles. <i>Computer Graphics Forum 29</i>, 3, 853--862.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GPU Based Graph Bundling Using Geographic Reference Information Matthias Th ¨ony * Renato Pajarola 
 University of Z urich¨University of Z ¨ urich  Figure 1: The Swiss commuter dataset (green), containing 
31609 lines and 2886 vertices, shows the commuters within Swiss communes. The underlying geographic reference 
data shows the Swiss railroad network (left) and the Swiss VECTOR25 street map (right) 1 Introduction 
A common source of challenges in geographic visualisation sys­tems is the growing amount of available 
data. Datasets contain­ing different information need to be merged and analysed under a common context. 
For some purposes these datasets can interact intelligently and append additional information to a scienti.c 
visu­alisation. In this research proposal, we show how we are planning to optimize the process of bundling 
graphs to traf.c networks and present a general method to solve this challenge. Bundling of graph information 
is a common problem in informa­tion visualisation. The approach of bundling a graph according to geographic 
coordinates is described in [Lambert et al. 2010] and shows that much more visual information could be 
extracted from already available datasets. The force directed graph bundling de­scribed in [Holten and 
van Wijk 2009] uses force.elds to attract subdivided lines to each other and will be used to compare 
the vi­sual results of the .nal product. Geometry-based graph bundling [Holten 2006] uses spatial structuring 
to improve the performance in graph bundling and was an inspiration for using spatial data structures 
within geograhic information. The following set of requirements describes the research target of our 
graph bundling method. First, to incorporate a feature within an existing 3D visualisation that is able 
to link two datasets, one con­taining point to point information and one containing a reference network, 
so that the point to point data is bundled to streams ac­cording to the reference information. Second, 
to reduce remaining clutter in the resulting images so that it is possible to clearly iden­tify these 
streams. Finally, to allow the user to be able to identify single features within a single dataset. Figure 
1 is an example for the need of clutter reduction. It is almost impossible to extract detailed visual 
information about commuter­streams from these images. Bundling the streams will lead to a visualisation, 
which makes it easier to read the connections. How­ever, there are other hidden aspects, e.g. which roads 
the commuters use most. Furthermore, it is necessary to be certain that datasets are not crossing through 
other scene objects. *e-mail:mthoeny@i..uzh.ch e-mail:pajarola@i..uzh.ch 2 Our Approach In our approach, 
bundles are streamed according to a reference traf­.c network, like roads or railroads. The result is 
the visual feedback on how many commuters in.uence the traf.c on any single road. In addition, the streamlines 
are speci.ed to follow more realistic paths. As a .rst step, the traf.c network is processed to a graph 
network, which contains street crossings and simple representation of lines. The traf.c network can be 
simpli.ed, because only the adjacency information is needed. As a second step, the information about 
where a commuterline starts and ends is extracted. For this purpose we take the graph from the traf.c 
network, build a KD-tree based on these points and search within this KD-tree the nearest graph node 
for all start and end points of the commuterlines. Using this information, we can perform a shortest 
path search for every commuterline based on the simpli.ed reference graph. The resulting path is connected 
with the start and endpoints of a single commuterline and describes the path of one single commuterline. 
An advantage of this processing method over other graph bundling approaches is, that until this point 
the information remains in 3D and is not rasterized, and can be used for further processing. It is also 
possible to use the resulting path as basis for a spline rendering or other curvature representations. 
 References HO LT EN , D., AND VA N WIJK , J. J. 2009. Force-directed edge bundling for graph visualization. 
Computer Graphics Forum 28, 3, 983 990. HO LT E N , D. 2006. Hierarchical edge bundles: Visualization 
of adjacency relations in hierarchical data. IEEE Transactions on Visualization and Computer Graphics 
12, 5 (sept.-oct.), 741 748. LA M B ERT, A., BOUR QU I, R., A ND AU B E R, D. 2010. Winding roads: Routing 
edges into bundles. Computer Graphics Forum 29, 3, 853 862. Permission to make digital or hard copies 
of part or all of this work for personal or classroom use isgranted without fee provided that copies 
are not made or distributed for commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for third-party components of thiswork must be honored. For all other uses, 
contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by 
the Owner/Author.ACM 978-1-4503-2261-4/13/07 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503508</article_id>
		<sort_key>1230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>112</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Visualizing attractive periods of popular photo spots using Flickr data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503508</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503508</url>
		<abstract>
			<par><![CDATA[<p>The popularity of cameras with GPS receivers and the emergence of photo-sharing websites such as Flickr have created enormous collections of photos that are annotated with GPS locations, time-stamps, photographers, etc. Thus, one can obtain a large number of observations of where and when people take photos. Since people tend to take photos when they meet visually interesting things on their sightseeing tours, attention has recently been devoted to constructing sightseeing guide systems that exploit the information revealed by the collective behavior of users in photo-sharing websites (see, e.g., [Lu et al. 2010]). Previous work [Crandall et al. 2009] discovered popular photo spots from a large number of geo-tagged photos, and visualized them with the found representative images on maps.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4191000</person_id>
				<author_profile_id><![CDATA[82458851757]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwabuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ryukoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[t12m014@mail.ryukoku.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4191001</person_id>
				<author_profile_id><![CDATA[81100048039]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masahito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kumano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ryukoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kumano@rins.ryukoku.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4191002</person_id>
				<author_profile_id><![CDATA[82459319557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Motonori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Koseki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ryukoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[t12m023@mail.ryukoku.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4191003</person_id>
				<author_profile_id><![CDATA[82458955857]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Keiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ono]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ryukoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kono@rins.ryukoku.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P4191004</person_id>
				<author_profile_id><![CDATA[81100106230]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Masahiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ryukoku University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kimura@rins.ryukoku.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>513076</ref_obj_id>
				<ref_obj_pid>513073</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Comaniciu, D., and Meer, P. 2002. Mean shift: a robust approach toward feature space analysis. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence 24</i>, 603--619.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1526812</ref_obj_id>
				<ref_obj_pid>1526709</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Crandall, D., Backstrom, L., Huttenlocner, D., and Kleinberg, J. 2009. Mapping the world's photos. In <i>Proceedings of the 18th International World Wide Web Conference (WWW'09)</i>, 761--770.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1873972</ref_obj_id>
				<ref_obj_pid>1873951</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lu, X., Wang, C., Yang, J., Pang, Y., and Zhang, L. 2010. Photo2trip: generating travel routes from geo-tagged photos for trip planning. In <i>Proc. of MM'10</i>, 143--152.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing Attractive Periods of Popular Photo Spots Using Flickr Data Satoshi Iwabuchi, Masahito Kumano, 
Motonori Koseki, Keiko Ono, and Masahiro Kimura Ryukoku University*  50 100 150 200 250 300 350 Day 
 (a) Result of the season detection for spot Kiyomizu (b) Visualization result by the proposed method 
(Kyoto) Figure 1: Results for Japanese Flickr data. 1 Introduction The popularity of cameras with GPS 
receivers and the emergence of photo-sharing websites such as Flickr have created enormous collections 
of photos that are annotated with GPS locations, time­stamps, photographers, etc. Thus, one can obtain 
a large number of observations of where and when people take photos. Since peo­ple tend to take photos 
when they meet visually interesting things on their sightseeing tours, attention has recently been devoted 
to constructing sightseeing guide systems that exploit the information revealed by the collective behavior 
of users in photo-sharing web­sites (see, e.g., [Lu et al. 2010]). Previous work [Crandall et al. 2009] 
discovered popular photo spots from a large number of geo­tagged photos, and visualized them with the 
found representative images on maps. A photo spot represents a spatially localized region, and can have 
its own proper collection of characteristic periods with respect to the number of visitors (see Figure 
1(a)), where we refer to each charac­teristic period as a season of the spot. As a candidate for the 
attrac­tive period of a spot, we focus on its burst season such that it has many visitors within a relatively 
short period. By e.ectively visual­izing burst seasons, we aim to increase the sophistication of a sight­seeing 
guide system based on collective wisdom. To this end, we deal with a large collection of geo-tagged and 
time-stamped pho­tos that are taken within a given year, detect the seasons for each of the popular photo 
spots extracted, and develop a novel visualization system that can e.ectively analyze and compare the 
spots in terms of burstiness. In this research, we propose such a visual analysis method that simultaneously 
visualizes 1) how many seasons each spot has, 2) when each season is, 3) to what extent each season is 
bursty, 4) how many visitors each spot has in the year, and 5) what spatial relationship the spots have. 
 2 Approach Let D0 be a set of geo-tagged and time-stamped photos that are taken within a speci.ed year. 
In order to absorb the wide variabil­ity in photo-taking behavior across di.erent individuals, we .rst 
construct a dataset of photos for our analysis, D1 = {dn}, by buck­eting the geographic locations (lat-long 
values) and time-stamps of photos in D0, and sampling a single photo from each bucket for each photographer. 
Let xn and tn denote the geographic location and time-stamp of photo dn, respectively. Next, we extract 
a set of popular photo spots {R1,...,RK } by applying mean shift cluster­ing to the set {xn} according 
to [Crandall et al. 2009], where each Rk is the minimal rectangular region including some cluster ob­tained 
from {xn}. Moreover, in order to detect the seasons for each spot Rk, we partition the whole period (the 
year) J into Mk sub­periods Jk,1,...,Jk,Mk by applying mean shift clustering to the set {tn | xn .Rk}. 
For mean shift clustering in time-domain, we use the variable bandwidth mean shift [Comaniciu and Meer 
2002]to determine the kernel bandwidth for each data point adaptively. We now present a visual analysis 
method satisfying our aim. First, we visualize each spot Rk as a rectangular prism called a base that 
is located on a map, where its height indicates the number Vk of visitors to Rk in the whole period J. 
We measure the burst degree of each season Jk,m for Rk as Bk,m = (Vk,m/Vk)/(|Jk,m|/|J |), where Vk,m 
is the number of visitors to Rk in Jk,m. Next, in order to ana­lyze seasonal characteristics of each 
spot Rk in terms of burstiness, we visualize each season Jk,m of Rk as a cylinder called a .ber that 
is put on the base of Rk, where its height indicates Bk,m and its col­ors show when the period Jk,m is. 
Here, by continuously changing hue on the basis of the HSV color model, we specify the color of an arbitrary 
day in the year. Using the Japanese Flickr data in 2010, we have examined the e.ec­tiveness of the proposed 
method. Figure 1 shows the visualization result. We can easily observe how the burstiness di.er according 
to the popular photo spots extracted, and discover the seasons of high burst degrees and the spots having 
them (see Figure 1(b)). We have also con.rmed that the burst seasons discovered are really attractive 
periods by investigating the corresponding photos in detail.  References Comaniciu, D., and Meer, P. 
2002. Mean shift: a robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis 
and Machine Intelligence 24, 603 619. Crandall, D., Backstrom, L., Huttenlocner, D., and Kleinberg,J. 
2009. Mapping the world s photos. In Proceedings of the 18th International World Wide Web Conference 
(WWW 09), 761 770. Lu, X., Wang, C., Yang, J., Pang,Y., and Zhang, L. 2010. Photo2trip: generating travel 
routes from geo-tagged photos for trip planning. In Proc. of MM 10, 143 152. *e-mail: {t12m014, t12m023}@mail.ryukoku.ac.jp, 
{kumano, kono, kimura}@rins.ryukoku.ac.jp Permission to make digital or hard copies of part or all of 
this work for personal or classroom use isgranted without fee provided that copies are not made or distributed 
for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights 
for third-party components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 
2013, July 21 25, 2013, Anaheim, California. 2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503509</article_id>
		<sort_key>1240</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>113</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Visualizing scientific graphics for astronomy and astrophysics]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503509</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503509</url>
		<abstract>
			<par><![CDATA[<p>The combination of astronomy and computational sciences plays an important role in how members of the astronomical community visualize their data. Larger detectors in the optical/IR, increasing bandwidth in radio interferometers, and large N astrophysical simulations drive the need for visualization solutions. These visualization purposes include exploring the dynamical phase space of data cubes, surface mapping, large catalogs, and volumetric rendering. The use of 3D computer graphics in the movie, television, and gaming industry has led the development of useful computer algorithms for optimizing the display of complex data structures while taking advantage of new hardware paradigms like GPU processing. Melding the exciting scientific results with state of the art computer graphics cannot only help with scientific analysis and phase space discovery, but also with graphics for education and public outreach. The visual impact of astronomy cannot be understated - for both the community of scientists and how to present to the public with higher accuracy our results.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4191005</person_id>
				<author_profile_id><![CDATA[82458840457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[R.]]></middle_name>
				<last_name><![CDATA[Kent]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Radio Astronomy Observatory (NRAO)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bkent@nrao.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Christensen, P. R., et al. 2001, J. of Geophys. Rsrch-Planets, 23823]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Durech, J., Sidorin, V., et al. 2010, Astr. & Aphys, 513, A46]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2222160</ref_obj_id>
				<ref_obj_pid>2221977</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Farouki, R. T. 2012, Comput. Aided Geom. Des., 29, 379]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Springel, V. 2005, MNRAS, 364, 1105, 1105]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Tully, R. B., Rizzi, L., et al. 2009, AJ, 138, 323]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Walter, F., Brinks, E., de Blok, W.., et al. 2008, AJ, 136, 2563]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing Scientific Graphics for Astronomy and Astrophysics Figure 1: N-­body simulations of colliding 
galaxies, planetary surface maps, and the NRAO Very Large Array 1. Introduction The combination of astronomy 
and computational sciences plays an important role in how members of the astronomical community visualize 
their data. Larger detectors in the optical/IR, increasing bandwidth in radio interferometers, and large 
N astrophysical simulations drive the need for visualization solutions. These visualization purposes 
include exploring the dynamical phase space of data cubes, surface mapping, large catalogs, and volumetric 
rendering. The use of 3D computer graphics in the movie, television, and gaming industry has led the 
development of useful computer algorithms for optimizing the display of complex data structures while 
taking advantage of new hardware paradigms like GPU processing. Melding the exciting scientific results 
with state of the art computer graphics cannot only help with scientific analysis and phase space discovery, 
but also with graphics for education and public outreach. The visual impact of astronomy cannot be understated 
-­for both the community of scientists and how to present to the public with higher accuracy our results. 
We present a practical overview for astronomers to the capabilities of the 3D graphics program Blender. 
We take the approach of describing the features that will be of interest to various parties in astronomy 
-­for both research visualization and presentation graphics. We aim to describe methods of data import, 
modeling, texturing, lighting, rendering, and compositing for astronomy.  2. Visualizing the Universe 
Data cubes and galaxy dynamics Data obtained with the NRAO Very Large Array radio telescope (Figure 
1) allows astronomers to study the dynamics of galaxies in 3D. The data cube of a galaxy shown in Figure 
2 is rendered by mapping the Doppler shifted frequency of neutral hydrogen. This allows astronomers to 
measure both the amount of gas and the rotation of the galaxy [Walter et al. 2008]. N-­body simulations 
 This example of a galaxy simulation was generated with GADGET-­2 [Springel 2005]. Each large spiral 
disk galaxy has 10000 disk particles and 20000 halo particles with Milky Way scale lengths and masses. 
The simulation is run for approximately 1100 timesteps for a total simulation runtime of 2 billion years. 
We read in the particle x, y, z coordinates as single vertex with a small Gaussian halo as its texture. 
The snapshot file for each timestep is keyframed as one frame in the animation. In addition, a Bézier 
curve can be added to the scene as an object path [Farouki 2012]. The camera can then be flown along 
the curve as the galaxy interaction progresses (Figure 1). Planetary surfaces We explore the utility 
of UV mapping -­taking astronomical surface map projections onto 3D objects for display and analysis. 
The examples that will be shown are from the Mars Global Surveyor TES [Christensen et al. 2001]. Each 
map file is given in a simple cylindrical projection. We use a high resolution UV-­sphere mesh with 2048 
surface polygons, which gives a good balance between resolution and rendering time (Figure 1). References 
 Christensen, P.R., et al. 2001, J. of Geophys. Rsrch-­Planets, 23823 Durech, J., Sidorin, V., et al. 
2010, Astr. &#38; Aphys, 513, A46 Farouki, R.T. 2012, Comput. Aided Geom. Des., 29, 379 Springel, V. 
2005, MNRAS, 364, 1105, 1105 Tully, R.B., Rizzi, L., et al. 2009, AJ, 138, 323 Walter, F., Brinks, E., 
de Blok, W.., et al. 2008, AJ, 136, 2563 1 http://www.cv.nrao.edu/~bkent/ bkent@nrao.edu Permission to 
make digital or hard copies of part or all of this work for personal or classroom use isgranted without 
fee provided that copies are not made or distributed for commercial advantage and that copies bear this 
notice and the full citation on the first page. Copyrights for third-party components of thiswork must 
be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 
2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2503510</article_id>
		<sort_key>1250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>114</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Visualizing the flow of users on a wireless network]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503510</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503510</url>
		<abstract>
			<par><![CDATA[<p>This information visualization project depicts the flow of users within the various access points of the eduroam network of the University of Coimbra. The developed application allows the visualization of an interactive graph (see Figure 1), which covers a certain time span, and animates the changes in the network's flow of information over time. The nodes of the graph represent access points and the links represent transitions of users between them. A transition is registered when a user stops the connection to an access point and connects to another one. Clustering is used to merge access points that are deeply connected, promoting the visual clarity of the artifact while providing additional information to the viewer.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4191006</person_id>
				<author_profile_id><![CDATA[82458942057]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ruslan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamolov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ruslan@student.dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4191007</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4191008</person_id>
				<author_profile_id><![CDATA[81318497551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pmcruz@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>857683</ref_obj_id>
				<ref_obj_pid>857190</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Stasko, J. and Zhang, E., 2000. Focus+context display and navigation techniques for enhancing radial, space-filling hierarchy visualizations. <i>IEEE Symposium on Information Visualization</i>, IEEE, pp. 57--65.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing the Flow of Users on a Wireless Network Ruslan Kamolov1, Penousal Machado2 and Pedro Cruz3 
CISUC, Department of Informatics Engineering, University of Coimbra  This information visualization 
project depicts the .ow of userswithin the various access points of the eduroam network of theUniversity 
of Coimbra. The developed application allows thevisualization of an interactive graph (see Figure 1), 
which coversa certain time span, and animates the changes in the network s.ow of information over time. 
The nodes of the graph representaccess points and the links represent transitions of users between them. 
A transition is registered when a user stops the connection to an access point and connects to another 
one. Clustering is used to merge access points that are deeply connected, promoting thevisual clarity 
of the artifact while providing additional informa­tion to the viewer. 2 Approach Figure 2 Detail of 
expanded and collapsed clusters, and therespective incoming and outgoing transitions. We adopt a circular 
layout inspired in sunburst diagrams (Stasko and Zhang, 2000), applying it to the visualization of wireless 
net­works with a new interactive technique to explore the network sstructure on a local and global level. 
The access points (APs) arenodes represented by arc blocks along the main circle. The length of a block 
is proportional to the sum of all its incoming and outgo­ing transitions, the thickness of a node is 
proportional to the num­ber of its children. Each node has a color that provides the direc­ 1 ruslan@student.dei.uc.pt 
2 machado@dei.uc.pt 3 pmcruz@dei.uc.pt tion to its transitions, since the outgoing transitions inherit 
thesource node s color. The transitions are depicted by Bézier curves, thinner on bends and larger on 
extremities. The width of an ex­tremity corresponds to the total number of transitions between thesource 
and the target nodes. The incoming and outgoing transi­tions of a node are organized by width in descending 
order, fromthe center to the right and left margins of a node. The transitionswithin the same node or 
cluster are drawn outside of the main circle (see Figure 2). In the absence of spatial information aboutthe 
localizations of APs, the implemented clustering algorithmgroups nodes by proximity, using the average 
transition time as­sociated between pairs of nodes. First, the most accessed point ischosen, becoming 
the central node of the new cluster. Then, allnodes reachable within a timeframe of 60 seconds are recursively 
added to the cluster. Cluster s children inherit the color of the father cluster with a small variation 
in saturation. To promoteclarity and ef.ciency, we provide a .ltering technique that deletesrarely accessed 
APs and rarely occurred transitions, below a given threshold. The processed data is mapped to the graphic 
layout, which con­sists of nodes forming a circumference, with transitions curvesconnecting them. The 
following types of transitions are repre­sented: cluster-cluster, cluster-node, node-cluster, and node-node. 
The novel interactive technique allows users to expand or collapseany cluster, allowing them to control 
the level of detail of thevisualization. When a cluster is collapsed, the transitions of all itsnodes 
are represented by the cluster. When expanded, one can observe the incoming and outgoing transitions 
of the individualnodes represented at the cluster s position. Pointing with themouse cursor over a node 
shows its ID number and its technical name. Users may turn on all node IDs at once, expand top 5 big­gest 
clusters and .lter the displayed data using the respectivecontrollers included in the interface. The 
dynamic artifact wasproduced by considering a sliding time-frame of one week, which moves in 3 hour increments. 
The animation reveals how the net­work s structure and connectivity changes over time. 3 Experimental 
Results The interactive visualization and the animations represent theoverall user movement within the 
network, allowing one to per­ceive the .ow of users and common patterns of movement. Thegraph emphasizes 
the most common movement patterns in thenetwork due to the edges thickness. The most accessed points 
arealso highlighted due to their bigger arc lengths. The network structure is revealed by the clusters 
grouping factor and the nodearcs thickness. Transitions between the same node/cluster prevailover the 
transitions between distant nodes, which occur less often. Acknowledgements This research is partially 
funded by the Portuguese Foundation for Science and Technology, project SUM with reference PTDC/EIA­EIA/113933/2009. 
 References STASKO, J. AND ZHANG, E., 2000. Focus+context display and navigation techniques for enhancing 
radial, space-.lling hier­archy visualizations. IEEE Symposium on Information Visu­alization, IEEE, pp. 
57 65. Permission to make digital or hard copies of part or all of this work for personal or classroom 
use is granted without fee provided that copies are not made or distributed for commercial advantage 
and that copies bear this notice and the full citation on the first page. Copyrights for third-party 
components of thiswork must be honored. For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 
21 25, 2013, Anaheim, California.2013 Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Portuguese Foundation for Science and Technology, project SUM</funding_agency>
			<grant_numbers>
				<grant_number>PTDC/EIAEIA/113933/2009</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2503511</article_id>
		<sort_key>1260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>115</display_no>
		<article_publication_date>07-21-2013</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Visualizing urban mobility]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2503385.2503511</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2503511</url>
		<abstract>
			<par><![CDATA[<p>The goal of this research is understanding urban mobility through the visualization of the use of public transport systems. We focus on the visualization of anomalies regarding the number of passengers. To find patterns of use we analyze the raw data, which contains people counts for every bus stop in Coimbra. For each stop, and for each day of the week, we calculate the average number of passengers and its standard deviation for each 30 minute interval. This allows us to identify situations that deviate from the norm.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node/>
				<descriptor/>
				<type/>
			</primary_category>
		</categories>
		<authors>
			<au>
				<person_id>P4191009</person_id>
				<author_profile_id><![CDATA[82458954457]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Evgheni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Polisciuc]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[evgheni@student.dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4191010</person_id>
				<author_profile_id><![CDATA[81436599594]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Alves]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ana@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4191011</person_id>
				<author_profile_id><![CDATA[81100145277]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Carlos]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bento]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bento@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P4191012</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>357310</ref_obj_id>
				<ref_obj_pid>357306</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blinn, J. F. 1982. A Generalization of Algebraic Surface Drawing. ACM Transactions on Graphics 1 (3), 235--256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing Urban Mobility Evgheni Polisciuc1, Ana Alves2, Carlos Bento3 and Penousal Machado4 CISUC, 
Department of Informatics Engineering, University of Coimbra  Figure 1 Visualizations of the urban mobility 
created by applying the Metaball technique to colorize the pixels, the two images on the left, and to 
colorize the vertices of the map with global view, the image on the right. 1 Introduction The goal of 
this research is understanding urban mobility through the visualization of the use of public transport 
systems. We focus on the visualization of anomalies regarding the number of passengers. To find patterns 
of use we analyze the raw data, which contains people counts for every bus stop in Coimbra. For each 
stop, and for each day of the week, we calculate the average number of passen­gers and its standard deviation 
for each 30 minute interval. This allows us to identify situations that deviate from the norm. To produce 
the visual artifacts we rely on the Metaballs [Blinn, 1982] technique. To improve performance the data 
is pushed to video card and the output calculated using fragment and vertex shaders written in GLSL. 
Our representation of urban dynamics is embod­ied by two visualization models. The first represents the 
deviations and the city map on separate layers; The second embeds the devia­tions into the vectors of 
the city map. These complex and computa­tionally heavy representations simplify the identification of 
patterns in large quantities of data. To further promote visibility, we resort to exaggeration, which 
allows the visualization of small details, which would be invisible by direct mapping. 2 Implementation 
The representation of the patterns in urban mobility is based on Metaball technique. We generate an isosurface 
where the position of each charge point is the GPS position transformed to the screen coordinates, and 
the force is the absolute value of the deviation. A positive deviation, i.e. an abnormally high number 
of passengers, is represented in red, while a negative deviation is represented in green. Forces superior 
to 1 are represented with an alpha compo­nent of 0.8, forces inferior to 1 and superior to 0.5 are represented 
with an alpha component of 0.5. The analysis of the global view of the city, presented in figure 1, and 
of the zoomed area, presented in figure 2, reveals the justification for using a two-band threshold. 
As it can be observed, forces superior to 1 tend to result in circles since the influence of other charge 
points tends to be negligible by comparison. This contrasts with the areas of transparent green and red 
generated by forces in the [0.5,1] interval, which assume a more organic nature filling in the gaps among 
strong forces and highlighting areas where anomalies are occurring. When we zoom in, the exaggerations 
caused by the representation of these small forces are reduced and the visualization becomes more rigorous. 
1 evgheni@student.dei.uc.pt 2 ana@dei.uc.pt 3 bento@dei.uc.pt 4 machado@dei.uc.pt Permission to make 
digital or hard copies of part or all of this work for personal or classroom use isgranted without fee 
provided that copies are not made or distributed for commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for third-party components of thiswork must be honored. 
For all other uses, contact the Owner/Author.SIGGRAPH 2013, July 21 25, 2013, Anaheim, California. 2013 
Copyright held by the Owner/Author.ACM 978-1-4503-2261-4/13/07 Figure 2 Detail of visualization where 
we can see the behavior of Metaball tech­nique with two classes of charge points. The second approach 
is based on the same technique. The differ­ence is that the output of the function is applied directly 
to the shape vertices of the map. First, the road map is retrieved from OpenStreetMap and unnecessary 
objects are filtered. Then, the Metaball technique is applied to determine the color for each vertex 
of the map (figure 3). Although, this approach is less computationally expensive, in our opinion, the 
visual output is not as clear and informative. Figure 3 Detail of visualization with Metaball applied 
over vertices of the map. The outcome of this process is an interactive visualization applica­tion with 
two models of visualization, one presents a clear and informative, albeit exaggerated, view of the anomalies, 
while the other sacrifices visibility for the sake of rigor. Acknowledgements This research is partially 
funded by the Portuguese Foundation for Science and Technology (FCT), project CROWDS PTDC/EIA-EIA/ 115014/2009, 
and by CISUC, financed by FEDER funds via POFC COMPETE and by FCT, project FCOMP-01-0124-FEDER-022703. 
 References Blinn, J. F. 1982. A Generalization of Algebraic Surface Drawing. ACM Transactions on Graphics 
1 (3), 235 256.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>FCT</funding_agency>
			<grant_numbers>
				<grant_number>FCOMP-01-0124-FEDER-022703</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>CISUC</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Portuguese Foundation for Science and Technology (FCT), project CROWDS</funding_agency>
			<grant_numbers>
				<grant_number>PTDC/EIA-EIA/115014/2009</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>FEDER funds via POFC - COMPETE</funding_agency>
			<grant_numbers>
				<grant_number></grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	</section>
</content>
</proceeding>
