<?xml version="1.0" encoding="ISO-8859-1"?>
<!DOCTYPE proceeding SYSTEM "proceeding.dtd">
<proceeding ver="6.0" ts="04/10/2010">
<conference_rec>
	<conference_date>
		<start_date>08/07/2011</start_date>
		<end_date>08/11/2011</end_date>
	</conference_date>
	<conference_loc>
		<city><![CDATA[Vancouver]]></city>
		<state>British Columbia</state>
		<country>Canada</country>
	</conference_loc>
	<conference_url></conference_url>
</conference_rec>
<series_rec>
	<series_name>
		<series_id>SERIES382</series_id>
		<series_title><![CDATA[International Conference on Computer Graphics and Interactive Techniques]]></series_title>
		<series_vol></series_vol>
	</series_name>
</series_rec>
<proceeding_rec>
	<proc_id>2037715</proc_id>
	<acronym>SIGGRAPH '11</acronym>
	<proc_desc>ACM SIGGRAPH 2011 Posters</proc_desc>
	<conference_number>2011</conference_number>
	<proc_class>conference</proc_class>
	<proc_title></proc_title>
	<proc_subtitle></proc_subtitle>
	<proc_volume_no></proc_volume_no>
	<isbn13>978-1-4503-0971-4</isbn13>
	<issn></issn>
	<eissn></eissn>
	<copyright_year>2011</copyright_year>
	<publication_date>08-07-2011</publication_date>
	<pages>92</pages>
	<plus_pages></plus_pages>
	<price><![CDATA[]]></price>
	<other_source></other_source>
	<abstract>
		<par><![CDATA[<p>Posters are a light-weight, low-tech method for presenting student, in-progress, and late-breaking work. Poster topics range from applications of computer graphics to novel interactive techniques and in-depth research in specific areas. They are displayed throughout the conference for attendees to browse at their leisure, and poster authors meet and discuss their work with attendees during Poster Presentations.</p> <p>Posters also present work submitted to the ACM Student Research Competition (SRC).</p>]]></par>
	</abstract>
	<publisher>
		<publisher_id>PUB27</publisher_id>
		<publisher_code>ACMNY</publisher_code>
		<publisher_name>ACM</publisher_name>
		<publisher_address>2 Penn Plaza, Suite 701</publisher_address>
		<publisher_city>New York</publisher_city>
		<publisher_state>NY</publisher_state>
		<publisher_country>USA</publisher_country>
		<publisher_zip_code>10121-0701</publisher_zip_code>
		<publisher_contact>Bernard Rous</publisher_contact>
		<publisher_phone>212 869-7440</publisher_phone>
		<publisher_isbn_prefix></publisher_isbn_prefix>
		<publisher_url>www.acm.org/publications</publisher_url>
	</publisher>
	<sponsor_rec>
		<sponsor>
			<sponsor_id>SP932</sponsor_id>
			<sponsor_name>ACM Special Interest Group on Computer Graphics and Interactive Techniques</sponsor_name>
			<sponsor_abbr>SIGGRAPH</sponsor_abbr>
		</sponsor>
	</sponsor_rec>
	<categories>
		<primary_category>
			<cat_node/>
			<descriptor/>
			<type/>
		</primary_category>
	</categories>
	<chair_editor>
		<ch_ed>
			<person_id>P2809524</person_id>
			<author_profile_id><![CDATA[81342515703]]></author_profile_id>
			<orcid_id></orcid_id>
			<seq_no>1</seq_no>
			<first_name><![CDATA[Dan]]></first_name>
			<middle_name><![CDATA[]]></middle_name>
			<last_name><![CDATA[Wexler]]></last_name>
			<suffix><![CDATA[]]></suffix>
			<affiliation><![CDATA[]]></affiliation>
			<role><![CDATA[Conference Chair]]></role>
			<email_address><![CDATA[]]></email_address>
		</ch_ed>
	</chair_editor>
</proceeding_rec>
<content>
	<section>
		<section_id>2037716</section_id>
		<sort_key>10</sort_key>
		<section_seq_no>1</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Animation]]></section_title>
		<section_page_from>1</section_page_from>
	<article_rec>
		<article_id>2037717</article_id>
		<sort_key>20</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>1</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A walking motion morphing method based on statistical data of the elderly]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037717</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037717</url>
		<abstract>
			<par><![CDATA[<p>Technologies to create numerous motions from a motion data are sought for in order to increase a variety of motions of 3D characters in virtual space. A number of methods have been proposed to increase the variation of walking of people. They consider physical geometric attributes such as standing height and seating height, and motion attributes, but do not consider the postural changes with aging.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809476</person_id>
				<author_profile_id><![CDATA[81474687456]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Katsuhisa]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanazawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University and Tokyo Healthcare University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kanazawa@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809477</person_id>
				<author_profile_id><![CDATA[81488663634]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takafumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[arai@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809478</person_id>
				<author_profile_id><![CDATA[81331499700]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomoaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Moriya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[moriya@vcl.im.dendai.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809479</person_id>
				<author_profile_id><![CDATA[81319502327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tokiichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takahashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Denki University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[toki@vcl.im.dendai.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwaya, T., et al. "The effects of low back pain on physical and social function in elderly people living in rural area", <i>Journal of the Japanese Society of Lumbar Spine Disorders</i>, Vol.11, pp27--34 (2005).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Himann, JE, et al.: "Age-related changes in speed of walking", <i>Med. Sci. Sports Exec.</i>, Vol.20, pp.161--166 (1988).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 AWalkingMotionStatistic MorphingMethodBasedonalDataoftheElderly Katsuhisaee e ee e Kanazawa TakafumiArai 
TomoakiMoriya TokiichiroTakahashi e e TokyoHealthcareUni TokyoDenkiUniversitye versity{kanazawa,moriya,toki}@vcl.im.dendai.ac.jp 
arai, (a)t Originalmotion (b)t Flatbacktype (c)t Roundbacktype (d)t Swaybacktype Fig.1Morphedourmethod. 
motionsby 1.Introductiond Technologiestocreatenumerousmotionsfromamoti ondataaree soughte forine ordere 
toe increaseae varietyofe motion se of3De characterse invirtualspace.e Ae numberofe methodsha vee beene 
proposede toincreasee thevariatione ofe walkingofpe ople.e Theye considerphysicalgeometricattributessuchasstan 
dingheightande seatinge height,e andmotionattributes,butdoe note c onsidere thee postural changeswithaging.Thise 
papere proposese thee methode toe automaticallye gen eratee variationse ofwalkinge e thee postur ale 
changese (b)Rounde motionconsidering(a)Flate (c)Swaye withonthemedicalstatisticaldata. agingbasedbacke 
backe backe e 2.RelatedWorkd Fig.2 Categorizationofposturese e Fig.3 Anglesofspine. oftheelderly. e (1)PostureModificationwithAging 
t Iwayae etal.showedthreee typesposturesofe elderly e peopleine Fig.2:Flatbacktype,Roundbacktype,Swaybacke 
type.Further,e Table1 SagittalSpineAlignment.(unit:degree)theymeasuredthemotionrangeofspecificjointan 
glesof50e Flat backRound backSwayback CWA 26.9±15.2 52.8±12.4 27.5±12.1 elderlypeopleand1.CW A,andshowedtheresultinTableLWA, 
STAaretheanglestodescribetheshapeofspinea sshownine LWA 11.0±17.2 42.2±16.7 20.6±19.2 Fig.3.e STA 9.6±7.4 
23.7±7.1 19.7±13.8 e e (2)ShortStepswithAging t 4.Experimental  Results Accordinge toHimannete al.,agingdegradesmotionf 
unctions.e Wegeneratenewwalkingmotiondatabymodifyingae walkinge Thate leadstoe slowere walkingspeedwithaging:mal 
ee showse motiondatabasedonthechangesofposture,step,e andwalking12.4%,e andfemalee shows16.1%e ofe slowerwalkingspe 
ede pere speede duetoe aging.e Fig.1(b)-(d)showe stroboscopee p icturese ofe decadeof afterage63.e generatede 
walkinge motionsequence.e Oure proposede met hode e realizede togeneratee variouswalkinge motiondatain 
cludinge thee 3.ProposedMethodd onesoftheelderlyfromawalkingdata.e Accordinge toe thee medicale statisticale 
datae mentioned e ine thee e previouschapter,weproposethemethodtogenerate thewalkinge 5.Conclusionsd 
movementoftheelderlyfromthemovementoftheyo ung.Oure Wehaveproposedamethodofgeneratingvariousmot iondatae 
proposedfollowingtwosteps. methodconsistsofincludingtheonesoftheelderlyfromsinglemotio ndata.Oure Inthefirststep,wemodifythepostureoftheyou 
ngintooneofe proposedmethodrealizedthechangesofpostureand stepbye thethreetypesdescribedearlier.Thiscanbeimpl 
ementedbye usingmedicalstatisticaldata,andweexpectthate thismethodwille restrictinge themotione rangeofe 
jointanglesfromn ecke tolowere contributee toadde varietyine generatingmotionofa vatare in3De backasshowninTable1. 
virtual space. Next,basedondata,stepisnarrowede tofallintothethemeasured e motionrangeofjointanglesandwegeneratethewa 
lkingmotione References datatheyoung. oftheelderlyfromcapturedwalkingmotionof Iwaya,T.,etal. Theeffectsoflowbackpainone 
physicalande socialrurala rea ,e Journal functioninelderlypeoplelivinginofThesestepscanconvertthewalkingmotiondataofe 
theyoungtoe theLumbarSpinef Vol.11, JapaneseSocietyofDisorders,pp27­ theoneoftheelderlyconsideringtheposturechan 
ges,motione 34 (2005). rangestep. ofjointangles,andnarrowed e Himann,e JE,e etal.: Age-relatede changesinspeedo 
fe walking ,e Med.,pp.161-166(1988). Sci.SportsExec. Vol.20, Copyrightisheldbytheauthor /owner(s). SIGGRAPH 
2011, Vancouver,BritishColumbia,Canada, August7 11, 2011. ISBN978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037718</article_id>
		<sort_key>30</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>2</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Adaptive training of hidden Markov models for stylistic walk synthesis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037718</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037718</url>
		<abstract>
			<par><![CDATA[<p>In this extended abstract, we present the use of Hidden Markov Models (HMMs) in order to synthesize walk sequences with a given style using a small amount of training data from the target style. As a first step, a general model of walk is built. Starting from that model, an adaptive training enables to adapt our model to any particular style using only a small amount of training data. This technique, which was originally developed for speaker adaptation in speech synthesis [Zen et al. 2007], enables to reduce the main problem of machine learning techniques which is the large amount of data needed to train each new model, and to adapt models to the exaggerated style variations of our database that were far from an average walk.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809480</person_id>
				<author_profile_id><![CDATA[81481650415]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;elle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tilmanne]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Mons]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joelle.tilmanne@umons.ac.be]]></email_address>
			</au>
			<au>
				<person_id>P2809481</person_id>
				<author_profile_id><![CDATA[81100009839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Thierry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dutoit]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Mons]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Zen, H., Nose, T., Yamagishi, J., Sako, S., Black, T. M. A. W., and Tokuda, K. 2007. The HMM-based Speech Synthesis System (HTS) Version 2.0. In <i>Proceedings of the 6th ISCA Workshop on Speech Synthesis, Bonn, Germany</i>, 294--299.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Adaptive training of Hidden Markov Models for stylistic walk synthesis Jo¨Thierry Dutoit elle Tilmanne* 
University of Mons University of Mons Figure 1: Postures taken from four different synthesized styles 
(from left to right: sad, afraid, drunk and decided walks) 1 Introduction In this extended abstract, 
we present the use of Hidden Markov Models (HMMs) in order to synthesize walk sequences with a given 
style using a small amount of training data from the target style. As a .rst step, a general model of 
walk is built. Starting from that model, an adaptive training enables to adapt our model to any particular 
style using only a small amount of training data. This technique, which was originally developed for 
speaker adaptation in speech synthesis [Zen et al. 2007], enables to reduce the main problem of machine 
learning techniques which is the large amount of data needed to train each new model, and to adapt models 
to the exaggerated style variations of our database that were far from an average walk. 2 HMMs for motion 
synthesis Numerous kinds of HMMs have been used to study motion at dif­ferent complexity levels, whether 
it was for classi.cation, model­ing or synthesis purposes. Some researchers have integrated dif­ferent 
forms of style variable into their HMMs. Unfortunately there is still no style parameter that can be 
used to synthesize styles that are very different from the motions on which the system was trained. Increasing 
the number of styles that can be synthesized means increasing the complexity of the model and re-training 
the whole model, with the additional problem that enough data must be gathered for each different style. 
 3 Our Approach Our approach consists in .rst building one simple left-to-right HMM for both walk steps 
(left and right feet) from a large database as normally required. That average model can then be used 
as a basis for the adaptive training of any style-speci.c model. A new style-adapted model can thus be 
build very easily. Compared to ex­isting walk synthesis methods, it only requires a few motion capture 
steps of the desired walk style and running the adaptive training. For our database we captured an actor 
performing walk under eleven different states-of-mind, using an inertial motion capture suit (IGS-190 
from Animazoo). Our style component consists thus in exaggerated variations that can be far from a plain 
walk. We chose *e-mail: joelle.tilmanne@umons.ac.be to model rotations of the 18 captured joints rather 
than 3D positions in order to insure that the limb length constraints were respected in the synthesized 
motion. Our angles were converted into the ex­ponential map parameterization which is locally linear 
and where singularities can be avoided. For our HMM synthesizer, we adapted the functions implemented 
for speech within the HMM-based Speech Synthesis System (HTS) to build our procedure [Zen et al. 2007]. 
We took into account the dynamic aspect of the data, the modeling of the time spent in each state of 
the HMM (hidden semi-Markov models), and the global variance of the training data to avoid over-smoothed 
results. The adaptive training is performed based on constrained maximum like­lihood linear regression 
(CMLLR). We adapted thus the latest re­sults of the HMM speech synthesis .eld to our motion synthesis 
problem. Once our adapted model is built, we can synthesize as many walk sequences as we want. The model 
gives us joint angles and the displacement of the skeleton can be computed using our knowledge of the 
limb lengths and the step part in which we are. 4 Conclusion Our method gave very convincing synthesized 
walk sequences where the styles can easily be recognized (some examples of synthesized motion sequences 
can be found at http://tcts. fpms.ac.be/ tilmanne/). Future work will include an user evaluation to assess 
the naturalness of the produced motions, and generalization of the use of the style adaptation matrices. 
 Acknowledgements This project was partly funded by the Ministry of R´ egion Wal­lonne under the Numediart 
research program (grant N0716631) and by the Fonds pour la formation ` a la recherche dans l industrie 
et l agriculture (FRIA) . References ZEN, H., NOSE, T., YAMAGISHI, J., SAKO, S., BLACK, T. M. A. W., 
AND TOKUDA, K. 2007. The HMM-based Speech Syn­thesis System (HTS) Version 2.0. In Proceedings of the 
6th ISCA Workshop on Speech Synthesis, Bonn, Germany, 294 299. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Ministry of Region Wallonne under the Numediart research program</funding_agency>
			<grant_numbers>
				<grant_number>N0716631</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2037719</article_id>
		<sort_key>40</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>3</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Estimating fluid simulation parameters from videos]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037719</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037719</url>
		<abstract>
			<par><![CDATA[<p>Recently, a video-based high quality 3D shape and motion modeling methods for fluid are proposed. [Huamin et al. 2009] However, this approach only aims to capture and generate original fluid action as it is.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809482</person_id>
				<author_profile_id><![CDATA[81488654875]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Naoya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iwamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iwamoto@toki.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809483</person_id>
				<author_profile_id><![CDATA[81322504926]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ryusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sagawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[AIST]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809484</person_id>
				<author_profile_id><![CDATA[81466642917]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Shoji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kunitomo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809485</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shigeo@waseda.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1531396</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Huamin, W. et.al, 2009. Physically Guided Liquid Surface Modeling from Videos. <i>ACM SIGGRAPH 2009 Proceeding</i>, 28, 3, 1--11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Sagawa, R. et.al. 2009. Dense 3D Reconstruction Method Using a Single Pattern for Fast Moving Object. <i>IEEE ICCV</i>, 1779--1786.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Estimating Fluid Simulation Parameters from Videos Naoya Iwamoto*i Ryusuke Sagawa Shoji Kunitomo* Shigeo 
Morishima*ii  *Waseda University AIST  (a)Captured fluid motion (b)Reconstructed fluid depth (c)Optimized 
simulation result (d)Retargeted by optimized parameter  Figure 1: Fluid simulator optimization result 
based on camera captured fluid motion, (a): Capturing 3D fluid motion by single camera with patterns 
projection, (b): Reconstructed depth of fluid surface when a ball is swinging, (c): Result of particle 
simulation with optimized parameters. (d): Result of retargeting another action. 1 Introduction Recently, 
a video-based high quality 3D shape and motion mod-eling methods for fluid are proposed. [Huamin et al. 
2009] However, this approach only aims to capture and generate origi-nal fluid action as it is. Therefore, 
we propose a new method to capture and retarget an original liquid feature to generate another action 
for fluid by optimizing particle simulation parameters. Even if the correct physical parameters of a 
target liquid is known, sometimes the impression of liquid action reconstructed by simulator is not so 
real as original depending on a simulator performance. In this paper, we treat a simulator as a blackbox 
and try to optimize parameters to make an impression as close as original action captured by video camera. 
Especially, in case of an unidentified liquid, the parameters are also optimized to make a look as same 
as original. 2 Liquid Motion Capturing Liquid motion is captured by single video camera (30 fps) look-ing 
down upon a liquid surface in aquarium. The liquid is whit-ened by water base paint to make the structured 
light pattern reflect sharper. Then based on the calculated distance between liquid and camera, the liquid 
surface is estimated with about 1,200,000 vertices [Sagawa 2009]. A reconstructed liquid surface is shown 
in Fig.1-(b). 3 Correspondence in Real and Virtual World To unify the coordinates of real world and simulator, 
and then get the correspondence, we divide both real and virtual world as same number and size of small 
voxels considering the scale and the origin position located on the center of water tank. To make resampling 
the surface of liquid, the vertex and particles located close to the center of the voxels in the liquid 
surface are chosen to be compared and minimize cost function. The sampling points are situated with equal 
intervals in x-y coordinate paralleling with resting state water surface. By this process, it was chosen 
215 particles from total of 60,000 particles In fig1-(c), yellow ball is the trigger controlled by hand 
to ruf-fle the water surface. The shape and location are estimated liquid surface vertices by Hough transformation 
of depth data. And then the original captured motion is reflected in the liquid simulator to generate 
a copy action. 4 Cost Function By considering a factor that the motion of simulated fluid im-pressed 
correctly assembled, a cost function is defined as the difference of 3D shapes of fluid surface. Cost 
function can be defined as a sum of an averaged absolute correspondence of norm per scene between real 
and virtual. 5 Optimization Parameters NVIDIA® PhysX is selected as a fluid simulator. In PhysX, four 
parameters: stiffness, viscosity, rest density, damping are select-ed to be optimized. If we gave only 
those values of random pa-rameters in simulator, the values of those parameters are esti-mated by minimizing 
cost function by iterative calculation with Simulated Annealing. This process was repeated 1174 times 
and took 15 hours 24 minutes. The process is simulator independent. 6 Result and Conclusion Fig.1 (b) 
and (c) are real motion and simulation result of water when yellow ball is swinging. Initial parameters 
give an impres-sion of sticky liquid like honey, but after optimization it becomes more fluently very 
close to real water. Fig.1 (d) is the retargeting result for ball dropping to water with same parameters. 
Then, POV-RAY is selected as rendering software. The result of opti-mized parameters is splashed the 
liquid like water on surface. The quantitative evaluation for open retargeting test is next sub-ject. 
Furthermore, we need to try other simulators and also other simulation except swinging the ball. References 
HUAMIN, W. et.al, 2009. Physically Guided Liquid Surface Model-ing from Videos. ACM SIGGRAPH 2009 Proceeding, 
28, 3, 1-11. SAGAWA, R. et.al. 2009. Dense 3D Reconstruction Method Using a Single Pattern for Fast Moving 
Object. IEEE ICCV, 1779-1786. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037720</article_id>
		<sort_key>50</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>4</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Fluid simulation without pressure]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037720</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037720</url>
		<abstract>
			<par><![CDATA[<p>We suggests a new framework for enforcing the divergence-free condition on a velocity field. In this framework, the incompressibility is achieved by summing all the other nodal vorticities rather than via solving the Poisson equation. Our method is very simple to implement in both two and three dimensions and able to substitute for the conventional pressure projection step of the fluid simulation. In contrast with the pressure projection step, the proposed method can be efficiently parallelizable on multi-core or GPU architectures.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809486</person_id>
				<author_profile_id><![CDATA[81488657315]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wanho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University Digital IDEa, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809487</person_id>
				<author_profile_id><![CDATA[81488648306]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[In-Yong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jeon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809488</person_id>
				<author_profile_id><![CDATA[81488663556]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Jong-Chul]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yoon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FXGear, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809489</person_id>
				<author_profile_id><![CDATA[81423594747]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Hyeong-Seok]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ko]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Seoul National University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073406</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Park, S. I., and Kim, M. J. 2005. Vortex fluid for gaseous phenomena. In <i>Proc. ACM SIGGRAPH/Eurographics Symp. on Comp. Anim.</i>, 261--270.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>36901</ref_obj_id>
				<ref_obj_pid>36895</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Greengard, L., and Rokhlin, V. 1987. A fast algorithm for particle simulation. <i>J. Comp. Phys. 73</i>, 325--348.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Fluid Simulation without Pressure Wanho Choi In-Yong Jeon Seoul National University Seoul National University 
Digital IDEa, Inc. Abstract We suggests a new framework for enforcing the divergence-free condition on 
a velocity .eld. In this framework, the incompressibil­ity is achieved by summing all the other nodal 
vorticities rather than via solving the Poisson equation. Our method is very simple to im­plement in 
both two and three dimensions and able to substitute for the conventional pressure projection step of 
the .uid simulation. In contrast with the pressure projection step, the proposed method can be ef.ciently 
parallelizable on multi-core or GPU architectures. 1 Introduction Numerical simulations of inviscid and 
incompressible .ows have two main parts: the advection step and the pressure projection step. Parallelizing 
the advection step is relatively easy when the simula­tor is based on the semi-Lagrangian advection scheme. 
However, parallelizing the pressure projection step is not easy because one must solve the Poisson equation, 
which is globally coupled system. On the other hand, we solve the N-vortex problem by modifying the governing 
equations and using the Biot-Savart law. As a result, our method is simpler and more ef.cient for the 
parallelization than the conventional pressure prejection method. It is also not required any iteratons. 
 2 Our Approach Usually, incompressible inviscid .ows are simulated according to the following governing 
equations, u = -u ·u -p + f (1) t · u = 0 (2) where t is time, u is the velocity, p is the pressure, 
and f is the ex­ternal force. In combination with the pressure term of equation (1), equation (2) reduces 
to the Poisson equation,·p =· u". Here, u is the intermediate velocity. After solving p from the Poisson 
" equation, the divergence-free velocity .eld unew for the next time step can be obtained according to 
unew = u"-p. On the other hand, we note that the intermediate velocity "u can be decomposed into the 
sum of conservative and solenoidal parts (the Helmholtz-Hodge decomposition), "(3) u = "uconservative 
+ "usolenoidal such thatuconservative = 0 and· usolenoidal = 0. Hence, the ×"" solenoidal velocity .eld 
"usolenoidal is identical to the divergence­free velocity .eld unew to be determined for the next time 
step. ×unew Taking the curl on both sides of equation (3) gives=× u. Therefore, the velocity .eld unew 
can be calculated according to " the Biot-Savart law [Park and Kim 2005]: 1 l--" (x) ×(p -x) unew(p)= 
dx (4) AA x lp -xld Jong-Chul Yoon Hyeong-Seok Ko FXGear, Inc. Seoul National University Figure 1: 2D 
real-time .uid simulation using CUDA. A 512×512 regular grid with 64×64 clusters was used. where -" =×u"and 
A = 2, d = 2 for two dimensions(2D), and A = 4, d = 3 for three dimensions(3D). Equation (4) can be transformed 
into a discrete form: hd ­ -" i ×(p -xi) new(p)= u (5) AAlp -xild i.nodes where h is the cell size, xi 
and ­ -" i are the position and vorticity of the i-th node, and p is the position at which we would like 
to calculate the velocity. Now, it reduces to the N-vortex problem. A general solution to this problem 
is the Fast Multipole Method (FMM), [Greengard and Rokhlin 1987], which provides an approx­imate solution 
while reducing the computation time by clustering long-range forces using truncated series expansions. 
However, im­plementation of FMM is particularly complicated in 3D. We suggest a new method, Fast Vorticity 
Summation Method (FVSM). We apply the heuristic assumption that if a cluster is suf­.ciently far from 
the position p, lp -xilin the denominator of equation (5) can be approximated by lp -xrefl, where xref 
is the reference point of the cluster. With the above assumption, the ve­locity at p can be approximated 
by new(p) hd 1 {(-" i ×p -(-" i ×xi (6) uA Alp -xrefld where i is the nodal index within the cluster. 
We set the xref as the weighed average point by the magnitude of the vorticity. As one can see, for suf.ciently 
distant clusters, only three vectors(-" i, ---" i ×xi and xref) have to be transmitted. Before proceeding 
to the main calculation, these three vectors are calculated for each cluster independently in advance. 
Neighboring clusters should share their nodal velocities for the calculation of vorticities and the direct 
sum­mation, which is the same procedure as is used in the FMM. How­ever, these data are transmitted only 
between neighboring clusters, and the transmission is also necessary for the semi-Lagrangian ad­vection 
step.  References PARK, S. I., AND KIM, M. J. 2005. Vortex .uid for gaseous phenomena. In Proc. ACM 
SIGGRAPH/Eurographics Symp. on Comp. Anim., 261 270. GREENGARD, L., AND ROKHLIN, V. 1987. A fast algorithm 
for particle simulation. J. Comp. Phys. 73, 325 348. Copyright is held by the author / owner(s). SIGGRAPH 
2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037721</article_id>
		<sort_key>60</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>5</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[GAMIC]]></title>
		<subtitle><![CDATA[exaggerated real time character animation control method for full-body gesture interaction systems]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037721</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037721</url>
		<abstract>
			<par><![CDATA[<p>This article reports a new method and tools for exaggerated real-time character animation control method for full-body gesture interaction systems. Game Action Motion Interaction Controller (GAMIC) is a motion interaction design tool that can be used by motion interaction designer with KINECT andWiiRemote. It comprises a generic evaluation function with thresholds and does not require any additional programming.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809490</person_id>
				<author_profile_id><![CDATA[81488671582]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Hajime]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Misumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809491</person_id>
				<author_profile_id><![CDATA[81488670906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Wataru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809492</person_id>
				<author_profile_id><![CDATA[81448597336]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kosaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809493</person_id>
				<author_profile_id><![CDATA[81430601538]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Motofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hattori]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809494</person_id>
				<author_profile_id><![CDATA[81319501381]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Akihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kanagawa Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gamic@shirai.la]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 GAMIC: Exaggerated real time character animation control method for full-body gesture interaction systems 
Hajime MISUMI, Wataru FUJIMURA, Takayuki KOSAKA, Motofumi HATTORI, Akihiko SHIRAI* Kanagawa Institute 
of Technology  Figure 1: Evaluation function and screenshot of GAMIC Abstract This article reports a 
new method and tools for exaggerated real­time character animation control method for full-body gesture 
in­teraction systems. Game Action Motion Interaction Controller (GAMIC) is a motion interaction design 
tool that can be used by motion interaction designer with KINECT and WiiRemote. It com­prises a generic 
evaluation function with thresholds and does not require any additional programming. 1 Motivation: intuitive 
animation control method and tool for KINECT generation Real-time animation in-game avatars driven by 
full-body gesture in­teraction systems using real-time motion capture data are extremely appealing. Further, 
it is becoming more popular since researchers and developers can easily access consumer-priced depth 
sensors in Microsoft KINECT and OpenNI frameworks. However, a system that can realize suitable real-time 
animations for each player actions is required. This article describes a new method and a tool for exaggerated 
but sophisticated interactive an­imation control systems for real-time animation playback timings. Our 
method can improve best player experiences without program­ming by of.ine/online GUI s for current video 
games and interac­tive systems.  2 GAMIC: Game Action Motion Interaction Controller Game Action Motion 
Interaction Controller (GAMIC) is a tool that can link the timings between KINECT recognition and avatar 
ani­mation playback by GUI, WiiRemote, and actual motions. GAMIC requires linking between physical players 
action and real-time avatar animation playback timings for the development of game systems that assume 
full-body gesture interactions using KINECT. GAMIC de.nes a recognition timing of KINECT for animation 
playback timing by GUI. Motion Interaction Designer (MID) stores two target gestures for the start and 
end as (T 1, T2) using WiiRe­mote in front of KINECT on the GAMIC GUI. *e-mail: gamic@shirai.la This 
recognition can be expressed as an evaluation function of the current posture and its threshold of a 
target frame. The evaluation function expresses a similarity between the current player s kine­matics 
and target postures from KINECT inputs, and it can be ob­tained as a summation of the inner products 
of target and current bones. If a current posture V .ts a target posture T, the evaluation function f(T,V 
) outputs 1. Its threshold P1 can control the recognition dif.culty by f(S, V ) as a starting target 
frame S. The trigger frame (TF) is an intermediate posture, and it exists be­tween SF(T1) and EF(T2) 
as a result of the continuity of human motions. TF is an indescribable and dynamic posture but it can 
be expressed by an evaluation function f(E, V ) with its threshold P2. TF must be set correctly. It can 
generate togetherness if the posterior half-animation is synchronous with the recognition. GAMIC can 
control the recognition sensitivity, timing, and anima­tion impressions simultaneously by adjusting P2. 
 3 Conclusion By using GAMIC, we realized a higher quality of animation-timing implementations with resource 
effective tools by non-programming methods. This process requires some staffs (MIA, Programmers, Actors) 
to create the core of the interaction sense. It will be also integrated with physics and/or machine-learning­based 
posture estimations and animation blending to create effec­tive interaction experiments in the near future. 
Template matching, physics-and learning; based approaches can improve player ges­ture recognitions and 
dynamic animation however, these techniques also require human decisions and large trial-and-error periods 
for improvement. In contrast, GAMIC has the advantage that intuitive player ac­tions can be explored 
for real-time animations by MID instead of machine-learning methods. Hence, assignment and linkage be­tween 
player actions and prede.ned animations can be improved, especially for special actions that are otherwise 
impossible by phys­ical motions. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008   
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037722</article_id>
		<sort_key>70</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>6</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[MoCCA]]></title>
		<subtitle><![CDATA[motion capture cloth analysis]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037722</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037722</url>
		<abstract>
			<par><![CDATA[<p>Our project investigates the re-tasking of digital cloth simulation for purposes beyond entertainment. We are simulating applications where garments are employed to support the deployment of a range of wearable technologies (e.g. search and rescue, criminal investigation, counter terrorism) In these cases it is useful to be able to accurately predict the behaviours of various materials in a range of environments and scenarios.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809495</person_id>
				<author_profile_id><![CDATA[81335496667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rowland]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DJCAD, University of Dundee, Dundee, Scotland, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[c.rowland@dundee.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P2809496</person_id>
				<author_profile_id><![CDATA[81488671085]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imrie]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[DJCAD, University of Dundee, Dundee, Scotland, UK]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[p.imrie@dundee.ac.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882357</ref_obj_id>
				<ref_obj_pid>1201775</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baraff, D., Kass, M., and Witkin, A. 2003. Untangling Cloth. In <i>Computer Graphics (Proceedings of Siggraph 03)</i>, ACM, 862--870.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280821</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Baraff, D., And Witkin, A. 1998. Large Steps in Cloth Simulation. In <i>Computer Graphics (Proceedings of Siggraph 98)</i>, ACM, 43--54.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>280826</ref_obj_id>
				<ref_obj_pid>280814</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[DeRose, T., Kass, M., and Truon, T. 1998. Subdivision Surfaces in Computer Animation. In <i>Computer Graphics (Proceedings of Siggraph 98)</i>, ACM, 85--94.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Terzopoulos, G., and Fleischer, K. 1988. Deformable Models. In <i>Visual Computer 4</i>, ACM, 306--331.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037723</article_id>
		<sort_key>80</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>7</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Simulation of breathing for medical applications]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037723</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037723</url>
		<abstract>
			<par><![CDATA[<p>This work describes further research on the Structured Light Plethysmography (SLP) [de Boer et al. 2010] project which is a non-invasive method for pulmonary function testing using visible light. This technique uses two cameras and a known grid which is projected onto the chest of a patient. Using stereo vision algorithms, the 3D coordinates of each grid point projected on the chest wall are recovered over time. As the device captures only the front of the chest wall, we cannot infer the absolute volumes of the lungs but only volume changes of the thoracic cage. However, when it comes to examining respiratory function in a patient, absolute volumes represent a crucial piece of information. In order to address this problem we use optimisation techniques to fit SLP data to a highly detailed 3D model of the human torso (composed of rigid parts and muscles) that we have created. Our simulation provides us with both the optimal muscle inputs to simulate breathing - to date, these muscle activations have remained mysterious and been simplified to sine functions in the literature - and the absolute volumes of the lungs for a given SLP dataset.</p> <p>As we wish to investigate pulmonary function, the model must be anatomically and physiologically accurate enough to be medically approved. Previous attempts have concentrated more on the visual side of the simulations than on the medical applications, which often resulted in overly simplified models. [Lee et al. 2009] describes a model of the whole upper body but with few respiratory muscles of the rib cage and with no diaphragm, which is an essential muscle in breathing. [Zordan et al. 2004] uses a skeleton model which anatomically-wise lacks realism and simplifies the articular bones in the spine and the rib cage by grouping and treating them as a single rigid body. Furthermore, the different muscles are grouped and receive the same input (a step function for the diaphragm and sine functions for all the others) in order to produce visually pleasing results. In comparison, our model has high anatomical accuracy in the dimensions of the rigid parts and in the locations of the joints linking them and the muscles involved in breathing (the rigid parts, the joints and the muscle locations were designed using current anatomy books). In addition, it has high-level controls, is fully tunable (each muscle can be activated independently) and can be fitted to different patient anatomies.</p> <p>Breathing entails expanding (inspiration) and contracting (expiration) the chest wall. From a physiological point of view, this is done through two different moving parts of the body: the rib cage and the abdominal cavity. The rib cage is driven by three types of muscles: the scalene muscles and the external intercostals which expand the rib cage by lifting up the ribs and the internal intercostals which move the rib cage down. The abdominal cavity is put into motion by the diaphragm, which contributes in expanding the lung volume by pushing down the organs inside the abdominal cavity resulting in the ventral abdominal wall moving outwards, and the abdominal muscles, which push the ventral abdominal wall inwards. To model the abdominal breathing, when the diaphragm contracts, the vertices located at the ventral part of the abdominal wall move according to the volume decrease inside the abdominal cavity. In total, we use 510 muscle elements for the rib cage and 270 for the abdominal cavity. Each muscle element is modelled as a spring and a damper in parallel, parameters of which are tuned individually depending on the type of muscle with their activations depending on their lengths. We note that [Zordan et al. 2004] and [Lee et al. 2009] use the Hill type muscle model. The additional complexity in [Lee et al. 2009], which uses a FEM of the actual muscle surface, is not appropriate for our respiratory muscles which are just thin layers and do not affect the skin deformation.</p> <p>We begin by shaping our skeleton model and creating an adapted skin mesh according to anatomical characteristics of the patient. Next, we acquired data of the patient breathing with the SLP technique which gives coordinates of grid points located on the chest of the patient (corrected for projection effects). An optimisation then derives the different muscle activations of our simulation to fit the SLP dataset. Currently, we use the Nelder-Mead Downhill Simplex Method to optimise over several muscle parameters and we use as cost function the distance error over a whole dataset between the grid points of the SLP and their closest vertices on the skin model. Thus, we get the volume of the lungs of our simulation over time and the different activation muscle parameters that drive the breathing. We present results which show the visual realism of the resultant breathing. The validation of our method is done by comparing the simulated lung volumes with spirograms.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809497</person_id>
				<author_profile_id><![CDATA[81488673258]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Thierry]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Maldonado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tjm53@cam.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P2809498</person_id>
				<author_profile_id><![CDATA[81100575511]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Joan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lasenby]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jl221@cam.ac.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[de Boer, W., Lasenby, J., Cameron, J., Wareham, R., Ahmad, S., Roach, C., Hills, W., and Iles, R. 2010. Slp: A zero-contact non-invasive method for pulmonary function testing. In <i>Proc. BMVC</i>, 85.1--12. doi:10.5244/C.24.85.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1559756</ref_obj_id>
				<ref_obj_pid>1559755</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lee, S., Sifakis, E., and Terzopoulos, D. 2009. Comprehensive biomechanical modeling and simulation of the upper body. <i>ACM Trans. Graph</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028528</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Zordan, V., Celly, B., Chiu, B., and DiLorenzo, P. 2004. Breathe easy: model and control of simulated respiration for animation. In <i>Proc. 2004 ACM SIGGRAPH/Eurographics Symp. on Computer animation</i>, Eurographics Association.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simulation of breathing for medical applications Thierry J. Maldonado and Joan Lasenby* Department of 
Engineering, University of Cambridge  Figure 1: (a) SLP: using two cameras, the space coordinates of 
the corners of a projected grid on a patient s chest are reconstructed (b) 3D model of the human torso: 
the rib cage muscles (white) and the abdominal muscles (green) (c) the model is .tted to the patient 
and is driven by the SLP dataset via an optimisation algorithm (d) the simulation provides realistic 
absolute volume of the lungs. 1 Abstract This work describes further research on the Structured Light 
Plethysmography (SLP) [de Boer et al. 2010] project which is a non-invasive method for pulmonary function 
testing using visible light. This technique uses two cameras and a known grid which is projected onto 
the chest of a patient. Using stereo vision algo­rithms, the 3D coordinates of each grid point projected 
on the chest wall are recovered over time. As the device captures only the front of the chest wall, we 
cannot infer the absolute volumes of the lungs but only volume changes of the thoracic cage. However, 
when it comes to examining respiratory function in a patient, absolute vol­umes represent a crucial piece 
of information. In order to address this problem we use optimisation techniques to .t SLP data to a highly 
detailed 3D model of the human torso (composed of rigid parts and muscles) that we have created. Our 
simulation provides us with both the optimal muscle inputs to simulate breathing -to date, these muscle 
activations have remained mysterious and been simpli.ed to sine functions in the literature -and the 
absolute vol­umes of the lungs for a given SLP dataset. As we wish to investigate pulmonary function, 
the model must be anatomically and physiologically accurate enough to be medically approved. Previous 
attempts have concentrated more on the visual side of the simulations than on the medical applications, 
which of­ten resulted in overly simpli.ed models. [Lee et al. 2009] describes a model of the whole upper 
body but with few respiratory muscles of the rib cage and with no diaphragm, which is an essential mus­cle 
in breathing. [Zordan et al. 2004] uses a skeleton model which anatomically-wise lacks realism and simpli.es 
the articular bones in the spine and the rib cage by grouping and treating them as a single rigid body. 
Furthermore, the different muscles are grouped and receive the same input (a step function for the diaphragm 
and sine functions for all the others) in order to produce visually pleas­ing results. In comparison, 
our model has high anatomical accu­racy in the dimensions of the rigid parts and in the locations of 
the joints linking them and the muscles involved in breathing (the rigid parts, the joints and the muscle 
locations were designed using cur­rent anatomy books). In addition, it has high-level controls, is fully 
tunable (each muscle can be activated independently) and can be .tted to different patient anatomies. 
Breathing entails expanding (inspiration) and contracting (expira­tion) the chest wall. From a physiological 
point of view, this is done through two different moving parts of the body: the rib cage and the abdominal 
cavity. The rib cage is driven by three types of muscles: *e-mail: {tjm53, jl221}@cam.ac.uk the scalene 
muscles and the external intercostals which expand the rib cage by lifting up the ribs and the internal 
intercostals which move the rib cage down. The abdominal cavity is put into motion by the diaphragm, 
which contributes in expanding the lung volume by pushing down the organs inside the abdominal cavity 
resulting in the ventral abdominal wall moving outwards, and the abdomi­nal muscles, which push the ventral 
abdominal wall inwards. To model the abdominal breathing, when the diaphragm contracts, the vertices 
located at the ventral part of the abdominal wall move ac­cording to the volume decrease inside the abdominal 
cavity. In to­tal, we use 510 muscle elements for the rib cage and 270 for the abdominal cavity. Each 
muscle element is modelled as a spring and a damper in parallel, parameters of which are tuned individually 
depending on the type of muscle with their activations depending on their lengths. We note that [Zordan 
et al. 2004] and [Lee et al. 2009] use the Hill type muscle model. The additional complexity in [Lee 
et al. 2009], which uses a FEM of the actual muscle surface, is not appropriate for our respiratory muscles 
which are just thin layers and do not affect the skin deformation. We begin by shaping our skeleton model 
and creating an adapted skin mesh according to anatomical characteristics of the patient. Next, we acquired 
data of the patient breathing with the SLP tech­nique which gives coordinates of grid points located 
on the chest of the patient (corrected for projection effects). An optimisation then derives the different 
muscle activations of our simulation to .t the SLP dataset. Currently, we use the Nelder-Mead Downhill 
Simplex Method to optimise over several muscle parameters and we use as cost function the distance error 
over a whole dataset be­tween the grid points of the SLP and their closest vertices on the skin model. 
Thus, we get the volume of the lungs of our simulation over time and the different activation muscle 
parameters that drive the breathing. We present results which show the visual realism of the resultant 
breathing. The validation of our method is done by comparing the simulated lung volumes with spirograms. 
 References DE BOER, W., LASENBY, J., CAMERON, J., WAREHAM, R., AHMAD, S., ROACH, C., HILLS, W., AND 
ILES, R. 2010. Slp: A zero-contact non-invasive method for pulmonary function testing. In Proc. BMVC, 
85.1 12. doi:10.5244/C.24.85. LEE, S., SIFAKIS, E., AND TERZOPOULOS, D. 2009. Comprehensive biomechanical 
modeling and simulation of the upper body. ACM Trans. Graph. ZORDAN, V., CELLY, B., CHIU, B., AND DILORENZO, 
P. 2004. Breathe easy: model and control of simulated respiration for animation. In Proc. 2004 ACM SIG­ 
GRAPH/Eurographics Symp. on Computer animation, Eurographics Association. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037724</article_id>
		<sort_key>90</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>8</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Simultaneous speech and animation synthesis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037724</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037724</url>
		<abstract>
			<par><![CDATA[<p>Talking computer animated characters are a common sight in video games and movies. Although doing the mouth animation by hand gives the best results, because of cost and time constraints it is not always feasible. Furthermore the amount of speech in current games is ever increasing with some games having more than 200,000 lines of dialogue. This work proposes a system that can produce speech and the corresponding lip animation simultaneously using a statistical machine learning framework based on Hidden Markov Models (HMMs). The key point is that with the developed system never before seen or heard animated dialogues can be produced at a push of a button.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809499</person_id>
				<author_profile_id><![CDATA[81447601777]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Dietmar]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schabus]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FTW Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[schabus@ftw.at]]></email_address>
			</au>
			<au>
				<person_id>P2809500</person_id>
				<author_profile_id><![CDATA[81418596529]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pucher]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FTW Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pucher@ftw.at]]></email_address>
			</au>
			<au>
				<person_id>P2809501</person_id>
				<author_profile_id><![CDATA[81335491640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Gregor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hofer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[FTW Austria]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hofer@ftw.at]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tokuda, K., Yoshimura, T., Masuko, T., Kobayashi, T., and Kitamura, T. 2000. Speech parameter generation algorithms for HMM-based speech synthesis. In <i>Proc. ICASSP</i>, 1315--1318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Simultaneous Speech and Animation Synthesis Dietmar Schabus* Michael Pucher Gregor Hofer FTW Austria 
FTW Austria FTW Austria  Figure 1: True audio-visual speech synthesis from the same underlying model. 
1 Introduction Talking computer animated characters are a common sight in video games and movies. Although 
doing the mouth animation by hand gives the best results, because of cost and time constraints it is 
not always feasible. Furthermore the amount of speech in cur­rent games is ever increasing with some 
games having more than 200,000 lines of dialogue. This work proposes a system that can produce speech 
and the corresponding lip animation simultaneously using a statistical machine learning framework based 
on Hidden Markov Models (HMMs). The key point is that with the developed system never before seen or 
heard animated dialogues can be pro­duced at a push of a button. 2 Audio-visual trajectory HMM synthesis 
The core of the developed system consists of a statistical model of speech that was trained on a database 
of motion capture and audio recordings. We utilize the trajectory HMM framework for both speech synthesis 
and lip synchronization. Figure 2 shows the training and synthesis process of the multimodal speech models. 
Training a single model for both speech and motion has the ad­vantage over previous approaches that for 
example duration mod­eling of individual phonemes (e.g. vowels and consonants) can be shared across both 
domains. The animation training data consisted of 15 PCA components of 32 tracked markers on the face. 
The speech training data consisted of standard spectral features (mel cepstrum), aperiodicity, and pitch 
features. Both the static anima­tion and speech features are augmented with their corresponding delta 
and delta-delta values. As a modeling unit for the animation and speech model context dependent phonemes 
were employed, for the speech model additional features like the phoneme position in an utterance were 
also used. For synthesis the text of an utterance is translated into a phoneme sequence using standard 
text analysis common in speech syn­thesis. This phoneme sequence serves as input to the synthesis model, 
where each phoneme consists of corresponding probability density functions (pdfs) over speech features 
and animation fea­tures. The maximum likelihood parameter generation algorithm *e-mail: schabus@ftw.at 
e-mail: pucher@ftw.at e-mail: hofer@ftw.at  Figure 2: The training process uses a multimodal database 
of recorded audio and motion capture data, which produces a mul­timodal speech model. During synthesis 
input text is automatically converted into corresponding speech and animation using maxi­mum likelihood 
parameter generation on the distributions from the model of speech. (MLPG)[Tokuda et al. 2000] is then 
applied to this sequence of pdfs in order to obtain a single, most probable trajectory which op­timizes 
the constraints between the distributions of static, delta and delta delta features. The trajectory then 
both drives the animation and produces an audible speech utterance. References TOKUDA, K., YOSHIMURA, 
T., MASUKO, T., KOBAYASHI, T., AND KITAMURA, T. 2000. Speech parameter generation al­gorithms for HMM-based 
speech synthesis. In Proc. ICASSP, 1315 1318. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037725</article_id>
		<sort_key>100</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>9</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Visual simulation of bleeding on skin surface]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037725</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037725</url>
		<abstract>
			<par><![CDATA[<p>The liquid behavior has a significant impact on video works, and <i>bleeding</i> is no exception. Hemorrhage scenes possess highly important implications, as seen from the fact that they might make the work rated age limit. Lack of visual reality in the involved scenes would reduce the overall quality of the work.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809502</person_id>
				<author_profile_id><![CDATA[81488652044]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kazuhide]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ueda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ueda-kazuhide@z7.keio.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809503</person_id>
				<author_profile_id><![CDATA[81100355096]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Issei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fujishiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fuji@ics.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1073400</ref_obj_id>
				<ref_obj_pid>1073368</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Clavet, S., Beaudoin, P., and Poulin, P.: "Particle-based Viscoelastic Fluid Simulation," in <i>Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation</i>, pp. 219--228, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1276437</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Adams, B., Pauly, M., Keiser, R., and Guibas, L. J.: "Adaptively Sampled Particle Fluids," <i>ACM Transactions on Graphics</i>, Vol. 26, No. 3, Article 48, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visual Simulation of Bleeding on Skin Surface Kazuhide UEDA* Issei FUJISHIRO Keio University  Figure 
1: Clotting of blood after bleeding from the irregularly-chapped wound on the forehead. Note that blood 
is sliding underneath the lower-superciliary surface. 1 Introduction The liquid behavior has a signi.cant 
impact on video works, and bleeding is no exception. Hemorrhage scenes possess highly im­portant implications, 
as seen from the fact that they might make the work rated age limit. Lack of visual reality in the involved 
scenes would reduce the overall quality of the work. Against this background, we have used Lagrangian 
method, called SPH (Smoothed Particle Hydro­dynamics), to simulate bleeding on skin surface faithfully 
(see Fig. 1). Salient features of the extension are three-fold: (1) bleeding from wounds modeled along 
with in-vivo blood .ow re.ecting their geometry; (2) congelation of blood represented by varying viscos­ity 
according to the time exposed to the air; and (3) physisorption of blood on skin surface formulated by 
introducing an additional adsorption term due to van der Waals force to the original Navier-Stokes equations. 
 2 Our Approach 2.1 Bleeding from wound The shape of a wound on the skin surface is de.ned by user-input 
strokes, and its depth and width are determined automatically by the type of blade to be speci.ed. .ow 
model is evaluated within the resulting prismoid, where accel­eration of blood is directed in parallel 
to the skin surface normal. When the particle collides with an exit polygon, it bleeds.  2.2 Congelation 
of blood After bleeding from the wound, blood starts to contact with the air and clots through the action 
of platelets. This phenomenon can be represented by changing the viscous force of each particle accord­ing 
to the time exposed to the air, as shown in Fig. 2. Any particle can be classi.ed into the three phases 
depending on the distance from the blood surface at each time step, and a particle closer to the surface 
clots faster by moving forward its congelation timer. (a) Changes in viscosity (b) Particle phase Figure 
2: Viscosity µ of particle changes according to the degree of exposition to the air. *e-mail: ueda-kazuhide@z7.keio.jp 
e-mail: fuji@ics.keio.ac.jp (1) [1] Clavet, S., Beaudoin, P., and Poulin, P.: Particle-based Vis­coelastic 
Fluid Simulation, in Proceedings of the 2005 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, 
pp. 219 228, 2005. [2] Adams, B., Pauly, M., Keiser, R., and Guibas, L. J.: Adap­tively Sampled Particle 
Fluids, ACM Transactions on Graphics, Vol. 26, No. 3, Article 48, 2007. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037726</article_id>
		<sort_key>110</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>10</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Wet cloth simulation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037726</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037726</url>
		<abstract>
			<par><![CDATA[<p>Both cloth and fluid simulation are important areas of research in Computer Graphics. However, only a surprisingly small amount of authors have investigated the two-way coupling of these simulations. Lenaerts et al. [2008] present a porous flow technique based on Darcy's law using a 3D particle simulation and apply their approach to textiles as well, however, their approach is computationally expensive and they use a rather simple cloth simulation system that cannot fully represent the complex effects of wet textiles. [Morimoto et al. 2007] present a technique that allows the visualization of dyeing based on diffusion terms. However, they do not consider their approach in a simulation context.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809504</person_id>
				<author_profile_id><![CDATA[81488640961]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Markus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809505</person_id>
				<author_profile_id><![CDATA[81351601516]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pabst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809506</person_id>
				<author_profile_id><![CDATA[81100575262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stra&#223;er]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360648</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lenaerts, T., Adams, B., and Dutr&#233;, P. 2008. Porous flow in particle-based fluid simulations. <i>ACM Trans. Graph. 27</i> (August), 49:1--49:8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Li, Y., and Zhu, Q. 2003. Simultaneous heat and moisture transfer with moisture sorption, condensation, and capillary liquid diffusion in porous textiles. <i>Textile Research Journal 73</i> (July), 515--524.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338554</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Morimoto, Y., Tanaka, M., Tsuruno, R., and Tomimatsu, K. 2007. Visualization of dyeing based on diffusion and adsorption theories. In <i>Pacific Graphics '07</i>, 57--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Wet Cloth Simulation Markus Huber and Simon Pabst and Wolfgang Straßer Graphical-Interactive Systems 
(GRIS) Wilhelm Schickard Institute for Computer Science, University of T¨ubingen, Germany  Figure 1: 
A piece of cloth is hit by a jet of water. It soaks up some .uid, which is then further transported inside 
the textile. The added mass due to the absorbed water in.uences the fabric drape. 1 Introduction Both 
cloth and .uid simulation are important areas of research in Computer Graphics. However, only a surprisingly 
small amount of authors have investigated the two-way coupling of these simula­tions. Lenaerts et al. 
[2008] present a porous .ow technique based on Darcy s law using a 3D particle simulation and apply their 
ap­proach to textiles as well, however, their approach is computation­ally expensive and they use a rather 
simple cloth simulation system that cannot fully represent the complex effects of wet textiles. [Mo­rimoto 
et al. 2007] present a technique that allows the visualization of dyeing based on diffusion terms. However, 
they do not consider their approach in a simulation context. 2 Wet Cloth Simulation We present a new 
two-way coupling technique for cloth and .uid simulations that also handles liquid diffusion in porous 
textiles. We combine a state-of-the-art .nite element-based cloth simulation en­gine with a smoothed 
particle hydrodynamics (SPH) .uid simula­tion. The .uid can interact with the textile and also wet it. 
We use Fick s law of translational diffusion to model the liquid trans­port in the wet cloth. A very 
fast discrete cellular automaton is used to compute the diffusion states. Variable diffusion coef.cients 
depending on the volume fraction of liquids and the material prop­erties of the textile .bres control 
the spreading of the liquid. We adaptively change the cloth material properties such as mass, stretching/bending 
coef.cients and friction of the textile to account for the soaked-up .uid. Additionally, the textile 
weave can in.u­ence the liquid transport. This allows for a variety of interesting vi­sual effects (see 
Fig. 2), especially in combination with orthotropic cloth material models and different yarn types in 
weft and warp directions. Our approach lends itself easily to parallelization and achieves interactive 
performance with tens of thousands of cloth and .uid particles on common off-the-shelf hardware. 2.1 
Two-Way Cloth-Fluid Coupling The .uid and the cloth objects interact at the interface. Pressure forces 
from the .uid in.uence the textile while it imposes bound­ary .ux conditions. Depending on the material 
properties, the tex­tile soaks up water, which then .ows into drier regions, driven by surface tension 
forces. We model this diffusion process using Fick s second law, which describes the changes in concentration 
over time Figure 2: A drop of water is soaked up by a piece of cloth. The liq­uid is spreading in an 
anisotropic fashion due to the weave pattern. The simulation runs in realtime. due to diffusion: .F= 
V· (DVF), (1) .t where F is the concentration, t is time, and D is the non-constant diffusion coef.cient. 
We discretize Eq. 1 and choose D using the dynamic model of [Li and Zhu 2003], which considers a number 
of key fabric properties such as porosity, contact angles and capillary radii. This simple yet powerful 
approach allows us to ef.ciently simulate a number of very interesting and complex phenomena that were 
not possible to represent with previous models.  References LENAERTS, T., ADAMS, B., AND DUTR E´, P. 
2008. Porous .ow in particle-based .uid simulations. ACM Trans. Graph. 27 (Au­gust), 49:1 49:8. LI, Y., 
AND ZHU, Q. 2003. Simultaneous heat and moisture trans­fer with moisture sorption, condensation, and 
capillary liquid diffusion in porous textiles. Textile Research Journal 73 (July), 515 524. MORIMOTO, 
Y., TANAKA, M., TSURUNO, R., AND TOMIMATSU, K. 2007. Visualization of dyeing based on diffusion and adsorp­tion 
theories. In Paci.c Graphics 07, 57 64. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037727</section_id>
		<sort_key>120</sort_key>
		<section_seq_no>2</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Art]]></section_title>
		<section_page_from>2</section_page_from>
	<article_rec>
		<article_id>2037728</article_id>
		<sort_key>130</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>11</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Abstract ocean waves]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037728</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037728</url>
		<abstract>
			<par><![CDATA[<p>Typically the models for ocean waves are computationally explored to replicate natural phenomena. Instead of aiming towards realism, this work explores aesthetic derivations of a simple state-of-the-art model for ocean waves simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809507</person_id>
				<author_profile_id><![CDATA[81488667706]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[L&#237;gia]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Duro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ligia@student.dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P2809508</person_id>
				<author_profile_id><![CDATA[81318497551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pmcruz@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P2809509</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>15894</ref_obj_id>
				<ref_obj_pid>15886</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fournier, A. and Reeves, T., 1986. A simple model of ocean waves. In <i>Computer Graphics (Proceedings of SIGGRAPH 86)</i>, vol 20, 4 (August), ACM, 75--84.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lawrence, J., 1972. Algebraic Curves of High Degree. In <i>A Catalog of Special Plane Curves</i>, Dover, New York, 160--178.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Tessendorf, J., 1999. Simulating Ocean waves. In <i>SIGGRAPH 99 Course Notes</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Abstract Ocean Waves Lígia Duro1, Pedro Cruz2 and Penousal Machado3 CISUC, Department of Informatics 
Engineering, University of Coimbra  Figure 1 Two rendered artifacts of the same family, resembling ocean 
waves. 1 Introduction Typically the models for ocean waves are computationally ex­plored to replicate 
natural phenomena. Instead of aiming towardsrealism, this work explores aesthetic derivations of a simple 
state­of-the-art model for ocean waves simulation. The Basic Model [Fournier and Reeves 1986] based on 
Gerstner s, is a classical physics model for realistic ocean wavesin computer graphics [Tessendorf 1999]. 
The behavior of themodel depends on a set of parameters. Establishing parameter values that are outside 
the typical range, and exploring unusualcombinations of values for different parameters, may yield ab­stract 
and unexpected shapes that are, nevertheless, evocative of water, oceans and waves. 2 Approach The Basic 
Model for ocean waves described in Fournier and Reeves [1986] is based on a trochoid the curve generated 
by apoint at a certain distance from the center of a circle rolling on a.xed straight line (S). Inspired 
by the circular nature of thismodel, we replaced the straight line S by an ellipse, altering thetrochoid 
to an elliptical hypotrochoid [Lawrence 1972] (Fig. 2). with low eccentricity. The parametric equations 
for these curves were implementedtogether with the wind effect on the top of the crests as in Four­nier 
and Reeves [1986]. For each iterated point over the hypotro­choid a small circle is drawn, representing 
the water particles and enabling the visualization of the movement of each wave (Fig. 1). 1 ligia@student.dei.uc.pt 
2 pmcruz@dei.uc.pt 3 machado@dei.uc.pt 4 http://abstract-ocean-waves.dei.uc.pt  3 Results By varying 
parameters such as wave length, phase speed, wind strength, number of waves and number of particles for 
each wave, several families of artifacts were produced. Each family resultsfrom the same variation rules 
of a set of constrained parameters, which results in aesthetically similar artifacts and produces ani­mations 
of mutating shapes4. Figure 3 Two rendered artifacts from different families. The emphasis of this work 
is on the produced shapes, thereforethe use of color was discarded. However, the use of slight trans­parencies 
provides depth and elegant complexity to the artifacts, also enabling the visualization of each single 
wave. Although each wave is a single entity, the artifacts emerge as uni.ed composi­tions of waves and 
a great diversity of abstract forms is possible. Movement is implicit in static and animated artifacts, 
conveying the .uidity and pace of ocean waves. References FOURNIER, A. AND REEVES, T., 1986. A simple 
model of ocean waves. In Computer Graphics (Proceedings of SIGGRAPH 86), vol 20, 4 (August), ACM, 75-84. 
L......., J., 1972. Algebraic Curves of High Degree. In A Catalog of Special Plane Curves, Dover, New 
York, 160-178. TESSENDORF, J., 1999. Simulating Ocean waves. In SIGGRAPH 99 Course Notes. Copyright is 
held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037729</article_id>
		<sort_key>140</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>12</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Rendering Op Art lines]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037729</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037729</url>
		<abstract>
			<par><![CDATA[<p>The 1960's marked the start of the Op Art movement. Typically, Op artists make use of optical illusions and high contrast to create artworks that "trick the eye", such as revealing hidden images or inducing a sense of movement. Artist Reginald H. Neal created a piece entitled <i>Square of Two</i> (see Figure 1a) which at first glance is a 2 x 2 tiling of concentric squares that seems to be scintillate as the eye moves across the image. A closer look reveals that the entire image is constructed out of lines arranged in two orthogonal directions, and the boundaries of the concentric squares are actually illusory contours created by the bending of these lines.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809510</person_id>
				<author_profile_id><![CDATA[81470655373]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tiffany]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Inglis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo, Waterloo, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[piffany@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2809511</person_id>
				<author_profile_id><![CDATA[81100155157]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Craig]]></first_name>
				<middle_name><![CDATA[S.]]></middle_name>
				<last_name><![CDATA[Kaplan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo, Waterloo, ON, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[csk@uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Rendering Op Art Lines Ti.any C. Inglis Craig S. Kaplan University of Waterloo University of Waterloo 
Waterloo, ON, Canada Waterloo, ON, Canada piffany@gmail.com csk@uwaterloo.ca Introduction The 1960 
s marked the start of the Op Art movement. Typically, Op artists make use of optical illusions and high 
contrast to create artworks that trick the eye , such as revealing hidden images or inducing a sense 
of movement. ArtistReginaldH.Neal created apiece enti­tled Square of Two (seeFigure1a)which at .rstglanceis 
a2× 2 tiling of concentric squares that seems to be scin­tillate as the eye moves across the image. A 
closer look reveals that the entire image is constructed out of lines arranged in two orthogonal directions, 
and the bound­aries of the concentric squares are actually illusory con­tours created by the bending 
of these lines. AN EXHIBITION OF UNSPEAKABLE THINGS WORKS INSPIRED BY H. P. LOVECRAFT S COMMONPLACE BOOK 
(a) Square of Two (top layer) and its (b) Op Art skull designed underlying image (bottom layer). by Vigne 
and Notter. Figure 1: Examples of line-based Op Art works. This style ofOpArthasbeen exploredbyother 
artists. Figure1bshows an example containing a skullimage cre­ated by Swiss graphic designers S. Vigne 
and J. Notter. By examining related artworks, wehaveidenti.ed an un­explored area of research. That is, 
we aim to develop a general algorithm for creating an Op Art composition from an arbitrary underlying 
image such that the num­ber of artifacts (e.g. line breaks and T-junctions) are minimized. These artifacts 
are undesirable because they interfere with the illusory contours created by the line bends. We use a 
colouredplanar map(with neighbouring re­gions assigneddi.erentcolours) to represent the underly­ingimage 
of anOpArt composition we want toproduce. Due to the FourColour Theorem, four colours are neces­sary 
to colour any map. Therefore it su.ces to develop k-colour Op Art algorithms for k =2, 3, 4. 2 2-colour 
algorithm The2-colour algorithm(Figure2) takes a2-colour map and a line spacing as inputs and returns 
a correspond­ing Op Art composition. This algorithm guarantees an output without any artifacts. The lines 
in the resulting Op Art composition can also be transformed such as by rotation or by shear mapping. 
(a) Step 1. (b) Step 2. (c) Step 3. Figure 2: Steps in the 2-colour algorithm. 3 3-and 4-colour extension 
Creating an Op Art composition from a 3-colour map is more challenging because, in general, there are 
no artifact-free solutions. Instead, we consider the problem as an optimizationproblem and use simulated 
annealing to search the con.guration space for an Op Art compo­sition with the fewest artifacts. Figure 
3 illustrates the steps in the 3-colour algorithm. (a) Step 1. (b) Step 2. (c) Step 3. Figure 3: Steps 
in the 3-colour algorithm. The 4-colour algorithm is left for future work because, even if we use the 
simulated annealing approach, there are further complications such as uneven line spacings for lines 
of di.erent directions. 4 Conclusion Our algorithms have potential applications for plotting data non-preferentially, 
creatingmazes,designingpicture logicpuzzlessuch asSlitherlink, andimprovinghatching algorithms used in 
line drawings. Copyright is held by the author / owner(s). . SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011 ISBN 978-1-4503-0921-9/11/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037730</article_id>
		<sort_key>150</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>13</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Slow art with a trillion frames per second camera]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037730</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037730</url>
		<abstract>
			<par><![CDATA[<p>How will the world look with a one trillion frame per second camera? Although such a camera does not exist today, we converted high end research equipment to produce conventional movies at 0.5 trillion (5&#183; 10<sup>11</sup>) frames per second, with light moving barely 0.6 mm in each frame. Our camera has the game changing ability to capture objects moving at the speed of light. Inspired by the classic high speed photography art of Harold Edgerton [Kayafas and Edgerton 1987] we use this camera to capture movies of several scenes.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[computational photography]]></kw>
			<kw><![CDATA[time of flight camera]]></kw>
			<kw><![CDATA[video capture]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809512</person_id>
				<author_profile_id><![CDATA[81490676712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velten]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809513</person_id>
				<author_profile_id><![CDATA[81487645512]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Everett]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lawson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809514</person_id>
				<author_profile_id><![CDATA[81488669467]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Andrew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bardagjy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809515</person_id>
				<author_profile_id><![CDATA[81488671861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Moungi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bawendi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809516</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[raskar@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[CameraCulture, 2010. Femto photography: http://web.media.mit.edu/raskar/femto/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Goda, K., Tsia, K. K., and Jalali, B. 2009. Serial time-encoded amplified imaging for real-time observation of fast dynamic phenomena. <i>Nature 458</i>, 1145--1149.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kayafas, G., and Edgerton, H. 1987. <i>Stopping Time: The Photographs of Harold Edgerton</i>. Harry N Abrams.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Slow Art with a Trillion Frames Per Second Camera Andreas Velten1, Everett Lawson1, Andrew Bardagjy1, 
Moungi Bawendi2, Ramesh Raskar1* MIT Media Lab1, MIT Department of Chemistry2  Figure 1: A candle pulse 
being hit by a laser pulse. This sequence of images taken from the movie included in our submission reveals 
the beauty of processes happening at the speed of light. The light pulse enters the scene through a pinhole 
on the left and strikes the side of the candle. Details like wick of the candle are visible. In the attached 
movie you will also note light bouncing off several mounts in the scene such as the pinhole on the left 
and the white wall in the left behind the candle. Abstract How will the world look with a one trillion 
frame per second cam­era? Although such a camera does not exist today, we converted high end research 
equipment to produce conventional movies at 0.5 trillion (5· 1011) frames per second, with light moving 
barely 0.6 mm in each frame. Our camera has the game changing abil­ity to capture objects moving at the 
speed of light. Inspired by the classic high speed photography art of Harold Edgerton [Kayafas and Edgerton 
1987] we use this camera to capture movies of sev­eral scenes. Keywords: computational photography, time 
of .ight camera, video capture Introduction and Motivation Visualizing processes at ever higher speeds 
is of of high interest in scienti.c applications as well as in art and entertainment applications. Harold 
Edgerton dedicated a lifetime of work to this effort producing stunning and powerful works of art. In 
this tradition we extend our high speed streak cam­era and use it in an unconventional way to capture 
common two dimensional movies at extremely high speeds, visualizing a world unfamiliar to human intuition 
and visual perception and creating a new kind of art. While regular high speed video cameras can only 
capture at several thousand frames per second, recently a two di­mensional camera using a femtosecond 
laser capable of a frame rate of 6,100,000 frames per second has been demonstrated [Goda et al. 2009]. 
Time-of-.ight light .eld analysis offers a variety of new possibili­ties to computer graphics and computer 
vision. Among other things it becomes possible to analyze sub-surface scattering and to look around the 
corner [CameraCulture 2010]. At the heart of this ef­fort is an imaging system capable of illuminating 
and capturing at speeds that make the travel time of light from the source, through the scene and to 
the detector accessible. Here we present a modi­.cation of our setup, that produces two dimensional movies 
with a time resolution of about two picoseconds (500,000,000,000 frames per second). Technical Approach 
and Results Our setup consists of a Ti:Sapphire laser emitting 50 fs pulses at a repetition rate of 75 
MHz. The scene is placed on a stage that can be adjusted in height. A pinhole attached to the left side 
of this stage directs the laser beam onto the scene. The camera is a Hamamatsu C5680 streak camera that 
captures a horizontal line on the scene at a .xed *e-mail: raskar@media.mit.edu Figure 2: Top Left: 
Part of our setup. The laser beam enters from the bottom left, the object in this case is diluted milk, 
the camera objective is visible on the bottom left. Top Right: Frame of a movie of a tomato. Bottom: 
Movie frame of a sugar crystal. height with two picosecond time resolution. The images produced by the 
camera are two dimensional 512x672 pixel images, with one dimension corresponding to time and one to 
the captured line in space. To cover the second spacial dimension we move the scene and light source 
vertically. A complete movie is combined from a large number of single images. During the capture time 
of up to several hours the laser and camera have to be kept in calibration by reference signals built 
into our optical setup. All captured videos are attached to this submission. Single frames of selected 
videos are shown in Figures and the teaser images. While the main purpose of this work is to demonstrate 
the beauty of ultra-fast optical processes and provide an intuitive window into a world otherwise only 
seen through numbers and graphs, it also provides important insight in the possibilities of the emerging 
.eld of femto photography.  References CAMERACULTURE, 2010. Femto photography: http://web.media.mit.edu/ 
raskar/femto/. GODA, K., TSIA, K. K., AND JALALI, B. 2009. Serial time-encoded ampli.ed imaging for real-time 
observation of fast dynamic phenomena. Nature 458, 1145 1149. KAYAFAS, G., AND EDGERTON, H. 1987. Stopping 
Time: The Photographs of Harold Edgerton. Harry N Abrams. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037731</section_id>
		<sort_key>160</sort_key>
		<section_seq_no>3</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Design]]></section_title>
		<section_page_from>3</section_page_from>
	<article_rec>
		<article_id>2037732</article_id>
		<sort_key>170</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>14</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[ChoreoGraphics]]></title>
		<subtitle><![CDATA[an authoring environment for dance shows]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037732</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037732</url>
		<abstract>
			<par><![CDATA[<p>Novel graphics technologies have had a great impact on the movie industry, allowing the combination of real actors with virtual ones that are synthesized from captured performances. Although applications of these resources in other media have so far been little explored, recent trends indicates that artists will soon begin to use them creatively to produce new visual effects in dance shows, concerts, musicals and other spectacles. An important research direction that will allow this revolution to, in fact, take place is the development of authoring environments that serve as a bridge between artists and the emerging technologies.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809517</person_id>
				<author_profile_id><![CDATA[81464649658]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adriana]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schulz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VISGRAF Laboratory, IMPA, Rio de Janeiro, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809518</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[VISGRAF Laboratory, IMPA, Rio de Janeiro, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>566605</ref_obj_id>
				<ref_obj_pid>566570</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kovar, L., Gleicher, M., and Pighin, F. 2002. Motion graphs. In <i>SIGGRAPH '02</i>, ACM, New York, NY, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Reynolds, C. 1999. Steering behaviors for autonomous characters. In <i>Game Developers Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ChoreoGraphics: An Authoring Environment for Dance Shows Adriana Schulz Luiz Velho VISGRAF Laboratory, 
IMPA, Rio de Janeiro, Brazil  1 Motivation Novel graphics technologies have had a great impact on the 
movie industry, allowing the combination of real actors with virtual ones that are synthesized from captured 
performances. Although ap­plications of these resources in other media have so far been lit­tle explored, 
recent trends indicates that artists will soon begin to use them creatively to produce new visual effects 
in dance shows, concerts, musicals and other spectacles. An important research di­rection that will allow 
this revolution to, in fact, take place is the development of authoring environments that serve as a 
bridge be­tween artists and the emerging technologies. The intention of this work is to adapt existing 
methods for lever­aging MoCap data to the context of creating dance content from artistic input and propose 
an authoring and collaboration platform for dance shows that integrates the creative elements that compose 
a choreography. In this way, we indicate how graphics technologies can be used not only to facilitate 
creation but also to suggest new forms of artistic expressions. 2 Overview We propose a platform for 
designing content of dance shows which covers all elements of the creative process and promotes the inter­action 
of dancers, musicians and choreographers. The dancer s movements are acquired from a MoCap setup and 
or­ganized in a variation of a motion graph [Kovar et al. 2002], which is structured and measure-synchronous. 
We create a measure­synchronous graph by capturing the movement of the dancers while they perform to 
speci.c music pieces and by using the synchroniza­tion between music and dance to segment the motion 
data accord­ing to the melodic phases. We then annotate the motion segments according to the dance steps 
they represent to create a structured graph (this can be done manually or using one of the many existent 
automatic methods). Although the purpose of this segmentation is to facilitate the combination of motion 
sequences in an application that makes extensive use of musical references, this structure can also be 
explored for motion editing. In this work, we take advan­tage of this structure for interpolating movements, 
for combining upper and lower body motions, for allowing rotations, and for in­serting amplitude variations 
and time warpings to make the group dance more natural. While the music s rhythm determines the duration 
of the steps and guides the segmentation of the motion, its melody is used to de­termine control signals 
that should be synchronized with motion events. These signals can either be extracted from a selected 
mu­sic and used as a reference to guide the dance or can be iteratively edited by both the musician and 
the choreographer in a scenario where the show is designed in a collaborative effort of both artists. 
We propose an intuitive interface for choreographers that integrates the contributions of the dancers 
and the musicians. The interface is based on a timeline which describes the musical measures and events. 
The choreographers can thus determine both the combina­tion of the dance steps for each performer and 
the group motions, i.e, the way the dancers move on stage creating formations and fol­lowing trajectories. 
Group motions can be speci.ed in three ways: still formations (shape and pattern speci.cations), boundaries 
(ini­tial and .nal formations that are matched following an optimization rule with collision control), 
and evolution rules (an initial position followed by a speci.cation of consecutive movements). Evolution 
rules can be declarative (e.g., a sketch of a trajectory on stage) or based on simulation and control 
mechanisms. In this work we explore behavioral animation methods [Reynolds 1999] that allow dancers to 
follow attraction/repulsion forces, avoid neighbors and obstacles, spread out on the stage, etc. An interesting 
aspect of the latter approach is that we can use musical events to guide the pro­cedural control signals, 
creating effects that would not be trivially speci.ed by a declarative method. 3 Discussion We propose 
an authoring environment that can both be used as a mechanism to synthesize virtual performances and 
as a tool to as­sist the planning of dance shows, allowing design and visualization of full motion sequences. 
Though it is highly applicable in conven­tional shows, guiding the artists through conception, production 
and execution, it also suggests a new paradigm for creation. In offering an integrated platform, this 
work promotes a new form of collab­oration between the artists, allowing the show to naturally evolve 
from iterative contributions of dancers, musicians and choreogra­phers. Future direction of this research 
should also integrate the contributions of the art director, such as staging and lighting. Other interesting 
applications of this framework and its extensions are on stage productions and improvisations, which 
would join efforts from choreographers, dancers and musicians in real time. An example of this would 
be a performance that combines live and virtual dancers projected on stage, whose movements are guided 
by the combination of different artistic inputs. In such scenarios, the artists would be able to in.uence 
not only the virtual dancers movements, but also one another through a framework of instant feedback. 
 References KOVAR, L., GLEICHER, M., AND PIGHIN, F. 2002. Motion graphs. In SIGGRAPH 02, ACM, New York, 
NY, USA. REYNOLDS, C. 1999. Steering behaviors for autonomous charac­ters. In Game Developers Conference. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037733</article_id>
		<sort_key>180</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>15</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Turning a graphics tablet into a transparent blackboard]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037733</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037733</url>
		<abstract>
			<par><![CDATA[<p>Most lectures in mathematics and in the natural sciences make prominent use of blackboards. Developing an idea is much more natural and interactive with hand-drawn sketches than with glitzy prepared slides. The art of drawing is undergoing a renaissance, which can be seen in literature [Roam 2008] as well as in educational videos on the Web. This work demonstrates a low-cost real-time system to visually enhance such lectures. Unlike related past approaches that started with [Ishii and Kobayashi 1992], it works with a minimal setup such as a tablet PC with integrated webcam.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809519</person_id>
				<author_profile_id><![CDATA[81408599742]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[J&#246;rn]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Loviscach]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fachhochschule Bielefeld (University of Applied Sciences)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[joern.loviscach@fh-bielefeld.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>142977</ref_obj_id>
				<ref_obj_pid>142750</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, H., and Kobayashi, M. 1992. ClearBoard: a seamless medium for shared drawing and conversation with eye contact. In <i>CHI '92</i>, 525--532.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Roam, D. 2008. <i>The Back of the Napkin: Solving Problems and Selling Ideas with Pictures</i>. Portfolio, Penguin, NY.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[RSA, 2011. RSA Animates. http://comment.rsablogs.org.uk/videos/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1294259</ref_obj_id>
				<ref_obj_pid>1294211</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Wigdor, D., Forlines, C., Baudisch, P., Barnwell, J., and Shen, C. 2007. Lucid touch: a see-through mobile device. In <i>UIST '07</i>, 269--278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Turning a Graphics Tablet into a Transparent Blackboard J¨orn Loviscach* Fachhochschule Bielefeld (University 
of Applied Sciences) 1 Introduction Most lectures in mathematics and in the natural sciences make prominent 
use of blackboards. Developing an idea is much more natural and interactive with hand-drawn sketches 
than with glitzy prepared slides. The art of drawing is undergoing a renaissance, which can be seen in 
literature [Roam 2008] as well as in educa­tional videos on the Web. This work demonstrates a low-cost 
real­time system to visually enhance such lectures. Unlike related past approaches that started with 
[Ishii and Kobayashi 1992], it works with a minimal setup such as a tablet PC with integrated webcam. 
A regular blackboard and a regular sheet of paper lack visual clarity as opposed to a computer s crisp 
output. Hence, it makes sense to use a graphics tablet or a Tablet PC in conjunction with appropri­ate 
drawing and writing software. This offers the bene.t of digi­tal storage, endless scrolling, and editing 
operations such as undo, cut/copy/paste, resize, rotate as well as precise and residue-free delete. Handwriting 
recognition enables automated indexing. Even though this recognition is of limited reliability, it is 
highly superior to recognizing writing on a .lmed regular blackboard. However, lectures presented on 
a graphics tablet and the pencasts produced from them suffer from two problems: First, the hand of the 
presenter is not visible. This makes it harder to follow the writ­ing and pointing, even though the tip 
of the digital pen may be highlighted on the screen. Second, the presenter s talking head is not visible 
in close vicinity to the writing. Sometimes the head is added in a distracting fashion as a separate, 
second video stream. 2 Techniques To address these issues, this work proposes a novel style and a novel 
technique for pencasts: The input from a regular graphics tablet or tablet PC controls an animated model 
of a writing hand. A web­cam captures the face of the presenter. These graphical ingredients are composed 
to create the illusion of the presenter writing on the back of a transparent board, see Fig. 1. This 
style is inspired by the visuals of science .ction movies which themselves borrow heav­ily from the plotting 
boards of historic .ghter-control centers as well as Hollywood s depictions of mathematics nerds such 
as the main character of A Beautiful Mind, who scribbles on windows. Color controls allow picking a highly 
subdued look for the face and the hand so that they convey just enough information. The hand is behind 
the board so that it never occludes the writing. The illusion of a transparent board is sold through 
re.ections on the imaginary surface and through a grid of guidelines that scroll with the blackboard 
s content. A soft glow below the tip indicates if the stylus touches the surface and shows the current 
drawing color. The hand and arm are 2D images animated through inverse kine­matics driven by the motion 
of the stylus. When the stylus leaves the detection range of the graphics tablet, the tip s position 
is no longer known and the system resorts to letting the arm drop out of the image with constant acceleration. 
When the presenter turns the stylus around to switch to the eraser tool, the image of the hand changes 
accordingly. For instance, the palm s hypothenar can be highlighted when it is used for erasing. *e-mail: 
joern.loviscach@fh-bielefeld.de Figure 1: A graphics tablet and a webcam emulate a transparent blackboard; 
additional styles include handwriting on paper. People are used to looking at mirrors, but not to interact 
with true­sided images of themselves. Hence, to not confuse the presenter, the hand and the face are 
shown horizontally .ipped. Note that an actual transparent blackboard requires writing in mirror script. 
Stereo sound is synthesized from the velocity, pressure, and loca­tion data of the graphics tablet. These 
control volume, pitch, and stereo position in real time. The sound is built from two layers of long seamless 
loops created from actual audio recordings of pen writing. The pen tool has a hissing base sound to which 
a squeaking sound is mixed if the pressure is high. For the eraser tool, only the squeaking sound is 
used. These sounds indicate when and where writing is going on with which intensity. 3 Outlook Future 
work will implement further styles. A second mode already simulates a hand drawing on a white background 
as found in the highly popular RSA Animates by Cognitive Media [RSA 2011]. A ruler to draw lines and 
a compass to draw circles can be added. A multitouch panel may be used to show both hands or even to 
an­imate several .ngers independently, which is of particular interest for rear-touch displays such as 
[Wigdor et al. 2007]. References ISHII, H., AND KOBAYASHI, M. 1992. ClearBoard: a seamless medium for 
shared drawing and conversation with eye contact. In CHI 92, 525 532. ROAM, D. 2008. The Back of the 
Napkin: Solving Problems and Selling Ideas with Pictures. Portfolio, Penguin, NY. RSA, 2011. RSA Animates. 
http://comment.rsablogs. org.uk/videos/. WIGDOR, D., FORLINES, C., BAUDISCH, P., BARNWELL, J., AND SHEN, 
C. 2007. Lucid touch: a see-through mobile de­vice. In UIST 07, 269 278. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037734</section_id>
		<sort_key>190</sort_key>
		<section_seq_no>4</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Games]]></section_title>
		<section_page_from>4</section_page_from>
	<article_rec>
		<article_id>2037735</article_id>
		<sort_key>200</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>16</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A framework for distributed audio smartphone games]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037735</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037735</url>
		<abstract>
			<par><![CDATA[<p>The explosive growth of smartphone sales in recent years has almost no comparison in the sector of consumer electronics. Sales in 2010 almost doubled those of 2009 year, with Android-based units showing a startling 1,580% growth in the last quarter of 2010, as compared to the same period the previous year [3]. Given such a proliferation of mobile computing power, there is much need for further exploration in the areas of distributed gaming, multimedia, networking, and audio for mobile devices. This project explores the use of smartphone technology as computing units for ad hoc gaming support. The underlying environment assumes a close-proximity setting, such as a classroom, conference room, or social setting (e.g. an activity area on the order of 15m). The work presented here is the first of four phases encompassing an over-arching research programme that explores the potential for networking smartphones in distributed audio-based applications:</p> <p>1. audio playback using smartphones of synchronized tracks for social gaming applications, providing opportunity for interactivity between participants;</p> <p>2. exploration of the role smartphones can play interacting with a smart-table in gaming;</p> <p>3. using smartphones to explore educational games relating to acoustics, using sound field effects created by physically moving the phones in a monitored environment.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809520</person_id>
				<author_profile_id><![CDATA[81488662651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[K.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Collins]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CCAT University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809521</person_id>
				<author_profile_id><![CDATA[81100546623]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[P.]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Taillon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[CCAT University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809522</person_id>
				<author_profile_id><![CDATA[81331495944]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[B.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kapralos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University Ontario Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809523</person_id>
				<author_profile_id><![CDATA[81488672941]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[J.-M.]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Trivi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Google, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Audio Engineering Society (AES), "Best Practices in Network Audio," <i>J. of Audio Engineering Society</i>, Vol.57, No.9, pp.729--741, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[A. Ault, et al., "<i>K</i>-Nearest-neighbor analysis of received signal strength distance estimation across environments," in <i>Proceedings of the First Workshop on Wireless Network Measurements</i>, WiNMee 2005, Trentino, Italy, April 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Gartner Inc., "Worldwide mobile phone sales grew 17 per cent in first quarter 2010," May 2010. {On-line}: Available: http://www.gartner.com/it/page.jsp?id=1372013.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Internet Engineering Task Force (IETF), "Real Time Streaming Protocol (RTSP)," Apr. 1998. {On-line}: Available: http://www.ietf.org/rfc/rfc2326.txt.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[X. Li, "Collaborative localization with received-signal strength in wireless sensor networks," <i>IEEE Trans. on Vehicular Tech.</i>, Vol.56, No.6, pp.3807--3817, 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1613945</ref_obj_id>
				<ref_obj_pid>1613858</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[A. S Shirazi, et al., "Poker surface: Combining a multi-touch table and mobile phones in interactive card games," in <i>Proc. of the 11th International Conference on Human-Computer Interaction with Mobile Devices and Services</i>, MobileHCI 2009, Bonn, Germany, Sept. 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Framework For Distributed Audio Smartphone Games K. Collins1, P.J. Taillon1, B. Kapralos2, J.-M. Trivi3 
1CCAT University of Waterloo, 2University Ontario Institute of Technology, 3Google, Inc. The explosive 
growth of smartphone sales in recent years has almost no comparison in the sector of consumer electronics. 
Sales in 2010 almost doubled those of 2009 year, with Android-based units showing a startling 1,580% 
growth in the last quarter of 2010, as compared to the same period the previous year [3]. Given such 
a proliferation of mobile computing power, there is much need for further exploration in the areas of 
distributed gaming, multimedia, networking, and audio for mobile devices. This project explores the use 
of smartphone technology as computing units for ad hoc gaming support. The underlying environment assumes 
a close-proximity setting, such as a classroom, conference room, or social setting (e.g. an activity 
area on the order of 15m2). The work presented here is the first of four phases encompassing an over-arching 
research programme that explores the potential for networking smartphones in distributed audio-based 
applications: 1. audio playback using smartphones of synchronized tracks for social gaming applications, 
providing opportunity for interactivity between participants; 2. exploration of the role smartphones 
can play interacting with a smart-table in gaming; 3. using smartphones to explore educational games 
relating to acoustics, using sound field effects created by physically moving the phones in a monitored 
environment.  We report on the research activity implementing the system framework in support of distributed 
audio gaming applications. The framework consists of two networking subsystems: media streaming from 
a server to the phones over WiFi;  a Bluetooth control network established between the phones, that 
enables initial relative position determination; information exchange; and network teardown.  The system 
architecture takes into account the bandwidth requirements for audio streaming discussed in [1], as well 
as the hardware and software capabilities of the phones. We use the Real Time Streaming Protocol (RTSP) 
to establish and control one (or more) time-synchronized streams of continuous audio data [4]. The server 
operates in a manner oblivious to the number of phones participating, i.e. it simply broadcasts the streams 
over WiFi and each phone node implements an RTSP app client. The field is generated by streaming a specific 
audio channel to each phone, requires a preliminary phase whereby the phones determine which stream they 
should play to produce the desired effect, based on their relative position [2,5]. The system employs 
lightweight communication between nodes using the control network, to minimize any impact on the audio 
playback. To create additional effects, the phones need only exchange short messages containing server 
URL or channel information. Ongoing experiments have produced interesting observations related to the 
use of smartphones in audio game activity, in regard to sound field quality as a function of the speaker 
limitations, and audio synchronization and phasing issues. In addition to the future work as outlined 
above, use of the current framework provides immediate game development opportunities: for example, we 
will explore a music-based game where streamed audio data (MIDI) of musical sequences is sent as individual, 
separate music tracks to a series of players who have a synchronized music sequencer on their phone; 
players can re-mix the track in real-time, playing a kind of musical telephone game where they change 
a note or beat and send their selected sequence on to the next person; eventually the sequence returns 
to them, altered from its original but still fitting with the other tracks that are simultaneously being 
altered. References [1] Audio Engineering Society (AES), Best Practices in Network Audio, J. of Audio 
Engineering Society, Vol.57, No.9, pp.729-741, 2009. [2] A. Ault, et al., K-Nearest-neighbor analysis 
of received signal strength distance estimation across environments, in Proceedings of the First Workshop 
on Wireless Network Measurements, WiNMee 2005, Trentino, Italy, April 2005. [3] Gartner Inc., Worldwide 
mobile phone sales grew 17 per cent in first quarter 2010, May 2010. [On-line]: Available: http://www.gartner.com/it/page.jsp?id=1372013. 
[4] Internet Engineering Task Force (IETF), Real Time Streaming Protocol (RTSP), Apr. 1998. [On-line]: 
Available: http://www.ietf.org/rfc/rfc2326.txt. [5] X. Li, Collaborative localization with received-signal 
strength in wireless sensor networks, IEEE Trans. on Vehicular Tech., Vol.56, No.6, pp.3807­3817, 2007. 
[6] A. S Shirazi, et al., Poker surface: Combining a multi-touch table and mobile phones in interactive 
card games, in Proc. of the 11th International Conference on Human-Computer Interaction with Mobile Devices 
and Services, MobileHCI 2009, Bonn, Germany, Sept. 2009. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037736</article_id>
		<sort_key>210</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>17</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Character shading in EA Sports MMA<sup>#8482;</sup> using projected Poisson disk based ambient occlusion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037736</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037736</url>
		<abstract>
			<par><![CDATA[<p>In computer graphics, Ambient Occlusion (AO) is one of the quickest ways to add realism to a scene by approximating the effects of global illumination. While there are many different ways of approximating the effects of AO; Screen Space AO (SSAO) techniques have been utilized by plenty of games as these algorithms take full advantage of the custom fragment shading capabilities of today's GPUs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809254</person_id>
				<author_profile_id><![CDATA[81488671502]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Volga]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aksoy]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Tiburon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[vaksoy@ea.com]]></email_address>
			</au>
			<au>
				<person_id>P2809255</person_id>
				<author_profile_id><![CDATA[81488671913]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gerald]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Phaneuf]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Electronic Arts Tiburon]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gphaneuf@ea.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401061</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., Sainz, M., and Dimitrov, R. 2008. Image-space horizon-based ambient occlusion. In SIGGRAPH '08: ACM SIGGRAPH 2008 talks.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1404441</ref_obj_id>
				<ref_obj_pid>1404435</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Filion, D., and Mcnaughton, R. 2008. Effects & techniques. In SIGGRAPH '08: ACM SIGGRAPH 2008 classes, 133--164.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hargreaves, Shawn, Deferred Shading, http://www.talula.demon.co.uk/DeferredShading.pdf]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 CharacterShadinginUsing EASportsMMA s ProjectedPoissonOcclusions DiskBasedAmbient VolgaAksoye ElectronicArtsTiburone 
vaksoy@ea.come e  Figure1 :(above)OurAOtechniqueappliedtoEASportsMMA e andeache frame. (below)correspondingSSAObuffersrenderedfore 
e Ine computergraphics,AmbientOcclusione (AO)isone e ofthee quickestwaystoaddrealismtoascenebyapproxim 
atingthee effectse ofglobalillumination.e Whilee theree aree man ye differente wayse ofapproximatinge 
theeffectsofe AO;Screene Spa cee AOe beenutilizedplentyofg(SSAO)techniqueshavebyamesasthesee algorithmse 
takefulladvantageofe thee customfragme nte shadinge capabilities oftoday sGPUs.ForEASportsMMA ,thedynamicnatureofhowtwof 
ighterse caninteractwithutili adynamicAO eachotherrequiredthatwezee solutione insteadofresortingtoe prebakedAOe 
soluti ons.e Initiallye wee experimentede withe SSAOe algorithmse likee Blizzard se hemisphere-samplede 
AO[Filione ete al],andNVidia se horizon­basede AO[Bavoilete al] h techniques.e Unfortunatelybudgetarye 
constraintse one GPUcyclesforcede use toe investigatee fastere alternatives.e Heree wepresentourmethode 
fore calculatingSSAOwhi che wase utilizedinEASportsMMA .Thestrengthofthetec hniqueise thatitisfastenoughtoallowEASportsMMA 
tor unat60fpse duringinteractivegameplayonboththePlayStation 3andXboxe 360 platformswhilemaintaininggoodvisualquality 
(Figure1).e e AlgorithmUsingProjectedPoissonDisks1h AO 1. Usingeye-spacenormalandpositionbufferse 2. 
 foreachh fragment(as )e 3. Resizes pace Poisson-Disktoremainconsistentinworld 4. let=0 floatAO 
 5. fore .) eachh sampleinPoisson-Discaround (as 6. leth = .position) vectorh aNormalize( ..position 
 7. let=a, float DotComparedot( .normal)-dotBiase 8. ifDotCompare<0e 9. AO++e 10. elsee 11. AO- 
.position)) +=saturate(length( ..position 12.  endfore 13. AO Normalize(i.e.AO/=NumberOfDiscSamples)e 
 14. end  for e LikemostSSAOtechniques,werequireaview-spacee normalande positione buffertoe befede intothee 
fragmentshader. e However,e unlikemostSSAOtechniquesthatrelyonasphereo rhemispheree Gerald Phaneufe ElectronicArtsTiburone 
gphaneuf@ea.com ofe evenlydistributede randomsamples[Filionande Mc Naughtone 2008],weusea2DPoisson-Diskkernel.Specificall 
y,weprojecte the2Ddiskkernelintotheview-spaceandcheckto seeifeache projectedsamplee .fallsbehindtheplanedefinedbythenormalofe 
thefragment atthecenterofthedisk(line7in Algorithm1).Ife samplee . sorientedplane(i.e.DotCompare>afallsbehind=0),e 
thenweusetheviewspacedistancebetweenthepos itionof ande .todeterminetheocclusioncontributiontothefin 
alAO(line11e inAlgorithm1).Thisallowsfore asmoothAOfallof fasfurthere samplese contributelesstothee finale 
AO.Inordert oe maintainae consistente kernelsizee inworldspace,wee resizeou re kernelsizee basede onfragment 
sdepthandprojectionmatrix( linee 3ine Algorithm 1). Figure2 :Conceptualoverviewofhowtheocclusioncontributi 
onise calculatedfortwoseparatescreen-spacefragments . s Applications InEASportsMMA ourSSAOtechniquerunswith32s 
amplese onawidediskkernelinadditionto16sampleswit hasmallere diskkerneltocapturethefinecreviceslikethef 
ighters faciale detail.e Thegeneratede SSAOe bufferise 480x270e ande th ee stablee natureofthetechniquedidnotrequireustoapply 
ablurpass.e WhensamplingthefinalSSAObufferweinitiallyus edbilaterale upsamplinge whiche latere provede 
toe bee unnecessarye and e wase consequentlyremoved.Unlikemostgamesthatutiliz eSSAO,wee onlycalculateSSAOforthefighters.Thisallowede 
ustofurthere optimizee thetechniquee byskippinge overunnecessary e fragmentse viae thee stencile buffere 
testing.e Anye dynamice AO on e thee surroundingenvironmentiscalculatedbyothermean sunrelatede to thisalgorithm.Ourtechniqueisstraightforwardtointegrateinto 
othergamese andweassumethatthecostassociatedwiththegen erationofthee view-spacee positionandnormale buffersaree 
independ ente ofthee finalSSAOtechniqueused,whichcanbehiddenift hegameusese a]. deferredshadingtechnique[Hargreaves 
Initsfinalform,ourSSAObuffergenerationtechn iqueusese 0.8mse ine EASportse MMA e duringe oure 60-fps-worst-cas 
ee situation3Xbox onboththePlayStationand360. e Referencess BAVOIL,e L.,e SAINZ,e M.,e ANDe DIMITROV,e 
R.e 2008.e IMAGE-SPACEe HORIZON-BASEDAMBIENTOCCLUSION .e INe SIGGRAPHe 08:e ACMe SIGGRAPHe 2008 TALKS 
.FILION,e D.,e ANDe MCNAUGHTON,e R.e e EFFECTSe &#38; 2008.TECHNIQUES .INe SIGGRAPHe e ACMe e 2008e 08:SIGGRAPHCLASSES 
,133 164.HARGREAVES,e SHAWN,e DEFERREDe SHADING,e HTTP://WWW.TALULA.DEMON.CO.UK/DEFERREDSHADING.PDFe 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037737</article_id>
		<sort_key>220</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>18</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Design goal-oriented level design]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037737</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037737</url>
		<abstract>
			<par><![CDATA[<p>Design Goal-Oriented Level Design is our new process for linking mechanics, narrative and environment design into one streamlined document for use by level designers. The most significant innovation of this research is the creation of a formal grammar of level design terms. This work is influenced by the previous work [Bj&#246;rk et al. 2003] on a formal grammar for all aspects of game design. It was also influenced by [Vick etal. 2010], from which we adopted the process of abstracting any gameplay mechanic and categorizing it as a subset of a larger categorization of mechanics.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809256</person_id>
				<author_profile_id><![CDATA[81547882156]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Eric]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809257</person_id>
				<author_profile_id><![CDATA[81488664815]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Brian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[May]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Rochester Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bjork, S., Lundgren, S. and Holopainen, J., Game Design Patterns. in <i>Level Up: Digital Games Research Conference 2003</i>, (Utrecht, The Netherlands, 2003).]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836139</ref_obj_id>
				<ref_obj_pid>1836135</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Erik Henry Vick, Rudy McDaniel, and Stephen Jacobs. 2010. Using semiotic grammars for the rapid design of evolving video game mechanics. In <i>Proceedings of the 5th ACM SIGGRAPH Symposium on Video Games</i> (Sandbox '10), Stephen N. Spencer (Ed.). ACM, New York, NY, USA, 25--30. DOI=10.1145/1836135.1836139 http://doi.acm.org/10.1145/1836135.1836139]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Design Goal-Oriented Level Design  Eric Baker, Brian May Rochester Institute of Technology  (a)  
(b) Figure 1: (a) Pace digraph, (b) level layout sketch from pace diagraph. Design Goal-Oriented Level 
Design is our new process for linking mechanics, narrative and environment design into one streamlined 
document for use by level designers. The most significant innovation of this research is the creation 
of a formal grammar of level design terms. This work is influenced by the previous work [Björk et al. 
2003] on a formal grammar for all aspects of game design. It was also influenced by [Vick etal. 2010], 
from which we adopted the process of abstracting any gameplay mechanic and categorizing it as a subset 
of a larger categorization of mechanics. By applying these concepts to level design it is possible to 
design levels focusing on the pace of the level as well as the specifics of gameplay mechanics. Another 
significant accomplishment of this process is the ability to pre-visualize an entire level before any 
design or construction has taken place thanks to the pacing digraph. By using this piece of the document, 
we can see exactly how the level will play in a moment-to-moment scale. This visualization is extremely 
helpful and quickly makes major problems very apparent. Another significant accomplishment of this process 
is the ability to pre-visualize an entire level before any design or construction has taken place thanks 
to the pacing digraph. By using this piece of the document, we can see exactly how the level will play 
in a moment-to-moment scale. This visualization can help to identify problems with the pace of a level 
quickly. In addition to quickly finding problems, this technique can allow a designer to very rapidly 
include details into a level that will achieve very specific design goals without having to shoe-horn 
them in after construction. By taking a very top-down approach to the design, the designer is able to 
very effectively mold the level to be as effectively as possible. References Bjork, S., Lundgren, S. 
and Holopainen, J., Game Design Patterns. in Level Up: Digital Games Research Conference 2003, (Utrecht, 
The Netherlands, 2003). Erik Henry Vick, Rudy McDaniel, and Stephen Jacobs. 2010. Using semiotic grammars 
for the rapid design of evolving video game mechanics. In Proceedings of the 5th ACM SIGGRAPH Symposium 
on Video Games (Sandbox '10), Stephen N. Spencer (Ed.). ACM, New York, NY, USA, 25-30. DOI=10.1145/1836135.1836139 
http://doi.acm.org/10.1145/1836135.1836139 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037738</article_id>
		<sort_key>230</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>19</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Tiled directional flow]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037738</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037738</url>
		<abstract>
			<par><![CDATA[<p>For a real-time visualization of one of the Dutch harbors we needed a realistic looking water surface. The old shader showed the same waves everywhere, but inside a harbor waves have many different directions and sizes. To solve this problem we needed a shader capable of visualizing flow. We developed a new algorithm called Tiled Directional Flow which has several advantages over other implementations.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809258</person_id>
				<author_profile_id><![CDATA[81488668601]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Frans]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[van Hoesel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Groningen, The Netherlands]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>846297</ref_obj_id>
				<ref_obj_pid>846276</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fabrice Neyret, "Advected Textures", Eurographics/ACMSIGGRAPH Symposium on Computer Animation, 2003]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1028553</ref_obj_id>
				<ref_obj_pid>1028523</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Stephen Chenney, "Flow Tiles", Eurographics/ACMSIGGRAPH Symposium on Computer Animation, 2004]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Qizhi Yu, Fabrice Neyret, Eric Bruneton, Nicolas Holzschuch, "Scalable Real-Time Animation of Rivers", Eurographics 2009]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Alex Vlachos, "Water Flow in Left 4 Dead 2", Siggraph presentation, 2010]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Tiled Directional Flow Frans van Hoesel, University of Groningen, The Netherlands  1 Introduction 
For a real-time visualization of one of the Dutch harbors we needed a realistic looking water surface. 
The old shader showed the same waves everywhere, but inside a harbor waves have many different directions 
and sizes. To solve this problem we needed a shader capable of visualizing flow. We developed a new algorithm 
called Tiled Directional Flow which has several advantages over other implementations. After trying something 
different first, we got inspired by the Siggraph 2010 presentation Water Flow in Left 4 Dead 2 by Alex 
Vlachos from Valve. In general, animating the flow by simply distorting the texture coordinates of a 
sliding normalmap for directing waves in a certain direction, looks only good for a short time. That 
s something we have seen, but is more clearly demonstrated in Vlachos presentation. He solved that by 
introducing a second normalmap and slowly go from one map to the other, by changing a blendfactor over 
time. At the point a map becomes invisible, he resets the sliding texture coordinates, so the animation 
can start from the beginning (where the distortions are still acceptable good, and look like the desired 
flow). A disadvantage is that it only works for non-directional wave patterns. If you looked at a still 
from the animation, you couldn t see in which direction the waves are travelling. Another disadvantage 
is that the changing blendfactor introduces a visible pulsing behaviour. Vlachos did hide that, by introducing 
noise 2 Our Approach Our contribution is a new algorithm, implemented in a shader that generates overlapping 
tiles. We used a low resolution flow texture for local speed, direction and size of the waves. Within 
a tile, the speed, direction and size of the waves is constant, so there is no problem with animating 
the waves, as this can simply be implemented using a sliding normalmap. Using overlapping tiles, each 
point has a contribution of four normals. Adding the different normalmaps of each tile together results 
in nice animated waves, which don t look like sliding normalmaps anymore. The contribution of each tiles 
goes slowly to zero near the edge of the tile. This would result in a visible effect, because the addition 
of many normalvectors together tends to create a more upright vector. In the shader this is prevented 
by assuming the waves behave like noise and by scaling the normal to compensate for this effect (adding 
N noise sources together, each with amplitude A creates a noise with an amplitude of sqrt(N)*A). To maximize 
the area where each tile adds to the animation of the waves, the contribution doesn t go linear to zero, 
but with a third power. Note that the blendfactor doesn t change over time and avoids the pulsating behaviour 
from Vlachos implementation. The end result is a very visually appealing animation that shows a strong 
correlation to the flow direction, even when the frame stops. A short video explaining the algorithm 
and a link to the source code can be found at http://goo.gl/RZByt. and http://goo.gl/gvct. Related work 
Fabrice Neyret, Advected Textures , Eurographics/ACMSIGGRAPH Symposium on Computer Animation, 2003 Stephen 
Chenney, Flow Tiles , Eurographics/ACMSIGGRAPH Symposium on Computer Animation, 2004 Qizhi Yu, Fabrice 
Neyret, Eric Bruneton, Nicolas Holzschuch, Scalable Real-Time Animation of Rivers , Eurographics 2009 
Alex Vlachos, Water Flow in Left 4 Dead 2 , Siggraph presentation, 2010  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037739</section_id>
		<sort_key>240</sort_key>
		<section_seq_no>5</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Hardware]]></section_title>
		<section_page_from>5</section_page_from>
	<article_rec>
		<article_id>2037740</article_id>
		<sort_key>250</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>20</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A new model for adaptive displays based on von Kries hypothesis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037740</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037740</url>
		<abstract>
			<par><![CDATA[<p>Color appearance of images on self-luminous displays, such as LCD monitors, depends on two sources of illumination: scene illumination within the image itself and ambient illumination in the viewing environment. Color appearance models (CAMs) attempt to predict how perception changes as ambient illumination changes. At their best they provide viewers with an illumination-independent viewing experience. However, existing CAMs are complex and application-specific. Thus they are poorly suited to adaptive displays. For example, existing adaptive displays provide only achromatic adaptation, limiting the complexity of CAMs and minimizing tuning. We propose a new model based on von Kries adaptation, the theory on which most CAMs are based upon. It takes into account the mixed adaptation state produced by self-luminous displays. Compared to existing CAMs, our model is simpler and more efficient, yet shares the generic features of more complex CAMs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809259</person_id>
				<author_profile_id><![CDATA[81488647679]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cherry]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo, Waterloo, Ontario, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[c23zhang@uwaterloo.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809260</person_id>
				<author_profile_id><![CDATA[81442603072]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Bill]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cowan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo, Waterloo, Ontario, Canada]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[wmcowan@cgl.uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Braun, K., and Fairchild, D. 1997. Testing Five Color Appearance Models for Changes in Viewing Conditions. In <i>Color Research & Application</i>, vol. 33, 165--173.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fairchild, M. D. 2005. <i>Color Appearance Models</i>, 118--204. Wiley-IS&T, Chichester, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Fairchild, M. D. 2005. <i>Color Appearance Models</i>, 215--300. Wiley-IS&T, Chichester, UK.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037741</article_id>
		<sort_key>260</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>21</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Design and optimization of image processing algorithms on mobile GPU]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037741</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037741</url>
		<abstract>
			<par><![CDATA[<p>The advent of GPUs with programmable shaders on mobile phones has motivated developers to utilize GPU to offload computationally intensive tasks and relive the burden of embedded CPU. In this paper, we present a set of metrics to measure characteristics of a mobile phone GPU with the focus on image processing algorithms. These measures assist users in design and implementation stage and in classifying bottlenecks. We propose techniques to achieve increased performance with optimized shader design. To show the effectiveness of the proposed techniques, we employ cartoon-style non-photorealistic rendering (NPR), belief propagation (BP) stereo matching [Yang et al. 2006], and speeded up robust features (SURF) detection [Bay et al. 2008] as our example algorithms.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809261</person_id>
				<author_profile_id><![CDATA[81436594342]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nitin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singhal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Samsung Electronics Co. Ltd., Suwon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[n.singhal@samsung.com]]></email_address>
			</au>
			<au>
				<person_id>P2809262</person_id>
				<author_profile_id><![CDATA[81488654619]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jin]]></first_name>
				<middle_name><![CDATA[Woo]]></middle_name>
				<last_name><![CDATA[Yoo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inha University, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809263</person_id>
				<author_profile_id><![CDATA[81488658341]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ho]]></first_name>
				<middle_name><![CDATA[Yeol]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inha University, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809264</person_id>
				<author_profile_id><![CDATA[81100157808]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[In]]></first_name>
				<middle_name><![CDATA[Kyu]]></middle_name>
				<last_name><![CDATA[Park]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Inha University, Incheon, Korea]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pik@inha.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1370556</ref_obj_id>
				<ref_obj_pid>1370312</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bay, H., Ess, A., Tuytelaars, T., and Gool, L. V. 2008. SURF: Speeded up robust features. <i>Computer Vision and Image Understanding 110</i>, 3, 346--359.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Yang, Q., Wang, L., Yang, R., Wang, S., Liao, M., and Nist&#233;r, D. 2006. Real-time global stereo matching using hierarchical belief propagation. In <i>Proc. British Machine Vision Conference</i>, 989--998.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Design and Optimization of Image Processing Algorithms on Mobile GPU Nitin Singhal* Samsung Electronics 
Co. Ltd., Suwon 443-742, Korea 1 Introduction The advent of GPUs with programmable shaders on mobile 
phones has motivated developers to utilize GPU to of.oad computation­ally intensive tasks and relive 
the burden of embedded CPU. In this paper, we present a set of metrics to measure characteristics of 
a mobile phone GPU with the focus on image processing algorithms. These measures assist users in design 
and implementation stage and in classifying bottlenecks. We propose techniques to achieve increased performance 
with optimized shader design. To show the effectiveness of the proposed techniques, we employ cartoon­style 
non-photorealistic rendering (NPR), belief propagation (BP) stereo matching [Yang et al. 2006], and speeded 
up robust features (SURF) detection [Bay et al. 2008] as our example algorithms. 2 Metrics to Characterize 
Mobile GPUs Memory Transfer Bandwidth: On a mobile phone application processor, the memory is shared 
between CPU and GPU. However, textures have to be properly wrapped for the graphics core and can­not 
be accessed directly as the CPU image array. The overhead in­curred in texture wrapping serves as a signi.cant 
bottleneck when operating on large memory buffers. For CPU (ARM CORTEX A8 @1GHz) and GPU (POWERVR SGX 
540 @200MHz), the mem­ory bandwidth (processor to memory) stands at 220.54 MB/s (CPU to GPU) and 30.92 
MB/s (GPU to CPU); Floating Point vs. Fixed Point: Mobile phone GPU such as POWERVR SGX 540, has fast 
vectored .oats compared to VF-PLite hardware accelerator in ARM CORTEX A8 CPU. As a result, an image 
processing algorithm with high .oating point arithmetic is likely to have high acceleration when implemented 
on a mobile phone GPU as compared to the CPU. For example, a 5×5 Gaus­sian .lter, implemented on a mobile 
phone GPU shows 30x and 2x speedup in comparison to CPUs .oating and .xed point implemen­tation, respectively; 
Shader Instruction vs. Rendering Cycles: On a mobile phone GPU, the number of instruction slots for a 
vertex and fragment shader is limited. There is a trade-off between combining multi­ple rendering cycles 
in a single pass and splitting a single pass into multiple rendering cycles. Packing multiple rendering 
cycles into a single fragment shader increases the instruction count. On the other hand, increasing the 
number of rendering cycles reduces the parallel fraction. The best solution depends on the needs of the 
application. 3 Performance Optimization Techniques We present optimization techniques which are customized 
for im­age processing and address the need for compact shader code that matches the smaller hardware 
limits of a mobile phone GPU. Precision Control : Shaders use precision such as highp, medi­ump, and 
lowp, to provide hints to the compiler on how the variable is used. The lowp is useful for representing 
colors in the 0.0 to 1.0 range. Choosing a lower precision increases the shader perfor­ mance. Loop Unrolling 
: To process a loop (for or while), a shader needs more instructions in increment and comparison. Eliminating 
loop by either an optimized unrolling or using vectors to perform the operations results in lower shader 
instruction count. *email: n.singhal@samsung.com pik@inha.ac.kr Jin Woo Yoo Ho Yeol Choi In Kyu Park 
Inha University, Incheon 402-751, Korea Table 1: Fragment shader optimization example. Optimization 
Instruction Count Execution Time (ms) Basic 532 537.63 Loop Unroll 181 105.93 Load Sharing 150 90.25 
Precision Control 69 48.90 Table 2: Execution time comparison (in milliseconds). Algorithm Parameters 
CPU GPU Speedup NPR 800×480, 5×5 mask 1545.7 242.7 6.37x BP Stereo Matching 384×288, 4 iterations 6976 
1086 6.42x 384×288, 8 iterations 12161 2083 5.84x 384×288, 12 iterations 17413 3125 5.57x SURF 800×480 
1703 943 1.81x Load Sharing : Accessing neighborhood color values in a frag­ment shader commonly results 
in dependent texture read, which results in a stall until the texture information is retrieved. To avoid 
dependent texture read, a straightforward way is to pre-compute neighboring texture coordinates in a 
vertex shader. Texture Compression : Texture compression helps in reducing the memory transfer overhead. 
If the texture cannot be compressed, a lower precision pixel format such as RGB565, RGBA5551, or RGBA444 
should be used. Table 1 shows the acceleration achieved after different optimization steps on a 5×5 Gaussian 
.lter fragment shader. 4 Experimental Results Table 2 shows the acceleration results for the test algorithmss. 
All CPU implementation uses .xed point arithmetic. In case of cartoon-style NPR, the large number of 
dependent texture lookups force the GPU to execute instructions sequentially, which limit the speedup 
achieved. High memory access intensity coupled with in­tensive logical operations signi.cantly increases 
the execution time for BP stereo matching GPU implementation. SURF implementa­tion achieves the lowest 
speedup. The overhead incurred in group­ing and splitting the integral image values into RGBA texture 
com­ponents in one of the major bottleneck. Additionally, the large num­ber of rendering cycles, inherently 
limit the maximum speedup. Acknowledgement This research was supported by The Ministry of Knowledge 
Econ­omy, Korea and Samsung Electronics CO., LTD., under IT/SW Creative research program supervised by 
the NIPA (NIPA-2010­(C1810-1004-0002)). References BAY, H., ESS, A., TUYTELAARS, T., AND GOOL, L. V. 
2008. SURF: Speeded up robust features. Computer Vision and Image Understanding 110, 3, 346 359. YANG, 
Q., WANG, L., YANG, R., WANG, S., LIAO, M., AND NIST ´ ER, D. 2006. Real-time global stereo matching 
using hi­erarchical belief propagation. In Proc. British Machine Vision Conference, 989 998. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>The Ministry of Knowledge Economy, Korea and Samsung Electronics CO., LTD., under IT/SW Creative research program supervised by the NIPA</funding_agency>
			<grant_numbers>
				<grant_number>NIPA-2010-(C1810-1004-0002)</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2037742</article_id>
		<sort_key>270</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>22</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[HORT]]></title>
		<subtitle><![CDATA[Hadoop online ray tracing with mapreduce]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037742</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037742</url>
		<abstract>
			<par><![CDATA[<p>High quality computer-generated imagery (CGI) rendering is a demanding computational task [Hearn and Baker 1997]. To generate high-quality CGI, film studios rely on expensive, specialized, rendering clusters (render farms) [Rath 2009]. Render farms cost a premium over more general infrastructure such as infrastructure as a service (IaaS) offerings. Furthermore, render farms often provide limited selection of commercial rendering applications and have significant overhead with respect to requesting a job. This contrasts general IaaS offerings such as Amazon's Elastic Compute Cloud (EC2), which allows for straight-forward, automated provisioning of many virtualized machine instances.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809265</person_id>
				<author_profile_id><![CDATA[81466647327]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lesley]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Northam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lanortha@cs.uwaterloo.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809266</person_id>
				<author_profile_id><![CDATA[81490682237]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rob]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Smits]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rdfsmits@cs.uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bialecki, A., et al., 2005. Hadoop: a framework for running applications on large clusters built of commodity hardware. http://lucene.apache.org/hadoop.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1327492</ref_obj_id>
				<ref_obj_pid>1327452</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Dean, J., and Ghemawat, S. 2008. MapReduce: Simplified data processing on large clusters. <i>Communications of the ACM 51</i>, 1, 107--113.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>231010</ref_obj_id>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Hearn, D., and Baker, M. P. 1997. <i>Computer graphics (2nd ed.): C version</i>. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rath, J., 2009. The data-crunching powerhouse behind 'avatar'. http://www.datacenterknowledge.com/archives/2009/12/22/the-data-crunching-power, December.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 HORT: Hadoop Online Ray Tracing with MapReduce Lesley Northam* Rob Smits University of Waterloo  Figure 
1: The HORT rendering pipeline. CR Categories: I.3.2 [Computer Graphics]: Graphics Systems Distributed/network 
graphics; Keywords: ray tracer, mapreduce, high-performance rendering 1 Introduction High quality computer-generated 
imagery (CGI) rendering is a de­manding computational task [Hearn and Baker 1997]. To generate high-quality 
CGI, .lm studios rely on expensive, specialized, ren­dering clusters (render farms) [Rath 2009]. Render 
farms cost a premium over more general infrastructure such as infrastructure as a service (IaaS) offerings. 
Furthermore, render farms often provide limited selection of commercial rendering applications and have 
signi.cant overhead with respect to requesting a job. This contrasts general IaaS offerings such as Amazon 
s Elastic Compute Cloud (EC2), which allows for straight-forward, automated provisioning of many virtualized 
machine instances. To evaluate the effectiveness of rendering in a general IaaS envi­ronment, we have 
built and tested HORT, a MapReduce ray tracer which runs as a Hadoop ([Dean and Ghemawat 2008]) task. 
 2 Background MapReduce is a programming paradigm and framework for batch processing large sets of data 
distributed over a cluster of commod­ity systems [Dean and Ghemawat 2008]. MapReduce is successful because 
it allows different types distributed batch jobs to be written in a similar way, and it allows these 
jobs to be executed, managed and monitored on the same infrastructure consistently. The MapReduce implementation 
as described by Dean et al. is a proprietary system. For our experiments, we use Hadoop, a popular open 
source Java implementation of MapReduce [Bialecki et al. 2005]. Amazon offers several cloud computing 
products branded under the banner Amazon Web Services (AWS). AWS s IaaS offering is branded as Elastice 
Cloud Computing (EC2). To simplify the cre­ation of running Hadoop jobs in EC2, Amazon released a service 
called Elastic MapReduce (EMR) which runs Hadoop. Our HORT experiments use EMR, although HORT could run 
on any Hadoop deployment. *e-mail: lanortha@cs.uwaterloo.ca e-mail: rdfsmits@cs.uwaterloo.ca 3 Approach 
To utilize the MapReduce programming model, HORT is com­prised of several C++ programs which split the 
ray tracing pipeline into stages. Speci.cally, ray casting and shading operations are distinct MapReduce 
jobs, allowing for greater user control over job distribution. Additionally, we identi.ed that sending 
large scene graphs (hundreds of megabytes to gigabytes) over the network to each MapReduce worker introduces 
large delays (transfer time) and unnecessary data replication. To solve this issue we split the scene 
graph into equal-size blocks, and each MapReduce worker receives a subset of blocks instead of the entire 
graph. The scene graph is split by decomposing meshes into single­intersection components (i.e., individual 
triangles) and then com­ponents are divided into k user-speci.ed blocks with an equal num­ber of components. 
Map workers perform computation (e.g., ray­object intersection tests) on their blocks, and Reduce workers 
com­bine results to produce the .nal answer. For example, Map workers from the ray casting stage produce 
a list of intersection points, and the corresponding Reduce workers .nd the intersection nearest the 
camera. We executed several experiments with HORT using EMR to de­termine the feasibility of using IaaS 
services for rendering. We found that HORT continues to show improved performance as more machine instances 
are made available. To demonstrate fault toler­ance, we intentionally terminated an EC2 node used by 
our Hadoop cluster during a rendering job and observed its successful comple­tion. Finally, we noted 
that adjusting the number of instances and MapReduce workers is .exible and user-friendly. References 
BIALECKI, A., ET AL., 2005. Hadoop: a framework for run­ning applications on large clusters built of 
commodity hardware. http://lucene.apache.org/hadoop. DEAN, J., AND GHEMAWAT, S. 2008. MapReduce: Simpli.ed 
data processing on large clusters. Communications of the ACM 51, 1, 107 113. HEARN, D., AND BAKER, M. 
P. 1997. Computer graphics (2nd ed.): C version. Prentice-Hall, Inc., Upper Saddle River, NJ, USA. RATH, 
J., 2009. The data-crunching pow­erhouse behind avatar . http://www. datacenterknowledge.com/archives/2009/ 
12/22/the-data-crunching-power, December. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037743</article_id>
		<sort_key>280</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>23</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Integrating multiple depth sensors into the virtual video camera]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037743</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037743</url>
		<abstract>
			<par><![CDATA[<p>In this ongoing work, we present our efforts to incorporate depth sensors [Microsoft Corp 2010] into a multi camera system for free view-point video [Lipski et al. 2010]. Both the video cameras and the depth sensors are consumer grade.</p> <p>Our free-viewpoint system, the Virtual Video Camera, uses image-based rendering to create novel views between widely spaced (up to 15 degrees) cameras, using dense image correspondences. The introduction of multiple depth sensors into the system allows us to obtain approximate depth information for many pixels, thereby providing a valuable hint for estimating pixel correspondences between cameras.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data acquisition]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809267</person_id>
				<author_profile_id><![CDATA[81488665176]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ruhl]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ruhl@cg.cs.tubs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809268</person_id>
				<author_profile_id><![CDATA[81385596095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[berger@cg.cs.tubs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809269</person_id>
				<author_profile_id><![CDATA[81365598480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lipski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lipski@cg.cs.tubs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809270</person_id>
				<author_profile_id><![CDATA[81385600572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Felix]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Klose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[klose@cg.cs.tubs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809271</person_id>
				<author_profile_id><![CDATA[81488673379]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yannic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schroeder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[schroeder@cg.cs.tubs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809272</person_id>
				<author_profile_id><![CDATA[81384596118]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Alexander]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scholz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[scholz@cg.cs.tubs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809273</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>7</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Computer Graphics Lab, TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[magnor@cg.cs.tubs.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lipski, C., Linz, C., Berger, K., Sellent, A., and Magnor, M. 2010. Virtual video camera: Image-based viewpoint navigation through space and time. <i>Computer Graphics Forum 29</i>, 8, 2555--2568.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Microsoft Corp, 2010. Kinect for Xbox 360, November. Redmond WA.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Integrating Multiple Depth Sensors into the Virtual Video Camera Kai Ruhl, Kai Berger, Christian Lipski, 
Felix Klose, Yannic Schroeder, Alexander Scholz, Marcus Magnor * Computer Graphics Lab,TU Braunschweig 
 Figure 1: The integration of low-resolution depth sensors into the Virtual Video Camera. Our new calibration 
pattern consists of a re.ective­diffuse-checkerboard, which is clearly visible both in the depth and 
image sensors (two left). The additional depth information can be used as a soft hint to guide the image 
correspondence estimation needed for spatial image interpolation (two right) Abstract In this ongoing 
work, we present our efforts to incorporate depth sensors [Microsoft Corp 2010] into a multi camera system 
for free view-point video [Lipski et al. 2010]. Both the video cameras and the depth sensors are consumer 
grade. Our free-viewpoint system, the Virtual Video Camera, uses image­based rendering to create novel 
views between widely spaced (up to 15 degrees) cameras, using dense image correspondences. The introduction 
of multiple depth sensors into the system allows us to obtain approximate depth information for many 
pixels, thereby pro­viding a valuable hint for estimating pixel correspondences between cameras. Keywords: 
image-based rendering, data acquisition 1 Motivation Free-viewpoint video systems render new viewpoints 
from a sparse set of given input views, e.g. from simultaneously recorded video streams. While most of 
these systems rely on an approximate scene geometry to guide the view interpolation, the system [Lipski 
et al. 2010] considered in this poster is purely image-based. It allows for interpolation both in space 
and time by treating the temporal and spatial domain equally. While this versatile system already proved 
to manage challenging scene recordings, e.g. a .rebreather or an outdoor skateboarding scene, it still 
suffers from small rendering artifacts (e.g. smearing) which are mainly caused by inaccurate pixel correspondences 
due to ambiguities in the optical .ow between two input images. In this poster, we propose to enhance 
the optical .ow by adding depth sensors to the setup. With this additional information we do not reconstruct 
the scene geometry fully, but instead we reduce ambi­guities in low-textured regions and provide occlusion 
information to guide the image correspondence estimation. 2 Our approach We calibrate the depth sensors 
together with the existing multi-view setup, then use world space points reprojected into all cameras 
as spatial .ow information. *e-mail: {berger,ruhl,lipski,klose,schroeder,scholz,magnor}@cg.cs.tu­bs.de 
Calibrating multiple depth sensors with color cameras In order to simultaneously align a depth sensor 
with a consumer-grade RGB camera, we introduce a new calibration pattern, consisting of a large re.ective 
patch (e.g. aluminum foil) and small white paper patches attached to it in alternating order, forming 
a checkerboard. The re.ective patches de.ect the emitted IR pattern, while the diffuse regions re.ect 
it back to the sensor. This results in a distinguishable pattern, which can be used for robust alignment, 
Fig. 1 (left). Image correspondence estimation with depth hints In image cor­respondence estimation (and 
optical .ow), two major challenges are low-textured regions and occlusions. Depth information, when accurate, 
helps to address these issues. However, the resolution of contemporary consumer depth sensors is considerably 
lower than camera resolutions and thus less accurate. We use the potentially in­accurate depth information 
merely as a hint to the correspondence estimation. The depth points are backprojected into world space, 
and then projected into all cameras image spaces, Fig. 1 (right), upsampling the depth points via splatting. 
In effect, foreground objects are correctly covered with depth information, while border regions may 
be covered wrongly. Then, image correspondences are obtained by calculating the displacement between 
two splatted projections of a depth point. However, some of this information is incorrect due to the 
generous foreground coverage; more inaccura­cies are added because our cameras are only loosely synchronized. 
Depth information can strictly only be considered as a hint. There­fore, our current work focuses on 
methods for the required .ltering. One of the main ideas is to use the depth hints only when it is con­sistent 
with the data term of the image correspondence algorithm. Using multiple depth sensors Due to the comparatively 
wide hor­izontal angles of our setup, a single depth sensor does not cover the scene appropriately. The 
inclusion of additional sensors im­proves the depth point coverage. However, due to the active light 
of the depth sensors, interference has to be expected. In practice, we found that a maximum of two depth 
sensors can be operated concurrently, giving us the desired wide angle coverage.  References LIPSKI, 
C., LINZ, C., BERGER, K., SELLENT, A., AND MAG-NOR, M. 2010. Virtual video camera: Image-based viewpoint 
navigation through space and time. Computer Graphics Forum 29, 8, 2555 2568. MICROSOFT CORP, 2010. Kinect 
for Xbox 360, November. Red­mond WA. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037744</article_id>
		<sort_key>290</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>24</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[ParXII]]></title>
		<subtitle><![CDATA[optimized, data-parallel exemplar-based image inpainting]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037744</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037744</url>
		<abstract>
			<par><![CDATA[<p>Completing unknown parts of a damaged input image, or removing objects from photographs and replacing them with visually plausible backgrounds is an important task in photo editing and video processing with a wide range of applications from the reconstruction of missing blocks introduced by packet loss during wireless transmission, reversing of impairments, removal of image objects such as logos, stamped dates, text, and persons, to completing panoramas. The problem with most of existing inpainting methods is the balance between efficiency and accuracy, one of the most accurate methods is exemplar-based image inpainting [Criminisi, et al. 2004], the problem with this method is that it's very slow and inefficient due to the fact that it needs to scan the whole image before inpaiting a certain block of pixels (every scan operation is called a query), this makes the algorithm take tens to hundreds of seconds on modern CPUs.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809274</person_id>
				<author_profile_id><![CDATA[81466643823]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Mohamed]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yousef]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mohamed.mahdi@compit.au.edu.eg]]></email_address>
			</au>
			<au>
				<person_id>P2809275</person_id>
				<author_profile_id><![CDATA[81488671837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Khaled]]></first_name>
				<middle_name><![CDATA[F.]]></middle_name>
				<last_name><![CDATA[Hussien]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Assiut University, Assiut, Egypt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[khaled.hussain2000@gmail.com]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2320602</ref_obj_id>
				<ref_obj_pid>2319036</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[A. Criminisi, P. P'erez, and K. Toyama, "Region filling and object removal by exemplar-based image inpainting," <i>IEEE Transactions on Image Processing</i>, vol. 13, pp. 1200--1212, September 2004.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1922177</ref_obj_id>
				<ref_obj_pid>1922174</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tsz-Ho Kwok, Hoi Sheung, and Charlie C. L. Wang. "Fast query for exemplar-based image completion". <i>IEEE Transactions on Image Processing</i>, vol. 19, pp. 3106--3115. December 2010.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 ParXII: Optimized, Data-Parallel Exemplar-Based Image Inpainting Mohamed Yousef1, Khaled F. Hussien2 
1 mohamed.mahdi@compit.au.edu.eg 2 khaled.hussain2000@gmail.com Assiut University, Assiut, Egypt  Figures 
1-7, Example input images along with inpaintng result 1 Introduction Completing unknown parts of a 
damaged input image, or removing objects from photographs and replacing them with visually plausible 
backgrounds is an important task in photo editing and video processing with a wide range of applications 
from the reconstruction of missing blocks introduced by packet loss during wireless transmission, reversing 
of impairments, removal of image objects such as logos, stamped dates, text, and persons, to completing 
panoramas. The problem with most of existing inpainting methods is the balance between efficiency and 
accuracy, one of the most accurate methods is exemplar-based image inpainting [Criminisi, et al. 2004], 
the problem with this method is that it s very slow and inefficient due to the fact that it needs to 
scan the whole image before inpaiting a certain block of pixels (every scan operation is called a query), 
this makes the algorithm take tens to hundreds of seconds on modern CPUs. An important feature of the 
exemplar-based image inpainting algorithm, along with its accuracy, is that it s highly parallel; comparisons 
involved on every query operation can be done completely in parallel, which makes the algorithm map very 
well on modern massively parallel architectures like modern GPUs. In this paper, we present ParXII, an 
optimized data-parallel variant of the exemplar image inpainting that seeks to achieve best utilization 
of today GPUs and then applies a number of optimizations to reduce the number of queries and the arithmetic 
intensity of each query, with approximately no noticeable effect on plausibility of results. 2 Algorithm 
 ParXII has the same basic steps of exemplar image inpainting, it modifies some of the steps either to 
parallelize them or to increase their speed, as follows: 1) Instead of sequentially searching the image 
for matching patches, the comparison of each block with the target block, using SSD, is done in parallel, 
after that SSD values are reduced in parallel. An important aspect here is that all modifications on 
the image must be done on the GPU with no GPU-CPU communication other than the reduced SSD values. 2) 
Instead of the CIE Lab color space we use the YCbCr color space; the idea here is to use only the chroma 
components (Cb and Cr) in the SSD comparison. The chroma components preserve the color information and 
are therefore sufficient for getting to the most appropriate block in the input image. 3) A problem with 
the formula P(p) = C(p)D(p) for calculating the priority of a block is that it totally neglects the effect 
the size of the inpainted part can have, increasing the size of the inpainted area of the block directly 
decreases the number of queries, and therefore increases the execution speed. The idea is to choose the 
block with highest-to-be-inpainted area from blocks in top 5%-10% blocks in priority, to compute this 
efficiently we approximate it trough modifying the original formula as follows: P(p) = C(p)D(p)A(p) , 
where A(p) is the area of to be inpainted part of block, associated with point p. 3 Implementation 
We implemented ParXII in OpenCL, we preferred it over CUDA and other platform specific APIs, as it s 
an open, platform-independent API. We used Image ImageMagick library to read, write, and modify images. 
We used the exemplar image inpainting implementation by Qiushuang Zhang as a basis for our modifications 
and experimentation till we reached ParXII. 4 Results In the following table (time in milliseconds) 
we summarize our results, especially compared with the only current attempt to GPU accelerate, exemplar 
image inpainting [Kwok et al. 2010]. Our results show that we are on the way to real-time video-inpainting. 
 Fast Query on GTX 280 ParXII on GTX 280 Fig 1 (538x403) 11560 1940 Fig 2 (206x308) 900 170 Fig 
3 (438x297) 7400 1920 Fig 4 (628x316) 3680 370 Fig 5 (700x389) 1640 125 Fig 6 (200x150) 670 80 Fig 
7 (700x438) 4180 430  5 References A. CRIMINISI, P. P´EREZ, AND K. TOYAMA, Region filling and object 
removal by exemplar-based image inpainting, IEEE Transactions on Image Processing, vol. 13, pp. 1200 
1212, September 2004. TSZ-HO KWOK, HOI SHEUNG, AND CHARLIE C. L. WANG. Fast query for exemplar-based 
image completion . IEEE Transactions on Image Processing, vol. 19, pp. 3106-3115. December 2010. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037745</article_id>
		<sort_key>300</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>25</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA["Seams" to make no difference]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037745</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037745</url>
		<abstract>
			<par><![CDATA[<p>Over the past several years Tiled Displays have slowly evolved from expensive blended-projection systems to Liquid Crystal Display (LCD) tiled displays. The LCD systems are typically cheaper to construct and maintain, but the bezels introduced at screen edges are often a common source of complaints. Our long term research goal is to settle the debate between the two technologies to see if there really is any perceivable benefit to one versus the other. The work outlined here presents the first in a series of comparative experiments.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809276</person_id>
				<author_profile_id><![CDATA[81100069081]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McNamara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809277</person_id>
				<author_profile_id><![CDATA[81100304539]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Frederic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Parke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809278</person_id>
				<author_profile_id><![CDATA[81466646043]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Mat]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sanford]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Darken, R. P., Peterson, B., and Orientation, B. S. 2001. Spatial orientation, wayfinding, and representation. In <i>In K. M. Stanney (Ed.), Handbook of Virtual Environments: Design, Implementation, and Applications</i>, Erlbaum, 493--518.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>986105</ref_obj_id>
				<ref_obj_pid>985921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mackinlay, J. D. 2004. Wideband displays: Mitigating multiple monitor seams. In <i>CHI 2004 Extended Abstracts, ACM Press</i>, ACM Press, 1521--1524.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Seams to make no difference Ann McNamara* Frederic Parke Mat Sanford Department of Visualization Texas 
A&#38;M 1 Introduction Over the past several years Tiled Displays have slowly evolved from expensive 
blended-projection systems to Liquid Crystal Dis­play (LCD) tiled displays. The LCD systems are typically 
cheaper to construct and maintain, but the bezels introduced at screen edges are often a common source 
of complaints. Our long term research goal is to settle the debate between the two technologies to see 
if there really is any perceivable bene.t to one versus the other. The work outlined here presents the 
.rst in a series of comparative ex­periments. Figure 1: Participant completing navigation task in IVC 
environ­ment. 2 Experiment We investigated performance on a simple navigation and way­.nding task in 
each type of system, one a low-cost, tiled, multi­screen immersive visualization system and the other 
a more expen­sive, continuous screen, immersive visualization facility. The low cost system (LCD) is 
designed using off-the-shelf components and constructed by arranging LCD displays in a curved tiled layout. 
The expensive system (IVC) is a Rockwell-Collins semi-rigid, rear projected, continuous curved screen. 
With the low cost paradigm, physical seams are introduced into the image where the displays are tiled. 
Our hypothesis is that the tiled system presents an equivalent visual experience for navigation and way-.nding 
tasks, despite the separating seams introduced by connecting the screens. Each immersive system was evaluated 
by measuring timely task performance in a psychophysical experiment. There were 20 par­ticipants, ten 
of which viewed the LCD system .rst and ten viewed the IVC system .rst. There were .ve conditions which 
manipulated the independent variable,seam size (please see supplemental mate­rial for further details). 
We introduced software seams in the IVC to mimic the physical bezels present in the LCD system. In each 
condition participants were asked to navigate .ve separate routes, labeled A, B, C, D and E. Each route 
was indicated by a starting point and a target end point. A 2D map was clearly marked with start and 
end points was provided. Participants could view the map for as long as they wished, but typically only 
studied it for a minute or two before declaring themselves ready to navigate the Virtual En­vironment 
(VE). After studying the map participants were placed in the VE located at the start point. The task 
was to navigate as quickly *e-mail: ann@viz as possible through the 3D environment, beginning from their 
start position and .nishing at the indicated end position. This is known as a goal-directed navigation, 
or primed navigation task [Darken et al. 2001]. 3 Results Time to target per participant across system 
was recorded and com­pared in an effort to gauge performance. As previously mentioned, our premise is 
that faster times to reach the end position demon­strate higher spatial understanding and hence performance 
in the system. Average end time across participant in each environment was compared. A one-way ANOVA 
showed no signi.cant differ­ences across seam conditions F (4) = 0.52; p> 0.05. These re­sults show strong 
evidence that, in this case, the presence of seams did not adversely impact the performance of simple 
navigation in the immersive environments. 4 Conclusion Navigation in VEs is one of the most common interaction 
tasks. Successful travel through an environment, is critical to enable in­teraction with the environment. 
The goal of this work was to show that navigation performance is equal (on average) in systems with or 
without seams, and, therefore, not impacted by the size or pres­ence of display seams, be they physical 
or virtual (introduced by the software). We compared performance on a simple goal-directed navigation 
task in both immersive systems and discovered that no signi.cant differences exist across systems, which 
validates our hy­pothesis that there is no disruption to the visual experience of the user when navigating 
a VE populated with physical or software seams. We have shown, by comparing navigation to a target across 
seamed VE systems, that seams have no signi.cant impact on the time taken to complete a simple guided 
navigation and way-.nding task. In fact in some cases participants actually performed navigation tasks 
better with the presence of seams. Informal comments indicated that some users actually preferred the 
presence of seams as they helped to partition the scene and made navigation more manage­able. This also 
may be due to an extension the phenomenon dis­covered by several researchers in which the separation 
of displays helps users to streamline tasks in 2D desktop systems [Mackinlay 2004]. Clearly, much work 
remains to be done in this area. The experiment represents the beginning of a larger study in which we 
hope to fully investigate the impact of seams on performance across immersive systems. References DARKEN, 
R. P., PETERSON, B., AND ORIENTATION, B. S. 2001. Spatial orientation, way.nding, and representation. 
In In K. M. Stanney (Ed.), Handbook of Virtual Environments: Design, Im­plementation, and Applications, 
Erlbaum, 493 518. MACKINLAY, J. D. 2004. Wideband displays: Mitigating multiple monitor seams. In CHI 
2004 Extended Abstracts, ACM Press, ACM Press, 1521 1524. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037746</section_id>
		<sort_key>310</sort_key>
		<section_seq_no>6</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Image/video processing]]></section_title>
		<section_page_from>6</section_page_from>
	<article_rec>
		<article_id>2037747</article_id>
		<sort_key>320</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>26</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D exploitation of 2D video from a hand-launched aerial glider]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037747</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037747</url>
		<abstract>
			<par><![CDATA[<p>Compact cameras and robotic platforms are growing ubiquitous as performance increases and price decreases for both of these transformational technologies. To encourage rapid experimentation with imaging sensors and robots, MIT Lincoln Laboratory held a Technology Challenge in September 2010 which involved remotely characterizing a 1 km<sup>2</sup> rural area. In this poster, we present video exploitation results from an aerial glider system fielded as part of the Technology Challenge.</p> <p>The aerial system's hardware setup included a Radian sailplane glider (&lt; $400), a Canon powershot camera (&lt; $300) and a Garmin GPS unit (&lt; $100). The camera and GPS clocks were synchronized by taking pictures of the latter with the former. Both sensors were subsequently mounted to the glider's underside prior to hand launching. Video imagery was collected at 3 Hz and GPS readings at 1 Hz over 20 minute aerial missions that flew up to 430 meters above ground.</p> <p>After 3000 aerial video frames had been collected, they were processed via the 3D image reconstruction pipeline developed by Snavely et al. SIFT features are first extracted and matched across all images on a parallelized computer Grid. Skeletal graph analysis and incremental bundle adjustment subsequently recover camera parameters and relative scene point positions. Multi-view stereo algorithms developed by Furukawa et al subsequently convert the initially sparse point cloud output into a dense reconstruction for the ground scene. By fitting the camera's reconstructed path with GPS measurements, we derive the global transformation needed to convert both extrinsic camera parameters and the dense point cloud from relative to absolute world coordinates.</p> <p>We demonstrate several useful examples of geometry-based exploitation of reconstructed aerial video frames which are difficult to perform via conventional image processing. Firstly, our imaging hardware and reconstruction plus georegistration software yield detailed 3D terrain maps with absolute altitudes above sea-level for ground, water and trees. The maps' approximate 1 meter Ground Sampling Distance is comparable to that from active-illumination ladars which cost orders of magnitude more than our inexpensive passive system. Secondly, stabilization and orthorectification of video frames from small air vehicles which experience significant jostling is readily achieved by projecting onto a common Z-plane. 3D-based video stabilization results are far superior to those from 2D warping. Finally, we illustrate how any interesting pixel in one video frame can be identified with counterpart pixels in other video frames via 3D backprojection and reprojection. These geometrical relationships enable automatic annotation of aerial video footage in the future.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809279</person_id>
				<author_profile_id><![CDATA[81330489484]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Lincoln Laboratory]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809280</person_id>
				<author_profile_id><![CDATA[81100156470]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Noah]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Snavely]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Cornell University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Exploitation of 2D Video from a Hand-Launched Aerial Glider Peter Cho (MIT Lincoln Laboratory) 
and Noah Snavely (Cornell University)  ABSTRACT Compact cameras and robotic platforms are growing ubiquitous 
as performance increases and price decreases for both of these transformational technologies. To encourage 
rapid experimentation with imaging sensors and robots, MIT Lincoln Laboratory held a Technology Challenge 
in September 2010 which involved remotely characterizing a 1 km2 rural area. In this poster, we present 
video exploitation results from an aerial glider system fielded as part of the Technology Challenge. 
 The aerial system's hardware setup included a Radian sailplane glider (< $400), a Canon powershot camera 
(< $300) and a Garmin GPS unit (< $100). The camera and GPS clocks were synchronized by taking pictures 
of the latter with the former. Both sensors were subsequently mounted to the glider's underside prior 
to hand launching. Video imagery was collected at 3 Hz and GPS readings at 1 Hz over 20 minute aerial 
missions that flew up to 430 meters above ground. After 3000 aerial video frames had been collected, 
they were processed via the 3D image reconstruction pipeline developed by Snavely et al. SIFT features 
are first extracted and matched across all images on a parallelized computer Grid. Skeletal graph analysis 
and incremental bundle adjustment subsequently recover camera parameters and relative scene point positions. 
Multi-view stereo algorithms developed by Furukawa et al subsequently convert the initially sparse point 
cloud output into a dense reconstruction for the ground scene. By fitting the camera's reconstructed 
path with GPS measurements, we derive the global transformation needed to convert both extrinsic camera 
parameters and the dense point cloud from relative to absolute world coordinates. We demonstrate several 
useful examples of geometry-based exploitation of reconstructed aerial video frames which are difficult 
to perform via conventional image processing. Firstly, our imaging hardware and reconstruction plus georegistration 
software yield detailed 3D terrain maps with absolute altitudes above sea-level for ground, water and 
trees. The maps' approximate 1 meter Ground Sampling Distance is comparable to that from active-illumination 
ladars which cost orders of magnitude more than our inexpensive passive system. Secondly, stabilization 
and orthorectification of video frames from small air vehicles which experience significant jostling 
is readily achieved by projecting onto a common Z-plane. 3D-based video stabilization results are far 
superior to those from 2D warping. Finally, we illustrate how any interesting pixel in one video frame 
can be identified with counterpart pixels in other video frames via 3D backprojection and reprojection. 
These geometrical relationships enable automatic annotation of aerial video footage in the future. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037748</article_id>
		<sort_key>330</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>27</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[A framework for GPU 3d model reconstruction using structure-from-motion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037748</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037748</url>
		<abstract>
			<par><![CDATA[<p>We propose a framework for three-dimensional (3D) model reconstruction using structure-from-motion (SfM). Our primary application is scanning forest canopies for ecological research (as shown in Figure 1) using photos taken from a radio-controlled helicopter, although the methods apply to any series of photographs taken along a path with the camera pointing perpendicular to the path.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809281</person_id>
				<author_profile_id><![CDATA[81488645557]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland, Baltimore County]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809282</person_id>
				<author_profile_id><![CDATA[81334487411]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marc]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Olano]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Maryland, Baltimore County]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>996342</ref_obj_id>
				<ref_obj_pid>993451</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lowe, D. G. 2004. Distinctive image features from scale-invariant keypoints. <i>International Journal of Computer Vision</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141964</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Snavely, N., Seitz, S. M., and Szeliski, R. 2006. Photo tourism: Exploring image collections in 3d. In <i>ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Framework for GPU 3D Model Reconstruction Using Structure-from-Motion Yu Wang Marc Olano Department 
of Computer Science and Electrical Engineering University of Maryland, Baltimore County Introduction 
We propose a framework for three-dimensional (3D) model recon­struction using structure-from-motion (SfM). 
Our primary applica­tion is scanning forest canopies for ecological research (as shown in Figure 1) using 
photos taken from a radio-controlled helicopter, al­though the methods apply to any series of photographs 
taken along a path with the camera pointing perpendicular to the path. Previous works in this area include 
Snavely et al. s Bundler [2006] where the datasets are unordered photos of famous landmarks downloaded 
from the Internet; using structure-from-motion, the ge­ometries are reconstructed, but this process takes 
a long time. Our approach is based on the fact that the photos are taken in a con­tinuous path, so we 
can utilize the image adjacencies. The major contributions of our work are 1) introduce a linear time 
complexity algorithm that reduces number of image pairs to match; 2) present a high performance framework 
for 3D reconstruction using SfM. Our Approach The reconstruction process contains several stages including 
im­age feature extraction, feature matching, and scene reconstruction/ camera position approximation. 
In the feature extraction stage, im­age features are extracted using an existing GPU implementation of 
the SIFT algorithm [Lowe 2004]. In the feature matching stage, an algorithm is introduced to reduce the 
number of images to match to calculate camera positions; for image data obtained in a continuous path, 
the presented algorithm limits the image correlation to nearby images. After image features are extracted, 
they are stored on the host memory. Three major CUDA kernels are designed to complete the rest of the 
pipeline, as shown in (a), (b) and (c) in Figure 2. Image features are copied to the GPU memory where 
features of every two adjacent images are matched (Figure 2 (a)), and feature correlations are stored 
in the device memory. By accessing the de­vice memory, a second kernel (Figure 2 (b)) applies SfM algorithms 
to the matched features, and camera positions are approximated. The third kernel (Figure 2 (c)) applies 
a k-nearest neighbor search to each camera in the collection of approximated camera positions, and locates 
k nearest neighboring cameras to each camera. Based on the approximated camera positions and the spatial 
relations be­tween them, feature matching kernel and scene reconstruction ker­nel (Figure 2 (a) (b)) 
are called to re.ne the original reconstruction. For instance, camera i has k nearest neighbors, which 
are camera i - 2, camera i - 1, camera i +1, and camera m. These cameras poses are calculated from image 
i - 2, image i - 1, image i +1 and image m. Features extracted from these images are matched, as a re.nement 
of the results from the .rst kernel (Figure 2 (a)) and geometry reconstruction is re.ned using only the 
images that have neighboring relations with each other. Results and Future Work Comparing with Bundler, 
our framework has signi.cantly im­proved the performance of image feature extraction stage by an average 
factor of 6x; since we apply a linear time complexity algo­rithm to the feature matching stage, the speedup 
of this stage over  Figure 2: (a) pairwise feature matching; (b) camera position ap­proximation; (c) 
re.nement of feature matching based on the ap­proximated camera positions (upper right) Bundler increases 
as the number of images in the dataset increases. We are currently experimenting on different image feature 
descrip­tors taking less device memory so that more features can be stored on the GPU which reduces costs 
for paging data from CPU to GPU; different methods for feature matching that replaces Euclidean dis­tance 
calculations, i.e. bitwise logic operation on hashing image feature descriptors. References LOWE, D. 
G. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer 
Vision. SNAVELY, N., SEITZ, S. M., AND SZELISKI, R. 2006. Photo tourism: Exploring image collections 
in 3d. In ACM Transac­tions on Graphics (Proceedings of SIGGRAPH 2006). Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037749</article_id>
		<sort_key>340</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>28</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[An image restoration method for extracting features from three-dimensional auto-stereoscopic integral photography images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037749</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037749</url>
		<abstract>
			<par><![CDATA[<p>Integral Photography (IP) [Hongen Liao 2011] is a visualization technique that produce true three-dimensional images in the real space that can be viewed without supplementary glasses. Using a micro lens array, different aspects of the object can be viewed from different directions, giving observers a sense of depth. Utilizing IP images into augmented reality systems (where images are merged into a real scene) requires an automated alignment between virtual IP images and real objects, leading to the need of understanding the displayed IP images. A natural approach would be capturing multiple views of the IP images and then extracting necessary information from those views. However, since IP images are viewed through a lens array, a lens-like pattern is produced in the observed image(Fig.1(a)). When captured by a camera, simply applying existing image processing methods such as feature extraction would result in unexpected behaviors (e.g. detection of fault features). The purpose of this study is to develop an image restoration method by removing those lens-like pattern from the IP image captured by a camera.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809283</person_id>
				<author_profile_id><![CDATA[81317501181]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Huy]]></first_name>
				<middle_name><![CDATA[Hoang]]></middle_name>
				<last_name><![CDATA[Tran]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tran@atre.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809284</person_id>
				<author_profile_id><![CDATA[81490675038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kenta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kuwana]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809285</person_id>
				<author_profile_id><![CDATA[81100507773]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Hongen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liao]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809286</person_id>
				<author_profile_id><![CDATA[81100447521]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Masamune]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809287</person_id>
				<author_profile_id><![CDATA[81100169268]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takeyoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dohi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809288</person_id>
				<author_profile_id><![CDATA[81452609848]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakajima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2053433</ref_obj_id>
				<ref_obj_pid>2053035</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hongen Liao, Takeyoshi Dohi, K. N. 2011. Autostereoscopic 3d display with long visualization depth using referential viewing area based integral photography. <i>IEEE Transactions on Visualization and Computer Graphics 17</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1405432</ref_obj_id>
				<ref_obj_pid>1405206</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Igor Aizenberg, C. B. 2008. A windowed gaussian notch filter for quasi-periodic noise removal. <i>IMAGE AND VISION COMPUTING 26</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Image restoration method for extracting features from three-dimensional auto-stereoscopic Integral 
Photography images Huy Hoang Tran *, Kenta Kuwana, Hongen Liao, Ken Masamune ,Takeyoshi Dohi, Susumu 
Nakajima The University of Tokyo 1 Introduction Integral Photography (IP) [Hongen Liao 2011] is a visualization 
technique that produce true three-dimensional images in the real space that can be viewed without supplementary 
glasses. Using a micro lens array, different aspects of the object can be viewed from different directions, 
giving observers a sense of depth. Utilizing IP images into augmented reality systems (where images are 
merged into a real scene) requires an automated alignment between virtual IP images and real objects, 
leading to the need of understanding the displayed IP images. A natural approach would be capturing multiple 
views of the IP images and then extracting necessary in­formation from those views. However, since IP 
images are viewed through a lens array, a lens-like pattern is produced in the observed image(Fig.1(a)). 
When captured by a camera, simply applying ex­ isting image processing methods such as feature extraction 
would result in unexpected behaviors (e.g. detection of fault features). The purpose of this study is 
to develop an image restoration method by removing those lens-like pattern from the IP image captured 
by a camera. 2 Proposed method Since the sizes and shapes of the lenses are relatively identical, the 
lens-like pattern appears like a periodic noise added to the image. It is well-known that periodic noises 
are represented by star-like peaks in the frequency domain. Therefore, localizing and removing those 
peaks in the frequency domain would be a good approach to the problem. In this study, we develope an 
extended version of windowed Gaussian notch .lter (WGNF) [Igor Aizenberg 2008] to effectively remove 
periodic noises in the image. The method consits of two steps. First, a peak detection is performed using 
the following peak detector. ci,ji,j > ., (1)medm,n where ci,j is the spectral coef.cient at (i, j), 
medi,j is the lo­ m,n cal median value of the m × n window around (i, j) and . is a prede.ned threshold. 
Using the above detector, peaks are found at positions where intensity changes drastically. However, 
due to the pepper and salt noise in the spectral image (which often occurs in commercial camera systems), 
fault peaks are likely to be detected, resulting in the deletion of valuable frequencies. To get rid 
of fault detections of peaks, we introduce the following intensity .lter to the peak detector. i,j m,n 
avg>., (2)medglobal i,j where avgm,n is the local average value of the m × n window around (i, j), medglobal 
is the global median value of the whole domain and . is a prede.ned threshold. The above condition im­plies 
that only peaks located in a relatively high intensity area are considered true peaks, otherwise they 
would be noises. In the next step, we need to .lter out the detected peaks. This is done by simply applying 
a Gaussian .lter function which is de.ned only over a neighborhood of the peak. *tran@atre.t.u-tokyo.ac.jp 
 3 Result and future work We applied our algorithm to the IP image of a CG object, the Stand­ford Dragon. 
The image was captured by a DSLR camera. Fig.1(b) shows the output image after being restored. While 
lens-like noises are almost removed at the center of the model, there are still some noticeable noises 
remained at the edges of the model. Fig.2 shows the results of Harris corner detection applied to the 
original and restored image. While fault corners are detected at the center of the model in the original 
image, only true corners are found in the .ltered image. A lot of works have to be done in the future, 
selection of parameters is done empirically and there is no guarantee that the set of param­eters works 
for this case will work for other cases. Also, the spatial arrangement and the geometry of the lenses 
can give us some hints about the arrangement of true peaks. (a) (b) Figure 1: IP image captured from 
camera: original image(a), re­stored image (b). (a) (b) Figure 2: Harris corner detection applied to 
original image (a), and restored image (b). References HONGEN LIAO, TAKEYOSHI DOHI, K. N. 2011. Autostereo­scopic 
3d display with long visualization depth using referential viewing area based integral photography. IEEE 
Transactions on Visualization and Computer Graphics 17. IGOR AIZENBERG, C. B. 2008. A windowed gaussian 
notch .lter for quasi-periodic noise removal. IMAGE AND VISION COM-PUTING 26. Copyright is held by the 
author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037750</article_id>
		<sort_key>350</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>29</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Background motion silences awareness of foreground change]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037750</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037750</url>
		<abstract>
			<par><![CDATA[<p>When a set of objects changing in brightness, color, size, or shape moves across the visual field, the objects appear to stop changing [Suchow & Alvarez, 2011]. In previous work introducing this effect ("silencing"), we showed that its strength depends on speed: the faster the objects move, the less noticeable the change. (See the demos at http://bit.ly/cW0YCB.) In order to explain it, we had proposed the <i>brief window hypothesis</i>, which supposes that change detection relies on the success of local detectors---<i>i.e.</i>, ones that monitor a fixed location in the visual field. Then, since a fast-moving object spends little time at any one location, each detector is afforded only a brief window in which to assess the changing object; this brevity may preclude the detection of change. Here, we show that the brief window hypothesis is wrong via a demonstration of silencing by background motion.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809289</person_id>
				<author_profile_id><![CDATA[81488668407]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jordan]]></first_name>
				<middle_name><![CDATA[W.]]></middle_name>
				<last_name><![CDATA[Suchow]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[suchow@fas.harvard.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809290</person_id>
				<author_profile_id><![CDATA[81488662532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[George]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Alvarez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Harvard University Cambridge, MA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[alvarez@wjh.harvard.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Suchow, J. W., and Alvarez, G. A. 2011. Motion silences awareness of visual change. In <i>Current Biology</i>, vol. 22, 140--143.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037751</article_id>
		<sort_key>360</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>30</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Content-aware display adaptation and editing for stereoscopic images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037751</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037751</url>
		<abstract>
			<par><![CDATA[<p>We propose a content-aware stereoscopic image display adaptation method which simultaneously resizes a binocular image to the target resolution and adapts its depth to the comfort zone of the display while preserving the perceived shapes of prominent objects [Chang et al. 2011]. This method does not require depth information or dense correspondences. Given the specification of the target display and a sparse set of correspondences, our method efficiently deforms the input stereoscopic images for display adaptation by solving a least-squares energy minimization problem. This can be used to adjust stereoscopic images to fit displays with different real estates, aspect ratios, and comfort zones. In addition, with slight modifications to the energy function, our method allows users to interactively adjust the sizes, locations, and depths of the selected objects, giving users aesthetic control for depth perception.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809291</person_id>
				<author_profile_id><![CDATA[81488667168]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Che-Han]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809292</person_id>
				<author_profile_id><![CDATA[81542646056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Chia-Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lytro Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809293</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>2334925</ref_obj_id>
				<ref_obj_pid>2334897</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chang, C.-H., Liang, C.-K., and Chuang, Y.-Y. 2011. Content-aware display adaptation and interactive editing for stereoscopic images. <i>IEEE Transactions on Multimedia</i>, to appear.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Content-Aware Display Adaptation and Editing for Stereoscopic Images Che-Han Chang1 Chia-Kai Liang2 
Yung-Yu Chuang1 1National Taiwan University 2Lytro Inc.    (a) (b) (c) (d) (e) Figure 1: Algorithm 
overview. The top row shows the left view and second row shows the right view. (a) The input binocular 
image pair, (b) their saliency maps, (c) original images with grid meshes and feature points, (d) retargeted 
image pair with deformed grid meshes and relocated feature points, and (e) retargeted image pair. 1 Introduction 
We propose a content-aware stereoscopic image display adaptation method which simultaneously resizes 
a binocu­lar image to the target resolution and adapts its depth to the comfort zone of the display while 
preserving the per­ceived shapes of prominent objects [Chang et al. 2011]. This method does not require 
depth information or dense corre­spondences. Given the speci.cation of the target display and a sparse 
set of correspondences, our method e.ciently deforms the input stereoscopic images for display adapta­tion 
by solving a least-squares energy minimization problem. This can be used to adjust stereoscopic images 
to .t dis­plays with di.erent real estates, aspect ratios, and comfort zones. In addition, with slight 
modi.cations to the energy function, our method allows users to interactively adjust the sizes, locations, 
and depths of the selected objects, giving users aesthetic control for depth perception. 2 Algorithm 
and results Our algorithm is based on warping-based image manipu­lation methods with two stereoscopic 
constraints: vertical alignment and horizontal disparity consistency. Figure 1 il­lustrates the overview 
of our method. Our method .rst de­tects a sparse set of robust correspondence points and then optimizes 
the warping .elds of the image pair according to the target display parameters, correspondence constraints, 
and saliency constraints that prevent the results from distor­tions. Our method can achieve various retargeting 
scenarios, including changing the display size, aspect ratio, allowable depth range, and viewing con.guration. 
It can also achieve e.ects not supported in traditional depth adaptation meth­ods, such as changes to 
the scene depth that do not a.ect its scale. In addition, by modeling the user interaction as constraints, 
our system can be extended to an interactive stereoscopic image editing system. The user can specify 
the transformation of the disparity/depth values, and our sys­(a) (b) (c) Figure 2: Resizing results. 
(a) The original stereoscopic im­age. Results of (b) traditional scaling, (c) our method. (a) (b) (c) 
(d) Figure 3: Results of depth adaptation by user editing. The boat is placed at di.erent depths. The 
.rst row presents the results as red/cyan images; and the second row displays the right-view images with 
features and disparities. (a) The original 476×555 stereoscopic image. All objects are behind the screen. 
(b) The boat is moved to the front of the screen (note that sign changes of disparities), and (c) behind 
the screen. (d) The depth range of the boat is tripled. tem accordingly warps the input to generate a 
new stereo­scopic image. The user can also select a single object and specify its position, depth, or 
even explicit 3D location. Our system automatically identi.es the depths of other regions and warps the 
input to match user s intention. The resultant system is the .rst content-aware system to simultaneously 
allow retargeting, depth adaptation, and interactive editing of stereoscopic images. Figure 2 shows the 
resizing results. The traditional scal­ing method could cause distortions on the perceived object shapes. 
By considering saliency and stereo constraints, our method maintains better perceived shapes of the .owers. 
In Figure 3, the user edits the position and shape of the boat. User studies show that the method is 
e.ective at editing depth and reducing occurrences of diplopia and distortions. References Chang, C.-H., 
Liang, C.-K., and Chuang, Y.-Y. 2011. Content­ aware display adaptation and interactive editing for stereo­ 
scopic images. IEEE Transactions on Multimedia, to appear. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037752</article_id>
		<sort_key>370</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>31</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Display pixel caching]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037752</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037752</url>
		<abstract>
			<par><![CDATA[<p>A variety of standard video modes that stretch or zoom lower resolution video content linearly to take full advantage of large screen sizes have been implemented in TV sets. When content and screen aspect ratios differ, format proportions may be compromised, video content may be clipped, or screen regions may remain unused. Newer techniques, such as video retargeting and video upsampling, rescale individual video frames and can potentially match them to the display resolution and aspect ratio. However, none of these methods can display simultaneously more than is contained in a single frame.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809294</person_id>
				<author_profile_id><![CDATA[81488672490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[clemens.birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809295</person_id>
				<author_profile_id><![CDATA[81488655525]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tianlun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tianlun.liu@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809296</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809297</person_id>
				<author_profile_id><![CDATA[81421597701]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Max]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grosse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[max.grosse@uni-weimar.de]]></email_address>
			</au>
			<au>
				<person_id>P2809298</person_id>
				<author_profile_id><![CDATA[81100631719]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Anselm]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Grundh&#246;fer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Bauhaus-University Weimar]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[anselm.grundhofer@uni-weimar.de]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Display Pixel Caching Clemens Birklbauer, Tianlun Liu, Oliver Bimber Max Grosse, Anselm Grundh¨ofer 
Johannes Kepler University Linz * Bauhaus-University Weimar Figure 1: Results of DPC for 4:3 content 
on a 16:9 screen after several frames of leftward camera motion while empty cache regions (left and right 
borders) are left blank (a-d). Result of DPC for a cinemascope format and downward camera motion while 
empty cache regions (bottom border) are pre-.lled with a smooth color transition (f). Result of DPC for 
a 16:9 format and forward camera motion (h). Frames a,e, and g are the original frames while frames b,c,d,f, 
and h were computed by DPC. The original frames are always unmodi.ed and embedded in the center of the 
DPC output. 1 Introduction and Motivation A variety of standard video modes that stretch or zoom lower 
res­olution video content linearly to take full advantage of large screen sizes have been implemented 
in TV sets. When content and screen aspect ratios differ, format proportions may be compromised, video 
content may be clipped, or screen regions may remain unused. Newer techniques, such as video retargeting 
and video upsampling, rescale individual video frames and can potentially match them to the display resolution 
and aspect ratio. However, none of these methods can display simultaneously more than is contained in 
a single frame. 2 Our Approach With display pixel caching (DPC), we take a completely different approach. 
Instead of zooming, stretching, or retargeting individ­ual frames, we merge the motion information from 
many subse­quent frames to generate high-resolution panoramas in an ad-hoc and fully automatic manner. 
Thus, we cache pixels in border re­gions as long as they are visually reasonable. In contrast to conven­tional 
video mosaicing, however, the challenges to DPC are achiev­ing real-time rates for high-resolution input 
content, and ensuring spatial and temporal consistency in complex local and global video motion patterns. 
The DPC video processing pipeline can be summarized as follows: Motion patterns of input video frames 
are analyzed and segmented into motion layers. The different motion layers are warped and ac­cumulated 
to .ll border regions (i.e., the display cache). Border regions that remain empty can optionally be initialized 
(e.g., by a smooth extrapolation). Uncertain cache content that accumulates vivid registration errors 
is identi.ed and is temporally faded out. If shot transitions are detected the cache content undergoes 
the same transition as the original frames. Subsequent cache states are tem­porally smoothed and .nally 
displayed together with the original *{.rstname.lastname}@jku.at {.rstname.lastname}@uni-weimar.de Figure 
2: User preferences for various video modes over the orig­inal (unmodi.ed) content: The scores range 
from 1 (strong prefer­ence) to 6 (low preference). The bar chart displays, average, me­dian, the lower, 
and the upper quartiles. For retargeting, we used Rubinstein s multi-operator media retargeting with 
face detection. frames. The result is a high-resolution panorama that successively .lls the empty screen 
borders while leaving the original frames un­touched, rather than a framed video as in common video modes. 
 3 Results DPC achieves real-time rates for high-resolution video content (e.g., 50fps for PAL videos 
displayed on a 720p screen, 29fps for PAL when displayed on a 1080p screen, and 26fps for 720p dis­played 
on a 1080p screen) while processing complex motion pat­terns fully automatically. We compared DPC to 
related video modes in the context of a user evaluation with 59 subjects. Thereby, different options 
for ini­tializing empty cache regions were also tested (see supplementary videos): no initialization 
leaves empty cache regions blank, full ini­tialization smears cache edges towards screen edges, progressive 
clipping initializes only the regions between original cache content and its outermost extent, conservative 
clipping cuts cache content at its inner limit. We found, that DPC is preferred most when straight and 
screen-aligned cache edges are preserved. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037753</article_id>
		<sort_key>380</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>32</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Enhancing underwater images by fusion]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037753</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037753</url>
		<abstract>
			<par><![CDATA[<p>When photographs are taken in underwater conditions the visibility of the scene is degraded significantly. This is due to the fact that the radiance of a point in the scene is directly influenced by the medium scattering. Practically, distant objects and part of the scene suffers from poor visibility, loss of contrast and faded colors. Recovering of such degraded visual information is important for applications such as oceanic engineering, mapping, research in marine biology, archeology, surveillance.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809299</person_id>
				<author_profile_id><![CDATA[81365594532]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Cosmin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ancuti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cosmin.ancuti@uhasselt.be]]></email_address>
			</au>
			<au>
				<person_id>P2809300</person_id>
				<author_profile_id><![CDATA[81421596067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Codruta]]></first_name>
				<middle_name><![CDATA[Orniana]]></middle_name>
				<last_name><![CDATA[Ancuti]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[codruta.ancuti@uhasselt.be]]></email_address>
			</au>
			<au>
				<person_id>P2809301</person_id>
				<author_profile_id><![CDATA[81335491115]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tom]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Haber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tom.haber@uhasselt.be]]></email_address>
			</au>
			<au>
				<person_id>P2809302</person_id>
				<author_profile_id><![CDATA[81100093388]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Philippe]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bekaert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hasselt University, Belgium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[philippe.bekaert@uhasselt.be]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Achantay, R., Hemamiz, S., Estraday, F., and Susstrunky, S. 2009. Frequency-tuned salient region detection. <i>IEEE CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ancuti, C. O., Ancuti, C., and Bekaert, P. 2010. Effective single image dehazing by fusion. <i>IEEE International Conference on Image Processing(ICIP)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1966032</ref_obj_id>
				<ref_obj_pid>1965992</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ancuti, C. O., Ancuti, C., Hermans, C., and Bekaert, P. 2010. A fast semi-inverse approach to detect and remove the haze from a single image. In <i>Asian Conference on Computer Vision (ACCV)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1360671</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Fattal, R. 2008. Single image dehazing. <i>ACM Trans. on Graphics, SIGGRAPH</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2286573</ref_obj_id>
				<ref_obj_pid>2286439</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y., and Averbuch, Y. 2007. Regularized image recovery in scattering media. <i>IEEE Trans Pattern Anal Mach Intell.</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Schechner, Y. Y., and Karpel, N. 2004. Clear underwater vision. <i>IEEE CVPR</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Cosmin Ancuti Enhancing Underwater Images by Fusion Codruta Orniana Ancuti Tom Haber Hasselt University, 
Belgium, .rstname.lastname@uhasselt.be Philippe Bekaert Initial image Schechner &#38; Averbuch [2007] 
Our result  Figure 1: Considering the initial underwater image (left side of the .rst image) our strategy 
is able to enhance better the .nest details compared with the specialized technique of [Schechner and 
Averbuch 2007]. Introduction When photographs are taken in underwater condi­tions the visibility of the 
scene is degraded signi.cantly. This is due to the fact that the radiance of a point in the scene is 
directly in.uenced by the medium scattering. Practically, distant objects and part of the scene suffers 
from poor visibility, loss of contrast and faded colors. Recovering of such degraded visual information 
is important for applications such as oceanic engineering, mapping, research in marine biology, archeology, 
surveillance. In general the existing specialized underwater techniques [Schech­ner and Karpel 2004; 
Schechner and Averbuch 2007] use several images of the same scene registered with different states of 
po­larization. Moreover, although the recent single image dehazing techniques [Fattal 2008; Ancuti et 
al. 2010b; Ancuti et al. 2010a] shown some reliability in some particular cases, they are in general 
not able to restore accurately as well the underwater images. Our approach We propose a simple but effective 
strategy built on a multi-scale fusion technique. By de.ning properly several in­puts and weights we 
demonstrate the utility of our fusion-based ap­proach to enhance the underwater images. The .rst input 
is de.ned by the white balanced version of the im­age. To obtain the color corrected image the algorithm 
searches to equalize the median values of the basic color channels. This step is important since the 
input color channels of the underwater images are rarely balanced. We perform a linear adjustment of 
the his­togram, by stretching the original mean value to the desired mean value of the scene. Additionally, 
we have chosen that the mean ref­erence value (default 0.5) should be increased with a small degree t 
(t =0.15) of the actual scene mean, in order to preserve both the gray values and to obtain the desired 
white appearance of the existing white objects in the scene. The second input is obtained by applying 
the classical global min­max windowing method that aims to enhance the image appearance in the selected 
intensity window. This simple technique exploit ef­fectively the object coherence by enhancing the contrast 
within a subrange of the intensity values at the expense of the remaining intensity values. The weights 
of our algorithm are de.ned as following: Luminance weight map controls the luminance gain in the .nal 
re­sult. As a photograph is visually degraded, the general appearance tends to become .at. The weight 
values represents the standard de­viation between every R,G and B color channels and the lightness of 
the original input image. Contrast weight map yields high values to image elements such as edges and 
texture. To generate this map we rely on an effec­tive contrast indicator built on the Laplacian .lter 
computed on the grayscale of each image input. Chromatic weight map is designed to control the saturation 
gain of the result. This map is a simple saturation indicator and computes for every pixel the distance 
between the saturation value and the maximum of the saturation range using a Gauss curve. Saliency weight 
map is a quality map that estimates the degree of conspicuousness with respect to the neighborhood regions. 
This value is effectively computed based on the formulation introduced by Achanta et al. [Achantay et 
al. 2009]. Once the weight are obtained, we employed the normalized weight values by constraining that 
the sum at each pixel location of the weight maps to equal one. In the .nal step the inputs and the weights 
are merged by a multi-scale fusion process. To avoid halo­ing artifacts we opted for the widely-used 
multi-scale Laplacian pyramid decomposition. Practically, the .nal restored image is ob­tained mixing 
between the Laplacian inputs and Gaussian normal­ized weights at each scale level independently. References 
ACHANTAY, R., HEMAMIZ, S., ESTRADAY,F., AND SUSSTRUNKY, S. 2009. Frequency-tuned salient region detection. 
IEEE CVPR. ANCUTI, C. O., ANCUTI, C., AND BEKAERT, P. 2010. Effective single image de­hazing by fusion. 
IEEE International Conference on Image Processing(ICIP). ANCUTI, C. O., ANCUTI, C., HERMANS, C., AND 
BEKAERT, P. 2010. A fast semi­ inverse approach to detect and remove the haze from a single image. In 
Asian Conference on Computer Vision (ACCV). FATTAL, R. 2008. Single image dehazing. ACM Trans. on Graphics, 
SIGGRAPH. SCHECHNER,Y., AND AVERBUCH, Y. 2007. Regularized image recovery in scatter­ing media. IEEE 
Trans Pattern Anal Mach Intell.. SCHECHNER,Y. Y., AND KARPEL, N. 2004. Clear underwater vision. IEEE 
CVPR. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037754</article_id>
		<sort_key>390</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>33</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Filter based deghosting for exposure fusion video]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037754</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037754</url>
		<abstract>
			<par><![CDATA[<p>This work deals with a well known problem - the fact that consumer cameras are unable to capture the whole range of color luminance variations the human eye can perceive. Many techniques deal with this, the most widespread probably being High Dynamic Range Imaging. These works focus mainly on still images. Our focus in this work, however, is to improve videos taken from mobile devices by extending one such technique, Exposure Fusion, introduced in [Mertens et al. ].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809303</person_id>
				<author_profile_id><![CDATA[81436601243]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexandre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chapiro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Pure and Applied Mathematics, IMPA, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809304</person_id>
				<author_profile_id><![CDATA[81466640832]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Marcelo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cicconet]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Pure and Applied Mathematics, IMPA, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809305</person_id>
				<author_profile_id><![CDATA[81442611891]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Pure and Applied Mathematics, IMPA, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Castro, T., Chapiro, A., Cicconet, M., and Velho, L. Towards mobile hdr video. <i>Eurographics 2011</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Gallo, O., Chen, W., Gelfand, N., Tico, M., and Pulli, K. Artifact-free high dynamic range imaging. <i>ICIP 2009</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1338586</ref_obj_id>
				<ref_obj_pid>1338439</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Mertens, T., Kautz, J., and Van Reeth, F. Exposure fusion. <i>Pacific Graphics 2007</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Filter Based Deghosting for Exposure Fusion Video Alexandre Chapiro Marcelo Cicconet Luiz Velho Visgraf 
laboratory, Institute of Pure and Applied Mathematics, IMPA, Brazil.  (a) (b) (c) (d) Figure 1: Video 
Creation Pipeline: (a) Inputs, (b) Ghosting Coef.cients (after High-Pass .ltering) (c) Deghosted result 
(d) Comparison Introduction and Related Work This work deals with a well known problem -the fact that 
consumer cameras are unable to cap­ture the whole range of color luminance variations the human eye can 
perceive. Many techniques deal with this, the most widespread probably being High Dynamic Range Imaging. 
These works fo­cus mainly on still images. Our focus in this work, however, is to improve videos taken 
from mobile devices by extending one such technique, Exposure Fusion, introduced in [Mertens et al. ]. 
Exposure Fusion is a very ef.cient procedure that produces results similar to those of tone-mapped HDR 
images through a clever per­pixel averaging process. This process, however, was not designed for video 
processing. Attempts to apply it in such a way result in un­desirable artifacts, or ghosts during the 
combination of multiple differently exposed images containing moving objects. Although several deghosting 
methods exist, most are inef.cient when applied to this situation. For instance, Optical Flow does not 
work well with changing exposures in the input frames. In this work we present a novel method that deals 
with the elim­ination of such artifacts by using several carefully selected .lters and performing a local 
analysis. In order to tackle the variations brought by the changing exposure times between frames, we 
used features that are known to stay reasonably constant in this case ­edges, saliences and textures 
as detected by a High-Pass .lter. This method was inspired by our past work dealing with HDR video on 
mobile devices [Castro et al. ] and a deghosting method for HDR that involved the use of pixel regions 
as an estimator [Gallo et al. ]. Deghosting Method Exposure Fusion relies on three numerical parameters 
assigned to each pixel in the inbound images -Well-Exposedness, Detail and Saturation. We propose a fourth 
parameter -Ghosting. This parameter will de.ne the likelihood of presence of object movement on the scene 
at the current pixel. Given a set of images I = I1, ..., In, we wish to construct another set of n improved 
images (in this work, n =2). We begin by analyzing I1 and I2. In order to obtain a better reference of 
possible object movement, we analyze pixel regions instead of single pixels. The original images pixel 
color variations were found too steep due to the implicit exposure variation to generate reliable results. 
Because of this, the process outlined below is applied to the result of a regular High-Pass Laplacian 
.lter applied to each image, de­scribed as a function HP (Ik). Thus to .nd the Ghosting parameter of 
pixel (i, j), G(i, j) we analyze the regions A and B as given by (i - l : i + l, i - l : i + l) in each 
image. The process is then repeated for each pixel using a different input. The initial images I1,I2 
are now subject to a Low-Pass Gaussian .lter, given by LP (Ik). Following, we take HP (LP (Ik)) as our 
inputs and proceed to analyze their pixel regions. This method is then repeated with additional Low-Pass 
.lter steps. The pixel ar­eas are evaluated according to the following formula: G(i, j)i = 2l 2l 1 -|A 
- B|n,m. n=1n=1 The resulting obtained Ghosting coef.cients are multiplied to ob­tain the .nal pixel 
Ghosting parameter value. This process atten­uates the contribution of capture noise and irrelevant weaker 
high­frequencies, which disappear after consecutive Low-Pass applica­tions, resulting in less erroneous 
detections of non-movement high­frequency variations and a strengthened Ghosting parameter for pixels 
that involve true object movement. Finally, the ghosting parameter is used in the same way as the three 
initially proposed coef.cients and allows us to reduce the contribu­tion of pixels from I2 that show 
moving objects in relation to I1. This process is then repeated for consecutive pairs of images Ik,Ik+1, 
k =1, ..., n - 1 to form the .nal video. The whole process is outlined in Fig.1. More information is 
available at w3.impa.br/ achapiro/deghost, where a video is also presented. Obtention and Registration 
of Input Images In order to obtain the video frames used in this work, a Nokia N900 running Maemo 5 and 
the FCam API was used. We proceed to perform a multi­resolution alignment based on image pyramids. This 
step is neces­sary, as background pixel correspondence is crucial for this work. Both are explained in 
detail in our previous work [Castro et al. ]. Results and Future Work Testing of the algorithm showed 
good results when applied to regular situations, removing most of the ghosting artifacts and resulting 
in improved video quality. Some issues arise with quick movement relative to the camera s capture rate, 
where sometimes ghosting is not properly treated. Future work may include improvements to the algorithm 
s robust­ness to quick object and camera movement as well as improvements of the .lter-based deghosting 
technique through the use of multi­resolution to aid in the location of shifting objects. References 
CASTRO, T., CHAPIRO, A., CICCONET, M., AND VELHO, L. To­wards mobile hdr video. Eurographics 2011. GALLO, 
O., CHEN, W., GELFAND, N., TICO, M., AND PULLI, K. Artifact-free high dynamic range imaging. ICIP 2009. 
MERTENS, T., KAUTZ, J., AND VAN REETH, F. Exposure fusion. Paci.c Graphics 2007. Copyright is held by 
the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037755</article_id>
		<sort_key>400</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>34</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Gradient domain HDR compositing]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037755</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037755</url>
		<abstract>
			<par><![CDATA[<p>High dynamic range (HDR) image compositing addresses dynamic range limitations by combining valid information from multiple differently-exposed low dynamic range (LDR) images. Traditional HDR methods (which we will call <i>intensity-based</i>) compute a weighted combination of pixel irradiance after correcting for differences in exposure. These methods require knowledge of the mapping from scene irradiance to image brightness called the camera response function <i>f</i>, as well as the exposure settings (combined information about shutter speed and film sensitivity), <i>k</i>. However, often times neither <i>f</i> nor <i>k</i> are available beforehand; most digital cameras do not have the option to shoot in raw (linear) mode, and do not offer a guarantee that the same processing will be performed on consecutive images.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809306</person_id>
				<author_profile_id><![CDATA[81365594740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Disney Research Z&#252;rich]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809307</person_id>
				<author_profile_id><![CDATA[81100603625]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[James]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Davis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[UC Santa Cruz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>258884</ref_obj_id>
				<ref_obj_pid>258734</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Debevec, P. E., and Malik, J. 1997. Recovering high dynamic range radiance maps from photographs. In <i>ACM Trans. Graph</i>., ACM Press/Addison-Wesley Publishing Co., New York, NY, USA, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>566573</ref_obj_id>
				<ref_obj_pid>566654</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Fattal, R., Lischinski, D., and Werman, M. 2002. Gradient domain high dynamic range compression. In <i>ACM Trans. Graph</i>., ACM, New York, NY, USA, 249--256.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>649359</ref_obj_id>
				<ref_obj_pid>645318</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Grossberg, M. D., and Nayar, S. K. 2002. What can be known about the radiometric response from images? In <i>European Conference on Computer Vision, 2002 Proceedings of</i>, Springer-Verlag, London, UK, 189--205.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>882269</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Perez, P., Gangnet, M., and Blake, A. 2003. Poisson image editing. <i>ACM Trans. Graph. 22</i>, 3, 313--318.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Gradient Domain HDR Compositing [Extended Abstract] Oliver Wang James Davis Disney Research Z¨UC Santa 
Cruz urich 1 Introduction and Context High dynamic range (HDR) image compositing addresses dynamic range 
limitations by combining valid information from multiple differently-exposed low dynamic range (LDR) 
images. Traditional HDR methods (which we will call intensity-based) compute a weighted combination of 
pixel irradiance after correcting for differ­ences in exposure. These methods require knowledge of the 
map­ping from scene irradiance to image brightness called the camera response function f, as well as 
the exposure settings (combined in­formation about shutter speed and .lm sensitivity), k. However, often 
times neither f nor k are available beforehand; most digital cameras do not have the option to shoot 
in raw (linear) mode, and do not offer a guarantee that the same processing will be performed on consecutive 
images. We propose a fundamentally different method for computing HDR images that combines log-gradient 
information from a stack of dif­ferently exposed images, and integrates the combined gradient .eld to 
compute a .nal HDR image. Our method does not require know­ing or estimating the exposure values k, and 
produces tonemapped results which have both visually and quantitatively equivalent accu­racy to intensity-based 
approaches. HDR imaging has been studied since before the advent of digi­tal cameras, but recent advances 
in computing have made the task simpler of combining LDR image stacks easier [Debevec and Ma­lik 1997]. 
However, all prior HDR methods have so far used an intensity-based paradigm for HDR compositing. For 
this reason, many of these HDR methods assume known k and compute f from image data. While methods exist 
that estimate both simultaneously, it was shown that images alone cannot uniquely determine both f and 
k [Grossberg and Nayar 2002] without limiting assumptions. Our method instead only requires f , and operates 
independent of k. Gradient information has been used in numerous imaging prob­lems. For example, modifying 
image gradients and solving a Pois­son equation has been used for image editing [Perez et al. 2003]. 
In the HDR domain, Fattal et al.[2002] uses gradient information to perform HDR range compression. This 
is a fundamentally differ­ent problem than we are attempting to solve, as we desire to com­bine images 
together to create an HDR image rather than produce a tonemapped LDR image. 2 Method Our method is simple 
and robust. First, an LDR image stack is cap­tured and the camera response function is used to convert 
each im­age into a linear space. Then, we perform a weighted combination of irradiance log-gradients 
(as opposed to irradiance intensities) to compute a .nal HDR gradient .eld. Each gradient is weighted 
by a function that is high at middle brightness values and low at brightness extremes. This encodes the 
fact that we have more information towards the center of the cam­era s brightness response than at the 
edges. In addition, irradiance values and log-gradients computed from saturated or underexposed pixels 
will be incorrect, and so this weighting scheme is also nec­essary to remove their contributions. Unlike 
prior methods, this log-gradient approach naturally handles regions that are saturated in all LDR images, 
reproducing the expected .at output. Ground Truth GradientHDR IntensityHDR Figure 1: A comparison of 
results showing visual similarity be­tween methods. Low-exposure and high-exposure linear mappings of 
the HDR images are shown above and below respectively. After we have solved for an HDR gradient .eld, 
we .nd the least­squares best .t surface that matches this gradient .eld by solving a Poisson equation 
assuming Neumann boundary conditions. This approach yields a solution valid up to an unknown additive 
term. However, in our application, reconstructed irradiance values are al­ready on an arbitrary scale 
and range, so for a grayscale image this unknown offset does not matter. However, the relative offsets 
be­tween the RGB color channels will affect the resulting color of our HDR image. We therefore lock down 
the relative color channels by introducing a single constraint in our linear system that .xes the rel­ative 
color at a randomly selected, non-saturated pixel in one LDR image. We found that our method was robust 
to the selection of this pixel. As an additional measure, random non-local irradiance differences are 
included in gradient constraints as well as local 4­way neighbor differences to prevent any long-range 
accumulation of error during integration. We validated our results with visual comparison as well as 
RMSE as compared to ground truth HDR images. Please refer to supplementary materials for further results. 
 3 Conclusion We have presented a novel method for generating HDR images by combining log-gradient information 
from a set of differently ex­posed LDR images. This approach is simple, fast and robust and achieves 
visually indistinguishable results to intensity based ap­proaches. We were able to use our algorithm 
without modi.cation on a large number of sample datasets with good results. References DEBEVEC, P. E., 
AND MALIK, J. 1997. Recovering high dynamic range radiance maps from photographs. In ACM Trans. Graph., 
ACM Press/Addison-Wesley Pub­ lishing Co., New York, NY, USA, 369 378. FATTAL, R., LISCHINSKI, D., AND 
WERMAN, M. 2002. Gradient domain high dynamic range compression. In ACM Trans. Graph., ACM, New York, 
NY, USA, 249 256. GROSSBERG, M. D., AND NAYAR, S. K. 2002. What can be known about the radiometric response 
from images? In European Conference on Computer Vision, 2002 Proceedings of, Springer-Verlag, London, 
UK, 189 205. PEREZ, P., GANGNET, M., AND BLAKE, A. 2003. Poisson image editing. ACM Trans. Graph. 22, 
3, 313 318. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037756</article_id>
		<sort_key>410</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>35</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Interactive Manga retargeting]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037756</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037756</url>
		<abstract>
			<par><![CDATA[<p>The existing content aware image retargeting methods are mainly suitable for natural images, and do not perform well on line drawings. Such methods tend to regard homogeneous areas as less important. For line drawings, this is not always true.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809308</person_id>
				<author_profile_id><![CDATA[81488646958]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Matsui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[matsui@hal.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809309</person_id>
				<author_profile_id><![CDATA[81100573369]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Toshihiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamasaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yamasaki@hal.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809310</person_id>
				<author_profile_id><![CDATA[81100430459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kiyoharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Aizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[aizawa@hal.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360615</ref_obj_id>
				<ref_obj_pid>1399504</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Rubinstein, M., Shamir, A., and Avidan, S. 2008. Improved seam carving for video retargeting. In <i>SIGGRAPH 2008</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Interactive Manga Retargeting YusukeMatsui * ToshihikoYamasaki KiyoharuAizawa TheUniversity ofTokyo 
 c Figure 1: (a)source image, (b)selected regions, (c)scaling, (d)cropping, (e)seam carving, (f)proposed 
method @Akira Toriyama / Shueisha 1 Introduction The existing content aware image retargeting methods 
are mainly suitable for natural images, and do not perform well on line draw­ings. Such methods tend 
to regard homogeneous areas as less im­portant.For linedrawings,thisisnot alwaystrue. We present a method 
for interactively retargeting black &#38; white Manga drawings. The method extracts important areas ofthe 
im­age using scribbles by the user. It then computes candidate images using cropping and seamcarving[Rubinsteinet 
al.2008]. Finally, the user interactively combines these two effects to create the best result.Ourmethodforces 
theuser to intervene,but createsabetter retargeted result than existing methods. 2 Region Selection 
We present a region selection method using mouse input and bi­narylabeling.A character inManga isusuallydrawnusing 
several, connected, black lines. The user selects a region by left-click and dragging on a part of a 
character. The labeling algorithm runs and selects the lines that have the same labels as the part srcibbled 
on. Similary, right clicking and dragging is used to release excess re­gions. In general, a character 
can be selected using only two or three scribbles. The selected region is .lled and used as an impor­tance 
mask. Fig.2 illustrates an example of region selection. The red linescorrespond toscribbleswithleft-clicks,andtheblue 
lines toright-clicks.Withthese,theimportancemaskiscreated toguide the seam carvingprocess. Figure 2: 
Object of interest selection c @Akira Toriyama / Shueisha * e-mail: matsui@hal.t.u-tokyo.ac.jp e-mail: 
yamasaki@hal.t.u-tokyo.ac.jp e-mail: aizawa@hal.t.u-tokyo.ac.jp 3 Interactive Retargeting We propose 
an interactive retargeting method. If an im­age should be made K pixels narrower, the method .rst ' computes 
candidate images using this equation. Ixyz = SCz (CRR(CRL(I))) s.t.x + y + z = K. CRL and CRR yx xy mean 
cropping the x leftmost columns and y rightmost columns of pixels. SCz conducts seam carving with the 
importance mask cre­atedby regionselection.Aftercomputing candidate images,auser creates thebest retargetedimagebyinteractivelyspecifying 
x and y usingtwo sliders(Fig.3). The seam carving algorithm automatically adjustsfor thecorrect valueof 
z. Figure 3: Selecting Interface c@Akira Toriyama / Shueisha 4 Results and Conclusion Fig.1 compares 
the proposed method with some other methods. Fig.1(a) is a source image. Fig.1(b) is the importance mask 
generated by region selection. Fig.1(c) and Fig.1(e) show im­ages retargeted by scaling &#38; seam carving, 
which are unnaturally shrunk.Thecroppedimage inFig.1(d)doesnot contain thebaloon. Fig.1(f),created using 
ourmethod,hasnorshrink or lost important regions. It isevident that theproposed method achives improved 
retargeting ofblack&#38; whitedrawings.While thepreviousmethodsaimatau­tomaticretargeting,wehaveshown 
thatmuchbetterresultscanbe obtained withminimal user intervention.Ourfuturework isreduc­ingthecomputational 
costbecausetheprocessingtimeincreasesin accordance with the number of variations of x and y. References 
RUBINSTEIN, M., SHAMIR, A., AND AVIDAN,S. 2008.Improved seam carvingfor video retargeting. In SIGGRAPH 
2008,ACM. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037757</article_id>
		<sort_key>420</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>36</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Layered photo pop-up]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037757</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037757</url>
		<abstract>
			<par><![CDATA[<p>A common technique in documentaries is to animate photographs by panning across them slowly. More recently, it has become popular to divide such photographs into layers, and to animate these layers as moving over each other to create a motion parallax effect, commonly known as the "3D Ken Burns effect". Producing this effect involves a laborious manual process that requires hours of manual rotoscoping, clone-brushing, positioning in 3D, and adjusting the panning speeds of individual layers.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809311</person_id>
				<author_profile_id><![CDATA[81488672247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Lech]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[&#346;wirski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lech.swirski@cl.cam.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P2809312</person_id>
				<author_profile_id><![CDATA[81443597280]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Christian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Richardt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[christian.richardt@cl.cam.ac.uk]]></email_address>
			</au>
			<au>
				<person_id>P2809313</person_id>
				<author_profile_id><![CDATA[81100184450]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Neil]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Dodgson]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Cambridge]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[neil.dodgson@cl.cam.ac.uk]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chuang, Y.-Y., Curless, B., Salesin, D. H., and Szeliski, R. 2001. A Bayesian approach to digital matting. In <i>CVPR</i>, 264--271.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2320602</ref_obj_id>
				<ref_obj_pid>2319036</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Criminisi, A., P&#233;rez, P., and Toyama, K. 2004. Region filling and object removal by exemplar-based image inpainting. <i>IEEE Transactions on Image Processing 13</i>, 9 (Sep.), 1--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1015720</ref_obj_id>
				<ref_obj_pid>1015706</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Rother, C., Kolmogorov, V., and Blake, A. 2004. Grab-Cut: Interactive foreground extraction using iterated graph cuts. <i>ACM Transactions on Computer Graphics 23</i>, 3, 309--314.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Layered Photo Pop-Up Lech ´ Swirski* Christian Richardt Neil A. Dodgson University of Cambridge University 
of Cambridge University of Cambridge  Figure 1: Starting from an image with depth, we detect and separate 
objects. Each object is segmented using a depth-aware soft segmentation, and the region behind is .lled 
using depth-aware inpainting. We use our layered representation to create novel views with depth-of-.eld, 
which is suitable for a documentary-style pop-up effect, similar to the popular 3D Ken Burns effect. 
1 Introduction A common technique in documentaries is to animate photographs by panning across them slowly. 
More recently, it has become pop­ular to divide such photographs into layers, and to animate these layers 
as moving over each other to create a motion parallax effect, commonly known as the 3D Ken Burns effect 
. Producing this effect involves a laborious manual process that requires hours of manual rotoscoping, 
clone-brushing, positioning in 3D, and adjust­ing the panning speeds of individual layers. Our work investigates 
how to automate this given depth informa­tion. We describe a novel work.ow which, given an image with 
depth, mimics the manual creation of this motion parallax effect, by creating a layered image representation. 
Objects in the image are segmented into separate layers, and the regions behind them are .lled by inpainting 
from the surrounding background. This imi­tation of the manual process gives a user the means to adjust 
the layers at various stages; with minimal interaction, objects can be manually marked for segmentation, 
or automatically .lled regions can be augmented using human knowledge of the scene. For our approach, 
we need a depth map. There are several ways of obtaining one. The most widespread is to calculate depth 
from stereoscopic imagery using stereo correspondence. Stereoscopic photographs are available historically, 
and are cheap to produce to­day. Another source of depth maps is active range-sensing cameras, for example 
using time-of-.ight or structured light. The most pop­ular recent example is the Microsoft Kinect. These 
greatly simplify the capture of high-quality depth maps alongside colour images. 2 Our Approach We create 
layered images by iteratively selecting the foremost ob­ject in the image, segmenting it onto a new layer, 
and .lling the hole behind it. We assume that layers are separated by depth dis­continuities, so that 
they can move independently over each other under motion parallax. To separate elements, we detect these 
depth discontinuities by thresholding depth differences between neighbouring pixels, and mark them as 
object boundaries. The foremost element is found by comparing the depths of pixels in neighbouring regions. 
*e-mail:lech.swirski@cl.cam.ac.uk e-mail:christian.richardt@cl.cam.ac.uk e-mail:neil.dodgson@cl.cam.ac.uk 
Once the foremost element has been selected, we use a modi.ed version of GrabCut [Rother et al. 2004] 
to segment it from the re­ maining background. This has two steps: a hard segmentation fol­lowed by a 
soft, alpha matting re.nement. Pixels are classi.ed as foreground or background by .tting 4D GMMs to 
the data in RGBZ space, and we use graph cuts to perform a clean segmentation. Following the hard segmentation, 
we apply alpha matting along the contour to create a soft segmentation. We calculate the alpha matte 
along the hard contours, using only the colour image as in GrabCut, and we compute the colour of each 
foreground pixel using Bayesian matting [Chuang et al. 2001]. Once the alpha matte is computed, we recalculate 
the depth of all semi-transparent pixels along the object border using plane-.tting. Segmenting out a 
layer leaves behind a hole of unknown data, which we mark as invalid . We .ll this hole with exemplar-based 
inpainting [Criminisi et al. 2004], extended to use and .ll depth information. This technique .lls holes 
by copying patches from other parts of the image; a patch is .lled by .nding the most similar patch from 
the remaining valid image pixels. Criminisi et al. use colour distance to determine patch similarity; 
we augment this to favour patches which are similar in 3D shape, and which are behind the object being 
erased. Our representation is designed to allow arbitrary rendering of the given scene. We render the 
model by transforming each layer s mesh based on data from its depth map. Each vertex is positioned in 
3D as a function of its (x, y) position in the image and the corre­sponding value d in the depth map. 
This function is implemented as part of the vertex shader in the graphics pipeline. References CHUANG, 
Y.-Y., CURLESS, B., SALESIN, D. H., AND SZELISKI, R. 2001. A Bayesian approach to digital matting. In 
CVPR, 264 271. CRIMINISI, A., P ´ EREZ, P., AND TOYAMA,K.2004.Region.lling and object removal by exemplar-based 
image inpainting. IEEE Transactions on Image Processing 13, 9 (Sep.), 1 13. ROTHER, C., KOLMOGOROV, V., 
AND BLAKE, A. 2004. Grab-Cut: Interactive foreground extraction using iterated graph cuts. ACM Transactions 
on Computer Graphics 23, 3, 309 314. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037758</article_id>
		<sort_key>430</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>37</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Light-field retargeting with focal stack seam carving]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037758</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037758</url>
		<abstract>
			<par><![CDATA[<p>With increasing sensor resolutions of digital cameras, light-field imaging is becoming more and more relevant, and might even replace classical 2D imaging in photography sooner or later. It enables, for instance, digital refocussing and perspective changes after capturing. Rescaling light fields to different resolutions and aspect ratios, however, is challenging. As for regular image and video content, a linear scaling alters the aspect ratio of recorded objects in an unnatural way. In contrast, image and video retargeting utilizes a nonlinear and content-based scaling. Applying image retargeting to individual video frames independently does not retain temporal consistency. Similarly, applying image retargeting naively to the spatial domain of light fields will not retain angular consistency. We present a first approach to light-field retargeting. It allows compressing or stretching light-fields while retaining angular consistency.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809314</person_id>
				<author_profile_id><![CDATA[81488672490]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Clemens]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Birklbauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[clemens.birklbauer@jku.at]]></email_address>
			</au>
			<au>
				<person_id>P2809315</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Levin, A., and Durand, F. 2010. Linear view synthesis using a dimensionality gap light field prior. In <i>CVPR</i>, 1831--1838.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light-Field Retargeting with Focal Stack Seam Carving Clemens Birklbauer and Oliver Bimber Johannes 
Kepler University Linz *  Figure 1: Steps of our light-.eld retargeting process with an example of 24% 
width compression. 1 Introduction and Motivation With increasing sensor resolutions of digital cameras, 
light-.eld imaging is becoming more and more relevant, and might even re­place classical 2D imaging in 
photography sooner or later. It en­ables, for instance, digital refocussing and perspective changes after 
capturing. Rescaling light .elds to different resolutions and aspect ratios, however, is challenging. 
As for regular image and video con­tent, a linear scaling alters the aspect ratio of recorded objects 
in an unnatural way. In contrast, image and video retargeting utilizes a nonlinear and content-based 
scaling. Applying image retargeting to individual video frames independently does not retain temporal 
consistency. Similarly, applying image retargeting naively to the spatial domain of light .elds will 
not retain angular consistency. We present a .rst approach to light-.eld retargeting. It allows com­pressing 
or stretching light-.elds while retaining angular consis­tency. 2 Our Approach While stereo-pair retargeting 
or 3D retargeting could potentially be extended to light-.eld retargeting, both would rely on high­quality 
depth reconstruction which is dif.cult to achieve for com­plex scenes. We follow an entirely different 
approach that does not depend on precise depth information (cf. .gure 1): First, we convert a light .eld 
(1) into a focal stack (2) by render­ing its focal slices from the central perspective (1, red border) 
with correspondingly adjusted focal length. The focal stack can be con­verted back to a light .eld (7,8) 
by shifting and averaging its slices according to the desired perspective and by deconvolving the re­sulting 
image with a perspective-dependent point-spread function, as explained in [Levin and Durand 2010]. Before 
we do this for all perspectives of the light .eld we apply seam carving to the central perspective. The 
seams (4) are mainly derived from the L1-norm of the gradients with forward energy calculation (3, green). 
Since this central perspective matches the perspective of the focal stack, we can remove the same calculated 
seams in all of its slices without violating angular consistency. Removing pixels where these seams are 
cutting through out-of-focus regions in slices of the focal stack, *{.rstname.lastname}@jku.at however, 
causes deconvolution (ringing) errors around edges dur­ing the reconstruction of the light-.eld perspectives. 
Therefore, we avoid cutting seams through these critical regions (5). They are determined by integrating 
the RGB differences between all focal stack slices and the deconvolved central perspective. This integral 
is used as an additional energy term for seam carving (3, red). It does not completely avoid critical 
seams. Remaining artifacts are attenuated by convolving the focal stack slices in regions with new gradients 
that are created by seam carving (6). We assume that background objects are visually less salient. Therefore, 
we penal­ize seams through foreground objects to better preserve them. This is achieved by reconstructing 
a coarse depth map from the focal stack. The depth is used as a third energy term for seam carving (3, 
blue). 3 Results and Limitations To our knowledge, we presented the .rst approach to light-.eld re­targeting. 
It enables stretching and compressing light .elds while preserving angular consistency without the need 
for reconstruct­ing precise depth information (see supplementary video for results). However, it has 
several limitations: The conversion of focal stacks to light .elds, as explained in [Levin and Durand 
2010], leads to imperfect reconstructions at occlusion boundaries. It also requires shifting the focal 
slices according to the constructed perspective images. For the inverse process, a sim­ilar shift of 
the perspective images is required to construct the focal slices. Therefore, the spatial resolution of 
resulting light .elds is cropped by the maximum shift that is applied during these two con­version processes. 
In future, we want to investigate more advanced techniques for .l­tering critical seams. References 
LEVIN, A., AND DURAND, F. 2010. Linear view synthesis using a dimensionality gap light .eld prior. In 
CVPR, 1831 1838. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037759</article_id>
		<sort_key>440</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>38</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Stereoscopic media editing based on 3D cinematography principles]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037759</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037759</url>
		<abstract>
			<par><![CDATA[<p>Developments of stereoscopic displays and binocular cameras have made capture and display of stereoscopic media easy. There will be strong needs for stereoscopic media processing to crop, make transitions, resize or stabilize stereoscopic imagery as for conventional 2D media. However, despite of fast progresses on hardware, few have been made towards the stereoscopic media processing side, especially for stereoscopic media captured by consumers. This paper introduces 3D cinematography principles and applies them to stereoscopic media processing to maintain viewing comfort and pleasure. Stereoscopic video stabilization and stereoscopic photo slideshows are used as examples to demonstrate these principles' usage in stereoscopic media processing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809316</person_id>
				<author_profile_id><![CDATA[81488659499]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Chun-Wei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809317</person_id>
				<author_profile_id><![CDATA[81490649733]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tz-Huan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Huang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809318</person_id>
				<author_profile_id><![CDATA[81490651550]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ming-Hsu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809319</person_id>
				<author_profile_id><![CDATA[81319495569]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ken-Yi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809320</person_id>
				<author_profile_id><![CDATA[81542646056]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Chia-Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Liang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Lytro Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809321</person_id>
				<author_profile_id><![CDATA[81350582710]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Yung-Yu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chuang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Taiwan University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Lee, K.-Y., Chuang, Y.-Y., Chen, B.-Y., and Ouhyoung, M. 2009. Video stabilization using robust feature trajectories. In <i>Proceedigns of the IEEE ICCV</i>, 1397--1404.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Mendiburu, B. 2009. <i>3D movie making: stereoscopic digital cinema from script to screen</i>. Focal Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Stereoscopic Media Editing based on 3D Cinematography Principles Chun-Wei Liu1 Tz-Huan Huang1 Ming-Hsu 
Chang1 Ken-Yi Lee1 Chia-Kai Liang2 Yung-Yu Chuang1 1National Taiwan University 2Lytro Inc. 1 Introduction 
Developments of stereoscopic displays and binocular cam­eras have made capture and display of stereoscopic 
media easy. There will be strong needs for stereoscopic media pro­cessing to crop, make transitions, 
resize or stabilize stereo­scopic imagery as for conventional 2D media. However, de­spite of fast progresses 
on hardware, few have been made to­wards the stereoscopic media processing side, especially for stereoscopic 
media captured by consumers. This paper in­troduces 3D cinematography principles and applies them to 
stereoscopic media processing to maintain viewing comfort and pleasure. Stereoscopic video stabilization 
and stereo­scopic photo slideshows are used as examples to demonstrate these principles usage in stereoscopic 
media processing. 2 Principles and applications 3D cinematography principles. 3D .lmmakers have learnt 
and introduced several 3D cinematography principles to reduce viewing discomfort and to enhance the immersion 
experience when watching 3D .lms. Some of these principles are not only useful for professional .lmmaking 
but also for stereoscopic media processing [Mendiburu 2009]. (1) Main­taining coordination among views. 
The left and right images should be processed in perfect coordination to avoid desyn­chronization and 
mismatch between views. (2) Having a continuous depth chart. If the depth brackets of two neigh­boring 
shots are too far apart, viewers are forced to readjust their convergence from one shot to another. In 
this case, the stereopsis will be interrupted and the suspension of disbelief is disturbed. (3) Placing 
rest areas between strong 3D shots. Viewers could experience eye strain if they stare at strong 3D e.ects 
for too long. Thus, strong 3D shots should be in­terspersed with low 3D sequences. (4) Using shallow 
depth of .eld for shots with excessive depth brackets. If the fore­ground and the background are too 
far away from each other, the excessive depth range could make depth fusion di.cult. Shallow depth of 
.eld can be used to isolate characters and to draw audience s attention on the main character. Stereoscopic 
video stabilization. Independent applica­tion of conventional 2D video stabilization on both video streams 
completely ignores the coordination and synchro­nization between two views. The results could bear the 
unwanted vertical parallax and inconsistent horizontal dis­parities. The former destroys the depth perception 
as the left and right views are not perfectly horizontally aligned any more. The later leads to shimmering 
artifacts. The in­consistent time-varying horizontal disparities are interpreted as time-varying depths 
by our brains. Thus, viewers per­ceive that objects move forward and backward arbitrarily. Therefore, 
it is important to obey the constraints of hori­zontal alignment and consistent disparity during stabiliza­tion. 
Our method achieves stereoscopic video stabilization by incorporating these constraints into the optimization 
pro­cess of Lee et al. s 2D video stabilization method [Lee et al. 2009]. Our method .rst extracts robust 
feature trajectories from the input video and builds correspondences of features Figure 1: After stabilization, 
the video maintains horizon­tal alignment and consistent disparity (left), and becomes stabilized as 
the trajectories are smooth in time (right).  Figure 2: Results of user studies for stereoscopic video 
sta­bilization (left) and stereoscopic photo slideshows (right). across the left and right views. Optimization 
is then per­formed to .nd a set of transformations to smooth out these trajectories while obeying these 
constraints. Stereoscopic photo slideshows. The proposed stereo­scopic slideshow system works as follows. 
(1) Optionally, shu.e the photo sequence so that the depth chart is more continuous if it is allowed 
to change the display order of images. For this, we have developed a photo shu.ing al­gorithm to reorder 
photos so that neighboring photos have similar depth ranges. (2) Within display of an image, if the depth 
bracket of the image is excessive, apply shallow depth of .eld blur .ltering to gradually direct viewer 
s attention from the foreground to the background. For bringing only a part of the image to focus, each 
pixel is blurred according to its estimated depth to achieve the depth of .eld e.ects. (3) During transitions 
between two images, if there exists a large depth jump between the in image and the out image, apply 
the active depth cuts trick. In addition, when switching from the out image to the in image, use the 
fade-in/fade-out ef­fects which gradually switch from the out image, to a blank image and then to the 
in image. Figure 1 gives an example for stereoscopic video stabi­lization. User studies show that, by 
incorporating 3D cine­matography principles, our methods provide more comfort­able and enjoyable 3D viewing 
experiences (Figure 2). References Lee, K.-Y., Chuang, Y.-Y., Chen, B.-Y., and Ouhyoung, M. 2009. Video 
stabilization using robust feature trajectories. In Proceedigns of the IEEE ICCV, 1397 1404. Mendiburu, 
B. 2009. 3D movie making: stereoscopic digital cinema from script to screen. Focal Press. Copyright is 
held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037760</article_id>
		<sort_key>450</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>39</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[Touch interface on back of the hand]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037760</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037760</url>
		<abstract>
			<par><![CDATA[<p>In this paper we propose a new computer-human interface which uses the back of the hand for pointer control.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809322</person_id>
				<author_profile_id><![CDATA[81421601844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakatsuma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tsuma@alab.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809323</person_id>
				<author_profile_id><![CDATA[81100555829]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiroyuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shinoda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[shino@alab.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809324</person_id>
				<author_profile_id><![CDATA[81100425693]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasutoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[makino@sdm.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809325</person_id>
				<author_profile_id><![CDATA[81508684469]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Katsunari]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sato@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809326</person_id>
				<author_profile_id><![CDATA[81100377136]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maeno]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[maeno@sdm.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1449746</ref_obj_id>
				<ref_obj_pid>1449715</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Butler, A., Izadi, S., and Hodges, S. 2008. Sidesight: multi-"touch" interaction around small devices. In <i>Proceedings of the 21st annual ACM symposium on User interface software and technology</i>, ACM, New York, NY, USA, UIST '08, 201--204.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1753394</ref_obj_id>
				<ref_obj_pid>1753326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Harrison, C., Tan, D., and Morris, D. 2010. Skinput: appropriating the body as an input surface. In <i>Proceedings of the 28th international conference on Human factors in computing systems</i>, ACM, New York, NY, USA, CHI '10, 453--462.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Kei Nakatsuma:, Hiroyuki Shinoda The UniversityofTokyo 1 Introduction In this paper we propose a new 
computer human interface which uses the back of the hand for pointer control. Recently we can use many 
touch based interface including touch padonPCs,cell phones, tabletdevicesand portablegames. With these 
devices, they can arrange manydifferent types ofbuttons on the screen depending on applications. It enables 
small devices to achieve various functions on a small screen. There is one drawback for these touch panel 
devices. Users cannot feel haptic feedback like click since they do not have physical buttons. As a result, 
it is dif.cult to input data without anyvisual or auditory cues. User cannot input date while theyarewalking, 
for example. In order to solve the issue, we propose a new interface which uses the back of the hand 
as an input surface of pointing device. If we utilize our skin as an input surface, we can feel and perceive 
which area is tapped. There are previous researches that used user s body as an input in­terface. Harrison 
et al. proposed Skinput which detects tapping sound with microphones put on the skin surface to localize 
tapping position [Harrison et al. 2010]. User can input commandby chang­ing the tapping position on their 
body. We apply the similar idea to the small area: opisthenar (back of the hand). The area is relatively 
.at and user can use there like a conventional touch pad device. User can feel a precise tactile feedback 
since the sensitivity of a hand is higher than the other body parts. One of the important advantages 
is that manypeople use a wristwatch. If we improve a wristwatch so that it can detect .nger positiononthe 
opishtenar,wedonotneed additionalexternal devices. Since we used IR re.ection for detecting .nger position, 
ourdevice canbeusednotonlyforthebackofthehand,butforany .at surfaces. In our demo, we show how it works. 
We also show how easy you can input data by using your body with a wristwatch size device. 2 Prototype 
System Figure1shows our system.It consistsof three parts. Oneisapo­sition detection unit. We used an 
infrared re.ector for measuring two dimensional positionof.ngeronthebackofthehand.Weput 7 infrared LED 
detector pairs at the side of the wristwatch. The basic principle of .nger position detection has been 
also used in SideSight [Butler et al. 2008]. The second unit is a piezoelectric sensor which detects 
tapping sound on the opisthenar. It might pos­sible to detect tapping motion only from the infrared sensors, 
how­ever,it becomeseasyandrobustwhenweuseapiezoelectric sensor in addition. The sensor also can detect 
friction sound. This can be used for identifying various gestures on the back of the hand. The e-mail: 
tsuma@alab.t.u-tokyo.ac.jp e-mail: shino@alab.t.u-tokyo.ac.jp e-mail: makino@sdm.keio.ac.jp §e-mail: 
sato@tachilab.org ¶e-mail: maeno@sdm.keio.ac.jp  Figure1:Touch interfacedevice.Auser can input datebytouching 
the back of the hand. The user can feel the position of the manip­ulation from his/her own hand, the 
system do not need anytactile display device to give touch sensation. thirdunitisthedisplay.Wecan controlthe 
cursoronthe screenjust beside the screen. It makes user easy to input data to small devices. 3 Applications 
We showed one example of application. Auser can control his/her presentation slides with our proposed 
interface. The device can make it easy to control the slides. Since we can know the contact position 
by tactile perception at the opisthenar, we do not need to see the surface for input. This advan­tage 
can be used when a user is walking on the street, for example. The user can change music or call someone 
by touching his/her back of the hand. Acknowledgements Thisworkwas supportedinpartbyGrant in AidforYoung 
Scien­tistsB(21700185) and Grant in Aid for JSPS Fellows (21-5508) in Japan. References BUTLER,A.,IZADI,S., 
ANDHODGES,S. 2008. Sidesight: multi­ touch interaction around small devices. In Proceedings of the 21st 
annual ACM symposium on User interface software and technology,ACM,NewYork,NY, USA, UIST 08, 201 204. 
HARRISON, C., TAN, D., AND MORRIS, D. 2010. Skinput: ap­propriating the body as an input surface. In 
Proceedings of the 28th international conference on Human factors in computing systems,ACM,NewYork,NY, 
USA, CHI 10, 453 462. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>Grant-in-Aid for JSPS Fellows in Japan</funding_agency>
			<grant_numbers>
				<grant_number>21-5508</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>Grant-in-Aid for Young Scientists B</funding_agency>
			<grant_numbers>
				<grant_number>21700185</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2037761</article_id>
		<sort_key>460</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>40</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>15</seq_no>
		<title><![CDATA[Towards a computer vision shader language]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037761</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037761</url>
		<abstract>
			<par><![CDATA[<p>Computer vision is a complex field which can be challenging for those outside the research community to apply in the real world. The problem preventing widespread adoption is the lack of a formulation which separates the understanding of a task from the knowledge of which algorithm to use as a solution. Successful abstractions in computer graphics, such as OpenGL and DirectX, hide the algorithmic details of rendering behind a powerful interface. We propose a similar abstraction for computer vision, analogous to a shader language, which provides developers with access to sophisticated vision methods without requiring specialist knowledge. Our contribution is an interface which presents developers with mechanisms to describe their vision task; the description is interpreted to select an appropriate method and provide a solution.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809327</person_id>
				<author_profile_id><![CDATA[81442605718]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gregor]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Miller]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[gregor@ece.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809328</person_id>
				<author_profile_id><![CDATA[81323494292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Oldridge]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[steveo@ece.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809329</person_id>
				<author_profile_id><![CDATA[81327488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ssfels@ece.ubc.ca]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Towards a Computer Vision Shader Language Gregor Miller, Steve Oldridge and Sidney Fels* Human Communication 
Technologies Laboratory University of British Columbia  (a) Colour Segmentation (b) HDR Registration 
(c) Image Differencing (d) Face Detection Figure 1: The Open Vision Language (OpenVL) is a new language 
analogous to a shader, designed to provide high-level access to computer vision methods covering a wide 
range of problems. OpenVL supports many vision problems; we have implemented segmentation (colour in 
(a)), various types of registration (such as HDR stacking in (b)), image differencing in (c) and face 
detection in (d). 1 Open Vision Language Computer vision is a complex .eld which can be challenging for 
those outside the research community to apply in the real world. The problem preventing widespread adoption 
is the lack of a formu­lation which separates the understanding of a task from the knowl­edge of which 
algorithm to use as a solution. Successful abstrac­tions in computer graphics, such as OpenGL and DirectX, 
hide the algorithmic details of rendering behind a powerful interface. We propose a similar abstraction 
for computer vision, analogous to a shader language, which provides developers with access to sophis­ticated 
vision methods without requiring specialist knowledge. Our contribution is an interface which presents 
developers with mecha­nisms to describe their vision task; the description is interpreted to select an 
appropriate method and provide a solution. Many computer vision problems can be divided up into smaller 
sub­problems and solved by providing solutions to each sub-problem. This applies conceptually as well 
as algorithmically and so we base our vision shader language on this principle. We allow the user to 
describe vision tasks by dividing the conceptual problem into sub­tasks, then the sequence of sub-tasks 
is analysed to select a suitable method to apply. Our language is made up of these sub-tasks, which we 
term operations, and we provide a core set to span the range of computer vision problems. Each of these 
operations accepts descriptions of the conditions un­der which they must operate. For example, if requesting 
detection we provide the developer with a means to describe the object to detect (e.g. a face , mostly 
front-facing, possible occlusion, mul­tiple instances, as shown in Figure 1d) and the means to describe 
the image (detailed, varied illumination). From this description we can infer which face-detection algorithm 
will work most effectively under these conditions. Behind the scenes, each algorithm which is incorporated 
into our framework is evaluated with respect to how it performs under the conditions descriptions; this 
allows us to select the best one when the developer describes their problem. The power of the abstraction 
comes from the evaluation of the op­erations as a sequence instead of individually; this approach allows 
us to establish the higher-level problem to solve and possibly select a single method capable of solving 
it more effectively. *e-mail: {gregor,steveo,ssfels}@ece.ubc.ca  2 Example Problems For the .rst version 
of our shader language we support a small set of operations: segment, match, detect, select and solve. 
We use segment as a means for the user to describe the image: conceptually it produces a segmentation, 
which becomes the men­tal model for the user (every operation applies to segments); if this is the only 
operation issued it will produce an actual segmentation, such as the one shown in Figure 1a. The type 
of segmentation de­ pends on the description supplied. If this is not the only operation (and so the 
problem is likely not segmentation), the segment op­eration provides a description of the image, and 
so contributes to the conditions of the problem. For example, image registration involves .nding similar 
regions in two images and optimizing for the global alignment. We can express this in our language as: 
segment, match, solve. segment provides a description of the images, match accepts a set of variances 
as input to describe the differences between the im­ages (such as intensity, as in Figure 1b) and solve 
is constrained to be global. Conceptually this is .nding segments in each image, matching them across 
images and then optimizing for the align­ment. Internally the language is using a single method to accom­plish 
the entire task. Use of the select method allows us to describe more problems, such as image differencing. 
First we segment the images, then we .nd the correspondences between them using match, and .nally select 
the segments in the second image which have no matches in the .rst. A result of this is shown in Figure 
1c. Finally, using detect we can let the user provide a template to match against in the input images. 
The template could be a model, an example image or a simple description. In this case, we provide a method 
to describe faces, and the sequence is simply segment, detect. We are continuously expanding the problems 
we can solve using this small set of operations, and enhancing the .exibility of the de­scription each 
uses to simplify access to computer vision for de­velopers. We are also working on other operations to 
extend our work into multi-view and 3D, such as project, intersect and calibrate. We hope that the research 
into this methodol­ogy will provide simpler and more intuitive access to sophisticated computer vision 
methods for developers, hobbyists and researchers in other .elds. Copyright is held by the author / 
owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037762</section_id>
		<sort_key>470</sort_key>
		<section_seq_no>7</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Interaction]]></section_title>
		<section_page_from>7</section_page_from>
	<article_rec>
		<article_id>2037763</article_id>
		<sort_key>480</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>41</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D editing tools for stereo images]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037763</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037763</url>
		<abstract>
			<par><![CDATA[<p>With the advent of 3DTVs and consumer 3D cameras on cell phones, stereo 3D images and videos are set to become ubiquitous in the near future. However, at present, 2D image editing software applications do not provide for 3D editing and overlay features. In this paper, we discuss various automatic as well as user assisted 3D editing tools for stereo images. Our main aims are to:</p> <p>A. Add 3D viewing comfort (remove frame violations).</p> <p>B. Add aesthetic modifications and provide for extensions of 2D image tools to 3D images. This includes annotations, text overlay, depth labeling and 3D picture in picture mode. The following examples illustrate the above ideas A and B.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809330</person_id>
				<author_profile_id><![CDATA[81335496290]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Vikas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ramachandra]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Qualcomm Inc., San Diego, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[vikasr@qualcomm.com]]></email_address>
			</au>
			<au>
				<person_id>P2809331</person_id>
				<author_profile_id><![CDATA[81488650813]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Kalin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Atanassov]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Qualcomm Inc., San Diego, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kalina@qualcomm.com]]></email_address>
			</au>
			<au>
				<person_id>P2809332</person_id>
				<author_profile_id><![CDATA[81488654590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sergio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Goma]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Qualcomm Inc., San Diego, CA]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sgoma@qualcomm.com]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D editing tools for stereo images Vikas Ramachandra, Kalin Atanassov and Sergio Goma   Qualcomm 
Inc., San Diego, CA {vikasr,kalina,sgoma}@qualcomm.com Introduction With the advent of 3DTVs and consumer 
3D cameras on cell phones, stereo 3D images and videos are set to become ubiquitous in the near future. 
However, at present, 2D image editing software applications do not provide for 3D editing and overlay 
features. In this paper, we discuss various automatic as well as user assisted 3D editing tools for stereo 
images. Our main aims are to: A. Add 3D viewing comfort (remove frame violations). B. Add aesthetic 
modifications and provide for extensions of 2D image tools to 3D images. This includes annotations, text 
overlay, depth labeling and 3D picture in picture mode. The following examples illustrate the above ideas 
A and B.  1. Example 1: Drapes or photo frame objects to cover 3D frame violations (A). It has been 
observed that in 3D images and videos, when there are objects at the boundary that pop out of the screen 
(and are only partially visible in the scene), this leads to visual discomfort to the viewers. The reason 
for this is that there is no analog for this in the real world, i.e. when looking at a real world 3D 
scene through a physical window. To avoid this, we propose adding drapes at the frame boundaries. This 
is achieved through the following steps: . Determine the range of the central portion of the scene: 
 o Rectify left and right images. o Identify keypoints in the scene.  (Unique vertical edges) o Determine 
the disparities at the keypoints with a reliability measure. o Build a scene disparity histogram and 
trim its tails to remove outliers. o Find the max, min and range of the histogram.  . Shift the left/right 
images to achieve convergence at the mid range of the central portion of the scene. . Locate keypoint 
regions at the boundaries which are popping out (crossed disparities): frame violations. . Build drapes 
for L/R images and introduce disparity for the drapes tapering from a fixed value to zero at the boundaries. 
 . Add drapes to cover the regions which are causing frame violations.  2. Example 2: 3D text annotation, 
labeling, tagging (semi automatic with user interaction) (B) For 3D images, when users want to annotate 
different regions (e.g. label faces), it is useful to add the tags at the same depth as the object. This 
is done by building the letters on a 2D plane and adding the plane near the user selected object such 
that it is at the same disparity as the selected region. For this, we use the mean of the disparities 
of the keypoints in the neighborhood (determined by the same procedure as in Example 1.) to be the disparity 
of the letter plane. Applications for this are: . Annotating, tracking regions/players in sports videos. 
 . Labeling faces on social networking site 3D-photos. . Tagging landmarks in tourist 3D-photos.  
3. Example 3: 3D picture-in-picture (B) . Identify a fronto-parallel 2D canvas plane in the 3D parent 
scene (either automatically or with user interaction) . Scale the 3D image to be inserted ( insert ) 
to the required disparity range (if D1 is the disparity of the 2D canvas plane and D2 is the maximum 
pop out disparity of the parent scene , scale the insert scene (and shift its left and right images) 
such that the insert scene has disparities only between D1 and D2). . Add insert scene onto the desired 
2D plane in the parent scene.         Fig. 1A Fig. 1B Fig 1A. Before processing. Fig. 1B.Drapes 
added to avoid frame violations.  Fig.2 Fig.3 Fig. 2. User selected objects annotated in depth (Ex. 
2) Fig. 3. PIP: Stones 3D added on the wall (Ex. 3) 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037764</article_id>
		<sort_key>490</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>42</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Comparison of front touch and back touch while using transparent double-sided touch display]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037764</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037764</url>
		<abstract>
			<par><![CDATA[<p>When using direct-touch displays, the users' fingers sometimes occlude the content. To avoid this problem, previous studies have proposed using the back side of a device [Hiraoka et. al., 2003] [Wigdor et al., 2007]. These approaches were shown to be useful for inputting from the backside of a device, for tasks such as selecting targets. Although these approaches have not been shown the usability of double sided touch displays, we explored how users touch both sides of a transparent display using the <i>LimpiDual Touch</i> (LDT) [Iwabuchi et al., 2008]. In our experiments, participants touched a target on the double-sided display. The location of the target was not predictable. We compared the reaction time (RT) and accuracy under the different input conditions (<i>front/back/dual</i>).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809333</person_id>
				<author_profile_id><![CDATA[81488662696]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohtani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fritz@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809334</person_id>
				<author_profile_id><![CDATA[81100411762]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Tomoko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hashida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hashida@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809335</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809336</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[naemura@nae-lab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hiraoka, S., Miyamoto, K., and Tomimatsu. I. 2003. Behind Touch, a Text input method for mobile phone by the back and tactile sense interface, Information Processing Society of Japan, <i>Interaction'03</i>, 131--138.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1294259</ref_obj_id>
				<ref_obj_pid>1294211</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wigdor, D., Forlines, C., Baudisch, P., Barnwell, J., and Shen, C. 2007. LucidTouch: A See-Through Mobile Device. In Proc. <i>UIST'07</i>, ACM, 269--278.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1400978</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Iwabuchi, M., Kakehi, Y., and Naemura, T. 2008. LimpiDual Touch: Interactive Limpid Display with Dual-Sided Touch Sensing. In <i>SIGGRAPH'08</i>, ACM, posters.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Comparison of front touch and back touch while using transparent double-sided touch display Tomoko 
Ohtani Tomoko Hashida Yasuaki Kakehi Takeshi Naemura The University of Tokyo The University of Tokyo 
Keio University The University of Tokyo fritz@nae-lab.org hashida@nae-lab.org ykakehi@sfc.keio.ac.jp 
naemura@nae-lab.org  Figure 1. User touches the back side under dual input condition. 1. Introduction 
When using direct-touch displays, the users fingers sometimes occlude the content. To avoid this problem, 
previous studies have proposed using the back side of a device [Hiraoka et. al., 2003] [Wigdor et al., 
2007]. These approaches were shown to be useful for inputting from the backside of a device, for tasks 
such as selecting targets. Although these approaches have not been shown the usability of double sided 
touch displays, we explored how users touch both sides of a transparent display using the LimpiDual Touch 
(LDT) [Iwabuchi et al., 2008]. In our experiments, participants touched a target on the double-sided 
display. The location of the target was not predictable. We compared the reaction time (RT) and accuracy 
under the different input conditions (front / back / dual). 2. Experiment Sixteen peoples participated 
in the experiment. All participants had normal or corrected-to-normal vision and were right-handed. Testing 
occurred in a quiet room. They were required to touch the target as quickly and accurately as possible 
from the input side indicated. They used their thumbs on the front of the device, and the other fingers 
on the back. To allow for input with five fingers, it was grasped by the right hand (see Figure 1). The 
visual stimuli appeared on an LDT screen positioned on a table with a tilt of 51degrees, 32cm from the 
participant s eyes. The participant fixated at the center of the screen. An input condition (front / 
back / dual) was presented. Under the dual condition, participants could select to touch the front or 
back. When the participant pushed the start button on the screen, a 5x2 grid square appeared. After a 
stimulus onset asynchrony of 500ms, a target stimulus (180x 180mm) appeared for 500ms. After the participant 
touched the target, feedback comments indicated whether he/she had selected the correct target. This 
experiment was to measure RT and accuracy. Following 30 practice trials, the participants took part in 
600 experimental trials. The trials presentation was balanced such that the target appeared an equal 
number of times for each condition. Trials conditions were randomly mixed.  3. Results Figure 2 depicts 
the mean RT and accuracy for each condition. An ANOVA on the mean RT revealed a significant main effect 
of input-condition, F (2, 30) = 24.83, p < 0.01. Bonferroni post hoc analysis confirmed that RT for the 
front condition were faster than those for the back condition (p < 0.01) and the dual condition (p < 
0.01). RT for the dual condition was faster than those for the back condition (p < 0.05). For the accuracy 
rate, there was a significant main effect for each condition, F (2, 30) = 5.89, p < 0.01. Bonferroni 
post hoc analysis confirmed that the dual condition was the most accurate compared to the other conditions 
(p < 0.05). In addition, the mean of the usage rate for touching the back of display was 0.38 (SD = 0.27) 
under the dual condition.  Figure 2. Mean reaction times and accuracy for each input condition. These 
results clearly suggested that when users touch both sides, this is the most accurate way though it is 
slower than from the front. The reason is that the user could see the location and motion of his/her 
finger on the back of the screen. In other words, even if users are accustomed to using the front, they 
tend to use a combination of both sides. References HIRAOKA, S., MIYAMOTO, K., AND TOMIMATSU.I. 2003. 
BEHIND TOUCH, A TEXT INPUT METHOD FOR MOBILE PHONE BY THE BACK AND TACTILE SENSE INTERFACE, INFORMATION 
PROCESSING SOCIETY OF JAPAN, INTERACTION 03, 131-138. WIGDOR, D., FORLINES,C., BAUDISCH,P., BARNWELL,J., 
AND SHEN,C. 2007. LUCIDTOUCH: A SEE-THROUGH MOBILE DEVICE. IN PROC.UIST 07, ACM, 269-278. IWABUCHI,M., 
KAKEHI,Y., AND NAEMURA,T. 2008. LIMPIDUAL TOUCH: INTERACTIVE LIMPID DISPLAY WITH DUAL-SIDED TOUCH SENSING. 
IN SIGGRAPH 08, ACM, POSTERS. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037765</article_id>
		<sort_key>500</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>43</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[E-letter]]></title>
		<subtitle><![CDATA[a paper-based instant messaging system using selective wireless power transfer]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037765</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037765</url>
		<abstract>
			<par><![CDATA[<p>Today as the rapid development of digital technology is turning our society into a paperless one, the traditional letter-writing has been replaced by digital media, such as EMail and Instant Messaging. However, research has shown that there are still rich advantages of physical paper in our daily life, and they are hard to attain in the realm of pure software [Hendrix and Eisenberg 2004]. For paper-based letter communication, Gregory Davis [Davis 2009] mentioned that unlike electronic communication, the letter engages the totality of intellect, the heart, and the senses.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809337</person_id>
				<author_profile_id><![CDATA[81455605722]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kening]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zhu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Univeristy of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809338</person_id>
				<author_profile_id><![CDATA[81100485839]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nii]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Univeristy of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809339</person_id>
				<author_profile_id><![CDATA[81100444317]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Owen]]></first_name>
				<middle_name><![CDATA[Noel Newton]]></middle_name>
				<last_name><![CDATA[Fernando]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Univeristy of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809340</person_id>
				<author_profile_id><![CDATA[81100418633]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Adrian]]></first_name>
				<middle_name><![CDATA[David]]></middle_name>
				<last_name><![CDATA[Cheok]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Univeristy of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Davis, G. J. 2009. The joys of a handwritten letter. <i>Journal of American Amateur Press Association</i>, 78, 46--61.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1149230</ref_obj_id>
				<ref_obj_pid>1149126</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hendrix, S., and Eisenberg, M. 2004. Computer-assisted engineering for children: a pop-up design application. In <i>ICLS '04: Proceedings of the 6th international conference on Learning sciences</i>, International Society of the Learning Sciences, 606--606.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>159630</ref_obj_id>
				<ref_obj_pid>159544</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wellner, P. 1993. Interacting with paper on the digitaldesk. <i>Commun. ACM 36</i> (July), 87--96.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 E-Letter: A Paper-based Instant Messaging System Using Selective Wireless Power Transfer Kening Zhu, 
Hideaki Nii, Owen Noel Newton Fernando, Adrian David Cheok Keio-NUS CUTE Center, National Univeristy 
of Singapore  Figure 1: E-Letter 1 Introduction Today as the rapid development of digital technology 
is turning our society into a paperless one, the traditional letter-writing has been replaced by digital 
media, such as EMail and Instant Mes­saging. However, research has shown that there are still rich ad­vantages 
of physical paper in our daily life, and they are hard to attain in the realm of pure software [Hendrix 
and Eisenberg2004]. For paper-based letter communication, Gregory Davis [Davis 2009] mentioned that unlike 
electronic communication, the letter engages the totality of intellect, the heart, and the senses. Considering 
the advantage of paper letter communication, in this paper we present E-Letter,a paper-based instant 
messaging system. By wirelessly powering the nichrome wires embedded in the paper to heat up and etch 
the paper, this system enables users to chat with each other using paper. The message will display physically 
on the paper similar to communicating through letters. Since the birth of the concept of paper computing, 
there have been a few researchers focusing on displaying digital information on paper. The .rst system 
closing thegap between paper and digi­tal information spaceswasWellner s DigitalDesk[Wellner 1993], which 
ultilized a camera-based tracking system and a projector­based tabletop to interact with paper document 
and link to the dig­ital service. However, most of the existing works utilized projector to display message 
on the paper, which is not really paper display­ing. The information will be removed when the paper is 
not under the projection. In addition, it is inconvenient for end users to set up an accurate projection 
system. Comparing to these existing works, E-Letter can display the digital message permanently on the 
paper using electromagnetic power. This provides the missing value of traditional paper letter to the 
digital messages. 2 System Overview The main technical innovation of this system is that the normal 
pa­per is powered wirelessly through electromagnetic .elds with con­trolled frequencies to display different 
messages. Nichrome wires are embedded in the paper and heated up wirelessly to generate dif­ferent display 
patterns. The system can selectively power up differ­ent receiving coils embedded in the paper by controlling 
the trans­mitting frequencyto activate different embedded receivers. Unlike massiveexternalpower connections,this 
methodkeepsthe intrinsic .exibility of paper interface, enabling users to manipulate the paper freely. 
AsshowninFigure1(a),the transmitterwasbuiltwithahighpower output MOSFET oscillator setup. The LC tank 
in the system gen­erates the oscillation, and two power MOSFETs amplies the signal to enable the system 
to transfer energy wirelessly. An array of the capacitors are respectively controlled by a relay and 
a switch so different capacitancevalue canbe includedat runtimeto generatea variety of frequencies, ranging 
from 200 kHz to 1MHz. For the receiving coils embedded in the paper in Figure 1(b), we used LC tank to 
harvest energy at its resonance frequency. The in­ductor coil is made of 0.5mm enameled copper wire in 
the circular shape with diameter 5cm and2turns to match the small resistance of the nichrome wire. The 
capacitor and the nichrome wire are attached to the copper coil. Each coil will have a different value 
of capacitor attached, thus each will have different resonant fre­quency. Therefore when there are multiple 
receiving coils with dif­ferent resonant frequencies, we can control the output frequencyin the primary 
transmitting coil to activate different receiving coils. The receivers are embedded in the paper during 
the paper-making process. Finally, we combined this paper powering system with instant mes­saging system. 
By simply matching the ASCII code of each letter to a 7-digit display, the system heats up the nichorme 
wires one by one, and displays the incoming message as shown in Figure 1(c). 3 Demonstration Scenario 
During the demonstration, audiences can experience the paper­based instant chatting. They can see the 
messages display on the paper, reply the messages, hold and read the paper freely, and read and write 
the letters. The audiences can also take their paper mes­sages back home as souvenir. References DAVIS, 
G. J. 2009. The joys of a handwritten letter. Journal of American Amateur Press Association, 78, 46 61. 
HENDRIX,S., AND EISENBERG,M. 2004. Computer-assisted en­gineering for children: apop-up design application. 
In ICLS 04: Proceedings of the 6th international conference on Learning sci­ences, International Society 
of the Learning Sciences, 606 606. WELLNER, P. 1993. Interacting with paper on the digitaldesk. Commun. 
ACM 36 (July), 87 96. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037766</article_id>
		<sort_key>510</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>44</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[HAPMAP]]></title>
		<subtitle><![CDATA[haptic walking navigation system with support by the sense of handrail]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037766</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037766</url>
		<abstract>
			<par><![CDATA[<p>In this study, we propose a method of displaying unaware-usage haptic sensation of navigation. People view a map when they visit an unfamiliar place, but when they are taken away their eyes from the map, their attention is diverted and margin of the heart. However, when we are relaxed and do not worry about getting lost, we can discover the intrinsic beauty of the unfamiliar land. Therefore, we have focused on the sense of touch and ensured security by using a support such as a wall or a handrail in the streets, along with the sense of touch. Therefore, we propose a haptic navigation system to release human eyes from the requirement of constantly looking into a map in order to enhance the experience of our daily walk or sightseeing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809341</person_id>
				<author_profile_id><![CDATA[81488642068]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Imamura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yuki.imr@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2809342</person_id>
				<author_profile_id><![CDATA[81488647459]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hironori]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arakawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University Yokohama Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[lamza@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809343</person_id>
				<author_profile_id><![CDATA[81421597185]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Sho]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kamuro]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kamuro@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809344</person_id>
				<author_profile_id><![CDATA[81331499667]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Kouta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Minamizawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kouta@tachilab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809345</person_id>
				<author_profile_id><![CDATA[81365592882]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Susumu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tachi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University Yokohama, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tachi@tachilab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187315</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Taro Maeda, Hideyuki Ando, Tomohiro Amemiya, Masahiko Inami, Naohisa Nagaya, Maki Sugimoto, "Shaking The World: Galvanic Vestibular Stimulation As A Novel Sensation Interface", ACM SIGGRAPH 2005 Emerging Technologies, 2005.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1402239</ref_obj_id>
				<ref_obj_pid>1402236</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[T. Amemiya, H. Ando and T. Maeda: "Lead-Me Interface" for a Pulling Sensation from Hand-held Devices, ACM Trans. On Applied Perception, Vol.5, No.4 (2008)]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037767</article_id>
		<sort_key>520</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>45</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[iPvlc]]></title>
		<subtitle><![CDATA[pixel-level visible light communication for smart mobile devices]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037767</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037767</url>
		<abstract>
			<par><![CDATA[<p>The collaboration between a public large display and personal Smart Mobile Devices (SMDs) is a promising technical field for the visual communication and interaction. This paper focuses on the combination of an image projector as the large display, and smartphones and tablet PCs as the SMDs. The pioneering work in this field used Personal Data Assistants (PDAs) and electromagnetic sensors for detecting their positions roughly [Rekimoto at el.2000]. It is clear that the latest SMDs are much more powerful than the PDAs at that time. However, technical challenges still remain in the precise detection of the relative positions and inclinations of SMDs placed on a large display. For this purpose, this paper introduces the Pixel-level Visible Light Communication (PVLC) projector [Kimura et al. 2007] as the large display and proposes a simple method for extending the communication function of iPod touch and iPad as the SMDs. We call the system iPvlc.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809346</person_id>
				<author_profile_id><![CDATA[81488660116]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoshikuni]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kato]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[vlc@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809347</person_id>
				<author_profile_id><![CDATA[81488669621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Naofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukasawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809348</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kimura, S., Kitamura, M., and Naemura, T. EmiTable: A Tabletop Surface Pervaded with Imperceptible Metadata. In Proceedings of Tabletop. 2007, 189--192.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>633368</ref_obj_id>
				<ref_obj_pid>633292</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ayatsuka, Y., Matsushita, N., and Rekimoto, J. 2000. HyperPalette: a Hybrid Computing Environment for Small Computing Devices. In <i>CHI 2000 Extended Abstracts</i>, 133--134.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 iPvlc: Pixel-level Visible Light Communication for Smart Mobile Devices Yoshikuni Kato* Naofumi Fukasawa* 
Takeshi Naemura* The University of Tokyo The University of Tokyo The University of Tokyo  Figure 1. 
Superimposed map. Figure 2. System configuration. Figure 3. Hardware construction. 1. Introduction The 
collaboration between a public large display and personal Smart Mobile Devices (SMDs) is a promising 
technical field for the visual communication and interaction. This paper focuses on the combination of 
an image projector as the large display, and smartphones and tablet PCs as the SMDs. The pioneering work 
in this field used Personal Data Assistants (PDAs) and electromagnetic sensors for detecting their positions 
roughly [Rekimoto at el.2000]. It is clear that the latest SMDs are much more powerful than the PDAs 
at that time. However, technical challenges still remain in the precise detection of the relative positions 
and inclinations of SMDs placed on a large display. For this purpose, this paper introduces the Pixel-level 
Visible Light Communication (PVLC) projector [Kimura et al. 2007] as the large display and proposes a 
simple method for extending the communication function of iPod touch and iPad as the SMDs. We call the 
system iPvlc. 2. iPvlc system First of all, Figure 1 shows the implemented system. Colorful aerial photographs 
appear in SMDs placed on a blank map image. We can see that the photographs and the map image are precisely 
aligned. The SMD can interactively superimpose the detailed information on the projected large image. 
This is achieved by the PVLC projector which can project a visible image and invisible pixel-by-pixel 
signals. It consists of a Digital Micromirror Device (DMD) and can generate high-speed flickers. People 
perceive just the integrated brightness as a visual image (a blank map). Each pixel of the projected 
image contains the latitude and longitude data embedded as high-speed flickers. Figure 2 illustrates 
the system configuration. The embedded data from the PVLC projector is received by photo detectors and 
passed to a microphone terminal of an SMD through a *e-mail: vlc@nae-lab.org microcontroller. While the 
PVLC projector works at 12,500 [bps], the baud rate of the microphone terminal is limited to 1,200 [bps] 
based on the standard of software modem. So, the microcontroller is designed to extract the embedded 
digital data at 12,500[bps] and convert a part of it to an analog microphone signal by the frequency-shift 
keying (FSK) modulation. In order to stabilize the transmission, preamble is also added. Figure 3 shows 
the actual set up of the system. Two photo detectors are placed apart on the backside. Each photo detector 
simply receives the latitude and longitude data. As for the distance between the two photo detectors, 
larger is better, since this distance affects the calculation of the inclination of the SMD. This approach 
has the following three major merits: The pixel-level accuracy can be achieved for the detection of 
relative positions and inclinations of SMDs placed on the projected image.  Just a small plug-in device 
is attached to an SMD to receive the embedded latitude and longitude data from the projected image. The 
system doesn t need any other calibration devices.  It is suitable for multiple user interaction. Even 
if many people put their own SMDs on the projected image at once, it works well.  From the experimental 
results shown in Figure 1, we can see that the potential applicability of the system to realize the above 
mentioned merits. References KIMURA, S., KITAMURA, M., AND NAEMURA, T. EmiTable: A Tabletop Surface 
Pervaded with Imperceptible Metadata. In Proceedings of Tabletop. 2007, 189-192. AYATSUKA, Y., MATSUSHITA, 
N., AND REKIMOTO, J. 2000. HyperPalette: a Hybrid Computing Environment for Small Computing Devices. 
In CHI 2000 Extended Abstracts, 133-134. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037768</article_id>
		<sort_key>530</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>46</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Media cushion]]></title>
		<subtitle><![CDATA[soft interface to control living environment using human natural behavior]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037768</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037768</url>
		<abstract>
			<par><![CDATA[<p>The ubiquitous era [Weiser 1995] has begun to unfold. Through sensors and actuators connected with the physical world, the computers support our daily life by sensing our behavior. However, most interfaces around us still remain in hard touch points and the old direct manipulations such as switches and buttons. This makes it difficult to give fully appropriate and flexible services to the user according to his/her situation. We propose Media Cushion as a new soft interface for the ubiquitous era, which enables us to control the living environment more naturally and adequately by taking in human body movements.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809349</person_id>
				<author_profile_id><![CDATA[81448595038]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Izumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[izumi@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809350</person_id>
				<author_profile_id><![CDATA[81488650911]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shigeru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mayfair@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809351</person_id>
				<author_profile_id><![CDATA[81488667535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kashiwagi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kashiwagi@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809352</person_id>
				<author_profile_id><![CDATA[81320495613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Daisuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uriu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[uriu@kmd.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809353</person_id>
				<author_profile_id><![CDATA[81319498464]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Naohito]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okude]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[okude@kmd.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mark Weiser, 1995, The computer for the 21&#60;sup&#62;st&#60;/sup&#62; century, In <i>Scientific American</i>, vol. 272, No. 3, pp. 78--89, ACM, 95--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 IMG_0582.JPG Media Cushion: Soft interface to control living environment using human natural behavior 
IMG_0582.JPG IMG_0582.JPG Izumi Yagi Shigeru Kobayashi Ryo Kashiwagi Daisuke Uriu Naohito Okude Graduate 
School of Media Design, Keio University {izumi, mayfair, kashiwagi, uriu, okude}@kmd.keio.ac.jp  1. 
Introduction The ubiquitous era [Weiser 1995] has begun to unfold. Through sensors and actuators connected 
with the physical world, the computers support our daily life by sensing our behavior. However, most 
interfaces around us still remain in hard touch points and the old direct manipulations such as switches 
and buttons. This makes it difficult to give fully appropriate and flexible services to the user according 
to his/her situation. We propose Media Cushion as a new soft interface for the ubiquitous era, which 
enables us to control the living environment more naturally and adequately by taking in human body movements. 
 2. Media Cushion Media Cushion controls the living environment according to the user s messages. By 
combining the natural physical movements of the human body when using a cushion with the context information 
(such as who the user is and when it is used), it can estimate the user s mood and needs, and produce 
an appropriate environment. For example, when you lie down on the sofa in a weekend evening, using Media 
Cushion as a pillow, the light turns dark and the music stops, so you can take a nap comfortably (Fig1). 
In this way, Media Cushion can create atmospheres by controlling the environment such as lights and sounds 
according to the user s mood.       Figure 1. Examples of usage of Media Cushion 3. Design Process 
and System Design Process: We often come in contact with fabric products like sofa and cushion in our 
daily life. Thus, we think they are appropriate items to be used to control living environment, and we 
chose cushion to start with. We conducted video observations at 7 houses and analyzed how people use 
the cushion at the sofa space in the living room. As a result, we found that there are several patterns 
in the usage of the cushion related to the human body movements, such as Hug (hugging the cushion), Head 
(pillowing), Back (putting at the back), Desk (using as a desk), and Elbow (resting one s elbow) (Fig2). 
We also found that the patterns of the usage of the cushion and the user s activity at the sofa are closely 
related. For example, when sleeping at the sofa, the user uses the cushion as a pillow, or when reading 
a book at the sofa with using the cushion as a desk.   Figure 2. Defined natural behavior of usage 
of the cushion (unconscious control) and command (conscious control).     Figure 3. The hardware 
structure of Media Cushion. System: Media Cushion has 3 types of sensors to estimate the cushion s usage: 
an acceleration sensor to detect the direction of the cushion, capacitive touch sensors to detect the 
way to hold, and a photo reflector as a proximity sensor to detect the level of the pressure on the cushion 
(Fig3). By combining these sensor data, the system can calculate the cushion s situation. To develop 
the calculating system, we created subjective probabilistic models for Bayesian network and implemented 
with the Netica API for Java. We introduced traditional linguistic theories (Jacobson s six elements 
and Martine s double articulation ) to the system. In addition to the natural usage of the cushion, Media 
Cushion also has several commands (conscious controls) such as double tap to cancel the environment (Fig2). 
Media Cushion learns a user s behaviors and preferences, and hence provides more appropriate environment 
for the user. 4. Conclusion Media Cushion uses a cushion, an ordinary item in our daily life, as a 
new touch point for responsive environments by introducing the natural behaviors that we already have 
as habits. We d like to expand to other items such as sofas and rugs, and make the ubiquitous home life 
more enjoyable and comfortable. Acknowledgements This research is carried out under CUTE Project No. 
WBS R-7050000-100-279 partially funded by a grant from the National Research Foundation (NRF) administered 
by the Media Development Authority (MDA) of Singapore. Additional Authors Malek Anouti, Ruri Araki, 
Mizuki Ohno, Takao Tanigaki, Jun Yabuki and Naoya Takei. References MARK WEISER, 1995, The computer 
for the 21st century, In Scientific American, vol. 272, No. 3, pp. 78-89, ACM, 95 104. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037769</article_id>
		<sort_key>540</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>47</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Meta-ryoshka]]></title>
		<subtitle><![CDATA[haptic illusion on perceiving shape]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037769</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037769</url>
		<abstract>
			<par><![CDATA[<p>Meta-ryoshka is a system which enables us to experience a haptic illusion as if we were touching variety of shapes even though we are touching onlu a simple cylinder in faict (Fig.1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809354</person_id>
				<author_profile_id><![CDATA[81488658104]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ban]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ban@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809355</person_id>
				<author_profile_id><![CDATA[81466640651]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Takashi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kajinami]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kaji@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809356</person_id>
				<author_profile_id><![CDATA[81331500452]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takuji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Narumi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[narumi@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809357</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tani@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809358</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1502413</ref_obj_id>
				<ref_obj_pid>1502409</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Anatole Lecuyer: "Simulating Haptic Feedback Using Vision": a Survy of Research and Application of Pseudo-Haptic Feedback", Teleoparators and Virtual Environments, MIT Press, Vol 18, Issue 1, pp. 39--53, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Meta-ryoshka: Haptic Illusion on Perceiving Shape Yuki Ban Takashi Kajinami , Takuji Narumi , Tomohiro 
Tanikawa , Michitaka Hirose School of Engineering, the University of Tokyo Guraduate School of Information 
Science and Technology, the University of Tokyo Graduate School of Engineering, the University of Tokyo 
Graduate School of Information Science and Technology, the University of Tokyo Graduate School of Information 
Science and Technology, the University of Tokyo    Fig.1 Concept: Haptic illusion on perceived shape 
 Fig.2 Visual-haptic system for illusion  Fig. 3 Optical system for video see-through 1. Introduction1 
  email: { ban, kaji, narumi,tani, hirose}@cyber.t.u-tokyo.ac.jp Meta-ryoshka is a system which enables 
us to experience a haptic illusion as if we were touching variety of shapes even though we are touching 
onlu a simple cylinder in faict (Fig.1). Recent psychological researches have revealed an illusionary 
phenomena called Pseudo-Haptics , which means a cross modality between our vision and haptic sense [1]. 
This illusion is evoked when we are stimulated by an incoherent set of real-time visual and haptic stimuli, 
and we feel an illusionary haptic sense. For example, when we are working on computer, a slowdown of 
a cursor evokes a virtual frictional force on our hand graving mouse. This phenomena has a potential 
for making up haptic sense without using physical devices, only with using visual feedback. In our system, 
we use it to "change" the shape of object, and construct a simple system which can display many complicated 
shapes, for example matoryoshka dolls, vases, glasses and so on. Fig. 4 Distorted visual feedback for 
evoking haptic illusion 2. System Configurations  In this system, a user traces on a physical object 
placed behind the visual display and see it through the display (Fig.2). We place cameras at the point 
corresponding to user's eyes optically using mirror (Fig.3), and capture the image around the object 
for visual feedback. We use two cameras for each eyes and realize stereoscopic display, and help users 
with their recognition of the shape and positional awareness between their hand and the object. From 
captured images we recognize the area of her/his hand, and calculate the position where s/he touches 
the object. We also recognize the physical object and calculate the distortion by comparing physical 
shape and virtual shape we want to make up (Fig.4) and based on this distortion we put user's hand at 
the corresponding position. By this process we make up the visual feedback as if he were touching a virtual 
object whose shape is different from physical one, and this difference evokes Pseudo-Haptic illusion 
as if he were really touching a virtual one. Using this system, users perceive visually changed shape 
although we don t change the physical shape itself. So we can display lots of shapes with a few kinds 
of shape of physical haptic devices. For example, when user trace on a simple cylindrical form, we can 
display a variety of different bottles, vases, dolls like Matryoshka, and so on. Currently more than 
a dozen people tried this system and almost 85% of them answered that they perceived the virtual object 
s shape deferent from the shape they traced in fact. With this system, we can display various shapes 
of virtual objects dramatically-easily. References [1]Anatole Lecuyer: Simulating Haptic Feedback Using 
Vision : a Survy of Research and Application of Pseudo-Haptic Feedback ",Teleoparators and Virtual Environ-ments,MIT 
Press,Vol18,Issue1,pp.39-53,2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037770</article_id>
		<sort_key>550</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>48</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Metamorphic light]]></title>
		<subtitle><![CDATA[a tabletop tangible interface using deformation of plain paper]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037770</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037770</url>
		<abstract>
			<par><![CDATA[<p>In the field of Spatial Augmented Reality, how to intuitively manipulate the projected image becomes an important issue. If we could use flexible physical objects as an interface and measure the deformation as well as the location and connection of the physical object, we could offer more intuitive operation through tangible interaction. As related works, ForceTile [Kakehi et al. 2008] developed by our research group, can detect force distribution on the surface of panels made of gel material. Khronos Projector [Cassinelli and Ishikawa 2005] and Illuminating Clay [Piper et al. 2002] are also typical interactive display systems using the deformation of a screen made of cloth and clay material. On the other hand, we focus on plain paper, a material commonly used in our daily life which has high flexibility and stiffness. With this, we propose a novel tangible interface using no fiducial markers named Metamorphic Light. By measuring the deformation of the paper placed (or held) on the tabletop in three dimension, the image will dynamically change and be projected directly onto the paper (see Figure 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809359</person_id>
				<author_profile_id><![CDATA[81488651214]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yukiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Makino]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[makino@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809360</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187308</ref_obj_id>
				<ref_obj_pid>1187297</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cassinelli, A., and Ishikawa, M. 2005. Khronos projector. In <i>SIGGRAPH '05 Emerging technologies</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1401632</ref_obj_id>
				<ref_obj_pid>1401615</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kakehi, Y., Jo, K., Sato, K., Minamizawa, K., Nii, H., Kawakami, N., Naemura, T., and Tachi, S. 2008. Forcetile: tabletop tangible interface with vision-based force distribution sensing. In <i>SIGGRAPH 2008 new tech demos</i>, ACM.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>503439</ref_obj_id>
				<ref_obj_pid>503376</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Piper, B., Ratti, C., and Ishii, H. 2002. Illuminating clay: a 3-d tangible interface for landscape analysis. In <i>Proceedings of the Conference on Human Factors in. Computing Systems</i>, ACM, 20--25.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Metamorphic Light: A Tabletop Tangible Interface Using Deformation of Plain Paper Yukiko Makino* Yasuaki 
Kakehi* Keio University Keio University Figure 1: Metamorphic Light Figure 2: System Design  1 Introduction 
 In the .eld of Spatial Augmented Reality, how to intuitively ma­nipulate the projected image becomes 
an important issue. If we could use .exible physical objects as an interface and measure the deformation 
as well as the location and connection of the physi­cal object, we could offer more intuitive operation 
through tangible interaction.As relatedworks,ForceTile[Kakehietal.2008]devel­opedbyour research group, 
can detect force distribution on the sur­face of panels made of gel material. Khronos Projector [Cassinelli 
and Ishikawa 2005] and Illuminating Clay [Piper et al. 2002] are also typical interactive display systems 
using the deformation of a screen made of cloth and clay material. On the other hand, we focus on plain 
paper, a material commonly used in our daily life which has high .exibility and stiffness. With this, 
we propose a novel tangible interface using no .ducial markers named Metamor­phic Light. By measuring 
the deformation of the paper placed (or held) on the tabletop in three dimension, the image will dynami­cally 
change and be projected directly onto the paper (see Figure 1). 2 Metamorphic Light In our system, 
we offer technical innovations as follows: First, we adopted projector and camera based approach for 
detecting the de­formation of a completely white piece of paper, which is not con­nectedtoanyelectricdeviceorprintedmarkers. 
Figure2showsthe basic system structure. We utilized an active stereo vision method for three-dimensional 
shape measurement. By including several .ducial lines within the projection image, the system can detect 
the shape variation and position of the tabletop paper in real-time. Second, this system can directly 
project images onto the paper in the appropriate position. Since the projector and the camera are installed 
inside the table, it does not create occlusions when users put their hands on the paper. In addition, 
note that users can access and operate information intuitively and naturally since the system does not 
require processing the paper or the user. Third, by classifying the measured deformation, manipulations 
of *e-mail: {makino, ykakehi}@sfc.keio.ac.jp pushing, blowing, picking up, twisting, and turning over, 
become possible. We have developed software architecture for detecting and classifying users actions 
and re.ecting projection images. 3 Applications and Future Works MetamorphicLightisasystemthatprovidesseveral 
interactiveap­plications. In the .rst application, we have developed an image­operation interface, by 
utilizing the .exibility and stiffness of pa­per. Users can control the projected images (e.g. playing 
speed and direction of video, viewing angle of multi-view images) by simply pushing or blowing a half 
cylindrical shaped paper like manip­ulating a joystick. Second, using physical paper, this system can 
become a digital book reader. Users can turn pages of the digital books projected onto the paper by simply 
turning over the paper (see Figure 3). Theycan continue .ipping pages of the digital book, or stop at 
a certain page like an actual book. The third application is forgaming, using a pinching action. User 
may pick up and move character images that are projected by pinching them with the small handheld paper. 
These applications provide users a sense of operating images directly. In the near future, we plan to 
combine the projection method of the ForceTile, providing multiple image projection onto the tabletop 
surface as well as the paper placed on the tabletop. Furthermore, we plan to develop various interactive 
tabletop applications. References CASSINELLI, A., AND ISHIKAWA, M. 2005. Khronos projector. In SIGGRAPH 
05 Emerging technologies,ACM. KAKEHI, Y., JO, K., SATO, K., MINAMIZAWA, K., NII, H., KAWAKAMI, N., NAEMURA, 
T., AND TACHI, S. 2008. Forcetile: tabletop tangible interface with vision-based force dis­tribution 
sensing. In SIGGRAPH 2008 new tech demos,ACM. PIPER,B.,RATTI,C., AND ISHII,H. 2002. Illuminating clay:a3­dtangible 
interface for landscape analysis. InProceedings of the Conference on Human Factors in. Computing Systems, 
ACM, 20 25. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037771</article_id>
		<sort_key>560</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>49</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[onNote]]></title>
		<subtitle><![CDATA[a musical interface using markerless physical scores]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037771</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037771</url>
		<abstract>
			<par><![CDATA[<p>Intuitive music playing using various digital musical instruments with specific tangible interfaces has become one of the ways to enjoy the music experience [Jorda et al. 2007]. When we start studying music, we need to learn how to read musical scores to understand the different instrumental parts and also understand the melody, rhythm, fingering and so on. However, reading scores is often a difficult task for beginners and could create a barrier for playing music. To solve this problem, we purpose a novel musical interface system named "onNote". In this system, physical markerless musical scores are used as instruments to play music intuitively (see Figure 1). The notes on the score are captured by a camera and are processed by the system which retrieves the music from a score database. In addition, the system can do a real-time recognition of the paper's position and the rotation. Thus, by physically moving and connecting the musical scores we can play music intuitively.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809361</person_id>
				<author_profile_id><![CDATA[81458646844]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yusuke]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yamamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[usuk@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809362</person_id>
				<author_profile_id><![CDATA[81413599383]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hideaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Uchiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA Rennes]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[uchiyama@hvrl.ics.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809363</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1226998</ref_obj_id>
				<ref_obj_pid>1226969</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Jorda, S., Geiger, G., Alonso, M., and Kaltenbrunner, M. 2007. reactable: Exploring the synergy between live music performance and tabletop interface. In <i>Proceedings of TEI '07</i>, ACM Press, 139--146.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2180233</ref_obj_id>
				<ref_obj_pid>2180175</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nakai, T., Kise, K., and Iwamura, M. 2006. Use of affine invariants in locally likely arrangement hashing for camera-based document image retrieval. In <i>Lecture Notes in Computer Science (7th International Workshop DAS2006)</i>, 3872, 541--552.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 onNote: A Musical Interface Using Markerless Physical Scores YusukeYamamoto * Hideaki Uchiyama Yasuaki 
Kakehi * Keio University INRIA Rennes Keio University  Figure 3: Playing Music with a Physical Figure 
1: onNote Figure 2: Image Processing for onNote Score 1 Introduction Intuitive music playing using various 
digital musical instruments with speci.c tangible interfaces has become one of the ways to enjoy the 
music experience [Jorda et al. 2007]. When we start studying music, we need to learn how to read musical 
scores to understand the different instrumental parts and also understand the melody, rhythm, .ngering 
and so on. However, reading scores is often a dif.cult task for beginners and could create a barrier 
for playing music. To solve this problem, we purpose a novel musical interface system named onNote . 
In this system, physical marker­less musical scores are used as instruments to play music intuitively 
(see Figure 1) . The notes on the score are captured by a camera and are processed by the system which 
retrieves the music from a score database. In addition, the system can do a real-time recognition of 
the paper s position and the rotation. Thus, by physically moving and connecting the musical scores we 
can play music intuitively. 2 onNote To develop our music instrument, we propose two technical inno­vations. 
First, we propose an image processing approach for track­ing and retrieving the markerless physical musical 
scores placed in front of the camera. Since users move the score freely, it is not always possible for 
the camera to cover the whole area of the pa­per. Thus, in this research, we adopted a Locally Likely 
Arrange­ment Hashing (LLAH) method [Nakai et al. 2006] for retrieving the binary document images. This 
method relies on the geometri­cal relationship of keypoints as a descriptor rather than using local texture. 
And also, we developed another method for extracting the stablekeypoints on the score and combinedit 
with LLAH forkey­point matching, then the system identi.es the score by referencing the score database. 
In addition, by using these methods, the system recognizes the position and rotation of the score in 
realtime. Second, we have developed software architecture for classifying the user s actions and generating 
the sounds. According to the various input information provided by the user such as position, rotation 
and page numbers, the system accesses the MIDI database and con­trols the sound in realtime. * e-mail: 
{usuk, ykakehi}@sfc.keio.ac.jp e-mail: uchiyama@hvrl.ics.keio.ac.jp One of the main features of this 
system is its strong robustness to­ward occlusions on the score because of the adoptedkeypoint based 
approach. Thus, the system can work effectively even with the us­age of an overhead projection that shows 
additional information on the score. And it can work effectively even if the users put their hands and 
.ngers on the score. Therefore, these features and meth­ods enable the system to intuitively and interactively 
play music. 3 Applications and Future Works OnNote is a system that provides several interactive applications. 
First, it can play music according to the movement of the score. By controlling the speed and direction 
of the score, users can manipu­late music just as a DJ would. The system projects a red pointer on the 
music notes and generates the corresponding sound in realtime. In addition, the system can use the user 
s .ngers or line of sight to play the music instead of using the red pointer. Second, the system can 
be used as a music effector. The user could adjust and control the outputted music by changing the score 
s po­sition and rotation. By rotating or moving the score up and down, the user could adjust sound parameters 
such as volume, pitch and tone. Third, the system generates different music pieces when presented with 
different segments. Users can enjoy music mashups easily by connecting and changing the arrangement of 
the music segments. This application makes use of the system as an edutainment tool which users could 
play puzzle games to .nd and learn the correct sequence of music. In the future, we plan to develop furthermore 
applications related to different situations using onNote system. References JORDA, S., GEIGER, G., 
ALONSO, M., AND KALTENBRUNNER, M. 2007. reactable: Exploring the synergy between live music performance 
and tabletop interface. In Proceedings of TEI 07, ACM Press, 139 146. NAKAI, T., KISE, K., AND IWAMURA, 
M. 2006. Use of af.ne in­variants in locally likely arrangement hashing for camera-based document image 
retrieval. In Lecture Notes in Computer Science (7th International Workshop DAS2006), 3872, 541 552. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037772</article_id>
		<sort_key>570</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>50</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[PocoPoco]]></title>
		<subtitle><![CDATA[a tangible device that allows users to play dynamic tactile interaction]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037772</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037772</url>
		<abstract>
			<par><![CDATA[<p>Today a tremendous amount of audio and visual information is stuffed onto the flat displays of smart phones. It is no doubt a very convenient age, but a little bit uniform. Our research focuses on interfaces with dynamic movements and interfaces that can change their own shape dynamically [1][2]. We have developed a box-shaped device called <i>PocoPoco</i> which controls the movement of columnar units with built-in solenoid actuators and utilizes them to give users dynamic tactile sensations (see Figure 1). <i>PocoPoco</i> is an input/output device that can be used without visual information, because it can indicate all input/output information through tactile sensations. This device is a versatile interface that can be used in a wide range of applications including games, telecommunication, and musical performance. It was conceived as a "new kind of interface which can be used by both people with visual impairments and people with normal vision".</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809364</person_id>
				<author_profile_id><![CDATA[81490670247]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Takaharu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kanai-takaharu@sd.tmu.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809365</person_id>
				<author_profile_id><![CDATA[81488673349]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kikukawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809366</person_id>
				<author_profile_id><![CDATA[81442606589]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tatsuhiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suzuki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809367</person_id>
				<author_profile_id><![CDATA[81319488008]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Tetsuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Baba]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809368</person_id>
				<author_profile_id><![CDATA[81100274861]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Kumiko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kushiyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Metropolitan University, Hino, Tokyo, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1400970</ref_obj_id>
				<ref_obj_pid>1400885</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Baba, T., Ushiama, T., and Tomimatsu, K. 2008. Emerging keys: interactive electromagnetic levitation keys. In ACM SIGGRAPH 2008 Posters (Los Angeles, California, August 11--15, 2008). SIGGRAPH '08. ACM, New York, NY, 1-1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>986050</ref_obj_id>
				<ref_obj_pid>985921</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Michelitsch, G., Williams, J., Osen, M., Jimenez, B., and Rapp, S. 2004. Haptic chameleon: a new concept of shape-changing user interface controls with force feedback. In CHI '04 Extended Abstracts on Human Factors in Computing Systems (Vienna, Austria, April 24--29, 2004). CHI '04. ACM, New York, NY, 1305--1308.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037773</article_id>
		<sort_key>580</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>51</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Second skin]]></title>
		<subtitle><![CDATA[motion capture with actuated feedback for motor learning]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037773</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037773</url>
		<abstract>
			<par><![CDATA[<p>How can we augment human performance and improve motor learning? How can we encourage and enhance the process of learning dance, martial arts, sports or movie action? How can we make such a process more immersive and playful?</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[motion capture]]></kw>
			<kw><![CDATA[motor learning]]></kw>
			<kw><![CDATA[training]]></kw>
			<kw><![CDATA[vibro-tactile feedback]]></kw>
			<kw><![CDATA[wearable]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809369</person_id>
				<author_profile_id><![CDATA[81488669449]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kenichiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fukushi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab and Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fukushi@isl.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809370</person_id>
				<author_profile_id><![CDATA[81100422084]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Jan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zizka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab and Comenius University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jzizka@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2809371</person_id>
				<author_profile_id><![CDATA[81100022847]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ramesh]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Raskar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MIT Media Lab]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[raskar@media.mit.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276422</ref_obj_id>
				<ref_obj_pid>1275808</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Raskar, R. and et al. 2007. Prakash: Lighting Aware Motion Capture using Photosensing Markers Multiplexed illuminators. In ACM SIGGRAPH 2007.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Second Skin: Motion Capture with Actuated Feedback for Motor Learning Kenichiro Fukushi1,2 Jan Zizka1,3 
Ramesh Raskar1 fukushi@isl.titech.ac.jp jzizka@gmail.com raskar@media.mit.edu MIT Media Lab1 Tokyo Institute 
of Technology2 Comenius University3  Figure 1: Our motor learning system with vibro­tactile feedback. 
Second Skin uses three­dimensional motion tracking to trace a user s movements, and provide automatic, 
real­time vibro­tactile feedback. This feedback is used to aid in the correction of motion and position 
errors to enhance and quicken motor learning process. This feedback is generated by small vibration motors 
attached to a suit that provides vibro­tactile stimuli to the user. Small photo sensors are also attached 
to the suit to receive coded infrared position­tracking information. Keywords: Vibro­tactile Feedback, 
Motor Learning, Wearable, Training, Motion Capture 1. Introduction How can we augment human performance 
and improve motor learning? How can we encourage and enhance the process of learning dance, martial arts, 
sports or movie action? How can we make such a process more immersive and playful? We introduce a lightweight 
wearable suit, Second Skin , which provides real­time vibro­tactile stimuli to a user. Previous research 
in Virtual Reality shows motion tracking facilitates deeper situational understanding and immersion. 
It has also been demonstrated that motion capture with vibro­tactile feedback is useful for motor learning. 
Second Skin aims to enhance and quicken the motor learning process by combining three­dimensional (3D) 
motion tracking and an automatic, real­time vibro­tactile feedback system. 2. Implementation The 3D 
tracking system is an extension of the Prakash [Raskar and et al.] motion tracking system, which inherently 
provides a number of promising qualities: it is unaffected by ambient lighting conditions, there is no 
issue with marker reacquisition of identification, and it is low­cost. A user s 3D motion data is compared 
with a reference such as recorded motion data of an expert, and corrective feedback is produced instantly 
in the form of vibro and visual information. The vibro­tactile feedback is created by small vibration 
motors similar to ones utilized in cell phones. The motors are attached to different parts of the body, 
and by controlling each of them independently, a user can experience many different tactile stimuli. 
For example, the system can track a user s swing motion as they are practicing golf. After comparison 
with a reference, the user is made aware of any deviation through a tactile cue. This cue indicates which 
arm should be moved in which direction , and vibrational intensity can indicate the degree of the deviation. 
 3. Application The Second Skin project presents a number of exciting future possibilities. Our low cost 
tracking system and its ambient light robustness will make it usable in a number of different locations 
and facilities that normally would not be able to operate a motion capture setup. This could sustain 
an entirely new approach to dance practice, sports training and healthcare applications. For example, 
dance schools and gyms could acquire our system for their clients to use, hospitals could use it to aid 
patients in rehabilitation, factories could use it to instruct employees to an optimized motion in an 
assembly line. Likewise, the Second Skin suit can be used in training for very dynamic motions such as 
movie action sequence. The Second Skin suit is designed to be lightweight and minimally inhibitive, as 
the name suggests. Thus, it would be natural to apply Second Skin to the increasingly popular genre of 
interactive dance and instrumental games. In exchange for omnidirectional motion tracking, we can replace 
the tracking system with the devices similar to KinectTM so that a number of people in the world can 
experience our immersive and playful motor learning system. 4. Future Work A more complete wearable 
suit with more photo sensors and more vibration motors currently being developed to cover the whole body. 
It would also improve the vibro­tactile stimuli to take advantage of psychological knowledge on tactile 
illusions . References RASKAR, R. AND ET AL. 2007. Prakash: Lighting Aware Motion Capture using Photosensing 
Markers Multiplexed illuminators. In ACM SIGGRAPH 2007. Copyright is held by the author / owner(s). SIGGRAPH 
2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037774</article_id>
		<sort_key>590</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>52</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[The composition context in point-and-shoot photography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037774</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037774</url>
		<abstract>
			<par><![CDATA[<p>With the recent popularization of digital cameras and cameraphones, everyone is now a photographer, and the devices provide new opportunities for improving the process and final results. While there has been research on what kinds of subjects users prefer to photograph, and what they do with the images once they are captured [Van House et al. 2005], no formal studies on the process of framing an image using a camera have been performed. To fill this gap, our study attempts to characterize the actions performed by users while framing photos using a point-and-shoot camera, in preparation for taking a photograph. This includes adjusting the camera's orientation and point of view and triggering zoom and autofocus controls.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809372</person_id>
				<author_profile_id><![CDATA[81310482354]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daniel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vaquero]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[daniel@cs.ucsb.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809373</person_id>
				<author_profile_id><![CDATA[81100457810]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Matthew]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Turk]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Santa Barbara]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mturk@cs.ucsb.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1778766</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Adams, A., et al. 2010. The Frankencamera: an experimental platform for computational photography. <i>ACM Trans. Graph. (Proc. SIGGRAPH) 29</i>, 4, 1--12.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1057039</ref_obj_id>
				<ref_obj_pid>1056808</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Van House, N., et al. 2005. The uses of personal networked digital imaging: an empirical study of cameraphone photos and sharing. In <i>CHI '05 Extended Abstracts</i>, 1853--1856.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037775</article_id>
		<sort_key>600</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>53</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[The cyclone display]]></title>
		<subtitle><![CDATA[rotation, reflection, flicker and recognition combined to the pixels.]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037775</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037775</url>
		<abstract>
			<par><![CDATA[<p>Most of us remember playing with colorful spinning tops when we were children. Here in our research, we made mechanical pixels with the spinning tops which introduces the mechanical display. This enables to enlarge the pixels without the direct lighting. Those mechanical pixels could be applied to make an ambient atmosphere in small or large space.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809374</person_id>
				<author_profile_id><![CDATA[81466641621]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yoichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ochiai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ochyai@acm.org]]></email_address>
			</au>
			<au>
				<person_id>P2809375</person_id>
				<author_profile_id><![CDATA[81488664879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hiromu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Tsukuba]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The Cyclone Display: Rotation, Re.ection, Flicker and Recognition combined to the pixels. Yoichi Ochiai* 
and Hiromu Takai* *University of Tsukuba, School of Information Science, College of Media Arts, Science 
and Technology  Figure 1: (top-left)Overviews of 37 pixels, (top-center) shows character 3 (top-right) 
Interaction with hand shadows and .icker (bottom-left)rainbow color on the pixels (bottom-center) enlargement 
of pixels, (bottom-right)texture variation of rotation disksIt s very dif.cult to take the picture as 
we see this display. It combines our color recognition and projected light. 1. Introduction Most of us 
remember playing with colorful spinning tops when we were children. Here in our research, wemade mechanical 
pixels with the spinning tops whichintroduces the mechanical display. This enables to en­large the pixels 
without the direct lighting. Those me­chanical pixels could be applied to make an ambientatmosphere in 
small or large space. We added some essentials factors to the tops such as.icker, projection, rotation 
speed. The combination en­ables us to make multicolored expression and to recog­nize the difference of 
rotating speed of the disks. This system should be able to contribute to a new kind of display engineering 
and interactive communication between display and humans. 2. Design The prototype of the Cyclone display 
is made up of 37 rotating mechanical pixels. The pixels consist of PWMmotor and pattern printing disk(Fig.1). 
Each pixel iscontrolled individually for its rotating speed. Now on .icker. Whether we use the .icker 
light or not we can see the colorful lines on rotating disks with variously printed patterns. So you 
could use rotating disk as apixel of no illuminated display. We can also observeseveral striped patterns 
by using white .icker or just by *email: ochyai@acm.org blinking the eyes. The colors and patterns depend 
on thespeeds of rotation and .icker. It is possible to change factors for each pixel according to the 
need and circumstance on the Cyclone display. Also I might add that if we use .ickered time-divided RGB 
lights, Cyclone display can express several color gradations. Cyclone display has many interesting factors 
and in­teractivity with the human(Fig. 2). By using natural body movement such as hands or eyes, the 
different color and different texture and different patterns are seen on theCyclone Display. 3. Application 
Cyclone Display offers the new and different approach for the interactions with the humans. We just have 
to wave our hand or blink the eyes. That enables us an in­dividual communication with the display. The 
individualcommunicability has a possibility for entertainment ap­plication. At the point of ambient display, 
it suggest thenew way to express Rotation and Recognition Pat­terns . Also, Cyclone pixels are easy to 
enlarge their sizes with low cost. Besides, we could choose the disk patterns to .t circumstances. 4. 
Future Work We introduced the prototype of Cyclone Display madeof printed-paper disks. If it was made 
of liquid crystalsdisplay we could change the patterns of each disk. On top of that, with projection 
images it would work as ro­tating disk screen for the new expression. It makes us to produce diffuse 
effect or re.ection effect on certain diskssuch as re.ective or unre.ective disks.They deepen thetexture 
expression of projected graphics. Furthermore, alittle illuminated Cyclone Display is good for decoration 
for buildings because of its low cost expansion fee and characterizing the individual/ambient communications. 
Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 
7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037776</article_id>
		<sort_key>610</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>54</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>14</seq_no>
		<title><![CDATA[VITA]]></title>
		<subtitle><![CDATA[visualization system for interaction with transmitted audio signals]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037776</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037776</url>
		<abstract>
			<par><![CDATA[<p>We propose a space-filling display system to visualize sound beam from ultrasonic parametric loudspeaker, which enables various spatial sound interactions with visual feedback.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809376</person_id>
				<author_profile_id><![CDATA[81488650480]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kentaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kimuken@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809377</person_id>
				<author_profile_id><![CDATA[81320490436]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Osamu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hoshuyama]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[houshu@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809378</person_id>
				<author_profile_id><![CDATA[81100172183]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tanikawa]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tanikawa@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809379</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[T. Kamakura, K. Aoki, and S. Sakai, "A highly directional audio system using a parametric array in air," in Proc. 9th Western Pacific Acoustics Conf., pp. 1--8, June, 2006.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[K. Nakamura, H. Ogura and T. Sugimoto. "Direct Visualization of High-Intensity Focused Ultrasonic Field Using Light-Emitting Diodes and Piezoelectric Elements," Acoustical Imaging, Volume 29, Part 5, pp. 309--316, 2009.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 VITA : Visualization system for Interaction with Transmitted Audio signals IMG_8900 P1110652 P1110661 
kyukakucircuit Kentaro Kimura , Osamu Hoshuyama Tomohiro Tanikawa Michitaka Hirose Graduate School of 
Information Science and Technology, The University of Tokyo. Graduate School of Information Science 
and Technology, The University of Tokyo. NEC Corporation. Graduate School of Information Science and 
Technology, The University of Tokyo. Graduate School of Information Science and Technology, The University 
of Tokyo.  Fig. 1: Proposed Space-Filling Display. Fig. 2: Visualization of Sound Beam.  Fig. 3: System 
with No Sound. Fig. 4: LED-and-Transducer Unit. Abstract: We propose a space-filling display system to 
visualize sound beam from ultrasonic parametric loudspeaker, which enables various spatial sound interactions 
with visual feedback. 1. Introduction1  email: {kimuken, houshu, tanikawa, hirose}@cyber.t.u-tokyo.ac.jp 
 In most of the existing sound system with ordinary loudspeaker, there is no spatial interaction with 
the emitted sound, such as delivering the sound to a limited area and controlling distribution of the 
sound. It is because the emitted sound spreads to a whole space. Recently, parametric loudspeakers have 
been put to practical use, which has very narrow directivity [1]. The parametric loudspeakers enable 
interaction such as reflecting and interrupting the sound and to deliver the sound. However, since sound 
is invisible, such interaction is still difficult. To interact with a sound, we propose a sound visualization 
system named VITA (Visualization system for Interaction with Transmitted Audio signals). The proposed 
visualization system provides visual feedback to the user in real-time at the same place and enables 
the interaction by detecting the ultrasonic sound beam. The system does not interfere the sound propagation 
nor disturb interactivity between the users and the sound. Moreover, audience in the same room can see 
the manner of interaction. 2. Proposed Space-Filling Visualization System  The proposed system shown 
in Fig. 1 has tens of cables with ultrasonic transducers (sensors) and LEDs. Each transducer transforms 
an ultrasonic sound from a parametric loudspeaker to electric power, and turns on the corresponding LED 
based on the electric power. Our idea is to fill the space with the units of the transducer and LED applying 
a sort of energy harvesting technology. (Nakamura et al. also use transducers and LEDs for visualization 
of an ultrasonic sound spot with similar idea [2]. However, their system is too small to display the 
sound beam in large room.) Light spots of the LEDs indicate the sound beam (Fig. 2). The cables with 
the units of transducers and LEDs are narrow and flexible not to disturb sound propagation and the user 
movement (Fig.3). The system consists of 1) Parametric loudspeaker: American Technology Corporation model 
H460, 2) LED-and-transducer units (Fig. 4) and 3) LED driving circuits for sufficient brightness. Each 
cable has 4 LED-and-transducer units. Cables of 6 6 are aligned in a space of 2.5-m width, 2.5-m length 
and 2.8-m height. In total, 144 LEDs visualize sound beam. In the proposed system, users can see the 
sound beam and interact with it, e.g. changing the direction of the sound beam with reflection and interrupting 
the beam. Other people in the same room also can see the user s interaction and can participate in it. 
For example, people can recognize which pair talks and see where a man intends to deliver his message. 
3. Conclusion  We have proposed a space-filling display system to visualize sound beam from ultrasonic 
parametric loudspeaker, which enables various spatial sound interactions with visual feedback. Our proposed 
system shows that people can interact with sound in the space by sound visualization and a parametric 
loudspeaker. This potential in our interaction with sound enables each user to control sound more flexibly, 
e.g. delivering a sound where a user intends and picking up what a listener intends in very noisy scene. 
References [1] T. Kamakura, K. Aoki, and S. Sakai, A highly directional audio system using a parametric 
array in air, in Proc. 9th Western Pacific Acoustics Conf., pp. 1 8, June, 2006. [2] K. Nakamura, H. 
Ogura and T. Sugimoto. Direct Visualization of High-Intensity Focused Ultrasonic Field Using Light-Emitting 
Diodes and Piezoelectric Elements, Acoustical Imaging, Volume 29, Part 5, pp. 309-316, 2009. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037777</section_id>
		<sort_key>620</sort_key>
		<section_seq_no>8</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Modeling]]></section_title>
		<section_page_from>8</section_page_from>
	<article_rec>
		<article_id>2037778</article_id>
		<sort_key>630</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>55</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[3D inverse dynamic modeling of strands]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037778</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037778</url>
		<abstract>
			<par><![CDATA[<p>1D deformable structures, often called <i>strands</i>, are ubiquitous in the real world. They range from plants (grass, lianas, stalks) to creatures organs (hair, tail, tentacles) and manufactured objects (cables, ropes). The realistic modeling and animation of such objects is essential for representing convincing virtual environments. Most often, the design pipeline is split in two distinct parts: the geometry creation process itself, namely <i>geometric design</i>, followed by the computation of motion, namely <i>animation</i>. For the sake of flexibility and control, the geometric design generally relies on a pure geometric process that allows the user to interactively edit curve primitives such as splines. In contrast, the animation of strands is often considered as a passive and complex phenomenon that can be realistically captured using physics-based simulation.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809380</person_id>
				<author_profile_id><![CDATA[81474697537]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alexandre]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Derouet-Jourdan]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA/LJK, Grenoble University, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809381</person_id>
				<author_profile_id><![CDATA[81474695479]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Florence]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bertails-Descoubes]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA/LJK, Grenoble University, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809382</person_id>
				<author_profile_id><![CDATA[81319502458]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[J&#246;elle]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Thollot]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA/LJK, Grenoble University, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1142012</ref_obj_id>
				<ref_obj_pid>1179352</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bertails, F., Audoly, B., Cani, M.-P., Querleux, B., Leroy, F., and L&#233;v&#234;que, J.-L. 2006. Super-helices for predicting the dynamics of natural hair. In <i>ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH'06 conference)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866159</ref_obj_id>
				<ref_obj_pid>1882262</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Derouet-Jourdan, A., Bertails-Descoubes, F., and Thollot, J. 2010. Stable inverse dynamic curves. <i>ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH Asia'10 Conference)</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Inverse Dynamic Modeling of Strands Alexandre Derouet-Jourdan Florence Bertails-Descoubes Jo¨elle 
Thollot INRIA /LJK, Grenoble University, France  Figure 1: A consistent dynamic strand synthesis pipeline. 
From left to right: The artist models a cat tail by editing a 3D spline, which is automatically converted 
into a dynamic rod model (super-helix) at rest under gravity. The tail can then be physically animated 
while retrieving its initial shape at the end of slight (possibly strong) motions. 1 Motivation 1Ddeformable 
structures, often calledstrands, are ubiquitous in the real world. They rangefromplants(grass, lianas,stalks) 
tocrea­turesorgans(hair, tail, tentacles) and manufactured objects(cables, ropes). The realistic modeling 
and animation of such objects is essential for representing convincing virtual environments. Most often,thedesignpipelineissplitin 
twodistinctparts: thegeometry creation process itself, namely geometric design, followed by the computation 
of motion, namely animation. For the sake of .exi­bility and control, the geometric design generally 
relies on a pure geometric process that allows the user to interactively edit curve primitives such as 
splines. In contrast, the animation of strands is oftenconsidered asapassiveand complexphenomenon that 
canbe realistically captured usingphysics-based simulation. Inthisposterweaddressthelack of consistencybetweenthese 
two steps. Indeed, no trivial solution is currently available to convert 3D geometric curves into physics-based 
primitives while keeping the precise shape chosen by the user as a stable resting con.gura­tion. Many 
approaches have been developed for deforming slen­der geometries while ensuring a nonlinear behavior, 
allowing for plausible keyframed animation. However, pure geometric meth­odscannot capture typical inertial 
effects(e.g.,buckling),and such complex transitional stateshave tobemanuallyde.nedbytheuser. Conversely,modeling 
strands in aphysics-based environmentbased on indirect operations such as cutting or frizzing may be 
tedious and not intuitiveenoughforanartist who,having apreciseshape in mind,wouldde.nitelyprefer the 
.exibilityofgeometricmethods. 2 Contributions We propose a new solution for bridging the gap between 
the ge­ometric modeling of 3D strand-like objects and the physics-based animationof material rods.Ourapproach 
allowstheartist tofreely designstrandsusing standard tools(splinecurves). Thegeometric curve is thenautomaticallyconverted 
intoaphysical model(super­helix[Bertailsetal.2006])thatfaithfullymatches thechosenshape at rest under 
gravity. The core of our approach relies on a 3D ex­tension of the 2D technique presented in [Derouet-Jourdan 
et al. 2010], leading to the inverse statics of the3D super-helixmodel. Geometric .tting Weproposeanewandrobust 
algorithmfor .t­ting a3D spline toapiecewisehelix,withsomedirect control over the number N of helical 
elements and thus over the time perfor­manceof subsequent animation.Asahelix ischaracterizedby con­stant 
curvature and torsion, we approximate the curvature-torsion pro.le of a spline by a piecewise constant 
function. Each con­stant is computed as the mean curvature-torsion of an element of the spline evenly 
cut into N pieces. Knowing the position and tan­gent at clamped end, we are able to reconstruct a piecewise 
helix that will serve as an initial approximation of the spline. Starting from this initial guess, we 
apply the Levenberg-Marquardt least­squares optimization algorithm in order to re.ne the approximation 
andget apiecewisehelix that closelymatches the input spline. Static .tting We automatically identify 
the physical parameters of the super-helix model such that the piecewise helix resulting frompreviousstep 
exactlymatches therest shapeof therod model undergravity.This isachievedby anatural3D extensionof the2D 
techniqueproposedin[Derouet-Jourdanet al.2010].Themaindif­ference is that analytical computations become 
much more tricky in3D.Inpractice, we useMathematica R ®toprecompute theanalyt­icalformulae.Thenatural 
curvaturesand torsionofthesuper-helix can then be retrieved by simply solving 3N linear equations. Sim­ilarly 
as in the 2D case, we get a suf.cient condition for stability: stiffness > S, where S onlydependson thegeometry 
of therod. mass 3 ResultsandDiscussion Weareableforthe.rst timetoautomaticallyand consistentlygen­erate3Ddynamicstrandsfroman 
inputgeometry.Unlikeprevious approaches, our method guarantees that strands closely match the input curves 
when simulated at rest under gravity, and go back to thisuser-de.ned rest shapeafterslight possiblystrong 
motion. Ourgeometric .ttingprovesfairlyrobustasstrictconvergenceoc­curredfor95% ofthecurves(smooth towavy) 
wehave tested,in areasonableamountof time(afewseconds toafewminuteson a standard PC for .tting 4 to 12 
helical elements). In the 5% re­maining cases, the .tting actually converges to some local mini­mum which 
is nevertheless visually close to the original curve. Inthefuturewewould liketotakeintoaccountcontactforcesinthe 
statics .ttingprocess.Thiswouldallow theuser tocreatestrands in contact withasurface(such ashaircolliding 
with thebody) without having toarti.ciallyincreasethestiffnessforpreserving stability. References BERTAILS, 
F., AUDOLY, B., CANI, M.-P., QUERLEUX, B., LEROY, F., AND LEV ´ EQUE,J.-L. 2006. Super-helicesforpre­dicting 
the dynamics of natural hair. In ACM Transactions on Graphics (Proceedings of the ACM SIGGRAPH 06 conference). 
DEROUET-JOURDAN, A., BERTAILS-DESCOUBES, F., AND THOLLOT, J. 2010. Stable inverse dynamic curves. ACM 
Transactions on Graphics (Proceedings of the ACM SIGGRAPH Asia 10 Conference). Copyright is held by the 
author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037779</article_id>
		<sort_key>640</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>56</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[3D reconstruction of detail change on dynamic non-rigid objects]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037779</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037779</url>
		<abstract>
			<par><![CDATA[<p>3D reconstruction of a moving object is notable technique in fields such as digital archives or entertainments. [de Aguiar et al. 2008] is able to reconstruct motion and spatio-temporally coherent time-varying geometry using by eight multi-view video and a range scanner. However, they couldn't capture the numerous high-frequency folds of garments, it make the realism of the re-constructed dynamic models reduce. On the other hand, [Hern&#225;ndez et al. 2007] proposed the technique to reconstruct detailed 3D shape of moving cloth including high frequency folds by using one camera. They also use three primary colored lights to acquire the shading information independently at the same time. This property made it possible to estimate surface normals on the object from the video sequence by using photometric stereo. However, there is a defect that the area which can be reconstructed is only limited to frontal surface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809383</person_id>
				<author_profile_id><![CDATA[81488673292]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Daichi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Taneda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[aim-to-novel@akane.waseda.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809384</person_id>
				<author_profile_id><![CDATA[81466645654]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Hirofumi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Suda]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809385</person_id>
				<author_profile_id><![CDATA[81319496663]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Akinobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maejima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809386</person_id>
				<author_profile_id><![CDATA[81100066959]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Shigeo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Morishima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Waseda University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1360697</ref_obj_id>
				<ref_obj_pid>1360612</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[de Aguiar, E., Stoll, C., Theobalt, C., Ahmed, N., Seidel, H.-P., and Thrun, S. 2008. Performance capture from sparse multi-view video. <i>ACM Transactions on Graphics 27</i>, 98.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hernandez, C., Vogiatzis, G., Brostow, G. J., Stenger, B., and Cipolla, R. 2007. Non-rigid photometric stereo with colored lights. In <i>Proc. of ICCV</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Reconstruction of Detail Change on Dynamic Non-Rigid Objects  D:\Data\20110111\Image\camera08\00000056.png 
 (1) Daichi Taneda* Hirofumi Suda Akinobu Maejima Shigeo Morishima  Waseda University (a) Base shape 
(b) Captured image (c) Deformed shape (d) Reconstructed wrinkle  Figure 1: 3D Reconstruction Result 
of the Wrinkle on a Shirt 1 Introduction 3D reconstruction of a moving object is notable technique in 
fields such as digital archives or entertainments. [de Aguiar et al. 2008] is able to reconstruct motion 
and spatio-temporally coher-ent time-varying geometry using by eight multi-view video and a range scanner. 
However, they couldn t capture the numerous high-frequency folds of garments, it make the realism of 
the re-constructed dynamic models reduce. On the other hand, [Her-nández et al. 2007] proposed the technique 
to reconstruct detailed 3D shape of moving cloth including high frequency folds by using one camera. 
They also use three primary colored lights to acquire the shading information independently at the same 
time. This property made it possible to estimate surface normals on the object from the video sequence 
by using photometric stereo. However, there is a defect that the area which can be recon-structed is 
only limited to frontal surface. In this paper, we present a method to reconstruct a detailed 3D geometry 
all around the deformable object accurately. First, we obtain a range scan data of garment which does 
not include wrinkles but well reproduced low frequency geometry. Then, we deform this initial shape to 
fit to each video frame. Finally, we estimate normal vectors on the target surface by our original reflection 
model using 3 colored lights, and add high frequency geometry such as folds and wrinkles. 2 Acquisition 
of Input Data In this work, we choose a T-shirt as a target object because wrin-kles are generated frequently 
in motion. Firstly, we obtain a 3D geometry of the target in detail with a laser range scanner. We refer 
to this range scan data as base shape . At this time, some small round stickers are attached on the target 
as markers to take correspondence between multi-video sequences. Secondly, we take videos of the moving 
target. To obtain entire images of the target, eight video cameras are mounted in a ring around the target. 
Moreover, light sources which have been filtered with three primary color are placed at equal intervals. 
Figure 1(b) shows an example frame from the captured video sequences. 3 Deformation of Base Shape To 
take correspondences, the base shape is deformed by using RBF (Radial Basis Function) interpolation so 
that the base shape is fitted to the shape of target in each frame. 3D positions of markers are acquired 
by triangulation, and the base shape is de-formed depending on position of markers. By the way, at first 
frame, the labeling of markers is performed by manual control, and after second frame, each marker is 
tracked referring to the marker position of the previous frame. We refer to this geometry as deformed 
shape . In case some markers couldn t be tracked, they are acquired by RBF interpolation based on position 
of markers which could be tracked. Figure 1(c) shows the deformed shape corresponding to Figure 1(b). 
4 Normal Estimation from Shading In real environment, it is difficult to create an ideal direction-al-light, 
thus Lambertian reflection model cannot be employed. Even if normal vectors are the same in each position, 
each brightness is not always constant because of the reflectance property of the cloth. Therefore, to 
adapt to the real environment, we make the original reflection model of target beforehand by acquiring 
the relationship between position, brightness, and nor-mal under irradiating three primary colors lights. 
We formulate the cost function Ei consisting of the difference between bright-ness of image Iimage and 
brightness of reflection model Icalc(v,n), under constraints of a continuity of normals and difference 
be-tween initial normals and new normals. And we estimate normals for all vertices   of the deformed 
shape in each frame by minimizing Ei. .....................2221,initiineighiineighimageiicalciNIIEnnnnnv. 
   where n is a new normal, ninit is an initial normal, nneigh is one of the normal of neighboring 
vertices, Nneigh is a number of neigh-boring vertices, and . is a weight of the second term. The value 
of . is chosen by the edge of the image. It is judged that the area where the edge is extracted clearly 
is a region that is folded, and so low value is set to .. 5 Result According to the mentioned procedure 
above, we show the result of 3D reconstruction by estimating normals from shading on the deformed shape. 
The reconstruction result of one frame of video sequence is shown in Figure 1(d). The result especially 
in wrin-kles part looks almost same as original. 6 Conclusion In this paper, we presented a method to 
reproduce entire 3D shape of the moving cloth which includes high frequency folds. From the reconstruction 
result, our normal estimation is effective to add accurate wrinkles when we deformed the base shape ac-quired 
with the laser range scanner. As a future work, we plan to integrate our algorithm to other method which 
can obtain the precise base shape without using a range scanner and markers. In addition, we need to 
examine the evaluation method to make sure an accuracy of normals estimated by our algorithm. Reference 
DE AGUIAR, E., STOLL, C., THEOBALT, C., AHMED, N., SEIDEL, H.-P., AND THRUN, S. 2008. Performance capture 
from sparse multi-view video. ACM Transactions on Graphics 27, 98. HERNANDEZ, C., VOGIATZIS, G., BROSTOW, 
G. J., STENGER, B., AND CIPOLLA, R. 2007. Non-rigid photometric stereo with colored lights. In Proc. 
of ICCV. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037780</article_id>
		<sort_key>650</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>57</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[3D shape scanning with a Kinect]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037780</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037780</url>
		<abstract>
			<par><![CDATA[<p>We describe a method for 3D object scanning by aligning depth and color scans which were taken from around an object with a Kinect camera. Our easy-to-use, cost-effective scanning solution could make 3D scanning technology more accessible to everyday users and turn 3D shape models into a much more widely used asset for many new applications, for instance in community web platforms or online shopping.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809387</person_id>
				<author_profile_id><![CDATA[81488656991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cui]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Yan.Cui@dfki.de]]></email_address>
			</au>
			<au>
				<person_id>P2809388</person_id>
				<author_profile_id><![CDATA[81100587368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Didier]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stricker]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[Didier.Stricker@dfki.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Cui, Y., Schuon, S., Derek, C., Thrun, S., and Theobalt, C. 2010. 3d shape scanning with a time-of-flight camera. <i>In Proc. of IEEE CVPR 2010</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 3D Shape Scanning with a Kinect Yan Cui Didier Stricker German Research Center for Arti.cial Intelligence 
(DFKI), Kaiserslautern, Germany *    Figure 1: 3D shape scanning results with a Kinect, mesh and textured 
models 1 Motivation and Introduction We describe a method for 3D object scanning by aligning depth and 
color scans which were taken from around an object with a Kinect camera. Our easy-to-use, cost-effective 
scanning solution could make 3D scanning technology more accessible to everyday users and turn 3D shape 
models into a much more widely used asset for many new applications, for instance in community web platforms 
or online shopping. The Kinect camera has a variety of advantages over alternative 3D scanning technologies: 
It can simultaneously measure depth and color at standard video rate which makes it suitable for integration 
into a fast object scanner. The depth sensor does not interfere with the scene in the visual spectrum, 
and in addition it is easy to operate. However, the biggest algorithmic challenge is that the sensor 
of the Kinect has a comparably low X/Y resolution and depth accuracy for the application of 3D shape 
scanning. To overcome these limi­tations, we developed a solution with the following main contribu­tions: 
1) the combination of several depth and color sensor frames to enhance the .nal resolution; 2) a method 
for 3D shape closure, enabling the combination of scans taken from around an object; 3) a probabilistic 
procedure for simultaneous non-rigid alignment of multiple scans to improve the .nal model quality. 
(a) (b) (c) Figure 2: (a) Raw input data (top) and superresolution result (bot­tom); (b) Loop closing 
with local (top) and global (bottom) align­ment methods; (c) 3D model without (top) and with non-rigid 
reg­istration (bottom). *e-mail: {Yan.Cui, Didier.Stricker}@dfki.de  2 Our Approach First, a superresolution 
approach [Cui et al. 2010] is applied to each chunk of ten captured frames. This yields a high-resolution 
depth map aligned to the center frame of the chunk. All depth and color maps in the chunk are aligned 
to the center frame using optical .ow. This is suf.ciently accurate since the maximum viewpoint displace­ments 
throughout the entire chunk are typically one to two pixels. Then a high-resolution denoised center depth 
and color map is ex­tracted from the aligned low resolution maps by solving an energy minimization problem. 
The energy function includes depth and color constraint data terms and a smoothing regularization term. 
Fig. 2a shows the raw input data (top) and the superresolution re­sult (bottom). The global alignment 
consists of three steps: 1) calculate corre­sponding points of two frames using pairwice ICP registration; 
2) label matches correct or incorrect using the absolute error; 3) calculate the exponential transform 
for each frame under a confor­mal geometric algebra using an energy function, which is based on the correct 
matches. Fig. 2b shows the results only with local (top) and with the global (bottom) alignment method. 
There are still many mismatched areas because of the low accuracy of the Kinect (Fig. 2c top). Applying 
the non-rigid probabilistic simultaneous alignment from [Cui et al. 2010] improves the result signi.cantly 
(Fig. 2c bottom). For each of the shown objects we captured about 1000 frames by either moving the Kinect 
around the object or turning the object in front of a static Kinect. A superreso­lution frame is computed 
for each ten consecutive raw frames. The superresolution frames are aligned by using our global and non­rigid 
approach. Finally a 3D mesh is generated using the Poisson reconstruction method. The runtime of the 
complete algorithm, implemented in C++, is about 5 minutes on an Intel Xeon 3520 (2.6 GHz) with 12 GB 
of RAM. References CUI, Y., SCHUON, S., DEREK, C., THRUN, S., AND THEOBALT, C. 2010. 3d shape scanning 
with a time-of-.ight camera. In Proc. of IEEE CVPR 2010. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037781</article_id>
		<sort_key>660</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>58</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Band decomposition of 2-manifold meshes for physical construction of large structures]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037781</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037781</url>
		<abstract>
			<par><![CDATA[<p>With the design and construction of more and more unusually shaped buildings, the computer graphics community has started to explore new methods to reduce the cost of the physical construction for large shapes. Most of currently suggested methods focus on reduction of the number of differently shaped components to reduce fabrication cost. In this work, we focus on physical construction using developable components such as thin metals or thick papers. In practice, for developable surfaces fabrication is economical even if each component is different. Such developable components can be manufactured fairly inexpensively by cutting large sheets of thin metals or thin paper using laser-cutters, which are now widely available.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809389</person_id>
				<author_profile_id><![CDATA[81100014535]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Qing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Xing]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809390</person_id>
				<author_profile_id><![CDATA[81488670486]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Gabriel]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Esquivel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809391</person_id>
				<author_profile_id><![CDATA[81100124987]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ergun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akleman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809392</person_id>
				<author_profile_id><![CDATA[81408598676]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Jianer]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Texas A&M University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809393</person_id>
				<author_profile_id><![CDATA[81342496250]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Jonathan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Gross]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Columbia University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Band Decomposition of 2-Manifold Meshes For Physical Construction of Large Structures Qing Xing, Gabriel 
Esquivel, Ergun Akleman &#38; Jianer Chen Jonathan Gross Texas A&#38;M University Columbia University 
 Figure 1: This large sculpture of Bunny is constructed with laser cut poster-board papers assembled 
with brass fasteners. With the design and construction of more and more unusu­ally shaped buildings, 
the computer graphics community has started to explore new methods to reduce the cost of the physical 
construction for large shapes. Most of currently suggested methods focus on reduction of the number of 
dif­ferently shaped components to reduce fabrication cost. In this work, we focus on physical construction 
using devel­opable components such as thin metals or thick papers. In practice, for developable surfaces 
fabrication is economical even if each component is di.erent. Such developable com­ponents can be manufactured 
fairly inexpensively by cutting large sheets of thin metals or thin paper using laser-cutters, which 
are now widely available. (a) (b) (c) Figure 2: Construction elements for bunny. (a) is an exam­ple 
of vertex component that is cut with laser cutter, (b) shows elements of vertex component and (c) shows 
the pro­cess of assembling vertex components with fasteners. We observe that one of the biggest expenses 
for construc­tion of large shapes comes from handling and assembling the large number components. This 
problem is like putting pieces of a large puzzle together. However, unlike puzzles we do not want construction 
process to be challenging. In­stead, we want to simplify the construction process in such a way that 
the components can be assembled with a minimum instruction by the construction workers who may not have 
extensive experience. In this work, we introduce an approach to automatically cre­ate such easily assembled 
developable components from any given manifold mesh. Our approach is based on classical Graph Rotation 
Systems (GRS). Each developable compo­nent, which we call vertex component, is a physical equiva­lent 
of a rotation at the vertex v of a graph G. Each vertex component is a star shaped polygon that physically 
corre­sponds to the cyclic permutation of the edge-ends incident on v (See Figure 2(a)). We engrave edge-numbers 
with laser­cutters directly on edge-ends of vertex components to sim­plify .nding corresponding edge 
ends. When we print edge­numbers, we actually de.ne a collection of rotations, one for each vertex in 
G. This is formally called a pure rotation system of a graph. The fundamental He.ter-Edmunds theorem 
of GRS asserts that there is a bijective correspondence between the set of pure rotation systems of a 
graph and the set of equivalence classes of embeddings of the graph in the orientable surfaces. As a 
direct consequence of the theorem, to assemble the structure all construction workers have to do is to 
attach the corresponding edge-ends of vertex components. Once all the components are attached to each 
other, the whole structure will correctly be assembled. Gauss-Bonnet theorem, moreover, asserts that 
the total Gaussian curvature of a surface is the Euler characteristics times 2p. Since the structure 
is made up only developable components, Gaussian curvature is zero everywhere on the solid parts. The 
Gaussian curvature happens only in empty regions and that are determined uniquely. Since, we cor­rectly 
form Gaussian curvature of holes, the structures will always be raised and formed 3-space. We also develop 
strategies to simplify .nding corresponding pieces among a large number of vertex components. Us­ing 
this approach, Architecture students have constructed a large version of Stanford Bunny (see Figure 1) 
in a design and fabrication course in College of Architecture. The costs of poster-board papers and fasteners 
were very minimal, less than $100. We are currently working on to construct even larger shapes using 
stronger materials. We are also planning to use the structures obtained by this approach as molds to 
cast large plaster or cement sculptures. This work partially supported by the National Science Foun­dation 
under Grant No. NSF-CCF-0917288. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>National Science Foundation</funding_agency>
			<grant_numbers>
				<grant_number>NSF-CCF-0917288</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2037782</article_id>
		<sort_key>670</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>59</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[An efficient PILP algorithm for 3D region guarding and star decomposition]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037782</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037782</url>
		<abstract>
			<par><![CDATA[<p>We propose an efficient algorithm to compute 3D region guarding and star decomposition. A star decomposition partitions a 3D region to a set of sub-regions, each of which is visible from an interior point and is called a star shape. Star decomposition is closely related to the well known gallery guarding problem [Chvatal 1975]. Despite their broad applications in graphics and robotics, 3D region guarding and star decomposition are highly challenging and have been little explored.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809394</person_id>
				<author_profile_id><![CDATA[81501654417]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Wuyi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yu]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Xiamen University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809395</person_id>
				<author_profile_id><![CDATA[81351597620]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Xin]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Li]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Louisiana State University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Chvatal, V. 1975. A combinatorial theorem in plane geometry. <i>Journal of Combinatorial Theory Series B 18</i>, 39--41.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Lien, J.-M. 2007. Approximate star-shaped decomposition of point set data. In <i>Eurographics Symposium on Point-Based Graphics</i>, 1--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 An Ef.cient PILP Algorithm for 3D Region Guarding and Star Decomposition Wuyi Yu Xin Li Xiamen University 
Louisiana State University 1 Introduction We propose an ef.cient algorithm to compute 3D region guarding 
and star decomposition. A star decomposition partitions a 3D re­gion to a set of sub-regions, each of 
which is visible from an interior point and is called a star shape. Star decomposition is closely re­lated 
to the well known gallery guarding problem [Chvatal 1975]. Despite their broad applications in graphics 
and robotics, 3D re­gion guarding and star decomposition are highly challenging and have been little 
explored. On a geometric region M, the optimal guarding problem seeks a smallest set of points (guards) 
{gi}.M to which the entire region .p . .M is visible. Solving optimal guarding problem is dif.cult. Even 
for a 2D region M, .nding optimal (fewest necessary) guards is shown to be NP-hard. [Lien 2007] presents 
a greedy algorithm for approximately guarding point cloud data. To our best knowl­edge, no other practical 
guarding computation algorithm has been developed for general 3D regions represented by polyhedra. We 
suggest an ef.cient optimization framework for this problem.  2 Our Approach Given a 3D region M bounded 
by a triangle mesh .M = {V, F }, where V and F are sets of the vertices and triangles, we say a point 
q on .M is visible to a point p . M, if the line segment pq con­necting p and q lies totally inside M, 
namely, it only intersects .M on q: pq .M = {q}. For a 3D region M discretely represented by a tetrahedral 
mesh, from its candidate vertex set, we can formu­late the optimal gallery guarding problem to cover 
the entire .M. Our intuition is to use medial axes (curve skeletons) as candidates because the skeleton 
usually has great visibility to the object bound­ary and hierarchical skeleton can be effectively computed 
to reduce the problem size. Therefore, we extract the shape skeletons of a se­quence of progressively 
simpli.ed meshes, then apply a multi-level optimization for ef.cient problem solving. 2.1 Visibility 
Detection A face fj . F is visible to a point p if all its vertices are visible. We need to detect the 
visible faces set of a given skeletal node. If a line segment pvi .M = {q}, and |pq|< |pvi|, then vi 
is not visible from p. Enumerating pvi for .vi . V to check its intersections with all triangles in F 
takes O(nV · nT )= O(n 2 ) complexity. We develop a sweep algorithm using spherical coordinates to pre-sort 
all faces and only check ones incident to pvi. This improves the ef.ciency of detecting the visible region 
of each p to O(n log n).  2.2 Optimization Algorithms for Guarding When the visibility of all skeletal 
nodes is computed, picking a node set of minimum size that covers all boundary faces can be converted 
to a set-covering problem, which is also NP-complete. A greedy solution for set-covering is to iteratively 
pick the skeletal node with the largest visible region on uncovered elements, remove covered faces from 
F , repeat this until F is empty. The greedy algorithm is ef.cient and its optimality is bounded by O(log 
C), C being the size of the optimal solution. Table 1: Runtime Table of Guarding Computation. #V is vertex 
number of .M, #G is number of computed guards, T is the com­putation time in seconds. #G T(s) Model #V 
 ILP Greedy PILP ILP Greedy PILP Armadillo 20,002 38 30 590.8 601.4 Female 10,002 13 18 14 2,046.2 
279.1 300.3 Male 10,002 14 16 15 3,074.3 312.6 330.8 Greek 9,994 15 22 18 4,122.4 307.4 312.9 David 9,996 
16 22 17 107,391.2 245.1 248.2 An optimal solution can be computed by 0 -1 programming, also called Integer 
Linear Programming (ILP). For every skeleton point pi,i =1,...,m, assign a variable xi such that xi =1 
if pi is picked, and xi =0 if not. The objective function to minimize is .m then i=1 xi. To cover the 
entire .M, for .j =1, . . . , n, fj . F is visible to a set of nodes Qj = {p(j,1),...,p(j,k)}, then at 
least one node in Qj should be chosen. So the constraints are xj = {0, 1}, and xk = 1, .j .{1,...,n}. 
ILP gets pk .Qj the optimal guarding, but has exponential complexity. We develop a progressive integer 
linear programming (PILP) framework for models with big sizes. We progressively simplify the boundary 
mesh .M into coarser resolutions. In the coarsest level, we solve the optimal guards by ILP. Then we 
progressive­ly move to .ner levels with more details: on each level, we map guards to the .ner skeleton, 
ignore least signi.cant guards, remove covered regions, and solve ILP again to .nd necessary new guards. 
As details increase, new guards are added until the .nest resolu­tion of .M is covered. PILP improves 
computational ef.ciency for several orders of magnitude over ILP, but still get very nice approx­imate 
optimization results (see Table 1). We perform experiments on large amount of models (see some in the 
teaser and more in the video).  2.3 Star Decomposition We can use the optimal guarding points to compute 
star decompo­sition. Guarding points are natural seeds for a region-growing seg­mentation scheme. We 
can grow different regions simultaneously with the restriction of preserving their star-property.  3 
Applications We are exploring applications of region guarding and star decom­position, in such as shape 
interpolation, shape matching/retrieval, and robotics for optimal autonomous environment inspection, 
as il­lustrated in the accompanied video. References CHVATAL, V. 1975. A combinatorial theorem in plane 
geometry. Journal of Combinatorial Theory Series B 18, 39 41. LIEN, J.-M. 2007. Approximate star-shaped 
decomposition of point set data. In Eurographics Symposium on Point-Based Graphics, 1 8. Copyright is 
held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037783</article_id>
		<sort_key>680</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>60</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Functional tree models reacting to the environment]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037783</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037783</url>
		<abstract>
			<par><![CDATA[<p>Visually realistic tree images competing for light and space have been created previously [Paubicki et al. 2009]. Here we present a method of generating tree forms reacting to the environment based on several biological hypothesis. In this method, a tree is composed of functional organs playing roles of sources and/or sinks of biomass. Each tree is a stand-alone artificial life with internal feedback between its structure and functioning.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809396</person_id>
				<author_profile_id><![CDATA[81488644879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Jing]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hua]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Automation, Chinese Academy of Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jhua@nlpr.ia.ac.cn]]></email_address>
			</au>
			<au>
				<person_id>P2809397</person_id>
				<author_profile_id><![CDATA[81367594198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Mengzhen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Institute of Automation, Chinese Academy of Science]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[mengzhen.kang@ia.ac.cn]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Mathieu, A., Courn&#232; de, P.-H., Letort, V., Barth&#233;l&#233;my, D., and de Reffye, P. 2009. A dynamic model of plant growth with interactions between development and functional mechanisms to study plant structural plasticity related to trophic competition. <i>Annals of Botany 103</i>, 1173--1186.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531364</ref_obj_id>
				<ref_obj_pid>1576246</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Paubicki, W., Horel, K., Longay, S., Runions, A., Lane, B., and Prusinkiewicz, R. M. P. 2009. Self-organizing tree models for image synthesis. In <i>ACM Transactions on Graphics - Proceedings of ACM SIGGRAPH 2009</i>, 1--10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Functional Tree Models Reacting to the Environment Jing HUA*, MengZhen KANG LIAMA, NLPR Key Laboratory 
of Complex Systems and Intelligence Science Institute of Automation, Chinese Academy of Science   Figure 
1: (a) two trees with different spacing 1 Introduction Visually realistic tree images competing for 
light and space have been created previously [Paubicki et al. 2009]. Here we present a method of generating 
tree forms reacting to the environment based on several biological hypothesis. In this method, a tree 
is com­posed of functional organs playing roles of sources and/or sinks of biomass. Each tree is a stand-alone 
arti.cial life with internal feed­back between its structure and functioning. We tested two hypothesis 
of controlling bud breakout: source sink ratio and light availability. In both cases, the tree form show 
adap­tion to the environment without human interaction. 2 Method The mathematical model underlying our 
system can be referred to [Mathieu et al. 2009]. Basically, the functional feature of our sys­tem lies 
in the computation of biomass availability and its partition­ing inside tree structure. Therefore, organ 
size is the result of inter­nal competition instead of being de.ned independently [Paubicki et al. 2009]. 
The ratio between available biomass and plant demand (sum of organ sink strength) is called source sink 
ratio. To obtain varying tree forms under different planting conditions, we tested two hypothesis. One 
hypothesis is that bud breakout takes place when source sink ratio is over a threshold. Another hypoth­esis 
is that bud fate is decided by light availability. A simple light model was implemented in our system. 
From each point of the leaf mesh, several rays were emitted evenly into sky sphere. For each ray, we 
counted leaves that this ray encounters (assuming n). The visibility of this ray is calculated as tn, 
where t is the light transmit­tance of leaf. Finally, the visibility of a leaf is estimated using the 
mean value of visibility of all rays emitted from this leaf mesh. A threshold controls the fate of the 
axillary bud linked to this leaf. *e-mail: jhua@nlpr.ia.ac.cn e-mail: mengzhen.kang@ia.ac.cn (b) two 
neighboring trees with light competition 3 Results and Future Work Two examples of our method are illustrated 
in Fig.1. The same set of functional and structural parameters controlled each pair of trees. Fig.1 (a) 
shows two trees with different spacing conditions. Denser spacing limited biomass availability, gave 
lower internal source sink ratio and .nally less forks. Fig.1 (b) shows two neighboring trees competing 
for light. As buds located at inner side see less light, fewer branches were produced, giving unsymmetrical 
tree struc­ture. The two trees are different in structure because of the random­ness in ray emission. 
Finally, the differences in tree size come from their different source and sink evolutions. This work 
can serve for providing virtual tree models as required by 3D scene environment. Future work includes 
collision detection with environment, accurate and ef.cient calculation of light distri­bution in tree 
canopy, and using elaborated photosynthesis model for biomass computation. Acknowledgements We thank 
Philippe de Reffye for his full support in plant mod­eling. This study was supported by 863 Program of 
China (2008AA10Z218) and NSFC (60703043). References MATHIEU, A., COURN `EL ´ EDE, P.-H., LETORT, V., 
BARTH ´EMY, D., AND DE REFFYE, P. 2009. A dynamic model of plant growth with interactions between development 
and functional mechanisms to study plant structural plasticity related to trophic competition. Annals 
of Botany 103, 1173 1186. PAUBICKI, W., HOREL, K., LONGAY, S., RUNIONS, A., LANE, B., AND PRUSINKIEWICZ, 
R. M. P. 2009. Self-organizing tree models for image synthesis. In ACM Transactions on Graphics -Proceedings 
of ACM SIGGRAPH 2009, 1 10. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
		<article_sponsors>
			<funding_agency>863 Program of China</funding_agency>
			<grant_numbers>
				<grant_number>2008AA10Z218</grant_number>
			</grant_numbers>
		</article_sponsors>
		<article_sponsors>
			<funding_agency>NSFC</funding_agency>
			<grant_numbers>
				<grant_number>60703043</grant_number>
			</grant_numbers>
		</article_sponsors>
	</article_rec>
	<article_rec>
		<article_id>2037784</article_id>
		<sort_key>690</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>61</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Hierarchical upsampling for fast image-based depth estimation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037784</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037784</url>
		<abstract>
			<par><![CDATA[<p>While many stereo vision algorithms can quickly and robustly estimate sparse geometry from sets of photos, <i>dense reconstruction</i>, where depth estimate is required at per-pixel or sub-pixel level, remains a time-consuming and memory-intensive process. In this work, we propose a fast hierarchical upsampling method for dense image-based depth estimation. The main idea is to start from sparse depth estimates that can be quickly computed using any existing multiview stereopsis tool, then iteratively upsample the depth values to obtain a dense reconstruction consisting of millions of points. Using a GPU-based implementation, the upsampling algorithm can perform up to 15 images per second. The results can be directly used for 3D modeling applications; in addition, they can be used to digitally manipulate the depth-of-field effects in the input images in order to simulate refocusing.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809398</person_id>
				<author_profile_id><![CDATA[81488644419]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Blake]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Foster]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Massachusetts Amherst]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[blfoster@cs.umass.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809399</person_id>
				<author_profile_id><![CDATA[81408591714]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Rui]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Wang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Massachusetts Amherst]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ruiwang@cs.umass.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1276497</ref_obj_id>
				<ref_obj_pid>1276377</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Kopf, J., Cohen, M. F., Lischinski, D., and Uyttendaele, M. 2007. Joint bilateral upsampling. <i>ACM Trans. Graph. 26</i>, 3.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1141964</ref_obj_id>
				<ref_obj_pid>1141911</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Snavely, N., Seitz, S. M., and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3d. <i>ACM Trans. Graph. 25</i>, 3, 835--846.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Hierarchical Upsampling for Fast Image-Based Depth Estimation Blake Foster and Rui Wang* University 
of Massachusetts Amherst  Figure 1: Reconstruction from 2 input images. The initial point cloud is obtained 
from running Bundler [Snavely et al. 2006], resulting in depth estimates for ~0.1% of the pixels in each 
image within 30 seconds. Our upsampling algorithm then produces a dense reconstruction containing more 
than 1 million points in less than a second. The three images on the right show the dense point cloud 
after reconstruction. 1 Introduction While many stereo vision algorithms can quickly and robustly es­timate 
sparse geometry from sets of photos, dense reconstruction, where depth estimate is required at per-pixel 
or sub-pixel level, re­mains a time-consuming and memory-intensive process. In this work, we propose 
a fast hierarchical upsampling method for dense image-based depth estimation. The main idea is to start 
from sparse depth estimates that can be quickly computed using any existing multiview stereopsis tool, 
then iteratively upsample the depth val­ues to obtain a dense reconstruction consisting of millions of 
points. Using a GPU-based implementation, the upsampling algorithm can perform up to 15 images per second. 
The results can be directly used for 3D modeling applications; in addition, they can be used to digitally 
manipulate the depth-of-.eld effects in the input images in order to simulate refocusing. 2 Our Approach 
 Given a set of high-resolution images, we .rst run multiview stere­opsis on downsampled images (at ~300 
pixels in either dimension) to quickly obtain a sparse point cloud. One possibility is to run the Bundler 
software [Snavely et al. 2006], which computes structure­ from-motion to recover camera parameters, while 
simultaneously outputting a sparse point cloud representing the scene geometry. Next, we combine the 
high-resolution color images with the sparse point cloud to obtain a dense reconstruction. This is achieved 
us­ing an algorithm inspired by joint bilateral upsampling [Kopf et al. 2007]. To begin, we select a 
set of reference views R from the in­put images. For each R, we compute a sparse depth map Ds by projecting 
the sparse 3D points to the view. We then use joint bi­lateral upsampling to estimate a depth value for 
every pixel in R. Speci.cally, the depth value of an unknown pixel p is computed as a linear sum of its 
valid neighbor pixels (i.e. pixels with known depth values), weighted by both the spatial and color similarities. 
The in­tuition is that pixels with similar colors in a local neighborhood are likely to have similar 
depth values. The upsampling equation is: 1 (q)f( p -q ) g( R(p) -Drgb Du(p)= Dss (q) ) (1) kpq.O 
where p is a target pixel, f is the spatial kernel, g is the range kernel, O is the spatial support, 
q is a pixel in O, k1 p is the normalization factor, Du and Ds are the upsampled and the sparse depth 
maps *e-mail: {blfoster, ruiwang}@cs.umass.edu  Figure 2: Illustration of hierarchical upsampling. At 
each level i, the reconstructed depth map at level Di+1 is upsampled under the guidance of image Ri to 
.ll in the missing depth values in Di . respectively, R is the color image, and Dsrgb is the color of 
q stored in the sparse depth map. For high-resolution inputs, the spatial neighbor size O typically has 
to be large. To improve the algorithm ef.ciency, we adopt a hierar­chical approach. Figure 2 illustrates 
the process. First, we generate image pyramids for both the image R and the sparse depth map D. For R 
we downsample k levels using a 2×2 box .lter; and for D we downsample k +1 levels using a min .lter. 
Next, the algorithm goes through k iterations from the bottom level. During each iter­ation, we use Equation 
1 to upsample the depth values from Di+1 to Di, guided by the pixel colors in Ri. For the newly interpolated 
pixels, we use Ri to initialize their colors, which are further stored in the depth map. The algorithm 
then proceeds to the next level. Results We have implemented our algorithm on modern GPUs. On an NVIDIA 
GTX 280, our implementation can process up to 15 images per second at 640 × 480 resolution. Figure 1 
shows an example of reconstruction from 2 input images. The results can be used directly for 3D modeling 
applications. In addition, the dense depth maps recovered can be used to for digital refocusing. Refer 
to the supplemental video for demos of both. References KOPF, J., COHEN, M. F., LISCHINSKI, D., AND 
UYTTENDAELE, M. 2007. Joint bilateral upsampling. ACM Trans. Graph. 26, 3. SNAVELY, N., SEITZ, S. M., 
AND SZELISKI, R. 2006. Photo tourism: exploring photo collections in 3d. ACM Trans. Graph. 25, 3, 835 
846. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, 
August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037785</article_id>
		<sort_key>700</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>62</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Model-based visualization of future forest landscapes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037785</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037785</url>
		<abstract>
			<par><![CDATA[<p>Forest landscapes are the type of landscape that British Columbians would call "home". One of the biggest challenges for future sustainable forest management is climate change: Impacts as well as human mitigation and adaptation actions will have considerable impacts on BC's landscapes. In order to help decision-makers proactively assessing mitigation and adaptation options and their landscape impact, the scenario method is particularly powerful because it allows decision-makers to consider alternative options. However, such scenarios are often abstract and do not allow any assessment of the visual qualities of future landscapes. Visual assessment requires computer visualizations based on geodata and forest models and capable of visualizing vegetation cover with a high numbers of 3D tree models in a level of realism sufficient for visual landscape assessment (Lange 2001; Pettit 2010). Therefore, the poster submission is part of the growing field of landscape visualization established by Zube et al. (1987), Bishop 2001, Ervin (2001), Lange (2001), Sheppard (2001), Appleton and Lovett (2003), Bishop and Lange (2005), and Paar and Clasen (2007).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809400</person_id>
				<author_profile_id><![CDATA[81488670590]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Olaf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schroth]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809401</person_id>
				<author_profile_id><![CDATA[81488669837]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ellen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pond]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809402</person_id>
				<author_profile_id><![CDATA[81488671860]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Philip]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paar]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National University of Singapore]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809403</person_id>
				<author_profile_id><![CDATA[81488669209]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sara]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Muir-Owen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809404</person_id>
				<author_profile_id><![CDATA[81488663503]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Cam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Campbell]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809405</person_id>
				<author_profile_id><![CDATA[81100550872]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Stephen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Sheppard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Model-based Visualization of Future Forest Landscapes Olaf Schrotha), Ellen Pond a), Philip Paar b), 
Sara Muir-Owen a), Cam Campbell a), Stephen Sheppard a) a) University of British Columbia, b)National 
University of Singapore, Dept. of Architecture Forest landscapes are the type of landscape that British 
Columbians would call home . One of the biggest challenges for future sustainable forest management is 
climate change: Impacts as well as human mitigation and adaptation actions will have considerable impacts 
on BC s landscapes. In order to help decision-makers proactively assessing mitigation and adaptation 
options and their landscape impact, the scenario method is particularly powerful because it allows decision-makers 
to consider alternative options. However, such scenarios are often abstract and do not allow any assessment 
of the visual qualities of future landscapes. Visual assessment requires computer visualizations based 
on geodata and forest models and capable of visualizing vegetation cover with a high numbers of 3D tree 
models in a level of realism sufficient for visual landscape assessment (Lange 2001; Pettit 2010). Therefore, 
the poster submission is part of the growing field of landscape visualization established by Zube et 
al. (1987), Bishop 2001, Ervin (2001), Lange (2001), Sheppard (2001), Appleton and Lovett (2003), Bishop 
and Lange (2005), and Paar and Clasen (2007). In order to overcome the lack of appropriate visualizations 
of future landscape options, an innovative workflow building on open data, spatial modeling in GIS software, 
and the novel open-source virtual globe software Biosphere3D as visualization platform was implemented. 
The workflow is distinct from artistic landscape renderings, which are not based on geodata and it is 
also different from the approach of Google Earth deriving vegetation representations from remote sensing 
information. In contrast, the workflow incorporates the provincial BC Vegetation Resource Inventory (VRI) 
with very detailed georeferenced information about forests stands and their composition with regard to 
tree species, age, diameter, density etc. On this data basis, mathematical models were implemented in 
GIS to calculate the future susceptibility of forest stands for mountain pine beetle (ILMB 2006, adapted 
from Shore and Safranyik 1992) and forest fires as climate change related impacts. Possible adaptation 
options were developed in workshop with local stakeholders and include logging and re-planting with more 
resistant species, underplanting with mixed species, and removing over-mature pine (Pond et al. 2009). 
The outcome of mathematical modeling and stakeholder workshops was assembled in ESRI ArcGIS, although 
an open source GIS to have a fully open workflow could replace this. Finally, the modeled vegetation 
information is exported as vegetation plots based on ESRI multipoint Shapefiles for visualization. Biosphere3D 
was chosen as the visualization platform because it is scale-independent and can handle large digital 
elevation models, satellite and aerial images and it provides a sophisticated Level of Detail management 
that is able to render highly realistic 3D vegetation models (Deussen 2003) without pre-calculations 
in the numbers that are required for landscape visualization: The poster visualizations include up to 
57000 trees with about 8 million polygons, rendered on HP 8530w laptop. The visualizations were presented 
as part of a Powerpoint presentation and on posters at a final community open house concluding the local 
stakeholder workshops. An empirical evaluation of the various tools used in the scenario process (Schroth 
et al. 2009) and anecdotal evidence show that the model-based visualizations did achieve their goal of 
enabling decision-makers to assess future landscape impacts of climate change and today s mitigation 
and adaptation response. Ninety percent of the 38 participants of the final workshop rated the overall 
visualization support including the landscape images presented here as very helpful or helpful. Anecdotal 
feedback showed that forest experts assessed the visualization of a mountain-pine beetle outbreak as 
realistic and very close to the real visual landscape impact. The workflow has been documented as a tutorial 
at http://sourceforge.net/apps/mediawiki/biosphere3d. This solution can easily be transferred to other 
landscape-related applications such as landscape preference studies or visual impact assessments. 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037786</article_id>
		<sort_key>710</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>63</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Multiscale feature-preserving smoothing of tomographic data]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037786</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037786</url>
		<abstract>
			<par><![CDATA[<p>Computer tomography (CT) has wide application in medical imaging and reverse engineering. Due to the limited number of projections used in reconstructing the volume, the resulting 3D data is typically noisy. Contouring such data, for surface extraction, yields surfaces with localised artifacts of complex topology. To avoid such artifacts, we propose a method for feature-preserving smoothing of CT data. The smoothing is based on anisotropic diffusion, with a diffusion tensor designed to smooth noise up to a given scale, while preserving features. We compute these diffusion kernels from the directional histograms of gradients around each voxel, using a fast GPU implementation.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[anisotropic diffusion]]></kw>
			<kw><![CDATA[feature-preserving smoothing]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809406</person_id>
				<author_profile_id><![CDATA[81488671864]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Nassim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jibai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809407</person_id>
				<author_profile_id><![CDATA[81100200748]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Cyril]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Soler]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809408</person_id>
				<author_profile_id><![CDATA[81416596616]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kartic]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Subr]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University College London]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809409</person_id>
				<author_profile_id><![CDATA[81100061765]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Nicolas]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Holzschuch]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Frangakis, A. S., and Hegerl, R. 2001. Noise reduction in electron tomographic reconstructions using nonlinear anisotropic diffusion. <i>Journal of Structural Biology 135</i>, 3, 239--250.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1778837</ref_obj_id>
				<ref_obj_pid>1778765</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kass, M., and Solomon, J. 2010. Smoothed local histogram filters. <i>ACM Trans. Graph. 29</i> (July), 100:1--100:10.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schaap, M., Schilham, A., Zuiderveld, K., Prokop, M., Vonken, E.-J., and Niessen, W. 2008. Fast noise reduction in computed tomography for improved 3-d visualization. <i>Medical Imaging, IEEE Transactions on 27</i>, 8, 1120--1129.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Multiscale Feature-Preserving Smoothing of Tomographic Data Nassim Jibai Cyril Soler Kartic Subr Nicolas 
Holzschuch INRIA, Grenoble University University College London INRIA, Grenoble University Abstract Computer 
tomography (CT) has wide application in medical imag­ing and reverse engineering. Due to the limited 
number of projec­tions used in reconstructing the volume, the resulting 3D data is typically noisy. Contouring 
such data, for surface extraction, yields surfaces with localised artifacts of complex topology. To avoid 
such artifacts, we propose a method for feature-preserving smoothing of CT data. The smoothing is based 
on anisotropic diffusion, with a diffusion tensor designed to smooth noise up to a given scale, while 
preserving features. We compute these diffusion kernels from the directional histograms of gradients 
around each voxel, using a fast GPU implementation. Keywords: Feature-preserving smoothing, anisotropic 
diffusion 1 Introduction Computer tomography is an effective way of digitally scanning objects with complex 
and intricate geometry. Example applica­tions include medical imaging and reverse-engineering of closed 
objects in a non-destructive acquisition process. Volumetric data from inverse tomography are prone to 
artifacts due to imperfectly acquired projections. These imperfections have multiple causes: high contrasts; 
lack of captor sensitivity; too few projections; non­monochromaticity of the X-Ray source, imperfect 
stability of the X-Ray source; pixel defects on the captor; non-uniformity of the absorbtion process 
accross wavelengths. A popular approach to smooth such 3D data is to use geometric diffusion. Geometric 
diffusion smoothes orthogonal to the gradient in the volume and therefore preserves planar regions, while 
blurring edges and corners. More general anisotropy has also been proposed where the .atness of the diffusion 
kernel is adapted to the gradient of the volume [Schaap et al. 2008], or the structure tensor [Fran­gakis 
and Hegerl 2001]. In all these cases, the diffusion kernel is radially symmetric (two eigenvalues are 
equal). We propose a new method of computing a diffusion tensor for each voxel based on the distribution 
of gradients in its neighborhood. We build a radially asymmetric diffusion tensor by combining the geometric 
diffusion tensors of the gradients in the neighborhood of each voxel. Two input parameters control the 
extent of smoothing: the size of the neighborhood and the number of diffusion steps. We propose an ef.cient 
method to compute these tensors on GPU. 2 Multi-Scale feature-preserving diffusion We need our diffusion 
kernel to respect the constraints of the indi­vidual geometric-diffusion kernels for each gradient in 
the neigh­borhood of the current voxel. Thus, we use the geometric aver­age of the geometric diffusion 
kernels. To avoid discretization arti­facts, we compute the continuous directional histogram [Kass and 
Solomon 2010] hx of gradients .f(x) of the volumetric data f around each point x in the volume using 
a Von Mises kernel K. The value of the histogram for a given direction is obtained by inte­grating over 
gradients of nearby voxels with a gaussian weight ga of variance a: 11 hx(.)= K(.f(y),.)ga(x-y)dy = K(.f(.),.).ga 
KK V where ais the input feature size, Kis the normalizing constant that accounts for the gaussian ga 
and the Vom Mises kernel K. Thus, Figure 1: Left: A contour surface extracted from noisy tomo­graphic 
data contains surface noise and several topological arti­facts such as small handles and holes. Right: 
The surface extracted from our smoothed volume is clean, and yet small features, such as the thread in 
the screw, and sharp edges have been preserved. we compute h(.) for the entire volume at once along each 
his­togram direction . using only two 3D FFTs. For a direction ., - xMg the geometric diffusion kernel 
is a 3D gaussian e t (.)x where Mg(.) is a 3D symmetric matrix. To build a diffusion kernel that accounts 
for the entire histogram of directions around voxel x we compute the geometric average of gaussian .lters 
weighted by the histogram for each direction. The matrix Mx of this kernel is there­fore the integral 
of matrices Mg for all directions weighted by the gradient histogram in this direction: 1 Mx = hx(.)Mg(.)d. 
H O Because we compute h(.) for all voxels at once, we can numeri­ cally compute Mx for all voxels x 
by sweeping accross a .nite set of directions .i. The diffusion then uses the three eigenvectors vi and 
eigenvalues .i of Mx, and the Hessian H of the volume: .v = .it viHvi .t i=1,2,3 The memory cost of computing 
the diffusion kernel per voxel is that of storing the symmetric matrix Mx plus the normalizing constant 
H, i.e. only 7 .oats per voxel. We can therefore treat volumes up to 5123 voxels in 4Gb of memory. It 
is possible to compute the diffusion kernels block per block as well, with a proper overlap between the 
blocks. We used our method to improve tomographic data (See Fig. 1). Our method is fast, as we are able 
to compute kernels in a 2563 volume in 700 seconds for 642 directions using a 103 feature size (while 
the same calculation needs hours on CPU). Each diffusion step then takes less than 5 sec. Our technique 
can be extended to repair surfaces as well, while eliminating artifacts of arbitrary topology up to a 
given scale, if by .rst converting the mesh into 3D level set data.  References FRANGAKIS, A. S., AND 
HEGERL, R. 2001. Noise reduction in electron tomographic reconstructions using nonlinear anisotropic 
diffusion. Journal of Structural Biology 135, 3, 239 250. KASS, M., AND SOLOMON, J. 2010. Smoothed local 
histogram .lters. ACM Trans. Graph. 29 (July), 100:1 100:10. SCHAAP, M., SCHILHAM, A., ZUIDERVELD, K., 
PROKOP, M., VONKEN, E.-J., AND NIESSEN, W. 2008. Fast noise reduction in computed tomography for im­proved 
3-d visualization. Medical Imaging, IEEE Transactions on 27, 8, 1120 1129. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037787</article_id>
		<sort_key>720</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>64</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Parts identification and motion estimation on CT scanned assembly meshes]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037787</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037787</url>
		<abstract>
			<par><![CDATA[<p>Along with the recent improvements of the industrial X-ray CT scanning systems, it is now possible to non-destructively acquire the entire meshes of mechanical assemblies. This technology has the potential to realize an advanced inspection of assemblies, such as examining assembling errors or dynamic behaviors in motion using the meshes reflecting really-assembled situations. However, to realize such advance inspections, it is required to identify each part and to estimate their motions in the meshes.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809410</person_id>
				<author_profile_id><![CDATA[81488665756]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tomohiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizoguchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809411</person_id>
				<author_profile_id><![CDATA[81488652879]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yoshikazu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kobayashi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809412</person_id>
				<author_profile_id><![CDATA[81488661531]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kenji]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Shirai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nihon University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809413</person_id>
				<author_profile_id><![CDATA[81100181577]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Satoshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanai]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Hokkaido University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>882369</ref_obj_id>
				<ref_obj_pid>882262</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Katz, S., and Tal, A. 2003. Hierarchical mesh decomposition using fuzzy clustering and cuts. <i>ACM Transactions on Graphics</i>, 22, 3, 954--961.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Chang, W., and Zwicker, M. 2009. Range Scan Registration Using Reduced Deformable Models, <i>Computer Graphics Forum</i>, 23, 2, 447--456.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Parts Identification and Motion Estimation on CT Scanned Assembly Meshes Tomohiro Mizoguchi Yoshikazu 
Kobayashi Kenji Shirai Satoshi Kanai Nihon University Nihon University Nihon University Hokkaido University 
 1 Introduction Along with the recent improvements of the industrial X-ray CT scanning systems, it is 
now possible to non-destructively acquire the entire meshes of mechanical assemblies. This technology 
has the potential to realize an advanced inspection of assemblies, such as examining assembling errors 
or dynamic behaviors in motion using the meshes reflecting really-assembled situations. However, to realize 
such advance inspections, it is required to identify each part and to estimate their motions in the meshes. 
Many methods have been proposed for estimating part boundaries and decomposing the models into parts 
by analyzing the concavity, e.g. [Katz and Tal 2003]. However these methods cannot uniquely find the 
correct boundaries between parts from the multiple concavities in the mechanical assemblies. Non-rigid 
registration methods, e.g. [Chang and Zwicker 2009], can identify the rigid parts in the registration 
process, however, they cannot explicitly find the motion parameters such as rotational axes. Moreover 
since these methods rely on the local properties such as principle curvatures or spin images for initial 
correspondence computations, they may fail to robustly indentify the parts for noisy meshes. 2 Our Approach 
In this work, we develop a new method for uniquely identifying each part and correctly estimating the 
motion of the mechanical assemblies on their CT scanned meshes acquired at different configurations. 
Our method is robust for scanning noise and does not depend on spatial positions and orientations of 
the assemblies in the CT scanning process. An overview is as follows. Step 1: Clustering In the first 
step of our method, mesh triangles are clustered based on Euclidean distances. It improves the accuracy, 
stability, and efficiency of the shape matching in the next step. Step 2: Parts identification Next, 
clusters are classified into parts. Each pair of parts in the meshes can be considered as the largest 
pair which can be closely matched under a certain rigid transformation. To identify such pairs of parts, 
we propose a combinatorial method of random sampling, congruency test based on ICP matching, and pairwise 
region growing. In our proposed method, a cluster in each mesh is randomly sampled, neighboring topologically-connected 
clusters within a certain distance are extracted, and then their congruency is tested based on ICP matching. 
If they are congruent, they are simultaneously enlarged using pairwise region growing where an ICP matching 
and a search of matched cluster-pairs are iterated. As a result of this process, the largest set of clusters 
which can be closely matched under the user specified tolerance are extracted. By repeating the above 
process, the maximally enlarged set of clusters are extracted and identified as a pair of parts. For 
identifying multiple parts, a series of above processes are repeated until the number of non-identified 
clusters are small enough. Step 3: Motion estimation Next, motion parameters such as rotational axes 
are estimated by evaluating the relative positions and orientations of the identified parts. First a 
mesh is transformed to the other so that a pair of corresponding parts are matched (a blue pair in the 
example in Figure 1). Then motion parameters are estimated so that the pair of connecting parts to the 
matched ones (green pair in the example) can be closely matched under the motion (a rotation in the example). 
 3 Results and Future Work An experimental result is shown in Figure 1. We scanned the bike pedal in 
Figure 1(a) at different joint angles and created the pair of meshes both including about 1.4 million 
triangles. The CT resolution is 0.3mm. Clusters are shown in Figure 1(b), identified parts are in Figure 
1(c), and estimated motion is in Figure 1(d). We visually confirmed that the appropriate parts boundaries 
are identified and the correct motion parameters are estimated. We also verified that our method enabled 
the identification and the estimation even on the same meshes including the artificial noise generated 
by displacing each vertex along its normal direction by a Gaussian distributed random distance with the 
standard deviation 200% proportional to the averaged mesh edge length. In future work, we optimize the 
parts boundaries and the motion parameters by local shape matching based on mesh triangles. We also extend 
this work for realizing kinematic simulation by classifying the types of assemblies. References Katz, 
S., and Tal, A. 2003. Hierarchical mesh decomposition using fuzzy clustering and cuts. ACM Transactions 
on Graphics, 22, 3, 954-961. Chang, W., and Zwicker, M. 2009. Range Scan Registration Using Reduced Deformable 
Models, Computer Graphics Forum, 23, 2, 447-456. Copyright is held by the author / owner(s). SIGGRAPH 
2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037788</article_id>
		<sort_key>730</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>65</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>11</seq_no>
		<title><![CDATA[Real-time terrain modeling using CPU]]></title>
		<subtitle><![CDATA[GPU coupled computation]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037788</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037788</url>
		<abstract>
			<par><![CDATA[<p>In many editing tools, especially sketch-based modeling, it is important to have real-time feedback to help improve the editing quality. This importance is emphasized particularly in sketch-based terrain modeling, being able to see the terrain morphing at the same time the drawing edition occurs constitutes a great user experience (see Figure 1(a)). In this work, we propose a real-time terrain modeling tool by combining a fast GPU-based terrain solver [Hnaidi et al. 2010] with a lightweight CPU-based data structure. Our tool is capable of dynamically generate multi-resolution heightmaps, enabling it to tessellate different parts of the terrain at different resolutions.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809414</person_id>
				<author_profile_id><![CDATA[81381604288]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Adrien]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bernhardt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble Univ., France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809415</person_id>
				<author_profile_id><![CDATA[81421597426]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Andr&#233;]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Maximo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809416</person_id>
				<author_profile_id><![CDATA[81100202267]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Luiz]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Velho]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[IMPA, Brazil]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809417</person_id>
				<author_profile_id><![CDATA[81488670368]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Houssam]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hnaidi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[LIRIS, CNRS, Univ. Lyon, France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809418</person_id>
				<author_profile_id><![CDATA[81407594555]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Marie-Paule]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cani]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[INRIA, Grenoble Univ., France]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1507155</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Gain, J., Marais, P., and Stra&#223;er, W. 2009. Terrain Sketching. In <i>Proceedings of the Symposium on Interactive 3D Graphics and Games</i>, ACM, New York, NY, USA, I3D '09, 31--38.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hnaidi, H., Gu&#233;rin, E., Akkouche, S., Peytavie, A., and Galin, E. 2010. Feature based terrain generation using diffusion equation. <i>Computer Graphics Forum 29</i>, 7 (September).]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-time Terrain Modeling using CPU GPU Coupled Computation Adrien Bernhardt1 Andr´e Maximo2 Luiz Velho2 
Houssam Hnaidi3 Marie-Paule Cani1 1 23 INRIA, Grenoble Univ., France IMPA, Brazil LIRIS, CNRS, Univ. 
Lyon 1, France (a) (b) (c) Figure 1: Terrain modeling example: as each stroke is drawn (a) or manipulated 
(c), the terrain is tessellated in the GPU to follow the stroke. In the CPU, the quadtree data structure 
(b) controls the quad patches sent to the GPU. Introduction and Related Work In many editing tools, espe­cially 
sketch-based modeling, it is important to have real-time feed­back to help improve the editing quality. 
This importance is empha­sized particularly in sketch-based terrain modeling, being able to see the terrain 
morphing at the same time the drawing edition occurs constitutes a great user experience (see Figure 
1(a)). In this work, we propose a real-time terrain modeling tool by combining a fast GPU-based terrain 
solver [Hnaidi et al. 2010] with a lightweight CPU-based data structure. Our tool is capable of dynamically 
gen­erate multi-resolution heightmaps, enabling it to tessellate different parts of the terrain at different 
resolutions. In our framework, we have two types of editing interactions: in the spirit of [Gain et al. 
2009], the user can draw strokes creating eleva­tions and crevices; and previous strokes can be interactively 
moved to different regions of the terrain. Differently than Gain et al. s sys­tem, we do not extract 
noise from the user strokes to make the ter­rain more realistic, however we use a CPU GPU coupled method 
to drastically improve the performance of our tool, generating terrains two orders of magnitude faster 
than Gain et al. s work. The terrain modeling in our approach is accomplished by combin­ing the multi-grid 
GPU terrain solver of [Hnaidi et al. 2010] with an adaptive tessellation-based rendering shader capable 
of handling dynamic heightmaps. The main contribution of Hnaidi et al. s work is to propose a GPU-based 
multi-grid diffusion equation solver, which interpolates not only heights but also amplitude and fre­quency 
noise. Our modeling tool uses Hnaidi et al. s solver to allow an interactive manipulation of complex 
terrain primitives. Real-time Terrain Modeling Creation of terrain models in real­time involves dealing 
with dynamically changing data that in­creases exponentially depending on the terrain resolution. In 
order to provide a real-time terrain modeling tool, we make use of two complementary approaches. First, 
a coarse version of the terrain is maintained in the CPU using a quadtree (see Figure 1(b)), where regions 
closer to the viewer are subdivided more than far regions. This simple and lightweight data structure 
.ts the CPU main role of data control, while allowing it to send adaptive quad primitives to the GPU. 
Second, a .ne version of the terrain is produced in the GPU using the tessellation control and evaluation 
shaders. The tes­sellation control shader is responsible to subdivide regularly each patch primitive, 
i.e. the quad leaf node sent by the CPU, while the tessellation evaluation shader reads the height values 
from a texture. The .rst step of our algorithm is to update the quadtree data struc­ture using a LOD-based 
approach. We consider the projection of the bounding box of each quad node by reading the minimum and 
maximum height value that falls inside the node. The projection is used to determine if the patch node 
needs to be sent to the GPU and to interactively adapt the quadtree in a way that each projection shape 
has about the same size. The second step of our algorithm is to translate the sketch-de.ned terrain primitives 
to constraints that are used by the multi-grid GPU solver of [Hnaidi et al. 2010]. This solver provides 
a sequence of in­creasing resolution grids, up to an arbitrary size, which we store in a mipmap-pyramid 
texture to be read in our tessellation control and evaluation shaders. The tessellation control shader 
uses it to decide the subdivision level of each patch, while the tessellation evaluation shader uses 
it to place each generated vertex at the proper height value. The multi-resolution texture is also used 
by the CPU, but only a small resolution (6th mipmap level) of it since the quadtree minimum leaf size 
is much bigger than the texel from the highest resolution texture. In our early experiments we discovered 
that by modifying several aspects of the original GPU solver, we are able to compute the en­tire multi-resolution 
heightmap texture (with 4K × 4K maximum size) in 80 ms using an off-the-shelf graphics card. Moreover, 
we use the CPU to control the solver iterations and stop at a certain res­olution and then resume computing 
when the GPU is idle. Another interesting feature of our method is the balance between terrain gen­eration 
in the CPU and in the GPU, we can control this balance by simply changing the quadtree re.nement. With 
these features, the user can draw strokes and see at the same time the terrain morphing to the drawing. 
Terrain primitives, such as cliffs and mountains, are controlled seamlessly in our framework. In short, 
we believe this work has the potential to become an effective terrain modeling tool, creating high-quality 
terrain models in real-time. References GAIN, J., MARAIS, P., AND STRAßER, W. 2009. Terrain Sketch­ing. 
In Proceedings of the Symposium on Interactive 3D Graph­ics and Games, ACM, New York, NY, USA, I3D 09, 
31 38. HNAIDI, H., GU´ ERIN, E., AKKOUCHE, S., PEYTAVIE, A., AND GALIN, E. 2010. Feature based terrain 
generation using diffu­sion equation. Computer Graphics Forum 29, 7 (September). Copyright is held by 
the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037789</article_id>
		<sort_key>740</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>66</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>12</seq_no>
		<title><![CDATA[Refractive index dependent bidirectional scattering distribution functions]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037789</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037789</url>
		<abstract>
			<par><![CDATA[<p>We investigate the effect of immersing real-world materials into media of different refractive indices. We show, only some materials follow the Fresnel-governed behaviour. In reality, many materials exhibit unexpected effects such as stronger localized highlights or a significant increase in the glossy reflection due to microgeometry. We propose a parametric model that, however, takes their Fresnel-governed behaviour into account.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[data acquisition]]></kw>
			<kw><![CDATA[image-based rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809419</person_id>
				<author_profile_id><![CDATA[81385596095]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Kai]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Berger]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[berger@cg.cs.tu-bs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809420</person_id>
				<author_profile_id><![CDATA[81488672405]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ilya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Reshetouski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MMCI, Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iresheto@mmci.uni-saarland.de]]></email_address>
			</au>
			<au>
				<person_id>P2809421</person_id>
				<author_profile_id><![CDATA[81100477906]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Marcus]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Magnor]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[TU Braunschweig]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[magnor@cg.cs.tu-bs.de]]></email_address>
			</au>
			<au>
				<person_id>P2809422</person_id>
				<author_profile_id><![CDATA[81331495034]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ivo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ihrke]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[MMCI, Saarbr&#252;cken]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ihrke@mpi-sb.mpg.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Madsen, K., Nielsen, K., and Tingleff, O., 2004. Methods for nonlinear least squares problems.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>975275</ref_obj_id>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Pharr, M., and Humphreys, G. 2004. <i>Physically based rendering: From theory to implementation</i>. Morgan Kaufmann.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Refractive Index Dependent Bidirectional Scattering Distribution Functions KaiBerger * IlyaReshetouski 
MarcusMagnor IvoIhrke§ TUBraunschweig MMCI,Saarbr¨TUBraunschweig ucken ucken MMCI,Saarbr¨ Figure 1: 
We investigate on the re.ectance of materials immersed into media of different refractive indices. We 
therefore use a capturing setup(left image), toplacepatches intodifferentsaltsolutionsrepresenting differentrefractive 
indices. Asa laserrotatesaround thepatch, we takemultiplepicturesof thescreenattached tocylinder. In 
theevaluationstep,we .tanewphysics-basedBRDF model withavariable for thesampled refractive indices to 
thedata(middle left image). Adepictionof thecompletematerial set isshown in(middleright image). Usingthe.ttedparameterswecangeneratesyntheticscenesshowcasing 
surfacesexposed towater(rightimage). Abstract We investigate the effect of immersing real-world materials 
into mediaofdifferent refractive indices.Weshow,onlysomematerials follow the Fresnel-governed behaviour. 
In reality, many materials exhibit unexpected effects such as stronger localized highlights or asigni.cantincreaseintheglossy 
re.ectiondue tomicrogeometry. We propose a parametric model that, however, takes their Fresnel­governedbehaviour 
intoaccount. Keywords: image-based rendering,data acquisition 1 Motivation A familiar effect in everyday 
life is that objects change their ap­pearance when immersed in water or other substances with refrac­tive 
indices different from air. This refractive index dependence of bidirectional re.ectancedistributionfunctions(BRDFs) 
has sofar beenmostlyignored.Implicitlyitisassumedtobegovernedbythe Fresnelequationsvia therefractive 
indexdependenceoftheFresnel re.ection and transmission factors. We analyze the dependence of current 
physics-based BRDF models on the refractive index of the immersing medium.Inparticular,weshow thattheFresnel 
termis thegoverningfactor in thosemodels. 2 Our approach Capturing setup Oursetup isshown inFig. 1(left 
image). Each material sample is immersed in a medium with a refractive index different from air. The 
cylinder contains the medium and the sam­ple. A laser, mounted on a rotation stage, illuminates the sample 
fromde.nedangles.Note, that the laserhits theglasscylinderwall orthogonally for all acquisition angles. 
This eliminates refraction upon entryinto the medium, which would occur otherwise. The re­.ected light 
hits a screen which is attached directly to the cylinder wallandis then imagedby aCCD camera.Thesystem 
iscalibrated using.ducidal markers.Thepotentiallyspatiallyvarying transmis­sionpropertiesofthescreenarecalibrated 
using aSpectralonpatch. * e-mail: berger@cg.cs.tu-bs.de e-mail: iresheto@mmci.uni-saarland.de e-mail: 
magnor@cg.cs.tu-bs.de §e-mail: ihrke@mpi-sb.mpg.de This material exhibits almost perfect Lambertian 
re.ectance and a high albedo of 99%. Preprocessing For each surrounding medium(with varying refrac­tive 
index) each incident angle is imaged with different exposure 1 1 1 1111 times ( s, s, s, s, s, s, s). 
The images are 4000 1000 250 250 60 25 4 combinedtoformaHDR-image.Thecylindergeometryisapproxi­mated 
usingRANSAC andtheHDR-imagesarebackprojected onto the surface of the cylinder. The resulting cylinder 
surface textures aredownsampled to 249 × 180pxand used as inputfor theBRDF-Fitting. BRDF Fitting WemodeltheFresnel-governedbehaviorof 
thesur­face patches with the following refractive index-dependent BRDF model,whichissimilar totheCook-Torrancemodel: 
D(.i ; .o ) · G(.i ; .o ) fr (.i ; .o ; ni ; nt )= .d +.s ·Fr (ni ; nt )· , (4 · .i · .o ) (1) where 
D(.i ; .o ) is the microfacet distribution and G(.i ; .o ) is a geometric term(for.i the incident angle 
and .o the exitant angle). Fr (ni ; nt ) denotes the Fresnel term with refractive indices for the surrounding 
mediumandthematerial itself.We.tfortheparame­ters .d , .s , ni (index of the surroundingmedium), nt (index 
of the immersed material),Fig. 1(middle left).TheFittingisbasedon the Levenberg-Marquardtnonlinear optimization[Madsen 
et al.2004]. Results We captured the following classes of materials: Acrylic paint, aluminum, bamboo, 
ceramics, cloth, oil paint, plastic, sand­paper, stone, Te.on and wood. We found that bamboo, cloth, 
plastic, sandpaper and ceramics show the Fresnel-governed be­havior and thus could be .tted with our 
proposed model. We compared the .tting results to the captured data, using the PBRT renderer[PharrandHumphreys2004]. 
A typical resultfor cloth spilled with water of refractive index 1.33 isshown inFig. 1(right). Detailed 
resultscanbefound in thesupplementary material.  References MADSEN, K., NIELSEN,K., AND TINGLEFF,O.,2004. 
Methods fornonlinear least squaresproblems. PHARR, M., AND HUMPHREYS, G. 2004. Physically based ren­dering: 
From theory to implementation. MorganKaufmann. Copyright is held by the author / owner(s). SIGGRAPH 2011, 
Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037790</article_id>
		<sort_key>750</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>67</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>13</seq_no>
		<title><![CDATA[Self-organized criticality as a method of procedural modeling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037790</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037790</url>
		<abstract>
			<par><![CDATA[<p>Self-organized criticality (SOC) is a theory of fractal dynamics in which a physical system approaches an attractor state that is scalefree. In the simple example of a sandpile, the grains of sand can slip down steep slopes and the resulting changes to the system propagate via avalanches. By the time the sandpile reaches the attractor state, the avalanches have propagated throughout the entire system and the features of the resulting landscape are not tied to a specific length scale.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[erosion]]></kw>
			<kw><![CDATA[procedural modeling]]></kw>
			<kw><![CDATA[self-organization]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>I.3.5</cat_node>
				<descriptor>Physically based modeling</descriptor>
				<type>S</type>
			</primary_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002950.10003741.10003742.10003745</concept_id>
				<concept_desc>CCS->Mathematics of computing->Continuous mathematics->Topology->Geometric topology</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010352.10010379</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Animation->Physical simulation</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10010147.10010371.10010396</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics->Shape modeling</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809423</person_id>
				<author_profile_id><![CDATA[81488671814]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Alex]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Pytel]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[apytel@cs.uwaterloo.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ahlenius, H. 2007. <i>Arctic Sea Ice Minimum Extent in September 1982 and 2008</i>. UNEP/GRID-Arendal. http://maps.grida.no/go/graphic/.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Duran, J. 2000. <i>Sands, Powders, and Grains: An Introduction to the Physics of Granular Materials</i>. Partially Ordered Systems. Springer.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Maslanik, J., and Fowler, C. 2009. <i>Arctic Sea Ice Age</i>. University of Colorado.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Rekacewicz, P. 2005. <i>Arctic, Topography and Bathymetry</i>. UNEP/GRID-Arendal. http://maps.grida.no/go/graphic/arctic-topography-and-bathymetry.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Rodr&#237;guez-Iturbe, I., and Rinaldo, A. 1997. <i>Fractal River Basins</i>. Cambridge University Press.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Sapoval, B., Baldassarri, A., and Gabrielli, A. 2004. Self-Stabilized Fractality of Seacoasts through Damped Erosion. <i>Physical Review Letters 93</i>, 9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Self-Organized Criticality as a Method of Procedural Modeling Alex Pytel* David R. Cheriton School of 
Computer Science University of Waterloo  Figure 1: SOC coastline evolution. CR Categories: I.3.5 [Computer 
Graphics]: Computational Ge­ometry and Object Modeling Physically based modeling; Keywords: procedural 
modeling, self-organization, erosion 1 Introduction Self-organized criticality (SOC) is a theory of fractal 
dynamics in which a physical system approaches an attractor state that is scale­free. In the simple example 
of a sandpile, the grains of sand can slip down steep slopes and the resulting changes to the system 
propa­gate via avalanches. By the time the sandpile reaches the attractor state, the avalanches have 
propagated throughout the entire system and the features of the resulting landscape are not tied to a 
speci.c length scale. As long as a process of erosion causes transport of material to occur within a 
physical system, it will exhibit SOC properties to some ex­tent. The physical systems that have been 
previously studied from this point of view include sandpiles [Duran 2000], coasts [Sapoval et al. 2004], 
and river basins [Rodr´iguez-Iturbe and Rinaldo 1997]. Noting how broadly the theory of SOC applies, 
I suggest that the elements of SOC system evolution can be used as a method for procedural creation of 
geometry. Speci.cally, SOC avalanches are geometric in nature and can be applied more freely than SOC, 
as a physical theory. My poster demonstrates that in order to use avalanches effectively as a modeling 
paradigm a certain notion of scale has to be re­introduced into them. As an illustration, I show that 
the extended SOC-based procedural modeling method can simulate the melting of the Earth s polar ice cap 
in a plausible way. Figure 2: Coastline length within a search radius. 2 Local and Global Shape The 
avalanches that dominate SOC coastline evolution (Figure 1) can be modi.ed to respond to local parameters, 
distinguishing local *email: apytel@cs.uwaterloo.ca and global shape. I re-introduce a notion of scale 
into the behaviour of the avalanches using a local formulation of coastline length, il­lustrated in Figure 
2. 3 Polar Ice Cap To simulate the melting of the polar ice cap, I also rely on local parameters to 
control avalanching. These parameters are: land dis­tribution [Rekacewicz 2005], ice distribution [Ahlenius 
2007], ap­ proximate ice thickness [Maslanik and Fowler 2009], and latitude. Figure 3 shows a result 
of the simulation following some erosion. Figure 3: Simulation of ice cap melting. References AHLENIUS, 
H. 2007. Arctic Sea Ice Minimum Extent in Septem­ber 1982 and 2008. UNEP/GRID-Arendal. http://maps. grida.no/go/graphic/. 
DURAN, J. 2000. Sands, Powders, and Grains: An Introduction to the Physics of Granular Materials. Partially 
Ordered Systems. Springer. MASLANIK, J., AND FOWLER, C. 2009. Arctic Sea Ice Age. Uni­versity of Colorado. 
REKACEWICZ, P. 2005. Arctic, Topography and Bathymetry. UNEP/GRID-Arendal. http://maps.grida.no/go/ graphic/arctic-topography-and-bathymetry. 
RODR´IGUEZ-ITURBE, I., AND RINALDO, A. 1997. Fractal River Basins. Cambridge University Press. SAPOVAL, 
B., BALDASSARRI, A., AND GABRIELLI, A. 2004. Self-Stabilized Fractality of Seacoasts through Damped Erosion. 
Physical Review Letters 93, 9. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037791</section_id>
		<sort_key>760</sort_key>
		<section_seq_no>9</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Rendering]]></section_title>
		<section_page_from>9</section_page_from>
	<article_rec>
		<article_id>2037792</article_id>
		<sort_key>770</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>68</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[A screen-space approach to rendering polylines on terrain]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037792</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037792</url>
		<abstract>
			<par><![CDATA[<p>A fundamental task in virtual globe and GIS applications is to render polyline vector data representing elements such as political borders, road networks, and rivers. Rendering polylines such that they precisely drape over terrain is challenging. We present an approach that extrudes the original polyline, forming a wall, and intersects the wall with the terrain in screen-space to find the desired polyline.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809424</person_id>
				<author_profile_id><![CDATA[81488670640]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Deron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ohlarik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Analytical Graphics, Inc.]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[deron@agi.com]]></email_address>
			</au>
			<au>
				<person_id>P2809425</person_id>
				<author_profile_id><![CDATA[81466641240]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cozzi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Pennsylvania]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pjcozzi@siggraph.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Schneider, M., and Klein, R. 2007. Efficient and accurate rendering of vector data on virtual landscapes. <i>Journal of WSCG 15</i>, 1--3 (January), 59--64.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>769946</ref_obj_id>
				<ref_obj_pid>769922</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Wartell, Z., Kang, E., Wasilewski, T., Ribarsky, W., and Faust, N. 2003. Rendering vector data over global, multi-resolution 3D terrain. In <i>Proceedings of the symposium on data visualisation 2003</i>, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, VISSYM '03, 213--222.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 A Screen-Space Approach to Rendering Polylines on Terrain Deron Ohlarik* Patrick Cozzi Analytical Graphics, 
Inc. Analytical Graphics, Inc. University of Pennsylvania (a) (b) (c) Figure 1: (a) Polyline extruded 
into a triangle-based wall. (b) Wall intersection with terrain. (c) Final polyline on terrain. Abstract 
A fundamental task in virtual globe and GIS applications is to ren­der polyline vector data representing 
elements such as political bor­ders, road networks, and rivers. Rendering polylines such that they precisely 
drape over terrain is challenging. We present an approach that extrudes the original polyline, forming 
a wall, and intersects the wall with the terrain in screen-space to .nd the desired polyline. 1 Previous 
Work A common approach to rendering polylines on terrain is to ren­der polylines to a texture, then render 
terrain using multitexturing. Creating the texture as a preprocessing step limits the texture res­olution, 
leading to aliasing as the viewer zooms in; generating the texture on-demand is expensive and does not 
prevent aliasing on steep slopes. Rendering a polyline on terrain by subsampling points along the polyline 
(Figures 2a and 2b) requires adjustment in response to ter­rain LOD switches (Figure 2c) and the coplanar 
geometry can lead to z-.ghting [Wartell et al. 2003]. Rendering a polyline by extrud­ing a shadow volume 
(Figures 2d-2f) leads to smearing on steep slopes and dashing when viewed lengthwise (Figure 3a) [Schneider 
and Klein 2007]. (a) (b) (c) (d) (e) (f)   Figure 2: (a)-(c): Subsampling a polyline. (d)-(f): Rendering 
a polyline with shadow volumes. 2 Our Approach Our approach requires no preprocessing, is decoupled 
from terrain LOD, produces high visual quality constant pixel-width polylines *e-mail: deron@agi.com 
e-mail: pjcozzi@siggraph.org (a) Shadow volumes. (b) Our method. Figure 3: Dashing artifacts. with 
no smearing or cracking, has virtually no CPU overhead, al­lows for dynamic terrain and polylines, and 
is simpler to implement than on-demand texture-based and subsampling methods. First, each point in the 
polyline is duplicated. One is moved above the terrain s surface along the globe s surface normal and 
the other below, forming a vertical wall (Figure 1a). The intersection of this wall with the terrain 
de.nes the desired polyline (Figure 1b). The intersection is found in screen-space using a fragment shader 
that has access to the terrain s depth and silhouette textures. If a fragment is in front of the terrain 
and one of the surrounding fragments from the wall is behind terrain, a potential intersection is found. 
To avoid false positives, fragments on the terrain s silhouette are detected using the silhouette texture 
and discarded (Figure 4). Our method doesn t generate fragments when the wall is viewed on-edge; a shadow 
volume is rendered to handle these cases. (a) Without. (b) With. Figure 4: Silhouette test.  References 
SCHNEIDER, M., AND KLEIN, R. 2007. Ef.cient and accurate rendering of vector data on virtual landscapes. 
Journal of WSCG 15, 1-3 (January), 59 64. WARTELL, Z., KANG, E., WASILEWSKI, T., RIBARSKY, W., AND FAUST, 
N. 2003. Rendering vector data over global, multi-resolution 3D terrain. In Proceedings of the symposium 
on data visualisation 2003, Eurographics Association, Aire-la-Ville, Switzerland, Switzerland, VISSYM 
03, 213 222. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037793</article_id>
		<sort_key>780</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>69</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Dual sphere-unfolding method for single pass omni-directional shadow mapping]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037793</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037793</url>
		<abstract>
			<par><![CDATA[<p>Shadow Mapping is a reliable technique to produce shadows in a scene in real time. This technique has been mostly applied to directional lights and only a few methods have used it for omnidirectional lighting [Brabec et al. 2002]. These methods need more than one full render pass to compute the whole shadow mapping. In this work we propose an approach to achieves an omnidirectional shadow map in a single pass.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809426</person_id>
				<author_profile_id><![CDATA[81488649869]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Marcel]]></first_name>
				<middle_name><![CDATA[Stockli]]></middle_name>
				<last_name><![CDATA[Contreras]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Guanajuato/CIMAT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[marcel@cimat.mx]]></email_address>
			</au>
			<au>
				<person_id>P2809427</person_id>
				<author_profile_id><![CDATA[81488669380]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Alberto]]></first_name>
				<middle_name><![CDATA[Jos&#233; Ram&#237;rez]]></middle_name>
				<last_name><![CDATA[Valadez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Guanajuato/CIMAT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[alberto@cimat.mx]]></email_address>
			</au>
			<au>
				<person_id>P2809428</person_id>
				<author_profile_id><![CDATA[81488656006]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Alejandro]]></first_name>
				<middle_name><![CDATA[Jim&#233;nez]]></middle_name>
				<last_name><![CDATA[Mart&#237;nez]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universidad de Guanajuato/CIMAT]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[alexmar@cimat.mx]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Brabec, S., Annen, T., and Seidel. 2002. Shadow mapping for hemispherical and omnidirectional light sources. <i>In Computer Graphics International</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Dual Sphere-Unfolding Method for Single Pass Omni-directional Shadow Mapping. Marcel Stockli Contreras, 
Alberto Jos ´irezValadez, Alejandro Jim´inez * e Ram´ enez Mart´ Figure 1: a) Render of the scene, b) 
1024x1024 Shadow map, c)1024x1024 Edge map, d) Sphere-Unfolding visualization. 1 Introduction Shadow 
Mapping is a reliable technique to produce shadows in a scene in real time. This technique has been mostly 
applied to directional lights and only a few methods have used it for omni­directional lighting [Brabec 
et al. 2002]. These methods need more than one full render pass to compute the whole shadowmapping. In 
this work we propose an approach to achieves an omni-directional shadow map in a single pass. Our method 
has three main advantages over previous related work: It is computationally ef.cient, since it provides 
onmidirec­tional shadow mapping at a similar speed as directional shadow mapping algorithms.  The depth 
data of the of the projected triangles does not change with the position of the triangle, like in the 
Dual-Paraboloid1 method.  The projection achievethe storage of the data in just one draw­call per triangle, 
instead of two drawcalls performed by the Cube Map and Dual-Paraboloid methods2.  Note that the Dual-Paraboloid 
can also be obtained in one pass, howeverthissinglepassvariation cause distortionsinthedepthdata near 
the boundaries of the paraboloids. 2 Our Work Our method projects the input geometry onto spherical 
triangles (triangles on the sphere), where the sphere is centered at the light source. We proceed by 
choosing two opposite poles of the sphere y + , y -, and unfold each arc connecting the poles in such 
a way that the unfolded arcs are connected by only y - (Fig. 1.d). We call this process sphere-unfolding 
and is de.ned by the projection ( : R3 -R2 givenby Eq. 1. Finally, we linearly render the depth value 
of the triangles in the depth texture. *GameCoder Research/Universidad de Guanajuato/CIMAT, e-mails: 
{alberto,marcel,alexmar}@cimat.mx 1Wich expands the triangles near the boundary of the paraboloid, losing 
precision. 2It is necesary two drawcalls for those triangles that intersect the edges of the cube or 
the boundary of the paraboloid, for Cube Map and Dual­paraboloid method, respectively. Note that there 
does notexista linear homeomorphism between the plane and the sphere. arcsin(y)+ 10.5 ((x, y, z)= 8 (x, 
z) (1) 2 + z2 1x There are basically two issues with this projection: 1. The spherical triangles that 
contain the y + pole, are inverted in the rasterization. 2. The projection is not linear, meaning that 
the preimage of the shadow mapping of each triangle is a curved triangle.  To solve the .rst issue, 
we split the depth texture in such a way that half of the texture contains the unfolded sphere with respect 
to the y + pole, and the other half contains the unfolded sphere with respect to y - pole3. We call this 
process dual sphere-unfolding. Then we render each spherical triangle onto one half of the texture according 
to their nearest pole. For the second issue, note that the triangle deformation is neglible for reasonable-sized 
triangles. For the aliasing in the shadow outline we introduce an antialiasing method that needs2passess: 
For each pixel Pi,j in the Shadow Map let e be the discontinuity threshold Let AR = {|i - k| : |i - k|: 
R, |Pi,j - Pk,j |. e} min(AR) if AR ==. Let Xi,j = = otherwise Let Zi,j = min|h-j|<R{|j - h| + Xi,h} 
This stores in Zi,j the nearest discontinuity, in Manhattan distance, to eachpixelinaregionof RxR. Using 
this information, it is posible to dim the illumination of the pixels that are relatively near to a shadowed 
pixel. This method is equal, in speed performance, to a separablegaussian .lterof size RxR. The projection 
also can be used to obtain environment maps. References BRABEC, S., ANNEN, T., AND SEIDEL. 2002. Shadow 
mapping for hemispherical and omnidirectional light sources. In Com­puter Graphics International. 3Tokeepthe 
resolutionofthedepthtexturewecanexpandthewidthof the depth texture by 2. Copyright is held by the author 
/ owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037794</article_id>
		<sort_key>790</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>70</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Embroidery modeling and rendering in real time]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037794</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037794</url>
		<abstract>
			<par><![CDATA[<p>Many non-photorealistic rendering algorithms have been developed recently that simulate different traditional artistic styles including painting, mosaics, and stippling. However, there is at least onemore traditional approach to rendering images non-photorealistically: embroidery. Embroidery creates images by stitching threads of different colours into a base cloth. In free hand embroidery, many different stitch styles are possible and stitches can be placed relatively freely on the surface.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809429</person_id>
				<author_profile_id><![CDATA[81488659112]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Xinling]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Chen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[cherrychen0602@gmail.com]]></email_address>
			</au>
			<au>
				<person_id>P2809430</person_id>
				<author_profile_id><![CDATA[81100112613]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Michael]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[McCool]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Intel/University of Waterloo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[michael.mccool@intel.com]]></email_address>
			</au>
			<au>
				<person_id>P2809431</person_id>
				<author_profile_id><![CDATA[81100060932]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Asanobu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kitamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[National Institute of Informatics, Japan]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kitamoto@nii.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383327</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hausner, A. 2001. Simulating decorative mosaics. In <i>Proceedings of the 28th annual conference on Computer graphics and interactive techniques</i>, SIGGRAPH '01, 573--580.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hiller, S., Hellwig, H., and Deussen, O. 2003. Beyond stippling -- methods for distributing objects on the plane. <i>Eurographics 22</i>, 515--522.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kurenai-Kai. 2005. <i>Traditional Japanese Patterns 1</i>. Seigensha.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Lengyel, E. 2001. <i>Computing Tangent Space Basis Vectors for an Arbitrary Mesh</i>. Terathon Software 3D Graphics Library, http://www.terathon.com/code/tangent.html.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Mallo, O., Peikert, R., Sigg, C., and Sadlo, F. 2005. Illuminated lines revisited. In <i>IEEE Visualization Conference</i>, 19--26.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>508537</ref_obj_id>
				<ref_obj_pid>508530</ref_obj_pid>
				<ref_seq_no>6</ref_seq_no>
				<ref_text><![CDATA[Secord, A. 2002. Weighted Voronoi stippling. In <i>Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering</i>, ACM, NPAR '02, 37--43.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Embroidery Modeling and Rendering in Real Time Xinling Chen* Michael McCool Asanobu Kitamoto University 
of Waterloo Intel/University of Waterloo National Institute of Informatics, Japan Figure 1: The extraction 
phase analyses a line drawing to determine regions and lines. The modeling phase places stitches according 
to traditional embroidery techniques. The rendering phase dynamically generates a lighting-dependent 
and antialiased texture which is mapped onto a potentially deforming 3D object. 1 Introduction Many non-photorealistic 
rendering algorithms have been developed recently that simulate different traditional artistic styles 
including painting, mosaics, and stippling. However, there is at least one more traditional approach 
to rendering images non-photorealistically: embroidery. Embroidery creates images by stitching threads 
of dif­ferent colours into a base cloth. In free hand embroidery, many dif­ferent stitch styles are possible 
and stitches can be placed relatively freely on the surface. There is no previous work dealing speci.cally 
with stitch place­ment to simulate embroidery, but there are some applicable tech­niques. For example, 
based on Lloyd s Method, a common iter­ative technique to place samples by relaxation, Hausner [Haus­ner 
2001] developed an algorithm to distribute small square tiles to form synthetic mosaics. The stippling 
algorithm presented by Secord successfully produces attractive results in a natural way by using a weighted 
variant of Lloyd s method [Secord 2002]. Embroidery also has a set of traditional patterns. As a reference, 
we have been studying the book Traditional Japanese Patterns [Kurenai-Kai 2005] which de.nes a set of 
sewing techniques used in Japanese free embroidery. The goal of our work is to transform a traditional 
embroidery pat­tern (typically consisting mainly of region outlines) into a high­quality embroidery image 
generated in real time. Real time render­ing enables the user to navigate around the image and manipulate 
it. We also want to be able to place the embroidery on deforming objects and use dynamic lighting models 
for the threads. 2 Our Approach We achieved embroidery simulation and rendering in three phases. In 
the .rst phase, vision algorithms were applied to extract lines and regions in embroidery patterns scanned 
from traditional sources. In the second phase, the selection and placement of primitives in the identi.ed 
regions is performed. This is a semi-automatic process since some artistic choice is involved, including 
but not limited to stitching style, thread parameters and colors. Therefore, an inter­active interface 
was built. We developed an algorithm to simulate *e-mail: cherrychen0602@gmail.com e-mail: michael.mccool@intel.com 
e-mail: kitamoto@nii.ac.jp a classic boundary style, line of staggered diagonals, as well as an algorithm 
to simulate the most used and oldest region-.lling stitch to be found in embroidery, long-short stitching. 
Since artists often put the stitches along the longest axis of the region, we compute the main inertia 
axis of each segmented region [Hiller et al. 2003] and use this to orient the stitching. Moreover, we 
separated lines that represent details from the region outline into a second layer. This was done manually 
although this could possibly be achieved automatically in future work using vision algorithms. When we 
render the model, we can easily place these details above the .ll layer, which is similar to what real 
embroiderer would normally do for such details. In the third phase, stitches are dynamically lit in real 
time in surface space by a simple thread thread lighting model [Mallo et al. 2005], but modulated with 
alpha mapping and tangent modi.cations to simulate curved stitches. Our rendering technique supports 
high-quality real-time rendering on deformable objects us­ing hardware acceleration. Before rendering, 
we compute the tan­gents from the texture parameterization [Lengyel 2001]. Then light and view vectors 
as well as local surface frame are captured and projected onto the local frame to light the stitches. 
Lastly, a .ltered texture pyramid, which supports good antialiasing, is constructed from the resulting 
texture and applied to the 3D object. References HAUSNER, A. 2001. Simulating decorative mosaics. In 
Proceed­ings of the 28th annual conference on Computer graphics and interactive techniques, SIGGRAPH 
01, 573 580. HILLER, S., HELLWIG, H., AND DEUSSEN, O. 2003. Beyond stippling methods for distributing 
objects on the plane. Euro­graphics 22, 515 522. KURENAI-KAI. 2005. Traditional Japanese Patterns 1. 
Seigensha. LENGYEL, E. 2001. Computing Tangent Space Basis Vectors for an Arbitrary Mesh. Terathon Software 
3D Graphics Library, http://www.terathon.com/code/tangent.html. MALLO, O., PEIKERT, R., SIGG, C., AND 
SADLO, F. 2005. Illu­minated lines revisited. In IEEE Visualization Conference, 19 26. SECORD, A. 2002. 
Weighted Voronoi stippling. In Proceedings of the 2nd international symposium on Non-photorealistic anima­tion 
and rendering, ACM, NPAR 02, 37 43. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037795</article_id>
		<sort_key>800</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>71</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Lace curtain: modeling and rendering of woven cloth using microfacet BSDF]]></title>
		<subtitle><![CDATA[production of a catalog of curtain animations]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037795</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037795</url>
		<abstract>
			<par><![CDATA[<p>The need for rendering woven fabrics arises frequently in computer graphics. Woven fabrics have a specific appearance, luster, and transparency. We have proposed a BTDF model using the Henyey-Greenstein Function (HGF)[Uno et al. 2008]. However, since the HGF is a phenomenologically based model, to make the model more accurate, a physically based model is introduced. This paper proposes a new microfacet BTDF model for woven fabrics. we embed a multi-band structure with a fluorescent property into the model, and the results of the rendering show the powerful influence of fluorescence on the appearance of the visible band.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809432</person_id>
				<author_profile_id><![CDATA[81442612264]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shuhei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nomura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809433</person_id>
				<author_profile_id><![CDATA[81328488825]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Atsushi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809434</person_id>
				<author_profile_id><![CDATA[81488671886]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Emi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ishigo]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809435</person_id>
				<author_profile_id><![CDATA[81488656760]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Takuya]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okamoto]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809436</person_id>
				<author_profile_id><![CDATA[81421599981]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Yoshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Mizushima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809437</person_id>
				<author_profile_id><![CDATA[81100522455]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>6</seq_no>
				<first_name><![CDATA[Noriko]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nagata]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Kwansei Gakuin University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[nagata@kwansei.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1401071</ref_obj_id>
				<ref_obj_pid>1401032</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Uno, H., Mizushima, Y., Nagata, N., and Sakaguchi, Y. 2008. Lace curtain: measurement of BTDF and rendering of woven cloth-production of a catalog of curtain animations-. In <i>ACM SIGGRAPH 2008 talks</i>, ACM, 1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2383874</ref_obj_id>
				<ref_obj_pid>2383847</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Walter, B., Marschner, S., Li, H., and Torrance, K. 2007. Microfacet models for refraction through rough surfaces. In <i>Rendering Techniques (Proc. EG Symposium on Rendering)</i>, Citeseer, 195--206.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Lace Curtain: Modeling and Rendering of Woven Cloth Using Microfacet BSDF -Production of a Catalog of 
Curtain Animations Shuhei Nomura Atsushi Ishida Emi Ishigo Takuya Okamoto Yoshiki Mizushima Noriko Nagata* 
Kwansei Gakuin University 1 Introduction The need for rendering woven fabrics arises frequently in computer 
graphics. Woven fabrics have a speci.c appearance, luster, and transparency. We have proposed a BTDF 
model using the Henyey-Greenstein Function (HGF)[Uno et al. 2008]. However, since the HGF is a phenomenologically 
based model, to make the model more accurate, a physically based model is introduced. This pa­per proposes 
a new microfacet BTDF model for woven fabrics. we embed a multi-band structure with a .uorescent property 
into the model, and the results of the rendering show the powerful in.uence of .uorescence on the appearance 
of the visible band. 2 Physically based model for woven cloth 2.1 Measurements of woven cloth We measured 
the BTDF and the .uorescence property of the four woven fabric samples. The BTDF measurements were conducted 
by using a BRDF instrument (Fig.1a)[Uno et al. 2008]. The excita­tion property and the .uorescence property 
of woven cloth (Fig.1b) were obtained by a Jobin YvonSpex FluoroMax-2 spectrophoto.u­orometer.  (a)Measured 
BTDF (b)The excitation and .uorescence property Figure 1: Measurements of woven cloth  2.2 Woven cloth 
microfacet BTDF model To choose the best-.t model for approximating the BTDF of fab­rics, we compared 
the approximate curves calculated from six typi­cal models (Phong, Ward, Ashikhmin-Shirly, Cook-Torrance, 
HGF, and GGX[Walter et al. 2007]) with the measured BTDF curves. The results showed that the GGX, a physically 
based model, .t the BTDFs most precisely. Therefore, we proposed a new woven cloth microfacet BTDF model 
on the basis of the GGX adding parame­ters such as the density of threads, the absorbing/scattering coef.­cients, 
the twisted structure of yarn and the woven structure in the woven cloth. Thus, this model can be thought 
as BSDF because of the introduction of scattering. 2.3 Multi-band BTDF We calculated the .uorescent 
color and the .uorescence intensity from the excitation spectrum and the .uorescence spectrum ob­tained 
by measurement, using XYZ/RGB color coordinate, and found each component value of the multi-band BTDF 
that consists of RGB bands.  3 Rendering 3.1 Woven cloth microfacet BTDF model Here, the results of 
the rendering based on the previous HGF-based model and the new microfacet-based model are compared in 
Fig­ure2. We introduced Global Illumination and performed the ray e-mail:nagata@kwansei.ac.jp (a)Without 
.uorescence property (b)With .uorescence property Figure 3: Results of the rendering based on the multi-band 
BTDF  4 Real-time rendering of the BTDF A real-time rendering algorithm of this BTDF model was imple­mented 
by using a combination of OpenGL and Nvidia s Cg. 5 Conclusion We proposed a new woven cloth microfacet 
BTDF model on the ba­sis of the GGX, and a multi-band BTDF model for including .uo­rescent properties. 
We are planning to generate a catalog of curtain animations that can express various types of woven fabrics 
under arbitrary light conditions.  References UNO, H., MIZUSHIMA, Y., NAGATA, N., AND SAKAGUCHI, Y. 
2008. Lace curtain: measurement of BTDF and rendering of woven cloth-production of a catalog of curtain 
animations-. In ACM SIGGRAPH 2008 talks, ACM, 1. WALTER, B., MARSCHNER, S., LI, H., AND TORRANCE, K. 
2007. Microfacet models for refraction through rough surfaces. In Rendering Techniques (Proc. EG Symposium 
on Rendering), Citeseer, 195 206. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037796</article_id>
		<sort_key>810</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>72</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Light-field caching]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037796</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037796</url>
		<abstract>
			<par><![CDATA[<p>With the continuously increasing sensor resolutions of cameras, light field imaging is becoming a more and more practical extension to conventional digital photography. It complements post-processing by synthetic aperture control, refocusing, as well as perspective and field-of-view changes. For being a true alternative to classical 2D imaging, however, the spatial resolution of light fields must be in the same megapixel-order as the resolution of today's digital images. The additional angular resolution must also be adequately high to prevent sampling artifacts (in particular for synthetic re-focussing). This will quickly cause gigabytes rather than megabytes of data that have to be rendered with limited graphics memory. We describe a light-field caching framework that makes it possible to render very large light fields in real-time.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809438</person_id>
				<author_profile_id><![CDATA[81488669251]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Simon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Opelt]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[simon@opelt.net]]></email_address>
			</au>
			<au>
				<person_id>P2809439</person_id>
				<author_profile_id><![CDATA[81100622976]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oliver]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bimber]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Johannes Kepler University Linz]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[oliver.bimber@jku.at]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Light-Field Caching Simon Opelt*and Oliver Bimber Johannes Kepler University Linz  Figure 1: Measured 
page faults (y-axis) for a sequence of frames (x-axis) of a rendered light-.eld sample trace (bottom) 
for a target frame rate of 30fps, a cache size of 450MB, and a light .eld of 4.2GB (2048×2048×19×19). 
While on-demand streaming (green) becomes quickly unacceptable (after increasing the .eld-of-view in 
the example above), caching effectively reduces the number of page faults to an applicable range. Our 
probability-based approach (red) outperforms LRU (blue) in cases of rapid viewing changes. 1 Introduction 
and Motivation With the continuously increasing sensor resolutions of cameras, light .eld imaging is 
becoming a more and more practical exten­sion to conventional digital photography. It complements post­processing 
by synthetic aperture control, refocusing, as well as per­spective and .eld-of-view changes. For being 
a true alternative to classical 2D imaging, however, the spatial resolution of light .elds must be in 
the same megapixel-order as the resolution of today s digital images. The additional angular resolution 
must also be ad­equately high to prevent sampling artifacts (in particular for syn­thetic re-focussing). 
This will quickly cause gigabytes rather than megabytes of data that have to be rendered with limited 
graphics memory. We describe a light-.eld caching framework that makes it possible to render very large 
light .elds in real-time. 2 Our Approach We have developed a CUDA-based light .eld renderer that is 
ex­tended by a software-managed cache. In a preprocessing step the light-.eld data is split into pages 
of 128×128 pixels (con.gurable) which results in 48kB blocks that are managed by the cache and that are 
stored in the main memory or on hard disk. Similar to virtual memory and virtual texturing our system 
maintains an indirection structure (i.e., an index table) that represents the current state of the cache. 
It is used by the rendering CUDA-kernel to determine if re­quired light .eld data is currently available 
in the graphics memory and where it is located. Computing the required pages is inverse to sampling a 
new image from the light .eld. Essentially, it is a pro­jection from the perspective cameras which make 
up the light .eld over the adjusted focal plane into the aperture of the virtual camera. For determining 
which pages should be loaded into the cache and which pages should be evicted if the cache is full different 
strate­gies can be applied. We have implemented a CUDA-accelerated version of Least Re­cently Used (LRU) 
as a reference caching strategy. For each frame the required pages are computed and compared with the 
state of the cache for deciding which data is additionally required. If no free cache slots are available, 
LRU replaces the pages that have been least recently used. *simon@opelt.net oliver.bimber@jku.at  In 
addition to a simple LRU strategy, we investigated a probabilis­tic approach that determines the likelihood 
of pages to be required in future frames as an alternative decision metric for loading, evict­ing, and 
additional prefetching. It applies dead reckoning to a set of previously adjusted virtual camera parameters 
(position, aperture, focus, and .eld-of-view) to estimate the future camera parameters. For each of the 
estimated future parameters we determine all pages that are required for all values within an (adjustable) 
range around the estimations in parameter space. The further the distance of these values from the corresponding 
estimations the lower are their prob­abilities. This leads to a set of parameter-individual probabilities 
that are assigned to the associated pages. The overall probabil­ity of a page is the weighted sum of 
its parameter-probabilities. The weighting allows for amplifying and attenuating the impor­tance of individual 
parameters. The pages are then loaded in the order of their overall probabilities, starting with the 
pages that are required in the current frame. Thus, pages with higher probabilities are prefetched earlier 
than pages with lower probabilities. The amount of pages that can be transferred per frame is limited 
for a given target frame-rate. A rapid change in viewing the light .eld (e.g., caused by fast camera 
movements, or aperture/.eld-of-view enlargements) may require large amounts of data to be loaded at once. 
As soon as missing pages cannot be transferred fast enough, this leads either to a dropping frame rate 
or to a visual degradation of the rendered image that is sampled from the incomplete light .eld. We therefore 
keep track of the number of pages that are miss­ing per frame and use this as an error metric for evaluation. 
 3 Results Sample results are presented in the accompanying video and in .gure 1. The light .eld shown 
has a size of 4.2GB and is ren­dered with 30fps using an allocated 450MB cache on an NVIDIA GeForce GTX 
480. An on-demand streaming of required data without caching leads constantly to missing data for rendering 
(or slower frame rates). With the same angular resolution, the spa­tial resolution would have to be downscaled 
to 512×512 to .t all the data into graphics memory and to render the light .eld appro­priately without 
caching. We are currently investigating optimal weighting strategies and additional compression options 
for our probability-based caching approach, as well as ef.cient extensions to support the stereoscopic 
viewing of very large light .elds. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037797</article_id>
		<sort_key>820</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>73</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[Non-photorealistic animation and rendering creative use of existing software]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037797</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037797</url>
		<abstract>
			<par><![CDATA[<p>The charm of traditional animation often results from the imperfection of frame by frame drawings.</p> <p>We developed a simple to use algorithm to simulate hand drawn lines inside of an existing software package. The direction, length and density of the lines renders the surface quality as curvature, reflection and occlusion in a handmade style. During animation the lines are updated to follow the mesh deformation. The solution integrates seamlessly with the existing production workflow.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[handmade style]]></kw>
			<kw><![CDATA[ink and paint illustration]]></kw>
			<kw><![CDATA[line drawing]]></kw>
			<kw><![CDATA[non photorealistic animation and rendering]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809440</person_id>
				<author_profile_id><![CDATA[81488671986]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Tilmann]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kohlhaase]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University for Applied Sciences Darmstadt]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tilmann.kohlhaase@h-da.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Tobias Isenberg a. o. 2006. Non-Photorealistic Rendering in Context: An Observational Study]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Tilke Judd a. o. 2007.Apparent Ridges for Line Drawing]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Forrester Cole a. o., 2008 Where Do People Draw Lines?]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Non-PhotorealisticAnimationandRenderingn Creative UseofexistingSoftwaren Prof. TilmannKohlhaaseUniversity 
forAppliedSciencesDarmstadte e ExamplesresultsusingtheStanfordmodels,thef showingacomposite f lineson 
ofmoviecharactersandfinalrenderedtopofan Abstractn n Thee charme ofe traditionale animatione oftene resultse 
imperfectionbyframe offramedrawings. Wealgorithmto developedasimpletousesimulate linese insideofanexistinge 
softwaree package.Thee lengthe anddensitye ofthee linesrendersthee surface curvature,reflectionandocclusioninahandmades 
animationthelinesareupdatedtofollowthemeshe Thesolutionintegratesseamlesslywiththeexistin workflow.e 
e n n fr ome thee n hand drawne d irection,e e qualityase tyle.Duringe deformation.e gproductione Keywords:None 
photorealistice Animationande Rendering,e Handmadedrawing,andpaintillustr ation. style,Lineinke 1Motivation 
Introductionandn Thee lookofcomputeranimationtendstocleanande p erfecte surfaces.Oneofthefascinationse 
ofhanddrawnanimationaree thee imperfectionsevokede throughthee passionofe tra ditionale animators,e drawingframee 
byframewithpene ande pape r.e Thee natureofanimationitselfhasabuildinlevelofe abstractionthate correlateslookofthistraditionalapproa 
ch. withtheOntheotherhandcomputeranimationmightbeusede tosupporte andspeeduptheanimationworkflowbyphysicalsi 
mulation,e controlledtweeningandtofreethecamerafromthe theatricale constraints.e Wee showae solutione 
fore thee simulatione ofe ae handmadee renderstylee wellintegratede insidee thee co mputere animationworkflow.Toprooftheconceptwerealise 
dashorte animationmovieforpreschoolchildrencreatingthe lookofae moving picturebook.e e 2Approach Technicaln 
Similare solutionsinsoftwarepackages,likee toonsh ading,e aree workinge withsimplifiede modelse ande 
don te evaluated irectione andlengthoflines,thatanartistwouldusedomo dulatethee topographyobject. oftheTherearegoodsolutionsforoutlinerenderingbute 
thereisnoe supporte forrenderinge thesurfacee structureande cur vaturee bye hatchinge ande scribblinge 
ine ae hande drawne way.Existi nge standalonee solutionsaree notsuitableine atightpr oductione process.Basedonrelatedworksandowntestswedev 
elopedae modellinesindirectionofthestro thatdrawsthengestcurvaturee one concavebendingse andtowardse 
thee lowestonee one c onvexe surfacee angles.e Duringe theanimationthee objecte sur facee ise evaluatedframebyframetoadaptthedirectionsto 
theactuale deformation.e Forthee illustrationofe lineswee teste de variouse solutionslikespritesorpainteffects,butfinall 
theuseofparticlestreakssimulatingthefadingo perfectly.e e  3FutureWork Implementationandn n Weappliedthestandardparticlesystemtothesurf 
particlesinregardofthelight,directionandsur resultingindenserhatchingindarkareas.Inthee aquarelleshaderf 
ycameupwithe fdrawnlinese ace,emittinge faceocclusion,e nextstepthee algorithme evaluatesthee directionofe 
thee strongeste ore loweste curvatureattheoriginofeachparticle.Toavoide problemswithe irregularuvtexturestheworldcoordinatesofthee 
surroundinge meshgeometrywasevaluatedusingabuildinMayae scripte neareste PointonSurface .Thusassigningeachparticlestr 
eake toe itsactualdirection.e Thee particlese streak sdir ectione liese alwayse inae planetangentiale 
toe thee surface .e Fromthee camerae directionthiscreatesadenserhatchingtowardsth eoutlineofe thee characters.e 
Settingsforrandomnessallowe fore a e veryvivide hatchwhichwereducedduringtheprojectbecausew efeltoure 
targete groupcoulde bedistractede frome thee character se faciale expressionsbytoomuchflurry.Toavoidtoomuchr 
andomnesse wee finallyconstrainedtheparticleoriginse tothee surface.e Ine additione wecoulde influencecolour,length,transpa 
rencye ande thicknessofthelinesaswellasthebehaviourdep endentonthee angleofthesurfaceandthecameraaxis.Hiddenpa 
rticlesone thearenot backoftheobjectsdrawn.e Tostayinsidetheproductionworkflowwewroteas criptthate coulde 
bee appliedtoe anye polygonale mesh,e creatinge an de connectinge allthee necessarynodesandruntimee expr 
Theworkflowprovedverystableduringthewholepr Duringasimulationruntheparticleswerecachedt ofinee tuninge 
ofe thee looke ofe thee renderede streaks,e wi recalculating thedirections.e Thee performancee sufferede 
frome thee Mayae essions.e oduction.e allowfore thoute scripte nearestPointOnSurface beingnotcapableofmultie 
threading.e Developinge asolutiontousemultiplee threadsaswelle ase integratinglightmeasurementinsidethescriptare 
some issuese forfuturedevelopment.Overallthisprojectshowe dupwithae rewardinge usee ofe standarde softwaree 
outsidee ofe thee b developinge aninnovativeandtouchingrenderstyle .e e  Referencesn n Tobiase Isenberga.o.2006.e 
Non-Photorealistice Render Context: AnObservationalStudye Tilke2007.ApparentLineDrawin Judda.o.Ridgesforge 
Forrester,2008DoPeopleLine s? Colea.o.WhereDrawe e-mail: tilmann.kohlhaase@h-da.de ox,e inge ine Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037798</article_id>
		<sort_key>830</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>74</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Real-time 4D ultrasound visualization with the Voreen framework]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037798</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037798</url>
		<abstract>
			<par><![CDATA[<p>Voreen is an open source volume rendering engine which allows interactive visualization of volumetric data sets with high flexibility when integrating new visualization techniques [Meyer-Spradow et al. 2009]. It is designed with minimal overhead, so that, even for difficult processing networks, high rendering speeds can be achieved. While Voreen is well-equipped to display Cartesian volumes, volumes in polar coordinates, such as raw ultrasound beam data, currently have to be interpolated and converted to Cartesian coordinates to be displayed. This is highly time-consuming on a CPU while interpolation and volume rendering can be done on a graphics card in real-time [Sumanaweera 2004].</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809441</person_id>
				<author_profile_id><![CDATA[81488662226]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ralf]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bruder]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of L&#252;beck, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[bruder@rob.uni-luebeck.de]]></email_address>
			</au>
			<au>
				<person_id>P2809442</person_id>
				<author_profile_id><![CDATA[81488672711]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Philipp]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jauer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of L&#252;beck, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809443</person_id>
				<author_profile_id><![CDATA[81453627865]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Floris]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Ernst]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of L&#252;beck, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809444</person_id>
				<author_profile_id><![CDATA[81488645819]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Lars]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Richter]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of L&#252;beck, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809445</person_id>
				<author_profile_id><![CDATA[81100329734]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>5</seq_no>
				<first_name><![CDATA[Achim]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schweikard]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of L&#252;beck, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bruder, R., Ernst, F., and Schweikard, A. 2011. A frame-work for real-time target tracking in radiosurgery using three-dimensional ultrasound. In <i>Proc CARS 2011</i>, accepted.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Hagenah J., Knig I., Becker B., et al. 2007. Substantia nigra hyperechogenicity correlates with clinical status and number of Parkin mutated alleles. <i>J Neurol 254</i> 1407--1413.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1674560</ref_obj_id>
				<ref_obj_pid>1674528</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Meyer-Spradow, J., Ropinski, T., Mensmann, J., and Hinrichs, K. 2009. Voreen: A rapid-prototyping environment for ray-casting-based volume visualizations. <i>IEEE Comput Graph 29</i>, 6, 6--13.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Sumanaweera, T. 2004. <i>GPU Gems: Programming Techniques, Tips and Tricks for Real-Time Graphics</i>, 1 ed. Addison-Wesley Professional, New York, chp. 40: Applying Real-Time Shading to 3D Ultrasound Visualization, 693--707.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Real-time 4D ultrasound visualization with the Voreen framework Ralf Bruder, Philipp Jauer, Floris Ernst, 
Lars Richter, Achim Schweikard* Institute for Robotics and Cognitive Systems, University of L¨ ubeck, 
Germany 1 Introduction Voreen is an open source volume rendering engine which allows interactive visualization 
of volumetric data sets with high .exibil­ity when integrating new visualization techniques [Meyer-Spradow 
et al. 2009]. It is designed with minimal overhead, so that, even for dif.cult processing networks, high 
rendering speeds can be achieved. While Voreen is well-equipped to display Cartesian vol­umes, volumes 
in polar coordinates, such as raw ultrasound beam data, currently have to be interpolated and converted 
to Cartesian coordinates to be displayed. This is highly time-consuming on a CPU while interpolation 
and volume rendering can be done on a graphics card in real-time [Sumanaweera 2004]. 2 Exposition We 
have integrated the functionality to display raw ultrasound vol­umes into the Voreen framework. In general, 
3D ultrasound data is given in polar coordinates (d,a,ß), i.e. as distance along a ray and two angles 
describing the direction away from the ultrasound transducer. An extended volume handle has been created 
to store the beam geometry additionally to the raw volume data. This han­dle is backward compatible to 
Cartesian volume handles to as well allow the use of the image processing functionality of the Voreen 
framework. Slice and volume renderers for 3D ultrasound volumes have been implemented, which use the 
beam geometry to render the ultrasound volume. Additionally, the slice and volume renderers were combined 
with standard renderers for Cartesian medical data (i.e. CT, MRI, or PET data) to allow simultaneous 
multi-volume raycasting. Some examples are shown in Fig. 1. We have also implemented a TCP/IP volume 
source which can re­ceive volumetric streaming data from external sources as well as information about 
the location of the captured data (i.e., the po­sition of the ultrasound transducer as seen by an optical 
tracking system). Volumetric data is transferred via Ethernet using a simple, yet extendable, binary 
tag-value protocol. In this way live data can be streamed into the Voreen framework and processed in 
real-time. Screening for Parkinson s disease has been identi.ed as a possible *e-mail: bruder@rob.uni-luebeck.de 
 Figure 2: Multivolume visualization of MRI and US data. application for our work [Hagenah et al. 2007]. 
Physicians look for hyperechogeneic regions in the mesencephalon. This task is greatly simpli.ed when 
the ultrasound volume is supplemented by an MRI scan which features much higher spatial resolution and 
clarity. An MRI scan of the patient s head has been registered with an ul­trasound volume captured through 
the temporal using an optical tracking system to follow the movements of the ultrasound trans­ducer in 
real-time. Using the Voreen framework, it is possible to visualize combined raycasting of both volumes 
and orthogonal slice views through the registered ultrasound and MRI volumes (see Fig. 2) with full streaming 
framerates (= 20 fps) and minimal latencies (approx. 100 -250 ms). Live ultrasound output is provided 
by our enhanced GE Vivid 7 Dimension 4D ultrasound station [Bruder et al. 2011]. 3 Conclusion We have 
shown that, using Voreen, real-time single and multimodal visualization of ultrasonic data is feasible. 
The implemented pro­cessors can be used as a basis for further real-time volume process­ing, including 
registration, image enhancement and visualization. References BRUDER, R., ERNST, F., AND SCHWEIKARD, 
A. 2011. A frame­work for real-time target tracking in radiosurgery using three­dimensional ultrasound. 
In Proc CARS 2011, accepted. HAGENAH J., KNIG I., BECKER B., et al. 2007. Substantia nigra hyperechogenicity 
correlates with clinical status and number of Parkin mutated alleles. J Neurol 254 1407 1413. MEYER-SPRADOW, 
J., ROPINSKI, T., MENSMANN, J., AND HINRICHS, K. 2009. Voreen: A rapid-prototyping environ­ment for ray-casting-based 
volume visualizations. IEEE Comput Graph 29, 6, 6 13. SUMANAWEERA, T. 2004. GPU Gems: Programming Techniques, 
Tips and Tricks for Real-Time Graphics, 1 ed. Addison-Wesley Professional, New York, chp. 40: Applying 
Real-Time Shading to 3D Ultrasound Visualization, 693 707. Copyright is held by the author / owner(s). 
SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037799</article_id>
		<sort_key>840</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>75</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Screen space anisotropic blurred soft shadows]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037799</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037799</url>
		<abstract>
			<par><![CDATA[<p>Generating soft shadows by area light sources is important for rendering a realistic scene in computer graphics. One of the methods for real time rendering of soft shadows is Screen-space percentage-closer soft shadows(SSPCSS)[MohammadBagher et al. 2010] which improved the performance of PCSS[Fernando 2005] when the light source is large, by using separable Gaussian filtering in screen space. However, SSPCSS does not consider the angle between a normal vector of a shadowed surface and a viewing vector. Therefore, it causes different soft shadows by viewpoints. Our algorithm solves this problem by using anisotropic Gaussian blurs which depend on the normal vectors of the surfaces (Fig. 1).</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809446</person_id>
				<author_profile_id><![CDATA[81488657099]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Zhongxiang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Zheng]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[zheng@img.cs.titech.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809447</person_id>
				<author_profile_id><![CDATA[81100652296]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Suguru]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Saito]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Tokyo Institute of Technology]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[suguru@img.cs.titech.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1187153</ref_obj_id>
				<ref_obj_pid>1187112</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Fernando, R. 2005. Percentage-closer soft shadows. In <i>ACM SIGGRAPH 2005 Sketches</i>, ACM, New York, NY, USA, SIGGRAPH '05.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1836987</ref_obj_id>
				<ref_obj_pid>1836845</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[MohammadBagher, M., Kautz, J., Holzschuch, N., and Soler, C. 2010. Screen-space percentage-closer soft shadows. In <i>ACM SIGGRAPH 2010 Posters</i>, ACM, New York, NY, USA, SIGGRAPH '10, 133:1--133:1.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Pham, T., and Van Vliet, L. 2005. Separable bilateral filtering for fast video preprocessing. In <i>Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</i>, IEEE, 4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen Space Anisotropic Blurred Soft Shadows Zhongxiang Zheng * Suguru Saito Tokyo Institute of Technology 
Tokyo Institute of Technology  (a) SSABSS (b) SSPCSS Figure 1: Comparison between SSPCSS and our method, 
SSABSS. There is the lit area between two shadows when viewing the shad­ows from above(top row). When 
viewing from another angle(bottom row), only SSABSS holds the lit area. 1 Introduction Generating soft 
shadows by area light sources is important for rendering a realistic scene in computer graphics. One 
of the methods for real time rendering of soft shadows is Screen-space percentage-closer soft shadows(SSPCSS)[MohammadBagher 
et al. 2010] which improved the performance of PCSS[Fernando 2005] when the light source is large, by 
using separable Gaussian .lter­ing in screen space. However, SSPCSS does not consider the angle between 
a normal vector of a shadowed surface and a viewing vec­tor. Therefore, it causes different soft shadows 
by viewpoints. Our algorithm solves this problem by using anisotropic Gaussian blurs which depend on 
the normal vectors of the surfaces (Fig. 1). 2 Method Our algorithm, Screen Space Anisotropic Blurred 
Soft Shad-ows(SSABSS) performs the following steps. The basic concept of our method is the same as PCSS 
and SSPCSS. We improved the most important process, that is the shadow blurring step. 1. Perform the 
original shadow mapping to .nd shadow pixels in screen space, and save the result as a texture. 2. Obtain 
local average depth values of ray blocker from the shadow map in the same way as PCSS. 3. Estimate the 
penumbra sizes in screen space in the same way as SSPCSS. 4. Blur hard shadows with anisotropic Gaussian 
.lters which de­pend on the penumbra sizes and the normal vectors of the shadowed surfaces.  *e-mail: 
zheng@img.cs.titech.ac.jp e-mail:suguru@img.cs.titech.ac.jp   3 Anisotropic Gaussian blur In step 
4, while SSPCSS uses a separable cross bilateral .lter[Pham and Van Vliet 2005] with typical 2D Gaussian 
functions whose con­tours are circular, we use anisotropic Gaussian functions instead of typical 2D Gaussian 
functions, because a circle on a plane viewed from an angle except for its normal direction becomes an 
ellipse. Let Nv =(nx,ny ,nz ) be the normalized normal vector of a sur­face, Ns = (0, 0, 1) be a normal 
vector of screen, and the radius of the circle be 1. The coef.cients of the ellipse is expressed by the 
followings. Aminor = normalize(nx,ny , 0),Amajor = Aminor ×Nv , (1) rminor = Nv · Ns,rmajor =1, (2) where 
Aminor, Amajor are the minor and the major axis vectors of the ellipse, and rminor, rmajor are the minor 
and the major radii, respectively. We separate the elliptical 2D Gaussian functions whose major and minor 
axes are calculated by eq. (1), and which are scaled by the penumbra sizes. 4 Discussion and Results 
The rendering time is compared with the previous methods in Fig. 2. Our method is faster than SSPCSS 
because .ltered regions be­come smaller. A fatal visual cue does not appear in almost all cases although 
the separable Gaussian .lter is not accurate approximation when the shadowed surface is not .at (Fig. 
3). References FERNANDO, R. 2005. Percentage-closer soft shadows. In ACM SIGGRAPH 2005 Sketches, ACM, 
New York, NY, USA, SIGGRAPH 05. MOHAMMADBAGHER, M., KAUTZ, J., HOLZSCHUCH, N., AND SOLER, C. 2010. Screen-space 
percentage-closer soft shadows. In ACM SIGGRAPH 2010 Posters, ACM, New York, NY, USA, SIGGRAPH 10, 133:1 
133:1. PHAM, T., AND VAN VLIET, L. 2005. Separable bilateral .ltering for fast video preprocessing. In 
Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on, IEEE, 4. Copyright is held by 
the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037800</article_id>
		<sort_key>850</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>76</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Screen space spherical harmonics occlusion (S<sub>3</sub>HO) sampling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037800</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037800</url>
		<abstract>
			<par><![CDATA[<p>One objective of global illumination (GI) models is to solve the lighting integral of the upper hemisphere at a surface point. One particular effect of GI that has become very popular in the past years is Ambient Occlusion (AO), where, independently of the lighting, a coverage factor that indicates the amount of occlusion induced by its local neighboring geometry is calculated for every surface point. Screen Space Ambient Occlusion (SSAO) algorithms allow for the approximation of this occlusion factor in real time. Ritschel et al. [2009] proposed Screen Space Directional Occlusion (SSDO), combining the calculation of occlusion and lighting by simultaneously accumulating the light of unoccluded directions when sampling the occlusion. Contrary to AO this results in colored shadows simulating directional lighting effects. Green [2003] outlined techniques based on Spherical Harmonics (SH) that approximate global lighting models with the basic principle of representing both, the incoming radiance field and the occlusion, as functions defined on the sphere that can be efficiently represented in the spherical Fourier domain. Calculating the spherical occlusion function, however, remains computationally expensive since ray tracing methods have to be applied.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809448</person_id>
				<author_profile_id><![CDATA[81488669994]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Sebastian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Herholz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GRIS, University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sebastian.herholz@student.uni-tuebingen.de]]></email_address>
			</au>
			<au>
				<person_id>P2809449</person_id>
				<author_profile_id><![CDATA[81335496966]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Timo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Schairer]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GRIS, University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[schairer@gris.uni-tuebingen.de]]></email_address>
			</au>
			<au>
				<person_id>P2809450</person_id>
				<author_profile_id><![CDATA[81100575262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Wolfgang]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stra&#223;er]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[GRIS, University of T&#252;bingen, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[strasser@gris.uni-tuebingen.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Green, R. 2003. Spherical harmonic lighting: The gritty details. <i>Archives of the Game Developers Conference</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1507161</ref_obj_id>
				<ref_obj_pid>1507149</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Ritschel, T., Grosch, T., and Seidel, H.-P. 2009. Approximating dynamic global illumination in image space. In <i>Proc. of the 2009 Symposium on Interactive 3D Graphics and Games</i>, ACM, New York, NY, USA, 75--82.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Screen Space Spherical Harmonics Occlusion (S3HO) Sampling Sebastian Herholz*and Timo Schairer and Wolfgang 
Straßer GRIS, University of T¨ ubingen, Germany Figure 1: Scene lit by a blue and yellow light source. 
Basic SSAO (left), the proposed approach (center) and side-by-side comparison of a close-up (right). 
Accounting for directional occlusion and incoming radiance leads to realistic colors and shapes of the 
shadows. 1 Introduction One objective of global illumination (GI) models is to solve the lighting integral 
of the upper hemisphere at a surface point. One particular effect of GI that has become very popular 
in the past years is Ambient Occlusion (AO), where, independently of the lighting, a coverage factor 
that indicates the amount of occlusion induced by its local neighboring geometry is calculated for every 
surface point. Screen Space Ambient Occlusion (SSAO) algorithms allow for the approximation of this occlusion 
factor in real time. Ritschel et al. [2009] proposed Screen Space Directional Occlusion (SSDO), combining 
the calculation of occlusion and lighting by simultane­ously accumulating the light of unoccluded directions 
when sam­pling the occlusion. Contrary to AO this results in colored shadows simulating directional lighting 
effects. Green [2003] outlined tech­niques based on Spherical Harmonics (SH) that approximate global 
lighting models with the basic principle of representing both, the incoming radiance .eld and the occlusion, 
as functions de.ned on the sphere that can be ef.ciently represented in the spherical Fourier domain. 
Calculating the spherical occlusion function, however, re­mains computationally expensive since ray tracing 
methods have to be applied. In this work we present the idea of combining SSDO with the con­cept of Spherical 
Harmonics Lighting evaluated per pixel in real time for dynamic scenes. We refer to our new technique 
as Screen Space Spherical Harmonics Occlusion (S3HO). 2 Directional Occlusion To determine the directional 
occlusion of a surface point the vol­ume of its upper hemisphere needs to be sampled. Since we want to 
perform the sampling in screen space we use a so-called G-Buffer containing the position and normal of 
each pixel in the camera coor­dinate system. Every sample is projected to screen space and com­pared 
to the corresponding position in the G-Buffer. We consider a sample occluded if this position is closer 
to the camera than the sample s and unoccluded otherwise. Using this sampling scheme, a vector of low-order 
SH coef.cients representing the spherical oc­clusion function is created per pixel. Only a relatively 
small number of samples can be evaluated in real­ *e-mail: sebastian.herholz@student.uni-tuebingen.de 
e-mail: {schairer,strasser}@gris.uni-tuebingen.de time. In order to minimize noise and aliasing effects 
resulting from the sampling process we use a 4 × 4 interleaved/jittered sampling pattern for the sampling 
directions. To eliminate remaining direc­tional noise, fast low-pass .ltering of the occlusion function 
is per­formed in the SH domain by applying the spherical equivalent of a Gaussian kernel represented 
as Zonal Harmonics. 3 Lighting Using the occlusion SH coef.cient vector in combination with the incoming 
radiance .eld represented in the SH basis, it is possible to compute dynamic diffuse lighting of a surface 
point in screen space by solving the integral over its upper hemisphere while taking into account its 
local occlusion. This is one of the key aspects of SH lighting, since it is easily done by calculating 
the scalar product of both SH coef.cient vectors. 4 Conclusion We presented an approach for calculating 
the SH representation of the local per-pixel occlusion in real-time with only marginally higher cost 
compared to a SSAO implementation with similar sam­pling. Projecting the occlusion function to the SH 
domain makes it possible to perform fast and ef.cient low-pass .ltering of di­rectional noise. The amount 
of directionality depends on the bandwidth of the SH projection decreasing gracefully to the non­directional 
effect of standard AO. Our approach for directional lighting results in colored shadow effects similar 
to SSDO, yet sepa­rating occlusion and lighting calculation. This separation allows the straight-forward 
integration of our method into modern 3D graphics engines that use SH lighting and SSAO for diffuse GI 
effects. References GREEN, R. 2003. Spherical harmonic lighting: The gritty details. Archives of the 
Game Developers Conference. RITSCHEL, T., GROSCH, T., AND SEIDEL, H.-P. 2009. Approx­imating dynamic 
global illumination in image space. In Proc. of the 2009 Symposium on Interactive 3D Graphics and Games, 
ACM, New York, NY, USA, 75 82. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, 
British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037801</article_id>
		<sort_key>860</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>77</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Z-fighting aware depth peeling]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037801</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037801</url>
		<abstract>
			<par><![CDATA[<p>Efficient capturing of the entire topological and geometric information of a 3D scene is an important feature in many graphics applications for rendering multi-fragment effects. Example applications include order independent transparency, volume rendering, CSG rendering, trimming, and shadow mapping all of which require operations on more than one fragment per pixel location.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809451</person_id>
				<author_profile_id><![CDATA[81482660140]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Andreas]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Vasilakis]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Ioannina]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[abasilak@cs.uoi.gr]]></email_address>
			</au>
			<au>
				<person_id>P2809452</person_id>
				<author_profile_id><![CDATA[81100590851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Ioannis]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fudos]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Ioannina]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fudos@cs.uoi.gr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1230117</ref_obj_id>
				<ref_obj_pid>1230100</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bavoil, L., Callahan, S. P., Lefohn, A., Comba, J. A. L. D., and Silva, C. T. 2007. Multi-fragment effects on the GPU using the k -buffer. <i>Proceedings of the 2007 symposium on Interactive 3D graphics and games - I3D '07</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Everitt, C., 2001. Interactive Order-Independent Transparency, Tech. Report, Nvidia Corporation.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Kessenich, J., 2009. The opengl shading language version: 1.50, document revision: 11.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1278806</ref_obj_id>
				<ref_obj_pid>1278780</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Myers, K., and Bavoil, L. 2007. Stencil Routed A-Buffer. <i>SIGGRAPH '07: ACM SIGGRAPH 2007 sketches</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Z-.ghting aware Depth Peeling Andreas A. Vasilakis* Ioannis Fudos University of Ioannina University 
of Ioannina 1 Introduction Ef.cient capturing of the entire topological and geometric infor­mation of 
a 3D scene is an important feature in many graphics applications for rendering multi-fragment effects. 
Example appli­cations include order independent transparency, volume rendering, CSG rendering, trimming, 
and shadow mapping all of which re­quire operations on more than one fragment per pixel location. An 
in.uential multi-pass technique is front-to-back (F2B) depth peeling [Everitt 2001] which works by peeling 
off a single frag­ ment per pass and by exploiting the GPU capabilities to accumulate the .nal result. 
The major drawback of this peeling algorithm is that fragment layers with depth identical to the fragment 
depth de­tected in the previous pass are discarded and so not peeled. Stencil Routed A-buffer (SRAB) 
[Myers and Bavoil 2007] treats z-.ghting for sorted fragments. However, SRAB is limited by the resolution 
of the stencil buffer and is incompatible with hardware supported multisample antialiasing. k-buffer 
[Bavoil et al. 2007] processes k fragments in a single pass, thus performing up to k times faster than 
F2B. k-buffer suffers from read-modify-write hazards and needs a small .xed amount of additional memory 
which is allocated in the form of multi render target buffers. Similarly to SRAB, k-buffer re­quires 
a pre-sorting of the primitives of the scene to treat correctly up to k Z-.ghting fragments. In this 
work, we introduce a novel technique for commodity graph­ics hardware that completely treats Z-.ghting 
by extending F2B depth peeling with the overhead of one extra geometry pass. To speed up depth peeling 
at scenes with large number of layers with same depth values, we also propose an approximate z-.ghting 
free depth peeling technique that combines the F2B and the k-buffer al­gorithms. Figure 1: Order independent 
transparency with z-.ghting correc­tions for a scene consisting of 3 bunnies. 2 Z-.ghting Correction 
Algorithms Extending F2B: We present the Z-.ghting free F2B (F2B ZF) depth peeling by adapting the F2B 
algorithm to peel all fragments placed at the same depth. When we have peeled all fragments at one depth, 
the next depth layer underneath is returned. To ob­tain all fragments at the same depth we discard the 
fragments that are not placed at this depth. To distinguish among z-.ghting frag­ments, we extract (peel) 
the color of the fragment which has the maximum primitive identi.er (build-in variable gl PrimitiveID 
of GLSL [Kessenich 2009]). One extra geometry pass is used to calculate the sum of the (remaining, not 
peeled) z-.ghting frag­ments and .nd which of them should be extracted next. We discard peeled fragments 
of the same depth by eliminating all fragments that have a primitive identi.er smaller than the maximum 
primi­tive id determined during the previous step. Both computations are performed in one pass using 
additive and maximum blending oper­ations respectively. The overhead of this algorithm as compared to 
*e-mail:abasilak@cs.uoi.gr e-mail:fudos@cs.uoi.gr Copyright is held by the author / owner(s). SIGGRAPH 
2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  the 
original F2B is the increase of the number of geometry passes from a scene with depth complexity N to 
2N +1. The plus one pass overhead is due to the fact that the .rst depth peeling pass serves only as 
an initialization of the depth (no color information is extracted): Algorithm 1 F2B ZF Depth Peeling 
 {1st Rendering Pass using Max Blending} 1: if not .rst pass then 2: return The color of the fragment 
with the maximum ID 3: end if 4: if all fragments at this depth have been peeled then 5: return next 
nearest layer depth 6: else 7: return same layer depth 8: end if {2nd Rendering Pass using Add and Max 
Blending} 1: for all not peeled fragments placed at current depth layer do 2: return The sum of them 
3: return The maximum ID of them 4: end for Combining F2B with k-buffer: We further introduce F2BKB 
ZF, a depth peeling technique combining traditional depth peeling with the k-buffer approach. At each 
iteration, a depth layer is extracted using the classical F2B rendering pass. Then, using a variation 
of k-buffer we only extract fragments positioned at the same depth layer. Note that initial primitive 
sorting is not needed since we peel fragments at the same depth. While this method is much faster for 
many scenes, it cannot eliminate the z-.ghting effect when there are than k fragments at the same depth. 
 3 Results Methods MB Sorting MSAA Peeled Layers Z-fighting Accuracy Total Passes FPS F2B 9 × v [8,8,8,8] 
[100,25,12.5,0.08] [8,8,8,8] [219,83,45,9] k-buffer 102 v v [8,29,56,56] [100,90,87.5,58.3] [1,4,7,7] 
[147,23,8,4] SRAB 126 v × [8,32,48,48] [100,100,75,50] [1,4,6,6] [123,19,10,7] F2B_ZF 33 × v [8,32,64,96] 
[100,100,100,100] [17,65,129,193] [125,14,4,1] F2BKB_ZF 33 × v [8,32,64,64] [100,100,100,66.6] [16,16,16,16] 
[59,30,11,5] Table 1: Comparison between traditional depth peeling techniques and proposed methods. 
Figure 1 illustrates transparent rendering with and without the use of z-.ghting correction for three 
cubes with stripes. Finally, Table 1 shows a comparison in terms of peeling accuracy, performance (in 
fps) and memory storage (in Mbytes) of the original F2B, k-buffer and SRAB methods and both of our proposed 
alternatives for a scene consisting of [1, 4, 8, 12] Stanford Bunnies (69,451 triangles) at a 1024 × 
768 viewport on an nVidia Geforce GTX 480. Finally, the software is available at.  References BAVOIL, 
L., CALLAHAN, S. P., LEFOHN, A., COMBA, J. A. L. D., AND SILVA, C. T. 2007. Multi-fragment effects on 
the GPU using the k -buffer. Proceedings of the 2007 symposium on Interactive 3D graphics and games -I3D 
07. EVERITT, C., 2001. Interactive Order-Independent Transparency, Tech. Report, Nvidia Corporation. 
KESSENICH, J., 2009. The opengl shading language version: 1.50, document revision: 11. MYERS, K., AND 
BAVOIL, L. 2007. Stencil Routed A-Buffer. SIGGRAPH 07: ACM SIGGRAPH 2007 sketches.  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037802</section_id>
		<sort_key>870</sort_key>
		<section_seq_no>10</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Virtual reality]]></section_title>
		<section_page_from>10</section_page_from>
	<article_rec>
		<article_id>2037803</article_id>
		<sort_key>880</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>78</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Fly-through Heijo palace site]]></title>
		<subtitle><![CDATA[augmented telepresence using aerial omnidirectional videos]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037803</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037803</url>
		<abstract>
			<par><![CDATA[<p>This study developed a virtual tourism system "beyond time and space" using augmented telepresence. Telepresence is a technique that provides a view of a remote site using immersive displays. In this paper, augmented telepresence (AT) refers to a kind of telepresence that provides a user with both a view of a remote site and related information using augmented reality (AR) techniques.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809453</person_id>
				<author_profile_id><![CDATA[81548032262]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Fumio]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Okura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology (NAIST)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[fumio-o@is.naist.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809454</person_id>
				<author_profile_id><![CDATA[81100078820]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Masayuki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kanbara]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology (NAIST)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kanbara@is.naist.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809455</person_id>
				<author_profile_id><![CDATA[81100392671]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Naokazu]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Yokoya]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Nara Institute of Science and Technology (NAIST)]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yokoya@is.naist.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ghadirian, P., and Bishop, I. D. 2008. Integration of augmented reality and GIS: A new approach to realistic landscape visualization. <i>Landscape and Urban Planning 86</i>, 226--232.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Igawa, N., Koga, Y., Matsuzawa, T., and Nakamura, H. 2004. Models of sky radiance distribution and sky luminance distribution. <i>Solar Energy 77</i>, 137--157.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Onoe, Y., Yamazawa, K., Takemura, H., and Yokoya, N. 1998. Telepresence by real-time view-dependent image generation from omnidirectional video streams. <i>Computer Vision and Image Understanding 71</i>, 2, 154--165.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>2114587</ref_obj_id>
				<ref_obj_pid>2114545</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Yokochi, Y., Ikeda, S., Sato, T., and Yokoya, N. 2006. Extrinsic camera parameter estimation based-on feature tracking and GPS data. In <i>Proc. Seventh Asian Conf. on Computer Vision (ACCV2006)</i>, vol. 1, 369--378.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: (a) An omnidirectional image aligned by the posture of omnidirectional camera. An invisible 
area appears at the top of the image. (b) An augmented omnidirectional image with virtual constructs 
superimposed using camera position and posture under the illumination environment of the real world. 
Invisible areas are inpainted from the input image. (c) A view-dependent perspective image converted 
from the omnidirectional augmented image in real-time. 1 Introduction This study developed a virtual 
tourism system beyond time and space using augmented telepresence. Telepresence is a technique that provides 
a view of a remote site using immersive displays. In this paper, augmented telepresence (AT) refers to 
a kind of telep­resence that provides a user with both a view of a remote site and related information 
using augmented reality (AR) techniques. The 1300th anniversary of Nara Heijo-kyo capital, an ancient 
cap­ital in Japan, was last year. The proposed system superimposes CG of the Heijo palace which was built 
1300 years ago beyond time on recorded videos captured from an unmanned airship at the remote site beyond 
space. Related AT works using recorded videos (e.g. [Ghadirian and Bishop 2008]) cannot resolve geomet­ric 
and photometric registration problems, and/or they do not pro­vide for immersive telepresence. This study 
handles geometric and photometric registration problems for automatic movie-quality reg­istration. In 
addition, the proposed system uses omnidirectional videos captured by an omnidirectional multi camera 
system (OMS) equipped on an airship to increase the immersiveness of telepres­ence by providing looking-around 
behavior for the user. To render augmented scenes, we use image-based-lighting (IBL) and global illumination 
(GI) techniques with an omnidirectional environmen­tal map. Invisible areas of the environmental map 
including areas which the OMS cannot capture and areas which the airship occludes a background scenery, 
are inpainted automatically. 2 Approach To overcome the geometric registration problem, the proposed 
sys­tem estimates the camera position and posture. The proposed sys­tem uses non-realtime processing 
with a high-accuracy camera po­sition and posture estimation method [Yokochi et al. 2006] with a video 
and position data measured by GPS. Each frame of the om­nidirectional video is aligned using the position 
and posture of the OMS (e.g. Figure 1(a)). Environmental maps for IBL are generated from an omnidirectional 
e-mail: {fumio-o, kanbara, yokoya}@is.naist.jp video to realize photometric registration. Environmental 
maps had to be generated without invisible areas. Such areas also decrease the immersiveness of telepresence. 
To inpaint invisible areas and generate a complete environmental map, other frames in the video are searched 
for an area corresponding to the invisible area using SSD, and the pixel intensities belonging to this 
area is copied to the pixels of the invisible area. The All-sky-model [Igawa et al. 2004] is used to 
inpaint areas having no corresponding areas, where the background scene is occluded in all the frames. 
The All-sky-model is a statistical model of various skies, with the intensities of pixels calculated 
from some parameters estimated from the whole sky. Augmented omnidirectional video is rendered using 
the complete environmental maps and the estimated position and posture of the OMS (c.f. Figure 1(b)). 
Commercial software is used for the ren­dering process. The augmented omnidirectional video is converted 
to view-dependent perspective images (c.f. Figure 1(c)) in real­time [Onoe et al. 1998], and is presented 
to the user via a HMD. The user can freely look around the augmented scene. The pro­posed system was 
demonstrated at the Commemorative Events for the 1300th Anniversary of Nara Heijo-kyo Capital, and over 
a thou­sand people experienced the system. References GHADIRIAN, P., AND BISHOP, I. D. 2008. Integration 
of aug­mented reality and GIS: A new approach to realistic landscape visualization. Landscape and Urban 
Planning 86, 226 232. IGAWA, N., KOGA, Y., MATSUZAWA, T., AND NAKAMURA, H. 2004. Models of sky radiance 
distribution and sky luminance distribution. Solar Energy 77, 137 157. ONOE, Y., YAMAZAWA, K., TAKEMURA, 
H., AND YOKOYA, N. 1998. Telepresence by real-time view-dependent image genera­tion from omnidirectional 
video streams. Computer Vision and Image Understanding 71, 2, 154 165. YOKOCHI, Y., IKEDA, S., SATO, 
T., AND YOKOYA, N. 2006. Extrinsic camera parameter estimation based-on feature tracking and GPS data. 
In Proc. Seventh Asian Conf. on Computer Vision (ACCV2006), vol. 1, 369 378. Copyright is held by the 
author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037804</article_id>
		<sort_key>890</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>79</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[Kinect-based augmented reality projection with perspective correction]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037804</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037804</url>
		<abstract>
			<par><![CDATA[<p>This work introduces an augmented reality 3D visualization technique combining object surface information detection through Kinect and perspective-corrected rendering and projection. Virtual scene is first altered and rendered with detected surface information, then mapped and projected onto corresponding surfaces of the object. Compared to traditional augmented reality which requires additional screens, our approach better utilized structural information of an object and enabled direct reference from physical objects used as display, therefore demonstrates better augmented reality experience and potential for more intuitive interaction.</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[3-d visualization]]></kw>
			<kw><![CDATA[augmented reality]]></kw>
			<kw><![CDATA[projection mapping]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809456</person_id>
				<author_profile_id><![CDATA[81487649724]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Yichen]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Tang]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[yichent@ece.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809457</person_id>
				<author_profile_id><![CDATA[81444600827]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Billy]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lam]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[billyl@ece.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809458</person_id>
				<author_profile_id><![CDATA[81319501712]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Stavness]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[stavness@ece.ubc.ca]]></email_address>
			</au>
			<au>
				<person_id>P2809459</person_id>
				<author_profile_id><![CDATA[81327488395]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Sidney]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Fels]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of British Columbia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ssfels@ece.ubc.ca]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Djajadiningrat, J., Smets, G., and Overbeeke, C. 1997. Cubby: a multiscreen movement parallax display for direct manual manipulation. <i>Displays 17</i>, 3--4, 191--197.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Milgram, P., Takemura, H., Utsumi, A., and Kishino, F. 1994. Augmented reality: A class of displays on the realityvirtuality continuum. In <i>Proceedings of Telemanipulator and Telepresence Technologies</i>, vol. 2351, Citeseer, 282--292.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1753535</ref_obj_id>
				<ref_obj_pid>1753326</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Stavness, I., Lam, B., and Fels, S. 2010. pCubee: a perspective-corrected handheld cubic display. In <i>Proceedings of the 28th international conference on Human factors in computing systems</i>, ACM, 1381--1390.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Kinect-based Augmented Reality Projection with Perspective Correction Yichen Tang, Billy Lam, Ian Stavness, 
Sidney Fels* Electrical and Computer Engineering University of British Columbia  Figure 1: Pipeline 
of kinect-based detection and projection augmented reality technique. (a) original object, (b) depth 
map from Kinect, (c) virtual scene matched with detected object geometry, (d) perspective correction 
for each face of the object, (e) labelling each face in projection image, (f) projection image generated 
by transforming and assembling rendered images, (g) .nal view from user s position Abstract This work 
introduces an augmented reality 3D visualization tech­nique combining object surface information detection 
through Kinect and perspective-corrected rendering and projection. Virtual scene is .rst altered and 
rendered with detected surface informa­tion, then mapped and projected onto corresponding surfaces of 
the object. Compared to traditional augmented reality which requires additional screens, our approach 
better utilized structural informa­tion of an object and enabled direct reference from physical objects 
used as display, therefore demonstrates better augmented reality ex­perience and potential for more intuitive 
interaction. Keywords: augmented reality, 3-d visualization, projection map­ping 1 Introduction Augmented 
reality techniques have been widely used as a way to combine computer generated virtual scene with real 
world. How­ever, most current techniques share a same limitation: graphics are rendered directly on top 
of live camera video, so the altered scene is shown through a screen, no matter it is a desktop monitor, 
head­mounted display or hand-held device [Milgram et al. 1994]. More­ over, graphics are always rendered 
from screen perspective, so users have to keep looking perpendicularly at the display and imagine a .xed 
view from it. This destroys the experience, making it like watching videos instead of looking at realistic 
scene. Therefore, in­teraction with the scene is limited by the additional layer of display standing 
in between the user and the real world. 2 Our Approach We propose a new way of presenting augmented 
reality through projection onto real objects based on detected surface information. First, Kinect is 
used to get depth maps of the object, using which we can detect .at surfaces with identical gradient 
value and direc­tion, as the three faces of a cube in our prototype. Next, a virtual cube is modi.ed 
to match with real object dimension. Using de­tected shape and orientation information for each face, 
we are able to apply off-axis projection on each of them for a speci.c perspec­ *e-mail: {yichent, billyl, 
stavness, ssfels}@ece.ubc.ca tive [Djajadiningrat et al. 1997] [Stavness et al. 2010]. Rendered images 
for three faces are then mapped with pre-labelled perspec­tive transformation and assembled to construct 
an image for projec­tion. Finally, user at correct position can have realistic experience of the virtual 
scene merged with real object, such as a chair inside transparent cube shown in Figure 1(g). To simplify 
the implementation, several assumptions are made in our current stage of prototype. The .nal perspective 
transforma­tion for each face in projection image is manually de.ned in ad­vance, which brings in a limitation 
that the position of object and projector should remain relatively static. An automatic process to map 
transformation through feedback mechanism is being carried out in order to overcome this limitation. 
We are also bringing in eye-tracking for dynamic perspective correction to enable motion parallax for 
better 3D effect and virtual experience. 3 Discussion Our work illustrates a novel approach of combining 
detection of object surface and projection of perspective-corrected images onto corresponding surfaces. 
It is expected to solve the issue brought by additional screens in traditional AR technique, and reduce 
manual interference in projection-based AR. Also, compared to other virtual reality display, our system 
brings in more .exibility in its form factor and reduces the design dif.culties by ideally enabling any 
surface to be a display. It also enables reference from real world object, and therefore shows potential 
for more natural interaction with virtual objects.  References DJAJADININGRAT, J., SMETS, G., AND OVERBEEKE, 
C. 1997. Cubby: a multiscreen movement parallax display for direct man­ual manipulation. Displays 17, 
3-4, 191 197. MILGRAM, P., TAKEMURA, H., UTSUMI, A., AND KISHINO, F. 1994. Augmented reality: A class 
of displays on the reality­virtuality continuum. In Proceedings of Telemanipulator and Telepresence Technologies, 
vol. 2351, Citeseer, 282 292. STAVNESS, I., LAM, B., AND FELS, S. 2010. pCubee: a perspective-corrected 
handheld cubic display. In Proceedings of the 28th international conference on Human factors in com­puting 
systems, ACM, 1381 1390. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037805</article_id>
		<sort_key>900</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>80</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[PRIMA]]></title>
		<subtitle><![CDATA[parallel reality-based interactive motion area]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037805</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037805</url>
		<abstract>
			<par><![CDATA[<p>It is interesting to superimpose scenes in another time at the same place over the real scene in real time simultaneously using virtual reality technologies. We call it as "Parallel Reality." Parallel Reality can connect different time at the same space and augment both scenes, while Telexistence technology can connect different spaces. You can interact with yourself and/or others in the past time at the same place. "Another Time, Another Space"[1] is a media art which shows real time video with effects of time axis taken by a camera of the artwork. We can enjoy video effects of time axis because it takes real world and add effects in real-time. We came up the idea from this artwork. The concept of this study is to propose a system to provide interactions between a person in real-time world and other persons in past time at the same place. In terms of connecting different time, "Computational rephotography"[2] that connect viewpoints of both past and current photos, is one of Parallel Reality technologies. The concept of our system presents us more immersive and curious experiences. Using our system, for example, unusual games can be made that plays it with past players, or creates enemies from people in the real world.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809460</person_id>
				<author_profile_id><![CDATA[81466646165]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Toshiki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takeuchi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[take@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809461</person_id>
				<author_profile_id><![CDATA[81474673977]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Totaro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakashima]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[tota@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809462</person_id>
				<author_profile_id><![CDATA[81100461884]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kunihiro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nishimura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kuni@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809463</person_id>
				<author_profile_id><![CDATA[81100257385]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Michitaka]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hirose]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[hirose@cyber.t.u-tokyo.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwai, T. 1993. Another Time, Another Space.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1805968</ref_obj_id>
				<ref_obj_pid>1805964</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Bae, S., Agarwala, A., and Durand F. 2010. Computational rephotography. <i>ACM Trans. Graph</i>., Vol.29, pp.24:1--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1866073</ref_obj_id>
				<ref_obj_pid>1866029</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Wilson, A. D., and Benko, H. 2010. Combining multiple depth cameras and projectors for interactions on, above and between surfaces. Proceedings of the 23nd annual ACM symposium on User interface software and technology, pp.273--282.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 PRIMA: Parallel Reality-based Interactive Motion Area Toshiki Takeuchi , Totaro Nakashima , Kunihiro 
Nishimura , Michitaka Hirose Graduate School of Graduate School of Graduate School of Information Graduate 
School of Information Interdisciplinary Interdisciplinary Science and Technology, Science and Technology, 
Information Studies, Information Studies, The University of Tokyo The University of Tokyo The University 
of Tokyo The University of Tokyo  Fig. 1 Whole view of PRIMA Fig. 2 Real-time reconstruction and touch 
control 1. Introduction It is interesting to superimpose scenes in another time at the same place over 
the real scene in real time simultaneously using virtual reality technologies. We call it as Parallel 
Reality. Parallel Reality can connect different time at the same space and augment both scenes, while 
Telexistence technology can connect different spaces. You can interact with yourself and/or others in 
the past time at the same place. Another Time, Another Space [1] is a media art which shows real time 
video with effects of time axis taken by a camera of the artwork. We can enjoy video effects of time 
axis because it takes real world and add effects in real-time. We came up the idea from this artwork. 
The concept of this study is to propose a system to provide interactions between a person in real-time 
world and other persons in past time at the same place. In terms of connecting different time, Computational 
rephotography [2] that connect viewpoints of both past and current photos, is one of Parallel Reality 
technologies. The concept of our system presents us more immersive and curious experiences. Using our 
system, for example, unusual games can be made that plays it with past players, or creates enemies from 
people in the real world.  2. Implementation We developed an interaction system, named PRIMA (Parallel 
Reality-based Interactive Motion Area), that could track and record persons, and play their complete 
3D data. PRIMA consists of four Kinects, four client PCs and a touch panel interface. Kinect is a sensor 
developed by Microsoft that can get image and depth data using an inner RGB camera and an infrared depth 
sensor. Kinects are put on the ceiling and a box-type touch panel interface is located in the center 
of the system (Fig. 1). Four client PCs get data from each Kinect and send it to a server PC. The server 
PC does all following processes: 3D reconstruction, synchronized user tracking, Fig. 3 Synchronized user 
tracking Fig. 4 Interaction with past oneselves recording, and playing. A structure of this system is 
similar to LightSpace [3]. Although LightSpace is a room interface for interactions between surfaces 
in real-time only, PRIMA deals with interactions including both current and past scenes. PRIMA can reconstruct 
scenes in real time (Fig. 2). We calibrated four Kinects and calculated transformation matrixes previously. 
This system can avoid occlusion problem to some extent because it uses multiple Kinects put on different 
positions. In addition, you can control a reconstructed scene intuitively with your fingers on a touch 
panel and see it from all directions of view: translation, rotation and zooming in/out. Each client connecting 
a Kinect sends image and depth data as well as tracks users and sends their labels to a server. We needed 
to synchronize these user labels since user labels of each Kinect were not related to others. We hypothesized 
that two user labels were for same one if a distance of centers of the user-labeled pixels was less than 
a threshold. This method can detect and track persons in the area almost good (Fig. 3). Thus, this system 
can record and play 3D motion data about a specific person. We can create a new scene that some users 
in various past times are reconstructed over the current scene by using person-separated motion logs. 
PRIMA has some attractive contents, for example, an interaction with past oneselves (Fig. 4), wandering 
of ghosts' crowd in several past times, etc. PRIMA has high potential for various applications and contents. 
 References [1] Iwai, T. 1993. Another Time, Another Space. [2] Bae, S., Agarwala, A., and Durand F. 
2010. Computational rephotography. ACM Trans. Graph., Vol.29, pp.24:1-15. [3] Wilson, A. D., and Benko, 
H. 2010. Combining multiple depth cameras and projectors for interactions on, above and between surfaces. 
Proceedings of the 23nd annual ACM symposium on User interface software and technology, pp.273-282. email: 
{take, tota, kuni, hirose}@cyber.t.u-tokyo.ac.jp Copyright is held by the author / owner(s). SIGGRAPH 
2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037806</article_id>
		<sort_key>910</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>81</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Science in the city AR]]></title>
		<subtitle><![CDATA[using mobile augmented reality for science inquiry activities]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037806</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037806</url>
		<abstract>
			<par><![CDATA[<p>The Exploratorium, a museum of science, art, and human perception in San Francisco, California, is developing science activities for informal learners to use with augmented reality (AR) browsers on mobile devices. We're experimenting with this technique to learn about engaging museum visitors, as well as other informal learners in our community, in personal inquiry. Using the Layar AR platform, we've developed a set of interactive science activity tools that smartphone users can use to explore phenomena via augmented exhibits at the museum and at locations in the San Francisco Bay Area. The tools, which are freely available, are part of our Science in the City video series and provide location-contextualized activities for different programs. This series introduces online visitors to stories about locations, people, and themes relating to natural phenomena and the built environment. The mobile AR science activity tools became available in the spring of 2011 in a Science in the City "layer" within the Layar mobile browser and via a link at the Exploratorium's website, www.exploratorium.edu/scienceinthecity.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809464</person_id>
				<author_profile_id><![CDATA[81100209713]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Robert]]></first_name>
				<middle_name><![CDATA[J.]]></middle_name>
				<last_name><![CDATA[Rothfarb]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Exploratorium]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[rrothfarb@exploratorium.edu]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Science in the City AR: Using Mobile Augmented Reality for Science Inquiry Activities Robert J. Rothfarb* 
Exploratorium 1. Introduction The Exploratorium, a museum of science, art, and human perception in San 
Francisco, California, is developing science activities for informal learners to use with augmented reality 
(AR) browsers on mobile devices. We re experimenting with this technique to learn about engaging museum 
visitors, as well as other informal learners in our community, in personal inquiry. Using the Layar AR 
platform, we ve developed a set of interactive science activity tools that smartphone users can use to 
explore phenomena via augmented exhibits at the museum and at locations in the San Francisco Bay Area. 
The tools, which are freely available, are part of our Science in the City video series and provide location-contextualized 
activities for different programs. This series introduces online visitors to stories about locations, 
people, and themes relating to natural phenomena and the built environment. The mobile AR science activity 
tools became available in the spring of 2011 in a Science in the City layer within the Layar mobile browser 
and via a link at the Exploratorium s website, www.exploratorium.edu/scienceinthecity. 2. Science Inquiry 
Activities One activity about microclimates, available as a Point of Interest (POI) in the layer, provides 
a tool for mobile visitors to explore fog and related weather phenomena in the area by using a 3-D model 
that marks the height of different hills and buildings in San Francisco. The model object is the familiar 
tower portion of the Golden Gate Bridge turned into an altimeter, with heights marked at different vertical 
points and scaled to actual size. A streaming audio narrative explains the scale markings and how to 
use the tool. Current weather conditions in the Bay are indicated with graphical and audio prompts. By 
aligning this virtual object with the horizon in the smartphone s augmented live view of the coastal 
area of San Francisco Bay, visitors can observe the height of fog at their location, and they can also 
learn about factors that affect how fog moves into nearby areas. Another activity uses augmented views 
to highlight geologic points of interest in the city and allows users to explore areas that feature observable 
effects of movement along local fault lines. Overlaid 3-D models show the direction of fault-line movement. 
At a location near the center of San Francisco, where an inactive fault has rendered a sheer rock face 
extremely smooth, users can control a simulation that lets them see the change in the landscape in front 
of them that would result from continued movement along the fault. ________________________________ * 
email: rrothfarb@exploratorium.edu Other POIs in the SF Bay Area related to outdoor exhibits, exhibits 
at the Exploratorium, and natural features of the local environment are featured in the Science in the 
City layer, and they provide information using images, animated 3-D graphics, video, audio, and text. 
 Figure 1: Golden Gate Bridge Fog Altimeter, a mobile augmented reality science inquiry activity using 
the Layar AR Browser platform  3. Conclusion Through this experiment with mobile AR, we hope to learn 
more about how the use of this emerging technology can be incorporated into exhibits and other interactive 
experiences for museums and other informal education institutions. Copyright is held by the author / 
owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037807</article_id>
		<sort_key>920</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>82</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[SonalShooter]]></title>
		<subtitle><![CDATA[a spatial augmented reality system using handheld directional speaker with camera]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037807</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037807</url>
		<abstract>
			<par><![CDATA[<p>Mainly in the field of Augmented Reality (AR), various systems that can overlap virtual information onto physical space in the real world have been developed. In such systems, wearing special equipment might obstruct users' natural interaction and communication. Contrarily, Spatial Augmented Reality, which project additional information directly on the targeted physical space, has attracted much attention. In our research, we aim to provide a novel auditory interaction with physical surfaces through the approach of Spatial Augmented Reality. Our system, named SonalShooter, can give auditory information to physical objects (Figure 1). When a user aims the device at a physical object, the device detects and identifies the target with a computer vision approach and he/she will be able to hear corresponding auditory information from the object itself as if it is speaking.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809465</person_id>
				<author_profile_id><![CDATA[81488670562]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ken]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Nakagaki]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[s09588kn@sfc.keio.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809466</person_id>
				<author_profile_id><![CDATA[81100527572]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Yasuaki]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kakehi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Keio University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ykakehi@sfc.keio.ac.jp]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1766652</ref_obj_id>
				<ref_obj_pid>1766591</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Ishii, K., Yamamoto, Y., Imai, M., and Nakadai, K. 2007. A navigation system using ultrasonic directional speaker with rotating base. In <i>Proceedings of the 12th International Conference on Human-Computer Interaction</i>, Springer LNCS, Volume 4558, 526--535.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Nishimura, T., Itoh, H., Nakamura, Y., Yamamoto, Y., and Nakashima, H. 2004. A compact battery-less information terminal for real world interaction. In <i>Proceedings of Pervasive 2004</i>, Springer LNCS, Volume 3001, 124--139.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Figure 1: SonalShooter Figure 2: System Design Figure 3: Application for Museum Guide 1 Introduction 
Mainly in the .eld of Augmented Reality (AR), various systems that can overlap virtual information onto 
physical space in the real world have been developed. In such systems, wearing spe­cial equipment might 
obstruct users natural interaction and com­munication. Contrarily, Spatial Augmented Reality, which project 
additional information directly on the targeted physical space, has attractedmuch attention.Inour research,weaimtoprovideanovel 
auditory interaction withphysical surfaces through the approachof Spatial Augmented Reality. Our system, 
named SonalShooter, can give auditory information to physical objects (Figure 1). When a user aims the 
device at a physical object, the device detects and identi.es the target with a computer vision approach 
and he/she will be able to hear corresponding auditory information from the object itself as if it is 
speaking. As related work, CoBIT [Nishimura et al. 2004] enables users to acquire auditory information 
related to real objects through an ear­phoneby installing infrared LEDs on them. On the other hand, our 
system does not require any electronic device on the targeted ob­jects and does not cover users ears 
like headphones. Coco [Ishii etal.2007]also enables userstohear auditory informationinapar­ticular space 
likeoursystemby usinga directional speaker. While the speaker in Coco is .xed, our system utilizes a 
handheld direc­tionalspeaker and provides mobile interactions with the realworld. 2 SonalShooter In 
the SonalShooter, we offer technical innovations as follows: First,wedevelopedasystem which enable usersto 
receiveauditory information selectively froma surfaceofphysical objects. Thisde­viceis composedofacompact 
directional speaker,acamera,alaser pointer, and a tact switch (see Figure 2). When a user aims the de­vice 
at one of the objects with the help of the laser pointer, the de­vice detects and identi.es the targetbythe 
camera, and correspond­ing auditory information is emitted from the directional speaker. Since the sound 
of the directional speaker is re.ected when it hits *e-mail: {s09588kn, ykakehi}@sfc.keio.ac.jp solid 
objects, the user can hear the sound not from the held device but from the surface of the object. Second, 
we developed an intuitive interaction method for control­ling embedded information. In this system, users 
can switch chan­nels of additional information by rotating the device. We put 2D AR markers to the target 
object for the system to identify the object as well as detect the rotation of the device. 3 Applications 
and Future Works As an application of the SonalShooter, we propose a system for museums: a system that 
makes caption panels speak explanations. Previously, we put AR markers to caption panels with written 
ex­planations of exhibits. When a user aims the device at one of the panels, the explanation of the corresponding 
exhibited object will be heard from the surface of the panel itself (see Figure 3). Since this system 
does not require visitors to wear headphones, users can easily recognize the connection between the object 
and the additive auditory information while converse with other audiences. Further­more, this application 
provides multilingual information for users. As a channel-switching function, when the device is rotated, 
the language of the explanation will be switched in real time. In the near future, we plan to apply this 
system for not only .at sur­facesbut also objectsof complicated shapes. Moreover,by adopt­ing a marker-less 
object recognition method, we hope to apply this system to more various situations. References ISHII,K.,YAMAMOTO,Y.,IMAI,M., 
AND NAKADAI,K. 2007. A navigation system using ultrasonic directional speaker with rotating base. In 
Proceedings of the 12th International Confer­ence on Human-Computer Interaction, Springer LNCS,Volume 
4558, 526 535. NISHIMURA, T., ITOH, H., NAKAMURA, Y., YAMAMOTO, Y., AND NAKASHIMA,H. 2004.Acompact battery-less 
informa­tion terminal for real world interaction. In Proceedings of Per­vasive 2004, Springer LNCS,Volume 
3001, 124 139. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, 
Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
	<section>
		<section_id>2037808</section_id>
		<sort_key>930</sort_key>
		<section_seq_no>11</section_seq_no>
		<section_type>SESSION</section_type>
		<section_title><![CDATA[Visualization]]></section_title>
		<section_page_from>11</section_page_from>
	<article_rec>
		<article_id>2037809</article_id>
		<sort_key>940</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>83</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>1</seq_no>
		<title><![CDATA[Deshaking endoscopic video for kymography]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037809</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037809</url>
		<abstract>
			<par><![CDATA[<p>The opening and closing of the vocal folds (<i>plica vocalis</i>) at high frequencies is a major source of sound in human speech. <i>Videokymography</i> [Svec and Schutte 1995] is a technique for visualizing the motion of the vocal folds for medical diagnosis: The vibrating folds are filmed with an endoscopic camera pointed into the larynx. The camera records at a high framerate to capture vocal fold vibration (see fig. 1 for example frames). The <i>kymogram</i> used for medical diagnosis is a time-slice image, i.e. an <i>X-t</i>-cut through the <i>X-Y-t</i> image cube of the endoscopic video (fig. 2). The quality and diagnostic interpretability of a kymogram deteriorates significantly if the camera moves relative to the scene as this motion interferes with the vibratory motion of the vocal fold in the kymogram. Therefore, we propose an approach to stabilizing the motion of endoscopic video for kymography.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809467</person_id>
				<author_profile_id><![CDATA[81464666578]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[David]]></first_name>
				<middle_name><![CDATA[C.]]></middle_name>
				<last_name><![CDATA[Schneider]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Heinrich Hertz Institute, Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[david.schneider@hhi.fraunhofer.de]]></email_address>
			</au>
			<au>
				<person_id>P2809468</person_id>
				<author_profile_id><![CDATA[81436600711]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Anna]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hilsmann]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Heinrich Hertz Institute, Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809469</person_id>
				<author_profile_id><![CDATA[81100336326]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Peter]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Eisert]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Fraunhofer Heinrich Hertz Institute, Berlin, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1853400</ref_obj_id>
				<ref_obj_pid>1853379</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hilsmann, A., Schneider, D. C., and Eisert, P. 2010. Realistic cloth augmentation in single view video under occlusions. <i>Comput. Graph. 34</i> (October), 567--574.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1531350</ref_obj_id>
				<ref_obj_pid>1531326</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Liu, F., Gleicher, M., Jin, H., and Agarwala, A. 2009. Contentpreserving warps for 3d video stabilization. <i>ACM Trans. Graphics 28</i> (July), 44:1--44:9.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Schneider, D. C., Hilsmann, A., and Eisert, P. 2011. Warp-based Motion Compensation for Endoscopic Kymography. In <i>Eurographics 2011, Llandudno</i>.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Svec, J. G., and Schutte, H. K. 1995. Videokymography: High-speed line scanning of vocal fold vibration. <i>Journal of Voice 10/2</i>, 201--205.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Deshaking Endoscopic Video for Kymography David C. Schneider* , Anna Hilsmann, Peter Eisert Fraunhofer 
Heinrich Hertz Institute, Berlin, Germany The opening and closing of the vocal folds (plica vocalis) 
at high frequencies is a major source of sound in human speech. Videoky­mography [Svec and Schutte 1995] 
is a technique for visualizing the motion of the vocal folds for medical diagnosis: The vibrating folds 
are .lmed with an endoscopic camera pointed into the larynx. The camera records at a high framerate to 
capture vocal fold vibration (see .g. 1 for example frames). The kymogram used for medical diagnosis 
is a time-slice image, i.e. an X-t-cut through the X-Y-t image cube of the endoscopic video (.g. 2). 
The quality and di­agnostic interpretability of a kymogram deteriorates signi.cantly if the camera moves 
relative to the scene as this motion interferes with the vibratory motion of the vocal fold in the kymogram. 
Therefore, we propose an approach to stabilizing the motion of endoscopic video for kymography. This 
motion compensation problem is challenging and different from deshaking handheld video (e. g. [Liu et 
al. 2009]) in several respects: Firstly, the camera motion to be eliminated may be signif­icantly larger 
than a typical camera shake due to the short distance between camera and scene. Secondly, not only the 
camera and the vocal folds move but the entire scene may be highly nonrigid. Fi­nally, the image quality 
of the input material can be challenging due to high noise levels, areas of saturated highlights, interlacing 
artifacts, etc. The proposed algorithm deviates from the typical feature-based ap­proaches to motion 
compensation, but is nevertheless parallelizable and realtime capable even on the CPU. We use an image-based 
in­verse mesh warping approach similar to [Hilsmann et al. 2010] that can be stated as an optimization 
problem and solved ef.ciently in a robust Gauss-Newton framework. Our method is described in more detail 
in [Schneider et al. 2011]. Mesh-based warping is a standard approach to computing com­plex image deformations 
by deforming a control mesh in the image plane. The inverse problem, i. e. solving for a control mesh 
defor­mation given two images, can be stated as an energy minimization task: De.ne the residual for pixel 
P as .u1 .v1 T rP = I (xP ) -KxP + bP DT, DT =.u2 .v2 .u3 .v3 where I, K are images and xP is a pixel 
coordinate. Vector bP contains the barycentric coordinates of pixel P with respect to its surrounding 
triangle T in the control mesh. DT is a matrix of T s vertex displacements .ui and .vi. Estimating DT 
for all triangles d amounts to solving arg min P . (rP )+ .S where . is a robust norm-like function 
such as Huber s and S is a smoothness term based on the mesh Laplacian. This energy can be minimized 
by a robust Gauss-Newton scheme that differs only slightly from the standard least squares case. The 
mesh warp is computed independently for each image pair of the sequence. This step is computationally 
the most expensive part of the algorithm but it can be trivially parallelized to several cores due to 
the independence of the frame pairs. The warp yields a piece­wise af.ne deformation .eld between each 
frame pair which can be ef.ciently evaluated between the vertex locations. Thereby, an ROI, user annotated 
or the center region of the .rst frame, can be tracked troughout the sequence and a stabilizing image 
transforma­tion, which is restricted to be rigid for the kymography application, can be computed. *e-mail: 
david.schneider@hhi.fraunhofer.de   Figure 2: Vocal fold kymograms from two endoscopic sequences. Left: 
no motion compensation. Center: Deshaker compensation. Right: proposed method. The image-based approach 
to motion estimation has several advan­tages for our application: (1) It is highly robust on images with 
high noise level and signi.cant artifacts if used with a robust error metric. (2) For computing the transformation, 
no explicit handling of outliers (e.g. RANSAC) is required. This is an advantage over feature-based approaches. 
(3) As a global optimization scheme, the approach bene.ts from the .lling in effect of the smoothness 
term that propagates information into image regions with little gradient information. (4) The choice 
of mesh granularity and weight of the regularization term allow for .ne-grained control over the degree 
of deformation the warp is allowed to follow. References HILSMANN, A., SCHNEIDER, D. C., AND EISERT, 
P. 2010. Realistic cloth augmentation in single view video under occlusions. Comput. Graph. 34 (October), 
567 574. LIU, F., GLEICHER, M., JIN, H., AND AGARWALA, A. 2009. Content­preserving warps for 3d video 
stabilization. ACM Trans. Graphics 28 (July), 44:1 44:9. SCHNEIDER, D. C., HILSMANN, A., AND EISERT, 
P. 2011. Warp-based Motion Compensation for Endoscopic Kymography. In Eurographics 2011, Llandudno. SVEC, 
J. G., AND SCHUTTE, H. K. 1995. Videokymography: High-speed line scanning of vocal fold vibration. Journal 
of Voice 10 / 2, 201 205. Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037810</article_id>
		<sort_key>950</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>84</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>2</seq_no>
		<title><![CDATA[KineReels]]></title>
		<subtitle><![CDATA[extension actuators for dynamic 3D shape]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037810</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037810</url>
		<abstract>
			<par><![CDATA[<p>For the purposes of information presentation and entertainment use, several devices that can mechanically change their shapes have been proposed. "FEELEX [Iwata et al., 2001]" is a 3D shape display that consists of an array of linear actuators covered by a soft silicon surface. "Hyposurface [Goulthorpe]" is a wall-sized structure constructed out of interconnected metallic plates actuated by an array of pneumatic cylinder and generates patterns such as waves and texts. The actuators used in these conventional 3D shape displays are classified in three types: the pneumatic cylinder, the feed screw and the shape memory alloy. However, they present a problem in the fact that their range of movement is proportionally fixed to the size of the unit. For example, if we require a 1,000 mm stroke, the unit's body requires the storage space of 1,000 mm or more.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809470</person_id>
				<author_profile_id><![CDATA[81488645082]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Shohei]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Takei]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[takei@nae-lab.org]]></email_address>
			</au>
			<au>
				<person_id>P2809471</person_id>
				<author_profile_id><![CDATA[81100307996]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Makoto]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Iida]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[iida@cfdl.t.u-tokyo.ac.jp]]></email_address>
			</au>
			<au>
				<person_id>P2809472</person_id>
				<author_profile_id><![CDATA[81100095332]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Takeshi]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Naemura]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[The University of Tokyo]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[naemura@nae-lab.org]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>383314</ref_obj_id>
				<ref_obj_pid>383259</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Iwata, H., Yano, H., Nakaizumi, F., and Kawamura, R. 2001. Project Feelex: Adding Haptic Surface to Graphics. Proceedings of SIGGRAPH 2001. 2001: ACM, 95--104.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 KineReels: Extension Actuators for Dynamic 3D Shape Shohei Takei* Makoto Iida Takeshi Naemura The University 
of Tokyo The University of Tokyo The University of Tokyo  Figure 1: KineReels 1 Introduction For the 
purposes of information presentation and entertainment use, several devices that can mechanically change 
their shapes have been proposed. FEELEX [Iwata et al., 2001] is a 3D shape display that consists of an 
array of linear actuators covered by a soft silicon surface. Hyposurface [Goulthorpe] is a wall-sized 
structure constructed out of interconnected metallic plates actuated by an array of pneumatic cylinder 
and generates patterns such as waves and texts. The actuators used in these conventional 3D shape displays 
are classified in three types: the pneumatic cylinder, the feed screw and the shape memory alloy. However, 
they present a problem in the fact that their range of movement is proportionally fixed to the size of 
the unit. For example, if we require a 1,000 mm stroke, the unit s body requires the storage space of 
1,000 mm or more. To address this challenge, we propose an extension actuator possessing an exceptionally 
high ratio of extended height to retracted height. This actuator has the ability to extend its rod up 
to 25 times the unit s body height (3,000mm stroke against 120mm storage space). With the unprecedentedly 
high extension ratio, it extends a range of 3D shape expression. 2 Design of Extension Actuator The 
extension actuator s development was based on the requirement of a compact body capable of delivering 
dynamic vertical range. We focused on the extension mechanism of a tape measure and connected it to an 
actuator to power the line s linear movement. However, the line was unable to sustain itself while elevated, 
warranting the need for additional support. In this study, a stable rigidity is acquired with the combination 
of three tape measures. We designed the extension actuator shown in Figure 2. It consists of three reels 
and one motor within its body size, W240mm x D190mm x H120mm. This particular prototype uses commercially 
available tape measures as reels. The ends of the three tape measures are secured together to form a 
triangle cross section. The unit s motor drives one line up and down while the other two unpowered lines 
move in conjunction with it. Each line is fixed with Velcro running lengthwise. As the *e-mail: takei@nae-lab.org 
e-mail: iida@cfdl.t.u-tokyo.ac.jp e-mail: naemura@nae-lab.org End of Rod Figure 2: Actuator module construction 
 lines are reeled out they fasten to one another to form a rigid rod. The rod of this actuator can offer 
a linear range of 120mm while in storage, or up to 3,000mm when fully extended and move an average of 
300mm/sec. The current prototype is equipped with an AVR microcontroller and a photosensor so that it 
moves interactively with surrounding environment. In addition, also PC can control the rod movement via 
a wireless ZigBee terminal. A DC power supply drives the motor and is connected to the actuator with 
cables. 3 Applications and Conclusions We cluster several of these extension actuators together and 
call them KineReels and implemented some applications. The first application uses stretch fabric attached 
to the ends of the actuators rods arranged in 4 x 4 matrix to express the surface area of a virtual object. 
In Figure 1 (left), the fabric is displayed on a particular gradient. It can also express full-scale 
landforms because it can lift the surface over a person s height. It can create and support user s intuitive 
understanding of 3D geometry. The current prototype can generate a slope of up to 30 degrees with a 1,800 
square millimeter piece of fabric, and it can be improved with the more stretchable fabric. The second 
application, as shown in Figure 1 (right), attaches spheres to the end of the rods plotting them in various 
positions above the ground. These can illustrate wave patterns or trace images of moving objects in 3D 
space. Furthermore, the compact body of the unit s actuator allows for easy installation and storage 
in a variety of places. Most importantly, the actuator s linear movement effectively changes perception 
of space, allowing KineReels to be utilized for practical applications such as information presentation, 
décor, architecture, or anything else the user s creativity can inspire. References IWATA, H., YANO, 
H., NAKAIZUMI, F., AND KAWAMURA, R. 2001. Project Feelex: Adding Haptic Surface to Graphics. Proceedings 
of SIGGRAPH 2001. 2001: ACM, 95 104. GOULTHORPE, M. HYPOSURFACE, IN HTTP://HYPOSURFACE.ORG/ Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037811</article_id>
		<sort_key>960</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>85</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>3</seq_no>
		<title><![CDATA[Protoviewer]]></title>
		<subtitle><![CDATA[a web-based visual design environment for Protovis]]></subtitle>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037811</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037811</url>
		<abstract>
			<par><![CDATA[<p>In this work we present Protoviewer, a web-based design and collaborative data visualization environment designed around Protovis. Protoviewer aims to lower cognitive effort as well as the significant gulf of execution in designing effective and powerful visualizations by providing accurate categorizations of data while providing the user with the fullest control over its rendering.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809473</person_id>
				<author_profile_id><![CDATA[81536542356]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ryo]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Akasaka]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Stanford University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1639215</ref_obj_id>
				<ref_obj_pid>1638611</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bostock, M., and Heer, J. 2009. A graphical toolkit for visualization. In IEEE Transactions on Visualization and Computer Graphics, vol. 15:1121--1128.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037812</article_id>
		<sort_key>970</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>86</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>4</seq_no>
		<title><![CDATA[Revisiting the k-means algorithm for fast trajectory segmentation]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037812</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037812</url>
		<abstract>
			<par><![CDATA[<p>Many problems in Computer Science require a trajectory segmentation, in part due to the notably huge spectrum of devices that capture sequentially-generated information (e.g., motion sensors, video cameras, RFID tags, eye trackers, etc.) Segmentation leads to simplify the structure of the data, so that original objects can be divided into smaller, more compact structures. Seen this way, segmentation can be approached as a compression technique, i.e., organizing trajectories into segments whose members are similar in some way. This can be solved as a clustering problem. Unfortunately, to date we have not found a suitable method that can tap in a really simple way the temporal constraint implicitly embedded in the data. Moreover, near-optimal solutions such as kernel methods or hidden Markov models can be prohibitive if processing power is a restriction (e.g., on mobile devices).</p>]]></par>
		</abstract>
		<keywords>
			<kw><![CDATA[clustering]]></kw>
			<kw><![CDATA[segmentation]]></kw>
			<kw><![CDATA[sequential data]]></kw>
		</keywords>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809474</person_id>
				<author_profile_id><![CDATA[81413601851]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Luis]]></first_name>
				<middle_name><![CDATA[A.]]></middle_name>
				<last_name><![CDATA[Leiva]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universitat Polit&#232;cnica de Val&#232;ncia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[luileito@iti.upv.es]]></email_address>
			</au>
			<au>
				<person_id>P2809475</person_id>
				<author_profile_id><![CDATA[81100420781]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Enrique]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Vidal]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Universitat Polit&#232;cnica de Val&#232;ncia]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[evidal@iti.upv.es]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Duda, R. O., Hart, P. E., and Stork, D. G. 2001. <i>Pattern Classification</i>, 2nd ed. John Wiley & Sons, ch. Unsupervised Learning and Clustering, 517--599.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Kuhn, M. H., Tomaschewski, H., and Ney, H. 1981. Fast nonlinear time alignment for isolated word recognition. In <i>Proc. ICASSP</i>, 736--740.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Lloyd, S. 1982. Least squares quantization in PCM. <i>IEEE Trans. on Information Theory 28</i>, 2, 129--137.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Revisiting the K-means Algorithm for Fast Trajectory Segmentation Luis A. Leiva* Enrique Vidal* Institut 
Tecnol`atica, Universitat Polit`encia ogic d Inform`ecnica de Val` (a) (b) (c) (d) (e) (f) Figure 1: 
A 2D example. An arbitrary trajectory (1a) is digitized at a constant sampling rate (1b), and our objective 
is to club the path coordinates together in, say, 7 clusters. K-means based algorithms do not deal with 
temporal information, therefore resulting groups are ill-de.ned (1c) and hence the resulting segmentation 
(1d). Our proposal, though, allows to easily cope with the sequentiality of data (1e,1f). Keywords: sequential 
data, clustering, segmentation 1 Introduction Many problems in Computer Science require a trajectory 
segmen­tation, in part due to the notably huge spectrum of devices that cap­ture sequentially-generated 
information (e.g., motion sensors, video cameras, RFID tags, eye trackers, etc.) Segmentation leads to 
sim­plify the structure of the data, so that original objects can be divided into smaller, more compact 
structures. Seen this way, segmentation can be approached as a compression technique, i.e., organizing 
tra­jectories into segments whose members are similar in some way. This can be solved as a clustering 
problem. Unfortunately, to date we have not found a suitable method that can tap in a really sim­ple 
way the temporal constraint implicitly embedded in the data. Moreover, near-optimal solutions such as 
kernel methods or hidden Markov models can be prohibitive if processing power is a restric­tion (e.g., 
on mobile devices). Here we present a novel trajectory segmentation technique based on the K-means algorithm, 
a special case of EM clustering. K-means is well known for its simplicity, relative robustness, and really 
fast convergence to local minima. It is also well known that its perfor­mance depends upon two key points: 
initial partition and instance order. For that reason, we tuned the algorithm of [Duda et al. 2001] for 
unsupervised classi.cation. This version, instead of using the classical minimum distance criterion [Lloyd 
1982], is a sequential, iterative optimization re.nement that evaluates the sum-of-squared error (SSE, 
also denoted as Je) at each step, reallocating a sam­ple to a different cluster if and only if that reassignment 
decreases Je. Also, given the sequentiality of data, we use the trace segmen­tation (TS) algorithm [Kuhn 
et al. 1981] for centroid initialization. TS consists in a non-linear sampling operation that redistributes 
ob­jects to enforce even spacing between them, eliminating thus redun­dancy. 2 Method We use TS to build 
an initial partition of the data. Then we im­pose the following constraint to the classi.cation step 
of K-means: a sample x in segment i is iteratively reallocated to the previous or next clusters (i ± 
1), characterized by their means µ and their number of samples n. The best reassignment j * is determined 
if the variation in SSE is bene.cial, i.e., when .Je < 0: j * = arg min .J(i, j) i-1=j=i+1 where nj ni 
.J(i, j)= I x - µj I2 -I x - µi I2 . nj +1 ni - 1 The algorithm stops when there are no samples to reallocate, 
ensur­ing thus that the partition has reached the minimum error boundary. 3 Contributions and Bene.ts 
Our proposal: 1) is accurate, since it guarantees the convergence to the best local minimum, i.e., the 
less distorted segmentation of the original trajectory; 2) is robust, as each run for a given K always 
yields in the same segments thanks to the TS initializa­tion; 3) is fast, because, instead of the classical 
one-against-all strategy of search, we only need to check two clusters in each clas­si.cation step computational 
complexity is thus T(kd) instead of T(nkd); 4) does not require extra input parameters, just the sample 
vectors and the number of desired segments, as in K-means; 5) is specially suited for real-time applications 
and large datasets, since the computational cost of updating the centroids is indepen­dent of the number 
of samples; and 6) supports on-line learning: clusters can be updated while new objects arrive without 
affecting the previous data structure. 4 Conclusion Ours is a really straightforward method to simplify 
the segmenta­tion of motion based trajectories. The introduced modi.cations to (the sequential version 
of) K-means allowed us to deploy a succinct clustering algorithm for segmentation while greatly accelerating 
the process to convergence. We believe this work opens a new door to novel applications in the computer 
graphics domain and beyond. Acknowledgements. Work supported by the research programme Consolider Ingenio 
2010: MIPRCV (CSD2007-00018). Interactive Demonstrator. Please visit the following URL: http://personales.upv.es/luileito/wkm/siggraph-demo.html 
 References DUDA, R. O., HART, P. E., AND STORK, D. G. 2001. Pattern Classi.cation, 2nd ed. John Wiley 
&#38; Sons, ch. Unsupervised Learning and Clustering, 517 599. KUHN, M. H., TOMASCHEWSKI, H., AND NEY, 
H. 1981. Fast nonlinear time align­ment for isolated word recognition. In Proc. ICASSP, 736 740. LLOYD, 
S. 1982. Least squares quantization in PCM. IEEE Trans. on Information Theory 28, 2, 129 137. *e-mail: 
{luileito,evidal}@iti.upv.es Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037813</article_id>
		<sort_key>980</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>87</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>5</seq_no>
		<title><![CDATA[Strata treemaps]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037813</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037813</url>
		<abstract>
			<par><![CDATA[<p>Treemap is a method of visualization to recognize a hierarchical structure of data using the size and arrangement of nested rectangles. A limitation of a treemap is the difficulty to discern the structure of a hierarchy. Several approaches have been proposed to improve the visibility of the hierarchical structure. These approaches involve the use of a border or padding to emphasize the hierarchical structure. However, this leads to a disparity between the node weight and the relative node size.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809525</person_id>
				<author_profile_id><![CDATA[81488663973]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Junghong]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Choi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sakool@ajou.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2809526</person_id>
				<author_profile_id><![CDATA[81488657139]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Oh-hyun]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Kwon]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kwonoh6@ajou.ac.kr]]></email_address>
			</au>
			<au>
				<person_id>P2809527</person_id>
				<author_profile_id><![CDATA[81458652821]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Kyungwon]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Lee]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Ajou University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[kwlee@ajou.ac.kr]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1375758</ref_obj_id>
				<ref_obj_pid>1375714</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Hao, L. And James, F. 2008. Cascaded treemaps: examining the visibility and stability of structure in treemaps, In Proc. of Graphics Interface 2008, Canadian Information Processing Society, 259--266.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Michael, B. And Oliver, D. 2005. Voronoi treemaps, In Proc. of the IEEE Symposium on Information Visualization 2005, IEEE CS Press, 49--56]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1477423</ref_obj_id>
				<ref_obj_pid>1477066</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Ziemkiewicz, C. and Kosara, R. 2008. The shaping of information by visual metaphors, IEEE Transactions on Visualization and Computer Graphics, IEEE Educational Activities Department, 14(6):1269--1276]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Strata Treemaps Junghong Choi*, Oh-hyun Kwon and Kyungwon Lee Division of Digital Media, Ajou University 
 *sakool@ajou.ac.kr, kwonoh6@ajou.ac.kr, kwlee@ajou.ac.kr 1. Introduction Treemap is a method of visualization 
to recognize a hierarchical structure of data using the size and arrangement of nested rectan­gles. A 
limitation of a treemap is the difficulty to discern the struc­ture of a hierarchy. Several approaches 
have been proposed to improve the visibility of the hierarchical structure. These ap­proaches involve 
the use of a border or padding to emphasize the hierarchical structure. However, this leads to a disparity 
between the node weight and the relative node size. Strata treemap, a new approach to treemap presentation 
that im­proves the visibility of the hierarchical structure without distorting the node sizes. Strata 
treemaps are based on Voronoi treemap. Strata treemap use the surface of a sphere instead of the traditional 
two-dimensional plane. For each node in the hierarchy, we extrude it along the its surface normal and 
stack up on the its parent. The result is a three-dimensional shape that consists of stacked blocks on 
a sphere. Each block represents a node in a hierarchy and its top surface area represents the quantitative 
value of the node. Additionally, original treemaps use the metaphor of hierarchy as containment. Accordingly, 
it is difficult to observe the hierarchy as a series of higher or lower levels. However, the stacked 
structure of a Strata treemap makes this easy. 2. Approach Our goal is simple: improve the visibility 
of the hierarchical struc­ture without distorting the node sizes or eliminating them. We created spaces 
between the nodes (siblings) to facilitate easy rec­ognition of the hierarchical structure. Figure 2 
shows our idea in a simple manner. The farther a cer­tain spot on the surface of sphere is from the center, 
that is, the longer the radius, the larger the sphere. We utilized this feature of a sphere but did it 
the other way around. Initially, we formulated treemaps on the surface of a sphere. We then extruded 
the surface of the nodes along the its surface normal, not by extending the radius of the sphere (Figure 
2-1). In this way, empty spaces be­tween the nodes are naturally generated while the size of a node does 
not change (Figure 2-3). Moreover, labels can be attached onto the lateral sides of the nodes without 
creating additional spaces for labeling. Figure 2. Basic Idea of Strata Treemaps 3. Algorithm We made 
an algorithm based on weighted centroidal voronoi tes­sellation. The basic structure of the algorithm 
is similar to the recursive structure of the original treemap layout algorithm. We divided the surface 
of the node to get the surfaces of its children nodes. For each children nodes we extruded it along the 
normal direction of its surface and stack up on its parent node. 4. Results and Conclusions As a final 
example, Figure 1 shows a Strata treemap illustrating a directory structure of openFrameworks (open source 
c++ library). This hierarchy contains 9,892 nodes, including 7,512 leaf nodes, and 14 depths. Top surface 
area of each nodes refer to revision count of files. Color used as classification of root directory. 
Labels are attached onto the lateral sides of nodes without creating addi­tional spaces for labeling. 
Most of the structure of this hierarchy is easily visible. Moreover, as expected, there is no distortion 
or missing nodes. The Strata treemap is a new type of data visualization scheme which maintains the size 
and emphasizes a hierarchical structure. In existing methods, node sizes are subject to distortion for 
better identification of the hierarchical structure. In Strata treemaps, however, distortion of the node 
sizes is no longer necessary. Therefore, the nodes in a Strata treemap can be compared with each other 
regardless of the portion they are in. Furthermore, hier­archical structure is easily recognized, as 
each node is stacked on its parent node. Our future work will improve user interface with the strata 
tree­maps that strata treemap browser . It will allow the user to peel back hierarchical layers, transform 
nodes, filter and navigate. It would be similar concept to google body browser that peels back anatomical 
layers, zooms in, and navigate to parts. 5. References HAO, L. AND JAMES, F. 2008. Cascaded treemaps: 
examining the visibility and stability of structure in treemaps, In Proc. of Graphics Interface 2008, 
Canadian Information Processing Society, 259-266. MICHAEL, B. AND OLIVER, D. 2005. Voronoi treemaps, 
In Proc. of the IEEE Symposium on Information Visualization 2005, IEEE CS Press, 49-56 ZIEMKIEWICZ, C. 
AND KOSARA, R. 2008. The shaping of informa­tion by visual metaphors, IEEE Transactions on Visualization 
and Computer Graphics, IEEE Educational Activities Department, 14(6):1269-1276 Copyright is held by the 
author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037814</article_id>
		<sort_key>990</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>88</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>6</seq_no>
		<title><![CDATA[The 3D bovine and porcine myology system]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037814</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037814</url>
		<abstract>
			<par><![CDATA[<p>Anatomy is an important part of the Animal Science curriculum at the University of Nebraska -- Lincoln. It is the foundation upon which skills and expertise are built. This understanding is important in all aspects of Animal Science from anatomy and culinary arts to meat science.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809528</person_id>
				<author_profile_id><![CDATA[81488668139]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Aaron]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Hosier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Nebraska - Lincoln]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809529</person_id>
				<author_profile_id><![CDATA[81320490740]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Steve]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Jones]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Nebraska - Lincoln]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809530</person_id>
				<author_profile_id><![CDATA[81538833956]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Vishal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Singh]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Nebraska - Lincoln]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 The 3D bovine and porcine myology system Aaron Hosier, Steve Jones, Vishal Singh University of Nebraska 
 Lincoln 1 Introduction Anatomy is an important part of the Animal Science curriculum at the University 
of Nebraska Lincoln. It is the foundation upon which skills and expertise are built. This understanding 
is important in all aspects of Animal Science from anatomy and culinary arts to meat science. The Animal 
Products course is a requirement for all Animal Science, Food Science and Veterinary Science students. 
As part of the course, detailed dissections and inspections of the various muscles and bones of a bovine 
and porcine carcass are performed. While extremely effective, these dissections have several limitations. 
Proper facility requirements do not allow for a large class size and close viewing by the students. The 
process of procuring and preparing the carcass is expensive and time consuming. Because of this the dissections 
can only be performed once during the semester. If a student were to miss this particular class period 
they would miss out on the entire experience. 2 Our Solution For these reasons the 3D Bovine/Porcine 
Myology (the science of anatomy dealing with muscles) project was conceived. This system would simulate 
the experience normally received by the students in the laboratory during an actual physical dissection. 
By developing the 3D carcass system this experience became repeatable, cost effective and, ultimately, 
allow a student to have the same experience at anytime from their own computer. Professors, researchers 
and industry experts were surveyed to determine their needs for the system with the most common features 
selected for the initial development. The initial criteria for the system were: examine individual muscles 
and bones; access anatomical information on the same muscles and bones; group muscles based on various 
characteristics; examination of industry standard sub-primal cuts and their characteristics. The system 
would also provided stereoscopic display capabilities using multiple methods and multiple language support. 
Accurate anatomical models were created using CAT scans, creating slices every 7 mm detailing the bone 
and muscle structure. These scans were then used to create a Lightwave model of each muscle and bone 
which were then used to assemble the carcass. Each muscle was photographed at various angles and those 
photos were then used to texture the individual muscles to make the experience more realistic. The system 
has now been incorporated into the instructional environment. It is an experience that students feel 
comfortable with and are excited about. As one student noted, It comes at you and you re able to turn 
it and you can see all the different angles. Using the system to introduce students to the bovine and 
porcine carcasses before the physical dissection, students are now able to gain the knowledge and experience 
to actually participate in the physical dissection and not be just a passive observer. With the advent 
of lower cost stereoscopic technology entering the consumer market, students will soon be able to have 
the same level of experience at home that is currently available in the classroom. Students will be able 
to repeat the experience multiple times to assure they understand material without the expensive cost 
of a laboratory dissection. In June of 2010 the system was demonstrated at the American Meat Science 
Association's annual conference. This conference was attended by 30 plus universities with Animal and 
Food Sciences departments, along with over 20 industry related companies such as Tyson Foods, Oscar Mayer, 
Kraft and Hormel. Great interest was expressed by the various schools about how they could utilize the 
system in their instructional endeavors.  3 Next Phases Next phases of the systems development are already 
being researched. In talks with various industry representatives, training has come up as a top priority. 
Currently the meat processing centers have a 25% yearly turnover rate in production line workers. Training 
is an expensive process where actual product is used then discarded. Well trained workers can be the 
difference between a $5 hamburger and a $20 fillet. A virtual training system could substantially reduce 
training costs. This is just one of the possible future enhancements currently being considered. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037815</article_id>
		<sort_key>1000</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>89</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>7</seq_no>
		<title><![CDATA[Usefulness of style transfer functions in medical diagnosis]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037815</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037815</url>
		<abstract>
			<par><![CDATA[<p>For medical and other applications, there is a lack of evaluation of <i>usefulness</i> of processes and tools <i>beyond efficiency</i>. In this paper we look at the usefulness of style transfer functions (style TFs) in volume rendering of large medical data sets as compared to simple Phong shading. We implemented the style TFs of [Bruckner and Gr&#246;ller 2007] and compared their usefulness for specific visual tasks with the Phong shaded volumes in a controlled experiment with 28 users. When asked about their expectations, 75% (21 users) answered that they <i>expect</i> a better performance with style TFs than with Phong. Style TFs enhance edges, so they seem to increase interpretability of orientation and texture in an image. To see how this expectation holds up in a controlled experiment, our general hypothesis "Users perform better on visual tasks when the style TF is used as compared to simple Phong shading" was tested.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809531</person_id>
				<author_profile_id><![CDATA[81100330482]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Gitta]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Domik]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[domik@upb.de]]></email_address>
			</au>
			<au>
				<person_id>P2809532</person_id>
				<author_profile_id><![CDATA[81488673067]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Felix]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Steffen]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809533</person_id>
				<author_profile_id><![CDATA[81488672991]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Stephan]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Arens]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[stephan.arens@upb.de]]></email_address>
			</au>
			<au>
				<person_id>P2809534</person_id>
				<author_profile_id><![CDATA[81464671655]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>4</seq_no>
				<first_name><![CDATA[Ingrid]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Scharlau]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Paderborn, Germany]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ingrid.scharlau@upb.de]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Bruckner, S., and Gr&#246;ller, M. E. 2007. Style transfer functions for illustrative volume rendering. <i>Comput. Graph. Forum 26</i>, 3, 715--724.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Usefulness of Style Transfer Functions in Medical Diagnosis Gitta Domik1, Felix Steffen1, Stephan Arens1, 
Ingrid Scharlau2 1Computer Science, 2Cognitive Psychology; University of Paderborn, Germany* Task 1 
Task 2 Task 3 Figure 1: (Task 1, a) Two Phong shaded volumes showing different roughness in image. (Task 
1, b) Same pair, but Style shaded. (Task 2, a) Two Phong shaded volumes showing coronaries with a different 
curvature. (Task 2, b) Same pair, but Style shaded. (Task 3, a-b) Phong and Style rendered volumes, resp. 
(Task 3, c) Planar reference image. All three images show markers for a speci.c position. CT data are 
courtesy of the Rotterdam Coronary Artery Algorithm Evaluation Framework (http://coronary.bigr.nl). 1 
The Problem and the Method of Solution For medical and other applications, there is a lack of evaluation 
of usefulness of processes and tools beyond ef.ciency. In this paper we look at the usefulness of style 
transfer functions (style TFs) in volume rendering of large medical data sets as compared to sim­ple 
Phong shading. We implemented the style TFs of [Bruckner and Gr¨ oller 2007] and compared their usefulness 
for speci.c visual tasks with the Phong shaded volumes in a controlled experiment with 28 users. When 
asked about their expectations, 75% (21 users) answered that they expect a better performance with style 
TFs than with Phong. Style TFs enhance edges, so they seem to increase interpretability of orientation 
and texture in an image. To see how this expectation holds up in a controlled experiment, our general 
hypothesis Users perform better on visual tasks when the style TF is used as compared to simple Phong 
shading was tested. 2 Design of the Controlled Experiment We are interested in .nding transfer functions 
to be used in volume rendering of CT data to support physicians in detecting plaques in coronary arteries 
and preparing surgery based on the localization of plaque. The design of our controlled experiment therefore 
uses tasks that relate to the performance of medical staff in that process. Each of these tasks was to 
be realistic (in order to apply to medical diagnosis), but simple (to keep the mental processes low and 
there­fore not interfere the visual tasks with cognitive tasks). In the .rst task, participants had to 
decide which picture in a pair represents rougher patterns. Pairs with Phong shading or with style TF 
were used; there was no mix of rendering modes within a pair. Each user judged 100 pairs, where the appearance 
of Phong or style TF was random (Fig. 1, Task 1). In medical terms, the roughness task evaluates the 
participant s ability to perceive the structure of surfaces which is required to identify objects and 
noise in data sets. In the second task, participants had to decide which picture in a pair contains the 
artery that is bent stronger than the other (Fig. 1, *e-mail:{domik, stephan.arens, ingrid.scharlau}@upb.de 
Task 2). Other conditions were as in Task 1. The medical relevance here is that the curvature of an artery 
is an indirect indicator for plaque inside the artery. In the third task, participants had to decide 
if the marker on a planar coronary image (Fig. 1, Task 3, c) is representing the same location as the 
marker on either the Phong or style (Fig. 1, Task 3, a-b) ren­dered data. Each user had to solve 100 
such pairs. The medical relevance here is that doctors locate plaque in planar images, and (if needed) 
relate that locations to the 3d volume. Typically (but not only) surgeons need this kind of ability. 
Instead of measuring the time required to solve each task for each user we measured the ability to solve 
the task under various time­length conditions, sorted into very short, short and medium time duration. 
 3 Results 28 students were tested by a within-subject design and by randomly applying dif.culty levels 
to the two shading types. Measured val­ues were interpreted for: (a) Performance was tested statistically 
against chance level (guessing) via a t-test; (b) Signi.cant differ­ences in performance between style 
and Phong (t-test); (c) Estima­tion in difference of performance between both distributions. On all three 
tasks we could show that we received meaningful and not random answers. For Task 1 we could even show 
an improvement of 4% when using style TF over using Phong. Both Task 2 and Task 3 failed to show a signi.cant 
difference between style TF and Phong. The interesting result here is that differences in the perfor­mance 
of style TFs and purely Phong rendering are much smaller than expected by users, who had by a large amount 
guessed on a better performance when using style TF. References BRUCKNER, S., AND GR¨ OLLER, M. E. 2007. 
Style transfer func­tions for illustrative volume rendering. Comput. Graph. Forum 26, 3, 715 724. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037816</article_id>
		<sort_key>1010</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>90</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>8</seq_no>
		<title><![CDATA[Using statistical topic models to organize and visualize large-scale architectural image databases]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037816</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037816</url>
		<abstract>
			<par><![CDATA[<p>Digital libraries have now become by far the preferred method for storing large image collections, making it increasingly important to improve issues of search and discovery within these databases. Content-based annotations of images, for classification purposes, have been a particularly significant area of research; however, even the best of such methods are prone to error -- potentially associating incorrect text to image features and overlooking significant features. In the broader field of classification and search, Newman et al. demonstrated benefits of using statistical topic models to enrich document metadata. [Newman et al. 2007] This assumes that a document can be described with sufficient accuracy using a "bag of words" model, i.e. described by the words it contains (instances from a large vocabulary of words), and the frequency with which they occur. [Horster et al. 2007] applied this idea to large-scale image datasets, using SIFT features as data for creating a topic model.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809535</person_id>
				<author_profile_id><![CDATA[81490649305]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Patrick]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Paczkowski]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[patrick.paczkowski@yale.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809536</person_id>
				<author_profile_id><![CDATA[81100369597]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Julie]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Dorse]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
			<au>
				<person_id>P2809537</person_id>
				<author_profile_id><![CDATA[81100255828]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[Holly]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Rushmeier]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[Yale University]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>944937</ref_obj_id>
				<ref_obj_pid>944919</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Blei, D., Ng, A., Jordan, M., and Lafferty, J. 2003. Latent dirichlet allocation. <i>Journal of Machine Learning Research 3</i>, 993--1022.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>361242</ref_obj_id>
				<ref_obj_pid>361237</ref_obj_pid>
				<ref_seq_no>2</ref_seq_no>
				<ref_text><![CDATA[Duda, R., and Hart, P. 1972. Use of the hough transformation to detect lines and curves in pictures. <i>Commun. ACM 15</i>, 11--15.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1282283</ref_obj_id>
				<ref_obj_pid>1282280</ref_obj_pid>
				<ref_seq_no>3</ref_seq_no>
				<ref_text><![CDATA[Horster, E., Lienhart, R., and Slaney, M. 2007. Image retrieval on large-scale image databases. In <i>CIVR '07: Proc. of the 6th ACM international conference on image and video retrieval</i>, 17--24.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1255248</ref_obj_id>
				<ref_obj_pid>1255175</ref_obj_pid>
				<ref_seq_no>4</ref_seq_no>
				<ref_text><![CDATA[Newman, D., Hagedorn, K., Chemudugunta, C., and Smyth, P. 2007. Subject metadata enrichment using statistical topic models. In <i>JCDL '07: Proceedings of the 7th ACM/IEEECS Joint Conference on Digital Libraries</i>, 366--375.]]></ref_text>
				<ref_id></ref_id>
			</ref>
			<ref>
				<ref_obj_id>1153548</ref_obj_id>
				<ref_obj_pid>1153171</ref_obj_pid>
				<ref_seq_no>5</ref_seq_no>
				<ref_text><![CDATA[Nister, D., and Stewenius, H. 2006. Scalable recognition with a vocabulary tree. In <i>CVPR '06: Proc. of the IEEE Conference on Comp. Vision and Pattern Recognition</i>, vol. 2, 2161--8.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Using Statistical Topic Models to Organize and Visualize Large-Scale Architectural Image Databases Patrick 
Paczkowski* , Julie Dorsey, and Holly Rushmeier Yale University Figure 1: Four topics obtained using 
our classi.cation procedure, each represented by images for which that speci.c topic is most dominant. 
1 Introduction Digital libraries have now become by far the preferred method for storing large image 
collections, making it increasingly important to improve issues of search and discovery within these 
databases. Content-based annotations of images, for classi.cation purposes, have been a particularly 
signi.cant area of research; however, even the best of such methods are prone to error potentially associ­ating 
incorrect text to image features and overlooking signi.cant features. In the broader .eld of classi.cation 
and search, Newman et al. demonstrated bene.ts of using statistical topic models to en­rich document 
metadata. [Newman et al. 2007] This assumes that a document can be described with suf.cient accuracy 
using a bag of words model, i.e. described by the words it contains (instances from a large vocabulary 
of words), and the frequency with which they occur. [Horster et al. 2007] applied this idea to large-scale 
im­age datasets, using SIFT features as data for creating a topic model. Here, we extend this idea of 
effectively organizing and visualizing large-scale image collections based on their visual content. We 
fo­cus on architecturally-themed images, due to the serviceability of such a tool in the .eld of architectural 
design. A key differentia­tor from previous work is that we do not attempt to assign semantic meaning 
to the images. We instead remain in the visual image space at each stage, relying on visual words and 
topics. Moreover, we attempt to use a more intuitive visual vocabulary that is naturally suited to a 
particular database. An overview of our method follows. 2 Technical Approach First, we had to settle 
on a visual vocabulary for our dataset, which would be able to provide a suf.ciently accurate description 
of im­ages in the database, and consisting of words that can be accurately detected and extracted from 
the images. We made an important ob­servation, namely that a natural way of visually deconstructing a 
building is by describing its exterior as a composition of architec­tural elements (e.g windows, archways, 
etc). Many of these can be approximated by one or more basic geometric shapes (i.e. rect­angles, triangles, 
ellipses); we used this fact, in conjunction with using a Canny edge detector and Hough transforms [Duda 
and Hart 1972], as the foundation for detecting architectural elements within the images in our dataset. 
We created a vector descriptor for each detected architectural element (image region) using positional 
and dimensional information, along with histograms of edge segments within the region. Initially, we 
clustered all the elements found in the dataset by noting the presence neighboring architectural el­ements 
within the original image, as the relative position of these elements is often signi.cant. Subsequently, 
we applied hierarchical k-means clustering, as outlined by [Nister and Stewenius 2006], to obtain the 
.nal set of visual words for our database. Based on these clustered features, we are able to now represent 
each image by the instances and frequencies of visual words, or architec­tural elements, appearing within 
it. This representation allows us to use Latent Dirichlet Allocation (LDA) [Blei et al. 2003] to describe 
each of our images as a mixture of topics, which in turn is used to meaningfully organize the dataset. 
We have tested our pipeline on 15,976 images acquired from the Visual Resources Collection of the Yale 
Library, and found roughly 35-40 of 100 generated topics to be meaningful. Each topic is visualized using 
around 16 images from the dataset for which that topic is most dominant (see Figure 1). We administered 
a user-study to validate the coherence of these topics in particular, relative to sets of random images. 
Details can be found in our poster draft under auxiliary materials. References BLEI, D., NG, A., JORDAN, 
M., AND LAFFERTY, J. 2003. Latent dirichlet allocation. Journal of Machine Learning Research 3, 993 1022. 
DUDA, R., AND HART, P. 1972. Use of the hough transformation to detect lines and curves in pictures. 
Commun. ACM 15, 11 15. HORSTER, E., LIENHART, R., AND SLANEY, M. 2007. Image re­trieval on large-scale 
image databases. In CIVR 07: Proc. of the 6th ACM international conference on image and video retrieval, 
17 24. NEWMAN, D., HAGEDORN, K., CHEMUDUGUNTA, C., AND SMYTH, P. 2007. Subject metadata enrichment using 
statistical topic models. In JCDL 07: Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital 
Libraries, 366 375. NISTER, D., AND STEWENIUS, H. 2006. Scalable recognition with a vocabulary tree. 
In CVPR 06: Proc. of the IEEE Confer­ence on Comp. Vision and Pattern Recognition, vol. 2, 2161 8. *e-mail: 
patrick.paczkowski@yale.edu Copyright is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British 
Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037817</article_id>
		<sort_key>1020</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>91</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>9</seq_no>
		<title><![CDATA[Visual navigation and classification of datasets in feature similarity space]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037817</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037817</url>
		<abstract>
			<par><![CDATA[<p>Discovering and understanding relationship patterns within a feature-rich archive (e.g. quantifying the degree of neuroanatomical similarity between the scanned subjects of a Magnetic Resonance Imaging (MRI) repository) is a nontrivial task. Scientists and expert users employ a variety of commodity algorithms for automated statistical analysis of feature patterns within a collection. But such analysis assumes the user is an experienced statistician, and disregards human visual processing capability. In this work we define a visual process for exploring the structure, relationships and patterns within a neuroimaging archive. Through dataset placement, our three-dimensional environment expresses similarity among the data. The application facilitates further analysis via two-stage exploratory clustering and classification.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809538</person_id>
				<author_profile_id><![CDATA[81466646370]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Ian]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Bowman]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[ibowman@loni.ucla.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809539</person_id>
				<author_profile_id><![CDATA[81327489198]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Shantanu]]></first_name>
				<middle_name><![CDATA[H.]]></middle_name>
				<last_name><![CDATA[Joshi]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[sjoshi@loni.ucla.edu]]></email_address>
			</au>
			<au>
				<person_id>P2809540</person_id>
				<author_profile_id><![CDATA[81453619830]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>3</seq_no>
				<first_name><![CDATA[John]]></first_name>
				<middle_name><![CDATA[D.]]></middle_name>
				<last_name><![CDATA[Van Horn]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of California, Los Angeles, Los Angeles]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[jack.vanhorn@loni.ucla.edu]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Joshi, S., Bowman, I., and Van Horn, J. D. 2011 (In Press). Brain pattern analysis of corticle valued distributions. In <i>Proceedings of the IEEE International Symposium on Biomedical Engineering</i>, 1--4.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visual Navigation and Classi.cation of Datasets in Feature Similarity Space Ian Bowman Shantanu H. 
Joshi John D. Van Horn Laboratory of Neuro Imaging, University of California, Los Angeles, Los Angeles, 
USA*  and color data members by feature similarity or subject metadata values. (c) Exploratory clustering 
and classi.cation quanti.es the feature and attribute distributions of the user-selected groups. 1 Introduction 
Discovering and understanding relationship patterns within a feature-rich archive (e.g. quantifying the 
degree of neuroanatomical similarity between the scanned subjects of a Magnetic Resonance Imaging (MRI) 
repository) is a nontrivial task. Scientists and ex­pert users employ a variety of commodity algorithms 
for automated statistical analysis of feature patterns within a collection. But such analysis assumes 
the user is an experienced statistician, and disre­gards human visual processing capability. In this 
work we de.ne a visual process for exploring the structure, relationships and pat­terns within a neuroimaging 
archive. Through dataset placement, our three-dimensional environment expresses similarity among the 
data. The application facilitates further analysis via two-stage ex­ploratory clustering and classi.cation. 
In summary, the contributions of this work are as follows: Our method i) is an interactive visualization 
technique applicable to large feature-rich collections, ii) facilitates individual as well as a macro-level 
investigation, iii) highlights the relationships between neuroanatomical feature and subject attribute 
data. 2 Our Approach In this paper we introduce the concept of a Feature Similarity Space (FSS). To 
create this exploratory environment, our user­driven dataset projection algorithm calculates placements 
suitable for representing feature similarity between data members. It con­sists of the following steps. 
1. Register and segment each MRI dataset, and construct surface meshes corresponding to neuroanatomical 
regions of interest. 2. Perform feature analysis on each surface to compute functions of curvature, 
thickness, and volume. 3. With a user-speci.ed subset of the region and feature values, apply a Jensen-Shannon 
based approach [Joshi et al. 2011 (In Press)] to de.ne pairwise distances between the data.  *e-mail: 
{ibowman,sjoshi,jack.vanhorn}@loni.ucla.edu 4. Use dimension reduction to compute a three-dimensional 
rep­resentation of the pairwise distance matrix. Our system draws each surface at its corresponding position 
deter­mined by the three-dimensional distance computed in 4. The envi­ronment facilitates discovery within 
this overview, providing tools for navigation, comparison and analysis. A multi-resolution strat­egy 
displays low-resolution meshes when the data is viewed at a distance. When the user zooms in on individual 
surfaces, the en­vironment swaps in a mesh with the polygon count increased by a factor of 10 (Fig. 1a). 
For cluster pattern analysis, input Boolean expressions highlight data with matching subject attribute 
values (e.g. Handedness = Left ). Due to FSS placement, such high­lighting rapidly illustrates relationships 
present between feature and attribute classes. The user is able to group such clusters for statis­tical 
processing (Fig. 1b). Graphical and automated implemen­tations classify these user selected groups by 
attribute and feature range, illustrating exact trends between neuroanatomical structure and subject 
metadata (Fig. 1c). From this analysis the user discov­ers speci.c hypotheses such as, Cortical thickness 
is proportional to subject weight. We demonstrated our approach on a T1-anatomical MRI image vol­ume 
archive consisting of approximately 600 subjects, collected from diverse clinical neuroimaging studies. 
With the resulting projection within FSS we were able to classify clusters of neu­roanatomically similar 
MRI images by diagnosis. In the future our system will provide save and exchange capabilities for speci.c 
views of data. We will also extend our implementation toward general feature-rich datasets with categorical 
attributes, neu­ral connectivity and genetics archives being a targeted application domain. References 
JOSHI, S., BOWMAN, I., AND VAN HORN, J. D. 2011 (In Press). Brain pattern analysis of corticle valued 
distributions. In Pro­ceedings of the IEEE International Symposium on Biomedical Engineering, 1 4. Copyright 
is held by the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. 
ISBN 978-1-4503-0921-9/11/0008  
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	<article_rec>
		<article_id>2037818</article_id>
		<sort_key>1030</sort_key>
		<display_label>Article No.</display_label>
		<pages>1</pages>
		<display_no>92</display_no>
		<article_publication_date>08-07-2011</article_publication_date>
		<seq_no>10</seq_no>
		<title><![CDATA[Visualizing the circulatory problems of Lisbon]]></title>
		<page_from>1</page_from>
		<page_to>1</page_to>
		<doi_number>10.1145/2037715.2037818</doi_number>
		<url>http://dl.acm.org/citation.cfm?id=2037818</url>
		<abstract>
			<par><![CDATA[<p>The traffic of Lisbon is portrayed exploring metaphors of living organisms with circulatory problems. Rather than being an aesthetic essay or a set of decorative artifacts, our approach focuses on synthesizing and conveying meaning through data portrayal [Donath et al, 2010]. Our portrayal is embodied by two visualization models with related visual metaphors: the clots in the traffic of Lisbon and the blood vessels in the traffic of Lisbon. The former evolves from a direct mapping of the data, producing clotted areas where the traffic velocity is low. The latter uses an adaptive physics system to build and manipulate the road network -- the thickness, the color and the length of the vessels are excited by the number of vehicles and average velocity in each road. With this system we distance ourselves from the strictness of contemporary visualizations that depict data accurately through direct mappings.</p>]]></par>
		</abstract>
		<categories>
			<primary_category>
				<cat_node>A.m</cat_node>
				<descriptor/>
				<type/>
			</primary_category>
			<other_category>
				<cat_node>I.3.0</cat_node>
				<descriptor></descriptor>
				<type></type>
			</other_category>
		</categories>
		<ccs2012>
			<concept>
				<concept_id>0.10010147.10010371</concept_id>
				<concept_desc>CCS->Computing methodologies->Computer graphics</concept_desc>
				<concept_significance>100</concept_significance>
			</concept>
			<concept>
				<concept_id>0.10002944</concept_id>
				<concept_desc>CCS->General and reference</concept_desc>
				<concept_significance>500</concept_significance>
			</concept>
		</ccs2012>
		<authors>
			<au>
				<person_id>P2809541</person_id>
				<author_profile_id><![CDATA[81318497551]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>1</seq_no>
				<first_name><![CDATA[Pedro]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Cruz]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[pmcruz@dei.uc.pt]]></email_address>
			</au>
			<au>
				<person_id>P2809542</person_id>
				<author_profile_id><![CDATA[81100379916]]></author_profile_id>
				<orcid_id></orcid_id>
				<seq_no>2</seq_no>
				<first_name><![CDATA[Penousal]]></first_name>
				<middle_name><![CDATA[]]></middle_name>
				<last_name><![CDATA[Machado]]></last_name>
				<suffix><![CDATA[]]></suffix>
				<affiliation><![CDATA[University of Coimbra]]></affiliation>
				<role><![CDATA[Author]]></role>
				<email_address><![CDATA[machado@dei.uc.pt]]></email_address>
			</au>
		</authors>
		<references>
			<ref>
				<ref_obj_id>1836793</ref_obj_id>
				<ref_obj_pid>1836786</ref_obj_pid>
				<ref_seq_no>1</ref_seq_no>
				<ref_text><![CDATA[Donath, J. et al., 2010. Data Portraits. In <i>SIGGRAPH 2010 Art Gallery</i>, ACM, 375--383.]]></ref_text>
				<ref_id></ref_id>
			</ref>
		</references>
		<fulltext>
			<file>
				<seq_no></seq_no>
				<fname></fname>
			</file>
			<ft_body><![CDATA[
 Visualizing the Circulatory Problems of Lisbon Pedro Cruz1 and Penousal Machado2 CISUC, Department of 
Informatics Engineering, University of Coimbra  .. Figure 1 Snapshots of the two visualization models. 
. and . depict the clots in Lisbon at 9:00 am and 7:00 pm respectively. These are theaftermath of the 
morning/afternoon rush hours, conveying that traf.c congestion problems diminish at the end of the day. 
. and . exhibit the blood vessels in Lisbon at 7:04 am and 8:44 am, right before and after the morning 
rush hour. It can be observed that at the beginning of the rush hour, the main vessels of Lisbon carry 
a high number of vehicles, but without traf.c congestions problems, as the vessels arecontracted. At 
8:44 am the average speed in the main vessels decreases originating an expansion of vessels and of the 
entire city. 1 Introduction The traf.c of Lisbon is portrayed exploring metaphors of living organisms 
with circulatory problems. Rather than being an aes­thetic essay or a set of decorative artifacts, our 
approach focuseson synthesizing and conveying meaning through data portrayal[Donath et al, 2010]. Our 
portrayal is embodied by two visualiza­tion models with related visual metaphors: the clots in the traf.cof 
Lisbon and the blood vessels in the traf.c of Lisbon. The for­mer evolves from a direct mapping of the 
data, producing clotted areas where the traf.c velocity is low. The latter uses an adaptivephysics system 
to build and manipulate the road network the thickness, the color and the length of the vessels are 
excited by thenumber of vehicles and average velocity in each road. With thissystem we distance ourselves 
from the strictness of contemporary visualizations that depict data accurately through direct mappings. 
2 Implementation and results The clots visualization displays each vehicle s GPS trace persis­tently 
during 30 minutes in simulation time, meaning that a vehi­cle leaves a trail upon its passage. This is 
a technique that investsin building visual statistics from the data, rather than working with it directly 
through data mining. Clots in the traf.c of Lisbon convey the city as an organism with circulatory problems. 
Thevessels emerge visually from the black trails of the vehicles, with a mutable thicknesses that represents 
the traf.c intensity of thosevessels. Clots are represented by red circles, which are drawn when the 
speed of a vehicle is low with a diameter inversely pro­portional to speed (Fig 1). The vessels visualization 
follows a different approach. First, theroad network of Lisbon was queried from OpenSreetMap, parsed 
and .ltered. Using this information, a spring based physics systemis built for the road network and the 
structure of each vessel de­termined (Fig. 2). The data is overlaid on the resultant structure to determine 
the road where each vehicle is at a given moment. Thisallows us to inject data at runtime and excite 
the system as fol­lows: a bigger number of vehicles on a vessel tends to make it thicker, high speeds 
tend to contract its length (and vice-versa). The latter behavior was chosen in order to transmit a global 
im­pression of the perceived distances within the city. This behavior shrinks the city when the traf.c 
velocities are high, and distends itin the rush hours when the city faces congestion problems. In what 
concerns coloring, low speeds imply the darkening of a ves­sel, expressing slow circulation and stagnant 
blood. Figure 2 Detail of the springs con.guration used to build thestructure of each vessel. Analyzing 
the results (Fig. 1), the clots visualization results in elegant artifacts, well accommodated with de 
facto aesthetics for traf.c visualization. The clots tend to emphasize the spots wherevehicles slow down 
and stop, being dif.cult to distinguish which of the vessels is more problematic in traf.c intensity 
and speed. In contrast, the vessels visualization, with crude aesthetics that areinnate to our visual 
metaphor, pulsates for each rush hour and stresses which roads are congested. Figure 3 Closeup of the 
blood vessels visualization depicting theemphasis on problematic highways, namely .CRIL, .Segunda Circular, 
.Eixo Norte-Sul and .CREL. References DONATH, J. et al., 2010. Data Portraits. In SIGGRAPH 2010 Art Gallery, 
ACM, 375-383. 1 pmcruz@dei.uc.pt This work has been partially supported by the2 machado@dei.uc.pt project 
PTDC/EIA-EIA/108785/2008 COSMO COllaborative System for Mobility Optimization. Copyright is held by 
the author / owner(s). SIGGRAPH 2011, Vancouver, British Columbia, Canada, August 7 11, 2011. ISBN 978-1-4503-0921-9/11/0008 
 
			]]></ft_body>
		</fulltext>
		<article_type art_type="regular_article" />
	</article_rec>
	</section>
</content>
</proceeding>
